Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394?1404,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Experimental Support for a Categorical Compositional
Distributional Model of Meaning
Edward Grefenstette
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
edward.grefenstette@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
mehrs@cs.ox.ac.uk
Abstract
Modelling compositional meaning for sen-
tences using empirical distributional methods
has been a challenge for computational lin-
guists. We implement the abstract categorical
model of Coecke et al (2010) using data from
the BNC and evaluate it. The implementation
is based on unsupervised learning of matrices
for relational words and applying them to the
vectors of their arguments. The evaluation is
based on the word disambiguation task devel-
oped by Mitchell and Lapata (2008) for intran-
sitive sentences, and on a similar new experi-
ment designed for transitive sentences. Our
model matches the results of its competitors
in the first experiment, and betters them in the
second. The general improvement in results
with increase in syntactic complexity show-
cases the compositional power of our model.
1 Introduction
As competent language speakers, we humans can al-
most trivially make sense of sentences we?ve never
seen or heard before. We are naturally good at un-
derstanding ambiguous words given a context, and
forming the meaning of a sentence from the mean-
ing of its parts. But while human beings seem
comfortable doing this, machines fail to deliver.
Search engines such as Google either fall back on
bag of words models?ignoring syntax and lexical
relations?or exploit superficial models of lexical
semantics to retrieve pages with terms related to
those in the query (Manning et al, 2008).
However, such models fail to shine when it comes
to processing the semantics of phrases and sen-
tences. Discovering the process of meaning as-
signment in natural language is among the most
challenging and foundational questions of linguis-
tics and computer science. The findings thereof will
increase our understanding of cognition and intelli-
gence and shall assist in applications to automating
language-related tasks such as document search.
Compositional type-logical approaches (Mon-
tague, 1974; Lambek, 2008) and distributional mod-
els of lexical semantics (Schutze, 1998; Firth, 1957)
have provided two partial orthogonal solutions to the
question. Compositional formal semantic models
stem from classical ideas from mathematical logic,
mainly Frege?s principle that the meaning of a sen-
tence is a function of the meaning of its parts (Frege,
1892). Distributional models are more recent and
can be related to Wittgenstein?s later philosophy of
?meaning is use?, whereby meanings of words can be
determined from their context (Wittgenstein, 1953).
The logical models relate to well known and robust
logical formalisms, hence offering a scalable theory
of meaning which can be used to reason inferen-
tially. The distributional models have found their
way into real world applications such as thesaurus
extraction (Grefenstette, 1994; Curran, 2004) or au-
tomated essay marking (Landauer, 1997), and have
connections to semantically motivated information
retrieval (Manning et al, 2008). This two-sortedness
of defining properties of meaning: ?logical form?
versus ?contextual use?, has left the quest for ?what is
the foundational structure of meaning?? even more
of a challenge.
Recently, Coecke et al (2010) used high level
cross-disciplinary techniques from logic, category
1394
theory, and physics to bring the above two ap-
proaches together. They developed a unified mathe-
matical framework whereby a sentence vector is by
definition a function of the Kronecker product of its
word vectors. A concrete instantiation of this the-
ory was exemplified on a toy hand crafted corpus
by Grefenstette et al (2011). In this paper we imple-
ment it by training the model over the entire BNC.
The highlight of our implementation is that words
with relational types, such as verbs, adjectives, and
adverbs are matrices that act on their arguments. We
provide a general algorithm for building (or indeed
learning) these matrices from the corpus.
The implementation is evaluated against the task
provided by Mitchell and Lapata (2008) for disam-
biguating intransitive verbs, as well as a similar new
experiment for transitive verbs. Our model improves
on the best method evaluated in Mitchell and Lapata
(2008) and offers promising results for the transitive
case, demonstrating its scalability in comparison to
that of other models. But we still feel there is need
for a different class of experiments to showcase mer-
its of compositionality in a statistically significant
manner. Our work shows that the categorical com-
positional distributional model of meaning permits
a practical implementation and that this opens the
way to the production of large scale compositional
models.
2 Two Orthogonal Semantic Models
Formal Semantics To compute the meaning of a
sentence consisting of n words, meanings of these
words must interact with one another. In formal se-
mantics, this further interaction is represented as a
function derived from the grammatical structure of
the sentence, but meanings of words are amorphous
objects of the domain: no distinction is made be-
tween words that have the same type. Such models
consist of a pairing of syntactic interpretation rules
(in the form of a grammar) with semantic interpreta-
tion rules, as exemplified by the simple model pre-
sented in Figure 1.
The parse of a sentence such as ?cats like milk?
typically produces its semantic interpretation by
substituting semantic representation for their gram-
matical constituents and applying ?-reduction where
needed. Such a derivation is shown in Figure 2.
Syntactic Analysis Semantic Interpretation
S? NP VP |V P |(|NP |)
NP? cats, milk, etc. |cats|, |milk|, . . .
VP? Vt NP |V t|(|NP |)
Vt? like, hug, etc. ?yx.|like|(x, y), . . .
Figure 1: A simple model of formal semantics.
|like|(|cats|, |milk|)
|cats| ?x.|like|(x, |milk|)
|milk| ?yx.|like|(x, y)
Figure 2: A parse tree showing a semantic derivation.
This methodology is used to translate sentences
of natural language into logical formulae, then use
computer-aided automation tools to reason about
them (Alshawi, 1992). One major drawback is that
the result of such analysis can only deal with truth
or falsity as the meaning of a sentence, and says
nothing about the closeness in meaning or topic of
expressions beyond their truth-conditions and what
models satisfy them, hence do not perform well on
language tasks such as search. Furthermore, an un-
derlying domain of objects and a valuation function
must be provided, as with any logic, leaving open
the question of how we might learn the meaning of
language using such a model, rather than just use it.
Distributional Models Distributional models of
semantics, on the other hand, dismiss the interaction
between syntactically linked words and are solely
concerned with lexical semantics. Word meaning
is obtained empirically by examining the contexts1
in which a word appears, and equating the meaning
of a word with the distribution of contexts it shares.
The intuition is that context of use is what we ap-
peal to in learning the meaning of a word, and that
words that frequently have the same sort of context
in common are likely to be semantically related.
For instance, beer and sherry are both drinks, al-
coholic, and often cause a hangover. We expect
these facts to be reflected in a sufficiently large cor-
pus: the words ?beer? and ?sherry? occur within the
1E.g. words which appear in the same sentence or n-word
window, or words which hold particular grammatical or depen-
dency relations to the word being learned.
1395
context of identifying words such as ?drink?, ?alco-
holic? and ?hangover? more frequently than they oc-
cur with other content words.
Such context distributions can be encoded as vec-
tors in a high dimensional space with contexts as
basis vectors. For any word vector ???word, the scalar
weight cwordi associated with each context basis vec-
tor ??ni is a function of the number of times the
word has appeared in that context. Semantic vectors
(cword1 , cword2 , ? ? ? , cwordn ) are also denoted by sums
of such weight/basis vector pairs:
???word =
?
i
cwordi ??ni
Learning a semantic vector is just learning its ba-
sis weights from the corpus. This setting offers ge-
ometric means to reason about semantic similarity
(e.g. via cosine measure or k-means clustering), as
discussed in Widdows (2005).
The principal drawback of such models is their
non-compositional nature: they ignore grammatical
structure and logical words, and hence cannot com-
pute the meanings of phrases and sentences in the
same efficient way that they do for words. Com-
mon operations discussed in (Mitchell and Lapata,
2008) such as vector addition (+) and component-
wise multiplication (, cf. ?4 for details) are com-
mutative, hence if ??vw = ??v + ??w or ??v  ??w , then
??vw = ??wv, leading to unwelcome equalities such as
??????????????the dog bit the man = ??????????????the man bit the dog
Non-commutative operations, such as the Kronecker
product (cf. ?4 for definition) can take word-order
into account (Smolensky, 1990) or even some more
complex syntactic relations, as described in Clark
and Pulman (2007). However, the dimensionality of
sentence vectors produced in this manner differs for
sentences of different length, barring all sentences
from being compared in the same vector space, and
growing exponentially with sentence length hence
quickly becoming computationally intractable.
3 A Hybrid Logico-Distributional Model
Whereas semantic compositional mechanisms for
set-theoretic constructions are well understood,
there are no obvious corresponding methods for vec-
tor spaces. To solve this problem, Coecke et al
(2010) use the abstract setting of category theory to
turn the grammatical structure of a sentence into a
morphism compatible with the higher level logical
structure of vector spaces.
One pragmatic consequence of this abstract idea
is as follows. In distributional models, there is a
meaning vector for each word, e.g. ??cats, ??like, and???milk. The logical recipe tells us to apply the mean-
ing of the verb to the meanings of subject and object.
But how can a vector apply to other vectors? The so-
lution proposed above implies that one needs to have
different levels of meaning for words with different
types. This is similar to logical models where verbs
are relations and nouns are atomic sets. So verb vec-
tors should be built differently from noun vectors,
for instance as matrices.
The general information as to which words should
be matrices and which words atomic vectors is in
fact encoded in the type-logical representation of the
grammatical structure of the sentence. This is the
linear map with word vectors as input and sentence
vectors as output. Hence, at least theoretically, one
should be able to build sentence vectors and com-
pare their synonymity in exactly the same way as
one measures word synonymity.
Pregroup Grammars The aforementioned linear
maps turn out to be the grammatical reductions
of a type-logic called a Lambek pregroup gram-
mar (Lambek, 2008)2. Pregroups and vector spaces
share the same high level mathematical structure, re-
ferred to as a compact closed category, for a proof
and details of this claim see Coecke et al (2010); for
a friendly introduction to category theory, see Co-
ecke and Paquette (2011). One consequence of this
parity is that the grammatical reductions of a pre-
group grammar can be directly transformed into lin-
ear maps that act on vectors.
In a nutshell, pregroup types are either atomic
or compound. Atomic types can be simple (e.g. n
for noun phrases, s for statements) or left/right
superscripted?referred to as adjoint types (e.g. nr
and nl). An example of a compound type is that of
a verb nrsnl. The superscripted types express that
the verb is a relation with two arguments of type n,
2The usage of pregroup types is not essential, the types of
any other logic, for instance CCG can be used, but should be
translated into the language of pregroups.
1396
which have to occur to the right and to the left of
it, and that it outputs an argument of the type s. A
transitive sentence has types as shown in Figure 3.
Each type n cancels out with its right adjoint nr
from the right and its left adjoint nl from the left;
mathematically speaking these mean3
nln ? 1 and nnr ? 1
Here 1 is the unit of concatenation: 1n = n1 =
n. The corresponding grammatical reduction of a
transitive sentence is nnrsnl ? 1s1 = s. Each such
reduction can be depicted as a wire diagram. The
diagram of a transitive sentence is shown in Figure 3.
Cats
n
like
nr s nl
milk.
n
Figure 3: The pregroup types and reduction diagram for
a transitive sentence.
Syntax-guided Semantic Composition Accord-
ing to Coecke et al (2010) and based on a general
completeness theorem between compact categories,
wire diagrams, and vector spaces, the meaning of
sentences can be canonically reduced to linear alge-
braic formulae. The following is the meaning vector
of our transitive sentence:
?????????cats like milk = (f)
(??cats???like????milk
)
(I)
Here f is the linear map that encodes the grammati-
cal structure. The categorical morphism correspond-
ing to it is denoted by the tensor product of 3 compo-
nents: V ?1S?W , where V andW are subject and
object spaces, S is the sentence space, the ?s are the
cups, and 1S is the straight line in the diagram. The
cups stand for taking inner products, which when
done with the basis vectors imitate substitution. The
straight line stands for the identity map that does
nothing. By the rules of the category, equation (I) re-
duces to the following linear algebraic formula with
3The relation? is the partial order of the pregroup. It corre-
sponds to implication =? in a logical reading thereof. If these
inequalities are replaced by equalities, i.e. if nln = 1 = nnr ,
then the pregroup collapses into a group where nl = nr .
lower dimensions, hence the dimensional explosion
problem for Kronecker products is avoided:
?
itj
citj???cats|??vi ???st ???wj|???milk? ? S (II)
??vi ,??wj are basis vectors of V and W . The inner
product ???cats | ??vi ? substitutes the weights of ??cats
into the first argument place of the verb (similarly
for object and second argument place). ??st is a basis
vector of the sentence space S in which meanings of
sentences live, regardless of their grammatical struc-
ture.
The degree of synonymity of sentences is ob-
tained by taking the cosine measure of their vectors.
S is an abstract space: it needs to be instantiated
to provide concrete meanings and synonymity mea-
sures. For instance, a truth-theoretic model is ob-
tained by taking the sentence space S to be the 2-
dimensional space with basis vectors |1? (True) and
|0? (False).
4 Building Matrices for Relational Words
In this section we present a general scheme to build
matrices for relational words. Recall that given
a vector space A with basis {??ni}i, the Kronecker
product of two vectors ??v = ?i cai??ni and ??w =?
i cbi??ni is defined as follows:
??v ???w =
?
ij
cai cbj (??ni ???nj)
where (??ni ???nj) is just the pairing of the basis of A,
i.e. (??ni ,??nj). The Kronecker product vectors belong
in the tensor product of A with itself: A?A, hence
ifA has dimension r, these will be of dimensionality
r?r. The point-wise multiplication of these vectors
is defined as follows
??v ??w =
?
i
cai cbi ??ni
The intuition behind having a matrix for a rela-
tional word is that any relation R on sets X and Y ,
i.e. R ? X ? Y can be represented as a matrix,
namely one that has as row-bases x ? X and as
column-bases y ? Y , with weight cxy = 1 where
(x, y) ? R and 0 otherwise. In a distributional set-
ting, the weights, which are natural or real numbers,
1397
will represent more: ?the extent according to which
x and y are related?. This can be determined in dif-
ferent ways.
Suppose X is the set of animals, and ?chase? is a
relation on it: chase ? X ? X . Take x = ?dog?
and y = ?cat?: with our type-logical glasses on, the
obvious choice would be to take cxy to be the num-
ber of times ?dog? has chased ?cat?, i.e. the number
of times the sentence ?the dog chases the cat? has
appeared in the corpus. But in the distributional set-
ting, this method will be too syntactic and dismissive
of the actual meaning of ?cat? and ?dog?. If instead
the corpus contains the sentence ?the hound hunted
the wild cat?, cxy will be 0, restricting us to only
assign meaning to sentences that have directly ap-
peared in the corpus. We propose to, instead, use a
level of abstraction by taking words such as verbs to
be distributions over the semantic information in the
vectors of their context words, rather than over the
context words themselves.
Start with an r-dimensional vector space N with
basis {??n i}i, in which meaning vectors of atomic
words, such as nouns, live. The basis vectors of N
are in principle all the words from the corpus, how-
ever in practice and following Mitchell and Lapata
(2008) we had to restrict these to a subset of the
most occurring words. These basis vectors are not
restricted to nouns: they can as well be verbs, adjec-
tives, and adverbs, so that we can define the mean-
ing of a noun in all possible contexts?as is usual
in context-based models?and not only in the con-
text of other nouns. Note that basis words with re-
lational types are treated as pure lexical items rather
than as semantic objects represented as matrices. In
short, we count how many times a noun has occurred
close to words of other syntactic types such as ?elect?
and ?scientific?, rather than count how many times it
has occurred close to their corresponding matrices:
it is the lexical tokens that form the context, not their
meaning.
Each relational word P with grammatical type pi
and m adjoint types ?1, ?2, ? ? ? , ?m is encoded as
an (r ? . . .? r) matrix with m dimensions. Since
our vector space N has a fixed basis, each such ma-
trix is represented in vector form as follows:
??P =
?
ij ? ? ? ?? ?? ?
m
cij???? (??n i ???n j ? ? ? ? ? ??n ?)? ?? ?
m
This vector lives in the tensor space
N ?N ? ? ? ? ?N? ?? ?
m
. Each cij???? is computed
according to the procedure described in Figure 4.
1) Consider a sequence of words containing a re-
lational word ?P? and its arguments w1, w2, ? ? ? ,
wm, occurring in the same order as described in
P?s grammatical type pi. Refer to these sequences
as ?P?-relations. Suppose there are k of them.
2) Retrieve the vector ??w l of each argument wl.
3) Suppose w1 has weight c1i on basis vector ??n i,
w2 has weight c2j on basis vector ??n j , ? ? ? , and
wm has weight cm? on basis vector ??n ? . Multiply
these weights
c1i ? c2j ? ? ? ? ? cm?
4) Repeat the above steps for all the k ?P?-
relations, and suma the corresponding weights
cij???? =
?
k
(
c1i ? c2j ? ? ? ? ? cm?
)
k
aWe also experimented with multiplication, but the spar-
sity of noun vectors resulted in most verb matrices being
empty.
Figure 4: Procedure for learning weights for matrices of
words ?P? with relational types pi of m arguments.
Linear algebraically, this procedure corresponds to
computing the following
??P =
?
k
(??w 1 ???w 2 ? ? ? ? ? ??wm
)
k
Type-logical examples of relational words are
verbs, adjectives, and adverbs. A transitive verb is
represented as a 2 dimensional matrix since its type
is nrsnl with two adjoint types nr and nl. The cor-
responding vector of this matrix is
???verb =
?
ij
cij (??n i ???n j)
1398
The weight cij corresponding to basis vector??n i???n j , is the extent according to which words that have
co-occurred with ??n i have been the subject of the
?verb? and words that have co-occurred with ??n j
have been the object of the ?verb?. This example
computation is demonstrated in Figure 5.
1) Consider phrases containing ?verb?, its subject
w1 and object w2. Suppose there are k of them.
2) Retrieve vectors ??w 1 and ??w 2.
3) Suppose ??w 1 has weight c1i on ??n i and ??w 2 has
c2j on ??n j . Multiply these weights c1i ? c2j .
4) Repeat the above steps for all k ?verb?-
relations and sum the corresponding weights?
k(c1i ? c2j )k
Figure 5: Procedure for learning weights for matrices of
transitive verbs.
Linear algebraically, we are computing
???verb =
?
k
(??w 1 ???w 2
)
k
As an example, consider the verb ?show? and sup-
pose there are two ?show?-relations in the corpus:
s1 = table show result
s2 = map show location
The vector of ?show? is
???show = ???table?????result + ???map??????location
Consider an N space with four basis vectors ?far?,
?room?, ?scientific?, and ?elect?. The TF/IDF-
weighted values for vectors of the above four nouns
(built from the BNC) are as shown in Table 1.
i ??ni table map result location
1 far 6.6 5.6 7 5.9
2 room 27 7.4 0.99 7.3
3 scientific 0 5.4 13 6.1
4 elect 0 0 4.2 0
Table 1: Sample weights for selected noun vectors.
Part of the matrix of ?show? is presented in Table 2.
As a sample computation, the weight c11 for
vector (1, 1), i.e. (??far,??far) is computed by multiply-
ing weights of ?table? and ?result? on??far, i.e. 6.6?7,
far room scientific elect
far 79.24 47.41 119.96 27.72
room 232.66 80.75 396.14 113.2
scientific 32.94 31.86 32.94 0
elect 0 0 0 0
Table 2: Sample semantic matrix for ?show?.
multiplying weights of ?map? and ?location? on ??far,
i.e. 5.6 ? 5.9 then adding these 46.2 + 33.04 and
obtaining the total weight 79.24.
The same method is applied to build matrices for di-
transitive verbs, which will have 3 dimensions, and
adjectives and adverbs, which will be of 1 dimension
each.
5 Computing Sentence Vectors
Meaning of sentences are vectors computed by tak-
ing the variables of the categorical prescription of
meaning (the linear map f obtained from the gram-
matical reduction of the sentence) to be determined
by the matrices of the relational words. For instance
the meaning of the transitive sentence ?sub verb obj?
is:
?????????sub verb obj =
?
itj
???sub | ??v i????w j | ??obj? citj??s t
We take V := W := N and S = N ? N , then?
itj citj??s t is determined by the matrix of the verb,
i.e. substitute it by ?ij cij(??n i ? ??n j)4. Hence?????????sub verb obj becomes:
?
ij
???sub | ??n i????n j | ??obj?cij(??n i ???n j) =
?
ij
csubi cobjj cij(??n i ???n j)
This can be decomposed to point-wise multiplica-
tion of two vectors as follows:
(?
ij
csubi cobjj (??n i???n j)
)

(?
ij
cij(??n i???n j)
)
4Note that by doing so we are also reducing the verb space
from N ? (N ?N)?N to N ?N , since for our construction
we only need tuples of the form ??n i ???n i ???n j ???n j which
are isomorphic to pairs (??n i ???n j).
1399
The left argument is the Kronecker product of sub-
ject and object vectors and the right argument is the
vector of the verb, so we obtain
(??sub???obj
)
???verb
Since  is commutative, this provides us with a dis-
tributional version of the type-logical meaning of the
sentence: point-wise multiplication of the meaning
of the verb to the Kronecker product of its subject
and object:
?????????sub verb obj = ???verb
(??sub???obj
)
This mathematical operation can be informally de-
scribed as a structured ?mixing? of the information
of the subject and object, followed by it being ?fil-
tered? through the information of the verb applied
to them, in order to produce the information of the
sentence.
In the transitive case, S = N ? N , hence ??s t =??n i ? ??n j . More generally, the vector space cor-
responding to the abstract sentence space S is the
concrete tensor space (N ? . . .?N) for m the di-
mension of the matrix of the ?verb?. As we have
seen above, in practice we do not need to build this
tensor space, as the computations thereof reduce to
point-wise multiplications and summations.
Similar computations yield meanings of sentences
with adjectives and adverbs. For instance the mean-
ing of a transitive sentence with a modified subject
and a modified verb we have
??????????????adj sub verb obj adv =
(??adv???verb
)

((??adj??sub
)
???obj
)
After building vectors for sentences, we can com-
pare their meaning and measure their degree of syn-
onymy by taking their cosine measure.
6 Evaluation
Evaluating such a framework is no easy task. What
to evaluate depends heavily on what sort of applica-
tion a practical instantiation of the model is geared
towards. In (Grefenstette et al, 2011), it is sug-
gested that the simplified model we presented and
expanded here could be evaluated in the same way as
lexical semantic models, measuring compositionally
built sentence vectors against a benchmark dataset
such as that provided by Mitchell and Lapata (2008).
In this section, we briefly describe the evaluation of
our model against this dataset. Following this, we
present a new evaluation task extending the experi-
mental methodology of Mitchell and Lapata (2008)
to transitive verb-centric sentences, and compare our
model to those discussed by Mitchell and Lapata
(2008) within this new experiment.
First Dataset Description The first experiment,
described in detail by Mitchell and Lapata (2008),
evaluates how well compositional models disam-
biguate ambiguous words given the context of a po-
tentially disambiguating noun. Each entry of the
dataset provides a noun, a target verb and landmark
verb (both intransitive). The noun must be com-
posed with both verbs to produce short phrase vec-
tors the similarity of which is measured by the can-
didate. Also provided with each entry is a classifi-
cation (?High? or ?Low?) indicating whether or not
the verbs are indeed semantically close within the
context of the noun, as well as an evaluator-set simi-
larity score between 1 and 7 (along with an evaluator
identifier), where 1 is low similarity and 7 is high.
Evaluation Methodology Candidate models pro-
vide a similarity score for each entry. The scores
of high similarity entries and low similarity entries
are averaged to produce a mean High score and
mean Low score for the model. The correlation of
the model?s similarity judgements with the human
judgements is also calculated using Spearman?s ?, a
metric which is deemed to be more scrupulous, and
ultimately that by which models should be ranked,
by Mitchell and Lapata (2008). The mean for each
model is on a [0, 1] scale, except for UpperBound
which is on the same [1, 7] scale the annotators used.
The ? scores are on a [?1, 1] scale. It is assumed
that inter-annotator agreement provides the theoret-
ical maximum ? for any model for this experiment.
The cosine measure of the verb vectors, ignoring the
noun, is taken to be the baseline (no composition).
Other Models The other models we compare
ours to are those evaluated by Mitchell and Lap-
ata (2008). We provide a selection of the results
1400
from that paper for the worst (Add) and best5 (Mul-
tiply) performing models, as well as the previous
second-best performing model (Kintsch). The ad-
ditive and multiplicative models are simply applica-
tions of vector addition and component-wise multi-
plication. We invite the reader to consult (Mitchell
and Lapata, 2008) for the description of Kintsch?s
additive model and parametric choices.
Model Parameters To provide the most accurate
comparison with the existing multiplicative model,
and exploiting the aforementioned feature that the
categorical model can be built ?on top of? existing
lexical distributional models, we used the parame-
ters described by Mitchell and Lapata (2008) to re-
produce the vectors evaluated in the original exper-
iment as our noun vectors. All vectors were built
from a lemmatised version of the BNC. The noun
basis was the 2000 most common context words,
basis weights were the probability of context words
given the target word divided by the overall proba-
bility of the context word. Intransitive verb function-
vectors were trained using the procedure presented
in ?4. Since the dataset only contains intransitive
verbs and nouns, we used S = N . The cosine mea-
sure of vectors was used as a similarity metric.
First Experiment Results In Table 3 we present
the comparison of the selected models. Our categor-
ical model performs significantly better than the ex-
isting second-place (Kintsch) and obtains a ? quasi-
identical to the multiplicative model, indicating sig-
nificant correlation with the annotator scores.
There is not a large difference between the mean
High score and mean Low score, but the distri-
bution in Figure 6 shows that our model makes a
non-negligible distinction between high similarity
phrases and low similarity phrases, despite the ab-
solute scores not being different by more than a few
percentiles.
5The multiplicative model presented here is what is quali-
fied as best in (Mitchell and Lapata, 2008). However, they also
present a slightly better performing (? = 0.19) model which
is a combination of their multiplicative model and a weighted
additive model. The difference in ? is qualified as ?not sta-
tistically significant? in the original paper, and furthermore the
mixed model requires parametric optimisation hence was not
evaluated against the entire test set. For these reasons, we chose
not to include it in the comparison.
Model High Low ?
Baseline 0.27 0.26 0.08
Add 0.59 0.59 0.04
Kintsch 0.47 0.45 0.09
Multiply 0.42 0.28 0.17
Categorical 0.84 0.79 0.17
UpperBound 4.94 3.25 0.40
Table 3: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, first experiment (Mitchell and Lapata, 2008). p <
0.05 for each ?.
High Low0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 6: Distribution of predicted similarities for the cat-
egorical distributional model on High and Low similarity
items.
Second Dataset Description The second dataset6,
developed by the authors, follows the format of the
(Mitchell and Lapata, 2008) dataset used for the first
experiment, with the exception that the target and
landmark verbs are transitive, and an object noun
is provided in addition to the subject noun, hence
forming a small transitive sentence. The dataset
comprises 200 entries consisting of sentence pairs
(hence a total of 400 sentences) constructed by fol-
lowing the procedure outlined in ?4 of (Mitchell and
Lapata, 2008), using transitive verbs from CELEX7.
For examples of these sentences, see Table 4. The
dataset was split into four sections of 100 entries
each, with guaranteed 50% exclusive overlap with
6http://www.cs.ox.ac.uk/activities/CompD
istMeaning/GS2011data.txt
7http://celex.mpi.nl/
1401
exactly two other datasets. Each section was given
to a group of evaluators, with a total of 25, who were
asked to form simple transitive sentence pairs from
the verbs, subject and object provided in each entry;
for instance ?the table showed the result? from ?table
show result?. The evaluators were then asked to rate
the semantic similarity of each verb pair within the
context of those sentences, and offer a score between
1 and 7 for each entry. Each entry was given an arbi-
trary classification of HIGH or LOW by the authors,
for the purpose of calculating mean high/low scores
for each model. For example, the first two pairs in
table 4 were classified as HIGH, whereas the second
two pairs as LOW.
Sentence 1 Sentence 2
table show result table express result
map show location map picture location
table show result table picture result
map show location map express location
Table 4: Example entries from the transitive dataset with-
out annotator score, second experiment.
Evaluation Methodology The evaluation
methodology for the second experiment was
identical to that of the first, as are the scales for
means and scores. Here also, Spearman?s ? is
deemed a more rigorous way of determining how
well a model tracks difference in meaning. This is
both because of the imprecise nature of the classifi-
cation of verb pairs as HIGH or LOW; and since the
objective similarity scores produced by a model that
distinguishes sentences of different meaning from
those of similar meaning can be renormalised in
practice. Therefore the delta between HIGH means
and LOW mean cannot serve as a definite indication
of the practical applicability (or lack thereof) of
semantic models; the means are provided just to aid
comparison with the results of the first experiment.
Model Parameters As in the first experiment, the
lexical vectors from (Mitchell and Lapata, 2008)
were used for the other models evaluated (additive,
multiplicative and baseline)8 and for the noun vec-
8Kintsch was not evaluated as it required optimising model
parameters against a held-out segment of the test set, and we
could not replicate the methodology of Mitchell and Lapata
tors of our categorical model. Transitive verb vec-
tors were trained as described in ?4 with S = N?N .
Second Experiment Results The results for the
models evaluated against the second dataset are pre-
sented in Table 5.
Model High Low ?
Baseline 0.47 0.44 0.16
Add 0.90 0.90 0.05
Multiply 0.67 0.59 0.17
Categorical 0.73 0.72 0.21
UpperBound 4.80 2.49 0.62
Table 5: Selected model means for High and Low similar-
ity items and correlation coefficients with human judge-
ments, second experiment. p < 0.05 for each ?.
We observe a significant (according to p < 0.0.5)
improvement in the alignment of our categorical
model with the human judgements, from 0.17 to
0.21. The additive model continues to make lit-
tle distinction between senses of the verb during
composition, and the multiplicative model?s align-
ment does not change, but becomes statistically in-
distinguishable from the non-compositional baseline
model.
Once again we note that the high-low means are
not very indicative of model performance, as the dif-
ference between high mean and the low mean of the
categorical model is much smaller than that of the
both the baseline model and multiplicative model,
despite better alignment with annotator judgements.
7 Discussion
In this paper, we described an implementation of the
categorical model of meaning (Coecke et al, 2010),
which combines the formal logical and the empiri-
cal distributional frameworks into a unified seman-
tic model. The implementation is based on build-
ing matrices for words with relational types (ad-
jectives, verbs), and vectors for words with atomic
types (nouns), based on data from the BNC. We
then show how to apply verbs to their subject/object,
in order to compute the meaning of intransitive and
transitive sentences.
(2008) with full confidence.
1402
Other work uses matrices to model meaning (Ba-
roni and Zamparelli, 2010; Guevara, 2010), but only
for adjective-noun phrases. Our approach easily ap-
plies to such compositions, as well as to sentences
containing combinations of adjectives, nouns, verbs,
and adverbs. The other key difference is that they
learn their matrices in a top-down fashion, i.e. by re-
gression from the composite adjective-noun context
vectors, whereas our model is bottom-up: it learns
sentence/phrase meaning compositionally from the
vectors of the compartments of the composites. Fi-
nally, very similar functions, for example a verb with
argument alternations such as ?break? in ?Y breaks?
and ?X breaks Y?, are not treated as unrelated. The
matrix of the intransitive ?break? uses the corpus-
observed information about the subject of break, in-
cluding that of ?Y?, similarly the matrix of the tran-
sitive ?break? uses information about its subject and
object, including that of ?X? and ?Y?. We leave a
thorough study of these phenomena, which fall un-
der providing a modular representation of passive-
active similarities, to future work.
We evaluated our model in two ways: first against
the word disambiguation task of Mitchell and Lap-
ata (2008) for intransitive verbs, and then against a
similar new experiment for transitive verbs, which
we developed.
Our findings in the first experiment show that
the categorical method performs on par with the
leading existing approaches. This should not sur-
prise us given that the context is so small and our
method becomes similar to the multiplicative model
of Mitchell and Lapata (2008). However, our ap-
proach is sensitive to grammatical structure, lead-
ing us to develop a second experiment taking this
into account and differentiating it from models with
commutative composition operations.
The second experiment?s results deliver the ex-
pected qualitative difference between models, with
our categorical model outperforming the others and
showing an increase in alignment with human judge-
ments in correlation with the increase in sentence
complexity. We use this second evaluation princi-
pally to show that there is a strong case for the devel-
opment of more complex experiments measuring not
only the disambiguating qualities of compositional
models, but also their syntactic sensitivity, which is
not directly measured in the existing experiments.
These results show that the high level categori-
cal distributional model, uniting empirical data with
logical form, can be implemented just like any other
concrete model. Furthermore it shows better results
in experiments involving higher syntactic complex-
ity. This is just the tip of the iceberg: the mathe-
matics underlying the implementation ensures that
it uniformly scales to larger, more complicated sen-
tences and enables it to compare synonymity of sen-
tences that are of different grammatical structure.
8 Future Work
Treatment of function words such as ?that?, ?who?,
as well as logical words such as quantifiers and con-
junctives are left to future work. This will build
alongside the general guidelines of Coecke et al
(2010) and concrete insights from the work of Wid-
dows (2005). It is not yet entirely clear how ex-
isting set-theoretic approaches, for example that of
discourse representation and generalised quantifiers,
apply to our setting. Preliminary work on integration
of the two has been presented by Preller (2007) and
more recently also by Preller and Sadrzadeh ( 2009).
As mentioned by one of the reviewers, our pre-
group approach to grammar flattens the sentence
representation, in that the verb is applied to its sub-
ject and object at the same time; whereas in other
approaches such as CCG, it is first applied to the
object to produce a verb phrase, then applied to the
subject to produce the sentence. The advantages and
disadvantages of this method and comparisons with
other systems, in particular CCG, constitutes ongo-
ing work.
9 Acknowledgement
We wish to thank P. Blunsom, S. Clark, B. Coecke,
S. Pulman, and the anonymous EMNLP review-
ers for discussions and comments. Support from
EPSRC grant EP/F042728/1 is gratefully acknowl-
edged by M. Sadrzadeh.
References
H. Alshawi (ed). 1992. The Core Language Engine.
MIT Press.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices. Proceedings of Conference
1403
on Empirical Methods in Natural Language Processing
(EMNLP).
S. Clark and S. Pulman. 2007. Combining Symbolic
and Distributional Models of Meaning. Proceedings
of AAAI Spring Symposium on Quantum Interaction.
AAAI Press.
B. Coecke, and E. Paquette. 2011. Categories for the
Practicing Physicist. New Structures for Physics, 167-
271. B. Coecke (ed.). Lecture Notes in Physics 813.
Springer.
B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-
ical Foundations for Distributed Compositional Model
of Meaning. Lambek Festschrift. Linguistic Analysis
36, 345?384. J. van Benthem, M. Moortgat and W.
Buszkowski (eds.).
J. Curran. 2004. From Distributional to Semantic Simi-
larity. PhD Thesis, University of Edinburgh.
K. Erk and S. Pado?. 2004. A Structured Vector Space
Model for Word Meaning in Context. Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 897?906.
G. Frege 1892. U?ber Sinn und Bedeutung. Zeitschrift
fu?r Philosophie und philosophische Kritik 100.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis.
E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,
S. Pulman. 2011. Concrete Compositional Sentence
Spaces for a Compositional Distributional Model of
Meaning. International Conference on Computational
Semantics (IWCS?11). Oxford.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer.
E. Guevara. 2010. A Regression Model of Adjective-
Noun Compositionality in Distributional Semantics.
Proceedings of the ACL GEMS Workshop.
Z. S. Harris. 1966. A Cycling Cancellation-Automaton
for Sentence Well-Formedness. International Compu-
tation Centre Bulletin 5, 69?94.
R. Hudson. 1984. Word Grammar. Blackwell.
J. Lambek. 2008. From Word to Sentence. Polimetrica,
Milan.
T. Landauer, and S. Dumais. 2008. A solution to Platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to information retrieval. Cambridge Uni-
versity Press.
J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, 236?244.
R. Montague. 1974. English as a formal language. For-
mal Philosophy, 189?223.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
A. Preller. Towards Discourse Representation via Pre-
group Grammars. Journal of Logic Language Infor-
mation 16 173?194.
A. Preller and M. Sadrzadeh. Semantic Vector Mod-
els and Functional Models for Pregroup Grammars.
Journal of Logic Language Information. DOI:
10.1007/s10849-011-9132-2. to appear.
J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-
tion: The role of distributional cues. Journal of Mem-
ory and Language 35, 606?621.
H. Schuetze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics 24, 97?123.
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Computational Linguistics 46, 1?
2, 159?216.
M. Steedman. 2000. The Syntactic Process. MIT Press.
D. Widdows. 2005. Geometry and Meaning. University
of Chicago Press.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
1404
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 655?665,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Convolutional Neural Network for Modelling Sentences
Nal Kalchbrenner Edward Grefenstette
{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk
Department of Computer Science
University of Oxford
Phil Blunsom
Abstract
The ability to accurately represent sen-
tences is central to language understand-
ing. We describe a convolutional architec-
ture dubbed the Dynamic Convolutional
Neural Network (DCNN) that we adopt
for the semantic modelling of sentences.
The network uses Dynamic k-Max Pool-
ing, a global pooling operation over lin-
ear sequences. The network handles input
sentences of varying length and induces
a feature graph over the sentence that is
capable of explicitly capturing short and
long-range relations. The network does
not rely on a parse tree and is easily ap-
plicable to any language. We test the
DCNN in four experiments: small scale
binary and multi-class sentiment predic-
tion, six-way question classification and
Twitter sentiment prediction by distant su-
pervision. The network achieves excellent
performance in the first three tasks and a
greater than 25% error reduction in the last
task with respect to the strongest baseline.
1 Introduction
The aim of a sentence model is to analyse and
represent the semantic content of a sentence for
purposes of classification or generation. The sen-
tence modelling problem is at the core of many
tasks involving a degree of natural language com-
prehension. These tasks include sentiment analy-
sis, paraphrase detection, entailment recognition,
summarisation, discourse analysis, machine trans-
lation, grounded language learning and image re-
trieval. Since individual sentences are rarely ob-
served or not observed at all, one must represent
a sentence in terms of features that depend on the
words and short n-grams in the sentence that are
frequently observed. The core of a sentence model
involves a feature function that defines the process
 The  cat  sat  on  the  red  mat  The  cat  sat  on  the  red  mat
Figure 1: Subgraph of a feature graph induced
over an input sentence in a Dynamic Convolu-
tional Neural Network. The full induced graph
has multiple subgraphs of this kind with a distinct
set of edges; subgraphs may merge at different
layers. The left diagram emphasises the pooled
nodes. The width of the convolutional filters is 3
and 2 respectively. With dynamic pooling, a fil-
ter with small width at the higher layers can relate
phrases far apart in the input sentence.
by which the features of the sentence are extracted
from the features of the words or n-grams.
Various types of models of meaning have been
proposed. Composition based methods have been
applied to vector representations of word meaning
obtained from co-occurrence statistics to obtain
vectors for longer phrases. In some cases, com-
position is defined by algebraic operations over
word meaning vectors to produce sentence mean-
ing vectors (Erk and Pad?o, 2008; Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Tur-
ney, 2012; Erk, 2012; Clarke, 2012). In other
cases, a composition function is learned and ei-
ther tied to particular syntactic relations (Guevara,
2010; Zanzotto et al, 2010) or to particular word
types (Baroni and Zamparelli, 2010; Coecke et
al., 2010; Grefenstette and Sadrzadeh, 2011; Kart-
saklis and Sadrzadeh, 2013; Grefenstette, 2013).
Another approach represents the meaning of sen-
tences by way of automatically extracted logical
forms (Zettlemoyer and Collins, 2005).
655
A central class of models are those based on
neural networks. These range from basic neu-
ral bag-of-words or bag-of-n-grams models to the
more structured recursive neural networks and
to time-delay neural networks based on convo-
lutional operations (Collobert and Weston, 2008;
Socher et al, 2011; Kalchbrenner and Blunsom,
2013b). Neural sentence models have a num-
ber of advantages. They can be trained to obtain
generic vectors for words and phrases by predict-
ing, for instance, the contexts in which the words
and phrases occur. Through supervised training,
neural sentence models can fine-tune these vec-
tors to information that is specific to a certain
task. Besides comprising powerful classifiers as
part of their architecture, neural sentence models
can be used to condition a neural language model
to generate sentences word by word (Schwenk,
2012; Mikolov and Zweig, 2012; Kalchbrenner
and Blunsom, 2013a).
We define a convolutional neural network archi-
tecture and apply it to the semantic modelling of
sentences. The network handles input sequences
of varying length. The layers in the network in-
terleave one-dimensional convolutional layers and
dynamic k-max pooling layers. Dynamic k-max
pooling is a generalisation of the max pooling op-
erator. The max pooling operator is a non-linear
subsampling function that returns the maximum
of a set of values (LeCun et al, 1998). The op-
erator is generalised in two respects. First, k-
max pooling over a linear sequence of values re-
turns the subsequence of k maximum values in the
sequence, instead of the single maximum value.
Secondly, the pooling parameter k can be dynam-
ically chosen by making k a function of other as-
pects of the network or the input.
The convolutional layers apply one-
dimensional filters across each row of features in
the sentence matrix. Convolving the same filter
with the n-gram at every position in the sentence
allows the features to be extracted independently
of their position in the sentence. A convolutional
layer followed by a dynamic pooling layer and
a non-linearity form a feature map. Like in the
convolutional networks for object recognition
(LeCun et al, 1998), we enrich the representation
in the first layer by computing multiple feature
maps with different filters applied to the input
sentence. Subsequent layers also have multiple
feature maps computed by convolving filters with
all the maps from the layer below. The weights at
these layers form an order-4 tensor. The resulting
architecture is dubbed a Dynamic Convolutional
Neural Network.
Multiple layers of convolutional and dynamic
pooling operations induce a structured feature
graph over the input sentence. Figure 1 illustrates
such a graph. Small filters at higher layers can cap-
ture syntactic or semantic relations between non-
continuous phrases that are far apart in the input
sentence. The feature graph induces a hierarchical
structure somewhat akin to that in a syntactic parse
tree. The structure is not tied to purely syntactic
relations and is internal to the neural network.
We experiment with the network in four set-
tings. The first two experiments involve predict-
ing the sentiment of movie reviews (Socher et
al., 2013b). The network outperforms other ap-
proaches in both the binary and the multi-class ex-
periments. The third experiment involves the cat-
egorisation of questions in six question types in
the TREC dataset (Li and Roth, 2002). The net-
work matches the accuracy of other state-of-the-
art methods that are based on large sets of en-
gineered features and hand-coded knowledge re-
sources. The fourth experiment involves predict-
ing the sentiment of Twitter posts using distant su-
pervision (Go et al, 2009). The network is trained
on 1.6 million tweets labelled automatically ac-
cording to the emoticon that occurs in them. On
the hand-labelled test set, the network achieves a
greater than 25% reduction in the prediction error
with respect to the strongest unigram and bigram
baseline reported in Go et al (2009).
The outline of the paper is as follows. Section 2
describes the background to the DCNN including
central concepts and related neural sentence mod-
els. Section 3 defines the relevant operators and
the layers of the network. Section 4 treats of the
induced feature graph and other properties of the
network. Section 5 discusses the experiments and
inspects the learnt feature detectors.
1
2 Background
The layers of the DCNN are formed by a convo-
lution operation followed by a pooling operation.
We begin with a review of related neural sentence
models. Then we describe the operation of one-
dimensional convolution and the classical Time-
Delay Neural Network (TDNN) (Hinton, 1989;
Waibel et al, 1990). By adding a max pooling
1
Code available at www.nal.co
656
layer to the network, the TDNN can be adopted as
a sentence model (Collobert and Weston, 2008).
2.1 Related Neural Sentence Models
Various neural sentence models have been de-
scribed. A general class of basic sentence models
is that of Neural Bag-of-Words (NBoW) models.
These generally consist of a projection layer that
maps words, sub-word units or n-grams to high
dimensional embeddings; the latter are then com-
bined component-wise with an operation such as
summation. The resulting combined vector is clas-
sified through one or more fully connected layers.
A model that adopts a more general structure
provided by an external parse tree is the Recursive
Neural Network (RecNN) (Pollack, 1990; K?uchler
and Goller, 1996; Socher et al, 2011; Hermann
and Blunsom, 2013). At every node in the tree the
contexts at the left and right children of the node
are combined by a classical layer. The weights of
the layer are shared across all nodes in the tree.
The layer computed at the top node gives a repre-
sentation for the sentence. The Recurrent Neural
Network (RNN) is a special case of the recursive
network where the structure that is followed is a
simple linear chain (Gers and Schmidhuber, 2001;
Mikolov et al, 2011). The RNN is primarily used
as a language model, but may also be viewed as a
sentence model with a linear structure. The layer
computed at the last word represents the sentence.
Finally, a further class of neural sentence mod-
els is based on the convolution operation and the
TDNN architecture (Collobert and Weston, 2008;
Kalchbrenner and Blunsom, 2013b). Certain con-
cepts used in these models are central to the
DCNN and we describe them next.
2.2 Convolution
The one-dimensional convolution is an operation
between a vector of weights m ? R
m
and a vector
of inputs viewed as a sequence s ? R
s
. The vector
m is the filter of the convolution. Concretely, we
think of s as the input sentence and s
i
? R is a sin-
gle feature value associated with the i-th word in
the sentence. The idea behind the one-dimensional
convolution is to take the dot product of the vector
m with each m-gram in the sentence s to obtain
another sequence c:
c
j
= m
?
s
j?m+1:j
(1)
Equation 1 gives rise to two types of convolution
depending on the range of the index j. The narrow
type of convolution requires that s ? m and yields
s1 s1ss ss
c1 c5c5
Figure 2: Narrow and wide types of convolution.
The filter m has size m = 5.
a sequence c ? R
s?m+1
with j ranging from m
to s. The wide type of convolution does not have
requirements on s or m and yields a sequence c ?
R
s+m?1
where the index j ranges from 1 to s +
m ? 1. Out-of-range input values s
i
where i < 1
or i > s are taken to be zero. The result of the
narrow convolution is a subsequence of the result
of the wide convolution. The two types of one-
dimensional convolution are illustrated in Fig. 2.
The trained weights in the filter m correspond
to a linguistic feature detector that learns to recog-
nise a specific class of n-grams. These n-grams
have size n ? m, where m is the width of the
filter. Applying the weights m in a wide convo-
lution has some advantages over applying them in
a narrow one. A wide convolution ensures that all
weights in the filter reach the entire sentence, in-
cluding the words at the margins. This is particu-
larly significant when m is set to a relatively large
value such as 8 or 10. In addition, a wide convo-
lution guarantees that the application of the filter
m to the input sentence s always produces a valid
non-empty result c, independently of the width m
and the sentence length s. We next describe the
classical convolutional layer of a TDNN.
2.3 Time-Delay Neural Networks
A TDNN convolves a sequence of inputs s with a
set of weights m. As in the TDNN for phoneme
recognition (Waibel et al, 1990), the sequence s
is viewed as having a time dimension and the con-
volution is applied over the time dimension. Each
s
j
is often not just a single value, but a vector of
d values so that s ? R
d?s
. Likewise, m is a ma-
trix of weights of size d?m. Each row of m is
convolved with the corresponding row of s and the
convolution is usually of the narrow type. Multi-
ple convolutional layers may be stacked by taking
the resulting sequence c as input to the next layer.
The Max-TDNN sentence model is based on the
architecture of a TDNN (Collobert and Weston,
2008). In the model, a convolutional layer of the
narrow type is applied to the sentence matrix s,
where each column corresponds to the feature vec-
657
tor w
i
? R
d
of a word in the sentence:
s =
?
?
w
1
. . . w
s
?
?
(2)
To address the problem of varying sentence
lengths, the Max-TDNN takes the maximum of
each row in the resulting matrix c yielding a vector
of d values:
c
max
=
?
?
?
max(c
1,:
)
.
.
.
max(c
d,:
)
?
?
?
(3)
The aim is to capture the most relevant feature, i.e.
the one with the highest value, for each of the d
rows of the resulting matrix c. The fixed-sized
vector c
max
is then used as input to a fully con-
nected layer for classification.
The Max-TDNN model has many desirable
properties. It is sensitive to the order of the words
in the sentence and it does not depend on external
language-specific features such as dependency or
constituency parse trees. It also gives largely uni-
form importance to the signal coming from each
of the words in the sentence, with the exception
of words at the margins that are considered fewer
times in the computation of the narrow convolu-
tion. But the model also has some limiting as-
pects. The range of the feature detectors is lim-
ited to the span m of the weights. Increasing m or
stacking multiple convolutional layers of the nar-
row type makes the range of the feature detectors
larger; at the same time it also exacerbates the ne-
glect of the margins of the sentence and increases
the minimum size s of the input sentence required
by the convolution. For this reason higher-order
and long-range feature detectors cannot be easily
incorporated into the model. The max pooling op-
eration has some disadvantages too. It cannot dis-
tinguish whether a relevant feature in one of the
rows occurs just one or multiple times and it for-
gets the order in which the features occur. More
generally, the pooling factor by which the signal
of the matrix is reduced at once corresponds to
s?m+1; even for moderate values of s the pool-
ing factor can be excessive. The aim of the next
section is to address these limitations while pre-
serving the advantages.
3 Convolutional Neural Networks with
Dynamic k-Max Pooling
We model sentences using a convolutional archi-
tecture that alternates wide convolutional layers
K-Max pooling
(k=3)
Fully connected 
layer
Folding
Wide
convolution
(m=2)
Dynamic
k-max pooling
 (k= f(s) =5)
 Projected
sentence 
matrix
(s=7)
Wide
convolution
(m=3)
 The cat sat on the red mat
Figure 3: A DCNN for the seven word input sen-
tence. Word embeddings have size d = 4. The
network has two convolutional layers with two
feature maps each. The widths of the filters at the
two layers are respectively 3 and 2. The (dynamic)
k-max pooling layers have values k of 5 and 3.
with dynamic pooling layers given by dynamic k-
max pooling. In the network the width of a feature
map at an intermediate layer varies depending on
the length of the input sentence; the resulting ar-
chitecture is the Dynamic Convolutional Neural
Network. Figure 3 represents a DCNN. We pro-
ceed to describe the network in detail.
3.1 Wide Convolution
Given an input sentence, to obtain the first layer of
the DCNN we take the embedding w
i
? R
d
for
each word in the sentence and construct the sen-
tence matrix s ? R
d?s
as in Eq. 2. The values
in the embeddings w
i
are parameters that are op-
timised during training. A convolutional layer in
the network is obtained by convolving a matrix of
weights m ? R
d?m
with the matrix of activations
at the layer below. For example, the second layer
is obtained by applying a convolution to the sen-
tence matrix s itself. Dimension d and filter width
m are hyper-parameters of the network. We let the
operations be wide one-dimensional convolutions
as described in Sect. 2.2. The resulting matrix c
has dimensions d? (s+m? 1).
658
3.2 k-Max Pooling
We next describe a pooling operation that is a gen-
eralisation of the max pooling over the time di-
mension used in the Max-TDNN sentence model
and different from the local max pooling opera-
tions applied in a convolutional network for object
recognition (LeCun et al, 1998). Given a value
k and a sequence p ? R
p
of length p ? k, k-
max pooling selects the subsequence p
k
max
of the
k highest values of p. The order of the values in
p
k
max
corresponds to their original order in p.
The k-max pooling operation makes it possible
to pool the k most active features in p that may be
a number of positions apart; it preserves the order
of the features, but is insensitive to their specific
positions. It can also discern more finely the num-
ber of times the feature is highly activated in p
and the progression by which the high activations
of the feature change across p. The k-max pooling
operator is applied in the network after the topmost
convolutional layer. This guarantees that the input
to the fully connected layers is independent of the
length of the input sentence. But, as we see next, at
intermediate convolutional layers the pooling pa-
rameter k is not fixed, but is dynamically selected
in order to allow for a smooth extraction of higher-
order and longer-range features.
3.3 Dynamic k-Max Pooling
A dynamic k-max pooling operation is a k-max
pooling operation where we let k be a function of
the length of the sentence and the depth of the net-
work. Although many functions are possible, we
simply model the pooling parameter as follows:
k
l
= max( k
top
, d
L? l
L
se ) (4)
where l is the number of the current convolutional
layer to which the pooling is applied and L is the
total number of convolutional layers in the net-
work; k
top
is the fixed pooling parameter for the
topmost convolutional layer (Sect. 3.2). For in-
stance, in a network with three convolutional lay-
ers and k
top
= 3, for an input sentence of length
s = 18, the pooling parameter at the first layer
is k
1
= 12 and the pooling parameter at the sec-
ond layer is k
2
= 6; the third layer has the fixed
pooling parameter k
3
= k
top
= 3. Equation 4
is a model of the number of values needed to de-
scribe the relevant parts of the progression of an
l-th order feature over a sentence of length s. For
an example in sentiment prediction, according to
the equation a first order feature such as a posi-
tive word occurs at most k
1
times in a sentence of
length s, whereas a second order feature such as a
negated phrase or clause occurs at most k
2
times.
3.4 Non-linear Feature Function
After (dynamic) k-max pooling is applied to the
result of a convolution, a bias b ? R
d
and a non-
linear function g are applied component-wise to
the pooled matrix. There is a single bias value for
each row of the pooled matrix.
If we temporarily ignore the pooling layer, we
may state how one computes each d-dimensional
column a in the matrix a resulting after the convo-
lutional and non-linear layers. Define M to be the
matrix of diagonals:
M = [diag(m
:,1
), . . . , diag(m
:,m
)] (5)
where m are the weights of the d filters of the wide
convolution. Then after the first pair of a convolu-
tional and a non-linear layer, each column a in the
matrix a is obtained as follows, for some index j:
a = g
?
?
?
M
?
?
?
w
j
.
.
.
w
j+m?1
?
?
?
+ b
?
?
?
(6)
Here a is a column of first order features. Sec-
ond order features are similarly obtained by ap-
plying Eq. 6 to a sequence of first order features
a
j
, ..., a
j+m
?
?1
with another weight matrix M
?
.
Barring pooling, Eq. 6 represents a core aspect
of the feature extraction function and has a rather
general form that we return to below. Together
with pooling, the feature function induces position
invariance and makes the range of higher-order
features variable.
3.5 Multiple Feature Maps
So far we have described how one applies a wide
convolution, a (dynamic) k-max pooling layer and
a non-linear function to the input sentence ma-
trix to obtain a first order feature map. The three
operations can be repeated to yield feature maps
of increasing order and a network of increasing
depth. We denote a feature map of the i-th order
by F
i
. As in convolutional networks for object
recognition, to increase the number of learnt fea-
ture detectors of a certain order, multiple feature
maps F
i
1
, . . . ,F
i
n
may be computed in parallel at
the same layer. Each feature map F
i
j
is computed
by convolving a distinct set of filters arranged in
a matrix m
i
j,k
with each feature map F
i?1
k
of the
lower order i? 1 and summing the results:
659
Fi
j
=
n
?
k=1
m
i
j,k
? F
i?1
k
(7)
where ? indicates the wide convolution. The
weights m
i
j,k
form an order-4 tensor. After the
wide convolution, first dynamic k-max pooling
and then the non-linear function are applied indi-
vidually to each map.
3.6 Folding
In the formulation of the network so far, feature
detectors applied to an individual row of the sen-
tence matrix s can have many orders and create
complex dependencies across the same rows in
multiple feature maps. Feature detectors in differ-
ent rows, however, are independent of each other
until the top fully connected layer. Full depen-
dence between different rows could be achieved
by making M in Eq. 5 a full matrix instead of
a sparse matrix of diagonals. Here we explore a
simpler method called folding that does not intro-
duce any additional parameters. After a convo-
lutional layer and before (dynamic) k-max pool-
ing, one just sums every two rows in a feature map
component-wise. For a map of d rows, folding re-
turns a map of d/2 rows, thus halving the size of
the representation. With a folding layer, a feature
detector of the i-th order depends now on two rows
of feature values in the lower maps of order i? 1.
This ends the description of the DCNN.
4 Properties of the Sentence Model
We describe some of the properties of the sentence
model based on the DCNN. We describe the no-
tion of the feature graph induced over a sentence
by the succession of convolutional and pooling
layers. We briefly relate the properties to those of
other neural sentence models.
4.1 Word and n-Gram Order
One of the basic properties is sensitivity to the or-
der of the words in the input sentence. For most
applications and in order to learn fine-grained fea-
ture detectors, it is beneficial for a model to be able
to discriminate whether a specific n-gram occurs
in the input. Likewise, it is beneficial for a model
to be able to tell the relative position of the most
relevant n-grams. The network is designed to cap-
ture these two aspects. The filters m of the wide
convolution in the first layer can learn to recognise
specific n-grams that have size less or equal to the
filter width m; as we see in the experiments, m in
the first layer is often set to a relatively large value
such as 10. The subsequence of n-grams extracted
by the generalised pooling operation induces in-
variance to absolute positions, but maintains their
order and relative positions.
As regards the other neural sentence models, the
class of NBoW models is by definition insensitive
to word order. A sentence model based on a recur-
rent neural network is sensitive to word order, but
it has a bias towards the latest words that it takes as
input (Mikolov et al, 2011). This gives the RNN
excellent performance at language modelling, but
it is suboptimal for remembering at once the n-
grams further back in the input sentence. Sim-
ilarly, a recursive neural network is sensitive to
word order but has a bias towards the topmost
nodes in the tree; shallower trees mitigate this ef-
fect to some extent (Socher et al, 2013a). As seen
in Sect. 2.3, the Max-TDNN is sensitive to word
order, but max pooling only picks out a single n-
gram feature in each row of the sentence matrix.
4.2 Induced Feature Graph
Some sentence models use internal or external
structure to compute the representation for the in-
put sentence. In a DCNN, the convolution and
pooling layers induce an internal feature graph
over the input. A node from a layer is connected
to a node from the next higher layer if the lower
node is involved in the convolution that computes
the value of the higher node. Nodes that are not
selected by the pooling operation at a layer are
dropped from the graph. After the last pooling
layer, the remaining nodes connect to a single top-
most root. The induced graph is a connected, di-
rected acyclic graph with weighted edges and a
root node; two equivalent representations of an
induced graph are given in Fig. 1. In a DCNN
without folding layers, each of the d rows of the
sentence matrix induces a subgraph that joins the
other subgraphs only at the root node. Each sub-
graph may have a different shape that reflects the
kind of relations that are detected in that subgraph.
The effect of folding layers is to join pairs of sub-
graphs at lower layers before the top root node.
Convolutional networks for object recognition
also induce a feature graph over the input image.
What makes the feature graph of a DCNN pecu-
liar is the global range of the pooling operations.
The (dynamic) k-max pooling operator can draw
together features that correspond to words that are
many positions apart in the sentence. Higher-order
features have highly variable ranges that can be ei-
660
ther short and focused or global and long as the
input sentence. Likewise, the edges of a subgraph
in the induced graph reflect these varying ranges.
The subgraphs can either be localised to one or
more parts of the sentence or spread more widely
across the sentence. This structure is internal to
the network and is defined by the forward propa-
gation of the input through the network.
Of the other sentence models, the NBoW is a
shallow model and the RNN has a linear chain
structure. The subgraphs induced in the Max-
TDNN model have a single fixed-range feature ob-
tained through max pooling. The recursive neural
network follows the structure of an external parse
tree. Features of variable range are computed at
each node of the tree combining one or more of
the children of the tree. Unlike in a DCNN, where
one learns a clear hierarchy of feature orders, in
a RecNN low order features like those of sin-
gle words can be directly combined with higher
order features computed from entire clauses. A
DCNN generalises many of the structural aspects
of a RecNN. The feature extraction function as
stated in Eq. 6 has a more general form than that
in a RecNN, where the value of m is generally 2.
Likewise, the induced graph structure in a DCNN
is more general than a parse tree in that it is not
limited to syntactically dictated phrases; the graph
structure can capture short or long-range seman-
tic relations between words that do not necessar-
ily correspond to the syntactic relations in a parse
tree. The DCNN has internal input-dependent
structure and does not rely on externally provided
parse trees, which makes the DCNN directly ap-
plicable to hard-to-parse sentences such as tweets
and to sentences from any language.
5 Experiments
We test the network on four different experiments.
We begin by specifying aspects of the implemen-
tation and the training of the network. We then re-
late the results of the experiments and we inspect
the learnt feature detectors.
5.1 Training
In each of the experiments, the top layer of the
network has a fully connected layer followed by
a softmax non-linearity that predicts the probabil-
ity distribution over classes given the input sen-
tence. The network is trained to minimise the
cross-entropy of the predicted and true distribu-
tions; the objective includes an L
2
regularisation
Classifier Fine-grained (%) Binary (%)
NB 41.0 81.8
BINB 41.9 83.1
SVM 40.7 79.4
RECNTN 45.7 85.4
MAX-TDNN 37.4 77.1
NBOW 42.4 80.5
DCNN 48.5 86.8
Table 1: Accuracy of sentiment prediction in the
movie reviews dataset. The first four results are
reported from Socher et al (2013b). The baselines
NB and BINB are Naive Bayes classifiers with,
respectively, unigram features and unigram and bi-
gram features. SVM is a support vector machine
with unigram and bigram features. RECNTN is a
recursive neural network with a tensor-based fea-
ture function, which relies on external structural
features given by a parse tree and performs best
among the RecNNs.
term over the parameters. The set of parameters
comprises the word embeddings, the filter weights
and the weights from the fully connected layers.
The network is trained with mini-batches by back-
propagation and the gradient-based optimisation is
performed using the Adagrad update rule (Duchi
et al, 2011). Using the well-known convolution
theorem, we can compute fast one-dimensional
linear convolutions at all rows of an input matrix
by using Fast Fourier Transforms. To exploit the
parallelism of the operations, we train the network
on a GPU. A Matlab implementation processes
multiple millions of input sentences per hour on
one GPU, depending primarily on the number of
layers used in the network.
5.2 Sentiment Prediction in Movie Reviews
The first two experiments concern the prediction
of the sentiment of movie reviews in the Stanford
Sentiment Treebank (Socher et al, 2013b). The
output variable is binary in one experiment and
can have five possible outcomes in the other: neg-
ative, somewhat negative, neutral, somewhat posi-
tive, positive. In the binary case, we use the given
splits of 6920 training, 872 development and 1821
test sentences. Likewise, in the fine-grained case,
we use the standard 8544/1101/2210 splits. La-
belled phrases that occur as subparts of the train-
ing sentences are treated as independent training
instances. The size of the vocabulary is 15448.
Table 1 details the results of the experiments.
661
Classifier Features Acc. (%)
HIER
unigram, POS, head chunks 91.0
NE, semantic relations
MAXENT
unigram, bigram, trigram 92.6
POS, chunks, NE, supertags
CCG parser, WordNet
MAXENT
unigram, bigram, trigram 93.6
POS, wh-word, head word
word shape, parser
hypernyms, WordNet
SVM
unigram, POS, wh-word 95.0
head word, parser
hypernyms, WordNet
60 hand-coded rules
MAX-TDNN unsupervised vectors 84.4
NBOW unsupervised vectors 88.2
DCNN unsupervised vectors 93.0
Table 2: Accuracy of six-way question classifica-
tion on the TREC questions dataset. The second
column details the external features used in the
various approaches. The first four results are re-
spectively from Li and Roth (2002), Blunsom et al
(2006), Huang et al (2008) and Silva et al (2011).
In the three neural sentence models?the Max-
TDNN, the NBoW and the DCNN?the word vec-
tors are parameters of the models that are ran-
domly initialised; their dimension d is set to 48.
The Max-TDNN has a filter of width 6 in its nar-
row convolution at the first layer; shorter phrases
are padded with zero vectors. The convolu-
tional layer is followed by a non-linearity, a max-
pooling layer and a softmax classification layer.
The NBoW sums the word vectors and applies a
non-linearity followed by a softmax classification
layer. The adopted non-linearity is the tanh func-
tion. The hyper parameters of the DCNN are as
follows. The binary result is based on a DCNN
that has a wide convolutional layer followed by a
folding layer, a dynamic k-max pooling layer and
a non-linearity; it has a second wide convolutional
layer followed by a folding layer, a k-max pooling
layer and a non-linearity. The width of the convo-
lutional filters is 7 and 5, respectively. The value
of k for the top k-max pooling is 4. The num-
ber of feature maps at the first convolutional layer
is 6; the number of maps at the second convolu-
tional layer is 14. The network is topped by a soft-
max classification layer. The DCNN for the fine-
grained result has the same architecture, but the
filters have size 10 and 7, the top pooling parame-
ter k is 5 and the number of maps is, respectively,
6 and 12. The networks use the tanh non-linear
Classifier Accuracy (%)
SVM 81.6
BINB 82.7
MAXENT 83.0
MAX-TDNN 78.8
NBOW 80.9
DCNN 87.4
Table 3: Accuracy on the Twitter sentiment
dataset. The three non-neural classifiers are based
on unigram and bigram features; the results are re-
ported from (Go et al, 2009).
function. At training time we apply dropout to the
penultimate layer after the last tanh non-linearity
(Hinton et al, 2012).
We see that the DCNN significantly outper-
forms the other neural and non-neural models.
The NBoW performs similarly to the non-neural
n-gram based classifiers. The Max-TDNN per-
forms worse than the NBoW likely due to the ex-
cessive pooling of the max pooling operation; the
latter discards most of the sentiment features of the
words in the input sentence. Besides the RecNN
that uses an external parser to produce structural
features for the model, the other models use n-
gram based or neural features that do not require
external resources or additional annotations. In the
next experiment we compare the performance of
the DCNN with those of methods that use heavily
engineered resources.
5.3 Question Type Classification
As an aid to question answering, a question may
be classified as belonging to one of many question
types. The TREC questions dataset involves six
different question types, e.g. whether the question
is about a location, about a person or about some
numeric information (Li and Roth, 2002). The
training dataset consists of 5452 labelled questions
whereas the test dataset consists of 500 questions.
The results are reported in Tab. 2. The non-
neural approaches use a classifier over a large
number of manually engineered features and
hand-coded resources. For instance, Blunsom et
al. (2006) present a Maximum Entropy model that
relies on 26 sets of syntactic and semantic fea-
tures including unigrams, bigrams, trigrams, POS
tags, named entity tags, structural relations from
a CCG parse and WordNet synsets. We evaluate
the three neural models on this dataset with mostly
the same hyper-parameters as in the binary senti-
662
POSITIVE
lovely	 	 	 	 	 comedic	 	 	 	 	 moments	 and	 	 	 	 several	 	 	 	 	 fine	 	 	 	 	 	 performances
good	 	 	 	 	 	 	 script	 	 	 	 	 	 ,	 	 	 	 	 	 	 good	 	 	 dialogue	 	 	 	 ,	 	 	 	 	 	 	 	 	 funny	 	 	 	 	 	 	 
sustains	 	 	 throughout	 	 is	 	 	 	 	 	 daring	 ,	 	 	 	 	 	 	 	 	 	 	 inventive	 and	 	 	 	 	 	 	 	 	 
well	 	 	 	 	 	 	 written	 	 	 	 	 ,	 	 	 	 	 	 	 nicely	 acted	 	 	 	 	 	 	 and	 	 	 	 	 	 	 beautifully	 
remarkably	 solid	 	 	 	 	 	 	 and	 	 	 	 	 subtly	 satirical	 	 	 tour	 	 	 	 	 	 de	 	 	 	 	 	 	 	 	 	 
NEGATIVE
,	 	 	 	 	 	 	 	 	 	 nonexistent	 plot	 	 	 	 and	 	 	 	 pretentious	 visual	 	 	 	 style	 	 	 	 	 	 	 
it	 	 	 	 	 	 	 	 	 fails	 	 	 	 	 	 	 the	 	 	 	 	 most	 	 	 basic	 	 	 	 	 	 	 test	 	 	 	 	 	 as	 	 	 	 	 	 	 	 	 	 
so	 	 	 	 	 	 	 	 	 stupid	 	 	 	 	 	 ,	 	 	 	 	 	 	 so	 	 	 	 	 ill	 	 	 	 	 	 	 	 	 conceived	 ,	 	 	 	 	 	 	 	 	 	 	 
,	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 	 	 dull	 	 	 	 and	 	 	 	 pretentious	 to	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 	 	 
hood	 	 	 	 	 	 	 rats	 	 	 	 	 	 	 	 butt	 	 	 	 their	 	 ugly	 	 	 	 	 	 	 	 heads	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 	 
'NOT'
n't	 	 	 	 have	 	 	 	 	 any	 	 	 	 	 	 	 	 	 huge	 laughs	 	 	 	 	 	 in	 	 	 	 	 	 	 	 	 	 	 its	 	 	 
no	 	 	 	 	 movement	 ,	 	 	 	 	 	 	 	 	 	 	 no	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 much	 	 
n't	 	 	 	 stop	 	 	 	 	 me	 	 	 	 	 	 	 	 	 	 from	 enjoying	 	 	 	 much	 	 	 	 	 	 	 	 	 of	 	 	 	 
not	 	 	 	 that	 	 	 	 	 kung	 	 	 	 	 	 	 	 pow	 	 is	 	 	 	 	 	 	 	 	 	 n't	 	 	 	 	 	 	 	 	 	 funny	 
not	 	 	 	 a	 	 	 	 	 	 	 	 moment	 	 	 	 	 	 that	 is	 	 	 	 	 	 	 	 	 	 not	 	 	 	 	 	 	 	 	 	 false	 
'TOO'
,	 	 	 	 	 	 too	 	 	 	 	 	 dull	 	 	 	 	 	 	 	 and	 	 pretentious	 to	 	 	 	 	 	 	 	 	 	 	 be	 	 	 	 	 	 	 	 
either	 too	 	 	 	 	 	 serious	 	 	 	 	 or	 	 	 too	 	 	 	 	 	 	 	 	 lighthearted	 ,	 	 	 	 	 	 	 	 	 
too	 	 	 	 slow	 	 	 	 	 ,	 	 	 	 	 	 	 	 	 	 	 too	 	 long	 	 	 	 	 	 	 	 and	 	 	 	 	 	 	 	 	 	 too	 	 	 	 	 	 	 
feels	 	 too	 	 	 	 	 	 formulaic	 	 	 and	 	 too	 	 	 	 	 	 	 	 	 familiar	 	 	 	 	 to	 	 	 	 	 	 	 	 
is	 	 	 	 	 too	 	 	 	 	 	 predictable	 and	 	 too	 	 	 	 	 	 	 	 	 self	 	 	 	 	 	 	 	 	 conscious	 	 
Figure 4: Top five 7-grams at four feature detectors in the first layer of the network.
ment experiment of Sect. 5.2. As the dataset is
rather small, we use lower-dimensional word vec-
tors with d = 32 that are initialised with embed-
dings trained in an unsupervised way to predict
contexts of occurrence (Turian et al, 2010). The
DCNN uses a single convolutional layer with fil-
ters of size 8 and 5 feature maps. The difference
between the performance of the DCNN and that of
the other high-performing methods in Tab. 2 is not
significant (p < 0.09). Given that the only labelled
information used to train the network is the train-
ing set itself, it is notable that the network matches
the performance of state-of-the-art classifiers that
rely on large amounts of engineered features and
rules and hand-coded resources.
5.4 Twitter Sentiment Prediction with
Distant Supervision
In our final experiment, we train the models on a
large dataset of tweets, where a tweet is automat-
ically labelled as positive or negative depending
on the emoticon that occurs in it. The training set
consists of 1.6 million tweets with emoticon-based
labels and the test set of about 400 hand-annotated
tweets. We preprocess the tweets minimally fol-
lowing the procedure described in Go et al (2009);
in addition, we also lowercase all the tokens. This
results in a vocabulary of 76643 word types. The
architecture of the DCNN and of the other neural
models is the same as the one used in the binary
experiment of Sect. 5.2. The randomly initialised
word embeddings are increased in length to a di-
mension of d = 60. Table 3 reports the results of
the experiments. We see a significant increase in
the performance of the DCNN with respect to the
non-neural n-gram based classifiers; in the pres-
ence of large amounts of training data these clas-
sifiers constitute particularly strong baselines. We
see that the ability to train a sentiment classifier on
automatically extracted emoticon-based labels ex-
tends to the DCNN and results in highly accurate
performance. The difference in performance be-
tween the DCNN and the NBoW further suggests
that the ability of the DCNN to both capture fea-
tures based on long n-grams and to hierarchically
combine these features is highly beneficial.
5.5 Visualising Feature Detectors
A filter in the DCNN is associated with a feature
detector or neuron that learns during training to
be particularly active when presented with a spe-
cific sequence of input words. In the first layer, the
sequence is a continuous n-gram from the input
sentence; in higher layers, sequences can be made
of multiple separate n-grams. We visualise the
feature detectors in the first layer of the network
trained on the binary sentiment task (Sect. 5.2).
Since the filters have width 7, for each of the 288
feature detectors we rank all 7-grams occurring in
the validation and test sets according to their ac-
tivation of the detector. Figure 5.2 presents the
top five 7-grams for four feature detectors. Be-
sides the expected detectors for positive and nega-
tive sentiment, we find detectors for particles such
as ?not? that negate sentiment and such as ?too?
that potentiate sentiment. We find detectors for
multiple other notable constructs including ?all?,
?or?, ?with...that?, ?as...as?. The feature detectors
learn to recognise not just single n-grams, but pat-
terns within n-grams that have syntactic, semantic
or structural significance.
6 Conclusion
We have described a dynamic convolutional neural
network that uses the dynamic k-max pooling op-
erator as a non-linear subsampling function. The
feature graph induced by the network is able to
capture word relations of varying size. The net-
work achieves high performance on question and
sentiment classification without requiring external
features as provided by parsers or other resources.
Acknowledgements
We thank Nando de Freitas and Yee Whye Teh
for great discussions on the paper. This work was
supported by a Xerox Foundation Award, EPSRC
grant number EP/F042728/1, and EPSRC grant
number EP/K036580/1.
663
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183?1193. ACL.
Phil Blunsom, Krystle Kocik, and James R. Curran.
2006. Question classification with log-linear mod-
els. In SIGIR ?06: Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 615?616, New York, NY, USA. ACM.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical Foundations for a Com-
positional Distributional Model of Meaning. March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning, ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing - EMNLP ?08,
(October):897.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Felix A. Gers and Jrgen Schmidhuber. 2001. Lstm
recurrent networks learn simple context-free and
context-sensitive languages. IEEE Transactions on
Neural Networks, 12(6):1333?1340.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette. 2013. Category-theoretic
quantitative compositional distributional models
of natural language semantics. arXiv preprint
arXiv:1311.1539.
Emiliano Guevara. 2010. Modelling Adjective-Noun
Compositionality by Regression. ESSLLI?10 Work-
shop on Compositionality and Distributional Se-
mantic Models.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Forthcoming.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.
Geoffrey E. Hinton. 1989. Connectionist learning pro-
cedures. Artif. Intell., 40(1-3):185?234.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using head words and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?08, pages 927?936, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Recur-
rent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing, Seattle, October. As-
sociation for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Recur-
rent Convolutional Neural Networks for Discourse
Compositionality. In Proceedings of the Workshop
on Continuous Vector Space Models and their Com-
positionality, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior disambiguation of word tensors for construct-
ing sentence vectors. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Seattle, USA, October.
Andreas K?uchler and Christoph Goller. 1996. Induc-
tive learning in symbolic domains using structure-
driven recurrent neural networks. In G?unther G?orz
and Steffen H?olldobler, editors, KI, volume 1137 of
Lecture Notes in Computer Science, pages 183?197.
Springer.
Yann LeCun, L?eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324, November.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT, pages 234?239.
664
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP, pages 5528?5531. IEEE.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, volume 8.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46:77?105.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071?1080.
Joo Silva, Lusa Coheur, AnaCristina Mendes, and An-
dreas Wichert. 2011. From symbolic to sub-
symbolic information in question classification. Ar-
tificial Intelligence Review, 35(2):137?154.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Grounded Composi-
tional Semantics for Finding and Describing Images
with Sentences. In Transactions of the Association
for Computational Linguistics (TACL).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
J. Artif. Intell. Res.(JAIR), 44:533?585.
Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-
ton, Kiyohiro Shikano, and Kevin J. Lang. 1990.
Readings in speech recognition. chapter Phoneme
Recognition Using Time-delay Neural Networks,
pages 393?404. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distri-
butional semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computa-
tional Linguistics.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666. AUAI Press.
665
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
New Directions in Vector Space Models of Meaning
Phil Blunsom, Edward Grefenstette
and Karl Moritz Hermann
?
University of Oxford
first.last@cs.ox.ac.uk
Georgiana Dinu
Center for Mind/Brain Sciences
University of Trento
georgiana.dinu@unitn.it
1 Abstract
Symbolic approaches have dominated NLP as a
means to model syntactic and semantic aspects of
natural language. While powerful inferential tools
exist for such models, they suffer from an inabil-
ity to capture correlation between words and to
provide a continuous model for word, phrase, and
document similarity. Distributed representations
are one mechanism to overcome these constraints.
This tutorial will supply NLP researchers with
the mathematical and conceptual background to
make use of vector-based models of meaning in
their own research. We will begin by motivating
the need for a transition from symbolic represen-
tations to distributed ones. We will briefly cover
how collocational (distributional) vectors can be
used and manipulated to model word meaning. We
will discuss the progress from distributional to dis-
tributed representations, and how neural networks
allow us to learn word vectors and condition them
on metadata such as parallel texts, topic labels, or
sentiment labels. Finally, we will present various
forms of semantic vector composition, and discuss
their relative strengths and weaknesses, and their
application to problems such as language mod-
elling, paraphrasing, machine translation and doc-
ument classification.
This tutorial aims to bring researchers up to
speed with recent developments in this fast-
moving field. It aims to strike a balance be-
tween providing a general introduction to vector-
based models of meaning, an analysis of diverg-
ing strands of research in the field, and also being
a hands-on tutorial to equip NLP researchers with
the necessary tools and background knowledge to
start working on such models. Attendees should
be comfortable with basic probability, linear alge-
bra, and continuous mathematics. No substantial
knowledge of machine learning is required.
?
Instructors listed in alphabetical order.
2 Outline
1. Motivation: Meaning in space
2. Learning distributional models for words
3. Neural language modelling and distributed
representations
(a) Neural language model fundamentals
(b) Recurrent neural language models
(c) Conditional neural language models
4. Semantic composition in vector spaces
(a) Algebraic and tensor-based composition
(b) The role of non-linearities
(c) Learning recursive neural models
(d) Convolutional maps and composition
3 Instructors
Phil Blunsom is an Associate Professor at the
University of Oxford?s Department of Computer
Science. His research centres on the probabilistic
modelling of natural languages, with a particular
interest in automating the discovery of structure
and meaning in text.
Georgiana Dinu is a postdoctoral researcher
at the University of Trento. Her research re-
volves around distributional semantics with a fo-
cus on compositionality within the distributional
paradigm.
Edward Grefenstette is a postdoctoral researcher
at Oxford?s Department of Computer Science. He
works on the relation between vector represen-
tations of language meaning and structured logi-
cal reasoning. His work in this area was recently
recognised by a best paper award at *SEM 2013.
Karl Moritz Hermann is a final-year DPhil stu-
dent at the Department of Computer Science in
Oxford. His research studies distributed and com-
positional semantics, with a particular emphasis
on mechanisms to reduce task-specific and mono-
lingual syntactic bias in such representations.
8
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 1?10, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Towards a Formal Distributional Semantics:
Simulating Logical Calculi with Tensors
Edward Grefenstette
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
edward.grefenstette@cs.ox.ac.uk
Abstract
The development of compositional distribu-
tional models of semantics reconciling the em-
pirical aspects of distributional semantics with
the compositional aspects of formal seman-
tics is a popular topic in the contemporary lit-
erature. This paper seeks to bring this rec-
onciliation one step further by showing how
the mathematical constructs commonly used
in compositional distributional models, such
as tensors and matrices, can be used to sim-
ulate different aspects of predicate logic.
This paper discusses how the canonical iso-
morphism between tensors and multilinear
maps can be exploited to simulate a full-blown
quantifier-free predicate calculus using ten-
sors. It provides tensor interpretations of the
set of logical connectives required to model
propositional calculi. It suggests a variant
of these tensor calculi capable of modelling
quantifiers, using few non-linear operations.
It finally discusses the relation between these
variants, and how this relation should consti-
tute the subject of future work.
1 Introduction
The topic of compositional distributional semantics
has been growing in popularity over the past few
years. This emerging sub-field of natural language
semantic modelling seeks to combine two seemingly
orthogonal approaches to modelling the meaning of
words and sentences, namely formal semantics and
distributional semantics.
These approaches, summarised in Section 2, dif-
fer in that formal semantics, on the one hand, pro-
vides a neatly compositional picture of natural lan-
guage meaning, reducing sentences to logical rep-
resentations; one the other hand, distributional se-
mantics accounts for the ever-present ambiguity and
polysemy of words of natural language, and pro-
vides tractable ways of learning and comparing
word meanings based on corpus data.
Recent efforts, some of which are briefly re-
ported below, have been made to unify both of
these approaches to language modelling to pro-
duce compositional distributional models of seman-
tics, leveraging the learning mechanisms of distri-
butional semantics, and providing syntax-sensitive
operations for the production of representations of
sentence meaning obtained through combination of
corpus-inferred word meanings. These efforts have
been met with some success in evaluations such
as phrase similarity tasks (Mitchell and Lapata,
2008; Mitchell and Lapata, 2009; Grefenstette and
Sadrzadeh, 2011; Kartsaklis et al, 2012), sentiment
prediction (Socher et al, 2012), and paraphrase de-
tection (Blacoe and Lapata, 2012).
While these developments are promising with
regard to the goal of obtaining learnable-yet-
structured sentence-level representations of lan-
guage meaning, part of the motivation for unifying
formal and distributional models of semantics has
been lost. The compositional aspects of formal se-
mantics are combined with the corpus-based empir-
ical aspects of distributional semantics in such mod-
els, yet the logical aspects are not. But it is these
logical aspects which are so appealing in formal se-
mantic models, and therefore it would be desirable
to replicate the inferential powers of logic within
1
compositional distributional models of semantics.
In this paper, I make steps towards addressing this
lost connection with logic in compositional distri-
butional semantics. In Section 2, I provide a brief
overview of formal and distributional semantic mod-
els of meaning. In Section 3, I give mathemati-
cal foundations for the rest of the paper by intro-
ducing tensors and tensor contraction as a way of
modelling multilinear functions. In Section 4, I dis-
cuss how predicates, relations, and logical atoms
of a quantifier-free predicate calculus can be mod-
elled with tensors. In Section 5, I present tenso-
rial representations of logical operations for a com-
plete propositional calculus. In Section 6, I discuss
a variant of the predicate calculus from Section 4
aimed at modelling quantifiers within such tensor-
based logics, and the limits of compositional for-
malisms based only on multilinear maps. I con-
clude, in Section 7, by suggesting directions for fur-
ther work based on the contents of this paper.
This paper does not seek to address the question
of how to determine how words should be trans-
lated into predicates and relations in the first place,
but rather shows how such predicates and relations
can be modelled using multilinear algebra. As such,
it can be seen as a general theoretical contribution
which is independent from the approaches to com-
positional distributional semantics it can be applied
to. It is directly compatible with the efforts of Co-
ecke et al (2010) and Grefenstette et al (2013), dis-
cussed below, but is also relevant to any other ap-
proach making use of tensors or matrices to encode
semantic relations.
2 Related work
Formal semantics, from the Montagovian school of
thought (Montague, 1974; Dowty et al, 1981), treats
natural languages as programming languages which
compile down to some formal language such as a
predicate calculus. The syntax of natural languages,
in the form of a grammar, is augmented by seman-
tic interpretations, in the form of expressions from
a higher order logic such as the lambda-beta calcu-
lus. The parse of a sentence then determines the
combinations of lambda-expressions, the reduction
of which yields a well-formed formula of a predi-
cate calculus, corresponding to the semantic repre-
sentation of the sentence. A simple formal semantic
model is illustrated in Figure 1.
Syntactic Analysis Semantic Interpretation
S? NP VP [[VP]]([[NP]])
NP? cats, milk, etc. [[cats]], [[milk]], . . .
VP? Vt NP [[Vt]]([[NP]])
Vt? like, hug, etc. ?yx.[[like]](x, y), . . .
[[like]]([[cats]], [[milk]])
[[cats]] ?x.[[like]](x, [[milk]])
?yx.[[like]](x, y) [[milk]]
Figure 1: A simple formal semantic model.
Formal semantic models are incredibly powerful,
in that the resulting logical representations of sen-
tences can be fed to automated theorem provers to
perform textual inference, consistency verification,
question answering, and a host of other tasks which
are well developed in the literature (e.g. see (Love-
land, 1978) and (Fitting, 1996)). However, the so-
phistication of such formal semantic models comes
at a cost: the complex set of rules allowing for
the logical interpretation of text must either be pro-
vided a priori, or learned. Learning such represen-
tations is a complex task, the difficulty of which is
compounded by issues of ambiguity and polysemy
which are pervasive in natural languages.
In contrast, distributional semantic models, best
summarised by the dictum of Firth (1957) that ?You
shall know a word by the company it keeps,? pro-
vide an elegant and tractable way of learning se-
mantic representations of words from text. Word
meanings are modelled as high-dimensional vectors
in large semantic vector spaces, the basis elements
of which correspond to contextual features such as
other words from a lexicon. Semantic vectors for
words are built by counting how many time a target
word occurs within a context (e.g. within k words
of select words from the lexicon). These context
counts are then normalised by a term frequency-
inverse document frequency-like measure (e.g. TF-
IDF, pointwise mutual information, ratio of proba-
bilities), and are set as the basis weights of the vec-
tor representation of the word?s meaning. Word vec-
tors can then be compared using geometric distance
2
furry
stroke
pet
cat
dog
snake
Figure 2: A simple distributional semantic model.
metrics such as cosine similarity, allowing us to de-
termine the similarity of words, cluster semantically
related words, and so on. Excellent overviews of dis-
tributional semantic models are provided by Curran
(2004) and Mitchell (2011). A simple distributional
semantic model showing the spacial representation
of words ?dog?, ?cat? and ?snake? within the context
of feature words ?pet?, ?furry?, and ?stroke? is shown
in Figure 2.
Distributional semantic models have been suc-
cessfully applied to tasks such as word-sense
discrimination (Schu?tze, 1998), thesaurus extrac-
tion (Grefenstette, 1994), and automated essay
marking (Landauer and Dumais, 1997). However,
while such models provide tractable ways of learn-
ing and comparing word meanings, they do not natu-
rally scale beyond word length. As recently pointed
out by Turney (2012), treating larger segments of
texts as lexical units and learning their representa-
tions distributionally (the ?holistic approach?) vio-
lates the principle of linguistic creativity, according
to which we can formulate and understand phrases
which we?ve never observed before, provided we
know the meaning of their parts and how they are
combined. As such, distributional semantics makes
no effort to account for the compositional nature of
language like formal semantics does, and ignores is-
sues relating to syntactic and relational aspects of
language.
Several proposals have been put forth over the
last few years to provide vector composition func-
tions for distributional models in order to introduce
compositionality, thereby replicating some of the as-
pects of formal semantics while preserving learn-
ability. Simple operations such as vector addition
and multiplication, with or without scalar or matrix
weights (to take word order or basic relational as-
pects into account), have been suggested (Zanzotto
et al, 2010; Mitchell and Lapata, 2008; Mitchell and
Lapata, 2009).
Smolensky (1990) suggests using the tensor prod-
uct of word vectors to produce representations that
grow with sentence complexity. Clark and Pulman
(2006) extend this approach by including basis vec-
tors standing for dependency relations into tensor
product-based representations. Both of these ten-
sor product-based approaches run into dimensional-
ity problems as representations of sentence mean-
ing for sentences of different lengths or grammati-
cal structure do not live in the same space, and thus
cannot directly be compared. Coecke et al (2010)
develop a framework using category theory, solving
this dimensionality problem of tensor-based models
by projecting tensored vectors for sentences into a
unique vector space for sentences, using functions
dynamically generated by the syntactic structure of
the sentences. In presenting their framework, which
partly inspired this paper, they describe how a verb
can be treated as a logical relation using tensors in
order to evaluate the truth value of a simple sentence,
as well as how negation can be modelled using ma-
trices.
A related approach, by Baroni and Zamparelli
(2010), represents unary relations such as adjectives
as matrices learned by linear regression from cor-
pus data, and models adjective-noun composition
as matrix-vector multiplication. Grefenstette et al
(2013) generalise this approach to relations of any
arity and relate it to the framework of Coecke et al
(2010) using a tensor-based approach to formal se-
mantic modelling similar to that presented in this pa-
per.
Finally, Socher et al (2012) apply deep learning
techniques to model syntax-sensitive vector compo-
sition using non-linear operations, effectively turn-
ing parse trees into multi-stage neural networks.
Socher shows that the non-linear activation func-
tion used in such a neural network can be tailored to
replicate the behaviour of basic logical connectives
such as conjunction and negation.
3
3 Tensors and multilinear maps
Tensors are the mathematical objects dealt with in
multilinear algebra just as vectors and matrices are
the objects dealt with in linear algebra. In fact, ten-
sors can be seen as generalisations of vectors and
matrices by introducing the notion of tensor rank.
Let the rank of a tensor be the number of indices re-
quired to describe a vector/matrix-like object in sum
notation. A vector v in a space V with basis {bVi }i can
be written as the weighted sum of the basis vectors:
v =
?
i
cvi b
V
i
where the cvi elements are the scalar basis weights
of the vector. Being fully described with one index,
vectors are rank 1 tensors. Similarly, a matrix M is
an element of a space V ?W with basis {(bVi ,b
W
j )}i j
(such pairs of basis vectors of V and W are com-
monly written as {bVi ?b
W
j }i j in multilinear algebra).
Such matrices are rank 2 tensors, as they can be fully
described using two indices (one for rows, one for
columns):
M =
?
i j
cMi j b
V
i ? b
W
j
where the scalar weights cMi j are just the i jth ele-
ments of the matrix.
A tensor T of rank k is just a geometric object with
a higher rank. Let T be a member of V1?. . .?Vk; we
can express T as follows, using k indices ?1 . . . ?k:
T =
?
?1...?k
cT?1...?k b
V1
?1
? . . . ? bVk?k
In this paper, we will be dealing with tensors of rank
1 (vectors), rank 2 (matrices) and rank 3, which can
be pictured as cuboids (or a matrix of matrices).
Tensor contraction is an operation which allows
us to take two tensors and produce a third. It is a
generalisation of inner products and matrix multipli-
cation to tensors of higher ranks. Let T be a tensor in
V1?. . .?V j?Vk and U be a tensor in Vk?Vm?. . .?Vn.
The contraction of these tensors, written T?U, cor-
responds to the following calculation:
T ? U =
?
?1...?n
cT?1...?k c
U
?k ...?n
bV1?1 ? . . . ? b
V j
? j ? b
Vm
?m
? . . . ? bVn?n
Tensor contraction takes a tensor of rank k and a
tensor of rank n ? k + 1 and produces a tensor of
rank n ? 1, corresponding to the sum of the ranks of
the input tensors minus 2. The tensors must satisfy
the following restriction: the left tensor must have
a rightmost index spanning the same number of di-
mensions as the leftmost index of the right tensor.
This is similar to the restriction that a m by n matrix
can only be multiplied with a p by q matrix if n = p,
i.e. if the index spanning the columns of the first ma-
trix covers the same number of columns as the index
spanning the rows of the second matrix covers rows.
Similarly to how the columns of one matrix ?merge?
with the rows of another to produce a third matrix,
the part of the first tensor spanned by the index k
merges with the part of the second tensor spanned by
k by ?summing through? the shared basis elements
bVk?k of each tensor. Each tensor therefore loses a
rank while being joined, explaining how the tensor
produced by T?U is of rank k+(n?k+1)?2 = n?1.
There exists an isomorphism between tensors and
multilinear maps (Bourbaki, 1989; Lee, 1997), such
that any curried multilinear map
f : V1 ? . . .? V j ? Vk
can be represented as a tensor T f ? Vk?V j? . . .?V1
(note the reversed order of the vector spaces), with
tensor contraction acting as function application.
This isomorphism guarantees that there exists such a
tensor T f for every f , such that the following equal-
ity holds for any v1 ? V1, . . . , v j ? V j:
f v1 . . . v j = vk = T f ? v1 ? . . . ? v j
4 Tensor-based predicate calculi
In this section, I discuss how the isomorphism be-
tween multilinear maps and tensors described above
can be used to model predicates, relations, and log-
ical atoms of a predicate calculus. The four aspects
of a predicate calculus we must replicate here us-
ing tensors are as follows: truth values, the logical
domain and its elements (logical atoms), predicates,
and relations. I will discuss logical connectives in
the next section.
Both truth values and domain objects are the ba-
sic elements of a predicate calculus, and therefore
it makes sense to model them as vectors rather than
higher rank tensors, which I will reserve for rela-
tions. We first must consider the vector space used
4
to model the boolean truth values of B. Coecke et al
(2010) suggest, as boolean vector space, the space B
with the basis {>,?}, where > = [1 0]> is inter-
preted as ?true?, and ? = [0 1]> as ?false?.
I assign to the domain D, the set of objects in
our logic, a vector space D on R|D| with basis vec-
tors {di}i which are in bijective correspondence with
elements of D. An element of D is therefore rep-
resented as a one-hot vector in D, the single non-
null value of which is the weight for the basis vector
mapped to that element of D. Similarly, a subset of
D is a vector of D where those elements ofD in the
subset have 1 as their corresponding basis weights in
the vector, and those not in the subset have 0. There-
fore there is a one-to-one correspondence between
the vectors in D and the elements of the power set
P(D), provided the basis weights of the vectors are
restricted to one of 0 or 1.
Each unary predicate P in the logic is represented
in the logical model as a set MP ? D containing the
elements of the domain for which the predicate is
true. Predicates can be viewed as a unary function
fP : D ? B where
fP(x) =
{
> if x ? MP
? otherwise
These predicate functions can be modelled as rank 2
tensors in B ? D, i.e. matrices. Such a matrix MP is
expressed in sum notation as follows:
MP =
?
??????
?
i
cM
P
1i > ? di
?
?????? +
?
??????
?
i
cM
P
2i ? ? di
?
??????
The basis weights are defined in terms of the set MP
as follows: cM
P
1i = 1 if the logical atom xi associ-
ated with basis weight di is in MP, and 0 otherwise;
conversely, cM
P
2i = 1 if the logical atom xi associated
with basis weight di is not in MP, and 0 otherwise.
To give a simple example, let?s consider a do-
main with three individuals, represented as the fol-
lowing one-hot vectors in D: john = [1 0 0]>,
chris = [0 1 0]>, and tom = [0 0 1]>. Let?s
imagine that Chris and John are mathematicians, but
Tom is not. The predicate P for ?is a mathemati-
cian? therefore is represented model-theoretically as
the set MP = {chris, john}. Translating this into a
matrix gives the following tensor for P:
MP =
[
1 1 0
0 0 1
]
To compute the truth value of ?John is a mathemati-
cian?, we perform predicate-argument application as
tensor contraction (matrix-vector multiplication, in
this case):
MP ? john =
[
1 1 0
0 0 1
]
?
?????????
0
1
0
?
?????????
=
[
1
0
]
= >
Likewise for ?Tom is a mathematician?:
MP ? tom =
[
1 1 0
0 0 1
]
?
?????????
0
0
1
?
?????????
=
[
0
1
]
= ?
Model theory for predicate calculus represents
any n-ary relation R, such as a verb, as the set MR
of n-tuples of elements from D for which R holds.
Therefore such relations can be viewed as functions
fR : Dn ? B where:
fR(x1, . . . , xn) =
{
> if (x1, . . . , xn) ? MR
? otherwise
We can represent the boolean function for such a re-
lation R as a tensor TR in B ? D ? . . . ? D?        ??        ?
n
:
TR =
?
???????
?
?1...?n
cT
R
1?1...?n> ? d?1 ? . . . ? d?n
?
???????
+
?
???????
?
?1...?n
cT
R
2?1...?n? ? d?1 ? . . . ? d?n
?
???????
As was the case for predicates, the weights for re-
lational tensors are defined in terms of the set mod-
elling the relation: cT
R
1?1...?n
is 1 if the tuple (x, . . . , z)
associated with the basis vectors d?n . . . d?1 (again,
note the reverse order) is in MR and 0 otherwise; and
cT
R
2?1...?n
is 1 if the tuple (x, . . . , z) associated with
the basis vectors d?n . . . d?1 is not in MR and 0 oth-
erwise.
To give an example involving relations, let our
domain be the individuals John ( j) and Mary (m).
Mary loves John and herself, but John only loves
himself. The logical model for this scenario is as
follows:
D = { j,m} Mloves = {( j, j), (m,m), (m, j)}
Distributionally speaking, the elements of the do-
main will be mapped to the following one-hot vec-
tors in some two-dimensional space D as follows:
5
j = [1 0]> and m = [0 1]>. The tensor for ?loves?
can be written as follows, ignoring basis elements
with null-valued basis weights, and using the dis-
tributivity of the tensor product over addition:
Tloves = > ? ((d1 ? d1) + (d2 ? d2) + (d1 ? d2))
+ (? ? d2 ? d1)
Computing ?Mary loves John? would correspond to
the following calculation:
(Tloves ?m) ? j =
((> ? d2) + (> ? d1)) ? j = >
whereas ?John loves Mary? would correspond to the
following calculation:
(Tloves ? j) ?m =
((> ? d1) + (? ? d2)) ?m = ?
5 Logical connectives with tensors
In this section, I discuss how the boolean connec-
tives of a propositional calculus can be modelled us-
ing tensors. Combined with the predicate and rela-
tion representations discussed above, these form a
complete quantifier-free predicate calculus based on
tensors and tensor contraction.
Negation has already been shown to be modelled
in the boolean space described earlier by Coecke et
al. (2010) as the swap matrix:
T? =
[
0 1
1 0
]
This can easily be verified:
T? ? > =
[
0 1
1 0
] [
1
0
]
=
[
0
1
]
= ?
T? ? ? =
[
0 1
1 0
] [
0
1
]
=
[
1
0
]
= >
All other logical operators are binary, and hence
modelled as rank 3 tensors. To make talking about
rank 3 tensors used to model binary operations eas-
ier, I will use the following block matrix notation for
2 ? 2 ? 2 rank 3 tensors T:
T =
[
a1 b1 a2 b2
c1 d1 c2 d2
]
which allows us to express tensor contractions as
follows:
T ? v =
[
a1 b1 a2 b2
c1 d1 c2 d2
] [
?
?
]
=
[
? ? a1 + ? ? a2 ? ? b1 + ? ? b2
? ? c1 + ? ? c2 ? ? d1 + ? ? d2
]
or more concretely:
T ? > =
[
a1 b1 a2 b2
c1 d1 c2 d2
] [
1
0
]
=
[
a1 b1
c1 d1
]
T ? ? =
[
a1 b1 a2 b2
c1 d1 c2 d2
] [
0
1
]
=
[
a2 b2
c2 d2
]
Using this notation, we can define tensors for the
following operations:
(?) 7? T? =
[
1 1 1 0
0 0 0 1
]
(?) 7? T? =
[
1 0 0 0
0 1 1 1
]
(?) 7? T? =
[
1 0 1 1
0 1 0 0
]
I leave the trivial proof by exhaustion that these fit
the bill to the reader.
It is worth noting here that these tensors pre-
serve normalised probabilities of truth. Let us con-
sider a model such at that described in Coecke et
al. (2010) which, in lieu of boolean truth values,
represents truth value vectors of the form [? ?]>
where ? + ? = 1. Applying the above logical op-
erations to such vectors produces vectors with the
same normalisation property. This is due to the fact
that the columns of the component matrices are all
normalised (i.e. each column sums to 1). To give
an example with conjunction, let v = [?1 ?1]> and
w = [?2 ?2]> with ?1 + ?1 = ?2 + ?2 = 1. The con-
junction of these vectors is calculated as follows:
(T? ? v) ? w
=
[
1 0 0 0
0 1 1 1
] [
?1
?1
] [
?2
?2
]
=
[
?1 0
?1 ?1 + ?1
] [
?2
?2
]
=
[
?1?2
?1?2 + (?1 + ?1)?2
]
6
To check that the probabilities are normalised we
calculate:
?1?2 + ?1?2 + (?1 + ?1)?2
= (?1 + ?1)?2 + (?1 + ?1)?2
= (?1 + ?1)(?2 + ?2) = 1
We can observe that the resulting probability distri-
bution for truth is still normalised. The same prop-
erty can be verified for the other connectives, which
I leave as an exercise for the reader.
6 Quantifiers and non-linearity
The predicate calculus described up until this point
has repeatedly been qualified as ?quantifier-free?,
for the simple reason that quantification cannot be
modelled if each application of a predicate or rela-
tion immediately yields a truth value. In perform-
ing such reductions, we throw away the informa-
tion required for quantification, namely the infor-
mation which indicates which elements of a domain
the predicate holds true or false for. In this sec-
tion, I present a variant of the predicate calculus
developed earlier in this paper which allows us to
model simple quantification (i.e. excluding embed-
ded quantifiers) alongside a tensor-based approach
to predicates. However, I will prove that this ap-
proach to quantifier modelling relies on non-linear
functions, rendering them non-suitable for compo-
sitional distributional models relying solely on mul-
tilinear maps for composition (or alternatively, ren-
dering such models unsuitable for the modelling of
quantifiers by this method).
We saw, in Section 4, that vectors in the seman-
tic space D standing for the logical domain could
model logical atoms as well as sets of atoms. With
this in mind, instead of modelling a predicate P as
a truth-function, let us now view it as standing for
some function fP : P(D)? P(D), defined as:
fP(X) = X ? MP
where X is a set of domain objects, and MP is the set
modelling the predicate. The tensor form of such a
function will be some T fP in D ? D. Let this square
matrix be a diagonal matrix such that basis weights
c
T fp
ii = 1 if the atom x corresponding to di is in MP
and 0 otherwise. Through tensor contraction, this
tensor maps subsets ofD (elements of D) to subsets
of D containing only those objects of the original
subset for which P holds (i.e. yielding another vector
in D).
To give an example: let us consider a domain with
two dogs (a and b) and a cat (c). One of the dogs (b)
is brown, as is the cat. Let S be the set of dogs, and P
the predicate ?brown?. I represent these statements
in the model as follows:
D = {a, b, c} S = {a, b} MP = {b, c}
The set of dogs is represented as a vector S =
[1 1 0]> and the predicate ?brown? as a tensor in
D ? D:
TP =
?
?????????
0 0 0
0 1 0
0 0 1
?
?????????
The set of brown dogs is obtained by computing
fB(S ), which distributionally corresponds to apply-
ing the tensor TP to the vector representation of S
via tensor contraction, as follows:
TP ? S =
?
?????????
0 0 0
0 1 0
0 0 1
?
?????????
?
?????????
1
1
0
?
?????????
=
?
?????????
0
1
0
?
?????????
= b
The result of this computation shows that the set of
brown dogs is the singleton set containing the only
brown dog, b. As for how logical connectives fit
into this picture, in both approaches discussed be-
low, conjunction and disjunction are modelled using
set-theoretic intersection and union, which are sim-
ply the component-wise min and max functions over
vectors, respectively.
Using this new way of modelling predicates as
tensors, I turn to the problem of modelling quantifi-
cation. We begin by putting all predicates in vector
form by replacing each instance of the bound vari-
able with a vector 1 filled with ones, which extracts
the diagonal from the predicate matrix.
An intuitive way of modelling universal quantifi-
cation is as follows: expressions of the form ?All Xs
are Ys? are true if and only if MX = MX?MY , where
MX and MY are the set of Xs and the set of Ys, re-
spectively. Using this, we can define the map forall
for distributional universal quantification modelling
expressions of the form ?All Xs are Ys? as follows:
forall(X,Y) =
{
> if X = min(X,Y)
? otherwise
7
To give a short example, the sentence ?All Greeks are
human? is verified by computing X = (Mgreek ? 1),
Y = (Mhuman ? 1), and verifying the equality X =
min(X,Y).
Existential statements of the form ?There exists
X? can be modelled using the function exists, which
tests whether or not MX is empty, and is defined as
follows:
exists(X) =
{
> if |X| > 0
? otherwise
To give a short example, the sentence ?there exists a
brown dog? is verified by computing X = (Mbrown ?
1) ? (Mdog ? 1) and verifying whether or not X is of
strictly positive length.
An important point to note here is that neither of
these quantification functions are multi-linear maps,
since a multilinear map must be linear in all argu-
ments. A counter example for forall is to consider
the case where MX and MY are empty, and multi-
ply their vector representations by non-zero scalar
weights ? and ?.
?X = X
?Y = Y
forall(?X, ?Y) = forall(X,Y) = >
forall(?X, ?Y) , ??>
I observe that the equations above demonstrate that
forall is not a multilinear map.
The proof that exists is not a multilinear map is
equally trivial. Assume MX is an empty set and ? is
a non-zero scalar weight:
?X = X
exists(?X) = exists(X) = ?
exists(?X) , ??
It follows that exists is not a multi-linear function.
7 Conclusions and future work
In this paper, I set out to demonstrate that it was
possible to replicate most aspects of predicate logic
using tensor-based models. I showed that tensors
can be constructed from logical models to represent
predicates and relations, with vectors encoding ele-
ments or sets of elements from the logical domain.
I discussed how tensor contraction allows for evalu-
ation of logical expressions encoded as tensors, and
that logical connectives can be defined as tensors to
form a full quantifier-free predicate calculus. I ex-
posed some of the limitations of this approach when
dealing with variables under the scope of quantifiers,
and proposed a variant for the tensor representation
of predicates which allows us to deal with quantifi-
cation. Further work on tensor-based modelling of
quantifiers should ideally seek to reconcile this work
with that of Barwise and Cooper (1981). In this sec-
tion, I discuss how both of these approaches to pred-
icate modelling can be put into relation, and suggest
further work that might be done on this topic, and on
the topic of integrating this work into compositional
distributional models of semantics.
The first approach to predicate modelling treats
predicates as truth functions represented as tensors,
while the second treats them as functions from sub-
sets of the domain to subsets of the domain. Yet both
representations of predicates contain the same infor-
mation. Let MP and M?P be the tensor represen-
tations of a predicate P under the first and second
approach, respectively. The relation between these
representations lies in the equality diag(pMP) =
M?P, where p is the covector [1 0] (and hence pMP
yields the first row of MP). The second row of MP
being defined in terms of the first, one can also re-
cover MP from the diagonal of M?P.
Furthermore, both approaches deal with separate
aspects of predicate logic, namely applying predi-
cates to logical atoms, and applying them to bound
variables. With this in mind, it is possible to see how
both approaches can be used sequentially by noting
that tensor contraction allows for partial application
of relations to logical atoms. For example, apply-
ing a binary relation to its first argument under the
first tensor-based model yields a predicate. Translat-
ing this predicate into the second model?s form using
the equality defined above then permits us to use it
in quantified expressions. Using this, we can eval-
uate expressions of the form ?There exists someone
who John loves?. Future work in this area should
therefore focus on developing a version of this ten-
sor calculus which permits seamless transition be-
tween both tensor formulations of logical predicates.
Finally, this paper aims to provide a starting point
for the integration of logical aspects into composi-
8
tional distributional semantic models. The work pre-
sented here serves to illustrate how tensors can sim-
ulate logical elements and operations, but does not
address (or seek to address) the fact that the vectors
and matrices in most compositional distributional
semantic models do not cleanly represent elements
of a logical domain. However, such distributional
representations can arguably be seen as represent-
ing the properties objects of a logical domain hold
in a corpus: for example the similar distributions of
?car? and ?automobile? could serve to indicate that
these concepts are co-extensive. This suggests two
directions research based on this paper could take.
One could use the hypothesis that similar vectors in-
dicate co-extensive concepts to infer a (probabilis-
tic) logical domain and set of predicates, and use the
methods described above without modification; al-
ternatively one could use the form of the logical op-
erations and predicate tensors described in this pa-
per as a basis for a higher-dimensional predicate cal-
culus, and investigate how such higher-dimensional
?logical? operations and elements could be defined
or learned. Either way, the problem of reconciling
the fuzzy ?messiness? of distributional models with
the sharp ?cleanliness? of logic is a difficult problem,
but I hope to have demonstrated in this paper that a
small step has been made in the right direction.
Acknowledgments
Thanks to Ondr?ej Rypa?c?ek, Nal Kalchbrenner
and Karl Moritz Hermann for their helpful com-
ments during discussions surrounding this pa-
per. This work is supported by EPSRC Project
EP/I03808X/1.
References
M. Baroni and R. Zamparelli. Nouns are vectors, adjec-
tives are matrices: Representing adjective-noun con-
structions in semantic space. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1183?1193. Association
for Computational Linguistics, 2010.
J. Barwise and R. Cooper Generalized quantifiers and
natural language. Linguistics and philosophy, pages
159?219. Springer, 1981.
W. Blacoe and M. Lapata. A comparison of vector-based
representations for semantic composition. Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing, 2012.
N. Bourbaki. Commutative Algebra: Chapters 1-7.
Springer-Verlag (Berlin and New York), 1989.
S. Clark and S. Pulman. Combining symbolic and distri-
butional models of meaning. In AAAI Spring Sympo-
sium on Quantum Interaction, 2006.
B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical
Foundations for a Compositional Distributional Model
of Meaning. Linguistic Analysis, volume 36, pages
345?384. March 2010.
J. R. Curran. From distributional to semantic similarity.
PhD thesis, 2004.
D. R. Dowty, R. E. Wall, and S. Peters. Introduction to
Montague Semantics. Dordrecht, 1981.
J. R. Firth. A synopsis of linguistic theory 1930-1955.
Studies in linguistic analysis, 1957.
M. Fitting. First-order logic and automated theorem
proving. Springer Verlag, 1996.
E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and
M. Baroni. Multi-step regression learning for com-
positional distributional semantics. In Proceedings of
the Tenth International Conference on Computational
Semantics. Association for Computational Linguistics,
2013.
E. Grefenstette and M. Sadrzadeh. Experimental support
for a categorical compositional distributional model of
meaning. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
2011.
G. Grefenstette. Explorations in automatic thesaurus dis-
covery. 1994.
D. Kartsaklis, and M. Sadrzadeh and S. Pulman. A
Unified Sentence Space for Categorical Distributional-
Compositional Semantics: Theory and Experiments.
In Proceedings of 24th International Conference on
Computational Linguistics (COLING 2012): Posters,
2012.
T. K. Landauer and S. T. Dumais. A solution to Plato?s
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review, 1997.
J. Lee. Riemannian manifolds: An introduction to curva-
ture, volume 176. Springer Verlag, 1997.
D. W. Loveland. Automated theorem proving: A logical
basis. Elsevier North-Holland, 1978.
J. Mitchell and M. Lapata. Vector-based models of se-
mantic composition. In Proceedings of ACL, vol-
ume 8, 2008.
J. Mitchell and M. Lapata. Language models based on se-
mantic composition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 430?439. As-
sociation for Computational Linguistics, 2009.
9
J. J. Mitchell. Composition in distributional models of
semantics. PhD thesis, 2011.
R. Montague. English as a Formal Language. Formal
Semantics: The Essential Readings, 1974.
H. Schu?tze. Automatic word sense discrimination. Com-
putational linguistics, 24(1):97?123, 1998.
P. Smolensky. Tensor product variable binding and the
representation of symbolic structures in connection-
ist systems. Artificial intelligence, 46(1-2):159?216,
1990.
R. Socher, B. Huval, C.D. Manning, and A.Y Ng.
Semantic compositionality through recursive matrix-
vector spaces. Proceedings of the 2012 Conference on
Empirical Methods in Natural Language Processing,
pages 1201?1211, 2012.
P. D. Turney. Domain and function: A dual-space model
of semantic relations and compositions. Journal of Ar-
tificial Intelligence Research, 44:533?585, 2012.
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and S. Man-
andhar. Estimating linear models for compositional
distributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 1263?1271. Association for Computational
Linguistics, 2010.
10
Concrete Sentence Spaces for Compositional Distributional
Models of Meaning
Edward Grefenstette?, Mehrnoosh Sadrzadeh?, Stephen Clark?, Bob Coecke?, Stephen Pulman?
?Oxford University Computing Laboratory, ?University of Cambridge Computer Laboratory
firstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.uk
Abstract
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional
semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the
morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional
vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map,
by constructing a corpus-based vector space for the type of sentence. Our construction method is based
on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical
structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun
spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This
enables us to compare meanings of sentences by simply taking the inner product of their vectors.
1 Background
Coecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributional
model of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.
The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector space
semantics. Each word is assigned a grammatical type and a meaning vector in the space corresponding to
its type. The meaning of a sentence is obtained by applying the function corresponding to the grammatical
structure of the sentence to the tensor product of the meanings of the words in the sentence. Based on the
type-logic used, some words will have atomic types and some compound function types. The compound
types live in a tensor space where the vectors are weighted sums (i.e. superpositions) of the pairs of bases
from each space. Compound types are ?applied? to their arguments by taking inner products, in a similar
manner to how predicates are applied to their arguments in Montague semantics.
For the type-logic we use Lambek?s Pregroup grammars [7]. The use of pregoups is not essential, but
leads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).
A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referred
to as an adjoint. It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6]. The
operational difference between a Pregroup and Lambek?s Syntactic Calculus is that, in the latter, the monoid
multiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a left
adjoint, whereas in the pregroup it is the elements themselves which have adjoints. The adjoint types are
used to denote functions, e.g. that of a transitive verb with a subject and object as input and a sentence as
output. In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjoints
of the elements themselves.
As an example, consider the sentence ?dogs chase cats?. We assign the type n (for noun phrase) to ?dog?
and ?cat?, and nrsnl to ?chase?, where nr and nl are the right and left adjoints of n and s is the type of a
125
(declarative) sentence. The type nrsnl expresses the fact that the verb is a predicate that takes two arguments
of type n as input, on its right and left, and outputs the type s of a sentence. The parsing of the sentence is
the following reduction:
n(nrsnl)n ? 1s1 = s
This parse is based on the cancellation of n and nr, and also nl and n; i.e. nnr ? 1 and nln ? 1 for 1
the unit of juxtaposition. The reduction expresses the fact that the juxtapositions of the types of the words
reduce to the type of a sentence.
On the semantic side, we assign the vector space N to the type n, and the tensor space N ?S?N to the
type nrsnl. Very briefly, and in order to introduce some notation, recall that the tensor space A?B has as a
basis the cartesian product of a basis of A with a basis of B. Recall also that any vector can be expressed as
a weighted sum of basis vectors; e.g. if (??v1 , . . . ,??vn) is a basis of A then any vector ??a ? A can be written as
??a =?i Ci??vi where each Ci ? R is a weighting factor. Now for (??v1 , . . . ,??vn) a basis of A and (
??
v?1 , . . . ,
??
v?n)
a basis of B, a vector ??c in the tensor space A ? B can be expressed as follows:
?
ij
Cij (??vi ?
??
v?j )
where the tensor of basis vectors ??vi ?
??
v?j stands for their pair (??vi ,
??
v?j ). In general ??c is not separable into
the tensor of two vectors, except for the case when ??c is not entangled. For non-entangled vectors we can
write ??c = ??a ???b for ??a =
?
i Ci
??vi and
??b =
?
j C ?j
??
v?j ; hence the weighting factor of ??c can be obtained
by simply multiplying the weights of its tensored counterparts, i.e. Cij = Ci ? C ?j . In the entangled case
these weights cannot be determined as such and range over all the possibilities. We take advantage of this
fact to encode meanings of verbs, and in general all words that have compound types and are interpreted as
predicates, relations, or functions. For a brief discussion see the last paragraph of this section. Finally, we
use the Dirac notation to denote the dot or inner product of two vectors ???a | ??b ? ? R defined by
?
i Ci?C ?i.
Returning to our example, for the meanings of nouns we have
???
dogs,??cats ? N , and for the meanings of
verbs we have
????
chase ? N ? S ? N , i.e. the following superposition:
?
ijk
Cijk (??ni ???sj ???nk)
Here ??ni and ??nk are basis vectors of N and ??sj is a basis vector of S. From the categorical translation method
presented in [3] and the grammatical reduction n(nrsnl)n ? s, we obtain the following linear map as the
categorical morphism corresponding to the reduction:
N ? 1s ? N : N ? (N ? S ? N)? N ? S
Using this map, the meaning of the sentence is computed as follows:
???????????
dogs chase cats = (N ? 1s ? N )
(???
dogs ?????chase ???cats
)
= (N ? 1s ? N )
?
?
???
dogs ?
?
?
?
ijk
Cijk(??ni ???sj ???nk)
?
????cats
?
?
=
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats?
The key features of this operation are, first, that the inner-products reduce dimensionality by ?consuming?
tensored vectors and by virtue of the following component function:
N : N ? N ? R :: ??a ?
??b 7? ???a | ??b ?
126
Thus the tensored word vectors
???
dogs ? ????chase ? ??cats are mapped into a sentence space S which is common
to all sentences regardless of their grammatical structure or complexity. Second, note that the tensor product???
dogs?????chase???cats does not need to be calculated, since all that is required for computation of the sentence
vector are the noun vectors and the Cijk weights for the verb. Note also that the inner product operations
are simply picking out basis vectors in the noun space, an operation that can be performed in constant
time. Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which use
the tensor product as a composition operation: first, that the sentence meaning space is high dimensional
and grammatically different sentences have representations with different dimensionalities, preventing them
from being compared directly using inner products; and second, that the space complexity of the tensored
representation grows exponentially with the length and grammatical complexity of the sentence. In constrast,
the model we propose does not require the tensored vectors being combined to be represented explicitly.
Note that we have taken the vector of the transitive verb, e.g.
????
chase, to be an entangled vector in the
tensor space N ? S ?N . But why can this not be a separable vector, in which case the meaning of the verb
would be as follows:
????
chase =
?
i
Ci??ni ?
?
j
C ?j??sj ?
?
k
C ??k??nk
The meaning of the sentence would then become ?1?2
?
j C ?j
??sj for ?1 =
?
i Ci?
???
dogs | ??ni? and ?2 =
?
k C ??k ?
??
cats | ??nk?. The problem is that this meaning only depends on the meaning of the verb and is
independent of the meanings of the subject and object, whereas the meaning from the entangled case,
i.e. ?1?2
?
ijk Cijk
??sj , depends on the meanings of subject and object as well as the verb.
2 From Truth-Theoretic to Corpus-based Meaning
The model presented above is compositional and distributional, but still abstract. To make it concrete, N and
S have to be constructed by providing a method for determining the Cijk weightings. Coecke, Sadrzadeh,
and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework. For
example, assume that N is spanned by all animals and S is the two-dimensional space spanned by ??true and???
false. We use the weighting factor to define a model-theoretic meaning for the verb as follows:
Cijk??sj =
{??
true chase(??ni ,??nk) = true ,???
false o.w.
The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.
So chase(???dogs,???cats) becomes true whenever ?dogs chase cats? is true and false otherwise. This is exactly
how meaning is computed in the model-theoretic view on semantics. One way to generalise this truth-
theoretic meaning is to assume that chase(??ni ,??nk) has degrees of truth, for instance by defining chase as a
combination of run and catch, such as:
chase = 23run+
1
3catch
Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a
worked out example see [3]. But neither of these examples provide a distributional sentence meaning.
Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning
for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes
beyond just composing the meanings of words using a vector operator, such as tensor product, summation
or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as
127
function arguments, according to how the words in the sentence are typed, and uses the syntactic structure
as a guide to determine how the functions are applied to their arguments. The intuition behind this approach
is that syntactic analysis guides semantic vector composition.
The contribution of this paper is to introduce some concrete constructions for a compositional distri-
butional model of meaning. These constructions demonstrate how the mathematical model of [3] can be
implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural
language semantics which is closer to the ideas underlying standard distributional models of word meaning.
We leave full evaluation to future work, in order to determine whether the following method in conjunction
with word vectors built from large corpora leads to improved results on language processing tasks, such as
computing sentence similarity and paraphrase evaluation.
Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are
annotated by ?properties? obtained by combining dependency relations with nouns, verbs and adjectives. For
example, basis vectors might be associated with properties such as ?arg-fluffy?, denoting the argument of
the adjective fluffy, ?subj-chase? denoting the subject of the verb chase, ?obj-buy? denoting the object of the
verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word
has been the argument of ?fluffy?, the subject of ?chase?, the object of ?buy?, and so on.
The framework in [3] offers no guidance as to what the sentence space should consist of. Here we take
the sentence space S to be N ? N , so its bases are of the form ??sj = (??ni ,??nk). The intuition is that, for a
transitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subject
and object.1 The verb vectors Cijk(??ni ,??nk) are built by counting how many times a word that is ni (e.g. has
the property of being fluffy) has been subject of the verb and a word that is nk (e.g. has the property that it?s
bought) has been its object, where the counts are moderated by the extent to which the subject and object
exemplify each property (e.g. how fluffy the subject is). To give a rough paraphrase of the intuition behind
this approach, the meaning of ?dog chases cat? is given by: the extent to which a dog is fluffy and a cat is
something that is bought (for the N ? N property pair ?arg-fluffy? and ?obj-buy?), and the extent to which
fluffy things chase things that are bought (accounting for the meaning of the verb for this particular property
pair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the N ?N
pair ?subj-run? and ?arg-cute?), and the extent to which things that run chase things that are cute (accounting
for the meaning of the verb for this particular property pair); and so on for all noun property pairs.
Adjective Phrases. Adjectives are dealt with in a similar way. We give them the syntactic type nnl and
build their vectors in N ? N . The syntactic reduction nnln ? n associated with applying an adjective to a
noun gives us the map 1N ? N by which we semantically compose an adjective with a noun, as follows:
?????
red fox = (1N ? N )(
??
red ???fox) =
?
ij
Cij??ni???nj |
??
fox?
We can view the Cij counts as determining what sorts of properties the arguments of a particular adjective
typically have (e.g. arg-red, arg-colourful for the adjective ?red?).
Prepositional Phrases. We assign the type nrn to the whole prepositional phrase (when it modifies a noun),
for example to ?in the forest? in the sentence ?dogs chase cats in the forest?. The pregroup parsing is as
follows:
n(nrsnl)n(nrn) ? 1snl1n ? snln ? s1 = s
The vector space corresponding to the prepositional phrase will thus be the tensor space N ? N and the
categorification of the parse will be the composition of two morphisms: (1S?lN )?(rN?1S?1N?rN?1N ).
1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see ?4.
128
The substitution specific to the prepositional phrase happens when computing the vector for ?cats in the
forest? as follows:
?????????????
cats in the forest = (rN ? 1N )
(??
cats ??????????in the forest
)
= (rN ? 1N )
(
??
cats ?
?
lw
Clw??nl ???nk
)
=
?
lw
Clw???cats | ??nl???nw
Here we set the weights Clw in a similar manner to the cases of adjective phrases and verbs with the counts
determining what sorts of properties the noun modified by the prepositional phrase has, e.g. the number of
times something that has attribute nl has been in the forest.
Adverbs. We assign the type srs to the adverb, for example to ?quickly? in the sentence ?Dogs chase cats
quickly?. The pregroup parsing is as follows:
n(nrsnl)n(srs) ? 1s1srs = ssrs ? 1s = s
Its categorification will be a composition of two morphisms (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S). The
substitution specific to the adverb happens after computing the meaning of the sentence without it, i.e. that
of ?Dogs chase cats?, and is as follows:
??????????????????
Dogs chase cats quickly = (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S)
(???
Dogs ?????chase ???cats ??????quickly
)
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?????
quickly
?
?
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?
lw
Clw??sl ???sw
?
?
=
?
lw
Clw
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? | ??sl
?
??sk
The Clw weights are defined in a similar manner to the above cases, i.e. according to the properties the
adverb has, e.g. which verbs it has modified. Note that now the basis vectors ??sl and ??sw are themselves pairs
of basis vectors from the noun space, (??ni ,??nj). Hence, Clw(??ni ,??nj) can be set only for the case when l = i
and w = j; these counts determine what sorts of properties the verbs that happen quickly have (or more
specifically what properties the subjects and objects of such verbs have). By taking the whole sentence into
account in the interpretation of the adverb, we are in a better position to semantically distinguish between
the meaning of adverbs such as ?slowly? and ?quickly?, for instance in terms of the properties that the verb?s
subjects have. For example, it is possible that elephants are more likely to be the subject of a verb which is
happening slowly, e.g. run slowly, and cheetahs are more likely to be the subject of a verb which is happening
quickly.
3 Concrete Computations
In this section we first describe how to obtain the relevant counts from a parsed corpus, and then give some
similarity calculations for some example sentence pairs.
129
Let Cl be the set of grammatical relations (GRs) for sentence sl in the corpus. Define verbs(Cl) to be
the function which returns all instances of verbs in Cl, and subj (and similarly obj ) to be the function which
returns the subject of an instance Vinstance of a verb V , for a particular set of GRs for a sentence:
subj(Vinstance) =
{
noun if Vinstance is a verb with subject noun
?n o.w.
where ?n is the empty string. We express Cijk for a verb V as follows:
Cijk =
{
?
l
?
v?verbs(Cl) ?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? if ??sj = (??ni ,??nk)
0 o.w.
where ?(v, V ) = 1 if v = V and 0 otherwise. Thus we construct Cijk for verb V only for cases where
the subject property ni and the object property nk are paired in the basis ??sj . This is done by counting the
number of times the subject of V has property ni and the object of V has property nk, then multiplying them,
as prescribed by the inner products (which simply pick out the properties ni and nk from the noun vectors
for the subjects and objects).
The procedure for calculating the verb vectors, based on the formulation above, is as follows:
1. For each GR in a sentence, if the relation is subject and the head is a verb, then find the complementary
GR with object as a relation and the same head verb. If none, set the object to ?n.
2. Retrieve the noun vectors
?????subject,????object for the subject dependent and object dependent from previ-
ously constructed noun vectors.
3. For each (ni, nk) ? basis(N)? basis(N) compute the inner-product of ??ni with
?????subject and ??nk with????object (which involves simply picking out the relevant basis vectors from the noun vectors). Multiply
the inner-products and add this to Cijk for the verb, with j such that ??sj = (??ni ,??nk).
The procedure for other grammatical types is similar, based on the definitions of C weights for the semantics
of these types.
We now give a number of example calculations. We first manually define the distributions for nouns,
which in practice would be obtained from a corpus:
bankers cats dogs stock kittens
1. arg-fluffy 0 7 3 0 2
2. arg-ferocious 4 1 6 0 0
3. obj-buys 0 4 2 7 0
4. arg-shrewd 6 3 1 0 1
5. arg-valuable 0 1 2 8 0
We aim to make these counts match our intuitions, in that bankers are shrewd and a little ferocious but not
furry, cats are furry but not typically valuable, and so on.
We also define the distributions for the transitive verbs ?chase?, ?pursue? and ?sell?, again manually
specified according to our intuitions about how these verbs are used. Since in the formalism proposed above,
Cijk = 0 if ??sj 6= (??ni ,??nk), we can simplify the weight matrices for transitive verbs to two dimensional Cik
matrices as shown below, where Cik corresponds to the number of times the verb has a subject with attribute
ni and an object with attribute nk. For example, the matrix below encodes the fact that something ferocious
130
(i = 2) chases something fluffy (k = 1) seven times in the hypothetical corpus from which we might have
obtained these distributions.
Cchase =
?
?
?
?
?
?
1 0 0 0 0
7 1 2 3 1
0 0 0 0 0
2 0 1 0 1
1 0 0 0 0
?
?
?
?
?
?
Cpursue =
?
?
?
?
?
?
0 0 0 0 0
4 2 2 2 4
0 0 0 0 0
3 0 2 0 1
0 0 0 0 0
?
?
?
?
?
?
Csell =
?
?
?
?
?
?
0 0 0 0 0
0 0 3 0 4
0 0 0 0 0
0 0 5 0 8
0 0 1 0 1
?
?
?
?
?
?
These matrices can be used to perform sentence comparisons:
????????????dogs chase cats | ??????????????dogs pursue kittens? =
=
?
?
?
?
ijk
Cchaseijk ?
???
dogs | ??ni???sj ???nk | ??cats?
?
?
?
?
?
?
?
?
?
?
?
ijk
Cpursueijk ?
???
dogs | ??ni???sj ???nk |
?????
kittens?
?
?
?
=
?
ijk
Cchaseijk C
pursue
ijk ?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats????nk |
?????
kittens?
The raw number obtained from the above calculation is 14844. Normalising it by the product of the length
of both sentence vectors gives the cosine value of 0.979.
Consider now the sentence comparison ????????????dogs chase cats | ???????????cats chase dogs?. The sentences in this pair
contain the same words but the different word orders give the sentences very different meanings. The raw
number calculated from this inner product is 7341, and its normalised cosine measure is 0.656, which demon-
strates the sharp drop in similarity obtained from changing sentence structure. We expect some similarity
since there is some non-trivial overlap between the properties identifying cats and those identifying dogs
(namely those salient to the act of chasing).
Our final example for transitive sentences is ????????????dogs chase cats | ????????????bankers sell stock?, as two sentences that
diverge in meaning completely. The raw number for this inner product is 6024, and its cosine measure is
0.042, demonstrating the very low semantic similarity between these two sentences.
Next we consider some examples involving adjective-noun modification. The Cij counts for an adjective
A are obtained in a similar manner to transitive or intransitive verbs:
Cij =
{
?
l
?
a?adjs(Cl) ?(a,A)?
???????
arg-of(a) | ??ni? if ??ni = ??nj
0 o.w.
where adjs(Cl) returns all instances of adjectives in Cl; ?(a,A) = 1 if a = A and 0 otherwise; and
arg-of(a) = noun if a is an adjective with argument noun, and ?n otherwise.
As before, we stipulate the Cij matrices by hand (and we eliminate all cases where i 6= j since Cij = 0
by definition in such cases):
Cfluffy = [9 3 4 2 2] Cshrewd = [0 3 1 9 1] Cvaluable = [3 0 8 1 8]
We compute vectors for ?fluffy dog? and ?shrewd banker? as follows:
???????
fluffy dog = (3 ? 9)???????arg-fluffy+ (6 ? 3)?????????arg-ferocious+ (2 ? 4)??????obj-buys+ (5 ? 2)????????arg-shrewd+ (2 ? 2)?????????arg-valuable
???????????
shrewd banker = (0 ? 0)???????arg-fluffy+ (4 ? 3)?????????arg-ferocious+ (0 ? 0)??????obj-buys+ (6 ? 9)????????arg-shrewd+ (0 ? 1)?????????arg-valuable
Vectors for
???????
fluffy cat and
??????????
valuable stock are computed similarly. We obtain the following similarity mea-
sures:
cosine(???????fluffy dog,???????????shrewd banker) = 0.389 cosine(???????fluffy cat,??????????valuable stock) = 0.184
131
These calculations carry over to sentences which contain the adjective-noun pairings compositionally and
we obtain an even lower similarity measure between sentences:
cosine(????????????????????fluffy dogs chase fluffy cats,?????????????????????????shrewd bankers sell valuable stock) = 0.016
To summarise, our example vectors provide us with the following similarity measures:
Sentence 1 Sentence 2 Degree of similarity
dogs chase cats dogs pursue kittens 0.979
dogs chase cats cats chase dogs 0.656
dogs chase cats bankers sell stock 0.042
fluffy dogs chase fluffy cats shrewd bankers sell valuable stock 0.016
4 Different Grammatical Structures
So far we have only presented the treatment of sentences with transitive verbs. For sentences with intransitive
verbs, the sentence space suffices to be just N . To compare the meaning of a transitive sentence with an
intransitive one, we embed the meaning of the latter from N into the former N ? N , by taking ???n (the
?object? of an intransitive verb) to be
?
i
??ni , i.e. the superposition of all basis vectors of N .
Following the method for the transitive verb, we calculate Cijk for an instransitive verb V and basis pair??sj = (??ni ,??nk) as follows, where l ranges over the sentences in the corpus:
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? =
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni?????n | ??nk?
and ????n | ??ni? = 1 for any basis vector ni.
We can now compare the meanings of transitive and intransitive sentences by taking the inner product of
their meanings (despite the different arities of the verbs) and then normalising it by vector length to obtain
the cosine measure. For example:
????????????dogs chase cats | ????????dogs chase? =
?
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ???cats ?
?
?
?
?
?
?
?
?
?
?
?
ijk
C ?ijk?
???
dogs | ??ni???sj
?
?
?
=
?
ijk
CijkC ?ijk?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats?
The raw number for the inner product is 14092 and its normalised cosine measure is 0.961, indicating high
similarity (but some difference) between a sentence with a transitive verb and one where the subject remains
the same, but the verb is used intransitively.
Comparing sentences containing nouns modified by adjectives to sentences with unmodified nouns is straight-
forward:
?????????????????????fluffy dogs chase fluffy cats | ???????????dogs chase cats? =
?
ij
Cfluffyi C
fluffy
j Cchaseij Cchaseij ?
???dogs | ??ni?2???nj |
???cats?2 = 2437005
132
From the above we obtain the following similarity measure:
cosine(????????????????????fluffy dogs chase fluffy cats,???????????dogs chase cats) = 0.971
For sentences with ditransitive verbs, the sentence space changes to N ? N ? N , on the basis of the verb
needing two objects; hence its grammatical type changes to nrsnlnl. The transitive and intransitive verbs
are embedded in this larger space in a similar manner to that described above; hence comparison of their
meanings becomes possible.
5 Ambiguous Words
The two different meanings of a word can be distinguished by the different properties that they have. These
properties are reflected in the corpus, by the different contexts in which the words appear. Consider the
following example from [4]: the verb ?catch? has two different meanings, ?grab? and ?contract?. They are
reflected in the two sentences ?catch a ball? and ?catch a disease?. The compositional feature of our meaning
computation enables us to realise the different properties of the context words via the grammatical roles they
take in the corpus. For instance, the word ?ball? occurs as argument of ?round?, and so has a high weight
for the base ?arg-round?, whereas the word ?disease? has a high weight for the base ?arg-contagious? and as
?mod-of-heart?. We extend our example corpus from previously to reflect these differences as follows:
ball disease
1. arg-fluffy 1 0
2. arg-ferocious 0 0
3. obj-buys 5 0
4. arg-shrewd 0 0
5. arg-valuable 1 0
6. arg-round 8 0
7. arg-contagious 0 7
8. mod-of-heart 0 6
In a similar way, we build a matrix for the verb ?catch? as follows:
Ccatch =
?
?
?
?
?
?
?
?
?
?
?
?
3 2 3 3 3 8 6 2
3 2 3 0 1 4 7 4
2 4 7 1 1 6 2 2
3 1 2 0 0 3 6 2
1 1 1 0 0 2 0 1
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
?
?
?
?
?
?
?
?
?
?
?
?
The last three rows are zero because we have assumed that the words that can take these roles are mostly
objects and hence cannot catch anything. Given these values, we compute the similarity measure between
the two sentences ?dogs catch a ball? and ?dogs catch a disease? as follows:
?????????????dogs catch a ball | ??????????????dogs catch a disease? = 0
In an idealised case like this where there is very little (or no) overlap between the properties of the objects
associated with one sense of ?catch? (e.g. a disease), and those properties of the objects associated with an-
other sense (e.g. a ball), disambiguation is perfect in that there is no similarity between the resulting phrases.
133
In practice, in richer vector spaces, we would expect even diseases and balls to share some properties. How-
ever, as long as those shared properties are not those typically held by the object of catch, and as long as the
usages of catch play to distinctive properties of diseases and balls, disambiguation will occur by the same
mechanism as the idealised case above, and we can expect low similarity measures between such sentences.
6 Related Work
Mitchell and Lapata introduce and evaluate a multiplicative model for vector composition [8]. The particular
concrete construction of this paper differs from that of [8] in that our framework subsumes truth-theoretic
as well as corpus-based meaning, and our meaning construction relies on and is guided by the grammatical
structure of the sentence. The approach of [4] is more in the spirit of ours, in that extra information about
syntax is used to compose meaning. Similar to us, they use a structured vector space to integrate lexical
information with selectional preferences. Finally, Baroni and Zamparelli model adjective-noun combinations
by treating an adjective as a function from noun space to noun space, represented using a matrix, as we do
in this paper [1].
References
[1] M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun construc-
tions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),
Cambridge, MA, 2010.
[2] S. Clark and S. Pulman. Combining symbolic and distributional models of meaning. In Proceedings of AAAI
Spring Symposium on Quantum Interaction. AAAI Press, 2007.
[3] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical Foundations for a Compositional Dis-
tributional Model of Meaning, volume 36. Linguistic Analysis (Lambek Festschrift), 2010.
http://arxiv.org/abs/1003.4394.
[4] K. Erk and S. Pado?. A structured vector space model for word meaning in context. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-08), pages 897?906, Honolulu, Hawaii, 2008.
[5] G. Grefenstette. Use of syntactic context to produce term association lists for text retrieval. In Nicholas J. Belkin,
Peter Ingwersen, and Annelise Mark Pejtersen, editors, SIGIR, pages 89?97. ACM, 1992.
[6] Z. Harris. Mathematical Structures of Language. Interscience Publishers John Wiley and Sons, 1968.
[7] J. Lambek. From Word to Sentence. Polimetrica, 2008.
[8] J. Mitchell and M. Lapata. Vector-based models of semantic composition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics, pages 236?244, Columbus, OH, 2008.
[9] P. Smolensky and G. Legendre. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar
Vol. I: Cognitive Architecture Vol. II: Linguistic and Philosophical Implications. MIT Press, 2005.
134
Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 62?66,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Experimenting with Transitive Verbs in a DisCoCat
Edward Grefenstette
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
edward.grefenstette@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
University of Oxford
Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
mehrs@cs.ox.ac.uk
Abstract
Formal and distributional semantic models
offer complementary benefits in modeling
meaning. The categorical compositional dis-
tributional model of meaning of Coecke et al
(2010) (abbreviated to DisCoCat in the title)
combines aspects of both to provide a gen-
eral framework in which meanings of words,
obtained distributionally, are composed using
methods from the logical setting to form sen-
tence meaning. Concrete consequences of
this general abstract setting and applications to
empirical data are under active study (Grefen-
stette et al, 2011; Grefenstette and Sadrzadeh,
2011). In this paper, we extend this study by
examining transitive verbs, represented as ma-
trices in a DisCoCat. We discuss three ways of
constructing such matrices, and evaluate each
method in a disambiguation task developed by
Grefenstette and Sadrzadeh (2011).
1 Background
The categorical distributional compositional model
of meaning of Coecke et al (2010) combines the
modularity of formal semantic models with the em-
pirical nature of vector space models of lexical se-
mantics. The meaning of a sentence is defined to
be the application of its grammatical structure?
represented in a type-logical model?to the kro-
necker product of the meanings of its words, as
computed in a distributional model. The concrete
and experimental consequences of this setting, and
other models that aim to bring together the log-
ical and distributional approaches, are active top-
ics in current natural language semantics research,
e.g. see (Grefenstette et al, 2011; Grefenstette and
Sadrzadeh, 2011; Clark et al, 2010; Baroni and
Zamparelli, 2010; Guevara, 2010; Mitchell and La-
pata, 2008).
In this paper, we focus on our recent concrete Dis-
CoCat model (Grefenstette and Sadrzadeh, 2011)
and in particular on nouns composed with transitive
verbs. Whereby the meaning of a transitive sentence
?sub tverb obj? is obtained by taking the component-
wise multiplication of the matrix of the verb with
the kronecker product of the vectors of subject and
object:
?????????
sub tverb obj = tverb (
??
sub?
??
obj) (1)
In most logical models, transitive verbs are modeled
as relations; in the categorical model the relational
nature of such verbs gets manifested in their ma-
trix representation: if subject and object are each r-
dimensional row vectors in some space N , the verb
will be a r ? r matrix in the space N ? N . There
are different ways of learning the weights of this ma-
trix. In (Grefenstette and Sadrzadeh, 2011), we de-
veloped and implemented one such method on the
data from the British National Corpus. The matrix of
each verb was constructed by taking the sum of the
kronecker products of all of the subject/object pairs
linked to that verb in the corpus. We refer to this
method as the indirect method. This is because the
weight cij is obtained from the weights of the sub-
ject and object vectors (computed via co-occurrence
with bases ??n i and
??n j respectively), rather than di-
rectly from the context of the verb itself, as would
be the case in lexical distributional models. This
construction method was evaluated against an exten-
62
sion of Mitchell and Lapata (2008)?s disambiguation
task from intransitive to transitive sentences. We
showed and discussed how and why our method,
which is moreover scalable and respects the gram-
matical structure of the sentence, resulted in better
results than other known models of semantic vector
composition.
As a motivation for the current paper, note that
there are at least two different factors at work in
Equation (1): one is the matrix of the verb, denoted
by tverb, and the other is the kronecker product of
subject and object vectors
??
sub ?
??
obj. Our model?s
mathematical formulation of composition prohibits
us from changing the latter kronecker product, but
the ?content? of the verb matrices can be built
through different procedures.
In recent work we used a standard lexical distri-
butional model for nouns and engineered our verbs
to have a more sophisticated structure because of
the higher dimensional space they occupy. In par-
ticular, we argued that the resulting matrix of the
verb should represent ?the extent according to which
the verb has related the properties of subjects to the
properties of its objects?, developed a general proce-
dure to build such matrices, then studied their em-
pirical consequences. One question remained open:
what would be the consequence of starting from the
standard lexical vector of the verb, then encoding
it into the higher dimensional space using different
(possibly ad-hoc but nonetheless interesting) mathe-
matically inspired methods.
In a nutshell, the lexical vector of the verb is de-
noted by
???
tverb and similar to vectors of subject and
object, it is an r-dimensional row vector. Since the
kronecker product of subject and object (
??
sub?
??
obj)
is r ? r, in order to make
???
tverb applicable in Equa-
tion 1, i.e. to be able to substitute it for tverb, we
need to encode it into a r ? r matrix in the N ? N
space. In what follows, we investigate the empirical
consequences of three different encodings methods.
2 From Vectors to Matrices
In this section, we discuss three different ways of en-
coding r dimensional lexical verb vectors into r? r
verb matrices, and present empirical results for each.
We use the additional structure that the kronecker
product provides to represent the relational nature
of transitive verbs. The results are an indication that
the extra information contained in this larger space
contributes to higher quality composition.
One way to encode an r-dimensional vector as a
r ? r matrix is to embed it as the diagonal of that
matrix. It remains open to decide what the non-
diagonal values should be. We experimented with
0s and 1s as padding values. If the vector of the verb
is [c1, c2, ? ? ? , cr] then for the 0 case (referred to as
0-diag) we obtain the following matrix:
tverb =
?
?
?
?
?
c1 0 ? ? ? 0
0 c2 ? ? ? 0
...
...
. . .
...
0 0 . . . cr
?
?
?
?
?
For the 1 case (referred to as 1-diag) we obtain the
following matrix:
tverb =
?
?
?
?
?
c1 1 ? ? ? 1
1 c2 ? ? ? 1
...
...
. . .
...
1 1 . . . cr
?
?
?
?
?
We also considered a third case where the vector is
encoded into a matrix by taking the kronecker prod-
uct of the verb vector with itself:
tverb =
???
tverb?
???
tverb
So for
???
tverb = [c1, c2, ? ? ? , cr] we obtain the follow-
ing matrix:
tverb =
?
?
?
?
?
c1c1 c1c2 ? ? ? c1cr
c2c1 c2c2 ? ? ? c2cr
...
...
. . .
...
crc1 crc2 ? ? ? crcr
?
?
?
?
?
3 Degrees of synonymity for sentences
The degree of synonymity between meanings of
two sentences is computed by measuring their ge-
ometric distance. In this work, we used the co-
sine measure. For two sentences ?sub1 tverb1 obj1?
and ?sub2 tverb2 obj2?, this is obtained by taking
the Frobenius inner product of
???????????
sub1 tverb1 obj1 and???????????
sub2 tverb2 obj2. The use of Frobenius product
rather than the dot product is because the calcula-
tion in Equation (1) produces matrices rather than
row vectors. We normalized the outputs by the mul-
tiplication of the lengths of their corresponding ma-
trices.
63
4 Experiment
In this section, we describe the experiment used to
evaluate and compare these three methods. The ex-
periment is on the dataset developed in (Grefenstette
and Sadrzadeh, 2011).
Parameters We used the parameters described by
Mitchell and Lapata (2008) for the noun and verb
vectors. All vectors were built from a lemmatised
version of the BNC. The noun basis was the 2000
most common context words, basis weights were
the probability of context words given the target
word divided by the overall probability of the con-
text word. These features were chosen to enable
easy comparison of our experimental results with
those of Mitchell and Lapata?s original experiment,
in spite of the fact that there may be more sophisti-
cated lexical distributional models available.
Task This is an extension of Mitchell and Lap-
ata (2008)?s disambiguation task from intransitive
to transitive sentences. The general idea behind
the transitive case (similar to the intransitive one) is
as follows: meanings of ambiguous transitive verbs
vary based on their subject-object context. For in-
stance the verb ?meet? means ?satisfied? in the con-
text ?the system met the criterion? and it means
?visit?, in the context ?the child met the house?.
Hence if we build meaning vectors for these sen-
tences compositionally, the degrees of synonymity
of the sentences can be used to disambiguate the
meanings of the verbs in them.
Suppose a verb has two meanings a and b and
that it has occurred in two sentences. Then if in
both of these sentences it has its meaning a, the two
sentences will have a high degree of synonymity,
whereas if in one sentence the verb has meaning a
and in the other meaning b, the sentences will have
a lower degree of synonymity. For instance ?the sys-
tem met the criterion? and ?the system satisfied the
criterion? have a high degree of semantic similarity,
and similarly for ?the child met the house? and ?the
child visited the house?. This degree decreases for
the pair ?the child met the house? and ?the child sat-
isfied the house?.
Dataset The dataset is built using the same guide-
lines as Mitchell and Lapata (2008), using transi-
tive verbs obtained from CELEX1 paired with sub-
jects and objects. We first picked 10 transitive verbs
from the most frequent verbs of the BNC. For each
verb, two different non-overlapping meanings were
retrieved, by using the JCN (Jiang Conrath) infor-
mation content synonymity measure of WordNet to
select maximally different synsets. For instance for
?meet? we obtained ?visit? and ?satisfy?. For each
original verb, ten sentences containing that verb with
the same role were retrieved from the BNC. Exam-
ples of such sentences are ?the system met the crite-
rion? and ?the child met the house?. For each such
sentence, we generated two other related sentences
by substituting their verbs by each of their two syn-
onyms. For instance, we obtained ?the system sat-
isfied the criterion? and ?the system visited the cri-
terion? for the first meaning and ?the child satisfied
the house? and ?the child visited the house? for the
second meaning . This procedure provided us with
200 pairs of sentences.
The dataset was split into four non-identical sec-
tions of 100 entries such that each sentence appears
in exactly two sections. Each section was given to
a group of evaluators who were asked to assign a
similarity score to simple transitive sentence pairs
formed from the verb, subject, and object provided
in each entry (e.g. ?the system met the criterion?
from ?system meet criterion?). The scoring scale for
human judgement was [1, 7], where 1 was most dis-
similar and 7 most identical.
Separately from the group annotation, each pair in
the dataset was given the additional arbitrary classi-
fication of HIGH or LOW similarity by the authors.
Evaluation Method To evaluate our methods, we
first applied our formulae to compute the similar-
ity of each phrase pair on a scale of [0, 1] and then
compared it with human judgement of the same
pair. The comparison was performed by measuring
Spearman?s ?, a rank correlation coefficient ranging
from -1 to 1. This provided us with the degree of
correlation between the similarities as computed by
our model and as judged by human evaluators.
Following Mitchell and Lapata (2008), we also
computed the mean of HIGH and LOW scores.
However, these scores were solely based on the au-
thors? personal judgements and as such (and on their
1http://celex.mpi.nl/
64
own) do not provide a very reliable measure. There-
fore, like Mitchell and Lapata (2008), the models
were ultimately judged by Spearman?s ?.
The results are presented in table 4. The additive
and multiplicative rows have, as composition oper-
ation, vector addition and component-wise multipli-
cation. The Baseline is from a non-compositional
approach; it is obtained by comparing the verb vec-
tors of each pair directly and ignoring their subjects
and objects. The UpperBound is set to be inter-
annotator agreement.
Model High Low ?
Baseline 0.47 0.44 0.16
Add 0.90 0.90 0.05
Multiply 0.67 0.59 0.17
Categorical
Indirect matrix 0.73 0.72 0.21
0-diag matrix 0.67 0.59 0.17
1-diag matrix 0.86 0.85 0.08
v ? v matrix 0.34 0.26 0.28
UpperBound 4.80 2.49 0.62
Table 1: Results of compositional disambiguation.
The indirect matrix performed better than the
vectors encoded in diagonal matrices padded with
0 and 1. However, surprisingly, the kronecker prod-
uct of this vector with itself provided better results
than all the above. The results were statistically sig-
nificant with p < 0.05.
5 Analysis of the Results
Suppose the vector of subject is [s1, s2, ? ? ? , sr] and
the vector of object is
??
obj = [o1, o2, ? ? ? , or], then
the matrix of
??
sub?
??
obj is:
?
?
?
?
?
s1o1 s1o2 ? ? ? s1or
s2o1 s2o2 ? ? ? s2or
...
sro1 sro2 ? ? ? sror
?
?
?
?
?
After computing Equation (1) for each generation
method of tverb, we obtain the following three ma-
trices for the meaning of a transitive sentence:
0-diag :
?
?
?
?
?
c1s1o1 0 ? ? ? 0
0 c2s2o2 ? ? ? 0
...
...
. . .
...
0 0 ? ? ? crsror
?
?
?
?
?
This method discards all of the non-diagonal infor-
mation about the subject and object, for example
there is no occurrence of s1o2, s2o1, etc.
1-diag :
?
?
?
?
?
c1s1o1 s1o2 ? ? ? s1or
s2o1 c2s2o2 ? ? ? s2or
...
...
. . .
...
sro1 sro2 ? ? ? crsror
?
?
?
?
?
This method conserves the information about the
subject and object, but only applies the information
of the verb to the diagonals: s1 and o2, s2 and o1,
etc. are never related to each other via the verb.
v ? v :
?
?
?
?
?
c1c1s1o1 c1c2s1o2 ? ? ? c1crs1or
c2c1s2o1 c2c2s2o2 ? ? ? c2crs2or
...
...
. . .
...
crc1sro1 crc2sro2 ? ? ? crcrsror
?
?
?
?
?
This method not only conserves the information
of the subject and object, but also applies to them
all of the information encoded in the verb. These
data propagate to Frobenius products when comput-
ing the semantic similarity of sentences and justify
the empirical results.
The unexpectedly good performance of the v ? v
matrix relative to the more complex indirect method
is surprising, and certainly demands further inves-
tigation. What is sure is that they each draw upon
different aspects of semantic composition to provide
better results. There is certainly room for improve-
ment and empirical optimisation in both of these
relation-matrix construction methods.
Furthermore, the success of both of these meth-
ods relative to the others examined in Table 1 shows
that it is the extra information provided in the matrix
(rather than just the diagonal, representing the lexi-
cal vector) that encodes the relational nature of tran-
sitive verbs, thereby validating in part the require-
ment suggested in Coecke et al (2010) and Grefen-
stette and Sadrzadeh (2011) that relational word vec-
tors live in a space the dimensionality of which be a
function of the arity of the relation.
65
References
H. Alshawi (ed). 1992. The Core Language Engine.
MIT Press.
M. Baroni and R. Zamparelli. 2010. Nouns are vectors,
adjectives are matrices. Proceedings of Conference
on Empirical Methods in Natural Language Processing
(EMNLP).
D. Clarke, R. Lutz and D. Weir. 2010. Semantic
Composition with Quotient Algebras. Proceedings
of Geometric Models of Natural Language Semantics
(GEMS-2010).
S. Clark and S. Pulman. 2007. Combining Symbolic
and Distributional Models of Meaning. Proceedings
of AAAI Spring Symposium on Quantum Interaction.
AAAI Press.
B. Coecke, M. Sadrzadeh and S. Clark. 2010. Mathemat-
ical Foundations for Distributed Compositional Model
of Meaning. Lambek Festschrift. Linguistic Analysis
36, 345?384. J. van Benthem, M. Moortgat and W.
Buszkowski (eds.).
J. Curran. 2004. From Distributional to Semantic Simi-
larity. PhD Thesis, University of Edinburgh.
K. Erk and S. Pado?. 2004. A Structured Vector Space
Model for Word Meaning in Context. Proceedings
of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 897?906.
G. Frege 1892. U?ber Sinn und Bedeutung. Zeitschrift
fu?r Philosophie und philosophische Kritik 100.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis.
E. Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,
S. Pulman. 2011. Concrete Compositional Sentence
Spaces for a Compositional Distributional Model of
Meaning. International Conference on Computational
Semantics (IWCS?11). Oxford.
E. Grefenstette, M. Sadrzadeh. 2011. Experimental Sup-
port for a Categorical Compositional Distributional
Model of Meaning. Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
G. Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer.
E. Guevara. 2010. A Regression Model of Adjective-
Noun Compositionality in Distributional Semantics.
Proceedings of the ACL GEMS Workshop.
Z. S. Harris. 1966. A Cycling Cancellation-Automaton
for Sentence Well-Formedness. International Compu-
tation Centre Bulletin 5, 69?94.
R. Hudson. 1984. Word Grammar. Blackwell.
J. Lambek. 2008. From Word to Sentence. Polimetrica,
Milan.
T. Landauer, and S. Dumais. 2008. A solution to Platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological review.
C. D. Manning, P. Raghavan, and H. Schu?tze. 2008. In-
troduction to information retrieval. Cambridge Uni-
versity Press.
J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, 236?244.
R. Montague. 1974. English as a formal language. For-
mal Philosophy, 189?223.
J. Nivre 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
J. Saffron, E. Newport, R. Asling. 1999. Word Segmenta-
tion: The role of distributional cues. Journal of Mem-
ory and Language 35, 606?621.
H. Schuetze. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics 24, 97?123.
P. Smolensky. 1990. Tensor product variable binding
and the representation of symbolic structures in con-
nectionist systems. Computational Linguistics 46, 1?
2, 159?216.
M. Steedman. 2000. The Syntactic Process. MIT Press.
D. Widdows. 2005. Geometry and Meaning. University
of Chicago Press.
L. Wittgenstein. 1953. Philosophical Investigations.
Blackwell.
66
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74?82,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
?Not not bad? is not ?bad?: A distributional account of negation
Karl Moritz Hermann Edward Grefenstette
University of Oxford Department of Computer Science
Wolfson Building, Parks Road
Oxford OX1 3QD, United Kingdom
firstname.lastname@cs.ox.ac.uk
Phil Blunsom
Abstract
With the increasing empirical success of
distributional models of compositional se-
mantics, it is timely to consider the types
of textual logic that such models are ca-
pable of capturing. In this paper, we ad-
dress shortcomings in the ability of cur-
rent models to capture logical operations
such as negation. As a solution we pro-
pose a tripartite formulation for a continu-
ous vector space representation of seman-
tics and subsequently use this representa-
tion to develop a formal compositional no-
tion of negation within such models.
1 Introduction
Distributional models of semantics characterize
the meanings of words as a function of the words
they co-occur with (Firth, 1957). These models,
mathematically instantiated as sets of vectors in
high dimensional vector spaces, have been applied
to tasks such as thesaurus extraction (Grefenstette,
1994; Curran, 2004), word-sense discrimination
(Schu?tze, 1998), automated essay marking (Lan-
dauer and Dumais, 1997), and so on.
During the past few years, research has shifted
from using distributional methods for modelling
the semantics of words to using them for mod-
elling the semantics of larger linguistic units such
as phrases or entire sentences. This move from
word to sentence has yielded models applied to
tasks such as paraphrase detection (Mitchell and
Lapata, 2008; Mitchell and Lapata, 2010; Grefen-
stette and Sadrzadeh, 2011; Blacoe and Lapata,
2012), sentiment analysis (Socher et al, 2012;
Hermann and Blunsom, 2013), and semantic re-
lation classification (ibid.). Most efforts approach
the problem of modelling phrase meaning through
vector composition using linear algebraic vector
operations (Mitchell and Lapata, 2008; Mitchell
and Lapata, 2010; Zanzotto et al, 2010), matrix
or tensor-based approaches (Baroni and Zampar-
elli, 2010; Coecke et al, 2010; Grefenstette et al,
2013; Kartsaklis et al, 2012), or through the use
of recursive auto-encoding (Socher et al, 2011;
Hermann and Blunsom, 2013) or neural-networks
(Socher et al, 2012). On the non-compositional
front, Erk and Pado? (2008) keep word vectors sep-
arate, using syntactic information from sentences
to disambiguate words in context; likewise Turney
(2012) treats the compositional aspect of phrases
and sentences as a matter of similarity measure
composition rather than vector composition.
These compositional distributional approaches
often portray themselves as attempts to recon-
cile the empirical aspects of distributional seman-
tics with the structured aspects of formal seman-
tics. However, they in fact only principally co-opt
the syntax-sensitivity of formal semantics, while
mostly eschewing the logical aspects.
Expressing the effect of logical operations in
high dimensional distributional semantic models
is a very different task than in boolean logic. For
example, whereas predicates such as ?red? are seen
in predicate calculi as functions mapping elements
of some set Mred to > (and all other domain ele-
ments to ?), in compositional distributional mod-
els we give the meaning of ?red? a vector-like
representation, and devise some combination op-
eration with noun representations to obtain the
representation for an adjective-noun pair. Under
the logical view, negation of a predicate therefore
yields a new truth-function mapping elements of
the complement of Mred to > (and all other do-
main elements to?), but the effect of negation and
other logical operations in distributional models is
not so sharp: we expect the representation for ?not
red? to remain close to other objects of the same
domain of discourse (i.e. other colours) while be-
ing sufficiently different from the representation of
?red? in some manner. Exactly how textual logic
74
would best be represented in a continuous vector
space model remains an open problem.
In this paper we propose one possible formu-
lation for a continuous vector space based repre-
sentation of semantics. We use this formulation
as the basis for providing an account of logical
operations for distributional models. In particu-
lar, we focus on the case of negation and how it
might work in higher dimensional distributional
models. Our formulation separates domain, value
and functional representation in such a way as to
allow negation to be handled naturally. We ex-
plain the linguistic and model-related impacts of
this mode of representation and discuss how this
approach could be generalised to other semantic
functions.
In Section 2, we provide an overview of work
relating to that presented in this paper, covering
the integration of logical elements in distributional
models, and the integration of distributional el-
ements in logical models. In Section 3, we in-
troduce and argue for a tripartite representation
in distributional semantics, and discuss the issues
relating to providing a linguistically sensible no-
tion of negation for such representations. In Sec-
tion 4, we present matrix-vector models similar to
that of Socher et al (2012) as a good candidate
for expressing this tripartite representation. We
argue for the elimination of non-linearities from
such models, and thus show that negation cannot
adequately be captured. In Section 5, we present
a short analysis of the limitation of these matrix-
vector models with regard to the task of modelling
non-boolean logical operations, and present an im-
proved model bypassing these limitations in Sec-
tion 6. Finally, in Section 7, we conclude by sug-
gesting future work which will extend and build
upon the theoretical foundations presented in this
paper.
2 Motivation and Related Work
The various approaches to combining logic with
distributional semantics can broadly be put into
three categories: those approaches which use
distributional models to enhance existing logical
tools; those which seek to replicate logic with the
mathematical constructs of distributional models;
and those which provide new mathematical defini-
tions of logical operations within distributional se-
mantics. The work presented in this paper is in the
third category, but in this section we will also pro-
vide a brief overview of related work in the other
two in order to better situate the work this paper
will describe in the literature.
Vector-assisted logic The first class of ap-
proaches seeks to use distributional models of
word semantics to enhance logic-based models of
textual inference. The work which best exempli-
fies this strand of research is found in the efforts of
Garrette et al (2011) and, more recently, Beltagy
et al (2013). This line of research converts logi-
cal representations obtained from syntactic parses
using Bos? Boxer (Bos, 2008) into Markov Logic
Networks (Richardson and Domingos, 2006), and
uses distributional semantics-based models such
as that of Erk and Pado? (2008) to deal with issues
polysemy and ambiguity.
As this class of approaches deals with improv-
ing logic-based models rather than giving a dis-
tributional account of logical function words, we
view such models as orthogonal to the effort pre-
sented in this paper.
Logic with vectors The second class of ap-
proaches seeks to integrate boolean-like logical
operations into distributional semantic models us-
ing existing mechanisms for representing and
composing semantic vectors. Coecke et al (2010)
postulate a mathematical framework generalising
the syntax-semantic passage of Montague Gram-
mar (Montague, 1974) to other forms of syntac-
tic and semantic representation. They show that
the parses yielded by syntactic calculi satisfying
certain structural constraints can be canonically
mapped to vector combination operations in dis-
tributional semantic models. They illustrate their
framework by demonstrating how the truth-value
of sentences can be obtained from the combina-
tion of vector representations of words and multi-
linear maps standing for logical predicates and re-
lations. They furthermore give a matrix interpre-
tation of negation as a ?swap? matrix which in-
verts the truth-value of vectorial sentence repre-
sentations, and show how it can be embedded in
sentence structure.
Recently, Grefenstette (2013) showed that the
examples from this framework could be extended
to model a full quantifier-free predicate logic using
tensors of rank 3 or lower. In parallel, Socher et
al. (2012) showed that propositional logic can be
learned using tensors of rank 2 or lower (i.e. only
matrices and vectors) through the use of non-linear
75
activation functions in recursive neural networks.
The work of Coecke et al (2010) and Grefen-
stette (2013) limits itself to defining, rather than
learning, distributional representations of logical
operators for distributional models that simulate
logic, and makes no pretense to the provision of
operations which generalise to higher-dimensional
distributional semantic representations. As for
the non-linear approach of Socher et al (2012),
we will discuss, in Section 4 below, the limita-
tions with this model with regard to the task of
modelling logic for higher dimensional represen-
tations.
Logic for vectors The third and final class of
approaches is the one the work presented here
belongs to. This class includes attempts to de-
fine representations for logical operators in high
dimensional semantic vector spaces. Such ap-
proaches do not seek to retrieve boolean logic and
truth values, but to define what logical operators
mean when applied to distributional representa-
tions. The seminal work in this area is found in the
work of Widdows and Peters (2003), who define
negation and other logical operators algebraically
for high dimensional semantic vectors. Negation,
under this approach, is effectively a binary rela-
tion rather than a unary relation: it expresses the
semantics of statements such as ?A NOT B? rather
than merely ?NOT B?, and does so by projecting
the vector for A into the orthogonal subspace of
the vector for B. This approach to negation is use-
ful for vector-based information retrieval models,
but does not necessarily capture all the aspects of
negation we wish to take into consideration, as
will be discussed in Section 3.
3 Logic in text
In order to model logical operations over semantic
vectors, we propose a tripartite meaning represen-
tation, which combines the separate and distinct
treatment of domain-related and value-related as-
pects of semantic vectors with a domain-driven
syntactic functional representation. This is a unifi-
cation of various recent approaches to the problem
of semantic representation in continuous distribu-
tional semantic modelling (Socher et al, 2012;
Turney, 2012; Hermann and Blunsom, 2013).
We borrow from Socher et al (2012) and oth-
ers (Baroni and Zamparelli, 2010; Coecke et al,
2010) the idea that the information words refer to
is of two sorts: first the semantic content of the
word, which can be seen as the sense or reference
to the concept the word stands for, and is typi-
cally modelled as a semantic vector; and second,
the function the word has, which models the effect
the word has on other words it combines with in
phrases and sentences, and is typically modelled
as a matrix or higher-order tensor. We borrow
from Turney (2012) the idea that the semantic as-
pect of a word should not be modelled as a single
vector where everything is equally important, but
ideally as two or more vectors (or, as we do here,
two or more regions of a vector) which stand for
the aspects of a word relating to its domain, and
those relating to its value.
We therefore effectively suggest a tripartite rep-
resentation of the semantics of words: a word?s
meaning is modelled by elements representing its
value, domain, and function, respectively.
The tripartite representation We argue that the
tripartite representation suggested above allows us
to explicitly capture several aspects of semantics.
Further, while there may be additional distinct as-
pects of semantics, we argue that this is a minimal
viable representation.
First of all, the differentiation between do-
main and value is useful for establishing similar-
ity within subspaces of meaning. For instance,
the words blue and red share a common domain
(colours) while having very different values. We
hypothesise that making this distinction explicit
will allow for the definition of more sophisticated
and fine-grained semantics of logical operations,
as discussed below. Although we will represent
domain and value as two regions of a vector, there
is no reason for these not to be treated as separate
vectors at the time of comparison, as done by Tur-
ney (2012).
Through the third part, the functional repre-
sentation, we capture the compositional aspect of
semantics: the functional representation governs
how a term interacts with its environment. In-
spired by the distributional interpretation (Baroni
and Zamparelli, 2010; Coecke et al, 2010) of
syntactically-paramatrized semantic composition
functions from Montogovian semantics (Mon-
tague, 1974), we will also assume the function part
of our representation to be parametrized princi-
pally by syntax and domain rather than value. The
intuition behind taking domain into account in ad-
dition to syntactic class being that all members of
a domain largely interact with their environment
76
in the same fashion.
Modeling negation The tripartite representation
proposed above allows us to define logical opera-
tions in more detail than competing approaches.
To exemplify this, we focus on the case of nega-
tion.
We define negation for semantic vectors to be
the absolute complement of a term in its domain.
This implies that negation will not affect the do-
main of a term but only its value. Thus, blue and
not blue are assumed to share a common domain.
We call this naive form of negation the inversion
of a term A, which we idealise as the partial inver-
sion Ainv of the region associated with the value
of the word in its vector representation A.
?
?
d
v
v
?
?
?
?
d
v
?v
?
?
?
?
d
v
??v
?
?
[
f
] [
f
] [
f
]
W Winv ?W
Figure 1: The semantic representations of a word
W , its inverse W inv and its negation ?W . The
domain part of the representation remains un-
changed, while the value part will partially be in-
verted (inverse), or inverted and scaled (negation)
with 0 < ? < 1. The (separate) functional repre-
sentation also remains unchanged.
Additionally, we expect negation to have a
diminutive effect. This diminutive effect is best
exemplified in the case of sentiment: good is more
positive than not bad, even though good and bad
are antonyms of each other. By extension not not
good and not not not bad end up somewhere in the
middle?qualitative statements still, but void of
any significant polarity. To reflect this diminutive
effect of negation and double negation commonly
found in language, we define the idealised diminu-
tive negation ?A of a semantic vectorA as a scalar
inversion over a segment of the value region of its
representation with the scalar ? : 0 < ? < 1, as
shown in Figure 1.
As we defined the functional part of our rep-
resentation to be predominately parametrized by
syntax and domain, it will remain constant under
negation and inversion.
4 A general matrix-vector model
Having discussed, above, how the vector compo-
nent of a word can be partitioned into domain and
value, we now turn to the partition between se-
mantic content and function. A good candidate for
modelling this partition would be a dual-space rep-
resentation similar to that of Socher et al (2012).
In this section, we show that this sort of represen-
tation is not well adapted to the modelling of nega-
tion.
Models using dual-space representations have
been proposed in several recent publications, no-
tably in Turney (2012) and Socher et al (2012).
We use the class of recursive matrix-vector mod-
els as the basis for our investigation; for a detailed
introduction see the MV-RNN model described in
Socher et al (2012).
We begin by describing composition for a gen-
eral dual-space model, and apply this model to the
notion of compositional logic in a tripartite repre-
sentation discussed earlier. We identify the short-
comings of the general model and subsequently
discuss alternative composition models and mod-
ifications that allow us to better capture logic in
vector space models of meaning.
Assume a basic model of compositionality for
such a tripartite representation as follows. Each
term is encoded by a semantic vector v captur-
ing its domain and value, as well as a matrix M
capturing its function. Thus, composition consists
of two separate operations to learn semantics and
function of the composed term:
vp = fv(va,vb,Ma,Mb) (1)
Mp = fM (Ma,Mb)
As we defined the functional representation to be
parametrized by syntax and domain, its compo-
sition function does not require va and vb as in-
puts, with all relevant information already being
contained in Ma,Mb. In the case of Socher et al
(2012) these functions are as follows:
Mp =WM
[
Ma
Mb
]
(2)
vp = g
(
Wv
[
Mavb
Mbva
])
(3)
where g is a non-linearity.
4.1 The question of non-linearities
While the non-linearity g could be equipped with
greater expressive power, such as in the boolean
77
logic experiment in Socher et al (2012)), the aim
of this paper is to place the burden of composition-
ality on the atomic representations instead. For
this reason we treat g as an identity function, and
WM , Wv as simple additive matrices in this inves-
tigation, by setting
g = I Wv =WM = [I I]
where I is an identity matrix. This simplification
is justified for several reasons.
A simple non-linearity such as the commonly
used hyperbolic tangent or sigmoid function will
not add sufficient power to overcome the issues
outlined in this paper. Only a highly complex non-
linear function would be able to satisfy the require-
ments for vector space based logic as discussed
above. Such a function would defeat the point
however, by pushing the ?heavy-lifting? from the
model structure into a separate function.
Furthermore, a non-linearity effectively en-
codes a scattergun approach: While it may have
the power to learn a desired behaviour, it similarly
has a lot of power to learn undesired behaviours
and side effects. From a formal perspective it
would therefore seem more prudent to explicitly
encode desired behaviour in a model?s structure
rather than relying on a non-linearity.
4.2 Negation
We have outlined our formal requirements for
negation in the previous section. From these re-
quirements we can deduce four equalities, con-
cerning the effect of negation and double nega-
tion on the semantic representation and function
of a term. The matrices J? and J? (illustrated in
?
?
?
?
?
?
?
?
?
?
1
. . . 0
1
??
0
. . .
??
?
?
?
?
?
?
?
?
?
?
Figure 2: A partially scaled and inverted identity
matrix J?. Such a matrix can be used to trans-
form a vector storing a domain and value repre-
sentation into one containing the same domain but
a partially inverted value, such as W and ?W de-
scribed in Figure 1.
Figure 2) describe a partially scaled and inverted
identity matrix, where 0 < ?, ? < 1.
fv(not, a) = J?va (4)
fM (not, a) ?Ma (5)
fv(not, fv(not, a)) = J?J?va (6)
fM (not, fM (not, a)) ?Ma (7)
Based on our assumption about the constant do-
main and interaction across negation, we can re-
place the approximate equality with a strict equal-
ity in Equations 5 and 7. Further, we assume that
both Ma 6= I and Ma 6= 0, i.e. that A has a spe-
cific and non-zero functional representation. We
make a similar assumption for the semantic repre-
sentation va 6= 0.
Thus, to satisfy the equalities in Equations 4
through 7, we can deduce the values of vnot and
Mnot as discussed below.
Value and Domain in Negation Under the sim-
plifications of the model explained earlier, we
know that the following is true:
fv(a, b) = g
(
Wv
[
Mavb
Mbva
])
= I
(
[
I I
]
[
Mavb
Mbva
])
=Mavb +Mbva
I.e. the domain and value representation of a par-
ent is the sum of the two Mv multiplications of
its children. The matrix Wv could re-weight this
addition, but that would not affect the rest of this
analysis.
Given the idea that the domain stays constant
under negation and that a part of the value is in-
verted and scaled, we further know that these two
equations hold:
?a ? A : fv(not, a) = J?va
?a ? A : fv(not, fv(not, a)) = J?J?va
Assuming that both semantic and functional
representation across all A varies and is non-zero,
these equalities imply the following conditions for
the representation of not:
Mnot = J? = J?
vnot = 0
These two equations suggest that the term not has
no inherent value (vnot = 0), but merely acts as a
function, inverting part of another terms semantic
representation (Mnot = J?).
78
Functional Representation in Negation We
can apply the same method to the functional rep-
resentation. Here, we know that:
fM (a, b) =WM
[
Ma
Mb
]
=
[
I I
]
[
Ma
Mb
]
=Ma +Mb
Further, as defined in our discussion of nega-
tion, we require the functional representation to
remain unchanged under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
These requirements combined leave us to con-
clude that Mnot = 0. Combined with the result
from the first part of the analysis, this causes a
contradiction:
Mnot = 0
Mnot = J?
=? J? = 0 
This demonstrates that the MV-RNN as de-
scribed in this paper is not capable of modelling
semantic logic according to the principles we out-
lined. The fact that we would require Mnot = 0
further supports the points made earlier about the
non-linearities and setting WM to
[
I I
]
. Even a
specific WM and non-linearity would not be able
to ensure that the functional representation stays
constant under negation given a non-zero Mnot.
Clearly, any other complex semantic represen-
tation would suffer from the same issue here?the
failure of double-negation to revert a representa-
tion to its (diminutive) original.
5 Analysis
The issue identified with the MV-RNN style mod-
els described above extends to a number of other
models of vector spaced compositionality. It can
be viewed as a problem of uninformed composi-
tion caused by a composition function that fails to
account for syntax and thus for scope.
Of course, identifying the scope of negation is a
hard problem in its own right?see e.g. the *SEM
2012 shared task (Morante and Blanco, 2012).
However, at least for simple cases, we can deduce
scope by considering the parse tree of a sentence:
S
VP
ADJP
JJ
blue
RB
not
VBZ
is
NP
N
car
Det
This
Figure 3: The parse tree for This car is not blue,
highlighting the limited scope of the negation.
If we consider the parse tree for this car is not blue,
it is clear that the scope of the negation expressed
includes the colour but not the car (Figure 3).
While the MV-RNN model in Socher et al
(2012) incorporates parse trees to guide the order
of its composition steps, it uses a single composi-
tion function across all steps. Thus, the functional
representation of not will to some extent propagate
outside of its scope, leading to a vector capturing
something that is not blue, but also not quite a car.
There are several possibilities for addressing
this issue. One possibility is to give greater weight
to syntax, for instance by parametrizing the com-
position functions fv and fM on the parse struc-
ture. This could be achieved by using specific
weight matrices Wv and WM for each possible
tag. While the power of this approach is limited
by the complexity of the parse structure, it would
be better able to capture effects such as the scoping
and propagation of functional representations.
Another approach, which we describe in greater
detail in the next section, pushes the issue of
propagation onto the word level. While both ap-
proaches could easily be combined, this second
option is more consistent with our aim of avoid-
ing the implicit encoding of logic into fixed model
parameters in favour of the explicit encoding in
model structure.
6 An improved model
As we outlined in this paper, a key requirement
for a compositional model motivated by formal se-
mantics is the ability to propagate functional rep-
resentations, but also to not propagate these repre-
sentations when doing so is not semantically ap-
propriate. Here, we propose a modification of the
MV-RNN class of models that can capture this dis-
79
tinction without the need to move the composition
logic into the non-linearity.
We add a parameter ? to the representation of
each word, controlling the degree to which its
functional representation propagates after having
been applied in its own composition step.
Thus, the composition step of the new model
requires three equations:
Mp =WM
[
?a
?a+?b
Ma
?b
?a+?b
Mb
]
(8)
vp = g
(
Wv
[
Mavb
Mbva
])
(9)
?p = max(?a, ?b) (10)
Going back to the discussion of negation, this
model has the clear advantage of being able to cap-
ture negation in the way we defined it. As fv(a, b)
is unchanged, these two equations still hold:
Mnot = J? = J?
vnot = 0
However, as fM (a, b) is changed, the second
set of equations changes. We use Z as the ?-
denominator (Z = ?a + ?B) for simplification:
fM (a, b) =WM
[?a
Z Ma
?b
Z Mb
]
=
[
I
I
] [?a
Z Ma
?b
Z Mb
]
=
?a
Z
Ma +
?b
Z
Mb
Further, we still require the functional representa-
tion to remain constant under negation:
?a ? A : fM (not, a) =Ma
?a ? A : fM (not, fM (not, a)) =Ma
Thus, we can infer the following two conditions
on the new model:
?not
Z
Mnot ? 0
?a
Z
Ma ?Ma
From our previous investigation we already know
that Mnot = J? 6= 0, i.e. that not has a non-
zero functional representation. While this caused
a contradiction for the original MV-RNN model,
the design of the improved model can resolve this
issue through the ?-parameter:
?not = 0
Thus, we can use this modified MV-RNN model
to represent negation according to the principles
outlined in this paper. The result ?not = 0 is in
accordance with our intuition about the propaga-
tion of functional aspects of a term: We commonly
expect negation to directly affect the things un-
der its scope (not blue) by choosing their semantic
complement. However, this behaviour should not
propagate outside of the scope of the negation. A
not blue car is still very much a car, and when a
film is not good, it is still very much a film.
7 Discussion and Further Work
In this paper, we investigated the capability of con-
tinuous vector space models to capture the seman-
tics of logical operations in non-boolean cases.
Recursive and recurrent vector models of meaning
have enjoyed a considerable amount of success in
recent years, and have been shown to work well on
a number of tasks. However, the complexity and
subsequent power of these models comes at the
price that it can be difficult to establish which as-
pect of a model is responsible for what behaviour.
This issue was recently highlighted by an inves-
tigation into recursive autoencoders for sentiment
analysis (Scheible and Schu?tze, 2013). Thus, one
of the key challenges in this area of research is the
question of how to control the power of these mod-
els. This challenge motivated the work in this pa-
per. By removing non-linearities and other param-
eters that could disguise model weaknesses, we fo-
cused our work on the basic model design. While
such features enhance model power, they should
not be used to compensate for inherently flawed
model designs.
As a prerequisite for our investigation we estab-
lished a suitable encoding of textual logic. Distri-
butional representations have been well explained
on the word level, but less clarity exists as to the
semantic content of compositional vectors. With
the tripartite meaning representation we proposed
one possible approach in that direction, which we
subsequently expanded by discussing how nega-
tion should be captured in this representation.
Having established a suitable and rigorous sys-
tem for encoding meaning in compositional vec-
tors, we were thus able to investigate the repre-
80
sentative power of the MV-RNN model. We fo-
cused this paper on the case of negation, which
has the advantage that it does not require many
additional assumptions about the underlying se-
mantics. Our investigation showed that the basic
MV-RNN model is incompatible with our notion
of negation and thus with any textual logic build-
ing on this proposal.
Subsequently, we analysed the reasons for this
failure. We explained how the issue of nega-
tion affects the general class of MV-RNN models.
Through the issue of double-negation we further
showed how this issue is largely independent on
the particular semantic encoding used. Based on
this analysis we proposed an improved model that
is able to capture such textual logic.
In summary, this paper has two key contribu-
tions. First, we developed a tripartite represen-
tation for vector space based models of seman-
tics, incorporating multiple previous approaches
to this topic. Based on this representation, the
second contribution of this paper was a modified
MV-RNN model that can capture effects such as
negation in its inherent structure.
In future work, we would like to build on the
proposals in this paper, both by extending our
work on textual logic to include formulations for
e.g. function words, quantifiers, or locative words.
Similarly, we plan to experimentally validate these
ideas. Possible tasks for this include sentiment
analysis and relation extraction tasks such as in
Socher et al (2012) but also more specific tasks
such as the *SEM shared task on negation scope
and reversal (Morante and Blanco, 2012).
Acknowledgements
The first author is supported by the UK Engineer-
ing and Physical Sciences Research Council (EP-
SRC). The second author is supported by EPSRC
Grant EP/I03808X/1.
References
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, E. Erk, and
R. Mooney. 2013. Montague meets markov: Deep
semantics with probabilistic logical form. June.
W. Blacoe and M. Lapata. 2012. A comparison of
vector-based representations for semantic composi-
tion. Proceedings of the 2012 Conference on Empir-
ical Methods in Natural Language Processing.
J. Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on
Semantics in Text Processing, pages 277?286. Asso-
ciation for Computational Linguistics.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical Foundations for a Compositional Distribu-
tional Model of Meaning. March.
J. R. Curran. 2004. From distributional to semantic
similarity. Ph.D. thesis.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. Proceedings
of the Conference on Empirical Methods in Natural
Language Processing - EMNLP ?08, (October):897.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis.
D. Garrette, K. Erk, and R. Mooney. 2011. Integrating
logical representations with probabilistic informa-
tion using markov logic. In Proceedings of the Ninth
International Conference on Computational Seman-
tics, pages 105?114. Association for Computational
Linguistics.
E. Grefenstette and M. Sadrzadeh. 2011. Experi-
mental support for a categorical compositional dis-
tributional model of meaning. In Proceedings of
EMNLP, pages 1394?1404.
E. Grefenstette, G. Dinu, Y. Zhang, M. Sadrzadeh, and
M. Baroni. 2013. Multi-step regression learning
for compositional distributional semantics. In Pro-
ceedings of the Tenth International Conference on
Computational Semantics. Association for Compu-
tational Linguistics.
E. Grefenstette. 2013. Towards a formal distributional
semantics: Simulating logical calculi with tensors.
Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics.
G. Grefenstette. 1994. Explorations in automatic the-
saurus discovery.
K. M. Hermann and P. Blunsom. 2013. The role of
syntax in vector space models of compositional se-
mantics. In Proceedings of ACL, Sofia, Bulgaria,
August. Association for Computational Linguistics.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
In Proceedings of 24th International Conference
on Computational Linguistics (COLING 2012):
Posters, pages 549?558, Mumbai, India, December.
81
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological review.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
volume 8.
J. Mitchell and M. Lapata. 2010. Composition in Dis-
tributional Models of Semantics. Cognitive Science.
R. Montague. 1974. English as a Formal Language.
Formal Semantics: The Essential Readings.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
?12, pages 265?274, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine learning, 62(1-2):107?136.
C. Scheible and H. Schu?tze. 2013. Cutting recursive
autoencoder trees. In Proceedings of the Interna-
tional Conference on Learning Representations.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational linguistics, 24(1):97?123.
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011. Dynamic pooling and un-
folding recursive autoencoders for paraphrase detec-
tion. Advances in Neural Information Processing
Systems, 24:801?809.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic compositionality through recursive
matrix-vector spaces. Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1201?1211.
P. D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
D. Widdows and S. Peters. 2003. Word vectors and
quantum logic: Experiments with negation and dis-
junction. Mathematics of language, 8(141-154).
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 1263?1271. Associa-
tion for Computational Linguistics.
82
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46?54,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
A Type-Driven Tensor-Based Semantics for CCG
Jean Maillard
University of Cambridge
Computer Laboratory
jm864@cam.ac.uk
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Edward Grefenstette
University of Oxford
Department of Computer Science
edward.grefenstette@cs.ox.ac.uk
Abstract
This paper shows how the tensor-based se-
mantic framework of Coecke et al. can
be seamlessly integrated with Combina-
tory Categorial Grammar (CCG). The inte-
gration follows from the observation that
tensors are linear maps, and hence can
be manipulated using the combinators of
CCG, including type-raising and compo-
sition. Given the existence of robust,
wide-coverage CCG parsers, this opens up
the possibility of a practical, type-driven
compositional semantics based on distri-
butional representations.
1 Intoduction
In this paper we show how tensor-based distribu-
tional semantics can be seamlessly integrated with
Combinatory Categorial Grammar (CCG, Steed-
man (2000)), building on the theoretical discus-
sion in Grefenstette (2013). Tensor-based distribu-
tional semantics represents the meanings of words
with particular syntactic types as tensors whose se-
mantic type matches that of the syntactic type (Co-
ecke et al., 2010). For example, the meaning of a
transitive verb with syntactic type (S\NP)/NP is
a 3rd-order tensor from the tensor product space
N ? S ? N . The seamless integration with CCG
arises from the (somewhat trivial) observation that
tensors are linear maps ? a particular kind of
function ? and hence can be manipulated using
CCG?s combinatory rules.
Tensor-based semantics arises from the desire to
enhance distributional semantics with some com-
positional structure, in order to make distribu-
tional semantics more of a complete semantic the-
ory, and to increase its utility in NLP applica-
tions. There are a number of suggestions for how
to add compositionality to a distributional seman-
tics (Clarke, 2012; Pulman, 2013; Erk, 2012).
One approach is to assume that the meanings of
all words are represented by context vectors, and
then combine those vectors using some operation,
such as vector addition, element-wise multiplica-
tion, or tensor product (Clark and Pulman, 2007;
Mitchell and Lapata, 2008). A more sophisticated
approach, which is the subject of this paper, is to
adapt the compositional process from formal se-
mantics (Dowty et al., 1981) and attempt to build
a distributional representation in step with the syn-
tactic derivation (Coecke et al., 2010; Baroni et al.,
2013). Finally, there is a third approach using neu-
ral networks, which perhaps lies in between the
two described above (Socher et al., 2010; Socher
et al., 2012). Here compositional distributed rep-
resentations are built using matrices operating on
vectors, with all parameters learnt through a su-
pervised learning procedure intended to optimise
performance on some NLP task, such as syntac-
tic parsing or sentiment analysis. The approach
of Hermann and Blunsom (2013) conditions the
vector combination operation on the syntactic type
of the combinands, moving it a little closer to the
more formal semantics-inspired approaches.
The remainder of the Introduction gives a short
summary of distributional semantics. The rest of
the paper introduces some mathematical notation
from multi-linear algebra, including Einstein nota-
tion, and then shows how the combinatory rules of
CCG, including type-raising and composition, can
be applied directly to tensor-based semantic rep-
resentations. As well as describing a tensor-based
semantics for CCG, a further goal of this paper is to
present the compositional framework of Coecke et
al. (2010), which is based on category theory, to a
computational linguistics audience using only the
mathematics of multi-linear algebra.
1.1 Distributional Semantics
We assume a basic knowledge of distributional se-
mantics (Grefenstette, 1994; Sch?utze, 1998). Re-
46
cent inroductions to the topic include Turney and
Pantel (2010) and Clark (2014).
A potentially useful distinction for this paper,
and one not commonly made, is between distri-
butional and distributed representations. Distri-
butional representations are inherently contextual,
and rely on the frequently quoted dictum from
Firth that ?you shall know a word from the com-
pany it keeps? (Firth, 1957; Pulman, 2013). This
leads to the so-called distributional hypothesis that
words that occur in similar contexts tend to have
similar meanings, and to various proposals for
how to implement this hypothesis (Curran, 2004),
including alternative definitions of context; alter-
native weighting schemes which emphasize the
importance of some contexts over others; alterna-
tive similarity measures; and various dimension-
ality reduction schemes such as the well-known
LSA technique (Landauer and Dumais, 1997). An
interesting conceptual question is whether a sim-
ilar distributional hypothesis can be applied to
phrases and larger units: is it the case that sen-
tences, for example, have similar meanings if they
occur in similar contexts? Work which does ex-
tend the distributional hypothesis to larger units
includes Baroni and Zamparelli (2010), Clarke
(2012), and Baroni et al. (2013).
Distributed representations, on the other hand,
can be thought of simply as vectors (or possibly
higher-order tensors) of real numbers, where there
is no a priori interpretation of the basis vectors.
Neural networks can perhaps be categorised in this
way, since the resulting vector representations are
simply sequences of real numbers resulting from
the optimisation of some training criterion on a
training set (Collobert and Weston, 2008; Socher
et al., 2010). Whether these distributed represen-
tations can be given a contextual interpretation de-
pends on how they are trained.
One important point for this paper is that the
tensor-based compositional process makes no as-
sumptions about the interpretation of the tensors.
Hence in the remainder of the paper we make no
reference to how noun vectors or verb tensors,
for example, can be acquired (which, for the case
of the higher-order tensors, is a wide open re-
search question). However, in order to help the
reader who would prefer a more grounded dis-
cussion, one possibility is to obtain the noun vec-
tors using standard distributional techniques (Cur-
ran, 2004), and learn the higher-order tensors us-
ing recent techniques from ?recursive? neural net-
works (Socher et al., 2010). Another possibility
is suggested by Grefenstette et al. (2013), extend-
ing the learning technique based on linear regres-
sion from Baroni and Zamparelli (2010) in which
?gold-standard? distributional representations are
assumed to be available for some phrases and
larger units.
2 Mathematical Preliminaries
The tensor-based compositional process relies on
taking dot (or inner) products between vectors and
higher-order tensors. Dot products, and a number
of other operations on vectors and tensors, can be
conveniently written using Einstein notation (also
referred to as the Einstein summation convention).
In the rest of the paper we assume that the vector
spaces are over the field of real numbers.
2.1 Einstein Notation
The squared amplitude of a vector v ? R
n
is given
by:
|v|
2
=
n
?
i=1
v
i
v
i
Similarly, the dot product of two vectors v,w ?
R
n
is given by:
v ?w =
n
?
i=1
v
i
w
i
Denote the components of an m?n real matrix
A by A
ij
for 1 ? i ? m and 1 ? j ? n. Then
the matrix-vector product of A and v ? R
n
gives
a vector Av ? R
m
with components:
(Av)
i
=
n
?
j=1
A
ij
v
j
We can also multiply an n?mmatrixA and an
m ? o matrix B to produce an n ? o matrix AB
with components:
(AB)
ij
=
m
?
k=1
A
ik
B
kj
The previous examples are some of the most
common operations in linear algebra, and they all
involve sums over repeated indices. They can be
simplified by introducing the Einstein summation
convention: summation over the relevant range
is implied on every component index that occurs
47
twice. Pairs of indices that are summed over are
known as contracted, while the remaining indices
are known as free. Using this convention, the
above operations can be written as:
|v|
2
= v
i
v
i
v ?w = v
i
w
i
(Av)
i
= A
ij
v
j
, i.e. the contraction of v with
the second index of A
(AB)
ij
= A
ik
B
kj
, i.e. the contraction of the
second index of A with the first of B
Note how the number of free indices is always
conserved between the left- and right-hand sides in
these examples. For instance, while the last equa-
tion has two indices on the left and four on the
right, the two extra indices on the right are con-
tracted. Hence counting the number of free indices
can be a quick way of determining what type of
object is given by a certain mathematical expres-
sion in Einstein notation: no free indices means
that an operation yields a scalar number, one free
index means a vector, two a matrix, and so on.
2.2 Tensors
Linear Functionals Given a finite-dimensional
vector space R
n
over R, a linear functional is a
linear map a : R
n
? R.
Let a vector v have components v
i
in a fixed ba-
sis. Then the result of applying a linear functional
a to v can be written as:
a(v) = a
1
v
1
+? ? ?+a
n
v
n
=
(
a
1
? ? ? a
n
)
?
?
?
v
1
.
.
.
v
n
?
?
?
The numbers a
i
are the components of the lin-
ear functional, which can also be pictured as a row
vector. Since there is a one-to-one correspondence
between row and column vectors, the above equa-
tion is equivalent to:
v(a) = a
1
v
1
+? ? ?+a
n
v
n
=
(
v
1
? ? ? v
n
)
?
?
?
a
1
.
.
.
a
n
?
?
?
Using Einstein convention, the equations above
can be written as:
a(v) = v
i
a
i
= v(a)
Thus every finite-dimensional vector is a linear
functional, and vice versa. Row and column vec-
tors are examples of first-order tensors.
Definition 1 (First-order tensor). Given a vector
space V over the field R, a first-order tensor T
can be defined as:
? an element of the vector space V ,
? a linear map T : V ? R,
? a |V |-dimensional array of numbers T
i
, for
1 ? i ? |V |.
These three definitions are all equivalent. Given
a first-order tensor described using one of these
definitions, it is trivial to find the two other de-
scriptions.
Matrices An n?mmatrixA over R can be rep-
resented by a two-dimensional array of real num-
bers A
ij
, for 1 ? i ? n and 1 ? j ? m.
Via matrix-vector multiplication, the matrix A
can be seen as a linear map A : R
m
? R
n
. It
maps a vector v ? R
m
to a vector
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
v
1
.
.
.
v
m
?
?
?
,
with components
A(v)
i
= A
ij
v
j
.
We can also contract a vector with the first index
of the matrix, which gives us a map A : R
n
?
R
m
. This corresponds to the operation
(
w
1
? ? ? w
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
,
resulting in a vector with components
(w
T
A)
i
= A
ji
w
j
.
We can combine the two operations and see a
matrix as a map A : R
n
? R
m
? R, defined by:
w
T
Av =
(
w
1
? ? ? w
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
v
1
.
.
.
v
m
?
?
?
In Einstein notation, this operation can be writ-
ten as
w
i
A
ij
v
j
,
48
which yields a scalar (constant) value, consistent
with the fact that all the indices are contracted.
Finally, matrices can also be characterised in
terms of Kronecker products. Given two vectors
v ? R
n
and w ? R
m
, their Kronecker product
v ?w is a matrix
v ?w =
?
?
?
v
1
w
1
? ? ? v
1
w
m
.
.
.
.
.
.
.
.
.
v
n
w
1
? ? ? v
n
w
m
?
?
?
,
with components
(v ?w)
ij
= v
i
w
j
.
It is a general result in linear algebra that any
n ? m matrix can be written as a finite sum of
Kronecker products
?
k
x
(k)
? y
(k)
of a set of
vectors x
(k)
and y
(k)
. Note that the sum over k
is written explicitly as it would not be implied by
Einstein notation: this is because the index k does
not range over vector/matrix/tensor components,
but over a set of vectors, and hence that index ap-
pears in brackets.
An n ? m matrix is an element of the tensor
space R
n
?R
m
, and it can also be seen as a linear
map A : R
n
? R
m
? R. This is because, given
a matrix B with decomposition
?
k
x
(k)
? y
(k)
,
the matrix A can act as follows:
A(B) = A
ij
?
k
x
(k)
i
y
(k)
j
=
?
k
(
x
(k)
1
? ? ? x
(k)
n
)
?
?
?
A
11
? ? ? A
1m
.
.
.
.
.
.
.
.
.
A
n1
? ? ? A
nm
?
?
?
?
?
?
y
(k)
.
.
.
y
(k)
m
?
?
?
= A
ij
B
ij
.
Again, counting the number of free indices in the
last line tells us that this operation yields a scalar.
Matrices are examples of second-order tensors.
Definition 2 (Second-order tensor). Given vector
spaces V,W over the field R, a second-order ten-
sor T can be defined as:
? an element of the vector space V ?W ,
? a |V | ? |W |-dimensional array of numbers
T
ij
, for 1 ? i ? |V | and 1 ? j ? |W |,
? a (multi-) linear map:
? T : V ?W ,
? T : W ? V ,
? T : V ?W ? R or T : V ?W ? R.
Again, these definitions are all equivalent. Most
importantly, the four types of maps given in the
definition are isomorphic. Therefore specifying
one map is enough to specify all the others.
Tensors We can generalise these definitions to
the more general concept of tensor.
Definition 3 (Tensor). Given vector spaces
V
1
, . . . , V
k
over the field R, a k
th
-order tensor T
is defined as:
? an element of the vector space V
1
? ? ? ??V
k
,
? a |V
1
| ? ? ? ? ? |V
k
|, k
th
-dimensional array of
numbers T
i
1
???i
k
, for 1 ? i
j
? |V
j
|,
? a multi-linear map T : V
1
? ? ? ? ? V
k
? R.
3 Tensor-Based CCG Semantics
In this section we show how CCG?s syntactic types
can be given tensor-based meaning spaces, and
how the combinator?s employed by CCG to com-
bine syntactic categories carry over to those mean-
ing spaces, maintaining what is often described
as CCG?s ?transparent interface? between syntax
and semantics. Here are some example syntactic
types, and the corresponding tensor spaces con-
taining the meanings of the words with those types
(using the notation syntactic type : semantic type).
We first assume that all atomic types have
meanings living in distinct vector spaces:
? noun phrases, NP : N
? sentences, S : S
The recipe for determining the meaning space
of a complex syntactic type is to replace each
atomic type with its corresponding vector space
and the slashes with tensor product operators:
? Intransitive verb, S\NP : S? N
? Transitive verb, (S\NP)/NP : S? N? N
? Ditransitive verb, ((S\NP)/NP)/NP :
S? N? N? N
? Adverbial modifier, (S\NP)\(S\NP) :
S? N? S? N
? Preposition modifying NP , (NP\NP)/NP :
N? N? N
49
Hence the meaning of an intransitive verb, for
example, is a matrix in the tensor product space
S ? N. The meaning of a transitive verb is a
?cuboid?, or 3rd-order tensor, in the tensor product
space S?N?N. In the same way that the syntac-
tic type of an intransitive verb can be thought of as
a function ? taking an NP and returning an S ?
the meaning of an intransitive verb is also a func-
tion (linear map) ? taking a vector in N and re-
turning a vector in S. Another way to think of this
function is that each element of the matrix spec-
ifies, for a pair of basis vectors (one from N and
one from S), what the result is on the S basis vec-
tor given a value on the N basis vector.
Now we describe how the combinatory rules
carry over to the meaning spaces.
3.1 Application
The function application rules of CCG are forward
(>) and backward (<) application:
X/Y Y =? X (>)
Y X\Y =? X (<)
In a traditional semantics for CCG, if function
application is applied in the syntax, then function
application applies also in the semantics (Steed-
man, 2000). This is also true of the tensor-based
semantics. For example, the meaning of a subject
NP combines with the meaning of an intransitive
verb via matrix multiplication, which is equivalent
to applying the linear map corresponding to the
matrix to the vector representing the meaning of
the NP . Applying (multi-)linear maps in (multi-
)linear algebra is equivalent to applying tensor
contraction to the combining tensors. Here is the
case for an intransitive verb:
Pat walks
NP S\NP
N S? N
Let Pat be assigned a vector P ? N and walks
be assigned a second-order tensor W ? S ? N.
Using the backward application combinator cor-
responds to feeding P , an element of N, into W ,
seen as a function N? S. In terms of tensor con-
traction, this is the following operation:
W
ij
P
j
.
Here we use the convention that the indices
maintain the same order as the syntactic type.
Therefore, in the tensor of an object of type X/Y ,
the first index corresponds to the type X and the
second to the type Y . That is why, when perform-
ing the contraction corresponding to Pat walks,
P ? N is contracted with the second index of
W ? S ? N, and not the first.
1
The first index
of W is then the only free index, telling us that the
above operation yields a first-order tensor (vector).
Since this index corresponds to S, we know that
applying backward application to Pat walks yields
a meaning vector in S.
Forward application is performed in the same
manner. Consider the following example:
Pat kisses Sandy
NP (S\NP)/NP NP
N S? N? N N
with corresponding tensors P ? N for Pat, K ?
S? N? N for kisses and Y ? N for Sandy.
The forward application deriving the type of
kisses Sandy corresponds to
K
ijk
Y
k
,
where Y is contracted with the third index of K
because we have maintained the order defined by
the type (S\NP)/NP : the third index then corre-
sponds to an argument NP coming from the right.
Counting the number of free indices in the
above expression tells us that it yields a second-
order tensor. Looking at the types corresponding
to the free indices tells us that this second-order
tensor is of type S?N, which is the semantic type
of a verb phrase (or intransitive verb), as we have
already seen in the walks example.
3.2 Composition
The forward (>
B
) and backward (<
B
) composi-
tion rules are:
X/Y Y/Z =? X/Z (>
B
)
Y \Z X\Y =? X\Z (<
B
)
Composition in the semantics also reduces to a
form of tensor contraction. Consider the following
example, in which might can combine with kiss
using forward composition:
Pat might kiss Sandy
NP (S\NP)/(S\NP) (S\NP)/NP NP
N S? N? S? N S? N? N N
1
The particular order of the indices is not important, as
long as a convention such as this one is decided upon and
consistently applied to all types (so that tensor contraction
contracts the relevant tensors from each side when a combi-
nator is used).
50
with tensors M ? S ? N ? S ? N for might and
K ? S?N?N for kiss. Combining the meanings
of might and kiss corresponds to the following op-
eration:
M
ijkl
K
klm
,
yielding a tensor in S ? N ? N, which is the
correct semantic type for a phrase with syntactic
type (S\NP)/NP . Backward composition is per-
formed analogously.
3.3 Backward-Crossed Composition
English also requires the use of backward-crossed
composition (Steedman, 2000):
X/Y Z\X =? Z/Y (<
B
?
)
In tensor terms, this is the same as forward com-
position; we just need to make sure that the con-
traction matches up the correct parts of each ten-
sor correctly. Consider the following backward-
crossed composition:
(S\NP)/NP (S\NP)\(S\NP) ?
<
B
?
(S\NP)/NP
Let the two items on the left-hand side be rep-
resented by tensors A ? S ? N ? N and B ?
S ? N ? S ? N. Then, combining them with
backward-crossed composition in tensor terms is
B
ijkl
A
klm
,
resulting in a tensor in S ? N ? N (correspond-
ing to the indices i, j and m). Note that we have
reversed the order of tensors in the contraction to
make the matching of the indices more transpar-
ent; however, tensor contraction is commutative
(since it corresponds to a sum over products) so
the order of the tensors does not affect the result.
3.4 Type-raising
The forward (>
T
) and backward (<
T
) type-
raising rules are:
X =? T/(T\X) (>
T
)
X =? T\(T/X) (<
T
)
where T is a variable ranging over categories.
Suppose we are given an item of atomic type Y ,
with corresponding vector A ? Y. If we apply
forward type-raising to it, we get a new tensor of
type A
?
? T ? T ? Y. Now suppose the item of
type Y is followed by another item of type X\Y ,
with tensor B ? X ? Y. A phrase consisting of
two words with types Y and X\Y can be parsed
in two different ways:
? Y X\Y ? X , by backward application;
? Y X\Y ?
T
X/(X\Y ) X\Y , by forward
type-raising, and X/(X\Y ) X\Y ? X , by
forward application.
Both ways of parsing this sentence yield an item
of type X , and crucially the meaning of the result-
ing item should be the same in both cases.
2
This
property of type-raising provides an avenue into
determining what the tensor representation for the
type-raised category should be, since the tensor
representations must also be the same:
A
j
B
ij
= A
?
ijk
B
jk
.
Moreover, this equation must hold for all items,
B. As a concrete example, the requirement says
that a subject NP combining with a verb phrase
S\NP must produce the same meaning for the
two alternative derivations, irrespective of the verb
phrase. This is equivalent to the requirement that
A
j
B
ij
= A
?
ijk
B
jk
, ?B ? X? Y.
So to arrive at the tensor representation, we sim-
ply have to solve the tensor equation above. We
start by renaming the dummy index j on the left-
hand side:
A
k
B
ik
= A
?
ijk
B
jk
.
We then insert a Kronecker delta (?
ij
= 1 if i = j
and 0 otherwise):
A
k
B
jk
?
ij
= A
?
ijk
B
jk
.
Since the equation holds for allB, we are left with
A
?
ijk
= ?
ij
A
k
,
which gives us a recipe for performing type-
raising in a tensor-based model. The recipe is par-
ticularly simple and elegant: it corresponds to in-
serting the vector being type-raised into the 3rd-
order tensor at all places where the first two in-
dices are equal (with the rest of the elements in
the 3rd-order tensor being zero). For example, to
type-raise a subject NP , its meaning vector in N is
placed in the 3rd-order tensor S?S?N at all places
where the indices of the two S dimensions are the
same. Visually, the 3rd-order tensor correspond-
ing to the meaning of the type-raised category is
2
This property of CCG resulting from the use of type-
raising and composition is sometimes referred to as ?spurious
ambiguity?.
51
a cubiod in which the noun vector is repeated a
number of times (once for each sentence index),
resulting in a series of ?steps? progressing diag-
onally from the bottom of the cuboid to the top
(assuming a particular orientation).
The discussion so far has been somewhat ab-
stract, so to finish this section we include some
more examples with CCG categories, and show
that the tensor contraction operation has an intu-
itive similarity with the ?cancellation law? of cat-
egorial grammar which applies in the syntax.
First consider the example of a subject NP
with meaning A, combining with a verb phrase
S\NP with meaning B, resulting in a sentence
with meaning C. In the syntax, the two NPs can-
cel. In the semantics, for each basis of the sentence
space S we perform an inner product between two
vectors in N:
C
i
= A
j
B
ij
Hence, inner products in the tensor space corre-
spond to cancellation in the syntax.
This correspondence extends to complex argu-
ments, and also to composition. Consider the sub-
ject type-raising case, in which a subject NP with
meaning A in S ? S ? N combines with a verb
phrase S\NP with meaning B, resulting in a sen-
tence with meaning C. Again we perform inner
product operations, but this time the inner product
is between two matrices:
3
C
i
= A
ijk
B
jk
Note that two matrices are ?cancelled? for each
basis vector of the sentence space (i.e. for each
index i in C
i
).
As a final example, consider the forward com-
position from earlier, in which a modal verb with
meaningA in S?N?S?N combines with a tran-
sitive verb with meaning B in S ? N ? N to give
a transitive verb with meaning C in S ? N ? N.
Again the cancellation in the syntax corresponds
to inner products between matrices, but this time
we need an inner product for each combination of
3 indices:
C
ijk
= A
ijlm
B
lmk
3
To be more precise, the two matrices can be thought of
as vectors in the tensor space S ? N and the inner product is
between these vectors. Another way to think of this opera-
tion is to ?linearize? the two matrices into vectors and then
perform the inner product on these vectors.
For each i, j, k, two matrices ? corresponding to
the l,m indices above ? are ?cancelled?.
This intuitive explanation extends to arguments
with any number of slashes. For example, a
composition where the cancelling categories are
(N /N )/(N /N ) would require inner products be-
tween 4th-order tensors in N? N? N? N.
4 Related Work
The tensor-based semantics presented in this pa-
per is effectively an extension of the Coecke et al.
(2010) framework to CCG, re-expressing in Ein-
stein notation the existing categorical CCG exten-
sion in Grefenstette (2013), which itself builds
on an earlier Lambek Grammar extension to the
framework by Coecke et al. (2013).
This work also bears some similarity to the
treatment of categorial grammars presented by Ba-
roni et al. (2013), which it effectively encompasses
by expressing the tensor contractions described by
Baroni et al. as Einstein summations. However,
this paper also covers CCG-specific operations not
discussed by Baroni et al., such as type-raising and
composition.
One difference between this paper and the orig-
inal work by Coecke et al. (2010) is that they use
pregroups as the syntactic formalism (Lambek,
2008), a context-free variant of categorial gram-
mar. In pregroups, cancellation in the syntax is
always between two atomic categories (or more
precisely, between an atomic category and its ?ad-
joint?), whereas in CCG the arguments in complex
categories can be complex categories themselves.
To what extent this difference is significant re-
mains to be seen. For example, one area where this
may have an impact is when non-linearities are
added after contractions. Since the CCG contrac-
tions with complex arguments happen ?in one go?,
whereas the corresponding pregroup cancellation
in the semantics would be a series of contractions,
many more non-linearities would be added in the
pregroup case.
Krishnamurthy and Mitchell (2013) is based on
a similar insight to this paper ? that CCG provides
combinators which can manipulate functions op-
erating over vectors. Krishnamurthy and Mitchell
consider the function application case, whereas we
have shown how the type-raising and composition
operators apply naturally in this setting also.
52
5 Conclusion
This paper provides a theoretical framework for
the development of a compositional distributional
semantics for CCG. Given the existence of ro-
bust, wide-coverage CCG parsers (Clark and Cur-
ran, 2007; Hockenmaier and Steedman, 2002),
together with various techniques for learning the
tensors, the opportunity exists for a practical im-
plementation. However, there are significant engi-
neering difficulties which need to be overcome.
Consider adapting the neural-network learning
techniques of Socher et al. (2012) to this prob-
lem.
4
In terms of the number of tensors, the lexi-
con would need to contain a tensor for every word-
category pair; this is at least an order of magnitude
more tensors then the number of matrices learnt in
existing work (Socher et al., 2012; Hermann and
Blunsom, 2013). Furthermore, the order of the
tensors is now higher. Syntactic categories such as
((N /N )/(N /N ))/((N /N )/(N /N )) are not un-
common in the wide-coverage grammar of Hock-
enmaier and Steedman (2007), which in this case
would require an 8th-order tensor. This combina-
tion of many word-category pairs and higher-order
tensors results in a huge number of parameters.
As a solution to this problem, we are investigat-
ing ways to reduce the number of parameters, for
example using tensor decomposition techniques
(Kolda and Bader, 2009). It may also be possi-
ble to reduce the size of some of the complex cat-
egories in the grammar. Many challenges remain
before a type-driven compositional distributional
semantics can be realised, similar to the work of
Bos for the model-theoretic case (Bos et al., 2004;
Bos, 2005), but in this paper we have set out the
theoretical framework for such an implementation.
Finally, we repeat a comment made earlier that
the compositional framework makes no assump-
tions about the underlying vector spaces, or how
they are to be interpreted. On the one hand, this
flexibility is welcome, since it means the frame-
work can encompass many techniques for building
word vectors (and tensors). On the other hand, it
means that a description of the framework is nec-
essarily abstract, and it leaves open the question
4
Non-linear transformations are inherent to neural net-
works, whereas the framework in this paper is entirely linear.
However, as hinted at earlier in the paper, non-linear transfor-
mations can be applied to the output of each tensor, turning
the linear networks in this paper into extensions of those in
Socher et al. (2012) (extensions in the sense that the tensors
in Socher et al. (2012) do not extend beyond matrices).
of what the meaning spaces represent. The lat-
ter question is particularly pressing in the case of
the sentence space, and providing an interpretation
of such spaces remains a challenge for the distri-
butional semantics community, as well as relating
distributional semantics to more traditional topics
in semantics such as quantification and inference.
Acknowledgments
Jean Maillard is supported by an EPSRC MPhil
studentship. Stephen Clark is supported by ERC
Starting Grant DisCoTex (306920) and EPSRC
grant EP/I037512/1. Edward Grefenstette is sup-
ported by EPSRC grant EP/I037512/1. We would
like to thank Tamara Polajnar, Laura Rimell, Nal
Kalchbrenner and Karl Moritz Hermann for useful
discussion.
References
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics (to appear). Linguistic Issues
in Language Technologies.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240?
1246, Geneva, Switzerland.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics
(IWCS-6), pages 42?53, Tilburg, The Netherlands.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction, Stanford, CA. AAAI Press.
Stephen Clark. 2014. Vector space models of lexical
meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics
second edition. Wiley-Blackwell.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41?71.
53
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345?384.
Bob Coecke, Edward Grefenstette, and Mehrnoosh
Sadrzadeh. 2013. Lambek vs. Lambek: Functorial
vector space semantics and string diagrams for Lam-
bek calculus. Annals of Pure and Applied Logic.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML, Helsinki,
Finland.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
D.R. Dowty, R.E. Wall, and S. Peters. 1981. Introduc-
tion to Montague Semantics. Dordrecht.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635?653.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1?32.
Oxford: Philological Society.
Edward Grefenstette, Georgiana Dinu, YaoZhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multistep regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS-13), Potsdam, Germany.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer.
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. Proceedings of ACL, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335?342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
T. G. Kolda and B. W. Bader. 2009. Tensor decompo-
sitions and applications. SIAM Review, 51(3):455?
500.
Jayant Krishnamurthy and Tom M. Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Joachim Lambek. 2008. From Word to Sentence.
A Computational Algebraic Approach to Grammar.
Polimetrica.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato?s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211?
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236?244, Columbus, OH.
Stephen Pulman. 2013. Distributional semantic mod-
els. In Sadrzadeh Heunen and Grefenstette, editors,
Compositional Methods in Physics and Linguistics.
Oxford University Press.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS Deep
Learning and Unsupervised Feature Learning Work-
shop, Vancouver, Canada.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201?
1211, Jeju, Korea.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
54
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 22?27,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Deep Architecture for Semantic Parsing
Edward Grefenstette, Phil Blunsom, Nando de Freitas and Karl Moritz Hermann
Department of Computer Science
University of Oxford, UK
{edwgre, pblunsom, nando, karher}@cs.ox.ac.uk
Abstract
Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.
1 Introduction
The ubiquity of always-online computers in the
form of smartphones, tablets, and notebooks has
boosted the demand for effective question answer-
ing systems. This is exemplified by the grow-
ing popularity of products like Apple?s Siri or
Google?s Google Now services. In turn, this cre-
ates the need for increasingly sophisticated meth-
ods for semantic parsing. Recent work (Artzi and
Zettlemoyer, 2013; Kwiatkowski et al., 2013; Ma-
tuszek et al., 2012; Liang et al., 2011, inter alia)
has answered this call by progressively moving
away from strictly rule-based semantic parsing, to-
wards the use of distributed representations in con-
junction with traditional grammatically-motivated
re-write rules. This paper seeks to extend this line
of thinking to its logical conclusion, by provid-
ing the first (to our knowledge) entirely distributed
neural semantic generative parsing model. It does
so by adapting deep learning methods from related
work in sentiment analysis (Socher et al., 2012;
Hermann and Blunsom, 2013), document classifi-
cation (Yih et al., 2011; Lauly et al., 2014; Her-
mann and Blunsom, 2014a), frame-semantic pars-
ing (Hermann et al., 2014), and machine trans-
lation (Mikolov et al., 2010; Kalchbrenner and
Blunsom, 2013a), inter alia, combining two em-
pirically successful deep learning models to form
a new architecture for semantic parsing.
The structure of this short paper is as follows.
We first provide a brief overview of the back-
ground literature this model builds on in ?2. In ?3,
we begin by introducing two deep learning models
with different aims, namely the joint learning of
embeddings in parallel corpora, and the generation
of strings of a language conditioned on a latent
variable, respectively. We then discuss how both
models can be combined and jointly trained to
form a deep learning model supporting the gener-
ation of knowledgebase queries from natural lan-
guage questions. Finally, in ?4 we conclude by
discussing planned experiments and the data re-
quirements to effectively train this model.
2 Background
Semantic parsing describes a task within the larger
field of natural language understanding. Within
computational linguistics, semantic parsing is typ-
ically understood to be the task of mapping nat-
ural language sentences to formal representations
of their underlying meaning. This semantic rep-
resentation varies significantly depending on the
task context. For instance, semantic parsing has
been applied to interpreting movement instruc-
tions (Artzi and Zettlemoyer, 2013) or robot con-
trol (Matuszek et al., 2012), where the underlying
representation would consist of actions.
Within the context of question answering?the
focus of this paper?semantic parsing typically
aims to map natural language to database queries
that would answer a given question. Kwiatkowski
22
et al. (2013) approach this problem using a multi-
step model. First, they use a CCG-like parser
to convert natural language into an underspecified
logical form (ULF). Second, the ULF is converted
into a specified form (here a FreeBase query),
which can be used to lookup the answer to the
given natural language question.
3 Model Description
We describe a semantic-parsing model that learns
to derive quasi-logical database queries from nat-
ural language. The model follows the structure of
Kwiatkowski et al. (2013), but relies on a series of
neural networks and distributed representations in
lieu of the CCG and ?-Calculus based representa-
tions used in that paper.
The model described here borrows heavily from
two approaches in the deep learning literature.
First, a noise-contrastive neural network similar to
that of Hermann and Blunsom (2014a, 2014b) is
used to learn a joint latent representation for nat-
ural language and database queries (?3.1). Sec-
ond, we employ a structured conditional neural
language model in ?3.2 to generate queries given
such latent representations. Below we provide the
necessary background on these two components,
before introducing the combined model and de-
scribing its learning setup.
3.1 Bilingual Compositional Sentence Models
The bilingual compositional sentence model
(BiCVM) of Hermann and Blunsom (2014a) pro-
vides a state-of-the-art method for learning se-
mantically informative distributed representations
for sentences of language pairs from parallel cor-
pora. Through the joint production of a shared la-
tent representation for semantically aligned sen-
tence pairs, it optimises sentence embeddings
so that the respective representations of dissim-
ilar cross-lingual sentence pairs will be weakly
aligned, while those of similar sentence pairs will
be strongly aligned. Both the ability to jointly
learn sentence embeddings, and to produce latent
shared representations, will be relevant to our se-
mantic parsing pipeline.
The BiCVM model shown in Fig. 1 assumes
vector composition functions g and h, which map
an ordered set of vectors (here, word embed-
dings from D
A
,D
B
) onto a single vector in R
n
.
As stated above, for semantically equivalent sen-
tences a, b across languages L
A
,L
B
, the model
aims to minimise the distance between these com-
posed representations:
E
bi
(a, b) = ?g(a)? h(b)?
2
In order to avoid strong alignment between dis-
similar cross-lingual sentence pairs, this error
is combined with a noise-contrastive hinge loss,
where n ? L
B
is a randomly sampled sentence,
dissimilar to the parallel pair {a, b}, and m de-
notes some margin:
E
hl
(a, b, n) = [m+ E
bi
(a, b)? E
bi
(a, n)]
+
,
where [x]
+
= max(0, x). The resulting objective
function is as follows
J(?) =
?
(a,b)?C
(
k
?
i=1
E
hl
(a, b, n
i
) +
?
2
???
2
)
,
with
?
2
???
2
as the L
2
regularization term and
?={g, h,D
A
,D
B
} as the set of model variables.
...
L1 sentence embedding
L1 word embeddings
L2 sentence embedding
L2 word embeddings
contrastive estimation
g
h
Figure 1: Diagrammatic representation of a
BiCVM.
While Hermann and Blunsom (2014a) applied
this model only to parallel corpora of sentences,
it is important to note that the model is agnostic
concerning the inputs of functions g and h. In this
paper we will discuss how this model can be ap-
plied to non-sentential inputs.
23
3.2 Conditional Neural Language Models
Neural language models (Bengio et al., 2006) pro-
vide a distributed alternative to n-gram language
models, permitting the joint learning of a pre-
diction function for the next word in a sequence
given the distributed representations of a subset
of the last n?1 words alongside the representa-
tions themselves. Recent work in dialogue act la-
belling (Kalchbrenner and Blunsom, 2013b) and
in machine translation (Kalchbrenner and Blun-
som, 2013a) has demonstrated that a particular
kind of neural language model based on recurrent
neural networks (Mikolov et al., 2010; Sutskever
et al., 2011) could be extended so that the next
word in a sequence is jointly generated by the
word history and the distributed representation for
a conditioning element, such as the dialogue class
of a previous sentence, or the vector representation
of a source sentence. In this section, we briefly de-
scribe a general formulation of conditional neural
language models, based on the log-bilinear mod-
els of Mnih and Hinton (2007) due to their relative
simplicity.
A log-bilinear language model is a neural net-
work modelling a probability distribution over the
next word in a sequence given the previous n?1,
i.e. p(w
n
|w
1:n?1
). Let |V | be the size of our vo-
cabulary, and R be a |V | ? d vocabulary matrix
where the R
w
i
demnotes the row containing the
word embedding in R
d
of a word w
i
, with d be-
ing a hyper-parameter indicating embedding size.
Let C
i
be the context transform matrix in R
d?d
which modifies the representation of the ith word
in the word history. Let b
w
i
be a scalar bias as-
sociated with a word w
i
, and b
R
be a bias vector
in R
d
associated with the model. A log-bilinear
model expressed the probability of w
n
given a his-
tory of n?1 words as a function of the energy of
the network:
E(w
n
;w
1:n?1
) =
?
(
n?1
?
i=1
R
T
w
i
C
i
)
R
w
n
? b
T
R
R
w
n
? b
w
n
From this, the probability distribution over the
next word is obtained:
p(w
n
|w
1:n?1
) =
e
?E(w
n
;w
1:n?1
)
?
w
n
e
?E(w
n
;w
1:n?1
)
To reframe a log-bilinear language model as a
conditional language model (CNLM), illustrated
?
w
n
w
n-1
w
n-2
w
n-3
Figure 2: Diagrammatic representation of a Con-
ditional Neural Language Model.
in Fig. 2, let us suppose that we wish to jointly
condition the next word on its history and some
variable ?, for which an embedding r
?
has been
obtained through a previous step, in order to com-
pute p(w
n
|w
1:n?1
, ?). The simplest way to do this
additively, which allows us to treat the contribu-
tion of the embedding for ? as similar to that of an
extra word in the history. We define a new energy
function:
E(w
n
;w
1:n?1
, ?) =
?
((
n?1
?
i=1
R
T
w
i
C
i
)
+ r
T
?
C
?
)
R
w
n
? b
T
R
R
w
n
? b
w
n
to obtain the probability
p(w
n
|w
1:n?1
, ?) =
e
?E(w
n
;w
1:n?1
,?)
?
w
n
e
?E(w
n
;w
1:n?1
,?)
Log-bilinear language models and their condi-
tional variants alike are typically trained by max-
imising the log-probability of observed sequences.
3.3 A Combined Semantic Parsing Model
The models in ??3.1?3.2 can be combined to form
a model capable of jointly learning a shared la-
tent representation for question/query pairs using
a BiCVM, and using this latent representation to
learn a conditional log-bilinear CNLM. The full
model is shown in Fig. 3. Here, we explain the
final model architecture both for training and for
subsequent use as a generative model. The details
of the training procedure will be discussed in ?3.4.
The combination is fairly straightforward, and
happens in two steps at training time. For the
24
...
Knowledgebase query
Question
Latent
representation
Query embedding
Question embedding
Relation/object
embeddings
Word embeddings
Conditional
Log-bilinear
Language Model
g
h
Figure 3: Diagrammatic representation of the full
model. First the mappings for obtaining latent
forms of questions and queries are jointly learned
through a BiCVM. The latent form for questions
then serves as conditioning element in a log-
bilinear CNLM.
first step, shown in the left hand side of Fig. 3,
a BiCVM is trained against a parallel corpora
of natural language question and knowledgebase
query pairs. Optionally, the embeddings for the
query symbol representations and question words
are initialised and/or fine-tuned during training,
as discussed in ?3.4. For the natural language
side of the model, the composition function g can
be a simple additive model as in Hermann and
Blunsom (2014a), although the semantic informa-
tion required for the task proposed here would
probably benefit from a more complex composi-
tion function such as a convolution neural net-
work. Function h, which maps the knowledgebase
queries into the shared space could also rely on
convolution, although the structure of the database
queries might favour a setup relying primarily on
bi-gram composition.
Using function g and the original training data,
the training data for the second stage is created
by obtaining the latent representation for the ques-
tions of the original dataset. We thereby obtain
pairs of aligned latent question representations and
knowledgebase queries. This data allows us to
train a log-bilinear CNLM as shown on the right
side of Fig. 3.
Once trained, the models can be fully joined to
produce a generative neural network as shown in
Fig. 4. The network modelling g from the BiCVM
...
Question
Generated Query
g
Figure 4: Diagrammatic representation of the final
network. The question-compositional segment of
the BiCVM produces a latent representation, con-
ditioning a CNLM generating a query.
takes the distributed representations of question
words from unseen questions, and produces a la-
tent representation. The latent representation is
then passed to the log-bilinear CNLM, which con-
ditionally generates a knowledgebase query corre-
sponding to the question.
3.4 Learning Model Parameters
We propose training the model of ?3.3 in a two
stage process, in line with the symbolic model of
Kwiatkowski et al. (2013).
First, a BiCVM is trained on a parallel corpus
C of question-query pairs ?Q,R? ? C, using com-
position functions g for natural language questions
and h for database queries. While functions g and
hmay differ from those discussed in Hermann and
Blunsom (2014a), the basic noise-contrastive op-
timisation function remains the same. It is possi-
ble to initialise the model fully randomly, in which
25
case the model parameters ? learned at this stage
include the two distributed representation lexica
for questions and queries, D
Q
and D
R
respec-
tively, as well as all parameters for g and h.
Alternatively, word embeddings inD
Q
could be
initialised with representations learned separately,
for instance with a neural language model or a
similar system (Mikolov et al., 2010; Turian et al.,
2010; Collobert et al., 2011, inter alia). Likewise,
the relation and object embeddings inD
R
could be
initialised with representations learned from dis-
tributed relation extraction schemas such as that
of Riedel et al. (2013).
Having learned representations for queries in
D
R
as well as function g, the second training phase
of the model uses a new parallel corpus consisting
of pairs ?g(Q), R? ? C
?
to train the CNLM as pre-
sented in ?3.3.
The two training steps can be applied iteratively,
and further, it is trivial to modify the learning
procedure to use composition function h as an-
other input for the CNLM training phrase in an
autoencoder-like setup.
4 Experimental Requirements and
Further Work
The particular training procedure for the model
described in this paper requires aligned ques-
tion/knowledgebase query pairs. There exist some
small corpora that could be used for this task
(Zelle and Mooney, 1996; Cai and Yates, 2013). In
order to scale training beyond these small corpora,
we hypothesise that larger amounts of (potentially
noisy) training data could be obtained using a
boot-strapping technique similar to Kwiatkowski
et al. (2013).
To evaluate this model, we will follow the ex-
perimental setup of Kwiatkowski et al. (2013).
With the provisio that the model can generate
freebase queries correctly, further work will seek
to determine whether this architecture can gener-
ate other structured formal language expressions,
such as lambda expressions for use in textual en-
tailement tasks.
Acknowledgements
This work was supported by a Xerox Foundation
Award, EPSRC grants number EP/I03808X/1 and
EP/K036580/1, and the Canadian Institute for Ad-
vanced Research (CIFAR) Program on Adaptive
Perception and Neural Computation.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Sofia, Bulgaria,
August. Association for Computational Linguistics.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of the 2nd International
Conference on Learning Representations, Banff,
Canada, April.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), Baltimore, USA,
June. Association for Computational Linguistics.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), Baltimore, USA, June. Association
for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013a. Re-
current continuous translation models. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing (EMNLP), Seattle,
USA. Association for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013b. Re-
current convolutional neural networks for discourse
compositionality. arXiv preprint arXiv:1306.3584.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
26
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545?1556, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2014. Learning multilingual word representa-
tions using a bag-of-words autoencoder. CoRR,
abs/1401.1803.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 590?599, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke S.
Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In Proceedings of the 29th Inter-
national Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
?13), June.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of EMNLP-CoNLL, pages 1201?1211.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL, Stroudsburg, PA, USA.
Wen-Tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning Discrimina-
tive Projections for Text Similarity Measures. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?11,
pages 247?256, Stroudsburg, PA, USA. Association
for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050?1055.
27
