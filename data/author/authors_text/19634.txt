Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 731?740, Dublin, Ireland, August 23-29 2014.
Low-Dimensional Manifold Distributional Semantic Models
Georgia Athanasopoulou
School of Electronic &
Computer Engineering
T.U.C. Chania, Greece
gathanasopoulou@isc.tuc.gr
Elias Iosif
Athena Research and
Innovation Center,
15125 Maroussi, Greece
iosif.elias@gmail.com
Alexandros Potamianos
School of Electrical &
Computer Engineering
N.T.U.A, Athens, Greece
apotam@gmail.com
Abstract
Motivated by evidence in psycholinguistics and cognition, we propose a hierarchical distributed
semantic model (DSM) that consists of low-dimensional manifolds built on semantic neighbor-
hoods. Each semantic neighborhood is sparsely encoded and mapped into a low-dimensional
space. Global operations are decomposed into local operations in multiple sub-spaces; results
from these local operations are fused to come up with semantic relatedness estimates. Manifold
DSM are constructed starting from a pairwise word-level semantic similarity matrix. The pro-
posed model is evaluated on semantic similarity estimation task significantly improving on the
state-of-the-art.
1 Introduction
The estimation of semantic similarity between words, sentences and documents is a fundamental problem
for many research disciplines including computational linguistics (Malandrakis et al., 2011), semantic
web (Corby et al., 2006), cognitive science and artificial intelligence (Resnik, 2011; Budanitsky and
Hirst, 2001). In this paper, we study the geometrical structure of the lexical space in order to extract se-
mantic relations among words. In (Karlgren et al., 2008), the high-dimensional lexical space is assumed
to consist of manifolds of very low dimensionality that are embedded in this high dimensional space.
The manifold hypothesis is compatible with evidence from psycholinguistics and cognitive science. In
(Tenenbaum et al., 2011), the question ?How does the mind work?? is answered as follows: cognitive
organization is based on domains with similar items connected to each other and lexical information
is represented hierarchically, i.e., a domain that consists of similar lexical entries may be represented
by a more abstract concept. An example of such a domain is {blue, red, yellow, pink, ...} that corre-
sponds by the concept of color. An inspiring analysis about the geometry of thought, as well as cognitive
evidence for the low-dimensional manifold assumption can be found in (Gardenfors, 2000), e.g., the
domain of color is argued to be cognitively represented as an one-dimensional manifold. Following the
low-dimensional manifold hypothesis we propose to extend distributional semantic models (DSMs) into
a hierarchical model of domains (or concepts) that contain semantically similar words. Global operations
on the lexical space are decomposed into local operations on the low-dimensional domain sub-manifolds.
Our goal is to exploit this hierarchical low-rank model to estimate relations between words, such as se-
mantic similarity.
There has been much research interest on devising data-driven approaches for estimating semantic
similarity between words. DSMs (Baroni and Lenci, 2010) are based on the distributional hypothesis
of meaning (Harris, 1954) assuming that semantic similarity between words is a function of the overlap
of their linguistic contexts. DSMs are typically constructed from co-occurrence statistics of word tuples
that are extracted on existing corpora or on corpora specifically harvested from the web. In (Iosif and
Potamianos, 2013), general-purpose, language-agnostic algorithms were proposed for estimating seman-
tic similarity using no linguistic resources other than a corpus created via web queries. The key idea of
this work was the construction of semantic networks and semantic neighborhoods that capture smooth
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are
added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
731
co-occurrence and context similarity statistics. The majority of DSMs adopt high-dimensional represen-
tations, while the underlying space geometry is not explicitly taken into consideration during the design
of algorithms aimed for performing several semantic tasks.
We propose the construction of a low-dimensional manifold DSM consting of four steps: 1) identify
the domains that correspond to the low-dimensional manifolds, 2) run the dimensionality reduction al-
gorithm for each domain, 3) construct a DSM for each domain, and 4) combine the manifold DSMs to
come up with global measures of lexical relations. A variety of algorithms can be found in the literature
for projecting a set of tokens into low dimensional sub-spaces, given a token similarity or dissimilarity
matrix. Depending on the nature of the dataset, these projection algorithms may or may not preserve
the local geometries of the original dataset. Most dimensionality reduction algorithms make the implicit
assumption that the underlying space is metric, e.g., Multidimensional Scaling (MDS) (Torgerson, 1952)
or Principal Component Analysis (PCA) (Jolliffe, 2005) or the ones using non-negative matrix factor-
ization (Tsuge et al., 2001) and typically fail to capture the geometry of manifolds embedded in high
dimensional spaces. A variety of dimensionality reduction algorithms have been developed that respect
the local geometry. Some examples are the Isomap algorithm (Tenenbaum et al., 2000) that performs
the projection based on a weighted neighborhood graph, Local Linear Embedings (LLE) (Roweis and
Saul, 2000) that assigns neighbors to each data point, Random Projections (Baraniuk and Wakin, 2009),
(Li et al., 2006) that preserves the manifold geometry by executing random linear projections and oth-
ers (Hessian Eigenmaps (HLLE) (Donoho and Grimes, 2003); Maximum Variance Unfolding (MVU)
(Wang, 2011)). The manifold hypothesis has also been studied by the representation learning commu-
nity where the local geometry is disentangled from the global geometry mainly by using neighborhood
graphs (Weston et al., 2012) or coding schemes (Yu et al., 2009). For a review see (Bengio et al., 2013).
A fundamental problem with all aforementioned methods when applied to lexical semantic spaces is
that they do not account for ambiguous tokens, i.e., word senses. The main assumption of dimensionality
reduction and manifold unfolding algorithms is that each token (word) belongs to a single sub-manifold.
This in fact is not true for polysemous words, for example the word ?green? could belong both to the
domain colors, as well as to the domain plants. In essence, lexical semantic spaces are manifolds that
have singularities: the manifold collapses in the neighborhood of polysemous words that can be thought
of semantic black holes that can instantaneously transfer you from one domain to another. Our proposed
solution to this problem is to allow words to live in multiple sub-manifolds.
The algorithms proposed in this paper build on recent research work on distributional semantic models
and manifold representational learning. Manifold DSMs can be trained directly from a corpus and do
not require a-priori knowledge or any human-annotated resources (just like DSMs). We show that the
proposed low-dimensional, sparse and hierarchical manifold representation significantly improves on the
state-of-the-art for the problem of semantic similarity estimation.
2 Metrics of Semantic Similarity
Semantic similarity metrics can be broadly divided into the following types: (i) metrics that rely on
knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge
source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and
Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954). DSMs can be distin-
guished into (i) unstructured: use bag-of-words model (Iosif and Potamianos, 2010) and (ii) structured:
exploitation of syntactic relationships between words (Grefenstette, 1994; Baroni and Lenci, 2010). The
vector space model (VSM) constitutes the main implementation for both unstructured and structured
DSMs. Cosine similarity constitutes a measurement of word similarity that is widely used on top of
the VSM. The similarity between two words is estimated as the cosine of their respective vectors whose
elements correspond to corpus-based co-occurrence statistics. In essence, the similarity between words
is computed via second-order co-occurrences.
Direct (i.e., first-order) co-occurrences can be also used for the estimation of semantic similarity (Bol-
legala et al., 2007; Gracia et al., 2006). The exploitation of first-order co-occurrence statistics constitutes
the simplest form of unstructured DSMs. A key parameter for such models is the definition of the context
in which the words of interest co-occur: from entire documents (Bollegala et al., 2007) to paragraphs
732
(V?eronis, 2004) and sentences (Iosif and Potamianos, 2013). The effect of co-occurrence context for
the task of similarity computation between nouns is discussed in (Iosif and Potamianos, 2013). The
underlying assumption is that two words that co-occur in a specified context are semantically related.
3 Collapsed Manifold Hypothesis, Low-Dimensionality and Sparsity
The intuition behind this work is that although the lexical semantic space proper is high-dimensional, it
is organized in such a way that interesting semantic relations can be exported from manifolds of much
lower dimensionality embedded in this high dimensional space (Karlgren et al., 2008). We assume that
(at least some of) these sub-manifolds contain semantically similar words (or word senses). For example,
a potential sub-manifold in the lexical space could be the one that contains the colors (e.g., red, blue,
green). But in fact many words, such as book, green, fruit, are expected to belong simultaneously in
semantically different manifolds because they have multiple meanings.
A simple way to bootstrap the manifold recreation process is to build a domain around each word,
i.e., the semantic neighborhood of each word defines a domain. For example, in Figure 1 we show
the semantic neighborhood of fruit. The connections between words indicate high semantic similarity,
i.e., this is a pruned semantic similarity graph of all words in the semantic neighborhood of the word
?fruit?. It is clear from this example that in a typical neighborhood there exist word pairs that should be
native
genus
b
shurb
b
plant
flowering
b
tree
b
species
b
b
garden
b
soil
b
animal
b
water
b
seed
flower
b
b
fruit
b
vegetable
b
apple
juice
daiquiri
b
orange
b
drink
b
zest
b
lemon
sugar
salt
b
flour
b
cream
b
butter
b
b
b
milk
b
corn
b
honey
b
tomato
b
b b
b
Figure 1: Visualization of the semantic neighborhood of the word ?fruit?.
?connected? to each other because they have close semantic relation, like {flower, plant} and others that
should not be ?connected? because they are semantically apart, like {garden, salt}. A sparse encoding of
the semantic similarity relations in a neighborhood is needed in order to achieve (via multi-dimensional
scaling) a parsimonious representation with good geometric properties
1
.
The graph connectivity or sparseness matrix identifies the word pairs that should be encoded in a
neighborhood is defined as
?
S ? {0, 1}
n?n
, where value
?
S(i, j) = 1 indicates that the i
th
, j
th
word
pair is encoded, while
?
S(i, j) = 0 indicates that the pair is ignored (n is the number of words and
i, j = 1, .., n in the neighborhood). We define the degree of sparseness of matrix
?
S as the percentage of
0?s in the matrix.
4 Dimensionality Reduction
In this section, the Sparse Projection (SP) algorithm is described (see also Algorithm 1). SP is the core
algorithm for constructing manifold DSMs presented in Section 5. SP is a dimensionality reduction
algorithm that projects a set of n words into a vector space of d dimensions. The input to the algorithm
is a dissimilarity or semantic distance matrix P ? R
n?n
, where element P(i, j) encodes the degree
of dissimilarity between words w
i
and w
j
. The output of SP are the d-dimensional coordinate vectors
of the n projected words that form a matrix X ? R
n?d
. Each row x
i
? R
1?d
of matrix X ? R
n?d
corresponds to the coordinates of the i
th
word w
i
. Once X is estimated the dissimilarity matrix is
recomputed and updated to new values, as discussed next. Each paragraph that follows corresponds to a
module in Algorithm 1.
1
Compare for example with Isomap (Tenenbaum et al., 2000) were a short- and long-distance metric is used. When using
sparse encoding the long-distance metric is set to a very large fixed number (similarity set to 0). In both cases, the underlying
manifold is unfolded and low-dimensional representation with (close to) metric properties are discovered.
733
Semantic Distance Re-estimation: Given the matrix X ? R
n?d
containing the vector projections of
words in the d-dimensional space, the dissimilarity matrix is re-estimated using the Euclidean distance
2
.
Let
?
P ? R
n?n
be the matrix with the new dissimilarity scores then the new dissimilarity score between
words w
i
and w
j
is simply:
?
P(i, j) = ?x
i
? x
j
?
2
, where x
i
, x
j
are the vectors corresponding to words
w
i
, w
j
respectively, i, j = 1, .., n and ?.?
2
is the Euclidean norm.
Connectivity Graph and Sparsity: As discussed in Section 3, given a set of words only a small
subset of lexical relations should be explicitly encoded between pairs of these words. Therefore,
the SP algorithm should only take into account strongly related word pairs and ignore the rest. This
is the main difference between our approach compared to the generic MDS algorithm proposed in
(Torgerson, 1952). In order to apply the sparseness constraint, we first construct the connectivity
matrix
?
S ? {0, 1}
n?n
. Word pairs (w
i
, w
j
) with small similarity values (or equivalently large semantic
distance) are penalized: zero values are assigned to their corresponding position (i, j) in
?
S matrix. In
essence, the matrix
?
S is obtained by hard {0, 1} thresholding on the dissimilarity matrix P: all values
that are under a threshold are set to 0, while all values equal or greater to the threshold are set to 1.
Let n be the number of words under investigation, then the number of word pairs is p =
n?(n?1)
2
. The
degree of sparseness is defined as the number of unordered word pairs (w
i
, w
j
), i 6= j where
?
S(i, j) = 0
normalized over the total number of pairs p
3
.
Error Criterion: The algorithm employs a local and a global error criterion defined as follows:
1. The local error corresponds to the projection error for each individual word w
i
e ? R
n?1
, where
i = 1...n and is defined as the sum of the dissimilarity matrix errors before and after projection
computed only for the words that are ?connected? to w
i
, as follows:
e
i
=
n
?
j=1
?
S(i, j) ?
(
?
P(i, j)?P(i, j)
)
2
(1)
2. The global error of the projection is simply the sum over local errors for all words: e
tot
=
?
n
i=1
e
i
Algorithm 1 Sparse projection (SP)
Require: v // Vocabulary: vector of n words
Require: P // n?n dissimilarity matrix
1:
?
S? ComputeConnectivityMatrix(S)
2: for each word w
i
? v do
3: X
i
? RandomInitialization(X
i
)
4: end for
5: k = 0 // Iteration counter: initialization
6: e
k
tot
= inf // Global error: initialization
7: repeat
8: k = k + 1
9: for each word w
i
? v do
10: for each direction z do
11: X?MoveWordToDirection(w
i
, z)
12: e
z
i
? ComputeLocalError(
?
S,P,X,i)
13: end for
14: z?
i
? FindDirectionOfMinLocalError(e
z
i
)
15: X = MoveWordToDirection(w
i
, z?
i
)
16: end for
17: e
k
tot
? UpdateGlobalError(
?
S,P,X)
18: until e
k?1
tot
< e
k
tot
// Stopping condition
19:
?
P? SemanticDistanceReestimation(X)
20:
?
P? SparseDistanceNormalizedRanges(
?
P,
?
S)
21: return X // n?d matrix with coordinates;
22: return
?
S // n?n matrix with connections;
23: return
?
P // n?n updated dissimilarity matrix;
24: return
?
P // n?n sparse-normalized distances;
Random Walk SP: In function MoveWordToDirection(?) of Algorithm 1, the pseudo-variable direction
z refers to a standard set of perturbations of each word in the d-dimensional space. For example, if the
dimension of the projection is d = 2 then the coordinates of each word are modeled as (k
1
, k
2
), where
k
1
, k
2
? R. A potential set of perturbations are the following: (k
1
, k
2
+ s), (k
1
, k
2
? s), (k
1
+ s, k
2
)
and (k
1
? s, k
2
), where s is the perturbation step parameter of the algorithm. For coordinates systems
normalized in [0, 1]
d
we chose a value of s equal to 0.1. Good convergence properties to global maxima
have been experimentally shown for this algorithm for multiple runs on (noisy) randomly generated data.
2
Other metrics, e.g., cosine similarity, have also been tested out but results are not shown here due to lack of space. Euclidean
distance performed somewhat better that cosine similarity for the semantic similarity estimation task.
3
The SP algorithm with 0% degree of sparseness is equivalent to the MDS algorithm.
734
Sparse Semantic Distance Normalized Ranges: This function normalizes all the distance scores of
?
P
in a range of values, [0 r
1
], where r
1
? R
+
is an arbitrary positive constant and also it imposes the
sparsity constraint as follows: if
?
S(i, j) = 0 then
?
P(i, j) = r
1
. If
?
S(i, j) = 1 then
?
P(i, j) = r
2
?
?
P(i,j)
r
3
,
where r
3
is the maximum distance over all ?connected? pairs, i.e. r
3
, max{
?
P 
?
S}, with  denoting
the Hadamard product, and r
2
? R
+
can be either equal to r
1
or slightly smaller than r
1
. The assignment
of r
2
< r
1
aims to differentiate the ?unconnected? pairs from the ?connected? but dissimilar ones
4
.
5 Low-Dimensional Manifold DSMs
The end-to-end low-dimensional manifold DSM (LDMS) system is depicted in Figure 2. Note that
v
1
, v
2
, ..., v
|V|
? V are the domains or sub-manifolds of the LDMS, for each domain v
i
a separate DSM
is built. V is the set of domains (concept vocabulary) and |V| denotes to the cardinality of V. The input
Figure 2: LDMS system.
to LDMS is a (global) similarity matrix S ? R
n?n
, where n is the total number of tokens (words) in
the LDMS model. Note that S can be estimated using any of the baseline semantic similarity metrics
5
presented in Section 2. Since the SP algorithm uses as input a dissimilarity or semantic distance matrix,
the pairwise word similarity matrix S ? R
n?n
is transformed to a semantic distance (or dissimilarity)
matrix P ? R
n?n
as: P(i, j) = c
1
? e
?c
2
?S(i,j)
where c
1
, c
2
? R are constants and the i, j indexes run
from 1 to n. In this work, we used c
1
=c
2
=20. The transformation defined by (5) was selected in order
to non-linearly scale and increase the relative distance of dissimilar words compared to similar ones
6
.
The steps followed by the LDMS system are the following:
1. Domain Selection: The domains v
1
, v
2
, ..., v
|V|
are created as follows: for each word w
i
in our
model we create a corresponding domain v
i
that consists of all the words that are semantically
similar to w
i
, i.e., the ith domain is the semantic neighborhood of word w
i
. Thus in our model
the vocabulary size is equal to the domain set cardinality, i.e., n = |V|. Domain v
i
is created by
selecting the top N most semantically similar words to w
i
based on the (global) similarity matrix
S ? R
n?n
. We have experimented with various domain sizes N ranging between 20 and 200
neighbors; note that each word in the LDMS may belong to multiple domains.
2. Sparse Projections on Domains: Following the selection of domain v
i
? V the (local) dissimilarity
matrix for each domain P
v
i
? R
N?N
is defined as a submatrix of P ? R
n?n
. Then, the SP
algorithm is applied to each domain separately, resulting in i = 1, .., |V| re-estimated bounded
semantic distance matrices
?
P
v
i
.
3. Fusion: To reach a decision on the strength of the semantic relation between words w
i
and w
j
the
semantic distance matrices from each domain
?
P
v
i
must be combined. Only domains were both
words w
i
and w
j
appear are relevant in this fusion process. This procedure is described next.
4
We experimented with various values for r
1
and r
2
achieving comparable performance; we selected r
2
? 0.9r
1
that had
slightly better performance. The value of r
1
can be chosen arbitrary, the results reported here were obtained for r
1
= 20 and
r
2
= 18.
5
Here, the Google-based Semantic Relatedness was employed using a corpus of web-harvested document snippets.
6
Similar nonlinear scaling function from similarity to distance can be found in the literature, e.g., (Borg, 2005)
735
5.1 Fusion
Motivation: Given a set of words L = {w
1
, w
2
, ...w
n
} we assume that their corresponding set of word
senses
7
is M = {s
11
, s
12
, .., s
1n
1
, .., .., s
n1
, s
n2
, .., s
nn
n
}. The set of senses is defined as M = ?
n
i=1
M
i
,
where M
i
= {s
i1
, s
i2
, ..., s
in
i
} is the set of senses for word w
i
. Let S(.) be a metric of semantic similar-
ity, e.g., the metric defined in Section 2, which is symmetric, i.e., S(x, y) ? S(y, x). The notations S
w
(.)
and S
s
(.) are used in order to distinguish the similarity at word and sense level, respectively. According to
the maximum sense similarity assumption (Resnik, 1995), the similarity between w
i
and w
j
, S
w
(w
i
, w
j
),
is defined as the pairwise maximum similarity between their corresponding senses S
s
(s
ik
, s
jl
):
S
w
(w
i
, w
j
) ? S
s
(s
ik
, s
jl
), where (k, l) = argmax
(p?M
i
,r?M
j
)
S
s
(s
ip
, s
jr
).
Note that the maximum pairwise similarity metric (or equivalently the minimum pairwise distance
metric) is also known as the ?common sense? set similarity (or distance) employed by human cognition
when evaluating the similarity (or distance) between two sets.
Fusion of local dissimilarity scores: Next we describe a domain fusion model that follows the min-
imum pairwise distance (dissimilarity) principle motivated by human cognition. The steps for the re-
computation of the (global) dissimilarity between words w
i
and w
j
are:
1. Search for all the domains where w
i
and w
j
co-exist.
2. Let U ? V be the subset of domains from the previous step. The distances between words w
i
and
w
j
are retrieved from domain dissimilarity matrices
?
P
u
for all u ? U . The distances are stored into
vector d ? R
|U |?1
.
3. Motivated by the maximum sense similarity assumption (see above) the dissimilarity between w
i
and w
j
is defined as
8
:
?
P(i, j) = min
k=1..|U |
{d
k
} (2)
4. If words w
i
and w
j
do not co-exist in any domain then r
1
is assigned as their dissimilarity score,
where r
1
is the upper bound of
?
P
u
matrices as defined in the previous section.
For example, let one pair of words (w
1
, w
2
) co-exists in |U | = 3 different domains with corresponding
local distances d = [9 20 11] then the global distance of (w
1
, w
2
) is 9.
6 Evaluation
In this section, we evaluate the performance of the proposed approach with respect to the task of simi-
larity judgment between nouns. Results are reported with respect to several domain/neighborhood sizes,
sparse percentages and domain dimensions.
The performance of similarity metrics were evaluated against human ratings from three standard
datasets of noun pairs, namely WS353 (Finkelstein et al., 2001), RG (Rubenstein and Goodenough,
1965) MC (Miller and Charles, 1991). The first and the second datasets consist of the subset of 272 and
57 pairs, respectively, that are also included in SemCor3
9
corpus, while the third dataset consists of 28
noun pairs. The Pearson?s correlation coefficient was selected as evaluation metric to compare estimated
similarities against the ground truth.
The similarity matrix computed using the Google-based Semantic Relatedness (Gracia et al., 2006)
was used as baseline, as well as to bootstrap the LDMS global similarity matrix S, for a list of 8752 nouns
extracted from the SemCor3 corpus
10
. The performance of the proposed LDMS approach is presented
in Table 1. In addition, the performance of other unsupervised similarity estimation algorithms are
reported for comparison purposes: 1) SEMNET is an alternative implementation of unstructured DSMs
based on the idea of semantic neighborhoods and networks (Iosif and Potamianos, 2013) 2) WikiRelate!
includes various taxonomy-based metrics that are typically applied to the WordNet hierarchy; the basic
7
This is a simplification. In reality, some of the word senses will be the same, so strictly speaking this is not a set definition.
8
Other fusion methods have also been evaluated, e.g., (weighted) average. Results are omitted here due to lack of space.
Minimum pairwise distance fusion outperformed other fusion schemes.
9
http://www.cse.unt.edu/
?
rada/downloads.html
10
The baseline similarity matrix and the 8752 nouns are public available in:
http://www.telecom.tuc.gr/
?
iosife/downloads.html
736
idea behind WikiRelate! is to adapt these metrics to a hierarchy extracted from the links between the
pages of the English Wikipedia (Strube and Ponzetto, 2006) . 3) TypeDM is a structured DSM (Baroni
and Lenci, 2010), 4) AAHKPS1 constitutes an unstructured paradigm of DSM development using four
billion web documents that were acquired via crawling (Agirre et al., 2009), 5) Moreover, two well-
established dimensionality reduction algorithms (Isomap and LLE) that support the manifold hypothesis,
were applied to the task of semantic similarity computation
11
. LDMS, Isomap and LLE were given as
input the matrix P ? R
n?n
, where n = 8752 is the number of words in our models. Isomap and LLE
used dimensionality reduction down to d = 5 and neighborhood size equal to N = 120. SEMNET was
run for neighborhood size equal to N = 100. While LDMS run for dimensionality down to d = 5,
domain/neighborhood size equal to N = 140 and degree of sparseness 90%. The proposed LDMS
system surpassed the performance of the baseline system for all three datasets, as well as the performance
of the other corpus-based approaches for the WS353 and MC datasets. The dimensionality reduction
algorithms (Isomap - LLE) are shown to perform poorly for this particular task.
Datasets Algorithm
Baseline SEMNET WikiRelate! TypeDM AAHKPS1 Isomap LLE LDMS
WS353 0.61 0.64 0.48 - - 0.14 0.04 0.69
RG 0.81 0.87 0.53 0.82 - 0.04 0 0.86
MC 0.85 0.91 0.45 - 0.89 -0.04 -0.04 0.94
Table 1: Performance of various algorithms for the task of similarity judgment.
The performance (Pearson correlation) of the LDMS approach is shown in Figures 3a, 3b and 4a as
a function of neighborhood size and degree of sparseness. Results are presented for all three datasets:
WS353, MC, and RG. The baseline performance is also plotted (dotted line). For all three datasets,
we see a clear relationship between neighborhood size, degree of sparseness and performance. Sparse
representations achieve peak performance for larger neighborhood sizes. High degree of sparseness
between 80 and 90% achieves the best results for domain/neighborhood sizes between 100 and 140. The
figures show that there is potential for even better performance by fine-tuning the LDMS parameters.
The performance of LDMS is shown in Figure 4b as a function of the projection dimension d. The de-
gree of sparseness is fixed at 80% and the domain/neighborhood size is equal to 100 for all experiments.
It is observed that the performance for all three datasets remains relatively constant when at least d = 3
is used. In fact results are slightly better for d = 3 than for higher dimensions but the differences in
performance are not significant. The results suggest that even a 3D sub-space is adequate for accurately
representing the semantics of each underlying domain.
20 40 60 80 100 120 140 160 180 2000.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.705
Neighborhood size
Corr
elati
on
 
 Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 0.947
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
Figure 3: Performance as a function of domain size N and sparseness percentage for the (a) WS353
dataset and (b) MC dataset.
11
LDMS is not directly comparable with Isomap-LLE algorithms because it represents only the domains in low-dimensional
spaces and not the whole dataset.
737
20 40 60 80 100 120 140 160 180 2000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.865
Neighborhood size
Corr
elati
on
 
 
Baseline95% Sparse90% Sparse80% Sparse40% Sparse0% Sparse
2 3 4 5 6 7 80.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Dimensions
Corr
elati
on
 
 MCRGWS353
Figure 4: Performance for the (a) RG dataset as a function of domain size N and sparseness percentage
and (b) WS353, MC, RG datasets as a function of projection dimension d.
7 Conclusions
In this work, we proposed a novel, hierarchical DSM that was applied to semantic relation estimation
task obtaining very good results. The proposed representation consists of low-dimensional manifolds
that are derived from sparse projections of semantic neighborhoods. The core idea of low dimensional
subspaces was motivated by cognitive models of conceptual spaces. The validity of this motivation was
experimentally verified via the estimation of semantic similarity between nouns. The proposed approach
was found to be (at least) competitive with other state-of-the-art DSM approaches that adopt flat feature
representations and do not explicitly include the sparsity and dimensionality as a key design parameter.
The poor performance of Isomap and LLE can be attributed to the nature of the specific application,
i.e., word semantics. A key characteristic of this application is the ambiguity of word senses. These
algorithms assume only one sense for each word (i.e., a word is represented as a single point in a high-
dimensional space). Although the disambiguation task is not explicitly addressed, LDMS approach
handles the ambiguity of words by isolating each word?s senses in different domains.
Our initial intuition regarding the semantic fragmentation of lexical neighborhoods due to singularities
introduced by word senses was supported by the high performance when large (i.e., 80% - 90%) degree of
sparseness was imposed. The hypothesis of low-dimensional representation was validated by the finding
that as little as three dimensions are adequate for representing domain/neighborhood semantics. It was
also observed that the parameters of the LDMS model, i.e., number of dimensions, neighborhoodsize
and degree of sparseness, are interrelated: very sparse projections achieve best results with very low
dimensionality when large neighborhood sizes are used.
This is only a first step toward using ensembles of low-dimensional DSMs for semantic relation esti-
mation. As future work we plan to further investigate the creation of domains based on more complex
geometric properties of the underlying space (Kreyszig, 2007). A more formal investigation of the re-
lation between sparseness, dimensionality and performance is also needed. Finally, creating multi-level
hierarchical representations that are consistent with cognitive organization is an important challenge that
can further improve manifold DSM performance.
Acknowledgments
This work has been partially funded by two projects supported by the EU Seventh Framework Pro-
gramme (FP7): 1) PortDial, grant number 296170 and 2) SpeDial, grant number 611396.
738
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa. 2009. A study on similarity and relatedness
using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies, pages
19?27. Association for Computational Linguistics.
R. G Baraniuk and M. B Wakin. 2009. Random projections of smooth manifolds. Foundations of computational
mathematics, 9(1):51?77.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
Y. Bengio, A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Measuring semantic similarity between words using web search
engines. In Proc. of International Conference on World Wide Web, pages 757?766, Banff, Alberta, Canada.
Ingwer Borg. 2005. Modern multidimensional scaling: Theory and applications. Springer.
A. Budanitsky and G. Hirst. 2001. Semantic distance in wordnet: An experimental, application-oriented evalua-
tion of five measures. In Workshop on WordNet and Other Lexical Resources.
O. Corby, R. Dieng-Kuntz, F. Gandon, and C. Faron-Zucker. 2006. Searching the semantic web: Approximate
query processing based on ontologies. Intelligent Systems, IEEE, 21(1):20?27.
D. L Donoho and C. Grimes. 2003. Hessian eigenmaps: Locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591?5596.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in
context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages
406?414. ACM.
P. Gardenfors. 2000. Conceptual spaces: The geometry of thought. Cambridge, Massachusetts: USA. ISBN,
262071991.
J. Gracia, R. Trillo, M. Espinoza, and E. Mena. 2006. Querying the web: A multiontology disambiguation method.
In Proc. of International Conference on Web Engineering, pages 241?248, Palo Alto, California, USA.
G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell,
MA, USA.
Z. Harris. 1954. Distributional structure. Word, 10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised semantic similarity computation between terms using web
documents. Knowledge and Data Engineering, IEEE Transactions on, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2013. Similarity computation using semantic networks created from web-harvested
data. Natural Language Engineering (DOI: 10.1017/S1351324913000144).
I. Jolliffe. 2005. Principal component analysis. Wiley Online Library.
J. Karlgren, A. Holst, and M. Sahlgren. 2008. Filaments of meaning in word space. In Advances in Information
Retrieval, pages 531?538. Springer.
E. Kreyszig. 2007. Introductory functional analysis with applications. Wiley. com.
P. Li, T. J Hastie, and K. W Church. 2006. Very sparse random projections. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 287?296. ACM.
N. Malandrakis, A. Potamianos, E. Iosif, and S. S Narayanan. 2011. Kernel models for affective lexicon creation.
In INTERSPEECH, pages 2977?2980.
G. A Miller and W. G Charles. 1991. Contextual correlates of semantic similarity. Language and cognitive
processes, 6(1):1?28.
P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxanomy. In Proc. of International
Joint Conference for Artificial Intelligence, pages 448?453.
739
P. Resnik. 2011. Semantic similarity in a taxonomy: An information-based measure and its application to prob-
lems of ambiguity in natural language. arXiv preprint arXiv:1105.5444.
S. T Roweis and L. K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323?2326.
H. Rubenstein and J. B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM,
8(10):627?633.
Michael Strube and Simone Paolo Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia.
In AAAI, pages 1419?1424.
J. B Tenenbaum, V. De Silva, and J. C Langford. 2000. A global geometric framework for nonlinear dimensional-
ity reduction. Science, 290(5500):2319?2323.
J. B Tenenbaum, C. Kemp, T. L Griffiths, and N. D Goodman. 2011. How to grow a mind: Statistics, structure,
and abstraction. science, 331(6022):1279?1285.
Warren S Torgerson. 1952. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401?419.
S. Tsuge, M. Shishibori, S. Kuroiwa, and K. Kita. 2001. Dimensionality reduction using non-negative matrix
factorization for information retrieval. In Systems, Man, and Cybernetics, 2001 IEEE International Conference
on, volume 2, pages 960?965 vol.2.
J. V?eronis. 2004. Hyperlex: Lexical cartography for information retrieval. Computer Speech and Language,
18(3):223?252.
Jianzhong Wang. 2011. Maximum variance unfolding. In Geometric Structure of High-Dimensional Data and
Dimensionality Reduction, pages 181?202. Springer.
J. Weston, F. Ratle, H. Mobahi, and R. Collobert. 2012. Deep learning via semi-supervised embedding. In Neural
Networks: Tricks of the Trade, pages 639?655. Springer.
K. Yu, T. Zhang, and Y. Gong. 2009. Nonlinear learning using local coordinate coding. In Advances in Neural
Information Processing Systems, pages 2223?2231.
740
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 565?570,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DeepPurple: Estimating Sentence Semantic Similarity using
N-gram Regression Models and Web Snippets
Nikos Malandrakis, Elias Iosif, Alexandros Potamianos
Department of ECE, Technical University of Crete, 73100 Chania, Greece
[nmalandrakis,iosife,potam]@telecom.tuc.gr
Abstract
We estimate the semantic similarity between
two sentences using regression models with
features: 1) n-gram hit rates (lexical matches)
between sentences, 2) lexical semantic sim-
ilarity between non-matching words, and 3)
sentence length. Lexical semantic similarity is
computed via co-occurrence counts on a cor-
pus harvested from the web using a modified
mutual information metric. State-of-the-art re-
sults are obtained for semantic similarity com-
putation at the word level, however, the fusion
of this information at the sentence level pro-
vides only moderate improvement on Task 6
of SemEval?12. Despite the simple features
used, regression models provide good perfor-
mance, especially for shorter sentences, reach-
ing correlation of 0.62 on the SemEval test set.
1 Introduction
Recently, there has been significant research activ-
ity on the area of semantic similarity estimation
motivated both by abundance of relevant web data
and linguistic resources for this task. Algorithms
for computing semantic textual similarity (STS) are
relevant for a variety of applications, including in-
formation extraction (Szpektor and Dagan, 2008),
question answering (Harabagiu and Hickl, 2006)
and machine translation (Mirkin et al, 2009). Word-
or term-level STS (a special case of sentence level
STS) has also been successfully applied to the prob-
lem of grammar induction (Meng and Siu, 2002)
and affective text categorization (Malandrakis et al,
2011). In this work, we built on previous research
on word-level semantic similarity estimation to de-
sign and implement a system for sentence-level STS
for Task6 of the SemEval?12 campaign.
Semantic similarity between words can be re-
garded as the graded semantic equivalence at the
lexeme level and is tightly related with the tasks of
word sense discovery and disambiguation (Agirre
and Edmonds, 2007). Metrics of word semantic sim-
ilarity can be divided into: (i) knowledge-based met-
rics (Miller, 1990; Budanitsky and Hirst, 2006) and
(ii) corpus-based metrics (Baroni and Lenci, 2010;
Iosif and Potamianos, 2010).
When more complex structures, such as phrases
and sentences, are considered, it is much harder
to estimate semantic equivalence due to the non-
compositional nature of sentence-level semantics
and the exponential explosion of possible interpre-
tations. STS is closely related to the problems of
paraphrasing, which is bidirectional and based on
semantic equivalence (Madnani and Dorr, 2010) and
textual entailment, which is directional and based
on relations between semantics (Dagan et al, 2006).
Related methods incorporate measurements of sim-
ilarity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Measures
from machine translation evaluation are often used
to evaluate lexical level approaches (Finch et al,
2005; Perez and Alfonseca, 2005), including BLEU
(Papineni et al, 2002), a metric based on word n-
gram hit rates.
Motivated by BLEU, we use n-gram hit rates and
word-level semantic similarity scores as features in
565
a linear regression model to estimate sentence level
semantic similarity. We also propose sigmoid scal-
ing of similarity scores and sentence-length depen-
dent modeling. The models are evaluated on the Se-
mEval?12 sentence similarity task.
2 Semantic similarity between words
In this section, two different metrics of word simi-
larity are presented. The first is a language-agnostic,
corpus-based metric requiring no knowledge re-
sources, while the second metric relies on WordNet.
Corpus-based metric: Given a corpus, the se-
mantic similarity between two words, wi and wj ,
is estimated as their pointwise mutual information
(Church and Hanks, 1990): I(i, j) = log p?(i,j)p?(i)p?(j) ,
where p?(i) and p?(j) are the occurrence probabili-
ties of wi and wj , respectively, while the probability
of their co-occurrence is denoted by p?(i, j). These
probabilities are computed according to maximum
likelihood estimation. The assumption of this met-
ric is that co-occurrence implies semantic similarity.
During the past decade the web has been used for
estimating the required probabilities (Turney, 2001;
Bollegala et al, 2007), by querying web search en-
gines and retrieving the number of hits required
to estimate the frequency of individual words and
their co-occurrence. However, these approaches
have failed to obtain state-of-the-art results (Bolle-
gala et al, 2007), unless ?expensive? conjunctive
AND queries are used for harvesting a corpus and
then using this corpus to estimate similarity scores
(Iosif and Potamianos, 2010).
Recently, a scalable approach1 for harvesting a
corpus has been proposed where web snippets are
downloaded using individual queries for each word
(Iosif and Potamianos, 2012b). Semantic similar-
ity can then be estimated using the I(i, j) metric
and within-snippet word co-occurrence frequencies.
Under the maximum sense similarity assumption
(Resnik, 1995), it is relatively easy to show that a
(more) lexically-balanced corpus2 (as the one cre-
1The scalability of this approach has been demonstrated in
(Iosif and Potamianos, 2012b) for a 10K vocabulary, here we
extend it to the full 60K WordNet vocabulary.
2According to this assumption the semantic similarity of two
words can be estimated as the minimum pairwise similarity of
their senses. The gist of the argument is that although words
often co-occur with their closest senses, word occurrences cor-
ated above) can significantly reduce the semantic
similarity estimation error of the mutual information
metric I(i, j). This is also experimentally verified in
(Iosif and Potamianos, 2012c).
In addition, one can modify the mutual informa-
tion metric to further reduce estimation error (for
the theoretical foundation behind this see (Iosif and
Potamianos, 2012a)). Specifically, one may intro-
duce exponential weights ? in order to reduce the
contribution of p(i) and p(j) in the similarity met-
ric. The modified metric Ia(i, j), is defined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012b) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly out-
perform I(i, j) and to achieve state-of-the-art results
on standard semantic similarity datasets (Rubenstein
and Goodenough, 1965; Miller and Charles, 1998;
Finkelstein et al, 2002). For more details see (Iosif
and Potamianos, 2012a).
WordNet-based metrics: For comparison pur-
poses, we evaluated various similarity metrics on
the task of word similarity computation on three
standard datasets (same as above). The best re-
sults were obtained by the Vector metric (Patward-
han and Pedersen, 2006), which exploits the lexical
information that is included in the WordNet glosses.
This metric was incorporated to our proposed ap-
proach. All metrics were computed using the Word-
Net::Similarity module (Pedersen, 2005).
3 N-gram Regression Models
Inspired by BLEU (Papineni et al, 2002), we pro-
pose a simple regression model that combines evi-
dence from two sources: number of n-gram matches
and degree of similarity between non-matching
words between two sentences. In order to incorpo-
rate a word semantic similarity metric into BLEU,
we apply the following two-pass process: first lexi-
cal hits are identified and counted, and then the se-
mantic similarity between n-grams not matched dur-
respond to all senses, i.e., the denominator of I(i, j) is overes-
timated causing large underestimation error for similarities be-
tween polysemous words.
566
ing the first pass is estimated. All word similar-
ity metrics used are peak-to-peak normalized in the
[0,1] range, so they serve as a ?degree-of-match?.
The semantic similarity scores from word pairs are
summed together (just like n-gram hits) to obtain
a BLEU-like semantic similarity score. The main
problem here is one of alignment, since we need
to compare each non-matched n-gram from the hy-
pothesis with an n-gram from the reference. We
use a simple approach: we iterate on the hypoth-
esis n-grams, left-to-right, and compare each with
the most similar non-matched n-gram in the refer-
ence. This modification to BLEU is only applied
to 1-grams, since semantic similarity scores for bi-
grams (or higher) were not available.
Thus, our list of features are the hit rates obtained
by BLEU (for 1-, 2-, 3-, 4-grams) and the total se-
mantic similarity (SS) score for 1-grams3. These
features are then combined using a multiple linear
regression model:
D?L = a0 +
4
?
n=1
an Bn + a5 M1, (2)
where D?L is the estimated similarity, Bn is the
BLEU hit rate for n-grams, M1 is the total semantic
similarity score (SS) for non-matching 1-grams and
an are the trainable parameters of the model.
Motivated by evidence of cognitive scaling of
semantic similarity scores (Iosif and Potamianos,
2010), we propose the use of a sigmoid function to
scale DL sentence similarities. We have also ob-
served in the SemEval data that the way humans rate
sentence similarity is very much dependent on sen-
tence length4. To capture the effect of length and
cognitive scaling we propose next two modifications
to the linear regression model. The sigmoid fusion
scheme is described by the following equation:
D?S = a6D?L + a7D?L
[
1 + exp
(
a8 ? l
a9
)]
?1
, (3)
where we assume that sentence length l (average
3Note that the features are computed twice on each sentence
in a forward and backward fashion (where the word order is
reversed), and then averaged between the two runs.
4We speculate that shorter sentences are mostly compared at
the lexical level using the short-term memory language buffers,
while longer sentences tend to be compared at a higher cogni-
tive level, where the non-compositional nature of sentence se-
mantics dominate.
length for each sentence pair, in words) acts as a
scaling factor for the linearly estimated similarity.
The hierarchical fusion scheme is actually a col-
lection of (overlapping) linear regression models,
each matching a range of sentence lengths. For ex-
ample, the first model DL1 is trained with sentences
with length up to l1, i.e., l ? l1, the second model
DL2 up to length l2 etc. During testing, sentences
with length l ? [1, l1] are decoded with DL1, sen-
tences with length l ? (l1, l2] with model DL2 etc.
Each of these partial models is a linear fusion model
as shown in (2). In this work, we use four models
with l1 = 10, l2 = 20, l3 = 30, l4 =?.
4 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. The exact type
of pre-processing used depends on the metric used.
For the plain lexical BLEU, we use lemmatization,
stemming (of lemmas) and remove all non-content
words, keeping only nouns, adjectives, verbs and ad-
verbs. For computing semantic similarity scores, we
don?t use stemming and keep only noun words, since
we only have similarities between non-noun words.
For the computation of semantic similarity we have
created a dictionary containing all the single-word
nouns included in WordNet (approx. 60K) and then
downloaded snippets of the 500 top-ranked docu-
ments for each word by formulating single-word
queries and submitting them to the Yahoo! search
engine.
Next, results are reported in terms of correlation
between the automatically computed scores and the
ground truth, for each of the corpora in Task 6 of
SemEval?12 (paraphrase, video, europarl, WordNet,
news). Overall correlation (?Ovrl?) computed on the
join of the dataset, as well as, average (?Mean?) cor-
relation across all task is also reported. Training is
performed on a subset of the first three corpora and
testing on all five corpora.
Baseline BLEU: The first set of results in Ta-
ble 1, shows the correlation performance of the
plain BLEU hit rates (per training data set and over-
all/average). The best performing hit rate is the one
567
calculated using unigrams.
Table 1: Correlation performance of BLEU hit rates.
par vid euro Mean Ovrl
BLEU 1-grams 0.62 0.67 0.49 0.59 0.57
BLEU 2-grams 0.40 0.39 0.37 0.39 0.34
BLEU 3-grams 0.32 0.36 0.30 0.33 0.33
BLEU 4-grams 0.26 0.25 0.24 0.25 0.28
Semantic Similarity BLEU (Purple): The perfor-
mance of the modified version of BLEU that in-
corporates various word-level similarity metrics is
shown in Table 2. Here the BLEU hits (exact
matches) are summed together with the normalized
similarity scores (approximate matches) to obtain a
single B1+M1 (Purple) score5. As we can see, there
are definite benefits to using the modified version,
particularly with regards to mean correlation. Over-
all the best performers, when taking into account
both mean and overall correlation, are the WordNet-
based and Ia metrics, with the Ia metric winning by
a slight margin, earning a place in the final models.
Table 2: Correlation performance of 1-gram BLEU
scores with semantic similarity metrics (nouns-only).
par vid euro Mean Ovrl
BLEU 0.54 0.60 0.39 0.51 0.58
SS-BLEU WordNet 0.56 0.64 0.41 0.54 0.58
SS-BLEU I(i, j) 0.56 0.63 0.39 0.53 0.59
SS-BLEU Ia(i, j) 0.57 0.64 0.40 0.54 0.58
Regression models (DeepPurple): Next, the per-
formance of the various regression models (fusion
schemes) is investigated. Each regression model is
evaluated by performing 10-fold cross-validation on
the SemEval training set. Correlation performance
is shown in Table 3 both with and without seman-
tic similarity. The baseline in this case is the Pur-
ple metric (corresponding to no fusion). Clearly
the use of regression models significantly improves
performance compared to the 1-gram BLEU and
Purple baselines for almost all datasets, and espe-
cially for the combined dataset (overall). Among
the fusion schemes, the hierarchical models perform
the best. Following fusion, the performance gain
from incorporating semantic similarity (SS) is much
smaller. Finally, in Table 4, correlation performance
of our submissions on the official SemEval test set is
5It should be stressed that the plain BLEU unigram scores
shown in this table are not comparable to those in Table 1, since
here scores are calculated over only the nouns of each sentence.
Table 3: Correlation performance of regression model
with (SS) and without semantic similarities on the train-
ing set (using 10-fold cross-validation).
par vid euro Mean Ovrl
None (SS-BLEU Ia) 0.57 0.64 0.40 0.54 0.58
Linear (D?L, a5=0) 0.62 0.72 0.47 0.60 0.66
Sigmoid (D?S, a5=0) 0.64 0.73 0.42 0.60 0.73
Hierarchical 0.64 0.74 0.48 0.62 0.73
SS-Linear (D?L) 0.64 0.73 0.47 0.61 0.66
SS-Sigmoid (D?S) 0.65 0.74 0.42 0.60 0.74
SS-Hierarchical 0.65 0.74 0.48 0.62 0.73
shown. The overall correlation performance of the
Hierarchical model ranks somewhere in the middle
(43rd out of 89 systems), while the mean correla-
tion (weighted by number of samples per set) is no-
tably better: 23rd out of 89. Comparing the individ-
ual dataset results, our systems underperform for the
two datasets that originate from the machine transla-
tion (MT) literature (and contain longer sentences),
while we achieve good results for the rest (19th for
paraphrase, 37th for video and 29th for WN).
Table 4: Correlation performance on test set.
par vid euro WN news Mean Ovrl
None 0.50 0.71 0.44 0.49 0.24 0.51 0.49
Sigm. 0.60 0.76 0.26 0.60 0.34 0.56 0.55
Hier. 0.60 0.77 0.43 0.65 0.37 0.60 0.62
5 Conclusions
We have shown that: 1) a regression model that
combines counts of exact and approximate n-gram
matches provides good performance for sentence
similarity computation (especially for short and
medium length sentences), 2) the non-linear scal-
ing of hit-rates with respect to sentence length im-
proves performance, 3) incorporating word semantic
similarity scores (soft-match) into the model can im-
prove performance, and 4) web snippet corpus cre-
ation and the modified mutual information metric
is a language agnostic approach that can (at least)
match semantic similarity performance of the best
resource-based metrics for this task. Future work,
should involve the extension of this approach to
model larger lexical chunks, the incorporation of
compositional models of meaning, and in general
the phrase-level modeling of semantic similarity, in
order to compete with MT-based systems trained on
massive external parallel corpora.
568
References
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applications.
Springer.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proc. of International Conference
on World Wide Web, pages 757?766.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
I. Dagan, O. Glickman, and B. Magnini. 2006.
The pascal recognising textual entailment challenge.
In Joaquin Quionero-Candela, Ido Dagan, Bernardo
Magnini, and Florence dAlch Buc, editors, Machine
Learning Challenges. Evaluating Predictive Uncer-
tainty, Visual Object Classification, and Recognising
Tectual Entailment, volume 3944 of Lecture Notes in
Computer Science, pages 177?190. Springer Berlin /
Heidelberg.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012a. Minimum error se-
mantic similarity using text corpora constructed from
web queries. IEEE Transactions on Knowledge and
Data Engineering (submitted to).
E. Iosif and A. Potamianos. 2012b. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. Proc. of Eighth International Con-
ference on Language Resources and Evaluation (to ap-
pear).
E. Iosif and A. Potamianos. 2012c. Similarity com-
putation using semantic networks created from web-
harvested data. Natural Language Engineering (sub-
mitted to).
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341387.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th Annual Meeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
569
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proc. of the EACL Workshop on
Making Sense of Sense: Bringing Computational Lin-
guistics and Psycholinguistics Together, pages 1?8.
T. Pedersen. 2005. WordNet::Similarity.
http://search.cpan.org/dist/
WordNet-Similarity/.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. D. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proc. of the European
Conference on Machine Learning, pages 491?502.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
570
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
Abstract
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
1 Introduction
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al, 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al, 2011). In
this work, we built on previous research and our sub-
mission to SemEval?2012 (Malandrakis et al, 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al, 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al, 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Ba?r et al, 2012; S?aric? et al, 2012).
Our approach is originally motivated by BLEU
and primarily utilizes ?hard? and ?soft? n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
2 Model
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al, 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
103
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
2.1 Word level semantic similarity
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p?(i,j)p?(i)p?(j) , where p?(i) and p?(j) are
the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence
is denoted by p?(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al, 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights ? were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al, 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
2.2 Sentence level similarities
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al, 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a ?degree-of-match?. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are ?soft? hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the ?hard? hit rates produced by baseline BLEU
as features of the final model.
2.3 String similarities
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
104
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = |S(X) ? S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
2.4 Affective similarity
We used the method proposed in (Malandrakis et al,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [?1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (2)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
andwj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an ?affective similarity metric? we
use the difference of means of the word affective rat-
ings between two sentences.
d?affect = 2? |?(v?(s1))? ?(v?(s2))| (3)
where ?(v?(si)) the mean of content word ratings in-
cluded in sentence i.
2.5 Fusion
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
D?L = a0 +
k
?
n=1
an fk, (4)
where D?L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ? l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ? [1, l1] are decoded
with DL1, sentences with length l ? (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = ?.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
3 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
105
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN ?
OnWN, headlines ? SMTnews, SMT ? Europarl
and FNWN? OnWN.
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
Table 2: Correlation performance on the evaluation set.
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
4 Conclusions
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
5 Acknowledgements
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
106
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385?393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435?440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21?29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499?3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328?334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) ? The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565?570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
107
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th AnnualMeeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
F. S?aric?, G. Glavas?, M. Karan, J. S?najder, and B. Dal-
belo Bas?ic?. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441?
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
108
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 438?442, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SAIL: A hybrid approach to sentiment analysis
Nikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan1
1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA
3Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission for Se-
mEval2013 Task 2: Sentiment Analysis in
Twitter. For the limited data condition we use
a lexicon-based model. The model uses an af-
fective lexicon automatically generated from a
very large corpus of raw web data. Statistics
are calculated over the word and bigram af-
fective ratings and used as features of a Naive
Bayes tree model. For the unconstrained data
scenario we combine the lexicon-based model
with a classifier built on maximum entropy
language models and trained on a large exter-
nal dataset. The two models are fused at the
posterior level to produce a final output. The
approach proved successful, reaching rank-
ings of 9th and 4th in the twitter sentiment
analysis constrained and unconstrained sce-
nario respectively, despite using only lexical
features.
1 Introduction
The analysis of the emotional content of text, is
relevant to numerous natural language processing
(NLP), web and multi-modal dialogue applications.
To that end there has been a significant scientific
effort towards tasks like product review analysis
(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),
speech emotion extraction (Lee and Narayanan,
2005; Lee et al, 2002; Ang et al, 2002) and pure
text word (Esuli and Sebastiani, 2006; Strappar-
ava and Valitutti, 2004) and sentence (Turney and
Littman, 2002; Turney and Littman, 2003) level
emotion extraction.
The rise of social media in recent years has seen
a shift in research focus towards them, particularly
twitter. The large volume of text data available is
particularly useful, since it allows the use of com-
plex machine learning methods. Also important is
the interest on the part of companies that are actively
looking for ways to mine social media for opinions
and attitudes towards them and their products. Sim-
ilarly, in journalism there is interest in sentiment
analysis for a way to process and report on the public
opinion about current events (Petulla, 2013).
Analyzing emotion expressed in twitter borrows
from other tasks related to affective analysis, but
also presents unique challenges. One common is-
sue is the breadth of content available in twitter: a
more limited domain would make the task easier,
however there are no such bounds. There is also a
significant difference in the form of language used
in tweets. The tone is informal and typographical
and grammatical errors are very common, making
even simple tasks, like Part-of-Speech tagging much
harder. Features like hashtags and emoticons can
also be helpful (Davidov et al, 2010).
This paper describes our submissions for Se-
mEval 2013 task 2, subtask B, which deals pri-
marily with sentiment analysis in twitter. For the
constrained condition (using only the organizer-
provided twitter sentences) we implemented a sys-
tem based on the use of an affective lexicon and part-
of-speech tag information, which has been shown
relevant to the task (Pak and Paroubek, 2010).
For the unconstrained condition (including external
sources of twitter sentences) we combine the con-
strained model with a maximum entropy language
438
model trained on external data.
2 Experimental procedure
We use two separate models, one for the constrained
condition and a combination for the unconstrained
condition. Following are short descriptions.
2.1 Lexicon-based model
The method used for the constrained condition is
based on an affective lexicon containing out-of-
context affective ratings for all terms contained in
each sentence. We use an automated algorithm of
affective lexicon expansion based on the one pre-
sented in (Malandrakis et al, 2011), which in turn
is an expansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence and arousal ratings of any term can be repre-
sented as a linear combination of its semantic simi-
larities to a set of seed words and the affective rat-
ings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (1)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj . For the purposes of this work, the seman-
tic similarity metric is the cosine similarity between
context vectors computed over a corpus of 116 mil-
lion web snippets collected by posing one query for
every word in the Aspell spellchecker?s vocabulary
to the Yahoo! search engine and collecting up to 500
of the top results.
Given a starting, manually annotated, lexicon we
can select part of it to serve as seed words and then
use 1 to create a system of linear equations where
the only unknowns are the weights ai. The system
is solved using Least Squares Estimation. That pro-
vides us with an equation that can generate affective
ratings for every term (not limited to words), as long
as we can estimate the semantic similarity between
it and the seed words.
Seed word selection is performed by a simple
heuristic (though validated through experiments):
we want seed words to have extreme affective rat-
ings (maximum absolute value) and we want the set
to be as closed to balanced as possible (sum of seed
ratings equal to zero).
Given these term ratings, the next step is combin-
ing them through statistics. To do that we use sim-
ple statistics (mean, min, max) and group by part
of speech tags. The results are statistics like ?max-
imum valence among adjectives?, ?mean arousal
among proper nouns? and ?number of verbs and
nouns?. The dimensions used are: valence, absolute
valence and arousal. The grouping factors are the 39
Penn treebank pos tags plus higher order tags (adjec-
tives, verbs, nouns, adverbs and combinations of 2,3
and 4 of them). The statistics extracted are: mean,
min, max, most extreme, sum, number, percentage
of sentence coverage. In the case of bigram terms no
part-of-speech filtering/grouping is applied. These
statistics form the feature vectors.
Finally we perform feature selection on the mas-
sive set of candidates and use them to train a model.
The model selected is a Naive Bayes tree, a tree with
Naive Bayes classifiers on each leaf. The motivation
comes by considering this a two stage problem: sub-
jectivity detection and polarity classification, mak-
ing a hierarchical model a natural choice. NB trees
proved superior to other types of trees during our
testing, presumably due to the smoothing of obser-
vation distributions.
2.2 N-gram language model
The method used for the unconstrained condition
is based on a combination of the automatically ex-
panded affective lexicon described in the previ-
ous section together with a bigram language model
based on the work of (Wang et al, 2012), which
uses a large set of twitter data from the U.S. 2012
Presidential election. As a part of the unconstrained
system, we were able to leverage external annotated
data apart from those provided by the SEMEVAL
2013 sentiment task dataset. Of the 315 million
tweets we collected about the election, we anno-
tated a subset of 40 thousand tweets using Ama-
zon Mechanical Turk. The annotation labels that
we used were ?positive?, ?negative?, ?neutral?, and
?unsure?, and additionally raters could mark tweets
for sarcasm and humor. We excluded tweets marked
as ?unsure? as well as tweets that had disagree-
439
ment in labels if they were annotated by more than
one annotator. To extract the bigram features, we
used a twitter-specific tokenizer (Potts, 2011), which
marked uniform resource locators (URLs), emoti-
cons, and repeated characters, and which lowercased
words that began with capital letters followed by
lowercase letters (but left words in all capitals). The
bigram features were computed as presence or ab-
sense in the tweet rather than counts due to the small
number of words in tweets. The machine learning
model used to classify the tweets was the Megam
maximum entropy classifier (Daume? III, 2004) in
the Natural Language Toolkit (NLTK) (Bird et al,
2009).
2.3 Fusion
The submitted system for the unconstrained condi-
tion leverages both the lexicon-based and bigram
language models. Due to the very different nature
of the models we opt to not fuse them at the feature
level, using a late fusion scheme instead. Both par-
tial models are probabilistic, therefore we can use
their per-class posterior probabilities as features of
a fusion model. The fusion model is a linear kernel
SVM using six features, the three posteriors from
each partial model, and trained on held out data.
3 Results
Following are results from our method, evaluated
on the testing sets (of sms and twitter posts) of
SemEval2013 task 2. We evaluate in terms of 3-
class classification, polarity classification (positive
vs. negative) and subjectivity detection (neutral vs.
other). Results shown in terms of per category f-
measure.
3.1 Constrained
The preprocessing required for the lexicon-based
model is just part-of-speech tagging using Treetag-
ger (Schmid, 1994). The lexicon expansion method
is used to generate valence and arousal ratings for
all words and ngrams in all datasets and the part of
speech tags are used as grouping criteria to gener-
ate statistics. Finally, feature selection is performed
using a correlation criterion (Hall, 1999) and the re-
sulting feature set is used to train a Naive Bayes
tree model. The feature selection and model train-
Table 1: F-measure results for the lexicon-based model,
using different machine learning methods, evaluated on
the 3-class twitter testing data.
model
per-class F-measure
neg neu pos
Nbayes 0.494 0.652 0.614
SVM 0.369 0.677 0.583
CART 0.430 0.676 0.593
NBTree 0.561 0.662 0.643
Table 2: F-measure results for the constrained condition,
evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.561 0.662 0.643
pos vs neg 0.679 0.858
neu vs other 0.685 0.699
sms
3-class 0.506 0.709 0.531
pos vs neg 0.688 0.755
neu vs other 0.730 0.628
ing/classification was conducted using Weka (Wit-
ten and Frank, 2000).
The final model uses a total of 72 features, which
can not be listed here due to space constraints. The
vast majority of these features are necessary to de-
tect the neutral category: positive-negative separa-
tion can be achieved with under 30 features.
One aspect of the model we felt worth investigat-
ing, was the type of model to be used. Using a multi-
stage model, performing subjectivity detection be-
fore positive-negative classification, has been shown
to provide an improvement, however single models
have also been used extensively. We compared some
popular models: Naive Bayes, linear kernel SVM,
CART-trained tree and Naive Bayes tree, all using
the same features, on the twitter part of the SemEval
testing data. The results are shown in Table 1. The
two Naive Bayes-based models proved significantly
better, with NBTree being clearly the best model for
these features.
Results from the submitted constrained model are
shown in Table 2. Looking at the twitter data re-
sults and comparing the positive-negative vs the
440
3-class results, it appears the main weakness of
this model is subjectivity detection, mostly on the
neutral-negative side. It is not entirely clear to us
whether that is an artifact of the model (the nega-
tive class has the lowest prior probability, thus may
suffer compared to neutral) or of the more complex
forms of negativity (sarcasm, irony) which we do not
directly address. There is a definite drop in perfor-
mance when using the same twitter-trained model on
sms data, which we would not expect, given that the
features used are not twitter-specific. We believe this
gap is caused by lower part-of-speech tagger perfor-
mance: visual inspection reveals the output on twit-
ter data is fairly bad.
Overall this model ranked 9th out of 35 in the
twitter set and 11th out of 28 in the sms set, among
all constrained submissions.
3.2 Unconstrained
Table 3: F-measure results for the maximum entropy
model with bigram features, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.403 0.661 0.623
pos vs neg 0.586 0.804
neu vs other 0.661 0.704
sms
3-class 0.390 0.587 0.542
pos vs neg 0.710 0.648
neu vs other 0.587 0.641
Table 4: F-measure results for the unconstrained condi-
tion, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.565 0.679 0.655
pos vs neg 0.672 0.881
neu vs other 0.667 0.732
sms
3-class 0.502 0.723 0.538
pos vs neg 0.625 0.772
neu vs other 0.710 0.637
In order to create the submitted unconstrained
model we train an SVM model using the lexicon-
based and bigram language model posterior proba-
bilities as features. This fusion model is trained on
held-out data (the development set of the SemEval
data). The results of classification using the bigram
language model alone are shown in Table 3 and the
results from the final fused model are shown in Ta-
ble 4. Looking at relative per-class performance, the
results follow a form most similar to the constrained
model, though there are gains in all cases. These
gains are less significant when evaluated on the sms
data, resulting in a fair drop in ranks: the bigram lan-
guage model (expectedly) suffers more when mov-
ing to a different domain, since it uses words as
features rather than the more abstract affective rat-
ings used by the lexicon-based model. Also, because
the external data used to train the bigram language
model was from discussions of politics on Twitter,
the subject matter also varied in terms of prior senti-
ment distribution in that the negative class was pre-
dominant in politics, which resulted in high recall
but low precision for the negative class.
This model ranked 4th out of 16 in the twitter set
and 7th out of 17 in the sms set, among all uncon-
strained submissions.
4 Conclusions
We presented a system of twitter sentiment analy-
sis combining two approaches: a hierarchical model
based on an affective lexicon and a language model-
ing approach, fused at the posterior level. The hier-
archical lexicon-based model proved very successful
despite using only n-gram affective ratings and part-
of-speech information. The language model was
not as good individually, but provided a noticeable
improvement to the lexicon-based model. Overall
the models achieved good performance, ranking 9th
of 35 and 4th of 16 in the constrained and uncon-
strained twitter experiments respectively, despite us-
ing only lexical information.
Future work will focus on incorporating im-
proved tokenization (including part-of-speech tag-
ging), making better use of twitter-specific features
like emoticons and hashtags, and performing affec-
tive lexicon generation on twitter data.
441
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP, pages 2037?2040.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
H. Daume? III. 2004. Notes on cg and lm-bfgs op-
timization of logistic regression. Paper available at
http://pub. hal3. name# daume04cg-bfgs, implementa-
tion available at http://hal3. name/megam.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proc. COLING, pages 241?249.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A pub-
licly available lexical resource for opinion mining. In
Proc. LREC, pages 417?422.
M. A. Hall. 1999. Correlation-based feature selection
for machine learning. Ph.D. thesis, The University of
Waikato.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proc. SIGKDD, KDD ?04, pages
168?177. ACM.
C. M. Lee and S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2):293?303.
C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. ICSLP, pages 873?876.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. In Proc.
LREC, pages 1320?1326.
S. Petulla. 2013. Feelings, nothing more than feelings:
The measured rise of sentiment analysis in journalism.
Neiman Journalism Lab, January.
C. Potts. 2011. Sentiment symposium tutorial: Tokeniz-
ing. Technical report, Stanford Linguistics.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. International Confer-
ence on New Methods in Language Processing, vol-
ume 12, pages 44?49.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proc. LREC,
volume 4, pages 1083?1086.
P. Turney and M. L. Littman. 2002. Unsupervised
Learning of Semantic Orientation from a Hundred-
Billion-Word Corpus. Technical report ERC-1094
(NRC 44929). National Research Council of Canada.
P. Turney and M. L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems, 21:315?346.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twitter
sentiment analysis of 2012 u.s. presidential election
cycle. In Proc. ACL, pages 115?120.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proc. COLING/ACL, pages 1065?1072.
Ian H.Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
442
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 9?16,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems
Ioannis Klasinas
1
, Elias Iosif
2,4
, Katerina Louka
3
, Alexandros Potamianos
2,4
1
School of ECE, Technical University of Crete, Chania 73100, Greece
2
School of ECE, National Technical University of Athens, Zografou 15780, Greece
3
Voiceweb S.A., Athens 15124, Greece
4
Athena Research Center, Marousi 15125, Greece
iklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouka@voiceweb.eu
Abstract
In this paper we present the SemEval-
2014 Task 2 on spoken dialogue gram-
mar induction. The task is to classify
a lexical fragment to the appropriate se-
mantic category (grammar rule) in order
to construct a grammar for spoken dia-
logue systems. We describe four sub-
tasks covering two languages, English and
Greek, and three speech application do-
mains, travel reservation, tourism and fi-
nance. The classification results are com-
pared against the groundtruth. Weighted
and unweighted precision, recall and f-
measure are reported. Three sites partic-
ipated in the task with five systems, em-
ploying a variety of features and in some
cases using external resources for training.
The submissions manage to significantly
beat the baseline, achieving a f-measure of
0.69 in comparison to 0.56 for the base-
line, averaged across all subtasks.
1 Introduction
This task aims to foster the application of com-
putational models of lexical semantics to the field
of spoken dialogue systems (SDS) for the problem
of grammar induction. Grammars constitute a vi-
tal component of SDS representing the semantics
of the domain of interest and allowing the system
to correctly respond to a user?s utterance.
The task has been developed in tight collabo-
ration between the research community and com-
mercial SDS grammar developers, under the aus-
pices of the EU-IST PortDial project
1
. Among the
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
1
http://www.portdial.eu/
project aims is to help automate the grammar de-
velopment and localization process. Unlike previ-
ous approaches (Wang and Acero, 2006; Cramer,
2007) that have focused on full automation, Port-
Dial adopts a human-in-the-loop approach were
a developer bootstraps each grammar rule or re-
quest type with a few examples (use cases) and
then machine learning algorithms are used to pro-
pose grammar rule enhancements to the developer.
The enhancements are post-edited by the devel-
oper and new grammar rule suggestions are pro-
posed by the system, in an iterative fashion un-
til a grammar of sufficient quality is achieved. In
this task, we focus on a snapshot of this process,
where a portion of the grammar is already induced
and post-edited by the developer and new candi-
date fragments are rolling in order to be classified
to an existing rule (or rejected). The goal is to de-
velop machine learning algorithms for classifying
candidate lexical fragments to the correct grammar
rule (semantic category). The task is equally rel-
evant for both finite-state machine and statistical
grammar induction.
In this task the semantic hierarchy of SDS
grammars has two layers, namely, low- and high-
level. Low-level rules are similar to gazetteers
referring to terminal concepts that can be as rep-
resented as sets of lexical entries. For example,
the concept of city name can be represented as
<CITY> = (?London?, ?Paris?, ...). High-level
rules are defined on top of low-level rules, while
they can be lexicalized as textual fragments (or
chunks), e.g., <TOCITY> = (?fly to <CITY>?,
...). Using the above examples the sentence ?I
want to fly to Paris? will be first parsed as ?I
want to fly to <CITY>? and finally as ?I want to
<TOCITY>?.
In this task, we focus exclusively on high-level
rule induction, assuming that the low-level rules
are known. The problem of fragment extraction
and selection is simplified by investigating the
9
binary classification of (already extracted) frag-
ments into valid and non-valid. The task boils
down mainly to a semantic similarity estimation
problem for the assignment of valid fragments into
high-level rules.
2 Prior Work
The manual development of grammars is a time-
consuming and tedious process that requires hu-
man expertise, posing an obstacle to the rapid port-
ing of SDS to new domains and languages. A
semantically coherent workflow for SDS gram-
mar development starts from the definition of low-
level rules and proceeds to high-level ones. This
process is also valid for the case of induction
algorithms. Automatic or machine-aided gram-
mar creation for spoken dialogue systems can
be broadly divided in two categories (Wang and
Acero, 2006): knowledge-based (or top-down)
and data-driven (or bottom-up) approaches.
Knowledge-based approaches rely on the man-
ual or semi-automatic development of domain-
specific grammars. They start from the domain on-
tology (or taxonomy), often in the form of seman-
tic frames. First, terminal concepts in the ontology
(that correspond to low-level grammar rules) get
populated with values, e.g., <CITY>, and then
high-level concepts (that correspond to high-level
grammar rules) get lexicalized creating grammar
fragments. Finally, phrase headers and trailers are
added to create full sentences. The resulting gram-
mars often suffer from limited coverage (poor re-
call). In order to improve coverage, regular ex-
pressions and word/phrase order permutations are
used, however at the cost of over-generalization
(poor precision). Moreover, knowledge-based
grammars are costly to create and maintain, as
they require domain and engineering expertise,
and they are not easily portable to new domains.
This led to the development of grammar authoring
tools that aim at facilitating the creation and adap-
tation of grammars. SGStudio (Semantic Gram-
mar Studio), (Wang and Acero, 2006), for exam-
ple, enables 1) example-based grammar learning,
2) grammar controls, i.e., building blocks and op-
erators for building more complex grammar frag-
ments (regular expressions, lists of concepts), and
3) configurable grammar structures, allowing for
domain-adaptation and word-spotting grammars.
The Grammatical Framework Resource Grammar
Library (GFRGL) (Ranta, 2004) enables the cre-
ation of multilingual grammars adopting an ab-
straction formalism, which aims to hide the lin-
guistic details (e.g., morphology) from the gram-
mar developer.
Data-driven approaches rely solely on corpora
(bottom-up) of transcribed utterances (Meng and
Siu, 2002; Pargellis et al., 2004). The induction
of low-level rules consists of two steps dealing
with the 1) identification of terms, and 2) assign-
ment of terms into rules. Standard tokenization
techniques can be used for the first step, however,
different approaches are required for the case of
multiword terms, e.g., ?New York?. In such cases,
gazetteer lookup and named entity recognition can
be employed (if the respective resources and tools
are available), as well as corpus-based colloca-
tion metrics (Frantzi and Ananiadou, 1997). Typ-
ically, the identified terms are assigned into low-
level rules via clustering algorithms operating over
a feature space that is built according to the term
semantic similarity. The distributional hypothe-
sis of meaning (Harris, 1954) is a widely-used ap-
proach for estimating term similarity. A compar-
ative study of similarity metrics for the induction
of SDS low-level rules is presented in (Pargellis
et al., 2004), while the combination of metrics
was investigated in (Iosif et al., 2006). Different
clustering algorithms have been applied includ-
ing hard- (Meng and Siu, 2002) and soft-decision
(Iosif and Potamianos, 2007) agglomerative clus-
tering.
High-level rule induction is a less researched
area that consists of two main sub-problems: 1)
the extraction and selection of candidate frag-
ments from a corpus, and 2) assignment of terms
into rules. Regarding the first sub-problem,
consider the fragments ?I want to depart from
<CITY> on? and ?depart from <CITY>? for the
air travel domain. Both express the meaning of de-
parture city, however, the (semantics of the) latter
fragment are more concise and generalize better.
The application of syntactic parsers for segment
extraction is not straightforward since the output
is a full parse tree. Moreover, such parsers are
typically trained over annotated corpora of formal
language usage, while the SDS corpora often are
ungrammatical due to spontaneous speech. There
are few statistical parsing algorithms that rely only
on plain lexical features (Ponvert et al., 2011; Bisk
and Hockenmaier, 2012) however, as other algo-
rithms, one needs to decide where to prune the
10
parse tree. In (Georgiladakis et al., 2014), the ex-
plicit extraction and selection of fragments is in-
vestigated following an example-driven approach
where few rule seeds are provided by the gram-
mar developer. The second sub-problem of high-
level rule induction deals with the formulation
of rules using the selected fragments. Each rule
is meant to consist of semantically similar frag-
ments. For this purpose, clustering algorithms can
be employed exploiting the semantic similarity be-
tween fragments as features. This is a challenging
problem since the fragments are multi-word struc-
tures whose overall meaning is composed accord-
ing to semantics of the individual constituents. Re-
cently, several models have been proposed regard-
ing phrase (Mitchell and Lapata, 2010) and sen-
tence similarity (Agirre et al., 2012), while an
approach towards addressing the issue of seman-
tic compositionality is presented in (Milajevs and
Purver, 2014).
The main drawback of data-driven approaches
is the problem of data sparseness, which may af-
fect the coverage of the grammar. A popular so-
lution to the data sparseness bottleneck is to har-
vest in-domain data from the web. Recently, this
has been an active research area both for SDS
systems and language modeling in general. Data
harvesting is performed in two steps: (i) query
formulation, and (ii) selection of relevant docu-
ments or sentences (Klasinas et al., 2013). Posing
the appropriate queries is important both for ob-
taining in-domain and linguistically diverse sen-
tences. In (Sethy et al., 2007), an in-domain lan-
guage model was used to identify the most ap-
propriate n-grams to use as web queries. An in-
domain language model was used in (Klasinas et
al., 2013) for the selection of relevant sentences.
A more sophisticated query formulation was pro-
posed in (Sarikaya, 2008), where from each in-
domain utterance a set of queries of varying length
and complexity was generated. These approaches
assume the availability of in-domain data (even if
limited) for the successful formulation of queries;
this dependency is also not eliminated when us-
ing a mildly lexicalized domain ontology to for-
mulate the queries, as in (Misu and Kawahara,
2006). Selecting the most relevant sentences that
get returned from web queries is typically done
using statistical similarity metrics between in do-
main data and retrieved documents, for example
the BLEU metric (Papineni et al., 2002) of n-
gram similarity in (Sarikaya, 2008) and a metric
of relative entropy (Kullback-Leibler) in (Sethy et
al., 2007). In cases where in-domain data is not
available, cf. (Misu and Kawahara, 2006), heuris-
tics (pronouns, sentence length, wh-questions) and
matches with out-of-domain language models can
be used to identify sentences for training SDS
grammars. In (Sarikaya, 2008), the produced
grammar fragments are also parsed and attached
to the domain ontology. Harvesting web data can
produce high-quality grammars while requiring up
to 10 times less in-domain data (Sarikaya, 2008).
Further, data-driven approaches induce syntac-
tic grammars but do not learn their corresponding
meanings, for this purpose an additional step is re-
quired of parsing the grammar fragments and at-
taching them to the domain ontology (Sarikaya,
2008). Also, in many cases it was observed
that the fully automated bottom-up paradigm re-
sults to grammars of moderate quality (Wang
and Acero, 2006), especially on corpora con-
taining longer sentences and more lexical vari-
ety (Cramer, 2007). Finally, algorithms focusing
on crosslingual grammar induction, like CLIoS
(Kuhn, 2004), are often even more resource-
intensive, as they require training corpora of par-
allel text and sometimes also a grammar for one of
the languages. Grammar quality can be improved
by introducing a human in the loop of grammar in-
duction (Portdial, 2014a); an expert that validates
the automatically created results (Meng and Siu,
2002).
3 Task Description
Next we describe in detail the candidate grammar
fragment classification SemEval task. This task
is part of a grammar rule induction scenario for
high-level rules. The evaluation focuses in spoken
dialogue system grammars for multiple domains
and languages.
3.1 Task Design
The goal of the task is to classify a number frag-
ment to the rules available in the grammar. For
each grammar we provide a training and develop-
ment set, i.e., a set of rules with the associated
fragments and the test set which is composed of
plain fragments. An excerpt of the train set for the
rule ?<TOCITY>? is ?ARRIVE AT <CITY>,
ARRIVES AT <CITY>, GOING TO <CITY>?
and of the test set ?GOING INTO <CITY>, AR-
11
RIVES INTO <CITY>?.
In preliminary experiments during the task de-
sign we noticed that if the test set consists of valid
fragments only, good classification performance is
achieved, even when using the naive baseline sys-
tem described later in this paper. To make the task
more realistic we have included a set of ?junk?
fragments not corresponding to any specific rule.
Junk fragments were added both in the train set
where they are annotated as such and in the test
set. For this task we have artificially created the
junk fragments by removing or adding words from
legitimate fragments. Example junk fragments
used are ?HOLD AT AT <TIME> TRY? and
?ANY CHOICE EXCEPT <AIRLINE> OR?, the
first one having a repetition of the word ?AT?
while the second one should include one more
time the concept ?<AIRLINE>? in the end to be
meaningful.
Junk fragments help better model a real-world
scenario, where the candidate fragments will in-
clude irrelevant examples too. For example, if
web corpora are used to extract the candidate frag-
ments grammatical mistakes and out-of-domain
sentences might appear. Similarly, if the transcrip-
tions from a deployed SDS system are used for
grammar induction, transcription errors might in-
troduce noise (Bechet et al., 2014).
Junk fragments account for roughly 5% of the
train test and 15% of the test set. The discrep-
ancy between train and test set ratios is due to a
conscious effort to model realistic train/test condi-
tions, where train data is manually processed and
does not include errors, while candidate fragments
are typically more noisy.
3.2 Datasets
We have provided four datasets, travel English,
travel Greek, tourism English and finance English.
The travel domain grammar covers flight, car and
hotel reservation utterances. The tourism domain
covers touristic information including accommo-
dation, restaurants and movies. The finance do-
main covers utterances of a bank client asking
questions about his bank account as well as re-
porting problems. In Table 1 are presented typical
examples of fragments for every subtask.
All grammars have been manually constructed
by a grammar developer. For the three English
grammars, a small corpus (between 500 and 2000
sentences) was initially available. The grammar
developer first identified terminal concepts, which
correspond to low-level rules. Typical examples
include city names for the travel domain, restau-
rant names for the tourism domain and credit card
names in the finance domain. After covering all
low-level rules the grammar developer proceeded
to identify high-level rules present in the corpus,
like the departure city in the travel domain, or the
user request type for a credit card. The gram-
mar developer was instructed to identify all rules
present in the corpus, but also spend some effort
to include rules not appearing in the corpus so that
the resulting grammar better covers the domain at
hand. For the case of Greek travel grammar no
corpus was initially available. The Greek gram-
mar was instead produced by manually translat-
ing the English one, accounting for the differences
in syntax between the two languages. The gram-
mars have been developed as part of the PortDial
FP7 project and are explained in detail in (Portdial,
2014b).
For the first three datasets that have been avail-
able from the beginning of the campaign we have
split the release into train, development and test
set. For the finance domain which was announced
when the test sets were released we only provided
the train and test set, to simulate a resource poor
scenario. The statistics of the datasets for all lan-
guage/domain pairs are given in Table 2.
In addition to the high-level rules we made
available the low-level rules for each grammar,
which although not used in the evaluation, can be
useful for expanding the high-level rules to cover
all lexicalizations expressed by the grammar.
3.3 Evaluation
For the evaluation of the task we have used preci-
sion, recall and f-measure, both weighted and un-
weighted.
If R
j
denotes the set of fragments for one rule
and C
j
the set of fragments classified to this rule
by a system then per-rule precision is computed by
the equation:
Pr
j
=
|R
j
? C
j
|
|C
j
|
and per-rule recall by:
Rc
j
=
|R
j
? C
j
|
|R
j
|
F-measure is then computed by:
12
Grammar Rule Fragment
Travel English <FLIGHTFROM> FLIGHT FROM <CITY>
Travel Greek <FLIGHTFROM> ?TH?H A?O <CITY>
Tourism English <TRANSFERQ> TRANSFERS FROM <airportname> TO <cityname>
Finance English <CARDNAME> <BANKNAME> CARD
Table 1: Example grammar fragments for each application domain.
Grammar Rules Fragments
Train set Dev set Test set
Travel English 32 623 331 284
Travel Greek 35 616 340 324
Tourism English 24 694 334 285
Finance English 9 136 - 37
Table 2: Number of rules in the training, development and test sets for each application domain.
F
j
=
2Pr
j
Rc
j
Pr
j
+ Rc
j
.
Precision for all the J rules R
j
, 1 ? j ? J is
computed by the following equation:
Pr =
?
j
Pr
j
w
j
In the unweighted case the weight w
j
has a fixed
value for all rules, so w
j
=
1
J
. Taking into account
the fact that the rules are not balanced in terms of
fragments, a better way to compute for the weight
is w
j
=
|R
j
|
?
j
|R
j
|
. In the latter, weighted, case the
total precision will better describe the results.
Recall is similarly computed using the same
weighting scheme as:
Rc =
?
j
Rc
j
w
j
3.4 Baseline
For comparison purposes we have developed a
naive baseline system. To classify a test fragment,
first its similarity with all the train fragments is
computed, and it is classified to the rule where
the most similar train fragment belongs. Fragment
similarity is computed as the ratio of their Longest
Common Substring (LCS) divided by the sum of
their lengths:
Sim(s, t) =
|LCS(s, t)|
|s|+ |t|
where s and t are two strings, |s| and |t| their
length in characters and |LCS(s, t)| the length of
their LCS. This is a very simple baseline, comput-
ing similarity without taking into account context
or semantics.
4 Participating Systems
Three teams have participated in the task with five
systems. All teams participated in all subtasks
with the exception of travel Greek, where only
two teams participated. An overview of core
system features is presented in Table 3. The
remainder of this section briefly describes each
of the submissions and then compares them. A
brief description for each system is provided in
the following paragraphs.
tucSage. The core of the tucSage system is
a combination of two components. The first
component is used for the selection of candidate
rule fragments from a corpus. Specifically, the
posterior probability of a candidate fragment
belonging to a rule is computed using a variety of
features. The feature set includes various lexical
features (e.g., the number of tokens), the fragment
perplexity computed using n-gram language
modeling, and features based on lexical similarity.
The second component is used for computing
the similarity between a candidate fragment and
a grammar rule. In total, two different types of
similarity metrics are used relying on the overlap
of character bigrams and contextual features.
These similarities are fused with the posterior
probabilities produced by the fragment selection
model. The contribution of the two components is
adjusted using an exponential weight.
SAIL-GRS. The SAIL-GRS system is based
on the well-established term frequency?inverse
document frequency (TF ?IDF ) measurement.
This metric is adapted to the present task by
considering each grammar rule as a ?document?.
For each rule, all its fragments are aggregated
13
System Use of Features Similarity External Language-
acronym machine learn. used metrics corpora specific
Baseline no lexical Longest Common no no
Substring
tucSage yes: lexical, perplexity, character overlap, web no
random forests similarity-based , heuristic cosine similarity documents
SAIL-GRS no lexical cosine similarity no no
Biel no lexical, expansion of cosine Wikipedia yes
low-level rules similarity articles
Table 3: Overview of the characteristics of the participating systems.
and the frequency of the respective n-grams
(constituents) is computed. The inverse document
frequency is casted as inverse rule frequency
and it is computed for the extracted n-grams.
The process is performed for both unigrams and
bigrams.
Biel. The fundamental idea behind the Biel
system is the encoding of domain semantics via
topic modeling. For this purpose a background
document space is constructed using thousands
of Wikipedia articles. Particular focus is given
to the transformation of the initial document
space according to the paradigm of explicit
semantic analysis. For each domain, a topic
space is defined and a language-specific function
is employed for the mapping of documents. In
essence, the mapping function is an association
measurement that is based on TF?IDF scores.
An approximation regarding the construction of
the topic space is investigated in order to reduce
data sparsity, while a number of normalization
schemes are also presented.
Overall, only the tucSage system employs a ma-
chine learning-based approach (random forests),
while an unsupervised approach is followed by the
SAIL-GRS and Biel systems. All systems exploit
lexical information extracted from rule fragments.
This information is realized as the lexical surface
form of the constituents of fragments. For ex-
ample, consider the ?depart for <CITY>? frag-
ment that corresponds to the high-level rule refer-
ring to the notion of departure city. The follow-
ing set of lexical features can be extracted from
the aforementioned fragment: (?depart?, ?from?,
?<CITY>?). Unlike the other systems, the Biel
system utilizes low-level rules to expand high-
level rules with terminal concept instances. For
example, the ?<CITY>? rule is not processed as
is, but it is represented as a list of city names
(?New York?, ?Boston?, . . . ). The most rich fea-
ture set is used by the tucSage system which com-
bines lexical, perplexity and similarity features
with a set of heuristic rules. All three systems
employ the widely-used cosine similarity metric.
Both SAIL-GRS and Biel systems rely solely on
this metric during the assignment of an unknown
fragment to a high-level rule. A more sophis-
ticated approach is presented by tucSage, where
first a classifier is built for every grammar rule,
computing the probability of a fragment belong-
ing to this rule and then the similarity between the
fragment and the rule is computed. Classification
is then performed by combining the two scores.
Also, another difference regarding the employ-
ment of the cosine similarity deals with the com-
putation of the vectorial feature values. A simple
binary scheme is used in the tucSage system, while
variations of the term frequency-inverse document
frequency scheme are used in SAIL-GRS and Biel.
Besides cosine similarity, a similarity metric based
on the overlap of character bigrams is used by the
tucSage system. External corpora (i.e., corpora
that were not provided as part of the official task
data) were used by the tucSage and Biel systems.
Such corpora were meant as an additional source
of information with respect to the domains under
investigation. Regarding tucSage, the training data
were exploited in order to construct web search
queries for harvesting a collection of web docu-
ments from which a number of sentences were se-
lected for corpus creation. In the case of the Biel
system, a set of Wikipedia articles was exploited.
Language specific resources where used for the
Biel system, while the other two teams used lan-
guage agnostic methods.
5 Results
The results for all participating teams and the
baseline system are given in Table 4. The tucSage
team submitted three runs, the first one being the
primary, indicated with an asterisk in the results.
14
Focusing on the weighted F-measure we see
that in all domains but the tourism English, at
least one submission manages to outperform the
baseline provided by the organizers. In travel En-
glish the baseline system achieves 0.51 weighted
f-measure, with two out of the three systems
achieving 0.68 and 0.58. The improvement over
the baseline is greater for the travel Greek sub-
task, where the baseline score of 0.26 is much
lower than the achieved 0.52 from tucSage. In the
tourism English subtask the best submitted sys-
tems managed to match the performance of the
baseline system, but not to exceed it. This can
be attributed to the good performance of the base-
line system, due to the fact that the tourism gram-
mar is composed of longer fragments than the rest,
helping the naive baseline system achieve top per-
formance exploiting lexical similarity only. We
can however assume that more complex systems
would beat the baseline if the test set fragments
were built using different lexicalizations, as would
be the case in unannotated data coming from de-
ployed SDS.
In the finance domain, even though the amount
of training data is quite smaller than in all other
subtasks the submitted systems still manage to
outperform the baseline system. This means that
the submitted systems display robust performance
both in resource-rich and resource-poor condi-
tions.
6 Conclusion
The tucSage and SAIL-GRS systems are shown to
be portable across domains and languages, achiev-
ing performance that exceeds the baseline for three
out of four datasets. The highest performance of
the tucSage system compared to the SAIL-GRS
system may be attributed to the use of a model for
fragment selection. Interestingly, the simple vari-
ation of the TF?IDF scheme used by the SAIL
system achieved very good results being a close
second performer. The UNIBI system proposed
a very interesting new application of the frame-
work of topic modeling to the task of grammar in-
duction, however, the respective performance does
not exceed the state-of-the-art. The combination
of the tucSage and SAIL-GRS systems could give
better results.
team Weighted Unweighted
Pr. Rec. F-m. Pr. Rec. F-m.
Travel English
Baseline 0.40 0.69 0.51 0.38 0.67 0.48
tucSage1
?
0.60 0.73 0.66 0.59 0.74 0.66
tucSage2 0.59 0.72 0.65 0.59 0.74 0.65
tucSage3 0.69 0.67 0.68 0.66 0.69 0.67
SAIL-GRS 0.54 0.62 0.58 0.57 0.66 0.61
Biel 0.13 0.39 0.20 0.09 0.34 0.14
Travel Greek
Baseline 0.17 0.65 0.26 0.16 0.73 0.26
tucSage1
?
0.47 0.58 0.52 0.55 0.72 0.62
tucSage2 0.46 0.53 0.49 0.50 0.59 0.54
tucSage3 0.51 0.48 0.49 0.52 0.56 0.54
SAIL-GRS 0.46 0.51 0.49 0.49 0.62 0.55
Biel - - - - - -
Tourism English
Baseline 0.80 0.94 0.87 0.82 0.94 0.87
tucSage1
?
0.79 0.94 0.86 0.76 0.91 0.83
tucSage2 0.78 0.93 0.85 0.73 0.90 0.80
tucSage3 0.80 0.93 0.86 0.77 0.90 0.83
SAIL-GRS 0.75 0.90 0.82 0.75 0.90 0.82
Biel 0.04 0.14 0.06 0.02 0.08 0.04
Finance English
Baseline 0.48 0.78 0.60 0.40 0.63 0.49
tucSage1
?
0.61 0.81 0.70 0.43 0.54 0.48
tucSage2 0.55 0.74 0.63 0.40 0.51 0.45
tucSage3 0.52 0.67 0.58 0.39 0.43 0.41
SAIL-GRS 0.78 0.78 0.78 0.67 0.62 0.65
Biel 0.22 0.30 0.25 0.06 0.18 0.09
Average over all four tasks
Baseline 0.46 0.73 0.56 0.44 0.74 0.53
tucSage1
?
0.62 0.77 0.69 0.58 0.73 0.65
tucSage2 0.60 0.73 0.66 0.56 0.69 0.61
tucSage3 0.63 0.69 0.65 0.59 0.65 0.61
SAIL-GRS 0.63 0.70 0.67 0.62 0.70 0.66
Biel 0.13 0.28 0.17 0.06 0.20 0.09
Table 4: Weighted and unweighted precision, re-
call and f-measure for all systems. Best perfor-
mance per metric and dataset shown in bold.
Acknowledgements
The task organizers wish to thank Maria Gian-
noudaki and Maria Vomva for the editing of the
hand-crafted grammars used in this evaluation
task. The authors would like to thank the anony-
mous reviewer for the valuable comments and sug-
gestions to improve the quality of the paper. This
work has been partially funded by the SpeDial and
PortDial projects, supported by the EU Seventh
Framework Programme (FP7), with grant number
611396 and 296170 respectively.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
15
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385?393.
Frederic Bechet, Benoit Favre, Alexis Nasr, and Math-
ieu Morey. 2014. Retrieving the syntactic structure
of erroneous ASR transcriptions for open-domain
spoken language understanding. In Proceedings of
the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 4125?4129.
Yonatan Bisk and Julia Hockenmaier. 2012. Simple
robust grammar induction with combinatory catego-
rial grammars. In Proceedings of the 26th Confer-
ence on Artificial Intelligence, pages 1643?1649.
Bart Cramer. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the 45th
annual meeting of the ACL: Student Research Work-
shop, pages 43?48.
Katerina T. Frantzi and Sophia Ananiadou. 1997. Au-
tomatic term recognition using contextual cues. In
Proceedings of the International Joint Conference
on Artificial Intelligence, pages 41?46.
Spiros Georgiladakis, Christina Unger, Elias Iosif,
Sebastian Walter, Philipp Cimiano, Euripides Pe-
trakis, and Alexandros Potamianos. 2014. Fusion
of knowledge-based and data-driven approaches to
grammar induction. In Proceedings of Interspeech
(accepted).
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of Interspeech, pages
1609?1612.
Elias Iosif, Athanasios Tegos, Apostolos Pangos, Eric
Fosler-Lussier, and Alexandros Potamianos. 2006.
Unsupervised combination of metrics for semantic
class induction. In Proceedings of the International
Workshop on Spoken Language Technology (SLT),
pages 86?89.
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spiros Georgiladakis, and Gianluca Mameli. 2013.
Web data harvesting for speech understanding gram-
mar induction. In Proceedings of Interspeech, pages
2733?2737.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd an-
nual meeting of the ACL, pages 470?477.
Helen M. Meng and Kai-chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Dmitrijs Milajevs and Matthew Purver. 2014. Inves-
tigating the contribution of distributional semantic
information for dialogue act classification. In Pro-
ceedings of the 2nd Workshop on Continuous Vec-
tor Space Models and their Compositionality, pages
40?47.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proceedings of Interspeech, pages 9?12.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the ACL, pages 311?318.
Andrew N. Pargellis, Eric Fosler-Lussier, Chin-Hui
Lee, Alexandros Potamianos, and Augustine Tsai.
2004. Auto-induced semantic classes. Speech Com-
munication, 43(3):183?203.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw
text with cascaded finite state models. In Proceed-
ings of the 49th annual meeting of the ACL, pages
1077?1086.
Portdial. 2014a. PortDial project, final
report on automatic grammar induction
and evaluation D3.3. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Portdial. 2014b. PortDial project, free
data deliverable D3.2. Technical report,
https://sites.google.com/site/portdial2/deliverables-
publications.
Aarne Ranta. 2004. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Ruhi Sarikaya. 2008. Rapid bootstrapping of statisti-
cal spoken dialogue systems. Speech Communica-
tion, 50(7):580?593.
Abhinav Sethy, Shrikanth S. Narayanan, and Bhuvana
Ramabhadran. 2007. Data driven approach for lan-
guage model adaptation using stepwise relative en-
tropy minimization. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 177?180.
Ye-Yi Wang and Alex Acero. 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3-4):390?416.
16
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512?516,
Dublin, Ireland, August 23-24, 2014.
SAIL: Sentiment Analysis using Semantic Similarity and Contrast
Features
Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni,
Alexandros Potamianos, Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
{malandra,mfalcone,cvaz,jbisogni}@usc.edu,
potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission to Se-
mEval2014 Task 9: Sentiment Analysis in
Twitter. Our model is primarily a lexi-
con based one, augmented by some pre-
processing, including detection of Multi-
Word Expressions, negation propagation
and hashtag expansion and by the use of
pairwise semantic similarity at the tweet
level. Feature extraction is repeated for
sub-strings and contrasting sub-string fea-
tures are used to better capture complex
phenomena like sarcasm. The resulting
supervised system, using a Naive Bayes
model, achieved high performance in clas-
sifying entire tweets, ranking 7th on the
main set and 2nd when applied to sarcastic
tweets.
1 Introduction
The analysis of the emotional content of text is
relevant to numerous natural language process-
ing (NLP), web and multi-modal dialogue appli-
cations. In recent years the increased popularity
of social media and increased availability of rele-
vant data has led to a focus of scientific efforts on
the emotion expressed through social media, with
Twitter being the most common subject.
Sentiment analysis in Twitter is usually per-
formed by combining techniques used for related
tasks, like word-level (Esuli and Sebastiani, 2006;
Strapparava and Valitutti, 2004) and sentence-
level (Turney and Littman, 2002; Turney and
Littman, 2003) emotion extraction. Twitter how-
ever does present specific challenges: the breadth
of possible content is virtually unlimited, the writ-
ing style is informal, the use of orthography and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
grammar can be ?unconventional? and there are
unique artifacts like hashtags. Computation sys-
tems, like those submitted to SemEval 2013 task
2 (Nakov et al., 2013) mostly use bag-of-words
models with specific features added to model emo-
tion indicators like hashtags and emoticons (Davi-
dov et al., 2010).
This paper describes our submissions to Se-
mEval 2014 task 9 (Rosenthal et al., 2014), which
deals with sentiment analysis in twitter. The sys-
tem is an expansion of our submission to the same
task in 2013 (Malandrakis et al., 2013a), which
used only token rating statistics as features. We
expanded the system by using multiple lexica and
more statistics, added steps to the pre-processing
stage (including negation and multi-word expres-
sion handling), incorporated pairwise tweet-level
semantic similarities as features and finally per-
formed feature extraction on substrings and used
the partial features as indicators of irony, sarcasm
or humor.
2 Model Description
2.1 Preprocessing
POS-tagging / Tokenization was performed
using the ARK NLP tweeter tagger (Owoputi et
al., 2013), a Twitter-specific tagger.
Negations were detected using the list from
Christopher Potts? tutorial. All tokens up to the
next punctuation were marked as negated.
Hashtag expansion into word strings was per-
formed using a combination of a word insertion
Finite State Machine and a language model. A
normalized perplexity threshold was used to
detect if the output was a ?proper? English string
and expansion was not performed if it was not.
Multi-word Expressions (MWEs) were detected
using the MIT jMWE library (Kulkarni and
Finlayson, 2011). MWEs are non-compositional
expressions (Sag et al., 2002), which should be
512
handled as a single token instead of attempting to
reconstruct their meaning from their parts.
2.2 Lexicon-based features
The core of the system was formed by the lexicon-
based features. We used a total of four lexica and
some derivatives.
2.2.1 Third party lexica
We used three third party affective lexica.
SentiWordNet (Esuli and Sebastiani, 2006) pro-
vides continuous positive, negative and neutral rat-
ings for each sense of every word in WordNet.
We created two versions of SentiWordNet: one
where ratings are averaged over all senses of a
word (e.g., one ratings for ?good?) and one where
ratings are averaged over lexeme-pos pairs (e.g.,
one rating for the adjective ?good? and one for the
noun ?good?).
NRC Hashtag (Mohammad et al., 2013) Senti-
ment Lexicon provides continuous polarity ratings
for tokens, generated from a collection of tweets
that had a positive or a negative word hashtag.
Sentiment140 (Mohammad et al., 2013) Lexi-
con provides continuous polarity ratings for to-
kens, generated from the sentiment140 corpus of
1.6 million tweets, with emoticons used as posi-
tive and negative labels.
2.2.2 Emotiword: expansion and adaptation
To create our own lexicon we used an automated
algorithm of affective lexicon expansion based on
the one presented in (Malandrakis et al., 2011;
Malandrakis et al., 2013b), which in turn is an ex-
pansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence, arousal and dominance ratings of any term
t
j
can be represented as a linear combination of
its semantic similarities d
ij
to a set of seed words
w
i
and the known affective ratings of these words
v(w
i
), as follows:
v?(t
j
) = a
0
+
N
?
i=1
a
i
v(w
i
) d
ij
, (1)
where a
i
is the weight corresponding to seed word
w
i
(that is estimated as described next). For the
purposes of this work, d
ij
is the cosine similarity
between context vectors computed over a corpus
of 116 million web snippets (up to 1000 for each
word in the Aspell spellchecker) collected using
the Yahoo! search engine.
Given the starting, manually annotated, lexi-
con Affective Norms for English Words (Bradley
and Lang, 1999) we selected 600 out of the 1034
words contained in it to serve as seed words and
all 1034 words to act as the training set and used
Least Squares Estimation to estimate the weights
a
i
. Seed word selection was performed by a sim-
ple heuristic: we want seed words to have extreme
affective ratings (high absolute value) and the set
to be close to balanced (sum of seed ratings equal
to zero). The equation learned was used to gener-
ate ratings for any new terms.
The lexicon created by this method is task-
independent, since both the starting lexicon and
the raw text corpus are task-independent. To cre-
ate task-specific lexica we used corpus filtering on
the 116 million sentences to select ones that match
our domain, using either a normalized perplex-
ity threshold (using a maximum likelihood trigram
model created from the training set tweets) or a
combination of pragmatic constraints (keywords
with high mutual information with the task) and
perplexity threshold (Malandrakis et al., 2014).
Then we re-calculated semantic similarities on the
filtered corpora. In total we created three lexica: a
task-independent (base) version and two adapted
versions (filtered by perplexity alone and filtered
by combining pragmatics and perplexity), all con-
taining valence, arousal and dominance token rat-
ings.
2.2.3 Statistics extraction
The lexica provide up to 17 ratings for each to-
ken. To extract tweet-level features we used sim-
ple statistics and selection criteria. First, all token
unigrams and bigrams contained in a tweet were
collected. Some of these n-grams were selected
based on a criterion: POS tags, whether a token is
(part of) a MWE, is negated or was expanded from
a hashtag. The criteria were applied separately
to token unigrams and token bigrams (POS tags
only applied to unigrams). Then ratings statistics
were extracted from the selected n-grams: length
(cardinality), min, max, max amplitude, sum, av-
erage, range (max minus min), standard deviation
and variance. We also created normalized versions
by dividing by the same statistics calculated over
all tokens, e.g., the maximum of adjectives over
the maximum of all unigrams. The results of this
process are features like ?maximum of Emotiword
valence over unigram adjectives? and ?average of
SentiWordNet objectivity among MWE bigrams?.
513
2.3 Tweet-level similarity ratings
Our lexicon was formed under the assumption
that semantic similarity implies affective similar-
ity, which should apply to larger lexical units like
entire tweets. To estimate semantic similarity
scores between tweets we used the publicly avail-
able TakeLab semantic similarity toolkit (
?
Sari?c et
al., 2012) which is based on a submission to Se-
mEval 2012 task 6 (Agirre et al., 2012). We used
the data of SemEval 2012 task 6 to train three
semantic similarity models corresponding to the
three datasets of that task, plus an overall model.
Using these models we created four similarity rat-
ings between each tweet of interest and each tweet
in the training set. These similarity ratings were
used as features of the final model.
2.4 Character features
Capitalization features are frequencies and rela-
tive frequencies at the word and letter level, ex-
tracted from all words that either start with a capi-
tal letter, have a capital letter in them (but the first
letter is non-capital) or are in all capital letters.
Punctuation features are frequencies, relative fre-
quencies and punctuation unigrams.
Character repetition features are frequencies,
relative frequencies and longest string statistics of
words containing a repetition of the same letter.
Emoticon features are frequencies, relative fre-
quencies, and emoticon unigrams.
2.5 Contrast features
Cognitive Dissonance is an important phe-
nomenon associated with complex linguistic cases
like sarcasm, irony and humor (Reyes et al., 2012).
To estimate it we used a simple approach, inspired
by one-liner joke detection: we assumed that the
final few tokens of each tweet (the ?suffix?) con-
trast the rest of the tweet (the ?prefix?) and created
split versions of the tweet where the last N tokens
are the suffix and all other tokens are the prefix,
for N = 2 and N = 3. We repeated the fea-
ture extraction process for all features mentioned
above (except for the semantic similarity features)
for the prefix and suffix, nearly tripling the total
number of features.
2.6 Feature selection and Training
The extraction process lead to tens of thousands
of candidate features, so we performed forward
stepwise feature selection using a correlation crite-
Table 1: Performance and rank achieved by our
submission for all datasets of subtasks A and B.
task dataset avg. F1 rank
A
LJ2014 70.62 16
SMS2013 74.46 16
TW2013 78.47 14
TW2014 76.89 13
TW2014SC 65.56 15
B
LJ2014 69.34 15
SMS2013 56.98 24
TW2013 66.80 10
TW2014 67.77 7
TW2014SC 57.26 2
rion (Hall, 1999) and used the resulting set of 222
features to train a model. The model chosen is a
Naive Bayes tree, a tree with Naive Bayes clas-
sifiers on each leaf. The motivation comes from
considering this a two stage problem: subjectivity
detection and polarity classification, making a hi-
erarchical model a natural choice. The feature se-
lection and model training/classification was con-
ducted using Weka (Witten and Frank, 2000).
Table 2: Selected features for subtask B.
Features number
Lexicon-derived 178
By lexicon
Ewrd / S140 / SWNet / NRC 71 / 53 / 33 / 21
By POS tag
all (ignore tag) 103
adj / verb / proper noun 25 / 11 / 11
other tags 28
By function
avg / min / sum / max 45 / 40 / 38 / 26
other functions 29
Semantic similarity 29
Punctuation 7
Emoticon 5
Other features 3
Contrast 72
prefix / suffix 54 / 18
3 Results
We took part in subtasks A and B of SemEval
2014 task 9, submitting constrained runs trained
with the data the task organizers provided. Sub-
task B was the priority and the subtask A model
was created as an afterthought: it only uses the
lexicon-based and morphology features for the tar-
get string and the entire tweet as features of an NB
Tree.
The overall performance of our submission
on all datasets (LiveJournal, SMS, Twitter 2013,
Twitter 2014 and Twitter 2014 Sarcasm) can be
seen in Table 1. The subtask A system performed
514
Table 3: Performance on all data sets of subtask B after removing 1 set of features. Performance differ-
ence with the complete system listed if greater than 1%.
Features removed
LJ2014 SMS2013 TW2013 TW2014 TW2014SC
avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff avg. F1 diff
None (Submitted) 69.3 57.0 66.8 67.8 57.3
Lexicon-derived 43.6 -25.8 38.2 -18.8 49.5 -17.4 51.5 -16.3 43.5 -13.8
Emotiword 67.5 -1.9 56.4 63.5 -3.3 66.1 -1.7 54.8 -2.5
Base 68.4 56.3 65.0 -1.9 66.4 -1.4 59.6 2.3
Adapted 69.3 57.4 66.7 67.5 50.8 -6.5
Sentiment140 68.1 -1.3 54.5 -2.5 64.4 -2.4 64.2 -3.6 45.4 -11.9
NRC Tag 70.6 1.3 58.5 1.6 66.3 66.0 -1.7 55.3 -2.0
SentiWordNet 68.7 56.0 66.2 68.1 52.7 -4.6
per Lexeme 69.3 56.7 66.1 68.0 52.7 -4.5
per Lexeme-POS 68.8 57.1 66.7 67.4 55.0 -2.2
Semantic Similarity 69.0 58.2 1.2 64.9 -2.0 65.5 -2.2 52.2 -5.0
Punctuation 69.7 57.4 66.6 67.1 53.9 -3.4
Emoticon 69.3 57.0 66.8 67.8 57.3
Contrast 69.2 57.5 66.7 67.0 51.9 -5.4
Prefix 69.5 57.2 66.8 67.2 47.4 -9.9
Suffix 68.6 57.2 66.5 67.9 56.3
badly, ranking near the bottom (among 20 submis-
sions) on all datasets, a result perhaps expected
given the limited attention we gave to the model.
The subtask B system did very well on the three
Twitter datasets, ranking near the top (among 42
teams) on all three sets and placing second on the
sarcastic tweets set, but did notably worse on the
two non-Twitter sets.
A compact list of the features selected by the
subtask B system can be seen in Table 2. The ma-
jority of features (178 of 222) are lexicon-based,
29 are semantic similarities to known tweets and
the rest are mainly punctuation and emoticon fea-
tures. The lexicon-based features mostly come
from Emotiword, though that is probably because
Emotiword contains a rating for every unigram
and bigram in the tweets, unlike the other lexica.
The most important part-of-speech tags are adjec-
tives and verbs, as expected, with proper nouns
being also highly important, presumably as indi-
cators of attribution. Still, most features are cal-
culated over all tokens (including stop words). Fi-
nally it is worth noting the 72 contrast features se-
lected.
We also conducted a set of experiments using
partial feature sets: each time we use all features
minus one set, then apply feature selection and
classification. The results are presented in Ta-
ble 3. As expected, the lexicon-based features are
the most important ones by a wide margin though
the relative usefulness of the lexica changes de-
pending on the dataset: the twitter-specific NRC
lexicon actually hurts performance on non-tweets,
while the task-independent Emotiword hurts per-
formance on the sarcastic tweets set. Overall
though using all is the optimal choice. Among the
other features only semantic similarity provides a
relatively consistent improvement.
A lot of features provide very little benefit on
most sets, but virtually everything is important for
the sarcasm set. Lexica, particularly the twitter
specific ones like Sentiment 140 and the adapted
version of Emotiword make a big difference, per-
haps indicating some domain-specific aspects of
sarcasm expression (though such assumptions are
shaky at best due to the small size of the test
set). The contrast features perform their intended
function well, providing a large performance boost
when dealing with sarcastic tweets and perhaps
explaining our high ranking on that dataset.
Overall the subtask B system performed very
well and the semantic similarity features and con-
trast features provide potential for further growth.
4 Conclusions
We presented a system of twitter sentiment anal-
ysis combining lexicon-based features with se-
mantic similarity and contrast features. The sys-
tem proved very successful, achieving high ranks
among all competing systems in the tasks of senti-
ment analysis of generic and sarcastic tweets.
Future work will focus on the semantic similar-
ity and contrast features by attempting more accu-
rately estimate semantic similarity and using some
more systematic way of identifying the ?contrast-
ing? text areas.
515
References
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In proc. Se-
mEval, pages 385?393.
Margaret Bradley and Peter Lang. 1999. Affective
Norms for English Words (ANEW): Stimuli, in-
struction manual and affective ratings. technical re-
port C-1. The Center for Research in Psychophysi-
ology, University of Florida.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proc. COLING, pages 241?249.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. In Proc. LREC, pages 417?422.
Mark A. Hall. 1999. Correlation-based feature selec-
tion for machine learning. Ph.D. thesis, The Univer-
sity of Waikato.
Nidhi Kulkarni and Mark Alan Finlayson. 2011.
jMWE: A java toolkit for detecting multi-word ex-
pressions. In proc. Workshop on Multiword Expres-
sions, pages 122?124.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2011. Kernel mod-
els for affective lexicon creation. In Proc. Inter-
speech, pages 2977?2980.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexandros
Potamianos, and Shrikanth Narayanan. 2013a.
SAIL: A hybrid approach to sentiment analysis. In
proc. SemEval, pages 438?442.
Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013b. Distri-
butional semantic models for affective text analy-
sis. Audio, Speech, and Language Processing, IEEE
Transactions on, 21(11):2379?2392.
Nikolaos Malandrakis, Alexandros Potamianos,
Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng,
Gerald C. Davison, and Shrikanth Narayanan. 2014.
Affective language model adaptation via corpus
selection. In proc. ICASSP, pages 4871?4874.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In proc. Se-
mEval, pages 321?327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment analysis in
Twitter. In Proc. SemEval, pages 312?320.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
proc. NAACL, pages 380?390.
Antonio Reyes, Paolo Rosso, and Davide Buscaldi.
2012. From humor recognition to irony detection:
The figurative language of social media. Data &
Knowledge Engineering, 74(0):1 ? 12.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proc. SemEval.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
volume 2276 of Lecture Notes in Computer Science,
pages 189?206.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-Affect: an affective extension of WordNet.
In Proc. LREC, volume 4, pages 1083?1086.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. technical report ERC-
1094 (NRC 44929). National Research Council of
Canada.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21:315?346.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Systems
for measuring semantic text similarity. In proc. Se-
mEval, pages 441?448.
Ian H. Witten and Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann.
516
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668?672,
Dublin, Ireland, August 23-24, 2014.
tucSage: Grammar Rule Induction for Spoken Dialogue Systems via
Probabilistic Candidate Selection
Arodami Chorianopoulou
?
, Georgia Athanasopoulou
?
, Elias Iosif
? ?
,
Ioannis Klasinas
?
, Alexandros Potamianos
?
?
School of ECE, Technical University of Crete, Chania 73100, Greece
?
School of ECE, National Technical University of Athens, Zografou 15780, Greece
?
?Athena? Research Center, Marousi 15125, Greece
{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.gr
iosife@telecom.tuc.gr, apotam@gmail.com
Abstract
We describe the grammar induction sys-
tem for Spoken Dialogue Systems (SDS)
submitted to SemEval?14: Task 2. A sta-
tistical model is trained with a rich fea-
ture set and used for the selection of can-
didate rule fragments. Posterior probabil-
ities produced by the fragment selection
model are fused with estimates of phrase-
level similarity based on lexical and con-
textual information. Domain and language
portability are among the advantages of
the proposed system that was experimen-
tally validated for three thematically dif-
ferent domains in two languages.
1 Introduction
A critical task for Spoken Dialogue Systems
(SDS) is the understanding of the transcribed user
input, that utilizes an underlying domain grammar.
An obstacle to the rapid deployment of SDS to
new domains and languages is the time-consuming
development of grammars that require human ex-
pertise. Machine-assisted grammar induction has
been an open research area for decades (K. Lari
and S. Young, 1990; S. F. Chen, 1995) aiming
to lower this barrier. Induction algorithms can
be broadly distinguished into resource-based, e.g.,
(A. Ranta, 2004), and data-driven, e.g., (H. Meng
and K.-C. Siu, 2002). The main drawback of
the resource-based paradigm is the requirement of
pre-existing knowledge bases. This is addressed
by the data-driven paradigm that relies (mostly)
on plain corpora. SDS grammars are built by uti-
lizing low- and high-level rules. Low-level rules
This work is licenced under a Creative Commons Attri-
bution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
are similar to gazetteers consisting of terminal en-
tries, e.g., list of city names. High-level rules can
be lexicalized as textual fragments (or chunks),
which are semantically defined on top of low-
level rules, e.g., ?depart from <City>?.
The data-driven induction of low-level rules is a
well-researched area enabled by various technolo-
gies including web harvesting for corpora creation
(Klasinas et al., 2013), term extraction (K. Frantzi
and S. Ananiadou, 1997), word-level similarity
computation (Pargellis et al., 2004) and cluster-
ing (E. Iosif and A. Potamianos, 2007). High-level
rule induction is a less researched area that poses
two main challenges: 1) the extraction and selec-
tion of salient candidate fragments from a corpus
that convey semantics relevant to the domain of in-
terests and 2) the organization of such fragments
(e.g., via clustering) according to their semantic
similarity. Despite the recent interest on phrase (J.
Mitchell and M. Lapata, 2010) and sentence simi-
larity, each respective problem remains open.
Next, our submission
1
for the Se-
mEval?14: Task2 is briefly described, which
constitutes a data-driven approach for inducing
high-level SDS grammar rules. At the system?s
core lies a statistical model for the selection of
textual fragments based on a rich set of features.
This set includes various lexical features, aug-
mented with statistics from n-gram language
models, as well as with heuristic features. The
candidate selection model posterior is fused
with a phrase-level semantic similarity metric.
Two different approaches are used for similarity
computation relying on the overlap of character
bigrams or context-based similarity according
to the distributional hypothesis of meaning.
The domain and language portability of the
proposed system is demonstrated by its successful
application across three different domains and
1
Please note that the last three authors of this submission
are among the organizers of this task.
668
two languages. All the four subtasks defined by
the organizers were completed with very good
performance that exceeds the baseline.
2 System Description
The basic functionality of the proposed system
is the mapping (assignment) of unknown textual
fragments into known high-level grammar rules.
Let E be the set of unknown fragments, while the
set of known rules is denoted byR. Each unknown
fragment f ?E is allowed to be mapped to a sin-
gle high-level rule r
s
?R, where 1? s? m and
m is the total number of rules in the grammar.
Figure 1: Overview of system architecture.
The system consists of three major components as
shown at the system architecture diagram in Fig.
1, specifically: 1) candidate selection: a set of
classifiers is built, one for each r
s
to select whether
f ? E is a candidate member of the specific rule
2
,
2) similarity computation between f and r
s
, and
3) mapping f to a high-level rule r
s
(denoted as
f 7? r
s
) according to the following model:
argmax
s
{p(r
s
|f)
w
S(f, r
s
)} : f 7? r
s
(1)
where p(r
s
|f) stands for the probability of f
belonging to rule r
s
and it is estimated via the
respective classifier. The similarity between
f and r
s
is denoted by S(f |r
s
), while w is
a fixed weight taking values in the interval
[0 ?). The fusion weight w controls the rela-
tive importance of the candidate selection and
semantic similarity modules, e.g., for w = 0
only the similarity metric S(f, r
s
) is used in the
decision. For example, consider the fragment f
?leaving <City>?. Also, assume two high-
level rules, namely, <ArrCity>={?arrive
2
The requirement for building a classifier for each gram-
mar rule is realistic for the case of SDS, especially for the typ-
ical iterative human-in-the-loop grammar development sce-
nario.
at <City>?,...} and <DepCity>=
{?depart <City>?,...}. According to (1)
f is mapped to the <DepCity> rule.
2.1 Candidate Selection
In this section, the features used for building the
candidate selection module for each r
s
? R are
briefly described. Given a pair (f ,r
s
) a two-class
statistical classification model that corresponds to
r
s
is used for estimating p(r
s
|f) in (1).
Definitions. A high-level rule r
s
can be con-
sidered as a set of fragments, e.g.,?depart
<City>?, ?leaving <City>?. For each
fragment there are two types of constituents,
namely, lexical (e.g., ?depart?,?leaving?)
and low-level rules (e.g., ?<City>?). The fol-
lowing features are extracted for r
s
considering its
respective fragments, as well as for f .
Shallow features. 1) the number of constituents
(i.e., tokens), 2) the count of lexical constituents
to the number of tokens, 3) the count of low-level
rules to the number of tokens, 4) the count of lex-
ical constituents that follow the right-most low-
level rule of the fragment, and 5) the count of low-
level rules that appear twice in a fragment.
Perplexity-based features. A fragment
?
f can
be represented as a sequence of tokens as
w
1
w
2
... w
z
. The perplexity of
?
f is defined as
PP (
?
f)=2
H(
?
f)
, where H(
?
f)=
1
z
log(p(
?
f)). p(
?
f)
stands for the probability of
?
f estimated using an
n-gram language model. Two PP values were
used as features computed for n=2, 3.
Features of lexical similarity. Four scores of lex-
ical similarity computed between f and r
s
were
used as features. Let N
s
denote the set of frag-
ments that are included in the training set of each
rule r
s
. The following metrics were employed
for computing the similarity between the unknown
fragment f and a fragment f
s
? N
s
: 1) the nor-
malized longest common subsequence (Stoilos et
al., 2005) denoted as S
C
, 2) the normalized over-
lap in character bigrams that is denoted as S
B
and
it is defined in (2), 3) a proposed variation of the
Levenshtein distance, S
L
, defined as S
L
(f, f
s
) =
l
1
?L(f,f
s
)
l
1
+d
, where l
1
and l
2
are the lengths (in char-
acters) of the lengthiest and the shortest fragment
between f and f
s
, respectively, while d= l
1
? l
2
.
L(.) stands for the Levenshtein distance (V. I. Lev-
enshtein, 1966; R. A. Wagner and M. J. Fischer,
1974). 4) if f and f
s
differ by one token exactly
S
L
is applied, otherwise their similarity is set to
0. Regarding S
C
and S
B
, the similarity between
669
f and r
s
was estimated as the maximum similarity
yielded when computing the similarities between
f and each f
s
?N
s
. For the rest metrics, the sim-
ilarity between f and r
s
was estimated by averag-
ing the |N
s
| similarities computed between f and
each f
s
?N
s
.
Heuristic features. Considering an unknown
fragment f and the set of training fragments N
s
corresponding to rule r
s
, in total nine features
were used: 1) the difference between the aver-
age length (in tokens) of fragments in N
s
and the
length of f , 2) the difference between the average
number of low-level rules in N
s
and the number
of low-level rules in f , 3) as 2) but considering
the lexical constituents instead of low-level rules,
4) the number of low-level rules shared between
N
s
and f , 5) as 4) but considering the lexical con-
stituents instead of low-level rules, 6) a boolean
function that equals 1 if f is a substring of at least
one f
s
? N
s
, 7) a boolean function that equals 1 if
f shares the same lexical constituents at least one
f
s
? N
s
, 8) a boolean function that equals 1 if f
is shorter by one token compared to any f
s
? N
s
,
9) a boolean function that equals 1 if f is lengthier
by one token compared to any f
s
? N
s
.
Selection. The aforementioned features are used
for building a binary classifier for each r
s
? R,
where 1 ? s ? m, for deciding whether f can
be regarded as a candidate member of r
s
or not.
Given an unknown fragment f these classifiers are
employed for estimating in total m probabilities
p(r
s
|f).
2.2 Similarity Metrics
Here, two types of similarity metrics are defined,
which are used for estimating S(f, r
s
) in (1).
String-based similarity. Consider two fragments
f
i
and f
j
whose sets of character bigrams are de-
noted as M
i
and M
j
, respectively. Also, M
min
=
min(|M
i
|, |M
j
|) and M
max
= max(|M
i
|, |M
j
|
). The similarity between f
i
and f
j
is based on
the overlap of their respective character bigrams
defined as (Jimenez et al., 2012):
S
B
(f
i
, f
j
) =
|M
i
?M
j
|
?M
max
+ (1? ?)M
min
, (2)
where 0??? 1, while, here we use ?=0.5. The
similarity between a fragment f and a rule r
s
is
computed by averaging the similarities computed
between f and each f
s
?N
s
.
Context-based similarity. This is a corpus-based
metric relying on the distributional hypothesis of
meaning suggesting that similarity of context im-
plies similarity of meaning (Z. Harris, 1954). A
contextual window of size 2K+1 words is cen-
tered on the fragment of interest f
i
and lexical
features are extracted. For every instance of f
i
in
the corpus the K words left and right of f
i
for-
mulate a feature vector v
i
. For a given value of K
the context-based semantic similarity between two
fragments, f
i
and f
j
, is computed as the cosine of
their feature vectors: S
K
(f
i
, f
j
) =
v
i
.v
j
||v
i
|| ||v
j
||
. The
elements of feature vectors can be weighted ac-
cording various schemes (E. Iosif and A. Potami-
anos, 2010), while, here we use a binary scheme.
The similarity between a fragment f and a rule
r
s
is computed by averaging the similarities com-
puted between f and each f
s
?N
s
.
2.3 Mapping of Unknown Fragments
The output of the described system is the mapping
of a fragment f to a single (i.e., one-to-one assign-
ment) high-level rule r
s
? R, where 1 ? s ? m.
This is achieved by applying (1). The p(r
s
|f)
probabilities were estimated as described in Sec-
tion 2.1. The S(f, r
s
) similarities were estimated
using either S
K
or S
B
defined in Section 2.2.
3 Datasets and Experiments
Datasets. The data was organized with respect to
three different domains: 1) air travel (flight book-
ing, car rental etc.), 2) tourism (information for
city guide), and 3) finance (currency exchange). In
total, there are four separate datasets: two datasets
for the air travel domain in English (EN) and
Greek (GR), one dataset for the tourism domain
in English, and one dataset for the finance domain
in English.
The number of high-level rules for each dataset
Domain #rules #train frag. #test frag.
Travel:EN 32 982 284
Travel:GR 35 956 324
Tourism:EN 24 1004 285
Finance:EN 9 136 37
Table 1: Number of rules and train/test fragments.
are shown in Table 1, along with the number
of fragments included in training and test data.
Experiments. Regarding the computation of
perplexity-based features (defined in Section 2.1)
the SRILM toolkit (A. Stolcke, 2002) was used.
The n-gram probabilities were estimated over a
corpus that was created by aggregating all the
670
valid fragments included in the training data.
For the computation of the context-based similar-
ity metric S
K
(defined in Section 2.2) a corpus
of web-harvested data was created for each do-
main/language. The context window size K was
Domain # sentences
Travel:EN 5721
Travel:GR 6359
Tourism:EN 829516
Finance:EN 168380
Table 2: Size of corpora used in S
K
metric.
set to 1. The size of the used corpora are presented
Table 2, while the process of corpus creation is
detailed in (Klasinas et al., 2013). The classifiers
used for the candidate selection module, described
in Section 2.1 were random forests with 50 trees
(L. Breiman, 2001).
4 Evaluation Metrics and Results
The proposed model defined by (1) was evaluated
in terms of weighted F-measure, (FM ). Initially,
we run our system using the training and develop-
ment set provided by the task organizers, in order
to tune the w and K parameters. The tuning was
conducted on the Travel English domain, while the
respective evaluation results are shown in Table 3
in terms of FM . We observe that the best re-
Weight w 0 1 50 500
FM 0.68 0.72 0.70 0.72
Table 3: Results for the tuning of w.
sults are achieved for w = 1 and w = 500. In
the case where w = 0 the rule mapping relies only
on the similarity metric. In addition, we exper-
imented with various values the context window
size K of the context-based similarity metric S
K
:
K = 1, 3, 7. For all values of K similar perfor-
mance was obtained (0.70). Given the aforemen-
Domains Baseline Run 1 Run 2 Run 3
Travel:EN 0.51 0.66 0.65 0.68
Travel:GR 0.26 0.52 0.49 0.49
Tourism:EN 0.87 0.86 0.85 0.86
Finance:EN 0.60 0.70 0.63 0.58
UA 0.56 0.69 0.66 0.65
WA 0.52 0.66 0.64 0.65
Table 4: Official results.
tioned tuning the following values were selected
for the official runs: w = 1, w = 500 and K = 1.
In total, three system runs were submitted:
Run 1. The character bigram similarity metric was
used, while w was set to 1.
Run 2. The context-based similarity metrics was
used with K = 1, while w was set to 1.
Run 3. The character bigram similarity metric was
used, while w was set to 500.
The results for the aforementioned runs, along
with the baseline performance are shown in Ta-
ble 4. An overview of the participating systems
suggests that our submission achieved the high-
est performance for almost all domains and lan-
guages. The weighted (WA) and unweighted (UA)
average across the 4 datasets are also presented,
where the weight depends on the number of rules
in the dataset. Using these measures, our main
run (Run 1) obtained the best results. We ob-
serve that the performance is consistently worse
for Runs 2 and 3, with the exception of the Travel
English dataset. Comparing the performance of
Runs 1 and 2, we observe that the character bigram
metric consistently outperforms the context-based
one. For individual datasets, our system underper-
forms for the Finance (in Run 3) and the Tourism
domain (in all Runs). For the case of the Finance
domain this may be attributed to the relatively lim-
ited training data.
5 Conclusions
We proposed a supervised grammar induction sys-
tem using the fusion of a grammar fragment se-
lection and similarity estimation modules. The
best configuration of our system was Run 1 which
achieved the highest performance compared to
other submissions, in almost all domains. To sum-
marize, 1) the selection module boost the sys-
tem?s performance significanlty, 2) the high per-
formance in different domains is a promising indi-
cator for domain and language portability. Future
work should involve the implementation of more
complex features for the candidate selection algo-
rithm and further investigation of phrase level sim-
ilarity metrics.
Acknowledgements
This work has been partially funded by the
projects: 1) SpeDial, and 2) PortDial, supported
by the EU Seventh Framework Programme (FP7),
with grant number 611396 and 296170, respec-
tively.
671
References
Elias Iosif and Alexandros Potamianos. 2010. Un-
supervised semantic similarity computation between
terms using web documents. IEEE Transactions on
Knowledge and Data Engineering, 22(11), pp. 1637-
1647.
Sergio Jimenez, Claudia Becerra and Alexander Gel-
bukh. 2012. Soft Cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pp. 449-453
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spyros Georgiladakis and Gianluka Mameli. 2013.
Web data harvesting for speech understanding
grammar induction. in Proceedings of the Inter-
speech.
Helen M. Meng and Kai-Chung Siu 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1), pp. 172-181.
PortDial Project free data deliverable D3.1.
https://sites.google.com/site/portdial2/deliverables-
publication
Andreas Stolcke 2002 Srilm-an extensible language
modeling toolkit in Proceedings of the Interspeech
2002
Karim Lari and Steve J. Young 2002. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1), pp. 35-56.
Stanley F. Chen 1995. Bayesian grammar induction
for language modeling. in Proceedings of the 33rd
annual meeting of ACL
Zellig Harris 1954. Distributional structure. Word,
10(23), pp. 146-162.
Rebecca Hwa 1999. Supervised grammar induction
using training data with limited constituent informa-
tion. in Proceedings of the 37th annual meeting of
ACL
Matthew Lease, Eugene Charniak, and Mark Johnson
2005. Parsing and its applications for conversa-
tional speech. in Proceedings of Acoustics, Speech,
and Signal Processing (ICASSP)
Vladimir I. Levenshtein 1966. Binary codes capable
of correcting deletions, insertions and reversals. in
Soviet physics doklady, 10(8), pp. 707-710.
Leo Breiman 2001. Random forests. in Machine
Learning, 45(1), pp. 5-32.
Dan Jurafsky and James H. Martin 2009. Speech
and language processing an introduction to natural
language processing, computational linguistics, and
speech. Pearson Education Inc
Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias
2005. A string metric for ontology alignment. in
The Semantic WebISWC, pp. 624637
Robert A. Wagner and Michael J. Fisher 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1), pp. 168-173
Katerina Frantzi and Sophia Ananiadou 1997. Au-
tomatic term recognition using contextual cues. in
Proceedings of International Joint Conferences on
Artificial Intelligence
Elias Iosif and Alexandros Potamianos 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. in Proceedings of Interspeech
Jeffrey Mitchell and Mirela Lapata 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388-1429.
Ye-Yi Wang and Alex Acero 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3), pp. 360-416.
Eric Brill 1992. A simple rule-based part of speech
tagger. in Proceedings of the workshop on Speech
and Natural Language
Alexander Clark 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. in Proceedings of the 2001 work-
shop on Computational Natural Language Learning
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
2009. Unsupervised multilingual grammar induc-
tion. in Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL
Aarne Ranta 2009. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming: 14(2), pp. 145-189
Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,
Alexandros Potamianos and Augustine Tsai 2009.
Auto-induced Semantic Classes. Speech Communi-
cation: 43(3), pp. 183-203
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. in Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*Sem), pp. 385-393
672
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 1?2,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Up from Limited Dialog Systems!
Giuseppe Riccardi
University of Trento
via Sommarive, 14
38050, Trento, Italy
riccardi@disi.unitn.it
Philipp Cimiano
Bielefeld University
Universita?tsstra?e 21?23
33615, Bielefeld, Germany
cimiano@cit-ec.uni-bielefeld.de
Alexandros Potamianos
Technical University of Crete
73100, Chania
Crete, Greece
potam@telecom.tuc.gr
Christina Unger
Bielefeld University
Universita?tsstra?e 21?23
33615, Bielefeld, Germany
cunger@cit-ec.uni-bielefeld.de
Abstract
In the last two decades, information-seeking
spoken dialog systems (SDS) have moved
from research prototypes to real-life commer-
cial applications. Still, dialog systems are lim-
ited by the scale, complexity of the task and
coverage of knowledge required by problem-
solving machines or mobile personal assis-
tants. Future spoken interaction are required
to be multilingual, understand and act on large
scale knowledge bases in all its forms (from
structured to unstructured). The Web re-
search community have striven to build large
scale and open multilingual resources (e.g.
Wikipedia) and knowledge bases (e.g. Yago).
We argue that a) it is crucial to leverage
this massive amount of Web lightly structured
knowledge and b) the scale issue can be ad-
dressed collaboratively and design open stan-
dards to make tools and resources available to
the whole speech and language community.
1 Introduction
In the last two decades, interactive spoken dialog
systems (SDS) have moved from research proto-
types to real-life commercial applications (Tur and
De Mori, 2011). Generally, SDS are built for a
specific task (e.g. call routing) with ad-hoc lim-
ited knowledge base and for a predefined target lan-
guage. However, one major limitation in commer-
cial SDS prototyping is that they are not easily and
quickly extensible and portable to new domains or
languages. Such porting requirements range from
defining (or extending) a domain ontology to hand-
crafting a new grammar or training stochastic mod-
els for speech recognition and understanding. These
are the research and engineering goals motivating
the PortDial project whose objectives include the
engagement of the whole technical community. In
the PortDial project we would like to engage re-
searchers in building resources that may be gener-
ated via top-down processes (grammars), bottom-up
processes (statistical models) or via a fusion of both.
In this position paper we want to address the crit-
ical limitations of SDS systems: a) poor ability to
cover the knowledge space and its interface to the
SDS components (speech recognition, language un-
derstanding and dialog manager) and b) collabora-
tively design open standards to make tools and re-
sources available to the whole speech and language
community.
2 Exploiting top-down knowledge
There are at least three main kinds of structured
knowledge sources that SDS modules may ex-
ploit: ontologies, grammars, and lexica. Ontolo-
gies explicitly model background knowledge about
a certain domain. In the last years, many free
and open collaboratively created resources have
emerged, including large multi-lingual corpora such
as Wikipedia, and broad-coverage ontologies, e.g.
as part of the Linked Data Cloud (Bizer et al, 2009),
either created manually or extracted automatically
from existing data (such as DBpedia or Yago). How-
ever, while also lexica such as Wiktionary are avail-
able today on the Web, ontologies typically lack in-
formation about linguistic realization. For this rea-
son, ontologies available on the Web are not di-
rectly exploitable by dialog systems. Linguistic in-
1
formation is commonly captured in grammars, that
are either hand-crafted or created by means of ma-
chine learning techniques. In order to be able to
generate high-quality grammars with as little man-
ual effort as possible, we aim at (semi) automat-
ing the knowledge-based generation of lexica and
grammars. To achieve this, it is crucial to lever-
age Web resources for enriching ontologies with
lexical and linguistic information, i.e. informa-
tion about how ontological concepts are lexicalized
in different languages, capturing in particular lex-
ical and syntactic variation (Unger et al, 2010).
This knowledge-centered grammar generation pro-
cess may be merged with methods for automatically
inferring structure from lightly annotated corpus, in-
cluding data harvested from the Web, in a bottom-
up fashion (Tur and De Mori, 2011). For a dialog
system to be able to exploit ontologies, lexica and
grammars, these three resources need to be tightly
aligned, i.e. they need to share domain-relevant vo-
cabulary. For this alignment, we propose to build on
Semantic Web standards, mainly in order to support
the incorporation of already existing data, to share
resources for SDS engineering, and facilitate collab-
orative knowledge engineering. From a larger per-
spective, such an approach has the potential of cre-
ating SDS resources (ontologies, lexica and gram-
mars) that are strongly aligned with each other as
well as with other resources available on the Web,
thus fostering the creation of an eco-system of linked
resources that can be reused to facilitate the process
of engineering and porting a dialog system to new
domains and languages.
3 Collaboratively building and sharing
knowledge
Today the lack of reusable linguistic resources and
annotated data hinders the rapid development of
spoken dialog systems in industry and academia
alike. Despite progress in standardization of the
format of SDS grammars and semantic represen-
tations, the data proper has to be hand-crafted for
new applications and languages with little or no au-
tomation available. We argue above that language
engineering technology is now mature to help cre-
ate such linguistic resources automatically or semi-
automatically using data that is either harvested
from the web or via community crowdsourcing us-
ing the ?collective wisdom of expert crowds?. Al-
though providing linguistic resources and tools for
cost-effective SDS development is important and
relevant, a data pool that is not openly sharable and
continuously enriched fails its purpose. It is thus im-
portant to guarantee the sustainability of the linguis-
tic SDS resources engineered via a community that
both uses and actively develops the data pool. To-
wards this end, we envision both a free and premium
data exchange targeting non-commercial users that
can maintain and enrich the free version of the data
pool, and commercial speech services developers
that can contribute to the premium data pool via
an electronic marketplace. This is the model we
are launching within the EC-funded PortDial project
and aiming at involving the research community at
large and existing communities for sharing linguistic
resources, such as METANET and METASHARE1.
We believe that the creation of sharable SDS data
and linguistic resources for both academic and com-
mercial use will lead to the democratization of spo-
ken dialog systems development, reduce the barrier
to entry for new developers, as well as lead to im-
proved technologies for authoring speech services.
References
C. Bizer, T. Heath, and T. Berners-Lee. 2009. Linked
data-the story so far. International Journal on Seman-
tic Web and Information Systems, 14:9.
G. Tur and R. De Mori, editors. 2011. Spoken Language
Understanding: Systems for Extracting Semantic In-
formation from Speech. Wiley.
Christina Unger, Felix Hieber, and Philipp Cimiano.
2010. Generating LTAG grammars from a lexicon-
ontology interface. In Srinivas Bangalore, Robert
Frank, and Maribel Romero, editors, Proceedings of
the 10th International Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+10), pages
61?68, 06/2010.
1http://www.meta-net.eu/meta-share
2
