Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 524?532, Prague, June 2007. c?2007 Association for Computational Linguistics
A Systematic Comparison of Training Criteria
for Statistical Machine Translation
Richard Zens and Sas?a Hasan and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,hasan,ney}@cs.rwth-aachen.de
Abstract
We address the problem of training the free
parameters of a statistical machine transla-
tion system. We show significant improve-
ments over a state-of-the-art minimum er-
ror rate training baseline on a large Chinese-
English translation task. We present novel
training criteria based on maximum likeli-
hood estimation and expected loss compu-
tation. Additionally, we compare the maxi-
mum a-posteriori decision rule and the min-
imum Bayes risk decision rule. We show
that, not only from a theoretical point of
view but also in terms of translation qual-
ity, the minimum Bayes risk decision rule is
preferable.
1 Introduction
Once we specified the Bayes decision rule for statis-
tical machine translation, we have to address three
problems (Ney, 2001):
? the search problem, i.e. how to find the best
translation candidate among all possible target
language sentences;
? the modeling problem, i.e. how to structure
the dependencies of source and target language
sentences;
? the training problem, i.e. how to estimate the
free parameters of the models from the training
data.
Here, the main focus is on the training problem. We
will compare a variety of training criteria for statisti-
cal machine translation. In particular, we are consid-
ering criteria for the log-linear parameters or model
scaling factors. We will introduce new training cri-
teria based on maximum likelihood estimation and
expected loss computation. We will show that some
achieve significantly better results than the standard
minimum error rate training of (Och, 2003).
Additionally, we will compare two decision rules,
the common maximum a-posteriori (MAP) deci-
sion rule and the minimum Bayes risk (MBR) de-
cision rule (Kumar and Byrne, 2004). We will show
that the minimum Bayes risk decision rule results
in better translation quality than the maximum a-
posteriori decision rule for several training criteria.
The remaining part of this paper is structured
as follows: first, we will describe related work in
Sec. 2. Then, we will briefly review the baseline
system, Bayes decision rule for statistical machine
translation and automatic evaluation metrics for ma-
chine translation in Sec. 3 and Sec. 4, respectively.
The novel training criteria are described in Sec. 5
and Sec. 6. Experimental results are reported in
Sec. 7 and conclusions are given in Sec. 8.
2 Related Work
The most common modeling approach in statistical
machine translation is to use a log-linear combina-
tion of several sub-models (Och and Ney, 2002). In
(Och and Ney, 2002), the log-linear weights were
tuned to maximize the mutual information criterion
(MMI). The current state-of-the-art is to optimize
these parameters with respect to the final evaluation
criterion; this is the so-called minimum error rate
training (Och, 2003).
Minimum Bayes risk decoding for machine trans-
524
lation was introduced in (Kumar and Byrne, 2004).
It was shown that MBR outperforms MAP decoding
for different evaluation criteria. Further experiments
using MBR for Bleu were performed in (Venugopal
et al, 2005; Ehling et al, 2007). Here, we will
present additional evidence that MBR decoding is
preferable over MAP decoding.
Tillmann and Zhang (2006) describe a percep-
tron style algorithm for training millions of features.
Here, we focus on the comparison of different train-
ing criteria.
Shen et al (2004) compared different algorithms
for tuning the log-linear weights in a reranking
framework and achieved results comparable to the
standard minimum error rate training.
An annealed minimum risk approach is presented
in (Smith and Eisner, 2006) which outperforms both
maximum likelihood and minimum error rate train-
ing. The parameters are estimated iteratively using
an annealing technique that minimizes the risk of an
expected-BLEU approximation, which is similar to
the one presented in this paper.
3 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the expected loss, also called Bayes risk:
e?I?1 = argmin
I,eI1
{
?
I?,e?I
?
1
Pr(e?I
?
1 |f
J
1 ) ? L(e
I
1, e
?I?
1 )
}
Here, L(eI1, e
?I?
1 ) denotes the loss function under
consideration. It measures the loss (or errors) of a
candidate translation eI1 assuming the correct trans-
lation is e?I
?
1 . In the following, we will call this de-
cision rule the MBR rule (Kumar and Byrne, 2004).
This decision rule is optimal in the sense that any
other decision rule will result (on average) in at least
as many errors as the MBR rule. Despite this, most
SMT systems do not use theMBR decision rule. The
most common approach is to use the maximum a-
posteriori (MAP) decision rule. Thus, we select the
hypothesis which maximizes the posterior probabil-
ity Pr(eI1|f
J
1 ):
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
}
This is equivalent to the MBR decision rule under
a 0-1 loss function:
L0?1(e
I
1, e
?I?
1 ) =
{
0 if eI1 = e
?I?
1
1 else
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics such as the Bleu
score. One reason for the popularity of the MAP
decision rule might be that, compared to the MBR
rule, its computation is simpler.
The posterior probability Pr(eI1|f
J
1 ) is modeled
directly using a log-linear combination of several
models (Och and Ney, 2002):
p?M1 (e
I
1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
I?,e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(1)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process and obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
Note that the denominator affects the results of the
MBR decision rule and, thus, cannot be omitted in
that case.
We use a state-of-the-art phrase-based translation
system similar to (Koehn, 2004; Mauser et al, 2006)
including the following models: an n-gram lan-
guage model, a phrase translation model and a word-
based lexicon model. The latter two models are used
for both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty.
525
In the following, we will discuss the so-called
training problem (Ney, 2001): how do we train the
free parameters ?M1 of the model? The current
state-of-the-art is to use minimum error rate train-
ing (MERT) as described in (Och, 2003). The free
parameters are tuned to directly optimize the evalu-
ation criterion.
Except for the MERT, the training criteria that
we will consider are additive at the sentence-level.
Thus, the training problem for a development set
with S sentences can be formalized as:
??M1 = argmax
?M1
S?
s=1
F (?M1 , (e
I
1, f
J
1 )s) (2)
Here, F (?, ?) denotes the training criterion that we
would like to maximize and (eI1, f
J
1 )s denotes a sen-
tence pair in the development set. The optimization
is done using the Downhill Simplex algorithm from
the Numerical Recipes book (Press et al, 2002).
This is a general purpose optimization procedure
with the advantage that it does not require the deriva-
tive information. Before we will describe the details
of the different training criteria in Sec. 5 and 6, we
will discuss evaluation metrics in the following sec-
tion.
4 Evaluation Metrics
The automatic evaluation of machine translation is
currently an active research area. There exists a
variety of different metrics, e.g., word error rate,
position-independent word error rate, BLEU score
(Papineni et al, 2002), NIST score (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), GTM
(Turian et al, 2003). Each of them has advantages
and shortcomings.
A popular metric for evaluating machine trans-
lation quality is the Bleu score (Papineni et al,
2002). It has certain shortcomings for compar-
ing different machine translation systems, especially
if comparing conceptually different systems, e.g.
phrase-based versus rule-based systems, as shown
in (Callison-Burch et al, 2006). On the other hand,
Callison-Burch concluded that the Bleu score is re-
liable for comparing variants of the same machine
translation system. As this is exactly what we will
need in our experiments and as Bleu is currently the
most popular metric, we have chosen it as our pri-
mary evaluation metric. Nevertheless, most of the
methods we will present can be easily adapted to
other automatic evaluation metrics.
In the following, we will briefly review the com-
putation of the Bleu score as some of the training
criteria are motivated by this. The Bleu score is a
combination of the geometric mean of n-gram pre-
cisions and a brevity penalty for too short translation
hypotheses. The Bleu score for a translation hypoth-
esis eI1 and a reference translation e?
I?
1 is computed as:
Bleu(eI1, e?
I?
1) = BP(I, I?) ?
4?
n=1
Precn(e
I
1, e?
I?
1)
1/4
with
BP(I, I?) =
{
1 if I ? I?
exp (1 ? I/I?) if I < I?
Precn(e
I
1, e?
I?
1) =
?
wn1
min{C(wn1 |e
I
1), C(w
n
1 |e?
I?
1)}
?
wn1
C(wn1 |e
I
1)
(3)
Here, C(wn1 |e
I
1) denotes the number of occur-
rences of an n-gram wn1 in a sentence e
I
1. The de-
nominators of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I ?n+1.
The n-gram counts for the Bleu score computa-
tion are usually collected over a whole document.
For our purposes, a sentence-level computation is
preferable. A problem with the sentence-level Bleu
score is that the score is zero if not at least one four-
gram matches. As we would like to avoid this prob-
lem, we use the smoothed sentence-level Bleu score
as suggested in (Lin and Och, 2004). Thus, we in-
crease the nominator and denominator of Precn(?, ?)
by one for n > 1. Note that we will use the
sentence-level Bleu score only during training. The
evaluation on the development and test sets will be
carried out using the standard Bleu score, i.e. at the
corpus level. As the MERT baseline does not require
the use of the sentence-level Bleu score, we use the
standard Bleu score for training the baseline system.
In the following, we will describe several crite-
ria for training the log-linear parameters ?M1 of our
model. For notational convenience, we assume that
there is just one reference translation. Nevertheless,
the methods can be easily adapted to the case of mul-
tiple references.
526
5 Maximum Likelihood
5.1 Sentence-Level Computation
A popular approach for training parameters is max-
imum likelihood estimation (MLE). Here, the goal
is to maximize the joint likelihood of the parameters
and the training data. For log-linear models, this re-
sults in a nice optimization criterion which is con-
vex and has a single optimum. It is equivalent to the
maximum mutual information (MMI) criterion. We
obtain the following training criterion:
FML?S(?
M
1 , (e
I
1, f
J
1 )) = log p?M1 (e
I
1|f
J
1 )
A problem that we often face in practice is that
the correct translation might not be among the can-
didates that our MT system produces. Therefore,
(Och and Ney, 2002; Och, 2003) defined the trans-
lation candidate with the minimum word-error rate
as pseudo reference translation. This has some bias
towards minimizing the word-error rate. Here, we
will use the translation candidate with the maximum
Bleu score as pseudo reference to bias the system
towards the Bleu score. However, as pointed out in
(Och, 2003), there is no reason to believe that the re-
sulting parameters are optimal with respect to trans-
lation quality measured with the Bleu score.
The goal of this sentence-level criterion is to dis-
criminate the single correct translation against all the
other ?incorrect? translations. This is problematic
as, even for human experts, it is very hard to define
a single best translation of a sentence. Furthermore,
the alternative target language sentences are not all
equally bad translations. Some of them might be
very close to the correct translation or even equiva-
lent whereas other sentences may have a completely
different meaning. The sentence-level MLE crite-
rion does not distinguish these cases and is therefore
a rather harsh training criterion.
5.2 N -gram Level Computation
As an alternative to the sentence-level MLE, we
performed experiments with an n-gram level MLE.
Here, we limit the order of the n-grams and assume
conditional independence among the n-gram prob-
abilities. We define the log-likelihood (LLH) of a
target language sentence eI1 given a source language
sentence fJ1 as:
FML?N (?
M
1 , (e
I
1, f
J
1 )) =
N?
n=1
?
wn1?e
I
1
log p?M1 (w
n
1 |f
J
1 )
Here, we use the n-gram posterior probability
p?M1 (w
n
1 |f
J
1 ) as defined in (Zens and Ney, 2006).
The n-gram posterior distribution is smoothed using
a uniform distribution over all possible n-grams.
p?M1 (w
n
1 |f
J
1 ) = ? ?
N?M1 (w
n
1 , f
J
1 )
?
w?n1
N?M1 (w
?n
1 , f
J
1 )
+ (1 ? ?) ?
1
V n
Here, V denotes the vocabulary size of the tar-
get language; thus, V n is the number of possi-
ble n-grams in the target language. We define
N?M1 (w
n
1 , f
J
1 ) as in (Zens and Ney, 2006):
N?M1 (w
n
1 , f
J
1 ) =
?
I,eI1
I?n+1?
i=1
p?M1 (e
I
1|f
J
1 )??(e
i+n?1
i , w
n
1 )
(4)
The sum over the target language sentences is lim-
ited to an N -best list, i.e. the N best translation
candidates according to the baseline model. In this
equation, we use the Kronecker function ?(?, ?), i.e.
the term ?(ei+n?1i , w
n
1 ) evaluates to one if and only
if the n-gram wn1 occurs in the target sentence e
I
1
starting at position i.
An advantage of the n-gram level computation
of the likelihood is that we do not have to define
pseudo-references as for the sentence-level MLE.
We can easily compute the likelihood for the human
reference translation. Furthermore, this criterion has
the desirable property that it takes partial correctness
into account, i.e. it is not as harsh as the sentence-
level criterion.
6 Expected Bleu Score
According to statistical decision theory, one should
maximize the expected gain (or equivalently mini-
mize the expected loss). For machine translation,
this means that we should optimize the expected
Bleu score, or any other preferred evaluation metric.
527
6.1 Sentence-Level Computation
The expected Bleu score for a given source sentence
fJ1 and a reference translation e?
I?
1 is defined as:
E[Bleu|e?I?1, f
J
1 ] =
?
eI1
Pr(eI1|f
J
1 ) ? Bleu(e
I
1, e?
I?
1)
Here, Pr(eI1|f
J
1 ) denotes the true probability dis-
tribution over the possible translations eI1 of the
given source sentence fJ1 . As this probability dis-
tribution is unknown, we approximate it using the
log-linear translation model p?M1 (e
I
1|f
J
1 ) from Eq. 1.
Furthermore, the computation of the expected Bleu
score involves a sum over all possible translations
eI1. This sum is approximated using an N -best list,
i.e. the N best translation hypotheses of the MT sys-
tem. Thus, the training criterion for the sentence-
level expected Bleu computation is:
FEB?S(?
M
1 , (e?
I?
1, f
J
1 )) =
?
eI1
p?M1 (e
I
1|f
J
1 )?Bleu(e
I
1, e?
I?
1)
An advantage of the sentence-level computation is
that it is straightforward to plug in alternative eval-
uation metrics instead of the Bleu score. Note that
the minimum error rate training (Och, 2003) uses
only the target sentence with the maximum posterior
probability whereas, here, the whole probability dis-
tribution is taken into account.
6.2 N -gram Level Computation
In this section, we describe a more fine grained com-
putation of the expected Bleu score by exploiting its
particular structure. Hence, this derivation is spe-
cific for the Bleu score but should be easily adapt-
able to other n-gram based metrics. We can rewrite
the expected Bleu score as:
E[Bleu|e?I?1, f
J
1 ] = E[BP|I? , f
J
1 ]
?
4?
n=1
E[Precn|e?I?1, f
J
1 ]
1/4
We assumed conditional independence between
the brevity penalty BP and the n-gram precisions
Precn. Note that although these independence as-
sumptions do not hold, the resulting parameters
might work well for translation. In fact, we will
show that this criterion is among the best perform-
ing ones in Sec. 7. This type of independence as-
sumption is typical within the naive Bayes classifier
framework. The resulting training criterion that we
will use in Eq. 2 is then:
FEB?N (?
M
1 , (e?
I?
1, f
J
1 )) = E?M1 [BP|I? , f
J
1 ]
?
4?
n=1
E?M1 [Precn|e?
I?
1, f
J
1 ]
1/4
We still have to define the estimators for the ex-
pected brevity penalty as well as the expected n-
gram precision:
E?M1 [BP|I? , f
J
1 ] =
?
I
BP(I, I?) ? p?M1 (I|f
J
1 )
E?M1 [Precn|e?
I?
1, f
J
1 ] = (5)
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
min{c, C(wn1 |e?
I?
1)} ? p?M1 (c|w
n
1 , f
J
1 )
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
c ? p?M1 (c|w
n
1 , f
J
1 )
Here, we use the sentence length posterior proba-
bility p?M1 (I|f
J
1 ) as defined in (Zens and Ney, 2006)
and the n-gram posterior probability p?M1 (w
n
1 |f
J
1 ) as
described in Sec. 5.2. Additionally, we predict the
number of occurrences c of an n-gram. This infor-
mation is necessary for the so-called clipping in the
Bleu score computation, i.e. the min operator in the
nominator of formulae Eq. 3 and Eq. 5. The denom-
inator of Eq. 5 is the expected number of n-grams in
the target sentence, whereas the nominator denotes
the expected number of correct n-grams.
To predict the number of occurrences within a
translation hypothesis, we use relative frequencies
smoothed with a Poisson distribution. The mean of
the Poisson distribution ?(wn1 , f
J
1 , ?
M
1 ) is chosen to
be the mean of the unsmoothed distribution.
p?M1 (c|w
n
1 , f
J
1 ) = ? ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
+ (1 ? ?) ?
?(wn1 , f
J
1 , ?
M
1 )
c ? e?c
c!
528
Table 1: Chinese-English TC-Star task: corpus
statistics.
Chinese English
Train Sentence pairs 8.3M
Running words 197M 238M
Vocabulary size 224K 389K
Dev Sentences 1 019 2 038
Running words 26K 51K
Eval 2006 Sentences 1 232 2 464
Running words 30K 62K
2007 Sentences 917 1 834
Running words 21K 45K
with
?(wn1 , f
J
1 , ?
M
1 ) =
?
c
c ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
Note that in case the mean ?(wn1 , f
J
1 , ?
M
1 ) is zero,
we do not need the distribution p?M1 (c|w
n
1 , f
J
1 ). The
smoothing parameters ? and ? are both set to 0.9.
7 Experimental Results
7.1 Task Description
We perform translation experiments on the Chinese-
English TC-Star task. This is a broadcast news
speech translation task used within the European
Union project TC-Star1. The bilingual training
data consists of virtually all publicly available LDC
Chinese-English corpora. The 6-gram language
model was trained on the English part of the bilin-
gual training data and additional monolingual En-
glish parts from the GigaWord corpus. We use the
modified Kneser-Ney discounting as implemented
in the SRILM toolkit (Stolcke, 2002).
Annual public evaluations are carried out for this
task within the TC-Star project. We will report re-
sults on manual transcriptions, i.e. the so-called ver-
batim condition, of the official evaluation test sets of
the years 2006 and 2007. There are two reference
translations available for the development and test
sets. The corpus statistics are shown in Table 1.
7.2 Translation Results
In Table 2, we present the translation results
for different training criteria for the development
1http://www.tc-star.org
set and the two blind test sets. The reported
case-sensitive Bleu scores are computed using
the mteval-v11b.pl2 tool using two reference
translations, i.e. BLEUr2n4c. Note that already the
baseline system (MERT-Bleu) would have achieved
the first rank in the official TC-Star evaluation 2006;
the best Bleu score in that evaluation was 16.1%.
The MBR hypotheses were generated using the
algorithm described in (Ehling et al, 2007) on a
10 000-best list.
On the development data, the MERT-Bleu
achieves the highest Bleu score. This seems reason-
able as it is the objective of this training criterion.
The maximum likelihood (MLE) criteria perform
somewhat worse under MAP decoding. Interest-
ingly, the MBR decoding can compensate this to
a large extent: all criteria achieve a Bleu score of
about 18.9% on the development set. The bene-
fits of MBR decoding become even more evident
on the two test sets. Here, the MAP results for the
sentence-level MLE criterion are rather poor com-
pared to the MERT-Bleu. Nevertheless, using MBR
decoding results in very similar Bleu scores for most
of the criteria on these two test sets. We can there-
fore support the claim of (Smith and Eisner, 2006)
that MBR tends to have better generalization capa-
bilities.
The n-gram level MLE criterion seems to perform
better than the sentence-level MLE criterion, espe-
cially on the test sets. The reasons might be that
there is no need for the use of pseudo references
as described in Sec. 5 and that partial correctness
is taken into account.
The best results are achieved using the expected
Bleu score criteria described in Sec. 6. Here, the sen-
tence level and n-gram level variants achieve more
or less the same results. The overall improvement
on the Eval?06 set is about 1.0% Bleu absolute for
MAP decoding and 0.9% for MBR decoding. On
the Eval?07 set, the improvements are even larger,
about 1.8% Bleu absolute for MAP and 1.1% Bleu
for MBR. All these improvements are statistically
significant at the 99% level using a pairwise signifi-
cance test3.
Given that currently the most popular approach is
to use MERT-Bleu MAP decoding, the overall im-
2http://www.nist.gov/speech/tests/mt/resources/scoring.htm
3The tool for computing the significance test was kindly pro-
vided by the National Research Council Canada.
529
Table 2: Translation results: Bleu scores [%] for the Chinese-English TC-Star task for various training
criteria (MERT: minimum error rate training; MLE: maximum likelihood estimation; E[Bleu]: expected
Bleu score) and the maximum a-posteriori (MAP) as well as the minimum Bayes risk (MBR) decision rule.
Development Eval?06 Eval?07
Decision Rule MAP MBR MAP MBR MAP MBR
Training Criterion MERT-Bleu (baseline) 19.5 19.4 16.7 17.2 22.2 23.0
MLE sentence-level 17.8 18.9 14.8 17.1 18.9 22.7
n-gram level 18.6 18.8 17.0 17.8 22.8 23.5
E[Bleu] sentence-level 19.1 18.9 17.5 18.1 23.5 24.1
n-gram level 18.6 18.8 17.7 17.6 24.0 24.0
provement is about 1.4% absolute for the Eval?06
set and 1.9% absolute on the Eval?07 set.
Note that the MBR decision rule almost always
outperforms theMAP decision rule. In the rare cases
where the MAP decision rule yields better results,
the difference in terms of Bleu score are small and
not statistically significant.
We also investigated the effect of the maximum
n-gram order for the n-gram level maximum like-
lihood estimation (MLE). The results are shown in
Figure 1. We observe an increase of the Bleu score
with increasing maximum n-gram order for the de-
velopment corpus. On the evaluation sets, however,
the maximum is achieved if the maximum n-gram
order is limited to four. This seems intuitive as the
Bleu score uses n-grams up to length four. However,
one should be careful here: the differences are rather
small, so it might be just statistical noise.
Some translation examples from the Eval?07 test
set are shown in Table 3 for different training criteria
under the maximum a-posteriori decision rule.
8 Conclusions
We have presented a systematic comparison of sev-
eral criteria for training the log-linear parameters of
a statistical machine translation system. Addition-
ally, we have compared the maximum a-posteriori
with the minimum Bayes risk decision rule.
We can conclude that the expected Bleu score
is not only a theoretically sound training criterion,
but also achieves the best results in terms of Bleu
score. The improvement over a state-of-the-art
MERT baseline is 1.3% Bleu absolute for the MAP
decision rule and 1.1% Bleu absolute for the MBR
decision rule for the large Chinese-English TC-Star
speech translation task.
1 2 3 4 5 6 7 8 9max. n-gram order
14
16
18
20
22
24
Bleu
 [%]
DevEval'06Eval'07
Figure 1: Effect of the maximum n-gram order on
the Bleu score for the n-gram level maximum like-
lihood estimation under the maximum a-posteriori
decision rule.
We presented two methods for computing the ex-
pected Bleu score: a sentence-level and an n-gram
level approach. Both yield similar results. We think
that the n-gram level computation has certain ad-
vantages: The n-gram posterior probabilities could
be computed from a word graph which would result
in more reliable estimates. Whether this pays off
in terms of translation quality is left open for future
work.
Another interesting result of our experiments is
that the MBR decision rule seems to be less affected
by sub-optimal parameter settings.
Although it is well-known that the MBR decision
rule is more appropriate than the MAP decision rule,
the latter is more popular in the SMT community
(and many other areas of natural language process-
ing). Our results show that it can be beneficial to
530
Table 3: Translation examples from the Eval?07 test set for different training criteria and the maximum a-
posteriori decision rule. (MERT: minimum error rate training, MLE-S: sentence-level maximum likelihood
estimation, E[Bleu]: sentence-level expected Bleu)
Criterion Translation
Reference 1 Saving Private Ryan ranks the third on the box office revenue list which is also a movie that is
possible to win an 1999 Oscar award
2 Saving Private Ryan ranked third in the box office income is likely to compete in the nineteen
ninety-nine Oscar Awards
MERT-Bleu Saving private Ryan in box office income is possible ranked third in 1999 Oscar a film
MLE-S Saving private Ryan box office revenue ranked third is possible in 1999 Oscar a film
E[Bleu]-S Saving private Ryan ranked third in the box office income is also likely to run for the 1999
Academy Awards a film
Reference 1 The following problem is whether people in countries like China and Japan and other countries
will choose Euros rather than US dollars in international business activities in the future
2 The next question is whether China or Japan or other countries will choose to use Euros instead
of US dollars when they conduct international business in the future
MERT-Bleu The next question is in China or Japan international business activities in the future they will not
use the Euro dollar
MLE-S The next question was either in China or Japan international business activities in the future they
will adopt the Euro instead of the dollar
E[Bleu]-S The next question was in China or Japan in the international business activities in the future they
will adopt the Euro instead of the US dollar
Reference 1 The Chairman of the European Commission Jacques Santer pointed out in this September that the
financial crisis that happened in Russia has not affected people?s confidence in adopting the Euro
2 European Commission President Jacques Santer pointed out in September this year that
Russia?s financial crisis did not shake people?s confidence for planning the use of the Euro
MERT-Bleu President of the European Commission Jacques Santer on September this year that the Russian
financial crisis has not shaken people ?s confidence in the introduction of the Euro
MLE-S President of the European Commission Jacques Santer September that the Russian financial crisis
has not affected people ?s confidence in the introduction of the Euro
E[Bleu]-S President of the European Commission Jacques Santer pointed out that Russia ?s financial crisis
last September has not shaken people ?s confidence in the introduction of the Euro
Reference 1 After many years of friction between Dutch and French speaking Belgians all of them now hope
to emphasize their European identities
2 After years of friction between Belgium?s Dutch-speaking and French-speaking people they now
all wish to emphasize their European identity
MERT-Bleu Belgium?s Dutch-speaking and French-speaking after many years of civil strife emphasized that
they now hope that Europeans
MLE-S Belgium?s Dutch-speaking and francophone after years of civil strife that they now hope that
Europeans
E[Bleu]-S Belgium?s Dutch-speaking and French-speaking after many years of civil strife it is now want
to emphasize their European identity
531
use the MBR decision rule. On the other hand, the
computation of the MBR hypotheses is more time
consuming. Therefore, it would be desirable to have
a more efficient algorithm for computing the MBR
hypotheses.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved correlation
with human judgments. In Proc. Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association of Com-
putational Linguistics (ACL), pages 65?72, Ann Arbor, MI,
June.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J.
Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics, 16(2):79?
85, June.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of BLEU in machine trans-
lation research. In Proc. 11th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
249?256, Trento, Italy, April.
George Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statistics. In
Proc. ARPA Workshop on Human Language Technology.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Mini-
mum Bayes risk decoding for BLEU. In Proc. 45th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Poster Session, Prague, Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Proc.
6th Conf. of the Assoc. for Machine Translation in the Amer-
icas (AMTA), pages 115?124, Washington DC, Septem-
ber/October.
Shankar Kumar and William Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In Proc.
Human Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual Meet-
ing (HLT-NAACL), pages 169?176, Boston, MA, May.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method
for evaluating automatic evaluation metrics for machine
translation. In Proc. COLING ?04: The 20th Int. Conf.
on Computational Linguistics, pages 501?507, Geneva,
Switzerland, August.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a Hasan,
and Hermann Ney. 2006. The RWTH statistical machine
translation system for the IWSLT 2006 evaluation. In Proc.
Int. Workshop on Spoken Language Translation (IWSLT),
pages 103?110, Kyoto, Japan, November.
Hermann Ney. 2001. Stochastic modelling: from pattern
classification to language translation. In Proc. 39th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Workshop on Data-Driven Machine Translation, pages 1?5,
Morristown, NJ, July.
Franz Josef Och and Hermann Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical machine
translation. In Proc. 40th Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 295?302, Philadel-
phia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. 41st Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. 40th Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and
Brian P. Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative reranking for machine translation. In Proc. Hu-
man Language Technology Conf. / North American Chapter
of the Assoc. for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 177?184, Boston, MA, May.
David A. Smith and Jason Eisner. 2006. Minimum risk anneal-
ing for training log-linear models. In Proc. 21st Int. Conf.
on Computational Linguistics and 44th Annual Meeting of
the Assoc. for Computational Linguistics (COLING/ACL):
Poster Session, pages 787?794, Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Speech and Language
Processing (ICSLP), volume 2, pages 901?904, Denver, CO,
September.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical MT. In Proc. 21st
Int. Conf. on Computational Linguistics and 44th Annual
Meeting of the Assoc. for Computational Linguistics (COL-
ING/ACL), pages 721?728, Sydney, Australia, July.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of machine translation and its evaluation. Technical
Report Proteus technical report 03-005, Computer Science
Department, New York University.
Ashish Venugopal, Andreas Zollmann, and Alex Waibel. 2005.
Training and evaluating error minimization rules for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL): Workshop
on Building and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 208?215, Ann Arbor, MI,
June.
Richard Zens and Hermann Ney. 2006. N -gram posterior prob-
abilities for statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL): Proc. Workshop on Statistical Machine Transla-
tion, pages 72?77, New York City, NY, June.
532
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 372?381,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Triplet Lexicon Models for Statistical Machine Translation
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, Jesu?s Andre?s-Ferrer??
Human Language Technology and Pattern Recognition, RWTH Aachen University, Germany
?Universidad Polite?cnica de Valencia, Dept. Sist. Informa?ticos y Computacio?n
{hasan,ganitkevitch,ney}@cs.rwth-aachen.de jandres@dsic.upv.es
Abstract
This paper describes a lexical trigger model
for statistical machine translation. We present
various methods using triplets incorporating
long-distance dependencies that can go be-
yond the local context of phrases or n-gram
based language models. We evaluate the pre-
sented methods on two translation tasks in a
reranking framework and compare it to the re-
lated IBM model 1. We show slightly im-
proved translation quality in terms of BLEU
and TER and address various constraints to
speed up the training based on Expectation-
Maximization and to lower the overall num-
ber of triplets without loss in translation per-
formance.
1 Introduction
Data-driven methods have been applied very suc-
cessfully within the machine translation domain
since the early 90s. Starting from single-word-
based translation approaches, significant improve-
ments have been made through advances in mod-
eling, availability of larger corpora and more pow-
erful computers. Thus, substantial progress made
in the past enables today?s MT systems to achieve
acceptable results in terms of translation quality for
specific language pairs such as Arabic-English. If
sufficient amounts of parallel data are available, sta-
tistical MT systems can be trained on millions of
?The work was carried out while the author was at the Hu-
man Language Technology and Pattern Recognition group at
RWTH Aachen University and partly supported by the Valen-
cian Conselleria d?Empresa, Universitat i Cie`ncia under grants
CTBPRA/2005/ and BEFPI/2007/014.
target
source
e e?
f
Figure 1: Triplet example: a source word f is triggered
by two target words e and e?, where one of the words is
within and the other outside the considered phrase pair
(indicated by the dashed line).
sentence pairs and use an extended level of context
based on bilingual groups of words which denote
the building blocks of state-of-the-art phrase-based
SMT systems.
Due to data sparseness, statistical models are of-
ten trained on local context only. Language mod-
els are derived from n-grams with n ? 5 and bilin-
gual phrase pairs are extracted with lengths up to
10 words on the target side. This captures the local
dependencies of the data in detail and is responsi-
ble for the success of data-driven phrase-based ap-
proaches.
In this work, we will introduce a new statistical
model based on lexicalized triplets (f, e, e?) which
we will also refer to as cross-lingual triggers of
the form (e, e? ? f). This can be understood
as two words in one language triggering one word
in another language. These triplets, modeled by
p(f |e, e?), are closely related to lexical translation
probabilities based on the IBM model 1, i.e. p(f |e).
Several constraints and setups will be described later
on in more detail, but as an introduction one can
372
think of the following interpretation which is de-
picted in Figure 1: Using a phrase-based MT ap-
proach, a source word f is triggered by its trans-
lation e which is part of the phrase being consid-
ered, whereas another target word e? outside this
phrase serves as an additional trigger in order to al-
low for more fine-grained distinction of a specific
word sense. Thus, this cross-lingual trigger model
can be seen as a combination of a lexicon model (i.e.
f and e) and a model similar to monolingual long-
range (i.e. distant bigram) trigger models (i.e. e and
e?, although these dependencies are reflected indi-
rectly via e? ? f ) which uses both local (in-phrase)
and global (in-sentence) information for the scoring.
The motivation behind this approach is to get non-
local information outside the current context (i.e. the
currently considered bilingual phrase pair) into the
translation process. The triplets are trained via the
EM algorithm, as will be shown later in more detail.
2 Related Work
In the past, a significant number of methods has
been presented that try to capture long-distance de-
pendencies, i.e. use dependencies in the data that
reach beyond the local context of n-grams or phrase
pairs. In language modeling, monolingual trigger
approaches have been presented (Rosenfeld, 1996;
Tillmann and Ney, 1997) as well as syntactical meth-
ods that parse the input and model long-range de-
pendencies on the syntactic level by conditioning on
the predecessing words and their corresponding par-
ent nodes (Chelba and Jelinek, 2000; Roark, 2001).
The latter approach was shown to reduce perplex-
ities and improve the WER in speech recognition
systems. One drawback is that the parsing process
might slow down the system significantly and the
approach is complicated to be integrated directly in
the search process. Thus, the effect is often shown
offline in reranking experiments using n-best lists.
One of the simplest models that can be seen in
the context of lexical triggers is the IBM model 1
(Brown et al, 1993) which captures lexical depen-
dencies between source and target words. It can be
seen as a lexicon containing correspondents of trans-
lations of source and target words in a very broad
sense since the pairs are trained on the full sentence
level. The model presented in this work is very close
to the initial IBM model 1 and can be seen as taking
another word into the conditioning part, i.e. the trig-
gering items.1 Furthermore, since the second trig-
ger can come from any part of the sentence, we also
have a link to long-range monolingual triggers as
presented above.
A long-range trigram model is presented in
(Della Pietra et al, 1994) where it is shown how to
derive a probabilistic link grammar in order to cap-
ture long-range dependencies in English using the
EM algorithm. Expectation-Maximization is used
in the presented triplet model as well which is de-
scribed in more detail in Section 3. Instead of deriv-
ing a grammar automatically (based on POS tags of
the words), we rely on a fully lexicalized approach,
i.e. the training is taking place at the word level.
Related work in the context of fine-tuning lan-
guage models by using cross-lingual lexical triggers
is presented in (Kim and Khudanpur, 2003). The
authors show how to use cross-lingual triggers on a
document level in order to extract translation lexi-
cons and domain-specific language models using a
mutual information criterion.
Recently, word-sense disambiguation (WSD)
methods have been shown to improve translation
quality (Chan et al, 2007; Carpuat and Wu, 2007).
Chan et al (2007) use an SVM based classifier for
disambiguating word senses which are directly in-
corporated in the decoder through additional fea-
tures that are part of the log-linear combination of
models. They use local collocations based on sur-
rounding words left and right of an ambiguous word
including the corresponding parts-of-speech. Al-
though no long-range dependencies are modeled, the
approach yields an improvement of +0.6% BLEU on
the NIST Chinese-English task. In Carpuat and Wu
(2007), another state-of-the-art WSD engine (a com-
bination of naive Bayes, maximum entropy, boost-
ing and Kernel PCA models) is used to dynamically
determine the score of a phrase pair under consid-
eration and, thus, let the phrase selection adapt to
the context of the sentence. Although the baseline is
significantly lower than in the work of Chan et al,
this setup reaches an improvement of 0.5% BLEU
on the NIST CE task and up to 1.1% BLEU on the
1Thus, instead of p(f |e) we model p(f |e, e?) with different
additional constraints as explained later on.
373
IWSLT?06 test sets.
The work in this paper tries to complement the
WSD approaches by using long-range dependen-
cies. If triggers from a local context determine dif-
ferent lexical choice for the word being triggered,
the setting is comparable to the mentioned WSD
approaches (although local dependencies might al-
ready be reflected sufficiently in the phrase models).
A distant second trigger, however, might have a ben-
eficial effect for specific languages, e.g. by captur-
ing word splits (as it is the case in German for verbs
with separable prefixes) or, as already mentioned, al-
lowing for a more fine-grained lexical choice of the
word being triggered, namely based on another word
which is not part of the current local, i.e. phrasal,
context.
The basic idea of triplets of the form (e, f ? ? f),
called multi-word extensions, is also mentioned in
(Tillmann, 2001) but neither evaluated nor investi-
gated in further detail.
In the following sections, we will describe the
model proposed in this work. In Section 3, a de-
tailed introduction is given, as well as the EM train-
ing and variations of the model. The different set-
tings will be evaluated in Section 4, where we show
experiments on the IWSLT Chinese-English and
TC-STAR EPPS English-Spanish/Spanish-English
tracks. A discussion of the results and further ex-
amples are given in Section 5. Final remarks and
future work are addressed in Section 6.
3 Model
As an extension to commonly used lexical word
pair probabilities p(f |e) as introduced in (Brown
et al, 1993), we define our model to operate on
word triplets. A triplet (f, e, e?) is assigned a value
?(f |e, e?) ? 0 with the constraint such that
?e, e? :
?
f
?(f |e, e?) = 1.
Throughout this paper, e and e? will be referred to as
the first and the second trigger, respectively. In view
of its triggers f will be termed the effect.
For a given bilingual sentence pair (fJ1 , e
I
1), the
probability of a source word fj given the whole tar-
get sentence eI1 for the triplet model is defined as:
pall (fj |e
I
1) =
1
Z
I?
i=1
I?
k=i+1
?(fj |ei, ek), (1)
where Z denotes a normalization factor based on the
corresponding target sentence length, i.e.
Z =
I(I ? 1)
2
. (2)
The introduction of a second trigger (i.e. ek in
Eq. 1) enables the model to combine local (i.e. word
or phrase level) and global (i.e. sentence level) infor-
mation.
In the following, we will describe the training pro-
cedure of the model via maximum likelihood esti-
mation for the unconstrained case.
3.1 Training
The goal of the training procedure is to maximize the
log-likelihood Fall of the triplet model for a given
bilingual training corpus {(fJ1 , e
I
1)}
N
1 consisting of
N sentence pairs:
Fall :=
N?
n=1
Jn?
j=1
log pall (fj |e
In
1 ),
where Jn and In are the lengths of the nth source
and target sentences, respectively. As there is no
closed form solution for the maximum likelihood es-
timate, we resort to iterative training via the EM al-
gorithm (Dempster et al, 1977). We define the aux-
iliary function Q(?; ??) based on Fall where ?? is the
new estimate within an iteration which is to be de-
rived from the current estimate ?. Here, ? stands for
the entire set of model parameters to be estimated,
i.e. the set of all {?(f |e, e?)}. Thus, we obtain
Q
(
{?(f |e, e?)}; {??(f |e, e?)}
)
=
N?
n=1
Jn?
j=1
In?
i=1
In?
k=i+1
[
Z?1n ?(fj |ei, ek)
pall (fj |e
In
1 )
? (3)
log
(
Z?1n ??(fj |ei, ek)
)
]
,
where Zn is defined as in Eq. 2. Using the
method of Lagrangian multipliers for the normaliza-
tion constraint, we take the derivative with respect to
374
??(f |e, e?) and obtain:
??(f |e, e?) =
A(f, e, e?)
?
f ? A(f
?, e, e?)
(4)
where A(f, e, e?) is a relative weight accumulator
over the parallel corpus:
A(f, e, e?) =
N?
n=1
Jn?
j=1
?(f, fj)
Z?1n ?(f |e, e
?)
pall (fj |e
In
1 )
Cn(e, e
?) (5)
and
Cn(e, e
?) =
In?
i=1
In?
k=i+1
?(e, ei)?(e
?, ek).
The function ?(?, ?) denotes the Kronecker delta.
The resulting training procedure is analogous to the
one presented in (Brown et al, 1993) and (Tillmann
and Ney, 1997).
The next section presents variants of the ba-
sic unconstrained model by putting restrictions on
the valid regions of triggers (in-phrase vs. out-of-
phrase) and using alignments obtained from either
GIZA++ training or forced alignments in order to
reduce the model size and to incorporate knowledge
already obtained in previous training steps.
3.2 Model variations
Based on the unconstrained triplet model presented
in Section 3, we introduce additional constraints,
namely the phrase-bounded and the path-aligned
triplet model in the following. The former reduces
the number of possible triplets by posing constraints
on the position of where valid triggers may originate
from. In order to obtain phrase boundaries on the
training data, we use forced alignments, i.e. translate
the whole training data by constraining the transla-
tion hypotheses to the target sentences of the training
corpus.
Path-aligned triplets use an alignment constraint
from the word alignments that are trained with
GIZA++. Here, we restrict the first trigger pair (f, e)
to the alignment path as based on the alignment ma-
trix produced by IBM model 4.
These variants require information in addition to
the bilingual sentence pair (fJ1 , e
I
1), namely a corre-
sponding phrase segmentation ? = {piij} with
piij =
{
1 ? a phrase pair that covers ei and fj
0 otherwise
for the phrase-bounded method and, similarly, a
word alignment A = {aij} where
aij =
{
1 if ei is aligned to fj
0 otherwise
.
3.2.1 Phrase-bounded triplets
The phrase-bounded triplet model (referred to as
pphr in the following), restricts the first trigger e to
the same phrase as f , whereas the second trigger e?
is set outside the phrase, resulting in
pphr (fj |e
I
1,?) =
1
Zj
I?
i=1
I?
k=1
piij(1 ? pikj)?(fj |ei, ek). (6)
3.2.2 Path-aligned triplet
The path-aligned triplet model (denoted by palign
in the following), restricts the scope of e to words
aligned to f by A, yielding:
palign(fj |e
I
1, A) =
1
Zj
I?
i=1
I?
k=1
aij?(fj |ei, ek) (7)
where the Zj are, again, the appropriate normaliza-
tion terms.
Also, to account for non-aligned words (analo-
gously to the IBM models), the empty word e0 is
considered in all three model variations. We show
the effect of the empty word in the experiments (Sec-
tion 4). Furthermore, we can train the presented
models in the inverse direction, i.e. p(e|f, f ?), and
combine the two directions in the rescoring frame-
work. The next section presents a set of experiments
that evaluate the performance of the presented triplet
model and its variations.
4 Experiments
In this section, we describe the system setup used in
this work, including the translation tasks and the cor-
responding training corpora. The experiments are
based on an n-best list reranking framework.
375
4.1 System
The experiments were carried out using a state-of-
the-art phrase-based SMT system. The dynamic
programming beam search decoder uses several
models during decoding by combining them log-
linearly. We incorporate phrase translation and word
lexicon models in both directions, a language model,
as well as phrase and word penalties including a
distortion model for the reordering. While gener-
ating the hypotheses, a word graph is created which
compactly represents the most likely translation hy-
potheses. Out of this word graph, we generate n-
best lists and use them to test the different setups as
described in Section 3.
In the experiments, we use 10,000-best lists con-
taining unique translation hypotheses, i.e. duplicates
generated due to different phrase segmentations are
reduced to one single entry. The advantage of this
reranking approach is that we can directly test the
obtained models since we already have fully gener-
ated translations. Thus, we can apply the triplet lex-
icon model based on p(f |e, e?) and its inverse coun-
terpart p(e|f, f ?) directly. During decoding, since e?
could be from anywhere outside the current phrase,
i.e. even from a part which lies beyond the current
context which has not yet been generated, we would
have to apply additional constraints during training
(i.e. make further restrictions such as i? < i for a
trigger pair (ei, ei?)).
Optimization of the model scaling factors is car-
ried out using minimum error rate training (MERT)
on the development sets. The optimization criterion
is 100-BLEU since we want to maximize the BLEU
score.
4.2 Tasks
4.2.1 IWSLT
For the first part of the experiments, we use
the corpora that were released for the IWSLT?07
evaluation campaign. The training corpus con-
sists of approximately 43K Chinese-English sen-
tence pairs, mainly coming from the BTEC cor-
pus (Basic Travel Expression Corpus). This is a
multilingual speech corpus which contains tourism-
related material, such as transcribed conversations
about making reservations, asking for directions or
conversations as taking place in restaurants. For the
experiments, we use the clean data track, i.e. tran-
scriptions of read speech. As the development set
which is used for tuning the parameters of the base-
line system and the reranking framework, we use
the IWSLT?04 evaluation set (500 sentence pairs).
The two blind test sets which are used to evaluate
the final performance of the models are the official
evaluation sets from IWSLT?05 (506 sentences) and
IWSLT?07 (489 sentences).
The average sentence length of the training cor-
pus is 10 words. Thus, the task is somewhat lim-
ited and very domain-specific. One of the advan-
tages of this setting is that preliminary experiments
can be carried out quickly in order to analyze the ef-
fects of the different models in detail. This and the
small vocabulary size (12K entries) makes the cor-
pus ideal for first ?rapid application development?-
style setups without having to care about possible
constraints due to memory requirements or CPU
time restrictions.
4.2.2 EPPS
Furthermore, additional experiments are based on
the EPPS corpus (European Parliament Plenary Ses-
sions) as used within the FTE (Final Text Edition)
track of the TC-STAR evaluations. The corpus con-
tains speeches held by politicians at plenary sessions
of the European Parliament that have been tran-
scribed, ?corrected? to make up valid written texts
and translated into several target languages. The lan-
guage pairs considered in the experiments here are
Spanish-English and English-Spanish.
The training corpus consists of roughly 1.3M sen-
tence pairs with 35.5M running words on the En-
glish side. The vocabulary sizes are considerably
larger than for the IWSLT task, namely around 170K
on the target side. As development set, we use
the development data issued for the 2006 evaluation
(1122 sentences), whereas the two blind test sets are
the official evaluation data from 2006 (TC-Star?06,
1117 sentences) and 2007 (TC-Star?07, 1130 sen-
tences).
4.3 Results
4.3.1 IWSLT experiments
One of the first questions that arises is how many
EM iterations should be carried out during training
of the triplet model. Since the IWSLT task is small,
376
 56.6
 56.8
 57
 57.2
 57.4
 57.6
 0  10  20  30  40  50 34.6
 34.8
 35
 35.2
 35.4
 35.6
BL
EU
 sc
ore
TE
R s
cor
e
EM iterations
IWSLT?04 BLEUIWSLT?04 TER
Figure 2: Effect of EM iterations on IWSLT?04, left axis
shows BLEU (higher numbers better), right axis (dashed
graph) shows TER score (lower numbers better).
IWSLT?04 IWSLT?05
BLEU TER BLEU TER
baseline 56.7 35.49 61.1 30.59
pall(e|f, f ?) 57.1 35.03 61.3 30.55
w/ singletons 57.3 35.04 61.3 30.61
w/ empties 57.3 35.00 61.2 30.65
+ pall(f |e, e?) 57.5 34.69 61.7 30.24
Table 1: Different setups showing the effect of singletons
and empty words for IWSLT CE IWSLT?04 (dev) and
IWSLT?05 (test) sets, pall triplets, 20 EM iterations.
we can quickly run the experiments on a full uncon-
strained triplet model without any cutoff or further
constraints. Figure 2 shows the rescoring perfor-
mance for different numbers of EM iterations. The
first 10 iterations significantly improve the triplet
model performance for the IWSLT task. After that,
there are no big changes. The performance even de-
grades a little bit after 30 iterations. For the IWSLT
task, we therefore set a fixed number of 20 EM iter-
ations for the following experiments since it shows a
good performance in terms of both BLEU and TER
score. The oracle TER scores of the 10k-best lists
are 14.18% for IWSLT?04, 11.36% for IWSLT?05
and 18.85% for IWSLT?07, respectively.
The next chain of experiments on the IWSLT task
investigates the impact of changes to the setup of
training an unconstrained triplet model, such as the
addition of the empty word and the inclusion of sin-
gletons (i.e. triplets that were only seen once in the
IWSLT?05 IWSLT?07
BLEU TER BLEU TER
baseline 61.1 30.59 38.9 45.60
IBM model 1 61.5 30.29 39.4 45.31
trip fe+ef pall 61.7 30.24 39.7 45.24
trip fe+ef pphr 61.5 30.32 39.1 45.36
trip fe+ef palign 61.2 30.60 39.7 45.02
Table 2: Comparison of triplet variants on IWSLT CE test
sets, 20 EM iterations, with singletons and empty words.
training data). This might show the importance of
rare events in order to derive strategies when mov-
ing to larger tasks where it is not feasible to train all
possible triplets, such as e.g. on the EPPS task (as
shown later) or the Chinese-English NIST task. The
results for the unconstrained model are shown in Ta-
ble 1, beginning with a full triplet model in reverse
direction, pall (e|f, f ?), that contains no singletons
and no empty words for the triggering side. In this
setting, singletons seem to help on dev but there is no
clear improvement on one of the test sets, whereas
empty words do not make a significant difference but
can be used since they do not harm either. The base-
line can be improved by +0.6% BLEU and around
-0.5% in TER on the IWSLT?04 set. For the vari-
ous setups, there are no big differences in the TER
score which might be an effect of optimization on
BLEU. Therefore, for further experiments using the
constraints from Section 3.2, we use both singletons
and empty words as the default.
Adding the other direction p(f |e, e?) results in an-
other increase, with a total of +0.8% BLEU and
-0.8% TER, which shows that the combination of
both directions helps overall translation quality. The
results on the two test sets are shown in Table 2.
As can be seen, we arrive at similar improvements,
namely +0.6% BLEU and -0.3% TER on IWSLT?05
and +0.8% BLEU and -0.4% TER on IWSLT?07, re-
spectively. The constrained models, i.e. the phrase-
bounded (pphr ) and path-aligned (palign ) triplets are
outperformed by the full unconstrained case, al-
though on IWSLT?07 both unconstrained and path-
aligned models are close.
For a fair comparison, we added a classical IBM
model 1 in the rescoring framework. It can be seen
that the presented triplet models slightly outperform
377
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 52.3 34.57 50.4 36.46
trip fe+ef pall 52.9 34.32 50.6 36.34
+ max dist 10 52.9 34.20 50.8 36.22
Table 3: Effect of using maximum distance constraint for
pall on EPPS Spanish-English test sets, occ3, 4 EM iter-
ations due to time constraints.
the simple IBM model 1. Note that IBM model 1
is a special case of the triplet lexicon model if the
second trigger is the empty word.
4.3.2 EPPS experiments
Since EPPS is a considerably harder task (larger
vocabulary and longer sentences), the training of a
full unconstrained triplet model cannot be done due
to memory restrictions. One possibility to reduce
the number of extracted triplets is to apply a max-
imum distance constraint in the training procedure,
i.e. only trigger pairs are considered where the dis-
tance between first and second trigger is below or
equal to the specified maximum.
Table 3 shows the effect of a maximum distance
constraint for the Spanish-English direction. Due
to the large amount of triplets (we extract roughly
two billion triplets2 for the EPPS data), we drop all
triplets that occur less than 3 times which results in
640 million triplets. Also, due to time restrictions3,
we only train 4 iterations and compare it to 4 itera-
tions of the same setting with the maximum distance
set to 10. The training with the maximum distance
constraints ends with a total of 380 million triplets.
As can be seen (Table 3), the performance is compa-
rable while cutting down the computation time from
9.2 to 3.1 hours. The experiments were carried out
on a 2.2GHz Opteron machine with 16 GB of mem-
ory. The overall gain is +0.4?0.6% BLEU and up to
-0.4% in TER. We even observe a slight increase in
BLEU for the TC-Star?07 set which might be a ran-
dom effect due to optimization on the development
set where the behavior is the same as for TC-Star?06.
2Extraction can be easily done in parallel by splitting the
corpus and merging identical triplets iteratively in a separate
step for two chunks at a time.
3One iteration needs more than 12 hours for the uncon-
strained case.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
trip fe+ef pphr 50.2 37.01 51.5 35.38
+ occ2 50.2 37.06 51.8 35.32
Table 4: Results on EPPS, English-Spanish, pphr com-
bined, occ3, 10 EM iterations.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
using FA 50.0 37.18 51.7 35.52
using IBM4 50.0 37.12 51.7 35.43
+ occ2 50.2 36.84 52.0 35.10
+ max dist 1 50.0 37.10 51.7 35.51
Table 5: Results on EPPS, English-Spanish, maximum
approximation, palign combined, occ3, 10 EM iterations.
Results on EPPS English-Spanish for the phrase-
bounded triplet model are presented in Table 4.
Since the number of triplets is less than for the un-
constrained model, we can lower the cutoff from 3
to 2 (denoted in the table by occ3 and occ2 , respec-
tively). There is a small additional gain on the TC-
Star?07 test set by this step, with a total of +0.7%
BLEU for TC-Star?06 and +0.8% BLEU for TC-
Star?07.
Table 5 shows results for a variation of the path-
aligned triplet model palign that restricts the first trig-
ger to the best aligned word as estimated in the IBM
model 1, thus using a maximum-approximation of
the given word alignment. The model was trained
on two word alignments, firstly the one contained in
the forced alignments on the training data, and sec-
ondly on an IBM-4 word alignment generated using
GIZA++. For this second model we also demon-
strate the improvement obtained when increasing the
triplet lexicon size by using less trimming.
Another experiment was carried out to investigate
the effect of immediate neighboring words used as
triggers within the palign setting. This is equivalent
to using a ?maximum distance of 1? constraint. We
obtained worse results, namely a 0.2-0.3% drop in
BLEU and a 0.3-0.4% raise in TER (cf. Table 5,
last row), although the training is significantly faster
with this setup, namely roughly 30 minutes per it-
378
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
IBM model 1 50.0 37.12 51.8 35.51
pall , occ3 50.0 37.17 51.8 35.43
pphr , occ2 50.2 37.06 51.8 35.32
palign , occ2 50.2 36.84 52.0 35.10
Table 6: Final results on EPPS English-Spanish, con-
strained triplet models, 10 EM iterations, compared to
standard IBM model 1.
eration using less than 2 GB of memory. However,
this shows that triggers outside the immediate con-
text help overall translation quality. Additionally, it
supports the claim that the presented methods are a
complementary alternative to the WSD approaches
mentioned in Section 2 which only consider the im-
mediate context of a single word.
Finally, we compare the constrained models to an
unconstrained setting and, again, to a standard IBM
model 1. Table 6 shows that the palign model con-
strained on using the IBM-4 word alignments yields
+0.7% in BLEU on TC-Star?06 which is +0.2%
more than with a standard IBM model 1. TER de-
creases by -0.3% when compared to model 1. For
the TC-Star?07 set, the observations are similar.
The oracle TER scores of the development n-best
list are 25.16% for English-Spanish and 27.0% for
Spanish-English, respectively.
5 Discussion
From the results of our reranking experiments, we
can conclude that the presented triplet lexicon model
outperforms the baseline single-best hypotheses of
the decoder. When comparing to a standard IBM
model 1, the improvements are significantly smaller
though measurable. So far, since IBM model 1
is considered one of the stronger rescoring mod-
els, these results look promising. An unconstrained
triplet model has the best performance if training is
feasible since it also needs the most memory and
time to be trained, at least for larger tasks.
In order to cut down computational requirements,
we can apply phrase-bounded and path-aligned
training constraints that restrict the possibilities of
selecting triplet candidates (in addition to simple
f e e? ?(f |e, e?)
pagar taxpayer bill 0.76
factura taxpayer bill 0.11
contribuyente taxpayer bill 0.10
f e ? pibm1 (f |e)
contribuyente taxpayer 0.40
contribuyentes taxpayer 0.18
europeo taxpayer 0.08
factura bill 0.19
ley bill 0.18
proyecto bill 0.11
Table 7: Example of triplets and related IBM model 1
lexical probabilities. The triggers ?taxpayer? and ?bill?
have a new effect (?pagar?), previously not seen in the
top ranks of the lexicon.
thresholding). Although no clear effect could be
observed for adding empty words on the trigger-
ing side, it does not harm and, thus, we get a sim-
ilar functionality to IBM model 1 being ?integrated?
in the triplet lexicon model. The phrase-bounded
training variant uses forced alignments computed
on the whole training data (i.e. search constrained
to producing the target sentences of the bilingual
corpus) but could not outperform the path-aligned
model which reuses the alignment path information
obtained in regular GIZA++ training.
Additionally, we observe a positive impact from
triggers lying outside the immediate context of one
predecessor or successor word.
5.1 Examples
Table 7 shows an excerpt of the top entries for
(e, e?) = (taxpayer , bill) and compares it to the top
entries of a lexicon based on IBM model 1. We ob-
serve a triggering effect since the Spanish word pa-
gar (to pay) is triggered at top position by the two
English words taxpayer and bill. The average dis-
tance of taxpayer and bill is 5.4 words. The models
presented in this work try to capture this property
and apply it in the scoring of hypotheses in order to
allow for better lexical choice in specific contexts.
In Table 8, we show an example translation where
rescoring with the triplet model achieves higher n-
gram coverage on the reference translation than the
variant based on IBM model 1 rescoring. The differ-
ing phrases are highlighted.
379
Source sen-
tence
. . . respecto de la Posicio?n Comu?n
del Consejo con vistas a la adopcio?n
del Reglamento del Parlamento Eu-
ropeo y del Consejo relativo al . . .
IBM-1
rescoring
. . . on the Council common position
with a view to the adoption of the
Rules of Procedure of the European
Parliament and of the Council . . .
Triplet
rescoring
. . . on the common position of the
Council with a view to the adop-
tion of the regulation of the Euro-
pean Parliament and of the Council
. . .
Reference
translation
. . . as regards the Common Position
of the Council with a view to the
adoption of a European Parliament
and Council Regulation as regards
the . . .
Table 8: A translation example on TC-Star?07 Spanish-
English comparing the effect of the triplet model to a
standard IBM-1 model.
6 Outlook
We have presented a new lexicon model based on
triplets extracted on a sentence level and trained it-
eratively using the EM algorithm. The motivation of
this approach is to add an additional second trigger
to a translation lexicon component which can come
from a more global context (on a sentence level) and
allow for a more fine-grained lexical choice given a
specific context. Thus, the method is related to word
sense disambiguation approaches.
We showed improvements by rescoring n-best
lists of the IWSLT Chinese-English and EPPS
Spanish-English/English-Spanish task. In total, we
achieve up to +1% BLEU for some of the test sets in
comparison to the decoder baseline and up to +0.3%
BLEU compared to IBM model 1.
Future work will address an integration into the
decoder since the performance of the current rescor-
ing framework is limited by the quality of the n-
best lists. For the inverse model, p(e|f, f ?), an in-
tegration into the search is directly possible. Further
experiments will be conducted, especially on large
tasks such as the NIST Chinese-English and Arabic-
English task. Training on these huge databases will
only be possible with an appropriate selection of
promising triplets.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
The authors would like to thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stephen A. Della Pietra, Vincent J. Della Pietra, John R.
Gillett, John D. Lafferty, Harry Printz, and Lubos?
Ures?. 1994. Inference and estimation of a long-range
trigram model. In J. Oncina and R. C. Carrasco, ed-
itors, Grammatical Inference and Applications, Sec-
ond International Colloquium, ICGI-94, volume 862,
pages 78?92, Alicante, Spain. Springer Verlag.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?22.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
lingual lexical triggers in statistical language model-
ing. In Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
380
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
Christoph Tillmann and Hermann Ney. 1997. Word trig-
gers and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Language
Learning (ACL), pages 117?124, Madrid, Spain, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, RWTH
Aachen University, Aachen, Germany, May.
381
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210?218,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Extending Statistical Machine Translation with
Discriminative and Trigger-Based Lexicon Models
Arne Mauser and Sa
?
sa Hasan and Hermann Ney
Human Language Technology and Pattern Recognition Group
Chair of Computer Science 6, RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this work, we propose two extensions of
standard word lexicons in statistical ma-
chine translation: A discriminative word
lexicon that uses sentence-level source in-
formation to predict the target words and
a trigger-based lexicon model that extends
IBM model 1 with a second trigger, allow-
ing for a more fine-grained lexical choice
of target words. The models capture de-
pendencies that go beyond the scope of
conventional SMT models such as phrase-
and language models. We show that the
models improve translation quality by 1%
in BLEU over a competitive baseline on a
large-scale task.
1 Introduction
Lexical dependencies modeled in standard phrase-
based SMT are rather local. Even though the deci-
sion about the best translation is made on sentence
level, phrase models and word lexicons usually do
not take context beyond the phrase boundaries into
account. This is especially problematic since the
average source phrase length used during decod-
ing is small. When translating Chinese to English,
e.g., it is typically close to only two words.
The target language model is the only model
that uses lexical context across phrase boundaries.
It is a very important feature in the log-linear setup
of today?s phrase-based decoders. However, its
context is typically limited to three to six words
and it is not informed about the source sentence.
In the presented models, we explicitly take advan-
tage of sentence-level dependencies including the
source side and make non-local predictions for the
target words. This is an important aspect when
translating from languages like German and Chi-
nese where long-distance dependencies are com-
mon. In Chinese, for example, tenses are often en-
coded by indicator words and particles whose po-
sition is relatively free in the sentence. In German,
prefixes of verbs can be moved over long distances
towards the end of the sentence.
In this work, we propose two models that can
be categorized as extensions of standard word lex-
icons: A discriminative word lexicon that uses
global, i.e. sentence-level source information to
predict the target words using a statistical classi-
fier and a trigger-based lexicon model that extends
the well-known IBM model 1 (Brown et al, 1993)
with a second trigger, allowing for a more fine-
grained lexical choice of target words. The log-
linear framework of the discriminative word lexi-
con offers a high degree of flexibility in the selec-
tion of features. Other sources of information such
as syntax or morphology can be easily integrated.
The trigger-based lexicon model, or simply
triplet model since it is based on word triplets,
is not trained discriminatively but uses the classi-
cal maximum likelihood approach (MLE) instead.
We train the triplets iteratively on a training cor-
pus using the Expectation-Maximization (EM) al-
gorithm. We will present how both models al-
low for a representation of topic-related sentence-
level information which puts them close to word
sense disambiguation (WSD) approaches. As will
be shown later, the experiments indicate that these
models help to ensure translation of content words
that are often omitted by the baseline system. This
is a common problem in Chinese-English transla-
tion. Furthermore, the models are often capable to
produce a better lexical choice of content words.
210
The structure of the paper is as follows: In Sec-
tion 2, we will address related work and briefly
pin down how our models differentiate from pre-
vious work. Section 3 will describe the discrimi-
native lexical selection model and the triplet model
in more detail, explain the training procedures and
show how the models are integrated into the de-
coder. The experimental setup and results will be
given in Section 4. A more detailed discussion
will be presented in Section 5. In the end, we con-
clude our findings and give an outlook for further
research in Section 6.
2 Related Work
Several word lexicon models have emerged in the
context of multilingual natural language process-
ing. Some of them were used as a machine transla-
tion system or as a part of one such system. There
are three major types of models: Heuristic models
as in (Melamed, 2000), generative models as the
IBM models (Brown et al, 1993) and discrimina-
tive models (Varea et al, 2001; Bangalore et al,
2006).
Similar to this work, the authors of (Varea et
al., 2001) try to incorporate a maximum entropy
lexicon model into an SMT system. They use
the words and word classes from the local con-
text as features and show improvements with n-
best rescoring.
The models in this paper are also related to
word sense disambiguation (WSD). For example,
(Chan et al, 2007) trained a discriminative model
for WSD using local but also across-sentence un-
igram collocations of words in order to refine
phrase pair selection dynamically by incorporat-
ing scores from the WSD classifier. They showed
improvements in translation quality in a hierar-
chical phrase-based translation system. Another
WSD approach incorporating context-dependent
phrasal translation lexicons is given in (Carpuat
and Wu, 2007) and has been evaluated on sev-
eral translation tasks. Our model differs from the
latter in three ways. First, our approach mod-
els word selection of the target sentence based on
global sentence-level features of the source sen-
tence. Second, instead of disambiguating phrase
senses as in (Carpuat and Wu, 2007), we model
word selection independently of the phrases used
in the MT models. Finally, the training is done in a
different way as will be presented in Sections 3.1.1
and 3.2.1.
Recently, full translation models using discrim-
inative training criteria emerged as well. They
are designed to generate a translation for a given
source sentence and not only score or disam-
biguate hypotheses given by a translation system.
In (Ittycheriah and Roukos, 2007), the model can
predict 1-to-many translations with gaps and uses
words, morphologic and syntactic features from
the local context.
The authors of (Venkatapathy and Bangalore,
2007) propose three different models. The first
one is a global lexical selection model which in-
cludes all words of the source sentence as features,
regardless of their position. Using these features,
the system predicts the words that should be in-
cluded in the target sentence. Sentence structure is
then reconstructed using permutations of the gen-
erated bag of target words. We will also use this
type of features in our model.
One of the simplest models in the context of
lexical triggers is the IBM model 1 (Brown et
al., 1993) which captures lexical dependencies be-
tween source and target words. It can be seen
as a lexicon containing correspondents of transla-
tions of source and target words in a very broad
sense since the pairs are trained on the full sen-
tence level. The trigger-based lexicon model used
in this work follows the training procedure intro-
duced in (Hasan et al, 2008) and is integrated di-
rectly in the decoder instead of being applied in
n-best list reranking. The model is very close to
the IBM model 1 and can be seen as an extension
of it by taking another word into the condition-
ing part, i.e. the triggering items. Thus, instead
of p(f |e), it models p(f |e, e
?
). Furthermore, since
the second trigger can come from any part of the
sentence, there is a link to long-range monolin-
gual triggers as presented in (Tillmann and Ney,
1997) where a trigger language model was trained
using the EM algorithm and helped to reduce per-
plexities and word error rates in a speech recog-
nition experiment. In (Rosenfeld, 1996), another
approach was chosen to model monolingual trig-
gers using a maximum-entropy based framework.
Again, this adapted LM could improve speech
recognition performance significantly.
A comparison of a variant of the trigger-based
lexicon model applied in decoding and n-best list
reranking can be found in (Hasan and Ney, 2009).
In order to reduce the number of overall triplets,
the authors use the word alignments for fixing the
211
first trigger to the aligned target word. In general,
this constraint performs slightly worse than the un-
constrained variant used in this work, but allows
for faster training and decoding.
3 Extended Lexicon Models
In this section, we present the extended lexicon
models, how they are trained and integrated into
the phrase-based decoder.
3.1 Discriminative Lexicon Model
Discriminative models have been shown to outper-
form generative models on many natural language
processing tasks. For machine translation, how-
ever, the adaptation of these methods is difficult
due to the large space of possible translations and
the size of the training data that has to be used to
achieve significant improvements.
In this section, we propose a discriminative
word lexicon model that follows (Bangalore et al,
2007) and integrate it into the standard phrase-
based machine translation approach.
The core of our model is a classifier that pre-
dicts target words, given the words of the source
sentence. The structure of source as well as tar-
get sentence is neglected in this model. We do
not make any assumptions about the location of
the words in the sentence. This is useful in many
cases, as words and morphology can depend on in-
formation given at other positions in the sentence.
An example would be the character? in Chinese
that indicates a completed or past action and does
not need to appear close to the verb.
We model the probability of the set of target
words in a sentence e given the set of source words
f . For each word in the target vocabulary, we can
calculate a probability for being or not being in-
cluded in the set. The probability of the whole set
then is the product over the entire target vocabu-
lary V
E
:
P (e|f) =
?
e?e
P (e
+
|f) ?
?
e?V
E
\e
P (e
?
|f) (1)
For notational simplicity, we use the event e
+
when the target word e is included in the target
sentence and e
?
if not. We model the individual
factors p(e|f) of the probability in Eq. 1 as a log-
linear model using the source words from f as bi-
nary features
?(f, f) =
{
1 if f ? f
0 else
(2)
and feature weights ?
f,?
:
P (e
+
|f) =
exp
(
?
f?f
?
f,e
+ ?(f, f)
)
?
e?{e
+
,e
?
}
exp
(
?
f?f
?
f,e
?(f, f)
)
(3)
Subsequently, we will call this model discrimina-
tive word lexicon (DWL).
Modeling the lexicon on sets and not on se-
quences has two reasons. Phrase-based MT along
with n-gram language models is strong at predict-
ing sequences but only uses information from a lo-
cal context. By using global features and predict-
ing words in a non-local fashion, we can augment
the strong local decisions from the phrase-based
systems with sentence-level information.
For practical reasons, translating from a set to
a set simplifies the parallelization of the training
procedure. The classifiers for the target words can
be trained separately as explained in the following
section.
3.1.1 Training
Common classification tasks have a relatively
small number of classes. In our case, the num-
ber of classes is the size of the target vocabulary.
For large translation tasks, this is in the range of a
hundred thousand classes. It is far from what con-
ventional out-of-the-box classifiers can handle.
The discriminative word lexicon model has the
convenient property that we can train a separate
model for each target word making paralleliza-
tion straightforward. Discussions about possible
classifiers and the choice of regularization can
be found in (Bangalore et al, 2007). We used
the freely available MegaM Toolkit
1
for training,
which implements the L-BFGS method (Byrd et
al., 1995). Regularization is done using Gaussian
priors. We performed 100 iterations of the train-
ing algorithm for each word in the target vocabu-
lary. This results in a large number of classifiers to
be trained. For the Arabic-English data (cf. Sec-
tion 4), the training took an average of 38 seconds
per word. No feature cutoff was used.
3.1.2 Decoding
In search, we compute the model probabilities as
an additional model in the log-linear model com-
bination of the phrase-based translation approach.
To reduce the memory footprint and startup time
of the decoding process, we reduced the number of
1
http://www.cs.utah.edu/
?
hal/megam/
212
parameters by keeping only large values ?
f,e
since
smaller values tend to have less effect on the over-
all probability. In experiments we determined that
we could safely reduce the size of the final model
by a factor of ten without losing predictive power.
In search, we compute the model probabilities as
an additional model in the log-linear combination.
When scoring hypotheses from the phrase-based
system, we see the translation hypothesis as the
set of target words that are predicted. Words from
the target vocabulary which are not included in
the hypothesis are not part of the set. During the
search process, however, we also have to score in-
complete hypotheses where we do not know which
words will not be included. This problem is cir-
cumvented by rewriting Eq. 1 as
P (e|f) =
?
e?V
E
P (e
?
|f) ?
?
e?e
P (e
+
|f)
P (e
?
|f)
.
The first product is constant given a source sen-
tence and therefore does not affect the search. Us-
ing the model assumption from Eq. 3, we can fur-
ther simplify the computation and compute the
model score entirely in log-space which is numer-
ically stable even for large vocabularies. Exper-
iments showed that using only the first factor of
Eq. 1 is sufficient to obtain good results.
In comparison with the translation model from
(Bangalore et al, 2007) where a threshold on the
probability is used to determine which words are
included in the target sentence, our approach relies
on the phrase model to generate translation candi-
dates. This has several advantages: The length of
the translation is determined by the phrase model.
Words occurring multiple times in the translation
do not have to be explicitly modeled. In (Banga-
lore et al, 2007), repeated target words are treated
as distinct classes.
The main advantage of the integration being
done in a way as presented here is that the phrase
model and the discriminative word lexicon model
are complementary in the way they model the
translation. While the phrase model is good in
predicting translations in a local context, the dis-
criminative word lexicon model is able to predict
global aspects of the sentence like tense or vocabu-
lary changes in questions. While the phrase model
is closely tied to the structure of word and phrase
alignments, the discriminative word lexicon model
completely disregards the structure in source and
target sentences.
3.2 Trigger-based Lexicon Model
The triplets of the trigger-based lexicon model,
i.e. p(e|f, f
?
), are composed of two words in the
source language triggering one target language
word. We chose this inverse direction since it
can be integrated directly into the decoder and,
thus, does not rely on a two-pass approach us-
ing reranking, as it is the case for (Hasan et al,
2008). The triggers can originate from words of
the whole source sentence, also crossing phrase
boundaries of the conventional bilingual phrase
pairs. The model is symmetric though, mean-
ing that the order of the triggers is not relevant,
i.e. (f, f
?
? e) = (f
?
, f ? e). Nevertheless,
the model is able to capture long-distance effects
such as verb splits or adjustments to lexical choice
of the target word given the topic-triggers of the
source sentence. In training, we determine the
probability of a target sentence e
I
1
given the source
sentence f
J
1
within the model by
p(e
I
1
|f
J
1
) =
I
?
i=1
p(e
i
|f
J
1
)
=
I
?
i=1
2
J(J + 1)
J
?
j=0
J
?
j
?
=j+1
p(e
i
|f
j
, f
j
?
), (4)
where f
0
denotes the empty word and, thus, for
f
j
= ?, allows for modeling the conventional (in-
verse) IBM model 1 lexical probabilities as well.
Since the second trigger f
j
?
always starts right of
the current first trigger, the model is symmetric
and does not need to look at all trigger pairs. Eq. 4
is used in the iterative EM training on all sentence
pairs of the training data which is described in
more detail in the following.
3.2.1 Training
For training the trigger-based lexicon model, we
apply the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). The goal is to max-
imize the log-likelihood F
trip
of this model for
a given bilingual training corpus {(f
J
n
1
, e
I
n
1
)}
N
1
consisting of N sentence pairs:
F
trip
:=
N
?
n=1
log p(e
I
n
1
|f
J
n
1
),
where I
n
and J
n
are the lengths of the n-th tar-
get and source sentence, respectively. An aux-
iliary function Q(?; ??) is defined based on F
trip
213
where ?? is the updated estimate within an itera-
tion which is to be derived from the current esti-
mate ?. Here, ? stands for the entire set of model
parameters, i.e. the set of all {?(e|f, f
?
)} with the
constraint
?
e
?(e|f, f
?
) = 1. The accumulators
?(?) are therefore iteratively trained on the train-
ing data by using the current estimate, i.e. deriv-
ing the expected value (E-step), and maximizing
their likelihood afterwards to reestimate the distri-
bution. Thus, the perplexity of the training data is
reduced in each iteration.
3.2.2 Decoding
In search, we can apply this model directly when
scoring bilingual phrase pairs. Given a trained
model for p(e|f, f
?
), we compute the feature score
h
trip
(?) of a phrase pair (e?,
?
f) as
h
trip
(e?,
?
f, f
J
0
) = (5)
?
?
i
log
(
2
J ? (J + 1)
?
j
?
j
?
>j
p(e?
i
|f
j
, f
j
?
)
)
,
where i moves over all target words in the phrase
e?, the second sum selects all source sentence
words f
J
0
including the empty word, and j
?
> j
incorporates the rest of the source sentence right of
the first trigger. We take negative log-probabilities
and normalize to obtain the final score (represent-
ing costs) for the given phrase pair. Note that in
search, we can only use this direction, p(e|f, f
?
),
since the whole source sentence is available for
triggering effects whereas not all target words
have been generated so far, as it would be neces-
sary for the standard direction, p(f |e, e
?
).
Due to the enormous number of triplets, we
trained the model on a subset of the overall train-
ing data. The subcorpus, mainly consisting of
newswire articles, contained 1.4M sentence pairs
with 32.3M running words on the English side.
We trained two versions of the triplet lexicon, one
using 4 EM iterations and another one that was
trained for 10 EM iterations. Due to trimming
of triplets with small probabilities after each it-
eration, the version based on 10 iterations was
slightly smaller, having 164 million triplets but
also performed slightly worse. Thus, for the ex-
periments, we used the version based on 4 itera-
tions which contained 291 million triplets.
Note that decoding with this model can be quite
efficient if caching is applied. Since the given
source sentence does not change, we have to cal-
culate p(e|f, f
?
) for each e only once and can re-
train (C/E) test08 (NW/WT)
Sent. pairs 9.1M 480 490
Run. words 259M/300M 14.8K 12.3K
Vocabulary 357K/627K 3.6K 3.2K
Table 1: GALE Chinese-English corpus statistics
including two test sets: newswire and web text.
train C/E ? A/E nist08 C/A
Sent. pairs 7.3M 4.6M 1357
Words (M) 185/196 142/139 36K/46K
Vocab. (K) 163/265 351/361 6.4K/9.6K
Table 2: NIST Chinese-English and Arabic-
English corpus statistics including the official
2008 test sets.
trieve the probabilities from the cache for consec-
utive scorings of the same target word e. This sig-
nificantly speeds up the decoding process.
4 Experimental Evaluation
In this section we evaluate our lexicon models on
the GALE Chinese-English task for newswire and
web text translation and additionally on the of-
ficial NIST 2008 task for both Chinese-English
and Arabic-English. The baseline system was
built using a state-of-the art phrase-based MT sys-
tem (Zens and Ney, 2008). We use the standard
set of models with phrase translation probabilities
for source-to-target and target-to-source direction,
smoothing with lexical weights, a word and phrase
penalty, distance-based and lexicalized reordering
and a 5-gram (GALE) or 6-gram (NIST) target
language model.
We used training data provided by the Linguis-
tic Data Consortium (LDC) consisting of 9.1M
parallel Chinese-English sentence pairs of vari-
ous domains for GALE (cf. Table 1) and smaller
amounts of data for the NIST systems (cf. Ta-
ble 2). The DWL and Triplet models were inte-
grated into the decoder as presented in Section 3.
For the GALE development and test set, we sep-
arated the newswire and web text parts and did
separate parameter tuning for each genre using
the corresponding development set which consists
of 485 sentences for newswire texts and 533 sen-
tences of web text. The test set has 480 sentences
for newswire and 490 sentences for web text. For
NIST, we tuned on the official 2006 eval set and
used the 2008 evaluation set as a blind test set.
214
GALE NW WT
test08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 32.3 59.38 25.3 64.40
DWL 33.1 58.90 26.2 63.75
Triplet 32.9 58.59 26.2 64.20
DWL+Trip. 33.3 58.23 26.3 63.87
Table 3: Results on the GALE Chinese-English
test set for the newswire and web text setting
(case-insensitive evaluation).
4.1 Translation Results
The translation results on the two GALE test
sets are shown in Table 3 for newswire and web
text. Both the discriminative word lexicon and the
triplet lexicon can individually improve the base-
line by approximately +0.6?0.9% BLEU and -0.5?
0.8% TER. For the combination of both lexicons
on the newswire setting, we observe only a slight
improvement on BLEU but also an additional
boost in TER reduction, arriving at +1% BLEU
and -1.2% TER. For web text, the findings are sim-
ilar: The combination of the discriminative and
trigger-based lexicons yields +1% BLEU and de-
creases TER by -0.5%.
We compared these results against an inverse
IBM model 1 but the results were inconclusive
which is consistent with the results presented in
(Och et al, 2004) where no improvements were
achieved using p(e|f). In our case, inverse IBM1
improves results by 0.2?0.4% BLEU on the devel-
opment set but does not show the same trend on
the test sets. Furthermore, combining IBM1 with
DWL or Triplets often even degraded the transla-
tion results, e.g. only 32.8% BLEU was achieved
on newswire for a combination of the IBM1, DWL
and Triplet model. In contrast, combinations of
the DWL and Triplet model did not degrade per-
formance and could benefit from each other.
In addition to the automatic scoring, we also
did a randomized subjective evaluation where the
hypotheses of the baseline was compared against
the hypotheses generated using the discrimina-
tive word lexicon and triplet models. We evalu-
ated 200 sentences from newswire and web text.
In 80% of the evaluated sentences, the improved
models were judged equal or better than the base-
line.
We tested the presented lexicon models also on
another large-scale system, i.e. NIST, for two lan-
NIST Chinese-Eng. Arabic-Eng.
nist08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 26.8 65.11 42.0 50.55
DWL 27.6 63.56 42.4 50.01
Triplet 27.7 63.60 42.9 49.76
DWL+Trip. 27.9 63.56 43.0 49.15
Table 4: Results on the test sets for the NIST 2008
Chinese-English and Arabic-English task (case-
insensitive evaluation).
guage pairs, namely Chinese-English and Arabic-
English. Interestingly, the results obtained for
Arabic-English are similar to the findings for
Chinese-English, as can be seen in Table 4. The
overall improvements for this language pair are
+1% BLEU and -1.4% TER. In contrast to the
GALE Chinese-English task, the triplet lexicon
model for the Arabic-English language pair per-
forms slightly better than the discriminative word
lexicon.
These results strengthen the claim that the pre-
sented models are capable of improving lexical
choice of the MT system. In the next section, we
discuss the observed effects and analyze our re-
sults in more detail.
5 Discussion
In terms of automatic evaluation measures, the re-
sults indicate that it is helpful to incorporate the
extended lexicon models into the search process.
In this section, we will analyze some more details
of the models and take a look at the lexical choice
they make and what differentiates them from the
baseline models. In Table 5, we picked an ex-
ample sentence from the GALE newswire test set
and show the different hypotheses produced by our
system. As can be seen, the baseline does not
produce the present participle of the verb restore
which makes the sentence somewhat hard to un-
derstand. Both the discriminative and the trigger-
based lexicon approach are capable of generating
this missing information, i.e. the correct use of
restoring. Figure 1 gives an example how discon-
tinuous triggers affect the word choice on the tar-
get side. Two cases are depicted where high proba-
bilities of triplets including emergency and restor-
ing on the target side influence the overall hypoth-
esis selection. The non-local modeling advantages
of the triplet model can be observed as well: The
215
?? , ?? ?? ? ?? ?? ?? ?? ?? .source
target [...] the emergency rescue group is [...] restoring  the ventilation system.
p(restoring | ??,  ? ) = 0.1572p(emergency | ??, ??) = 0.3445
Figure 1: Triggering effect for the example sentence using the triplet lexicon model. The Chinese source
sentence is shown in its segmented form. Two triplets are highlighted that have high probability and
favor the target words emergency and restoring.
Figure 2: Ranking of words for the example sentence for IBM1, Triplet and DWL model. Ranks are
sorted at IBM1, darker colors indicate higher probabilities within the model.
triggering events do not need to be located next
to each other or within a given phrase pair. They
move across the whole source sentence, thus al-
lowing for capturing of long-range dependencies.
Table 6 shows the top ten content words that are
predicted by the two models, discriminative word
lexicon and triplet lexicon model. IBM model 1
ranks are indicated by subscripts in the column
of the triplet model. Although the triplet model
is similar to IBM1, we observe differences in the
word lists. Comparing this to the visualization of
the probability distribution for the example sen-
tence, cf. Figure 2, we argue that, although the
IBM1 and Triplet distributions look similar, the
triplet model is sharper and favors words such as
the ones in Table 6, resulting in different word
choice in the translation process. In contrast, the
DWL approach gives more distinct probabilities,
selecting content words that are not chosen by the
other models.
Table 7 shows an example from the web text
test set. Here, the baseline hypothesis contains
an incorrect word, anna, which might have been
mistaken for the name ying. Interestingly, the hy-
potheses of the DWL lexicon and the combina-
tion of DWL and Triplet contain the correct con-
tent word remarks. The triplet model makes an er-
ror by selecting music, an artifact that might come
from words that co-occur frequently with the cor-
responding Chinese verb to listen, i.e. ? , in the
data. Although the TER score of the baseline is
better than the one for the alternative models for
this particular example, we still think that the ob-
served effects show how our models help produc-
ing different hypotheses that might lead to subjec-
tively better translations.
An Arabic-English translation example is
shown in Table 8. Here, the term incidents of mur-
der in apartments was chosen over the baseline?s
killings inside the flats. Both translations are un-
derstandable and the difference in the wording is
only based on synonyms. The translation using
the discriminative and trigger-based lexicons bet-
ter matches the reference translation and, thus, re-
flects a better lexical choice of the content words.
6 Conclusion
We have presented two lexicon models that use
global source sentence context and are capable
of predicting context-specific target words. The
models have been directly integrated into the de-
coder and have shown to improve the translation
quality of a state-of-the-art phrase-based machine
translation system. The first model was a dis-
criminative word lexicon that uses sentence-level
features to predict if a word from the target vo-
cabulary should be included in the translation or
not. The second model was a trigger-based lexi-
216
Source ?? , ?? ?? ? ?? ??
?????? .
Baseline at present, the accident and rescue
teams are currently emergency re-
covery ventilation systems.
DWL at present, the emergency rescue
teams are currently restoring the
ventilation system.
Triplet at present, the emergency rescue
group is in the process of restoring
the ventilation system.
DWL
+Triplet
at present, the accident emergency
rescue teams are currently restor-
ing the ventilation system.
Reference right now, the accident emergency
rescue team is making emergency
repair on the ventilation system.
Table 5: Translation example from the GALE
newswire test set, comparing the baseline and the
extended lexicon models given a reference trans-
lation. The Chinese source sentence is presented
in its segmented form.
con that uses triplets to model long-range depen-
dencies in the data. The source word triggers can
move across the whole sentence and capture the
topic of the sentence and incorporate more fine-
grained lexical choice of the target words within
the decoder.
Overall improvements are up to +1% in BLEU
and -1.5% in TER on large-scale systems for
Chinese-English and Arabic-English. Compared
to the inverse IBM model 1 which did not yield
consistent improvements, the presented models
are valuable additional features in a phrase-based
statistical machine translation system. We will test
this setup for other language pairs and expect that
languages like German where long-distance ef-
fects are common can benefit from these extended
lexicon models.
In future work, we plan to extend the discrimi-
native word lexicon model in two directions: ex-
tending context to the document level and feature
engineering. For the trigger-based model, we plan
to investigate more model variants. It might be
interesting to look at cross-lingual trigger mod-
els such as p(f |e, f
?
) or constrained variants like
p(f |e, e
?
) with pos(e
?
) < pos(e), i.e. the second
trigger coming from the left context within a sen-
tence which has already been generated. These
DWL Triplet
emergency 0.894 emergency
1
0.048
currently 0.330 system
2
0.032
current 0.175 rescue
8
0.027
emergencies 0.133 accident
3
0.022
present 0.133 ventilation
7
0.021
accident 0.119 work
33
0.021
recovery 0.053 present
5
0.011
group 0.046 currently
9
0.010
dealing 0.042 rush
60
0.010
ventilation 0.034 restoration
31
0.009
Table 6: The top 10 content words predicted by
each model for the GALE newswire example sen-
tence. Original ranks for the related IBM model 1
are given as subscripts for the triplet model.
Source ?? ???? , ?????
? .
Baseline i have listened to anna, happy and
laugh.
DWL i have listened to the remarks,
happy and laugh.
Triplet i have listened to the music, a roar
of laughter.
DWL
+Triplet
i have listened to the remarks,
happy and laugh.
Reference hearing ying?s remark, i laughed
aloud happily.
Table 7: Translation example from the GALE web
text test set. In this case, the baseline has a bet-
ter TER but we can observe a corrected content
word (remark) for the extended lexicon models.
The Chinese source sentence is shown in its seg-
mented form.
extensions could be integrated directly in search
as well and would enable the system to combine
both directions (standard and inverse) to some ex-
tent which was previously shown to help when ap-
plying the standard direction p(f |e, e
?
) as an addi-
tional reranking step, cf. (Hasan and Ney, 2009).
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly realized as part of
the Quaero Programme, funded by OSEO, French
State agency for innovation.
The authors would like to thank Christian Buck
217
Source
	
?j
.
??@ ?

I
	
?Q?

K ?



?? @

HBAm
?
'@
	
?? @XY?

HQ?

?
	
 Y

?

?K


X????@
	
?j??@
	
??K
.

I
	
KA? ?
. A?
Q



	
? ?

?

?

??@ ?
	
g@X ?

J

?? @

HX@?k
	
??K
.
??
	
Y? ? P
Q

.
?
	
??X
Baseline some saudi newspapers have published a number of cases that had been subjected to
imprisonment without justification, as well as some killings inside the flats and others.
DWL
+Triplet
some of the saudi newspapers have published a number of cases which were subjected
to imprisonment without justification, as well as some incidents of murder in apartments
and others.
Reference some saudi newspapers have published a number of cases in which people were unjusti-
fiably imprisoned, as well as some incidents of murder in apartments and elsewhere.
Table 8: Translation example from the NIST Arabic-English test set. The DWL and Triplet models
improve lexical word choice by favoring incidents of murder in apartments instead of killings inside the
flats. The Arabic source is shown in its segmented form.
and Juri Ganitkevitch for their help training the ex-
tended lexicon models.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2006. Se-
quence classification for machine translation. In
Ninth International Conf. on Spoken Language Pro-
cessing, Interspeech 2006 ? ICSLP, pages 1722?
1725, Pitsburgh, PA, September.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 152?159, Prague, Czech Republic,
June.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. 1995. A
limited memory algorithm for bound constrained op-
timization. SIAM Journal on Scientific Computing,
16(5):1190?1208.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In 45th Annual Meeting of the Association
of Computational Linguistics, pages 33?40, Prague,
Czech Republic, June.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?22.
S. Hasan and H. Ney. 2009. Comparison of extended
lexicon models in search and rescoring for SMT. In
NAACL HLT 2009, Companion Volume: Short Pa-
pers, pages 17?20, Boulder, Colorado, June.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In EMNLP, pages 372?381, Honolulu,
Hawaii, October.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL 2007: Main Conference,
pages 57?64, Rochester, New York, April.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. pages 161?168, Boston, MA, May.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
C. Tillmann and H. Ney. 1997. Word triggers
and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Lan-
guage Learning (ACL), pages 117?124, Madrid,
Spain, July.
I. Garc??a Varea, F. J. Och, H. Ney, and F. Casacu-
berta. 2001. Refined lexicon models for statistical
machine translation using a maximum entropy ap-
proach. In ACL ?01: 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 204?
211, Morristown, NJ, USA.
S. Venkatapathy and S. Bangalore. 2007. Three
models for discriminative machine translation us-
ing global lexical selection and sentence reconstruc-
tion. In SSST, NAACL-HLT 2007 / AMTA Workshop
on Syntax and Structure in Statistical Translation,
pages 96?102, Rochester, New York, April.
R. Zens and H. Ney. 2008. Improvements in dynamic
programming beam search for phrase-based statis-
tical machine translation. In International Work-
shop on Spoken Language Translation, Honolulu,
Hawaii, October.
218
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of NAACL HLT 2007, Companion Volume, pages 57?60,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Are Very Large N-best Lists Useful for SMT?
Sas?a Hasan, Richard Zens, Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{hasan,zens,ney}@cs.rwth-aachen.de
Abstract
This paper describes an efficient method
to extract large n-best lists from a word
graph produced by a statistical machine
translation system. The extraction is based
on the k shortest paths algorithm which
is efficient even for very large k. We
show that, although we can generate large
amounts of distinct translation hypothe-
ses, these numerous candidates are not
able to significantly improve overall sys-
tem performance. We conclude that large
n-best lists would benefit from better dis-
criminating models.
1 Introduction
This paper investigates the properties of large n-
best lists in the context of statistical machine trans-
lation (SMT). We present a method that allows for
fast extraction of very large n-best lists based on
the k shortest paths algorithm by (Eppstein, 1998).
We will argue that, despite being able to generate a
much larger amount of hypotheses than previously
reported in the literature, there is no significant gain
of such a method in terms of translation quality.
In recent years, phrase-based approaches evolved
as the dominating method for feasible machine
translation systems. Many research groups use a de-
coder based on a log-linear approach incorporating
phrases as main paradigm (Koehn et al, 2003). As a
by-product of the decoding process, one can extract
n-best translations from a word graph and use these
fully generated hypotheses for additional reranking.
In the past, several groups report on using n-best
lists with n ranging from 1 000 to 10 000. The ad-
vantage of n-best reranking is clear: we can apply
complex reranking techniques, based e.g. on syntac-
tic analyses of the candidates or using huge addi-
tional language models, since the whole sentence is
already generated. During the generation process,
these models would either need hard-to-implement
algorithms or large memory requirements.
1.1 Related work
The idea of n-best list extraction from a word graph
for SMT was presented in (Ueffing et al, 2002). In
(Zens and Ney, 2005), an improved method is re-
ported that overcomes some shortcomings, such as
duplicate removal by determinization of the word
graph (represented as a weighted finite state automa-
ton) and efficient rest-cost estimation with linear
time complexity.
There are several research groups that use a two-
pass approach in their MT systems. First, they gen-
erate n-best translation hypotheses with the decoder.
Second, they apply additional models to the out-
put and rerank the candidates (see e.g. (Chen et al,
2006)).
Syntactic features were investigated in (Och et al,
2004) with moderate success. Although complex
models, such as features based on shallow parsing or
treebank-based syntactic analyses, were applied to
the n-best candidates, the ?simpler? ones were more
promising (e.g. IBM model 1 on sentence-level).
In the following section 2, we describe our SMT
system and explain how an improved n-best extrac-
tion method is capable of generating a very large
number of distinct candidates from the word graph.
In section 3, we show our experiments related to
n-best list reranking with various sizes and the cor-
responding performance in terms of MT evaluation
measures. Finally, we discuss the results in section 4
and give some conclusive remarks.
57
2 Generating N-best lists
We use a phrase-based SMT system (Mauser et al,
2006) and enhance the n-best list extraction with
Eppstein?s k shortest path algorithm which allows
for generating a very large number of translation
candidates in an efficient way.
2.1 Baseline SMT system
The baseline system uses phrases automatically ex-
tracted from a word-aligned corpus (trained with
GIZA++) and generates the best translations using
weighted log-linear model combination with several
features, such as word lexicon, phrase translation
and language models. This direct approach is cur-
rently used by most state-of-the-art decoders. The
model scaling factors are trained discriminatively on
some evaluation measure, e.g. BLEU or WER, using
the simplex method.
2.2 N-best list extraction
We incorporated an efficient extraction of n best
translations using the k shortest path algorithm
(Eppstein, 1998) into a state-of-the-art SMT system.
The implementation is partly based on code that is
publicly available.1
Starting point for the extraction is a word graph,
generated separately by the decoder for each sen-
tence. Since these word graphs are directed and
acyclic, it is possible to construct a shortest path tree
spanning from the sentence begin node to the end
node. The efficiency of finding the k shortest paths
in this tree lies in the book-keeping of edges through
a binary heap that allows for an implicit representa-
tion of paths. The overall performance of the algo-
rithm is efficient even for large k. Thus, it is feasi-
ble to use in situations where we want to generate a
large number of paths, i.e. translation hypotheses in
this context.
There is another issue that has to be addressed.
In phrase-based SMT, we have to deal with differ-
ent phrase segmentations for each sentence. Due to
the large number of phrases, it is possible that we
have paths through the word graph representing the
same sentence but internally having different phrase
boundaries. In n-best list generation, we want to get
rid of these duplicates. Due to the efficiency of the
k shortest paths algorithm, we allow for generating
a very large number of hypotheses (e.g. 100 ? n) and
1http://www.ics.uci.edu/?eppstein/pubs/
graehl.zip
then filter the output via a prefix tree (also called
trie) until we get n distinct translations.
With this method, it is feasible to generate
100 000-best lists without much hassle. In gen-
eral, the file input/output operations are more time-
consuming than the actual n-best list extraction.
The average generation time of n-best candidates
for each of the sentences of the development list
is approximately 30 seconds on a 2.2GHz Opteron
machine, whereas 7.4 million hypotheses are com-
puted per sentence on average. The overall extrac-
tion time including filtering and writing to hard-disk
takes around 100 seconds per sentence. Note that
this value could be optimized drastically if checking
for how many duplicates are generated on average
beforehand and adjusting the initial number of hy-
potheses before applying the filtering. We only use
the k = 100 ? n as a proof of concept.
2.3 Rescoring models
After having generated the 100 000-best lists, we
have to apply additional rescoring models to all hy-
potheses. We select the models that have shown
to improve overall translation performance as used
for recent NIST MT evaluations. In addition to the
main decoder score (which is already a combination
of several models and constitutes a strong baseline),
these include several large language models trained
on up to 2.5 billion running words, a sentence-level
IBM model 1 score, m-gram posterior probabilities
and an additional sentence length model.
3 Experiments
The experiments in this section are carried out on n-
best lists with n going up to 100 000. We will show
that, although we are capable of generating this large
amount of hypotheses, the overall performance does
not seem to improve significantly beyond a certain
threshold. Or to put it simple: although we generate
lots of hypotheses, most of them are not very useful.
As experimental background, we choose the large
data track of the Chinese-to-English NIST task,
since the length of the sentences and the large vo-
cabulary of the task allow for large n-best lists. For
smaller tasks, e.g. the IWSLT campaign, the domain
is rather limited such that it does not make sense
to generate lists reaching beyond several thousand
hypotheses. As development data, we use the 2002
eval set, whereas for test, the 2005 eval set is chosen.
The corpus statistics are shown in Table 1.
58
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 222K 351K
Dev Sentence Pairs 878 3 512
Running Words 25K 105K
Test Sentence Pairs 1 082 4 328
Running Words 33K 148K
Table 1: Corpus statistics for the Chinese-English
NIST MT task.
3.1 Oracle-best hypotheses
In the first experiment, we examined the oracle-best
hypotheses in the n-best lists for several list sizes.
For an efficient calculation of the true BLEU oracle
(the hypothesis which has a maximum BLEU score
when compared to the reference translations), we
use approximations based on WER/PER-oracles, i.e.
we extract the hypotheses that have the lowest edit
distance (WER, word error rate) to the references.
The same is applied by disregarding the word or-
der (leading to PER, position-independent word er-
ror rate).
As can be seen in Table 2, the improvements are
steadily decreasing, i.e. with increasing number of
generated hypotheses, there are less and less use-
ful candidates among them. For the first 10 000
candidates, we therefore have the possibility to find
hypotheses that could increase the BLEU score by
at least 8.3% absolute if our models discriminated
them properly. For the next 90 000 hypotheses, there
is only a small potential to improve the whole sys-
tem by around 1%. This means that most of the
generated hypotheses are not very useful in terms of
oracle-WER and likely distracting the ?search? for
the needle(s) in the haystack. It has been shown in
(Och et al, 2004) that true BLEU oracle scores on
lists with much smaller n ? 4096 are more or less
linear in log(n). Our results support this claim since
the oracle-WER/PER is a lower bound of the real
BLEU oracle. For the PER criterion, the behavior of
the oracle-best hypotheses is similar. Here we can
notice that after 10,000 hypotheses, the BLEU score
of the oracle-PER hypotheses stays the same.
These observations already impair the alleged
usefulness of a large amount of translation hypothe-
ses by showing that the overall possible gain with in-
creasing n gets disproportionately small if one puts
it in relation to the exponential growth of the n.
Oracle-WER [%] Oracle-PER [%]
N BLEU abs. imp. BLEU abs. imp.
1 36.1 36.1
10 38.8 +2.7 38.0 +1.9
100 41.3 +2.5 39.8 +1.8
1000 43.3 +2.0 41.0 +1.2
10000 44.4 +1.1 42.0 +1.0
100000 45.3 +0.9 42.0 +0.0
Table 2: Dev BLEU scores of oracle-best hypothe-
ses based on minimum WER/PER.
3.2 Rescoring performance
As a next step, we show the performance of tuning
the model scaling factors towards best translation
performance. In our experiments, we use the BLEU
score as objective function of the simplex method.
Figure 1 shows the graphs for the development
(on the left) and test set (on the right). The up-
per graphs depict the oracle-WER BLEU scores (cf.
also Table 2) for comparison. As was already stated,
these are a lower bound since the real oracle-BLEU
hypotheses might have even higher scores. Still, it is
an indicator of what could be achieved if the models
discriminated good from bad hypotheses properly.
The lower two graphs show the behavior when
(a) optimizing and extracting hypotheses on a sub-
set (the first n) of the 100k-best hypotheses and (b)
optimizing on a subset but extracting from the full
100k set. As can be seen, extracting from the full
set does not even help for the development data on
which the scaling factors were tuned. Experiments
on the test list show similar results. We can also
observe that the improvement declines rapidly with
higher n. Note that an optimization on the full 100k
list was not possible due to huge memory require-
ments. The highest n that fit into the 16GB machine
was 60 000. Thus, this setting was used for extrac-
tion on the full 100k set.
The results so far indicate that it is not very use-
ful to go beyond n = 10000. For the development
set, the baseline of 36.1% BLEU can be improved
by 1.6% absolute to 37.7% for the first 10k entries,
whereas for the 60k setting, the absolute improve-
ment is only increased by a marginal 0.1%. For the
chosen setting, whose focus was on various list sizes
for optimization and extraction, the improvements
on the development lists do not carry over to the test
list. From the baseline of 31.5%, we only get a mod-
erate improvement of approximately 0.5% BLEU.
59
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) opt. on N, extr. on N
(b) opt. on N, extr. on 100k
 31
 32
 33
 34
 35
 36
 37
 38
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) extraction on N
(b) extraction on 100k
Figure 1: BLEU scores of the reranked system. Development set (left) vs. Test set (right).
One possible explanation for this lies in the poor
performance of the rescoring models. A short test
was carried out in which we added the reference
translations to the n-best list and determined the cor-
responding scores of the additional models, such as
the large LM and the IBM model 1. Interestingly,
only less than 1/4 of the references was ranked as
the best hypothesis. Thus, most reference transla-
tions would never have been selected as final candi-
dates. This strongly indicates that we have to come
up with better models in order to make significant
improvements from large n-best lists. Furthermore,
it seems that the exponential growth of n-best hy-
potheses for maintaining a quasilinear improvement
in oracle BLEU score has a strong impact on the
overall system performance. This is in contrast to a
word graph, where a linear increment of its density
yields disproportionately high improvements in ora-
cle BLEU for lower densities (Zens and Ney, 2005).
4 Conclusion
We described an efficient n-best list extraction
method that is based on the k shortest paths algo-
rithm. Experiments with large 100 000-best lists in-
dicate that the models do not have the discriminating
power to separate the good from the bad candidates.
The oracle-best BLEU scores stay linear in log(n),
whereas the reranked system performance seems to
saturate at around 10k best translations given the ac-
tual models. Using more hypotheses currently does
not help to significantly improve translation quality.
Given the current results, one should balance the
advantages of n-best lists, e.g. easily testing com-
plex rescoring models, and word graphs, e.g. repre-
sentation of a much larger hypotheses space. How-
ever, as long as the models are not able to correctly
fire on good candidates, both approaches will stay
beneath their capabilities.
Acknowledgments
This material is partly based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023, and was partly funded by
the Deutsche Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and M. Federico.
2006. The ITC-irst SMT system for IWSLT 2006. In Proc.
of the International Workshop on Spoken Language Transla-
tion, pages 53?58, Kyoto, Japan, November.
D. Eppstein. 1998. Finding the k shortest paths. SIAM J. Com-
puting, 28(2):652?673.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the Human Language Tech-
nology Conf. (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH statistical machine translation system for the
IWSLT 2006 evaluation. In Proc. of the International Work-
shop on Spoken Language Translation, pages 103?110, Ky-
oto, Japan, November.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin,
and D. Radev. 2004. A smorgasbord of features for statisti-
cal machine translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 161?168, Boston, MA, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word
graphs in statistical machine translation. In Proc. of the
Conf. on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 156?163, Philadelphia, PA, July.
R. Zens and H. Ney. 2005. Word graphs for statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI, June.
60
Proceedings of NAACL HLT 2009: Short Papers, pages 17?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Comparison of Extended Lexicon Models in Search and Rescoring for SMT
Sa
?
sa Hasan and Hermann Ney
Human Language Technology and Pattern Recognition Group
Chair of Computer Science 6, RWTH Aachen University, Germany
{hasan,ney}@cs.rwth-aachen.de
Abstract
We show how the integration of an extended
lexicon model into the decoder can improve
translation performance. The model is based
on lexical triggers that capture long-distance
dependencies on the sentence level. The re-
sults are compared to variants of the model
that are applied in reranking of n-best lists.
We present how a combined application of
these models in search and rescoring gives
promising results. Experiments are reported
on the GALE Chinese-English task with im-
provements of up to +0.9% BLEU and -1.5%
TER absolute on a competitive baseline.
1 Introduction
Phrase-based statistical machine translation has im-
proved significantly over the last decade. The avail-
ability of large amounts of parallel data and access to
open-source software allow for easy setup of trans-
lation systems with acceptable performance. Pub-
lic evaluations such as the NIST MT Eval or the
WMT Shared Task help to measure overall progress
within the community. Most of the groups use a
phrase-based decoder (e.g. Pharaoh or the more re-
cent Moses) based on a log-linear fusion of models
that enable the avid researcher to quickly incorpo-
rate additional features and investigate the effect of
additional knowledge sources to guide the search for
better translation hypotheses.
In this paper, we deal with an extended lexicon
model and its incorporation into a state-of-the-art
decoder. We compare the results of the integration
to a similar setup used within a rescoring frame-
work and show the benefits of integrating additional
models directly into the search process. As will
be shown, although a rescoring framework is suit-
able for obtaining quick trends of incorporating ad-
ditional models into a system, an alternative that in-
cludes the model in search should be preferred. The
integration does not only yield better performance,
we will also show the benefit of combining both ap-
proaches in order to boost translation quality even
more. The extended lexicon model which we apply
is motivated by a trigger-based approach (Hasan et
al., 2008). A standard lexicon modeling dependen-
cies of target and source words, i.e. p(e|f), is ex-
tended with a second trigger f
?
on the source side,
resulting in p(e|f, f
?
). This model allows for a more
fine-grained lexical choice of the target word de-
pending on the additional source word f
?
. Since the
second trigger can move over the whole sentence,
we capture global (sentence-level) context that is not
modeled in local n-grams of the language model or
in bilingual phrase pairs that cover only a limited
amount of consecutive words.
Related work A similar approach has been tried
in the word-sense disambiguation (WSD) domain
where local but also across-sentence unigram collo-
cations of words are used to refine phrase pair selec-
tion dynamically by incorporating scores from the
WSD classifier (Chan et al, 2007). A maximum-
entropy based approach with different features of
surrounding words that are locally bound to a con-
text of three positions to the left and right is re-
ported in (Garc??a-Varea et al, 2001). A logistic
regression-based word translation model is investi-
gated by Vickrey et al (2005) but has not been eval-
uated on a machine translation task. Another WSD
approach incorporating context-dependent phrasal
translation lexicons is presented by Carpuat and Wu
(2007) and has been evaluated on several translation
17
tasks. The triplet lexicon model presented in this
work can also be interpreted as an extension of the
standard IBM model 1 (Brown et al, 1993) with an
additional trigger.
2 Setup
The main focus of this work investigates an extended
lexicon model in search and rescoring. The model
that we consider here and its integration in the de-
coder and setup for rescoring are presented in the
following sections.
2.1 Extended lexicon model
The triplets of the extended lexicon model p(e|f, f
?
)
are composed of two words in the source language
triggering one target word. In order to limit the over-
all number of triplets, we apply a training constraint
that reuses the word alignment information obtained
in the GIZA
++
step. For source words f , we only
consider the ones that are aligned to a target word e
given the GIZA
++
word alignment. The second trig-
ger f
?
is allowed to move over the whole source sen-
tence, thus capturing long-distance effects that can
be observed in the training data:
p(e
I
1
|f
J
1
, {a
ij
}) =
I
?
i=1
p(e
i
|f
J
1
, {a
ij
}) =
I
?
i=1
1
Z
i
?
j?{a
i
}
J
?
j
?
=1
p(e
i
|f
j
, f
j
?
) (1)
where {a
ij
} denotes the alignment matrix of the sen-
tence pair f
J
1
and e
I
1
and the first sum goes over all
f
j
that are aligned to the current e
i
(expressed as
j ? {a
i
}). The factor Z
i
= J ? |{a
i
}| normalizes
the double summation accordingly. Eq. 1 is used in
the iterative EM training on all sentence pairs of the
training data. Empty words are allowed on the trig-
gering part and low probability triplets are trimmed.
2.2 Decoding
Regarding the search, we can apply this model di-
rectly when scoring bilingual phrase pairs. Given a
trained model for p(e|f, f
?
), we compute the feature
score h
t
of a phrase pair (e?,
?
f) as
h
t
(e?,
?
f, {a?
ij
}, f
J
1
) = (2)
?
?
i
log
?
j?{a?
i
}
?
j
?
p(e?
i
|
?
f
j
, f
j
?
) +
?
i
logZ
i
where i moves over all target words in the phrase e?,
the sum over j selects the aligned source words
?
f
j
given {a?
ij
}, the alignment matrix within the phrase
pair, and j
?
incorporates the whole source sentence
f
J
1
. Analogous to Eq. 1, Z
i
= J ? |{a?
i
}| denotes
the number of overall source words times the num-
ber of aligned source words to each e?
i
. In Eq. 2,
we take negative log-probabilities and normalize to
obtain the final score (representing costs) for the
given phrase pair. Note that in search, we can only
use this direction, p(e|f, f
?
), since the whole source
sentence is available for triggering effects whereas
not all target words have been generated so far,
as it would be necessary for the reverse direction,
p(f |e, e
?
). Due to data sparseness, we smooth the
model by using a floor value of 10
?7
for unseen
events during decoding. Furthermore, an implicit
backoff to IBM1 exists if the second trigger is the
empty word, i.e. for events of the form p(e|f, ?).
2.3 Rescoring
In rescoring, we constrain the scoring of our hy-
potheses to a limited set of n-best translations that
are extracted from the word graph, a pruned com-
pact representation of the search space. The advan-
tage of n-best list rescoring is the full availability of
both source text and target translation, thus allow-
ing for the application of additional (possibly more
complex) models that are hard to implement directly
in search, such as e.g. syntactic models based on
parsers or huge LMs that would not fit in memory
during decoding. Since we are limiting ourselves to
a small extract of translation hypotheses, rescoring
models cannot outperform the same models if ap-
plied directly in search. One advantage though is
that we can apply the introduced trigger model also
in the other direction, i.e. using p(f |e, e
?
), where two
target words trigger one source word. Generally, the
combination of two directions of a model yields fur-
ther improvements, so we investigated how this ad-
ditional direction helps in rescoring (cf. Section 3.1).
In our experiments, we use 10 000-best lists ex-
tracted from the word graphs. An initial setting uses
the baseline system, whereas a comparative setup in-
corporates the (e|f, f
?
) direction of the trigger lexi-
con model in search and adds the reversed direction
in rescoring. Additionally, we use n-gram posteri-
ors, a sentence length model and two large language
18
train (ch/en) test08 (NW/WT)
Sent. pairs 9.1M 480 490
Run. words 259M/300M 14.8K 12.3K
Vocabulary 357K/627K 3.6K 3.2K
Table 1: GALE Chinese-English corpus statistics.
models, a 5-gram count LM trained on 2.5G running
words and the Google Web 1T 5-grams. The feature
weights of the log-linear mix are tuned on a separate
development set using the Downhill Simplex algo-
rithm.
3 Experiments
The experiments are carried out with a GALE sys-
tem using the official development and test sets of
the GALE 2008 evaluation. The corpus statistics
are shown in Table 1. The triplet lexicon model was
trained on a subset of the overall data. We used 1.4M
sentence pairs with 32.3M running words on the En-
glish side. The vocabulary sizes were 76.5K for the
source and 241.7K for the target language. The final
lexicon contains roughly 62 million triplets.
The baseline system incorporates the standard
model setup used in phrase-based SMT which com-
bines phrase translation and word lexicon models
in both directions, a 5-gram language model, word
and phrase penalties, and two models for reorder-
ing (a standard distortion model and a discriminative
phrase orientation model). For a fair comparison, we
also added the related IBM model 1 p(e|f) to the
baseline since it can be computed on the sentence-
level for this direction, target given source. This step
achieves +0.5% BLEU on the development set for
newswire but has no effect on test. As will be pre-
sented in the next section, the extension to another
trigger results in improvements over this baseline,
indicating that the extended triplet model is superior
to the standard IBM model 1. The feature weights
were optimized on separate development sets for
both newswire and web text.
We perform the following pipeline of experi-
ments: A first run generates word graphs using the
baseline models. From this word graph, we ex-
tract 10k-best lists and compare the performance to
a reranked version including the additional models.
In a second step, we add one of the trigger lexi-
Chinese-English newswire web text
GALE test08 BLEU TER BLEU TER
baseline 32.5 59.4 25.8 64.0
rescore, no triplets 32.8 59.0 26.6 63.5
resc. triplets fe+ef 33.2 58.6 27.1 63.0
triplets in search ef 33.1 58.8 26.0 63.5
rescore, no triplets 33.2 58.6 26.7 63.5
rescore, triplets fe 33.7 58.1 27.2 62.0
Table 2: Results obtained for the two test sets. For the
triplet models, ?fe? means p(f |e, e
?
) and ?ef? denotes
p(e|f, f
?
). BLEU/TER scores are shown in percent.
con models to the search process, regenerate word
graphs, extract updated n-best lists and add the re-
maining models again in a reranking step.
3.1 Results
Table 2 presents results that were obtained on the
test sets. All results are based on lowercase eval-
uations since the system is trained on lowercased
data in order to keep computational resources fea-
sible. For the newswire setting, the baseline is
32.5% BLEU and 59.4% TER. Rescoring with addi-
tional models not including triplets gives only slight
improvements. By adding the path-aligned triplet
model in both directions, we observe an improve-
ment of +0.7% BLEU and -0.8% TER. Using the
triplet model in source to target direction (e, f, f
?
)
during the search process, we arrive at a similar
BLEU improvement of +0.6% without any rerank-
ing models. We add the other direction of the triplets
(f, e, e
?
) (the one that can not be used directly in
search) and obtain 33.7% BLEU on the newswire
set. The overall cumulative improvements of triplets
in search and reranking are +0.9% BLEU and -0.9%
TER when compared to the rescored baseline not in-
corporating triplet models and +1.2%/-1.3% on the
decoder baseline, respectively.
For the web text setting, the baseline is consid-
erably lower at 25.8% BLEU and 64.0% TER (cf.
right part of Table 2). We observe an improvement
for the baseline reranking models, a large part of
which is due to the Google Web LM. Adding triplets
to search does not help significantly (+0.2%/-0.5%
BLEU/TER). This might be due to training the
triplet lexicon mainly on newswire data. Rerank-
ing without triplets performs similar to the baseline
19
experiment. Mixing in the (f, e, e
?
) direction helps
again: The final score comes out at 27.2% BLEU
and 62.0% TER, the latter being significantly better
than the reranked baseline (-1.5% in TER).
3.2 Discussion
The results indicate that it is worth moving models
from rescoring to the search process. This is not
surprising (and probably well known in the com-
munity). Interestingly, the triplet model can im-
prove translation quality in addition to its related
IBM model 1 which was already part of the base-
line. It seems that the extension by a second trigger
helps to capture some language specific properties
for Chinese-English which go beyond local lexical
(word-to-word) dependencies. In Table 3, we show
an example of improved translation quality where a
triggering effect can be observed. Due to the topic of
the sentence, the phrase local employment was cho-
sen over own jobs. One of the top triplets in this con-
text is p(employment | ?? , ?? ), where ??
is ?employment? due to the path-aligned constraint
and ?? means ?talent?. Note that the distance be-
tween these two triggers is five tokens.
4 Conclusion
We presented the integration of an extended lexicon
model into the search process and compared it to a
variant which was used in reranking n-best lists. In
order to keep the overall number of triplets feasi-
ble, and thus memory footprints and training times
low, we chose a path-constrained triplet model that
restricts the first source trigger to the aligned target
word, whereas the second trigger can move along
the whole source sentence. The motivation was to
allow for a more fine-grained lexical choice of tar-
get words by looking at sentence-level context. The
overall improvements that can be accounted to the
triplets are up to +0.9% BLEU and -1.5% TER.
In the future, we plan to investigate more triplet
model variants and work on additional language
pairs such as French-English or German-English.
The reverse direction, p(f |e, e
?
), is hard to imple-
ment outside of a reranking framework where the
full target hypotheses are already fully generated. It
might be worth looking at cross-lingual trigger mod-
els such as p(f |e, f
?
) or constrained variants like
source ??????????? ,???
??????????? .
baseline germany, in order to protect their own
jobs, the introduction of foreign talent,
a relatively high threshold.
triplets in order to protect local employment,
germany has a relatively high threshold
for the introduction of foreign talent.
reference in order to protect native employment,
germany has set a relatively high thresh-
old for bringing in foreign talents.
Table 3: Translation example on the newswire test set.
p(f |e, e
?
) with e
?
< e, i.e. the second trigger com-
ing from the left context within a sentence which has
already been generated.
Acknowledgments
This material is partly based upon work supported by the
Defense Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023, and was partly
realized as part of the Quaero Programme, funded by
OSEO, French State agency for innovation.
The authors would like to thank Juri Ganitkevitch for
training the triplet model.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. EMNLP-CoNLL, Prague, Czech Republic, June.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proc. ACL, pages 33?40, Prague, Czech Re-
public, June.
I. Garc??a-Varea, F. J. Och, H. Ney, and F. Casacuberta.
2001. Refined lexicon models for statistical machine
translation using a maximum entropy approach. In
Proc. ACL Data-Driven Machine Translation Work-
shop, pages 204?211, Toulouse, France, July.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proc. EMNLP, pages 372?381, Hon-
olulu, Hawaii, October.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. HLT-EMNLP, pages 771?778, Morristown,
NJ, USA.
20
Reranking Translation Hypotheses Using Structural Properties
Sas?a Hasan, Oliver Bender, Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
D-52056 Aachen, Germany
{hasan,bender,ney}@cs.rwth-aachen.de
Abstract
We investigate methods that add syntac-
tically motivated features to a statistical
machine translation system in a reranking
framework. The goal is to analyze whether
shallow parsing techniques help in iden-
tifying ungrammatical hypotheses. We
show that improvements are possible by
utilizing supertagging, lightweight depen-
dency analysis, a link grammar parser and
a maximum-entropy based chunk parser.
Adding features to n-best lists and dis-
criminatively training the system on a de-
velopment set increases the BLEU score
up to 0.7% on the test set.
1 Introduction
Statistically driven machine translation systems
are currently the dominant type of system in the
MT community. Though much better than tradi-
tional rule-based approaches, these systems still
make a lot of errors that seem, at least from a hu-
man point of view, illogical.
The main purpose of this paper is to investigate
a means of identifying ungrammatical hypotheses
from the output of a machine translation system
by using grammatical knowledge that expresses
syntactic dependencies of words or word groups.
We introduce several methods that try to establish
this kind of linkage between the words of a hy-
pothesis and, thus, determine its well-formedness,
or ?fluency?. We perform rescoring experiments
that rerank n-best lists according to the presented
framework.
As methodologies deriving well-formedness of
a sentence we use supertagging (Bangalore and
Joshi, 1999) with lightweight dependency anal-
ysis (LDA)1 (Bangalore, 2000), link grammars
(Sleator and Temperley, 1993) and a maximum-
entropy (ME) based chunk parser (Bender et al,
2003). The former two approaches explicitly
model the syntactic dependencies between words.
Each hypothesis that contains irregularities, such
as broken linkages or non-satisfied dependencies,
should be penalized or rejected accordingly. For
the ME chunker, the idea is to train n-gram mod-
els on the chunk or POS sequences and directly
use the log-probability as feature score.
In general, these concepts and the underlying
programs should be robust and fast in order to be
able to cope with large amounts of data (as it is the
case for n-best lists). The experiments presented
show a small though consistent improvement in
terms of automatic evaluation measures chosen for
evaluation. BLEU score improvements, for in-
stance, lie in the range from 0.3 to 0.7% on the
test set.
In the following, Section 2 gives an overview
on related work in this domain. In Section 3
we review our general approach to statistical ma-
chine translation (SMT) and introduce the main
methodologies used for deriving syntactic depen-
dencies on words or word groups, namely su-
pertagging/LDA, link grammars and ME chunk-
ing. The corpora and the experiments are dis-
cussed in Section 4. The paper is concluded in
Section 5.
2 Related work
In (Och et al, 2004), the effects of integrating
syntactic structure into a state-of-the-art statistical
machine translation system are investigated. The
approach is similar to the approach presented here:
1In the context of this work, the term LDA is not to be
confused with linear discriminant analysis.
41
firstly, a word graph is generated using the base-
line SMT system and n-best lists are extracted ac-
cordingly, then additional feature functions repre-
senting syntactic knowledge are added and the cor-
responding scaling factors are trained discrimina-
tively on a development n-best list.
Och and colleagues investigated a large amount
of different feature functions. The field of appli-
cation varies from simple syntactic features, such
as IBM model 1 score, over shallow parsing tech-
niques to more complex methods using grammars
and intricate parsing procedures. The results were
rather disappointing. Only one of the simplest
models, i.e. the implicit syntactic feature derived
from IBM model 1 score, yielded consistent and
significant improvements. All other methods had
only a very small effect on the overall perfor-
mance.
3 Framework
In the following sections, the theoretical frame-
work of statistical machine translation using a di-
rect approach is reviewed. We introduce the su-
pertagging and lightweight dependency analysis
approach, link grammars and maximum-entropy
based chunking technique.
3.1 Direct approach to SMT
In statistical machine translation, the best trans-
lation e?I?1 = e?1 . . . e?i . . . e?I? of source words fJ1 =
f1 . . . fj . . . fJ is obtained by maximizing the con-
ditional probability
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )}
= argmax
I,eI1
{Pr(fJ1 |eI1) ? Pr(eI1)}
(1)
using Bayes decision rule. The first probability
on the right-hand side of the equation denotes the
translation model whereas the second is the target
language model.
An alternative to this classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ) which is utilized here. Us-
ing a log-linear model (Och and Ney, 2002), we
obtain
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
) ,
(2)
where ?m are the scaling factors of the models de-
noted by feature functions hm(?). The denomina-
tor represents a normalization factor that depends
only on the source sentence fJ1 . Therefore, we can
omit it during the search process, leading to the
following decision rule:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach. It has the advantage that ad-
ditional models h(?) can be easily integrated into
the overall system. The model scaling factors
?M1 are trained according to the maximum en-
tropy principle, e.g., using the GIS algorithm. Al-
ternatively, one can train them with respect to
the final translation quality measured by an error
criterion (Och, 2003). For the results reported
in this paper, we optimized the scaling factors
with respect to a linear interpolation of word error
rate (WER), position-independent word error rate
(PER), BLEU and NIST score using the Downhill
Simplex algorithm (Press et al, 2002).
3.2 Supertagging/LDA
Supertagging (Bangalore and Joshi, 1999) uses the
Lexicalized Tree Adjoining Grammar formalism
(LTAG) (XTAG Research Group, 2001). Tree Ad-
joining Grammars incorporate a tree-rewriting for-
malism using elementary trees that can be com-
bined by two operations, namely substitution and
adjunction, to derive more complex tree structures
of the sentence considered. Lexicalization allows
us to associate each elementary tree with a lexical
item called the anchor. In LTAGs, every elemen-
tary tree has such a lexical anchor, also called head
word. It is possible that there is more than one el-
ementary structure associated with a lexical item,
as e.g. for the case of verbs with different subcat-
egorization frames.
The elementary structures, called initial and
auxiliary trees, hold all dependent elements within
the same structure, thus imposing constraints on
the lexical anchors in a local context. Basically,
supertagging is very similar to part-of-speech tag-
ging. Instead of POS tags, richer descriptions,
namely the elementary structures of LTAGs, are
annotated to the words of a sentence. For this pur-
pose, they are called supertags in order to distin-
guish them from ordinary POS tags. The result
is an ?almost parse? because of the dependencies
42
very[?2]
food[?1] delicious[?3]
the[?1]
was[?2]
Figure 1: LDA: example of a derivation tree, ?
nodes are the result of the adjunction operation on
auxiliary trees, ? nodes of substitution on initial
trees.
coded within the supertags. Usually, a lexical item
can have many supertags, depending on the vari-
ous contexts it appears in. Therefore, the local am-
biguity is larger than for the case of POS tags. An
LTAG parser for this scenario can be very slow, i.e.
its computational complexity is in O(n6), because
of the large number of supertags, i.e. elementary
trees, that have to be examined during a parse. In
order to speed up the parsing process, we can ap-
ply n-gram models on a supertag basis in order to
filter out incompatible descriptions and thus im-
prove the performance of the parser. In (Banga-
lore and Joshi, 1999), a trigram supertagger with
smoothing and back-off is reported that achieves
an accuracy of 92.2% when trained on one million
running words.
There is another aspect to the dependencies
coded in the elementary structures. We can use
them to actually derive a shallow parse of the sen-
tence in linear time. The procedure is presented
in (Bangalore, 2000) and is called lightweight de-
pendency analysis. The concept is comparable to
chunking. The lightweight dependency analyzer
(LDA) finds the arguments for the encoded depen-
dency requirements. There exist two types of slots
that can be filled. On the one hand, nodes marked
for substitution (in ?-trees) have to be filled by the
complements of the lexical anchor. On the other
hand, the foot nodes (i.e. nodes marked for adjunc-
tion in ?-trees) take words that are being modified
by the supertag. Figure 1 shows a tree derived by
LDA on the sentence the food was very delicious
from the C-Star?03 corpus (cf. Section 4.1).
The supertagging and LDA tools are available
from the XTAG research group website.2
As features considered for the reranking exper-
iments we choose:
2http://www.cis.upenn.edu/?xtag/
D D EA EA
P P
SS
the food very deliciouswas
Figure 2: Link grammar: example of a valid link-
age satisfying all constraints.
? Supertagger output: directly use the log-
likelihoods as feature score. This did not im-
prove performance significantly, so the model
was discarded from the final system.
? LDA output:
? dependency coverage: determine the
number of covered elements, i.e. where
the dependency slots are filled to the left
and right
? separate features for the number of mod-
ifiers and complements determined by
the LDA
3.3 Link grammar
Similar to the ideas presented in the previous sec-
tion, link grammars also explicitly code depen-
dencies between words (Sleator and Temperley,
1993). These dependencies are called links which
reflect the local requirements of each word. Sev-
eral constraints have to be satisfied within the link
grammar formalism to derive correct linkages, i.e.
sets of links, of a sequence of words:
1. Planarity: links are not allowed to cross each
other
2. Connectivity: links suffice to connect all
words of a sentence
3. Satisfaction: linking requirements of each
word are satisfied
An example of a valid linkage is shown in Fig-
ure 2. The link grammar parser that we use is
freely available from the authors? website.3 Sim-
ilar to LTAG, the link grammar formalism is lex-
icalized which allows for enhancing the methods
with probabilistic n-gram models (as is also the
case for supertagging). In (Lafferty et al, 1992),
the link grammar is used to derive a new class of
3http://www.link.cs.cmu.edu/link/
43
[NP the food ] [VP was] [ADJP very delicious]
the/DT food/NN was/VBD very/RB delicious/JJ
Figure 3: Chunking and POS tagging: a tag next
to the opening bracket denotes the type of chunk,
whereas the corresponding POS tag is given after
the word.
language models that, in comparison to traditional
n-gram LMs, incorporate capabilities for express-
ing long-range dependencies between words.
The link grammar dictionary that specifies the
words and their corresponding valid links cur-
rently holds approximately 60 000 entries and han-
dles a wide variety of phenomena in English. It is
derived from newspaper texts.
Within our reranking framework, we use link
grammar features that express a possible well-
formedness of the translation hypothesis. The sim-
plest feature is a binary one stating whether the
link grammar parser could derive a complete link-
age or not, which should be a strong indicator of
a syntactically correct sentence. Additionally, we
added a normalized cost of the matching process
which turned out not to be very helpful for rescor-
ing, so it was discarded.
3.4 ME chunking
Like the methods described in the two preced-
ing sections, text chunking consists of dividing a
text into syntactically correlated non-overlapping
groups of words. Figure 3 shows again our ex-
ample sentence illustrating this task. Chunks are
represented as groups of words between square
brackets. We employ the 11 chunk types as de-
fined for the CoNLL-2000 shared task (Tjong Kim
Sang and Buchholz, 2000).
For the experiments, we apply a maximum-
entropy based tagger which has been successfully
evaluated on natural language understanding and
named entity recognition (Bender et al, 2003).
Within this tool, we directly factorize the poste-
rior probability and determine the corresponding
chunk tag for each word of an input sequence. We
assume that the decisions depend only on a lim-
ited window ei+2i?2 = ei?2...ei+2 around the current
word ei and on the two predecessor chunk tags
ci?1i?2. In addition, part-of-speech (POS) tags gI1
are assigned and incorporated into the model (cf.
Figure 3). Thus, we obtain the following second-
order model:
Pr(cI1|eI1, gI1) =
=
I
?
i=1
Pr(ci|ci?11 , eI1, gI1) (4)
=
I
?
i=1
p(ci|ci?1i?2, ei+2i?2, gi+2i?2), (5)
where the step from Eq. 4 to 5 reflects our model
assumptions.
Furthermore, we have implemented a set of bi-
nary valued feature functions for our system, in-
cluding lexical, word and transition features, prior
features, and compound features, cf. (Bender et
al., 2003). We run simple count-based feature
reduction and train the model parameters using
the Generalized Iterative Scaling (GIS) algorithm
(Darroch and Ratcliff, 1972). In practice, the
training procedure tends to result in an overfitted
model. To avoid this, a smoothing method is ap-
plied where a Gaussian prior on the parameters is
assumed (Chen and Rosenfeld, 1999).
Within our reranking framework, we firstly use
the ME based tagger to produce the POS and
chunk sequences for the different n-best list hy-
potheses. Given several n-gram models trained on
the WSJ corpus for both POS and chunk models,
we then rescore the n-best hypotheses and simply
use the log-probabilities as additional features. In
order to adapt our system to the characteristics of
the data used, we build POS and chunk n-gram
models on the training corpus part. These domain-
specific models are also added to the n-best lists.
The ME chunking approach does not model ex-
plicit syntactic linkages of words. Instead, it in-
corporates a statistical framework to exploit valid
and syntactically coherent groups of words by ad-
ditionally looking at the word classes.
4 Experiments
For the experiments, we use the translation sys-
tem described in (Zens et al, 2005). Our phrase-
based decoder uses several models during search
that are interpolated in a log-linear way (as ex-
pressed in Eq. 3), such as phrase-based translation
models, word-based lexicon models, a language,
deletion and simple reordering model and word
and phrase penalties. A word graph containing
the most likely translation hypotheses is generated
during the search process. Out of this compact
44
Supplied Data Track
Arabic Chinese Japanese English
Train Sentences 20 000
Running Words 180 075 176 199 198 453 189 927
Vocabulary 15 371 8 687 9 277 6 870
Singletons 8 319 4 006 4 431 2 888
C-Star?03 Sentences 506
Running Words 3 552 3 630 4 130 3 823
OOVs (Running Words) 133 114 61 65
IWSLT?04 Sentences 500
Running Words 3 597 3 681 4 131 3 837
OOVs (Running Words) 142 83 71 58
Table 1: Corpus statistics after preprocessing.
representation, we extract n-best lists as described
in (Zens and Ney, 2005). These n-best lists serve
as a starting point for our experiments. The meth-
ods presented in Section 3 produce scores that are
used as additional features for the n-best lists.
4.1 Corpora
The experiments are carried out on a subset
of the Basic Travel Expression Corpus (BTEC)
(Takezawa et al, 2002), as it is used for the sup-
plied data track condition of the IWSLT evaluation
campaign. BTEC is a multilingual speech corpus
which contains tourism-related sentences similar
to those that are found in phrase books. For the
supplied data track, the training corpus contains
20 000 sentences. Two test sets, C-Star?03 and
IWSLT?04, are available for the language pairs
Arabic-English, Chinese-English and Japanese-
English.
The corpus statistics are shown in Table 1. The
average source sentence length is between seven
and eight words for all languages. So the task is
rather limited and very domain-specific. The ad-
vantage is that many different reranking experi-
ments with varying feature function settings can
be carried out easily and quickly in order to ana-
lyze the effects of the different models.
In the following, we use the C-Star?03 set for
development and tuning of the system?s parame-
ters. After that, the IWSLT?04 set is used as a
blind test set in order to measure the performance
of the models.
4.2 Rescoring experiments
The use of n-best lists in machine translation has
several advantages. It alleviates the effects of the
huge search space which is represented in word
graphs by using a compact excerpt of the n best
hypotheses generated by the system. Especially
for limited domain tasks, the size of the n-best list
can be rather small but still yield good oracle er-
ror rates. Empirically, n-best lists should have an
appropriate size such that the oracle error rate, i.e.
the error rate of the best hypothesis with respect to
an error measure (such asWER or PER) is approx-
imately half the baseline error rate of the system.
N -best lists are suitable for easily applying several
rescoring techniques since the hypotheses are al-
ready fully generated. In comparison, word graph
rescoring techniques need specialized tools which
can traverse the graph accordingly. Since a node
within a word graph allows for many histories, one
can only apply local rescoring techniques, whereas
for n-best lists, techniques can be used that con-
sider properties of the whole sentence.
For the Chinese-English and Arabic-English
task, we set the n-best list size to n = 1500. For
Japanese-English, n = 1000 produces oracle er-
ror rates that are deemed to be sufficiently low,
namely 17.7% and 14.8% for WER and PER, re-
spectively. The single-best output for Japanese-
English has a word error rate of 33.3% and
position-independent word error rate of 25.9%.
For the experiments, we add additional fea-
tures to the initial models of our decoder that have
shown to be particularly useful in the past, such as
IBM model 1 score, a clustered language model
score and a word penalty that prevents the hy-
potheses to become too short. A detailed defini-
tion of these additional features is given in (Zens
et al, 2005). Thus, the baseline we start with is
45
Chinese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.17 46.2 48.6 41.4
with supertagging/LDA 8.29 46.5 48.4 41.0
with link grammar 8.43 45.6 47.9 41.1
with supertagging/LDA + link grammar 8.22 47.5 47.7 40.8
with ME chunker 8.65 47.3 47.4 40.4
with all models 8.42 47.0 47.4 40.5
Chinese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.67 45.5 49.1 39.8
with supertagging/LDA 8.68 45.4 49.8 40.3
with link grammar 8.81 45.0 49.0 40.2
with supertagging/LDA+link grammar 8.56 46.0 49.1 40.6
with ME chunker 9.00 44.6 49.3 40.6
with all models 8.89 46.2 48.1 39.6
Table 2: Effect of successively adding syntactic features to the Chinese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
BASE Any messages for me?
RESC Do you have any messages for me?
REFE Do you have any messages for me?
BASE She, not yet?
RESC She has not come yet?
REFE Lenny, she has not come in?
BASE How much is it to the?
RESC How much is it to the local call?
REFE How much is it to the city centre?
BASE This blot or.
RESC This is not clean.
REFE This still is not clean.
Table 3: Translation examples for the Chinese-
English test set (IWSLT?04): baseline system
(BASE) vs. rescored hypotheses (RESC) and refer-
ence translation (REFE).
already a very strong one. The log-linear inter-
polation weights ?m from Eq. 3 are directly opti-
mized using the Downhill Simplex algorithm on a
linear combination of WER (word error rate), PER
(position-independent word error rate), NIST and
BLEU score.
In Table 2, we show the effect of adding the
presented features successively to the baseline.
Separate entries for experiments using supertag-
ging/LDA and link grammars show that a combi-
nation of these syntactic approaches always yields
some gain in translation quality (regarding BLEU
score). The performance of the maximum-entropy
based chunking is comparable. A combination of
all three models still yields a small improvement.
Table 3 shows some examples for the Chinese-
English test set. The rescored translations are syn-
tactically coherent, though semantical correctness
cannot be guaranteed. On the test data, we achieve
an overall improvement of 0.7%, 0.5% and 0.3%
in BLEU score for Chinese-English, Japanese-
English and Arabic-English, respectively (cf. Ta-
bles 4 and 5).
4.3 Discussion
From the tables, it can be seen that the use of
syntactically motivated feature functions within
a reranking concept helps to slightly reduce the
number of translation errors of the overall trans-
lation system. Although the improvement on the
IWSLT?04 set is only moderate, the results are
nevertheless comparable or better to the ones from
(Och et al, 2004), where, starting from IBM
model 1 baseline, an additional improvement of
only 0.4% BLEU was achieved using more com-
plex methods.
For the maximum-entropy based chunking ap-
proach, n-grams with n = 4 work best for the
chunker that is trained on WSJ data. The domain-
specific rescoring model which results from the
chunker being trained on the BTEC corpora turns
out to prefer higher order n-grams, with n = 6 or
more. This might be an indicator of the domain-
specific rescoring model successfully capturing
more local context.
The training of the other models, i.e. supertag-
ging/LDA and link grammar, is also performed on
46
Japanese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.09 57.8 31.3 25.0
with supertagging/LDA 9.13 57.8 31.3 24.8
with link grammar 9.46 57.6 31.9 25.3
with supertagging/LDA + link grammar 9.24 58.2 31.0 24.8
with ME chunker 9.31 58.7 30.9 24.4
with all models 9.21 58.9 30.5 24.3
Japanese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.22 54.7 34.1 25.5
with supertagging/LDA 9.27 54.8 34.2 25.6
with link grammar 9.37 54.9 34.3 25.9
with supertagging/LDA + link grammar 9.30 55.0 34.0 25.6
with ME chunker 9.27 55.0 34.2 25.5
with all models 9.27 55.2 33.9 25.5
Table 4: Effect of successively adding syntactic features to the Japanese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
Arabic ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 10.18 64.3 23.9 20.6
with supertagging/LDA 10.13 64.6 23.4 20.1
with link grammar 10.06 64.7 23.4 20.3
with supertagging/LDA + link grammar 10.20 65.0 23.2 20.2
with ME chunker 10.11 65.1 23.0 19.9
with all models 10.23 65.2 23.0 19.9
Arabic ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.75 59.8 26.1 21.9
with supertagging/LDA 9.77 60.5 25.6 21.5
with link grammar 9.74 60.5 25.9 21.7
with supertagging/LDA + link grammar 9.86 60.8 26.0 21.6
with ME chunker 9.71 59.9 25.9 21.8
with all models 9.84 60.1 26.4 21.9
Table 5: Effect of successively adding syntactic features to the Arabic-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
out-of-domain data. Thus, further improvements
should be possible if the models were adapted to
the BTEC domain. This would require the prepa-
ration of an annotated corpus for the supertagger
and a specialized link grammar, which are both
time-consuming tasks.
The syntactically motivated methods (supertag-
ging/LDA and link grammars) perform similarly
to the maximum-entropy based chunker. It seems
that both approaches successfully exploit struc-
tural properties of language. However, one outlier
is ME chunking on the Chinese-English test data,
where we observe a lower BLEU but a larger NIST
score. For Arabic-English, the combination of all
methods does not seem to generalize well on the
test set. In that case, supertagging/LDA and link
grammar outperforms the ME chunker: the over-
all improvement is 1% absolute in terms of BLEU
score.
5 Conclusion
We added syntactically motivated features to a sta-
tistical machine translation system in a rerank-
ing framework. The goal was to analyze whether
shallow parsing techniques help in identifying un-
grammatical hypotheses. We showed that some
improvements are possible by utilizing supertag-
ging, lightweight dependency analysis, a link
47
grammar parser and a maximum-entropy based
chunk parser. Adding features to n-best lists and
discriminatively training the system on a develop-
ment set helped to gain up to 0.7% in BLEU score
on the test set.
Future work could include developing an
adapted LTAG for the BTEC domain or incor-
porating n-gram models into the link grammar
concept in order to derive a long-range language
model (Lafferty et al, 1992). However, we feel
that the current improvements are not significant
enough to justify these efforts. Additionally, we
will apply these reranking methods to larger cor-
pora in order to study the effects on longer sen-
tences from more complex domains.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-Star (Technology and Corpora for Speech
to Speech Translation, IST-2002-FP6-506738,
http://www.tc-star.org), and by the R&D project
TRAMES managed by Bertin Technologies as
prime contractor and operated by the french DGA
(De?le?gation Ge?ne?rale pour l?Armement).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore. 2000. A lightweight dependency
analyzer for partial parsing. Computational Linguis-
tics, 6(2):113?138.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment
templates and maximum entropy models for natural
language understanding. In EACL03: 10th Conf. of
the Europ. Chapter of the Association for Computa-
tional Linguistics, pages 11?18, Budapest, Hungary,
April.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon
University, Pittsburgh, PA.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematical Statistics, 43:1470?1480.
John Lafferty, Daniel Sleator, and Davy Temperley.
1992. Grammatical trigrams: A probabilistic model
of link grammar. In Proc. of the AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 89?97, Cambridge, MA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA,
July.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 161?168,
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies, Tilburg/Durbuy,
The Netherlands/Belgium, August.
Toshiyuki Takezawa, Eiichiro Sumita, F. Sugaya,
H. Yamamoto, and S. Yamamoto. 2002. Toward
a broad-coverage bilingual corpus for speech trans-
lation of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Resources
and Evaluation (LREC), pages 147?152, Las Pal-
mas, Spain, May.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, pages 127?132, Lisbon, Portugal,
September.
XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical Re-
port IRCS-01-03, IRCS, University of Pennsylvania,
Philadelphia, PA, USA.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguis-
tics: Proc. Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 191?198, Ann Arbor, MI, June.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based
statistical machine translation system. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
48
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 233?241,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Deep Learning Approach to Machine Transliteration
Thomas Deselaers and Sas?a Hasan and Oliver Bender and Hermann Ney
Human Language Technology and Pattern Recognition Group ? RWTH Aachen University
<surname>@cs.rwth-aachen.de
Abstract
In this paper we present a novel translit-
eration technique which is based on deep
belief networks. Common approaches
use finite state machines or other meth-
ods similar to conventional machine trans-
lation. Instead of using conventional NLP
techniques, the approach presented here
builds on deep belief networks, a tech-
nique which was shown to work well for
other machine learning problems. We
show that deep belief networks have cer-
tain properties which are very interesting
for transliteration and possibly also for
translation and that a combination with
conventional techniques leads to an im-
provement over both components on an
Arabic-English transliteration task.
1 Introduction
Transliteration, i.e. the transcription of words such
as proper nouns from one language into another or,
more commonly from one alphabet into another, is
an important subtask of machine translation (MT)
in order to obtain high quality output.
We present a new technique for transliteration
which is based on deep belief networks (DBNs),
a well studied approach in machine learning.
Transliteration can in principle be considered to be
a small-scale translation problem and, thus, some
ideas presented here can be transferred to the ma-
chine translation domain as well.
Transliteration has been in use in machine trans-
lation systems, e.g. Russian-English, since the ex-
istence of the field of machine translation. How-
ever, to our knowledge it was first studied as a
machine learning problem by Knight and Graehl
(1998) using probabilistic finite-state transducers.
Subsequently, the performance of this system was
greatly improved by combining different spelling
and phonetic models (Al-Onaizan and Knight,
2002). Huang et al (2004) construct a proba-
bilistic Chinese-English edit model as part of a
larger alignment solution using a heuristic boot-
strapped procedure. Freitag and Khadivi (2007)
propose a technique which combines conventional
MT methods with a single layer perceptron.
In contrast to these methods which strongly
build on top of well-established natural language
processing (NLP) techniques, we propose an al-
ternative model. Our new model is based on deep
belief networks which have been shown to work
well in other machine learning and pattern recog-
nition areas (cf. Section 2). Since translation and
transliteration are closely related and translitera-
tion can be considered a translation problem on the
character level, we discuss various methods from
both domains which are related to the proposed
approach in the following.
Neural networks have been used in NLP in
the past, e.g. for machine translation (Asuncio?n
Castan?o et al, 1997) and constituent parsing
(Titov and Henderson, 2007). However, it might
not be straight-forward to obtain good results us-
ing neural networks in this domain. In general,
when training a neural network, one has to choose
the structure of the neural network which involves
certain trade-offs. If a small network with no hid-
den layer is chosen, it can be efficiently trained
but has very limited representational power, and
may be unable to learn the relationships between
the source and the target language. The DBN ap-
proach alleviates some of the problems that com-
monly occur when working with neural networks:
1. they allow for efficient training due to a good
initialisation of the individual layers. 2. Overfit-
ting problems are addressed by creating generative
models which are later refined discriminatively. 3.
The network structure is clearly defined and only a
few structure parameters have to be set. 4. DBNs
can be interpreted as Bayesian probabilistic gener-
ative models.
Recently, Collobert and Weston (2008) pro-
posed a technique which applies a convolutional
DBN to a multi-task learning NLP problem. Their
approach is able to address POS tagging, chunk-
ing, named entity tagging, semantic role and simi-
lar word identification in one model. Our model is
similar to this approach in that it uses the same ma-
chine learning techniques but the encoding and the
233
processing is done differently. First, we learn two
independent generative models, one for the source
input and one for the target output. Then, these
two models are combined into a source-to-target
encoding/decoding system (cf. Section 2).
Regarding that the target is generated and not
searched in a space of hypotheses (e.g. in a word
graph), our approach is similar to the approach
presented by Bangalore et al (2007) who present
an MT system where the set of words of the tar-
get sentence is generated based on the full source
sentence and then a finite-state approach is used to
reorder the words. Opposed to this approach we
do not only generate the letters/words in the target
sentence but we generate the full sentence with or-
dering.
We evaluate the proposed methods on an
Arabic-English transliteration task where Arabic
city names have to be transcribed into the equiva-
lent English spelling.
2 Deep Belief Networks for
Transliteration
Although DBNs are thoroughly described in the
literature, e.g. (Hinton et al, 2006), we give a short
overview on the ideas and techniques and intro-
duce our notation.
Deep architectures in machine learning and ar-
tificial intelligence are becoming more and more
popular after an efficient training algorithm has
been proposed (Hinton et al, 2006), although the
idea is known for some years (Ackley et al, 1985).
Deep belief networks consist of multiple layers of
restricted Boltzmann machines (RBMs). It was
shown that DBNs can be used for dimensionality
reduction of images and text documents (Hinton
and Salakhutdinow, 2006) and for language mod-
elling (Mnih and Hinton, 2007). Recently, DBNs
were also used successfully in image retrieval to
create very compact but meaningful representa-
tions of a huge set of images (nearly 13 million)
for retrieval (Torralba et al, 2008).
DBNs are built from RBMs by first training an
RBM on the input data. A second RBM is built
on the output of the first one and so on until a
sufficiently deep architecture is created. RBMs
are stochastic generative artificial neural networks
with restricted connectivity. From a theoretical
viewpoint, RBMs are interesting because they are
able to discover complex regularities and find no-
table features in data (Ackley et al, 1985).
Figure 1: A schematic representation of our DBN
for transliteration.
Hinton and Salakhutdinow (2006) present a
deep belief network to learn a tiny representation
of its inputs and to reconstruct the input with high
accuracy which is demonstrated for images and
textual documents. Here, we use DBNs similarly:
first, we learn encoders for the source and tar-
get words respectively and then connect these two
through a joint layer to map between the two lan-
guages. This joint layer is trained in the same way
as the top-level neurons in the deep belief classi-
fier from (Hinton et al, 2006).
In Figure 1, a schematic view of our DBN for
transliteration is shown. On the left and on the
right are encoders for the source and target words
respectively. To transliterate a source word, it is
passed through the layers of the network. First, it
traverses through the source encoder on the left,
then it passes into the joint layer, finally travers-
ing down through the target encoder. Each layer
consists of a set of neurons receiving the output
of the preceding layer as input. The first layers in
the source and target encoders consist of S1 and
T1 neurons, respectively; the second layers have
S2 and T2 nodes, and the third layers have S3 and
T3 nodes, respectively. A joint layer with J nodes
connects the source and the target encoders.
Here, the number of nodes in the individual lay-
ers are the most important parameters. The more
234
nodes a layer has, the more information can be
conveyed through it, but the harder the training:
the amount of data needed for training and thus
the computation time required is exponential in the
size of the network (Ackley et al, 1985).
To transliterate a source word, it is first encoded
as a DF -dimensional binary vector SF (cf. Sec-
tion 2.1) and then fed into the first layer of the
source encoder. The S1-dimensional output vec-
tor OS1 of the first layer is computed as
OS1 ? 1/ exp (1 + wS1SF + bS1) , (1)
where wS1 is a S1 ?DF -dimensional weight ma-
trix and bS1 is an S1-dimensional bias vector.
The output of each layer is used as input to the
next layer as follows:
OS2 ? 1/ exp (1 + wS2OS1 + bS2) , (2)
OS3 ? 1/ exp (1 + wS3OS2 + bS3) . (3)
After the source encoder has been traversed, the
joint layer is reached which processes the data
twice: once using the input from the source en-
coder to get a state of the hidden neurons OSJ and
then to infer an output state OJT as input to the
topmost level of the output encoder
OSJ ? 1/ exp (1 + wSJOS3 + bSJ) , (4)
OJT ? 1/ exp (1 + wJTOSJ + bJT ) . (5)
This output vector is decoded by traversing down-
wards through the output encoder:
OT3 ? 1/ exp (1 + wT3OJT + bT3) , (6)
OT2 ? 1/ exp (1 + wT2OT3 + bT2) , (7)
OT1 ? wT1OT2 + bT1, (8)
where OT1 is a vector encoding a word in the tar-
get language.
Note that this model is intrinsically bidirec-
tional since the individual RBMs are bidirectional
models and thus it is possible to transliterate from
source to target and vice versa.
2.1 Source and Target Encoding
A problem with DBNs and transliteration is the
data representation. The input and output data are
commonly sequences of varying length but a DBN
expects input data of constant length. To repre-
sent a source or target language word, it is con-
verted into a sparse binary vector of dimensional-
ity DF = |F | ? J or DE = |E| ? I , respectively,
where |F | and |E| are the sizes of the alphabets
and I and J are the lengths of the longest words.
If a word is shorter than this, a padding letter w0
is used to fill the spaces. This encoding is depicted
in the bottom part of Figure 1.
Since the output vector of the DBN is not bi-
nary, we infer the maximum a posterior hypothe-
sis by selecting the letter with the highest output
value for each position.
2.2 Training Method
For the training, we follow the method proposed
in (Hinton et al, 2006). To find a good starting
point for backpropagation on the whole network,
each of the RBMs is trained individually. First, we
learn the generative encoders for the source and
target words, i.e. the weights wS1 and wT1, respec-
tively. Therefore, each of the layers is trained as a
restricted Boltzmann machine, such that it learns
to generate the input vectors with high probability,
i.e. the weights are learned such that the data val-
ues have low values of the trained cost function.
After learning a layer, the activity vectors of the
hidden units, as obtained from the real training
data, are used as input data for the next layer. This
can be repeated to learn as many hidden layers as
desired. After learning multiple hidden layers in
this way, the whole network can be viewed as a
single, multi-layer generative model and each ad-
ditional hidden layer improves a lower bound on
the probability that the multi-layer model would
generate the training data (Hinton et al, 2006).
For each language, the output of the first layer is
used as input to learn the weights of the next lay-
ers wS2 and wT2. The same procedure is repeated
to learn wS3 and wT3. Note that so far no con-
nection between the individual letters in the two
alphabets is created but each encoder only learns
feature functions to represent the space of possi-
ble source and target words. Then, the weights
for the joined layer are learned using concatenated
outputs of the top layers of the source and target
encoders to find an initial set of weights wSJ and
wJT .
After each of the layers has been trained in-
dividually, backpropagation is performed on the
whole network to tune the weights and to learn the
connections between both languages. We use the
average squared error over the output vectors be-
tween reference and inferred words as the training
criterion. For the training, we split the training
235
data into batches of 100 randomly selected words
and allow for 10 training iterations of the individ-
ual layers and up to 200 training iterations for the
backpropagation. Currently, we only optimise the
parameters for the source to target direction and
thus do not retain the bidirectionality1.
Thus, the whole training procedure consists of
4 phases. First, an autoencoder for the source
words is learnt. Second, an autoencoder for
the target words is learnt. Third, these autoen-
coders are connected by a top connecting layer,
and finally backpropagation is performed over the
whole network for fine-tuning of the weights.
2.3 Creation of n-Best Lists
N -best lists are a common means for combination
of several systems in natural language processing
and for rescoring. In this section, we describe how
a set of hypotheses can be created for a given in-
put. Although these hypotheses are not n-best lists
because they have not been obtained from a search
process, they can be used similarly and can bet-
ter be compared to randomly sampled ?good? hy-
potheses from a full word-graph.
Since the values of the nodes in the individ-
ual layers are probabilities for this particular node
to be activated, it is possible to sample a set of
states from the distribution for the individual lay-
ers, which is called Gibbs sampling (Geman and
Geman, 1984). This sampling can be used to cre-
ate several hypotheses for a given input sentence,
and this set of hypotheses can be used similar to
an n-best list.
The layer in which the Gibbs sampling is done
can in principle be chosen arbitrarily. However,
we believe it is natural to sample in either the first
layer, the joint layer, or the last layer. Sampling in
the first layer leads to different features traversing
the full network. Sampling in the joint layer only
affects the generation of the target sentence, and
sampling in the last layer is equal to directly sam-
pling from the distribution of target hypotheses.
Conventional Gibbs sampling has a very strong
impact on the outcome of the network because the
smoothness of the distributions and the encoding
of similar matches is entirely lost. Therefore, we
use a weak variant of Gibbs sampling. Instead of
replacing the states? probabilities with fully dis-
cretely sampled states, we keep the probabilities
1Note that it is easily possible to extend the backpropaga-
tion to include both directions, but to keep the computational
demands lower we decided to start with only one direction.
and add a fraction of a sampled state, effectively
modifying the probabilities to give a slightly bet-
ter score to the last sampled state. Let p be the
D-dimensional vector of probabilities for D nodes
in an RBM to be on. Normal Gibbs sampling
would sample a D-dimensional vector S contain-
ing a state for each node from this distribution.
Instead of replacing the vector p with S, we use
p? ? p + ?S, leading to smoother changes than
conventional Gibbs sampling. This process can
easily be repeated to obtain multiple hypotheses.
3 Experimental Evaluation
In this section we present experimental results for
an Arabic-English transliteration task. For evalu-
ation we use the character error rate (CER) which
is the commonly used word error rate (WER) on
character level.
We use a corpus of 10,084 personal names in
Arabic and their transliterated English ASCII rep-
resentation (LDC corpus LDC2005G02). The
Arabic names are written in the usual way, i.e.
lacking vowels and diacritics. 1,000 names were
randomly sampled for development and evalua-
tion, respectively (Freitag and Khadivi, 2007).
The vocabulary of the source language is 33 and
the target language has 30 different characters
(including the padding character). The longest
word on both sides consists of 14 characters,
thus the feature vector on the source side is 462-
dimensional and the feature vector on the target
side is 420-dimensional.
3.1 Network Structure
First, we evaluate how the structure of the network
should be chosen. For these experiments, we fixed
the numbers of layers and the size of the bottom
layers in the target and source encoder and evalu-
ate different network structures and the size of the
joint layer.
The experiments we performed are described in
Table 1. The top part of the table gives the results
for different network structures. We compare net-
works with increasing layer sizes, identical layer
sizes, and decreasing layer sizes. It can be seen
that decreasing layer sizes leads to the best results.
In these experiments, we choose the number of
nodes in the joint layer to be three times as large
as the topmost encoder layers.
In the bottom part, we kept most of the network
structure fixed and only vary the number of nodes
236
Table 1: Transliteration experiments using differ-
ent network structures.
number of nodes CER [%]
S1,T1 S2,T2 S3,T3 J train dev eval
400 500 600 1800 0.3 27.2 28.1
400 400 400 1200 0.7 26.1 25.2
400 350 300 900 1.8 25.1 24.3
400 350 300 1000 1.7 24.8 24.0
400 350 300 1500 1.3 24.1 22.7
400 350 300 2000 0.2 24.2 23.5
in the joint layer. Here, a small number of nodes
leads to suboptimal performance and a very high
number of nodes leads to overfitting which can be
seen in nearly perfect performance on the training
data and an increasing CER on the development
and eval data.
3.2 Network Size
Next, we evaluate systems with different numbers
of nodes. Therefore, we start from the best param-
eters (400-350-300-1500) from the previous sec-
tion and scale the number of nodes in the individ-
ual layers by a certain factor, i.e. factor 1.5 leads
to (600-525-450-2250).
In Figure 2 and Table 2, the results from the
experimental evaluation on the transliteration task
are given. The network size denotes the number
of nodes in the bottom layers of the source and the
target encoder (i.e. S1 and T1) and the other layers
are chosen according to the results from the exper-
iments presented in the previous section.
The results show that small networks perform
badly, the optimal performance is reached with
medium sized networks of 400-600 nodes in the
bottom layers, and larger networks perform worse,
which is probably due to overfitting.
For comparison, we give results for a state-of-
the-art phrase-based MT system applied on the
character level with default system parameters (la-
belled as ?PBT untuned?), and the same system,
where all scaling factors were tuned on dev data
(labelled as ?PBT tuned?). The tuned phrase-based
MT system clearly outperforms our approach.
Additionally, we perform an experiment with
a standard multi-layer perceptron. Therefore, we
choose the network structure with 400-350-300-
1500 nodes, initialised these randomly and trained
the entire network with backpropagation training.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 50  200  300  400  500  600  1000
CE
R [%
]
network size
traindevtest
Figure 2: Results for the Arabic-English translit-
eration task depending on the network size.
The results (line ?MLP-400? in Table 2) of this ex-
periment are far worse than any of the other re-
sults, which shows that, apart from the convenient
theoretical interpretation, the creation of the DBN
as described is a suitable method to train the sys-
tem. The reason for the large difference is likely
the bad initialisation of the network and the fact
that the backpropagation algorithm gets stuck in a
local optimum at this point.
3.3 Reordering capabilities
Although reordering is not an issue in transliter-
ation, the proposed model has certain properties
which we investigated and where interesting prop-
erties can be observed.
To investigate the performance under adverse
reordering conditions, we also perform an exper-
iment with reversed ordering of the target letters
(i.e. a word w = c1, c2, . . . , cJ is now written
cJ , cJ?1, . . . , c1). Since the DBN is fully sym-
metric, i.e. each input node is connected with each
output node in the same way and vice versa, the
DBN result is not changed except for some minor
numerical differences due to random initialisation.
Indeed, the DBN obtained is nearly identical ex-
cept for a changed ordering of the weights in the
joint layer, and if desired it is possible to construct
a DBN for reverse-order target language from a
fully trained DBN by permuting the weights.
On the same setup an experiment with our
phrase-based decoder has been performed and
here the performance is strongly decreased (bot-
tom line of Table 2). The phrase-based MT sys-
tem for this experiment used a reordering with
IBM block-limit constraints with distortion lim-
its and all default parameters were reasonably
tuned. We observed that the position-independent
237
Table 2: Results for the Arabic-English translit-
eration task depending on the network size and a
comparison with state of the art results using con-
ventional phrase-based machine translation tech-
niques
network CER [%]
size train dev eval
50 35.8 43.7 43.6
100 26.4 36.3 35.8
200 5.8 25.2 24.3
300 3.9 24.3 24.4
400 1.3 24.1 22.7
500 1.2 22.9 22.8
600 1.0 24.1 22.6
1000 0.2 26.6 24.4
MLP-400 22.0 64.1 63.2
untuned PBT 4.9 23.3 23.6
tuned PBT 2.2 12.9 13.3
(Freitag and Khadivi, 2007) n/a 11.1 11.1
reversed task: PBT 13.0 35.2 35.7
error rate of the phrase-based MT system is hardly
changed which also underlines that, in principle,
the phrase-based MT system is currently better but
that under adverse reordering conditions the DBN
system has some advantages.
3.4 N-Best Lists
As described above, different possibilities to cre-
ate n-best lists exists. Starting from the system
with 400-350-300-1500 nodes, we evaluate the
creation of n-best lists in the first source layer, the
joint layer, and the last target layer. Therefore,
we create n best lists with up to 10 hypotheses
(sometimes, we have less due to duplicates after
sampling, on the average we have 8.3 hypotheses
per sequence), and evaluate the oracle error rate.
In Table 3 it can be observed that sampling in the
first layer leads to the best oracle error rates. The
baseline performance (first best) for this system is
24.1% CER on the development data, and 22.7%
CER on the eval data, which can be improved by
nearly 10% absolute using the oracle from a 10-
best list.
3.5 Rescoring
Using the n-best list sampled in the first source
layer, we also perform rescoring experiments.
Therefore, we rescore the transliteration hypothe-
Table 3: Oracle character error rates on 10-best
lists.
sampling layer oracle CER [%]
dev eval
S1 15.8 14.8
joint layer 17.5 16.4
T1 18.7 18.2
CER [%]
System dev eval
DBN w/o rescoring 24.1 22.7
w/ rescoring 21.3 20.1
Table 4: Results from the rescoring experiments
and fusion with the phrase-based MT system.
ses (after truncating the padding letters w0) with
additional models, which are commonly used in
MT, and which we have trained on the training
data:
? IBM model 1 lexical probabilities modelling
the probability for a target sequence given a
source sequence
hIBM1(f
J
1 , e
I
1)=? log
?
?
1
(I + 1)J
J?
j=1
I?
i=0
p(fj |ei)
?
?
? m-gram language model over the letter se-
quences
hLM(e
I
1) = ? log
I?
i=1
p(ei|e
i?1
i?m+1),
with m being the size of the m-gram, we
choose m = 9.
? sequence length model (commonly referred
to as word penalty).
Then, these models are fused in a log-linear model
(Och and Ney, 2002), and we tune the model scal-
ing factors discriminatively on the development n-
best list using the downhill simplex algorithm. Re-
sults from the rescoring experiments are given in
Table 4.
The performance of the DBN system is im-
proved on the dev data from 24.1% to 21.3% CER
and on the eval data from 22.7% to 20.1% CER.
238
3.6 Application Within a System
Combination Framework
Although being clearly outperformed by the
phrase-based MT system, we applied the translit-
eration candidates generated by the DBN ap-
proach within a system combination framework.
Motivated by the fact that the DBN approach
differs decisively from the other statistical ap-
proaches we applied to the machine transliteration
task, we wanted to investigate the potential ben-
efit of the diverse nature of the DBN transliter-
ations. Taking the transliteration candidates ob-
tained from another study which was intended to
perform a comparison of various statistical ap-
proaches to the transliteration task, we performed
the system combination as is customary in speech
recognition, i.e. following the Recognizer Output
Voting Error Reduction (ROVER) approach (Fis-
cus, 1997).
The following methods were investigated:
(Monotone) Phrase-based MT on character level:
A state-of-the-art phrase-based SMT system
(Zens and Ney, 2004) was used for name
transliteration, i.e. translation of characters
instead of words. No reordering model
was employed due to the monotonicity
of the transliteration task, and the model
scaling factors were tuned on maximum
transliteration accuracy.
Data-driven grapheme-to-phoneme conversion:
In Grapheme-to-Phoneme conversion (G2P),
or phonetic transcription, we seek the most
likely pronunciation (phoneme sequence)
for a given orthographic form (sequence of
letters). Then, a grapheme-phoneme joint
multi-gram, or graphone for short, is a pair
of a letter sequence and a phoneme sequence
of possibly different length (Bisani and Ney,
2008). The model training is done in two
steps: First, maximum likelihood is used to
infer the graphones. Second, the input is
segmented into a stream of graphones and
absolute discounting with leaving-one-out
is applied to estimate the actual M -gram
model. Interpreting the characters of the
English target names as phonemes, we used
the G2P toolkit of (Bisani and Ney, 2008) to
transliterate the Arabic names.
Position-wise maximum entropy models / CRFs:
The segmentation as provided by the G2P
model is used and ?null words? are inserted
such that the transliteration task can be
interpreted as a classical tagging task (e.g.
POS, conceptual tagging, etc.). This means
that we seek for a one-to-one mapping and
define feature functions to model the pos-
terior probability. Maximum entropy (ME)
models are defined position-wise, whereas
conditional random fields (CRFs) consider
full sequences. Both models were trained
according to the maximum class posterior
criterion. We used an ME tagger (Bender et
al., 2003) and the freely available CRF++
toolkit.2
Results for each of the individual systems and
different combinations are given in Table 5. As
expected, the DBN transliterations cannot keep up
with the other approaches. The additional models
(G2P, CRF and ME) perform slightly better than
the PBT method. If we look at combinations of
systems without the DBN approach, we observe
only marginal improvements of around 0.1-0.2%
CER. Interestingly, a combination of all 4 models
(PBT, G2P, ME, CRF) works as good as individual
3-way combinations (the same 11.9% on dev are
obtained). This can be interpreted as a potential
?similarity? of the approaches. Adding e.g. ME to
a combination of PBT, G2P and CRF does not im-
prove results because the transliteration hypothe-
ses are too similar. If we simply put together all
5 systems including DBN with equal weights, we
have a similar trend. Since all systems are equally
weighted and at least 3 of the systems are similar
in individual performance (G2P, ME, CRF have all
around 12% CER on the tested data sets), the DBN
approach does not get a large impact on overall
performance.
If we drop similar systems and tune for 3-way
combinations, we observe a large reduction in
CER if DBN comes into play. Compared to the
best individual system of 12% CER, we now ar-
rive at a CER of 10.9% for a combination of PBT,
CRF and DBN which is significantly better than
each of the individual methods. Our interpreta-
tion of this is that the DBN system has different
hypotheses compared to all other systems and that
the hypotheses from the other systems are too sim-
ilar to be apt for combination. So, although DBN
is much worse than the other approaches, it obvi-
ously helps in the system combination. Using the
rescored variant of the DBN transliterations from
2http://crfpp.sourceforge.net/
239
CER [%]
System dev eval
DBN 24.1 22.7
PBT 12.9 13.3
G2P 12.2 12.1
ME 12.3 12.4
CRF 12.0 12.0
ROVER
best setting w/o DBN 11.9 11.8
5-way equal weights 11.7 11.9
best setting w/ DBN 10.9 10.9
Table 5: Results from the individual methods in-
vestigated versus ROVER combination.
Section 3.5, performance is similar to the one ob-
tained for the DBN baseline.
4 Discussion and Conclusion
We have presented a novel method for machine
transliteration based on DBNs, which despite not
having competitive results can be an important ad-
ditional cue for system combination setups. The
DBN model has some immediate advantages: the
model is in principle fully bidirectional and is
based on sound and valid theories from machine
learning. Instead of common techniques which
are based on finite-state machines or phrase-based
machine translation, the proposed system does not
rely on word alignments and beam-search decod-
ing and has interesting properties regarding the re-
ordering of sequences. We have experimentally
evaluated the network structure and size, reorder-
ing capabilities, the creation of multiple hypothe-
ses, and rescoring and combination with other
transliteration approaches. It was shown that, al-
beit the approach cannot compete with the cur-
rent state of the art, deep belief networks might
be a learning framework with some potential for
transliteration. It was also shown that the pro-
posed method is suited for combination with dif-
ferent state-of-the-art systems and that improve-
ments over the single models can be obtained
in a ROVER-like setting. Furthermore, adding
DBN-based transliterations, although individually
far behind the other approaches, significantly im-
proves the overall results by 1% absolute.
Outlook
In the future we plan to investigate several details
of the proposed model: we will exploit the inher-
ent bidirectionality, further investigate the struc-
ture of the model, such as the number of layers
and the numbers of nodes in the individual lay-
ers. Also, it is important to improve the efficiency
of our implementation to allow for working on
larger datasets and obtain more competitive re-
sults. Furthermore, we are planning to investigate
convolutional input layers for transliteration and
use a translation approach analogous to the one
proposed by Collobert and Weston (2008) in or-
der to allow for the incorporation of reorderings,
language models, and to be able to work on larger
tasks.
Acknowledgement. We would like to thank Ge-
offrey Hinton for providing the Matlab Code ac-
companying (Hinton and Salakhutdinow, 2006).
References
D. Ackley, G. Hinton, and T. Sejnowski. 1985. A
learning algorithm for Boltzmann machines. Cog-
nitive Science, 9(1):147?169.
Y. Al-Onaizan and K. Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
2002 Workshop on Computationaal Approaches to
Semitic Languages.
M. Asuncio?n Castan?o, F. Casacuberta, and E. Vidal.
1997. Machine translation using neural networks
and finite-state models. In Theoretical and Method-
ological Issues in Machine Translation (TMI), pages
160?167, Santa Fe, NM, USA, July.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Sta-
tistical machine translation through global lexical
selection and sentence reconstruction. In Annual
Meeting of the Association for Computational Lin-
gustic (ACL), pages 152?159, Prague, Czech Repub-
lic.
O. Bender, F. J. Och, and H. Ney. 2003. Maxi-
mum entropy models for named entity recognition.
In Proc. 7th Conf. on Computational Natural Lan-
guage Learning (CoNLL), pages 148?151, Edmon-
ton, Canada, May.
M. Bisani and H. Ney. 2008. Joint-sequence models
for grapheme-to-phoneme conversion. Speech Com-
munication, 50(5):434?451, May.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, Helsinki, Finn-
land, July.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 347?354, Santa Barbara, CA, USA, Decem-
ber.
240
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247, Prague, Czech
Republic, June.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence, 6(6):721?741, November.
G. Hinton and R. R. Salakhutdinow. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313:504?507, July.
G. Hinton, S. Osindero, and Y.-W. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527?1554.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In HLT-NAACL.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(2).
A. Mnih and G. Hinton. 2007. Three new graphical
models for statistical language modelling. In ICML
?07: International Conference on Machine Learn-
ing, pages 641?648, New York, NY, USA. ACM.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Annual Meeting of the As-
soc. for Computational Linguistics, pages 295?302,
Philadelphia, PA, USA, July.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 632?639,
Prague, Czech Republic, June.
A. Torralba, R. Fergus, and Y. Weiss. 2008. Small
codes and large image databases for recognition. In
IEEE Conference on Computer Vision and Pattern
Recognition, Anchorage, AK, USA, June.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: HLT-NAACL
2004, pages 257?264, Boston, MA, May.
241
