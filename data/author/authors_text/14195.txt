Proceedings of the ACL 2010 Student Research Workshop, pages 13?18,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
WSD as a Distributed Constraint Optimization Problem
Siva Reddy
IIIT Hyderabad
India
gvsreddy@students.iiit.ac.in
Abhilash Inumella
IIIT Hyderabad
India
abhilashi@students.iiit.ac.in
Abstract
This work models Word Sense Disam-
biguation (WSD) problem as a Dis-
tributed Constraint Optimization Problem
(DCOP). To model WSD as a DCOP,
we view information from various knowl-
edge sources as constraints. DCOP al-
gorithms have the remarkable property to
jointly maximize over a wide range of util-
ity functions associated with these con-
straints. We show how utility functions
can be designed for various knowledge
sources. For the purpose of evaluation,
we modelled all words WSD as a simple
DCOP problem. The results are competi-
tive with state-of-art knowledge based sys-
tems.
1 Introduction
Words in a language may carry more than one
sense. The correct sense of a word can be iden-
tified based on the context in which it occurs. In
the sentence, He took all his money from the bank,
bank refers to a financial institution sense instead
of other possibilities like the edge of river sense.
Given a word and its possible senses, as defined
by a dictionary, the problem of Word Sense Dis-
ambiguation (WSD) can be defined as the task of
assigning the most appropriate sense to the word
within a given context.
WSD is one of the oldest problems in com-
putational linguistics which dates back to early
1950?s. A range of knowledge sources have been
found to be useful for WSD. (Agirre and Steven-
son, 2006; Agirre and Mart??nez, 2001; McRoy,
1992; Hirst, 1987) highlight the importance of
various knowledge sources like part of speech,
morphology, collocations, lexical knowledge base
(sense taxonomy, gloss), sub-categorization, se-
mantic word associations, selectional preferences,
semantic roles, domain, topical word associations,
frequency of senses, collocations, domain knowl-
edge. etc. Methods for WSD exploit information
from one or more of these knowledge sources.
Supervised approaches like (Yarowsky and Flo-
rian, 2002; Lee and Ng, 2002; Mart??nez et al,
2002; Stevenson and Wilks, 2001) used collec-
tive information from various knowledge sources
to perform disambiguation. Information from var-
ious knowledge sources is encoded in the form of
a feature vector and models were built by training
on sense-tagged corpora. These approaches pose
WSD as a classification problem. They crucially
rely on hand-tagged sense corpora which is hard
to obtain. Systems that do not need hand-tagging
have also been proposed. Agirre and Martinez
(Agirre and Mart??nez, 2001) evaluated the contri-
bution of each knowledge source separately. How-
ever, this does not combine information from more
than one knowledge source.
In any case, little effort has been made in for-
malizing the way in which information from var-
ious knowledge sources can be collectively used
within a single framework: a framework that al-
lows interaction of evidence from various knowl-
edge sources to arrive at a global optimal solution.
Here we present a way for modelling informa-
tion from various knowledge sources in a multi
agent setting called distributed constraint opti-
mization problem (DCOP). In DCOP, agents have
constraints on their values and each constraint has
a utility associated with it. The agents communi-
cate with each other and choose values such that a
global optimum solution (maximum utility) is at-
tained. We aim to solve WSD by modelling it as a
DCOP.
To the best of our knowledge, ours is the first
attempt to model WSD as a DCOP. In DCOP
framework, information from various knowledge
sources can be used combinedly to perform WSD.
In section 2, we give a brief introduction of
13
DCOP. Section 3 describes modelling WSD as
a DCOP. Utility functions for various knowledge
sources are described in section 4. In section 5,
we conduct a simple experiment by modelling all-
words WSD problem as a DCOP and perform dis-
ambiguation on Senseval-2 (Cotton et al, 2001)
and Senseval-3 (Mihalcea and Edmonds, 2004)
data-set of all-words task. Next follow the sec-
tions on related work, discussion, future work and
conclusion.
2 Distributed Constraint Optimization
Problem (DCOP)
A DCOP (Modi, 2003; Modi et al, 2004) consists
of n variables V = x
1
, x
2
, ...x
n
each assigned
to an agent, where the values of the variables are
taken from finite, discrete domains D
1
, D
2
, ..., D
n
respectively. Only the agent has knowledge and
control over values assigned to variables associ-
ated to it. The goal for the agents is to choose
values for variables such that a given global objec-
tive function is maximized. The objective function
is described as the summation over a set of utility
functions.
DCOP can be formalized as a tuple (A, V, D, C,
F) where
? A = {a
1
, a
2
, . . . a
n
} is a set of n agents,
? V = {x
1
, x
2
, . . . x
n
} is a set of n variables,
each one associated to an agent,
? D = {D
1
, D
2
, . . . D
n
} is a set of finite and
discrete domains each one associated to the
corresponding variable,
? C = {f
k
: D
i
?D
j
? . . . D
m
? ?} is a set of
constraints described by various utility func-
tions f
k
. The utility function f
k
is defined
over a subset of variables V . The domain
of f
k
represent the constraints C
f
k
and f
k
(c)
represents the utility associated with the con-
straint c, where c ? C
f
k
.
? F =
?
k
z
k
? f
k
is the objective function to be
maximized where z
k
is the weight of the cor-
responding utility function f
k
An agent is allowed to communicate only with
its neighbours. Agents communicate with each
other to agree upon a solution which maximizes
the objective function.
3 WSD as a DCOP
Given a sequence of words W= {w
1
, w
2
, . . . w
n
}
with corresponding admissible senses D
w
i
=
{s
1
w
i
, s
2
w
i
. . .}, we model WSD as DCOP as fol-
lows.
3.1 Agents
Each word w
i
is treated as an agent. The agent
(word) has knowledge and control of its values
(senses).
3.2 Variables
Sense of a word varies and it is the one to be deter-
mined. We define the sense of a word as its vari-
able. Each agent w
i
is associated with the variable
s
w
i
. The value assigned to this variable indicates
the sense assigned by the algorithm.
3.3 Domains
Senses of a word are finite in number. The set of
senses D
w
i
, is the domain of the variable s
w
i
.
3.4 Constraints
A constraint specifies a particular configuration of
the agents involved in its definition and has a util-
ity associated with it. For e.g. If c
ij
is a constraint
defined on agents w
i
and w
j
, then c
ij
refers to a
particular instantiation of w
i
and w
j
, say w
i
= s
p
w
i
and w
j
= s
q
w
j
.
A utility function f
k
: C
f
k
? ? denote a set of
constraints C
f
k
= {D
w
i
?D
w
j
. . . D
w
m
}, defined
on the agents w
i
, w
j
. . . w
m
and also the utilities
associated with the constraints. We model infor-
mation from each knowledge source as a utility
function. In section 4, we describe in detail about
this modelling.
3.5 Objective function
As already stated, various knowledge sources are
identified to be useful for WSD. It is desirable to
use information from these sources collectively,
to perform disambiguation. DCOP provides such
framework where an objective function is defined
over all the knowledge sources (f
k
) as below
F =
?
k
z
k
? f
k
where F denotes the total utility associated with
a solution and z
k
is the weight given to a knowl-
edge source i.e. information from various sources
14
can be weighted. (Note: It is desirable to nor-
malize utility functions of different knowledge
sources in order to compare them.)
Every agent (word) choose its value (sense) in a
such a way that the objective function (global solu-
tion) is maximized. This way an agent is assigned
a best value which is the target sense in our case.
4 Modelling information from various
knowledge sources
In this section, we discuss the modelling of infor-
mation from various knowledge sources.
4.1 Part-of-speech (POS)
Consider the word play. It has 47 senses out of
which only 17 senses correspond to noun category.
Based on the POS information of a word w
i
, its
domain D
w
i
is restricted accordingly.
4.2 Morphology
Noun orange has at least two senses, one corre-
sponding to a color and other to a fruit. But plu-
ral form of this word oranges can only be used in
the fruit sense. Depending upon the morphologi-
cal information of a word w
i
, its domain D
w
i
can
be restricted.
4.3 Domain information
In the sports domain, cricket likely refers to a
game than an insect. Such information can be cap-
tured using a unary utility function defined for ev-
ery word. If the sense distributions of a word w
i
are known, a function f : D
w
i
? ? is defined
which return higher utility for the senses favoured
by the domain than to the other senses.
4.4 Sense Relatedness
Sense relatedness between senses of two words
w
i
, w
j
is captured by a function f : D
w
i
?D
w
j
?
? where f returns sense relatedness (utility) be-
tween senses based on sense taxonomy and gloss
overlaps.
4.5 Discourse
Discourse constraints can be modelled using a
n-ary function. For instance, to the extent one
sense per discourse (Gale et al, 1992) holds true,
higher utility can be returned to the solutions
which favour same sense to all the occurrences
of a word in a given discourse. This information
can be modeled as follows: If w
i
, w
j
, . . . w
m
are
the occurrences of a same word, a function f :
D
i
? D
j
? . . . D
m
? ? is defined which returns
higher utility when s
w
i
= s
w
j
= . . . s
w
m
and for
the rest of the combinations it returns lower utility.
4.6 Collocations
Collocations of a word are known to provide
strong evidence for identifying correct sense of the
word. For example: if in a given context bank co-
occur with money, it is likely that bank refers to
financial institution sense rather than the edge of
a river sense. The word cancer has at least two
senses, one corresponding to the astrological sign
and the other a disease. But its derived form can-
cerous can only be used in disease sense. When
the words cancer and cancerous co-occur in a dis-
course, it is likely that the word cancer refers to
disease sense.
Most supervised systems work through colloca-
tions to identify correct sense of a word. If a word
w
i
co-occurs with its collocate v, collocational in-
formation from v can be modeled by using the fol-
lowing function
coll infrm v
w
i
: D
w
i
? ?
where coll infrm v
w
i
returns high utility to
collocationally preferred senses of w
i
than other
senses.
Collocations can also be modeled by assigning
more than one variable to the agents or by adding a
dummy agent which gives collocational informa-
tion but in view of simplicity we do not go into
those details.
Topical word associations, semantic word asso-
ciations, selectional preferences can also be mod-
eled similar to collocations. Complex information
involving more than two entities can be modelled
by using n-ary utility functions.
5 Experiment: DCOP based All Words
WSD
We carried out a simple experiment to test the ef-
fectiveness of DCOP algorithm. We conducted
our experiment in an all words setting and used
only WordNet (Fellbaum, 1998) based relatedness
measures as knowledge source so that results can
be compared with earlier state-of-art knowledge-
based WSD systems like (Agirre and Soroa, 2009;
Sinha and Mihalcea, 2007) which used similar
knowledge sources as ours.
15
Our method performs disambiguation on sen-
tence by sentence basis. A utility function based
on semantic relatedness is defined for every pair
of words falling in a particular window size. Re-
stricting utility functions to a window size reduces
the number of constraints. An objective function is
defined as sum of these restricted utility functions
over the entire sentence and thus allowing infor-
mation flow across all the words. Hence, a DCOP
algorithm which aims to maximize this objective
function leads to a globally optimal solution.
In our experiments, we used the best similarity
measure settings of (Sinha and Mihalcea, 2007)
which is a sum of normalized similarity mea-
sures jcn, lch and lesk. We used used Distributed
Pseudotree Optimization Procedure (DPOP) algo-
rithm (Petcu and Faltings, 2005), which solves
DCOP using linear number of messages among
agents. The implementation provided with the
open source toolkit FRODO1 (Le?aute? et al, 2009)
is used.
5.1 Data
To compare our results, we ran our experiments
on SENSEVAL-2 and SENSEVAL -3 English all-
words data sets.
5.2 Results
Table 1 shows results of our experiments. All
these results are carried out using a window size
of four. Ideally, precision and recall values are ex-
pected to be equal in our setting. But in certain
cases, the tool we used, FRODO, failed to find a
solution with the available memory resources.
Results show that our system performs con-
sistently better than (Sinha and Mihalcea, 2007)
which uses exactly same knowledge sources as
used by us (with an exception of adverbs in
Senseval-2). This shows that DCOP algorithm
perform better than page-rank algorithm used in
their graph based setting. Thus, for knowledge-
based WSD, DCOP framework is a potential al-
ternative to graph based models.
Table 1 also shows the system (Agirre and
Soroa, 2009), which obtained best results for
knowledge based WSD. A direct comparison
between this and our system is not quantita-
tive since they used additional knowledge such
as extended WordNet relations (Mihalcea and
1http://liawww.epfl.ch/frodo/
Moldovan, 2001) and sense disambiguated gloss
present in WordNet3.0.
Senseval-2 All Words data set
noun verb adj adv all
P dcop 67.85 37.37 62.72 56.87 58.63
R dcop 66.44 35.47 61.28 56.65 57.09
F dcop 67.14 36.39 61.99 56.76 57.85
P Sinha07 67.73 36.05 62.21 60.47 58.83
R Sinha07 65.63 32.20 61.42 60.23 56.37
F Sinha07 66.24 34.07 61.81 60.35 57.57
Agirre09 70.40 38.90 58.30 70.1 58.6
MFS 71.2 39.0 61.1 75.4 60.1
Senseval-3 All Words data set
P dcop 62.31 43.48 57.14 100 54.68
R dcop 60.97 42.81 55.17 100 53.51
F dcop 61.63 43.14 56.14 100 54.09
P Sinha07 61.22 45.18 54.79 100 54.86
R Sinha07 60.45 40.57 54.14 100 52.40
F Sinha07 60.83 42.75 54.46 100 53.60
Agirre09 64.1 46.9 62.6 92.9 57.4
MFS 69.3 53.6 63.7 92.9 62.3
Table 1: Evaluation results on Senseval-2 and
Senseval-3 data-set of all words task.
5.3 Performance analysis
We conducted our experiment on a computer with
two 2.94 GHz process and 2 GB memory. Our
algorithm just took 5 minutes 31 seconds on
Senseval-2 data set, and 5 minutes 19 seconds on
Senseval-3 data set. This is a singable reduction
compared to execution time of page rank algo-
rithms employed in both Sinha07 and Agirre09. In
Agirre09, it falls in the range 30 to 180 minutes on
much powerful system with 16 GB memory hav-
ing four 2.66 GHz processors. On our system,
time taken by the page rank algorithm in (Sinha
and Mihalcea, 2007) is 11 minutes when executed
on Senseval-2 data set.
Since DCOP algorithms are truly distributed in
nature the execution times can be further reduced
by running them parallely on multiple processors.
6 Related work
Earlier approaches to WSD which encoded infor-
mation from variety of knowledge sources can be
classified as follows:
? Supervised approaches: Most of the super-
vised systems (Yarowsky and Florian, 2002;
16
Lee and Ng, 2002; Mart??nez et al, 2002;
Stevenson and Wilks, 2001) rely on the sense
tagged data. These are mainly discrimina-
tive or aggregative models which essentially
pose WSD a classification problem. Dis-
criminative models aim to identify the most
informative feature and aggregative models
make their decisions by combining all fea-
tures. They disambiguate word by word and
do not collectively disambiguate whole con-
text and thereby do not capture all the rela-
tionships (e.g sense relatedness) among all
the words. Further, they lack the ability to
directly represent constraints like one sense
per discourse.
? Graph based approaches: These approaches
crucially rely on lexical knowledge base.
Graph-based WSD approaches (Agirre and
Soroa, 2009; Sinha and Mihalcea, 2007) per-
form disambiguation over a graph composed
of senses (nodes) and relations between pairs
of senses (edges). The edge weights encode
information from a lexical knowledge base
but lack an efficient way of modelling in-
formation from other knowledge sources like
collocational information, selectional prefer-
ences, domain information, discourse. Also,
the edges represent binary utility functions
defined over two entities which lacks the abil-
ity to encode ternary, and in general, any N-
ary utility functions.
7 Discussion
This framework provides a convenient way of
integrating information from various knowledge
sources by defining their utility functions. Infor-
mation from different knowledge sources can be
weighed based on the setting at hand. For exam-
ple, in a domain specific WSD setting, sense dis-
tributions play a crucial role. The utility function
corresponding to the sense distributions can be
weighed higher in order to take advantage of do-
main information. Also, different combination of
weights can be tried out for a given setting. Thus
for a given WSD setting, this framework allows us
to find 1) the impact of each knowledge source in-
dividually 2) the best combination of knowledge
sources.
Limitations of DCOP algorithms: Solving
DCOPs is NP-hard. A variety of search algorithms
have therefore been developed to solve DCOPs
(Mailler and Lesser, 2004; Modi et al, 2004;
Petcu and Faltings, 2005) . As the number of
constraints or words increase, the search space in-
creases thereby increasing the time and memory
bounds to solve them. Also DCOP algorithms ex-
hibit a trade-off between memory used and num-
ber of messages communicated between agents.
DPOP (Petcu and Faltings, 2005) use linear num-
ber of messages but requires exponential memory
whereas ADOPT (Modi et al, 2004) exhibits lin-
ear memory complexity but exchange exponential
number of messages. So it is crucial to choose a
suitable algorithm based on the problem at hand.
8 Future Work
In our experiment, we only used relatedness based
utility functions derived from WordNet. Effect of
other knowledge sources remains to be evaluated
individually and in combination. The best possible
combination of weights of knowledge sources is
yet to be engineered. Which DCOP algorithm per-
forms better WSD and when has to be explored.
9 Conclusion
We initiated a new line of investigation into WSD
by modelling it in a distributed constraint opti-
mization framework. We showed that this frame-
work is powerful enough to encode information
from various knowledge sources. Our experimen-
tal results show that a simple DCOP based model
encoding just word similarity constraints performs
comparably with the state-of-the-art knowledge
based WSD systems.
Acknowledgement
We would like to thank Prof. Rajeev Sangal and
Asrar Ahmed for their support in coming up with
this work.
References
Eneko Agirre and David Mart??nez. 2001. Knowledge
sources for word sense disambiguation. In Text,
Speech and Dialogue, 4th International Conference,
TSD 2001, Zelezna Ruda, Czech Republic, Septem-
ber 11-13, 2001, Lecture Notes in Computer Sci-
ence, pages 1?10. Springer.
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
EACL ?09: Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 33?41, Morristown, NJ,
USA. Association for Computational Linguistics.
17
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Word Sense Disambiguation:
Algorithms and Applications, volume 33 of Text,
Speech and Language Technology, pages 217?252.
Springer, Dordrecht, The Netherlands.
Scott Cotton, Phil Edmonds, Adam Kilgarriff, and
Martha Palmer. 2001. Senseval-2. http://www.
sle.sharp.co.uk/senseval2.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
?91: Proceedings of the workshop on Speech and
Natural Language, pages 233?237, Morristown, NJ,
USA. Association for Computational Linguistics.
Graeme Hirst. 1987. Semantic interpretation and
the resolution of ambiguity. Cambridge University
Press, New York, NY, USA.
Thomas Le?aute?, Brammert Ottens, and Radoslaw Szy-
manek. 2009. FRODO 2.0: An open-source
framework for distributed constraint optimization.
In Proceedings of the IJCAI?09 Distributed Con-
straint Reasoning Workshop (DCR?09), pages 160?
164, Pasadena, California, USA, July 13. http:
//liawww.epfl.ch/frodo/.
Yoong Keok Lee and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithms for word sense disambiguation. In
EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing, pages 41?48, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Roger Mailler and Victor Lesser. 2004. Solving
distributed constraint optimization problems using
cooperative mediation. In AAMAS ?04: Proceed-
ings of the Third International Joint Conference on
Autonomous Agents and Multiagent Systems, pages
438?445, Washington, DC, USA. IEEE Computer
Society.
David Mart??nez, Eneko Agirre, and Llu??s Ma`rquez.
2002. Syntactic features for high precision word
sense disambiguation. In COLING.
Susan W. McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. COMPUTA-
TIONAL LINGUISTICS, 18:1?30.
Rada Mihalcea and Phil Edmonds, editors. 2004.
Proceedings Senseval-3 3rd International Workshop
on Evaluating Word Sense Disambiguation Systems.
ACL, Barcelona, Spain.
Rada Mihalcea and Dan I. Moldovan. 2001. ex-
tended wordnet: progress report. In in Proceedings
of NAACL Workshop on WordNet and Other Lexical
Resources, pages 95?100.
Pragnesh Jay Modi, Wei-Min Shen, Milind Tambe, and
Makoto Yokoo. 2004. Adopt: Asynchronous dis-
tributed constraint optimization with quality guaran-
tees. Artificial Intelligence, 161:149?180.
Pragnesh Jay Modi. 2003. Distributed constraint opti-
mization for multiagent systems. PhD Thesis.
Adrian Petcu and Boi Faltings. 2005. A scalable
method for multiagent constraint optimization. In
IJCAI?05: Proceedings of the 19th international
joint conference on Artificial intelligence, pages
266?271, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In ICSC ?07: Pro-
ceedings of the International Conference on Seman-
tic Computing, pages 363?369, Washington, DC,
USA. IEEE Computer Society.
Mark Stevenson and Yorick Wilks. 2001. The inter-
action of knowledge sources in word sense disam-
biguation. Comput. Linguist., 27(3):321?349.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation across diverse parameter
spaces. Natural Language Engineering, 8:2002.
18
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 387?391,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
IIITH: Domain Specific Word Sense Disambiguation
Siva Reddy
IIIT Hyderabad
India
gvsreddy@students.iiit.ac.in
Diana McCarthy
Lexical Computing Ltd.
United Kingdom
diana@dianamccarthy.co.uk
Abhilash Inumella
IIIT Hyderabad
India
abhilashi@students.iiit.ac.in
Mark Stevenson
University of Sheffield
United Kingdom
m.stevenson@dcs.shef.ac.uk
Abstract
We describe two systems that participated
in SemEval-2010 task 17 (All-words Word
Sense Disambiguation on a Specific Do-
main) and were ranked in the third and
fourth positions in the formal evaluation.
Domain adaptation techniques using the
background documents released in the
task were used to assign ranking scores to
the words and their senses. The test data
was disambiguated using the Personalized
PageRank algorithm which was applied
to a graph constructed from the whole of
WordNet in which nodes are initialized
with ranking scores of words and their
senses. In the competition, our systems
achieved comparable accuracy of 53.4 and
52.2, which outperforms the most frequent
sense baseline (50.5).
1 Introduction
The senses in WordNet are ordered according to
their frequency in a manually tagged corpus, Sem-
Cor (Miller et al, 1993). Senses that do not oc-
cur in SemCor are ordered arbitrarily after those
senses of the word that have occurred. It is known
from the results of SENSEVAL2 (Cotton et al,
2001) and SENSEVAL3 (Mihalcea and Edmonds,
2004) that first sense heuristic outperforms many
WSD systems (see McCarthy et al (2007)). The
first sense baseline?s strong performance is due to
the skewed frequency distribution of word senses.
WordNet sense distributions based on SemCor are
clearly useful, however in a given domain these
distributions may not hold true. For example, the
first sense for ?bank? in WordNet refers to ?slop-
ing land beside a body of river? and the second
to ?financial institution?, but in the domain of ?fi-
nance? the ?financial institution? sense would be
expected to be more likely than the ?sloping land
beside a body of river? sense. Unfortunately, it
is not feasible to produce large manually sense-
annotated corpora for every domain of interest.
McCarthy et al (2004) propose a method to pre-
dict sense distributions from raw corpora and use
this as a first sense heuristic for tagging text with
the predominant sense. Rather than assigning pre-
dominant sense in every case, our approach aims
to use these sense distributions collected from do-
main specific corpora as a knowledge source and
combine this with information from the context.
Our approach focuses on the strong influence of
domain for WSD (Buitelaar et al, 2006) and the
benefits of focusing on words salient to the do-
main (Koeling et al, 2005). Words are assigned
a ranking score based on its keyness (salience) in
the given domain. We use these word scores as
another knowledge source.
Graph based methods have been shown to
produce state-of-the-art performance for unsu-
pervised word sense disambiguation (Agirre and
Soroa, 2009; Sinha and Mihalcea, 2007). These
approaches use well-known graph-based tech-
niques to find and exploit the structural properties
of the graph underlying a particular lexical knowl-
edge base (LKB), such as WordNet. These graph-
based algorithms are appealing because they take
into account information drawn from the entire
graph as well as from the given context, making
them superior to other approaches that rely only
on local information individually derived for each
word.
Our approach uses the Personalized PageRank
algorithm (Agirre and Soroa, 2009) over a graph
387
representing WordNet to disambiguate ambigu-
ous words by taking their context into consider-
ation. We also combine domain-specific informa-
tion from the knowledge sources, like sense distri-
bution scores and keyword ranking scores, into the
graph thus personalizing the graph for the given
domain.
In section 2, we describe domain sense ranking.
Domain keyword ranking is described in Section
3. Graph construction and personalized page rank
are described in Section 4. Evaluation results over
the SemEval data are provided in Section 5.
2 Domain Sense Ranking
McCarthy et al (2004) propose a method for
finding predominant senses from raw text. The
method uses a thesaurus acquired from automat-
ically parsed text based on the method described
by Lin (1998). This provides the top k nearest
neighbours for each target word w, along with the
distributional similarity score between the target
word and each neighbour. The senses of a word
w are each assigned a score by summing over the
distributional similarity scores of its neighbours.
These are weighted by a semantic similarity score
(using WordNet Similarity score (Pedersen et al,
2004) between the sense of w and the sense of the
neighbour that maximizes the semantic similarity
score.
More formally, let N
w
= {n
1
, n
2
, . . . n
k
}
be the ordered set of the top k scoring
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w, n
1
), dss(w, n
2
), . . . dss(w, n
k
)}. Let
senses(w) be the set of senses of w. For each
sense of w (ws
i
? senses(w)) a ranking score is
obtained by summing over the dss(w, n
j
) of each
neighbour (n
j
? N
w
) multiplied by a weight.
This weight is the WordNet similarity score
(wnss) between the target sense (ws
i
) and the
sense of n
j
(ns
x
? senses(n
j
)) that maximizes
this score, divided by the sum of all such WordNet
similarity scores for senses(w) and n
j
. Each
sense ws
i
? senses(w) is given a sense ranking
score srs(ws
i
) using
srs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
where wnss(ws
i
, n
j
) =
max
ns
x
?senses(n
j
)
(wnss(ws
i
, ns
x
))
Since this approach requires only raw text,
sense rankings for a particular domain can be gen-
erated by simply training the algorithm using a
corpus representing that domain. We used the
background documents provided to the partici-
pants in this task as a domain specific corpus. In
general, a domain specific corpus can be obtained
using domain-specific keywords (Kilgarriff et al,
2010). A thesaurus is acquired from automatically
parsed background documents using the Stanford
Parser (Klein and Manning, 2003). We used k = 5
to built the thesaurus. As we increased k we found
the number of non-domain specific words occur-
ring in the thesaurus increased and negatively af-
fected the sense distributions. To counter this, one
of our systems IIITH2 used a slightly modified
ranking score by multiplying the effect of each
neighbour with its domain keyword ranking score.
The modified sense ranking msrs(ws
j
) score of
sense ws
i
is
msrs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
?krs(n
j
)
where krs(n
j
) is the keyword ranking score of
the neighbour n
j
in the domain specific corpus. In
the next section we describe the way in which we
compute krs(n
j
).
WordNet::Similarity::lesk (Pedersen et al,
2004) was used to compute word similarity wnss.
IIITH1 and IIITH2 systems differ in the way
senses are ranked. IIITH1 uses srs(ws
j
) whereas
IIITH2 system uses msrs(ws
j
) for computing
sense ranking scores in the given domain.
3 Domain Keyword Ranking
We extracted keywords in the domain by compar-
ing the frequency lists of domain corpora (back-
ground documents) and a very large general cor-
pus, ukWaC (Ferraresi et al, 2008), using the
method described by Rayson and Garside (2000).
For each word in the frequency list of the domain
corpora, words(domain), we calculated the log-
likelihood (LL) statistic as described in Rayson
and Garside (2000). We then normalized LL to
compute keyword ranking score krs(w) of word
w words(domain) using
388
krs(w) =
LL(w)
?
w
i
?words(domain)
LL(w
i
)
The above score represents the keyness of the
word in the given domain. Top ten keywords (in
descending order of krs) in the corpora provided
for this task are species, biodiversity, life, habitat,
natura
1
, EU, forest, conservation, years, amp
2
.
4 Personalized PageRank
Our approach uses the Personalized PageRank al-
gorithm (Agirre and Soroa, 2009) with WordNet
as the lexical knowledge base (LKB) to perform
WSD. WordNet is converted to a graph by repre-
senting each synset as a node (synset node) and the
relationships in WordNet (hypernymy, hyponymy
etc.) as edges between synset nodes. The graph is
initialized by adding a node (word node) for each
context word of the target word (including itself)
thus creating a context dependent graph (person-
alized graph). The popular PageRank (Page et al,
1999) algorithm is employed to analyze this per-
sonalized graph (thus the algorithm is referred as
personalized PageRank algorithm) and the sense
for each disambiguous word is chosen by choos-
ing the synset node which gets the highest weight
after a certain number of iterations of PageRank
algorithm.
We capture domain information in the personal-
ized graph by using sense ranking scores and key-
word ranking scores of the domain to assign initial
weights to the word nodes and their edges (word-
synset edge). This way we personalize the graph
for the given domain.
4.1 Graph Initialization Methods
We experimented with different ways of initial-
izing the graph, described below, which are de-
signed to capture domain specific information.
Personalized Page rank (PPR): In this method,
the graph is initialized by allocating equal prob-
ability mass to all the word nodes in the context
including the target word itself, thus making the
graph context sensitive. This does not include do-
main specific information.
1
In background documents this word occurs in reports de-
scribing Natura 2000 networking programme.
2
This new word ?amp? is created by our programs while
extracting body text from background documents. The
HTML code ?&amp;? which represents the symbol?&? is
converted into this word.
Keyword Ranking scores with PPR (KRS +
PPR): This is same as PPR except that context
words are initialized with krs.
Sense Ranking scores with PPR (SRS + PPR):
Edges connecting words and their synsets are as-
signed weights equal to srs. The initialization of
word nodes is same as in PPR.
KRS + SRS + PPR: Word nodes are initialized
with krs and edges are assigned weights equal to
srs.
In addition to the above methods of unsuper-
vised graph initialization, we also initialized the
graph in a semi-supervised manner. WordNet (ver-
sion 1.7 and above) have a field tag cnt for each
synset (in the file index.sense) which represents
the number of times the synset is tagged in vari-
ous semantic concordance texts. We used this in-
formation, concordance score (cs) of each synset,
with the above methods of graph initialization as
described below.
Concordance scores with PPR (CS + PPR): The
graph initialization is similar to PPR initialization
additionally with concordance score of synsets on
the edges joining words and their synsets.
CS + KRS + PPR: The initialization graph of
KRS + PPR is further initialized by assigning con-
cordance scores to the edges connecting words and
their synsets.
CS + SRS + PPR: Edges connecting words and
their synsets are assigned weights equal to sum of
the concordance scores and sense ranking scores
i.e. cs + srs. The initialization of word nodes is
same as in PPR.
CS + KRS + SRS + PPR: Word nodes are ini-
tialized with krs and edges are assigned weights
equal to cs + srs.
PageRank was applied to all the above graphs to
disambiguate a target word.
4.2 Experimental details of PageRank
Tool: We used UKB tool
3
(Agirre and Soroa,
2009) which provides an implementation of per-
sonalized PageRank. We modified it to incorpo-
rate our methods of graph initialization. The LKB
used in our experiments is WordNet3.0 + Gloss
which is provided in the tool. More details of the
tools used can be found in the Appendix.
Normalizations: Sense ranking scores (srs) and
keyword ranking scores (krs) have diverse ranges.
We found srs generally in the range between 0 to
3
http://ixa2.si.ehu.es/ukb/
389
Precision Recall
Unsupervised Graph Initialization
PPR 37.3 36.8
KRS + PPR 38.1 37.6
SRS + PPR 48.4 47.8
KRS + SRS + PPR 48.0 47.4
Semi-supervised Graph Initialization
CS + PPR 50.2 49.6
CS + KRS + PPR 50.1 49.5
* CS + SRS + PPR 53.4 52.8
CS + KRS + SRS + PPR 53.6 52.9
Others
1
st
sense 50.5 50.5
PSH 49.8 43.2
Table 1: Evaluation results on English test data of SemEval-2010 Task-17. * represents the system which
we submitted to SemEval and is ranked 3rd in public evaluation.
1 and krs in the range 0 to 0.02. Since these scores
are used to assign initial weights in the graph,
these ranges are scaled to fall in a common range
of [0, 100]. Using any other scaling method should
not effect the performance much since PageRank
(and UKB tool) has its own internal mechanisms
to normalize the weights.
5 Evaluation Results
Test data released for this task is disambiguated
using IIITH1 and IIITH2 systems. As described
in Section 2, IIITH1 and IIITH2 systems differ in
the way the sense ranking scores are computed.
Here we project only the results of IIITH1 since
IIITH1 performed slightly better than IIITH2 in all
the above settings. Results of 1
st
sense system pro-
vided by the organizers which assigns first sense
computed from the annotations in hand-labeled
corpora is also presented. Additionally, we also
present the results of Predominant Sense Heuristic
(PSH) which assigns every word w with the sense
ws
j
(ws
j
? senses(w)) which has the highest
value of srs(ws
j
) computed in Section 2 similar
to (McCarthy et al, 2004).
Table 1 presents the evaluation results. We used
TreeTagger
4
to Part of Speech tag the test data.
POS information was used to discard irrelevant
senses. Due to POS tagging errors, our precision
values were not equal to recall values. In the com-
petition, we submitted IIITH1 and IIITH2 systems
with CS + SRS + PPR graph initialization. IIITH1
4
http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
and IIIH2 gave performances of 53.4 % and 52.2
% precision respectively. In our later experiments,
we found CS + KRS + SRS + PPR has given the
best performance of 53.6 % precision.
From the results, it can be seen when srs in-
formation is incorporated in the graph, precision
improved by 11.1% compared to PPR in unsuper-
vised graph initialization and by 3.19% compared
to CS + PPR in semi-supervised graph initializa-
tion. Also little improvements are seen when krs
information is added. This shows that domain
specific information like sense ranking scores and
keyword ranking scores play a major role in do-
main specific WSD.
The difference between the results in unsu-
pervised and semi-supervised graph initializations
may be attributed to the additional information the
semi-supervised graph is having i.e. the sense dis-
tribution knowledge of non-domain specific words
(common words).
6 Conclusion
This paper proposes a method for domain specific
WSD. Our method is based on a graph-based al-
gorithm (Personalized Page Rank) which is mod-
ified to include information representing the do-
main (sense ranking and key word ranking scores).
Experiments show that exploiting this domain spe-
cific information within the graph based methods
produces better results than when this information
is used individually.
390
Acknowledgements
The authors are grateful to Ted Pedersen for his
helpful advice on the WordNet Similarity Pack-
age. We also thank Rajeev Sangal for supporting
the authors Siva Reddy and Abhilash Inumella.
References
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
EACL ?09: Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 33?41, Morristown, NJ,
USA. Association for Computational Linguistics.
Paul Buitelaar, Bernardo Magnini, Carlo Strapparava,
and Piek Vossen. 2006. Domain-specific wsd. In
Word Sense Disambiguation. Algorithms and Appli-
cations, Editors: Eneko Agirre and Philip Edmonds.
Springer.
Scott Cotton, Phil Edmonds, Adam Kilgarriff, and
Martha Palmer. 2001. Senseval-2. http://www.
sle.sharp.co.uk/senseval2.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac,
a very large web-derived corpus of english. In
Proceed-ings of the WAC4 Workshop at LREC 2008,
Marrakesh, Morocco.
Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and Avi-
nesh PVS. 2010. A corpus factory for many lan-
guages. In LREC 2010, Malta.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423?430, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
Rada Mihalcea and Phil Edmonds, editors. 2004.
Proceedings Senseval-3 3rd International Workshop
on Evaluating Word Sense Disambiguation Systems.
ACL, Barcelona, Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kauf-
man.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In HLT-NAACL ?04: Demon-
stration Papers at HLT-NAACL 2004 on XX, pages
38?41, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the workshop on Comparing corpora, pages 1?
6, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In ICSC ?07: Pro-
ceedings of the International Conference on Seman-
tic Computing, pages 363?369, Washington, DC,
USA. IEEE Computer Society.
Appendix
Domain Specific Thesaurus, Sense Ranking
Scores and Keyword Ranking Scores are accessi-
ble at
http://web.iiit.ac.in/
?
gvsreddy/
SemEval2010/
Tools Used:
? UKB is used with options ?ppr ?dict weight. Dictio-
nary files which UKB uses are automatically generated
using sense ranking scores srs.
? Background document words are canonicalized using
KSTEM, a morphological analyzer
? The Stanford Parser is used to parse background docu-
ments to build thesaurus
? Test data is part of speech tagged using TreeTagger.
391
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 557?564,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DSS: Text Similarity Using Lexical Alignments of Form, Distributional
Semantics and Grammatical Relations
Diana McCarthy
Saarland University?
diana@dianamccarthy.co.uk
Spandana Gella
University of Malta
spandanagella@gmail.com
Siva Reddy
Lexical Computing Ltd.
siva@sivareddy.in
Abstract
In this paper we present our systems for the
STS task. Our systems are all based on a
simple process of identifying the components
that correspond between two sentences. Cur-
rently we use words (that is word forms), lem-
mas, distributional similar words and gram-
matical relations identified with a dependency
parser. We submitted three systems. All sys-
tems only use open class words. Our first sys-
tem (alignheuristic) tries to obtain a map-
ping between every open class token using all
the above sources of information. Our second
system (wordsim) uses a different algorithm
and unlike alignheuristic, it does not use
the dependency information. The third sys-
tem (average) simply takes the average of
the scores for each item from the other two
systems to take advantage of the merits of
both systems. For this reason we only pro-
vide a brief description of that. The results
are promising, with Pearson?s coefficients on
each individual dataset ranging from .3765
to .7761 for our relatively simple heuristics
based systems that do not require training on
different datasets. We provide some analy-
sis of the results and also provide results for
our data using Spearman?s, which as a non-
parametric measure which we argue is better
able to reflect the merits of the different sys-
tems (average is ranked between the others).
1 Introduction
Our motivation for the systems entered in the STS
task (Agirre et al, 2012) was to model the contribu-
? The first author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
tion of each linguistic component of both sentences
to the similarity of the texts by finding an align-
ment. Ultimately such a system could be exploited
for ranking candidate paraphrases of a chunk of text
of any length. We envisage a system as outlined in
the future work section. The systems reported are
simple baselines to such a system. We have two
main systems (alignheuristic and wordsim) and
also a system which simply uses the average score
for each item from the two main systems (average).
In our systems we:
? only deal with open class words as to-
kens i.e. nouns, verbs, adjectives, adverbs.
alignheuristic and average also use num-
bers
? assume that tokens have a 1:1 mapping
? match:
? word forms
? lemmas
? distributionally similar lemmas
? (alignheuristic and average only) ar-
gument or head in a matched grammatical
relation with a word that already has a lex-
ical mapping
? score the sentence pair based on the size of the
overlap. Different formulations of the score are
used by our methods
The paper is structured as follows. In the next
section we make a brief mention of related work
though of course there will be more pertinent related
work presented and published at SemEval 2012. In
section 3 we give a detailed account of the systems
557
and in section 4 we provide the results obtained on
the training data on developing our systems. In sec-
tion 5 we present the results on the test data, along
with a little analysis using the gold standard data. In
section 6 we conclude our findings and discuss our
ideas for future work.
2 Related Work
Semantic textual similarity relates to textual entail-
ment (Dagan et al, 2005), lexical substitution (Mc-
Carthy and Navigli, 2009) and paraphrasing (Hirst,
2003). The key issue for semantic textual similarity
is that the task is to determine similarity, where sim-
ilarity is cast as meaning equivalence. 1 In textual
entailment the relation under question is the more
specific relation of entailment, where the meaning
of one sentence is entailed by another and a sys-
tem needs to determine the direction of the entail-
ment. Lexical substitution relates to semantic tex-
tual similarity though the task involves a lemma in
the context of a sentence, candidate substitutes are
not provided, and the relation at question in the task
is one of substitutability. 2 Paraphrase recognition
is a highly related task, for example using compa-
rable corpora (Barzilay and Elhadad, 2003), and it
is likely that semantic textual similarity measures
might be useful for ranking candidates in paraphrase
acquisition.
In addition to various works related to textual
entailment, lexical substitution and paraphrasing,
there has been some prior work explicitly on se-
mantic text similarity. Semantic textual similarity
has been explored in various works. Mihalcea et al
(2006) extend earlier work on word similarity us-
ing various WordNet similarity measures (Patward-
han et al, 2003) and a couple of corpus-based dis-
tributional measures: PMI-IR (Turney, 2002) and
LSA (Berry, 1992). They use a measure which
takes a summation over all tokens in both sen-
tences. For each token they find the maximum sim-
ilarity (WordNet or distributional) weighted by the
inverse document frequency of that word. The dis-
1See the guidelines given to the annotators at
http://www.cs.columbia.edu/?weiwei/workshop/
instructions.pdf
2This is more or less semantic equivalence since the an-
notators were instructed to focus on meaning http://www.
dianamccarthy.co.uk/files/instructions.pdf.
tributional similarity measures perform at a simi-
lar level to the knowledge-based measures that use
WordNet. Mohler and Mihalcea (2009) adapt this
work for automatic short answer grading, that is
matching a candidate answer to one supplied by
the tutor. Mohler et al (2011) take this applica-
tion forward, combining lexical semantic similarity
measures with a graph-alignment which considers
dependency graphs using the Stanford dependency
parser (de Marneffe et al, 2006) in terms of lexical,
semantic and syntactic features. A score is then pro-
vided for each node in the graph. The features are
combined using machine learning.
The systems we propose likewise use lexical sim-
ilarity and dependency relations, but in a simple
heuristic formulation without a man-made thesaurus
such as WordNet and without machine learning.
3 Systems
We lemmatize and part-of-speech tag the data using
TreeTagger (Schmid, 1994). We process the tagged
data with default settings of the Malt Parser (Nivre
et al, 2007) to dependency parse the data. All sys-
tems make use of a distributional thesaurus which
lists distributionally similar lemmas (?neighbours?)
for a given lemma. This is a thesaurus constructed
using log-dice (Rychly?, 2008) and UkWaC (Fer-
raresi et al, 2008). 3 Note that we use only the
top 20 neighbours for any word in all the methods
described below. We have not experimented with
varying this threshold.
In the following descriptions, we refer to our sen-
tences as s1 and s2 and these open classed tokens
within those sentences as ti ? s1 and t j ? s2 where
each token in either sentence is represented by a
word (w), lemma (l), part-of-speech (p) and gram-
matical relation (gr), identified by the Malt parser,
to its dependency head at a given position (hp) in
the same sentence.
3.1 alignheuristic
This method uses nouns, verbs, adjectives, adverbs
and numbers. The algorithm aligns words (w), or
lemmas (l) from left to right from s1 to s2 and vice
3This is the ukWaC distributional thesaurus avail-
able in Sketch Engine (Kilgarriff et al, 2004) at
http://the.sketchengine.co.uk/bonito/run.cgi/
first\_form?corpname=preloaded/ukwac2
558
versa (wmtch). If there is no alignment for words or
lemmas then it does the same matching process (s1
given s2 and vice versa) for distributionally similar
neighbours using the distributional thesaurus men-
tioned above (tmtch) and also another matching pro-
cess looking for a corresponding grammatical rela-
tion identified with the Malt parser in the other sen-
tence where the head (or argument) already has a
match in both sentences (rmtch).
A fuller and more formal description of the algo-
rithm follows:
1. retain nouns, verbs (not be), adjectives, adverbs
and numbers in both sentences s1 and s2.
2. wmtch:
(a) look for word matches
? wi ? s1 to w j ? s2, left to right i.e. the
first matching w j ? s2 is selected as a
match for wi.
? w j ? s2 to wi ? s1, left to right i.e. the
first matching wi ? s1 is selected as a
match for w j
(b) and then lemma matches for any ti ? s1
and t j ? s1 not matched in steps 2a
? li ? s1 to l j ? s2 , left to right i.e. the
first matching l j ? s2 is selected as a
match for li.
? l j ? s2 to li ? s1 , left to right i.e. the
first matching li ? s1 is selected as a
match for l j
3. using only ti ? s1 and t j ? s2 not matched in
the above steps:
? tmtch: match lemma and PoS (l + p) with
the distributional thesaurus against the top
20 most similar lemma-pos entries. That
is:
(a) For l + pi ? s1, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + p j ? s2, left to
right i.e. the first matching l + p j ? s2
to any of the most similar words to
l + pi according to the thesaurus is se-
lected as a match for l + pi ? s1.
(b) For l + p j ? s2, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + pi ? s1, left to
right
? rmtch: match the tokens, if not already
matched at step 2 above, by looking for
a head or argument relation with a token
that has been matched at step 2 to a token
with the inverse relation. That is:
i For ti ? s1, if not already matched at
step 2 above, if hpi ? s1 (the pointer
to the head, i.e. parent, of ti) refers to
a token tx ? s1 which has wmtch at tk
in s2, and there exists a tq ? s2 with
grq = gri and hpq = tk, then match ti
with tq
ii For ti ? s1 , if not already matched
at step 2 or the preceding step (rmtch
3i) and if there exists another tx ? s1
with a hpx which refers to ti (i.e. ti is
the parent, or head, of tx) with a match
between tx and tk ? s2 from step 2, 4
and where tk has grk = grx with hpk
which refers to tq in s2, then match ti
with tq 5
iii we do likewise in reverse for s2 to s1
and then check all matches are recip-
rocated with the same 1:1 mapping
Finally, we calculate the score sim(s1, s2):
|wmtch| + (wt ? |tmtch + rmtch|)
|s1| + |s2|
? 5 (1)
where wt is a weight of 0.5 or less (see below).
The sim score gives a score of 5 where two
sentences have the same open class tokens, since
matches in both directions are included and the de-
nominator is the number of open class tokens in both
sentences. The score is 0 if there are no matches.
The thesaurus and grammatical relation matches are
counted equally and are considered less important
4In the example illustrated in figure 1 and discussed below,
ti could be rose in the upper sentence (s1) and Nasdaq would be
tx and tk.
5So in our example, from figure 1, ti (rose) is matched with tq
(climb) as climb is the counterpart head to rose for the matched
arguments (Nasdaq).
559
NasdaqThe tech?loaded composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.
thesaurus
malt
malt
points, or 1.2 percent, to 1,615.02.The technology?laced climbed 19.11 Index <.IXIC>CompositeNasdaq
Figure 1: Example of matching with alignheuristic
for the score as the exact matches. We set wt to 0.4
for the official run, though we could improve perfor-
mance by perhaps setting a bit lower as shown below
in section 4.1.
Figure 1 shows an example pair of sentences from
the training data in MSRpar. The solid lines show
alignments between words. Composite and compos-
ite are not matched because the lemmatizer assumes
that the former is a proper noun and does not decap-
italise; we could decapitalise all proper nouns. The
dotted arcs show parallel dependency relations in the
sentences where the argument (Nasdaq) is matched
by wmtch. The rmtch process therefore assumes the
corresponding heads (rise and climb) align. In addi-
tion, tmtch finds a match from climb to rise as rise is
in the top 20 most similar words (neighbours) in the
distributional thesaurus. climb is not however in the
top 20 for rise and so a link is not found in the other
direction. We have not yet experimented with val-
idating the thesaurus and grammatical relation pro-
cesses together, though that would be worthwhile in
future.
3.2 wordsim
In this method, we first choose the shortest sentence
based on the number of open words. Let s1 and s2
be the shortest and longest sentences respectively.
For every lemma li ? s1, we find its best matching
lemma l j ? s2 using the following heuristics and
assigning an alignment score as follows:
1. if li=l j, then the alignment score of li
(algnscr(li)) is one.
2. else li ? s1 is matched with a lemma l j ? s2
with which it has the highest distributional sim-
ilarity. 6 The alignment score, algnscr(li) is
the distributional similarity between li and l j
(which is always less than one).
The final sentence similarity score between the
pair s1 and s2 is computed as
sim(s1, s2) =
?
li?s1 algnscr(li)
|s1|
(2)
3.3 average
This system simple uses the average score for each
item from alignheuristic and wordsim. This is
simply so we can make a compromise between the
merits of the two systems.
4 Experiments on the Training Data
Table 1 displays the results on training data for the
system settings as they were for the final test run. We
conducted further analysis for the alignheuristic
system and that is reported in the following subsec-
tion. We can see that while the alignheuristic
is better on the MSRpar and SMT-eur datasets, the
wordsim outperforms it on the MSRvid dataset,
which contains shorter, simpler sentences. One rea-
son might be that the wordsim credits alignments
in one direction only and this works well when sen-
tences are of a similar length but can loose out on the
longer paraphrase and SMT data. This behaviour is
6Provided this is within the top 20 most similar words in the
thesaurus.
560
MSRpar MSRvid SMT-eur
alignheuristic 0.6015 0.6994 0.5222
wordsim 0.4134 0.7658 0.4479
average 0.5337 0.7535 0.5061
Table 1: Results on training data
confirmed by the results on the test data reported be-
low in section 5, though we cannot rule out that other
factors play a part.
4.1 alignheuristic
We developed the system on the training data for the
purpose of finding bugs, and setting the weight in
equation 1. During development we found the opti-
mal weight for wt to be 0.4. 7 Unfortunately we did
not leave ourselves sufficient time to set the weights
after resolving the bugs. In table 1 we report the
results on the training data for the system that we
uploaded, however in table 2 we report more recent
results for the final system but with different values
of wt. From table 2 it seems that results may have
been improved if we had determined the final value
of wt after debugging our system fully, however this
depends on the type of data as 0.4 was optimal for
the datasets with more complex sentences (MSRpar
and SMT-eur).
In table 3, we report results for alignheuristic
with and without the distributional similarity
thesaurus (tmtch) and the dependency relations
(rmtch). In table 4 we show the total number of to-
ken alignments made by the different matching pro-
cesses on the training data. We see, from table 4
that the MSRvid data relies on the thesaurus and de-
pendency relations to a far greater extent than the
other datasets, presumably because of the shorter
sentences where many have a few contrasting words
in similar syntactic relations, for example s1 Some-
one is drawing. s2 Someone is dancing. 8 We see
from table 3 that the use of these matching processes
is less accurate for MSRvid and that while tmtch
improves performance, rmtch seems to degrade per-
formance. From table 2 it would seem that on this
type of data we would get the best results by reduc-
7We have not yet attempted setting the weight on alignment
by relation and alignment by distributional similarity separately.
8Note that the alignheuristic algorithm is symmetrical
with respect to s1 and s2 so it does not matter which is which.
wt MSRpar MSRvid SMT-eur
0.5 0.5998 0.6518 0.5290
0.4 0.6015 0.6994 0.5222
0.3 0.6020 0.7352 0.5146
0.2 0.6016 0.7577 0.5059
0.1 0.6003 0.7673 0.4964
0 0.5981 0.7661 0.4863
Table 2: Results for the alignheuristic algorithm on
the training data: varying wt
MSR MSR SMT
par vid -eur
-tmtch+rmtch 0.6008 0.7245 0.5129
+tmtch-rmtch 0.5989 0.7699 0.4959
-tmtch-rmtch 0.5981 0.7661 0.4863
+tmtch+rmtch 0.6015 0.6994 0.5222
Table 3: Results for the alignheuristic algorithm on
the training data: with and without tmtch and rmtch
ing wt to a minimum, and perhaps it would make
sense to drop rmtch. Meanwhile, on the longer more
complex MSRpar and SMT-eur data, the less precise
rmtch and tmtch are used less frequently (relative to
the wmtch) but can be seen from table 3 to improve
performance on both training datasets. Moreover, as
we mention above, from table 2 the parameter set-
ting of 0.4 used for our final test run was optimal for
these datasets.
MSRpar MSRvid SMT-eur
wmtch 10960 2349 12155
tmtch 378 1221 964
rmtch 1008 965 1755
Table 4: Number of token alignments for the different
matching processes
561
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news
alignheuristic .5253 (60) .5735 (24) .7123 (53) .4781 (25) .6984 (7) .4177 (38)
average .5490 (58) .5020 (48) .7645 (41) .4875 (16) .6677(14) .4324 (31)
wordsim .5130 (61) .3765 (75) .7761 (34) .4161 (58) .5728 (59) .3964 (48)
Table 5: Official results: Rank (out of 89) is shown in brackets
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news average ?
alignheuristic 0.5216 0.5539 0.7125 0.5404 0.6928 0.3655 0.5645
average 0.5087 0.4818 0.7653 0.5415 0.6302 0.3835 0.5518
wordsim 0.4279 0.3608 0.7799 0.4487 0.4976 0.3388 0.4756
Table 7: Spearman?s ? for the 5 datasets, ?all? and the average coefficient across the datasets
run mean Allnrm
alignheuristic 0.6030 (21) 0.7962 (42)
average 0.5943 (26) 0.8047 (35)
wordsim 0.5287 (55) 0.7895 (49)
Table 6: Official results: Further metrics suggested in dis-
cussion. Rank (out of 89) is shown in brackets
5 Results
Table 5 provides the official results for our submitted
systems, along with the rank on each dataset. The re-
sults in the ?all? column which combine all datasets
together are at odds with our intuitions. Our sys-
tems were ranked higher in every individual dataset
compared to the ?all? ranking, with the exception of
wordsim and the MSRpar dataset. This ?all? met-
ric is anticipated to impact systems that have dif-
ferent settings for different types of data however
we did not train our systems to run differently on
different data. We used exactly the same parame-
ter settings for each system on every dataset. We
believe Pearson?s measure has a significant impact
on results because it is a parametric measure and
as such the shape of the distribution (the distribu-
tion of scores) is assumed to be normal. We present
the results in table 6 from new metrics proposed by
participants during the post-results discussion: All-
nrm (normalised) and mean (this score is weighted
by the number of sentence pairs in each dataset). 9
The Allnrm score, proposed by a participant during
the discussion phase to try and combat issues with
9Post-results discussion is archived at http://groups.
google.com/group/sts-semeval/topics
the ?all? score, also does not accord with our intu-
ition given the ranks of our systems on the individ-
ual datasets. The mean score, proposed by another
participant, however does reflect performance on the
individual datasets. Our average system is ranked
between alignheuristic and wordsim which is
in line with our expectations given results on the
training data and individual datasets.
As mentioned above, an issue with the use of
Pearson?s correlation coefficient is that it is para-
metric and assumes that the scores are normally dis-
tributed. We calculated Spearman?s ? which is the
non-parametric equivalent of Pearson?s and uses the
ranks of the scores, rather than the actual scores. 10
The results are presented in table 7. We cannot cal-
culate the results for other systems, and therefore the
ranks for our system, since we do not have the other
system?s outputs but we do see that the relative per-
formance of our system on ?all? is more in line with
our expectations: average, which simply uses the
average of the other two systems for each item, is
usually ranked between the other two systems, de-
pending on the dataset. Spearman?s ?all? gives a sim-
ilar ranking of our three systems as the mean score.
We also show average ?. This is a macro average
of the Spearman?s value for the 5 datasets without
weighting by the number of sentence pairs. 11
10Note that Spearman?s ? is often a little lower than Pear-
son?s (Mitchell and Lapata, 2008)
11We do recognise the difficulty in determining metrics on a
new pilot study. The task organisers are making every effort to
make it clear that this enterprise is a pilot, not a competition and
that they welcome feedback.
562
6 Conclusions
The systems were developed in less than a week
including the time with the test data. There are
many trivial fixes that may improve the basic algo-
rithm, such as decapitalising proper nouns. There
are many things we would like to try, such as val-
idating the dependency matching process with the
thesaurus matching. We would like to match larger
units rather than tokens, with preferences towards
the longer matching blocks. In parallel to the devel-
opment of alignheuristic, we developed a sys-
tem which measures the similarity between a node
in the dependency tree of s1 and a node in the de-
pendency tree of s2 as the sum of the lexical sim-
ilarity of the lemmas at the nodes and the simi-
larity of its children nodes. We did not submit a
run for the system as it did not perform as well as
alignheuristic, probably because the score fo-
cused on structure too much. We hope to spend time
developing this system in future.
Ultimately, we envisage a system that:
? can have non 1:1 mappings between tokens, i.e.
a phrase may be paraphrased as a word for ex-
ample blow up may be paraphrased as explode
? can map between sequences of non-contiguous
words for example the words in the phrase blow
up may be separated but mapped to the word
explode as in the bomb exploded ? They blew
it up
? has categories (such as equivalence, entailment,
negation, omission . . . ) associated with each
mapping. Speculation, modality and sentiment
should be indicated on any relevant chunk so
that differences can be detected between candi-
date and referent
? scores the candidate using a function of the
scores of the mapped units (as in the systems
described above) but alters the score to reflect
the category as well as the source of the map-
ping, for example entailment without equiva-
lence should reduce the similarity score, in con-
trast to equivalence, and negation should re-
duce this still further
Crucially we would welcome a task where anno-
tators would also provide a score on sub chunks of
the sentences (or arbitrary blocks of text) that align
along with a category for the mapping (equivalence,
entailment, negation etc..). This would allow us to
look under the hood at the text similarity task and de-
termine the reason behind the similarity judgments.
7 Acknowledgements
We thank the task organisers for their efforts in or-
ganising the task and their readiness to take on board
discussions on this as a pilot. We also thank the
SemEval-2012 co-ordinators.
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Barzilay, R. and Elhadad, N. (2003). Sentence align-
ment for monolingual comparable corpora. In
Collins, M. and Steedman, M., editors, Proceed-
ings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 25?
32.
Berry, M. (1992). Large scale singular value compu-
tations. International Journal of Supercomputer
Applications, 6(1):13?49.
Dagan, I., Glickman, O., and Magnini, B. (2005).
The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL First Chal-
lenge Workshop, pages 1?8, Southampton, UK.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed dependency
parses from phrase structure parses. In To appear
at LREC-06.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the Sixth International
Conference on Language Resources and Evalua-
tion (LREC 2008), Marrakech, Morocco.
Hirst, G. (2003). Paraphrasing paraphrased. In-
vited talk at the Second International Workshop
563
on Paraphrasing, 41st Annual Meeting of the As-
sociation for Computational Linguistics.
Kilgarriff, A., Rychly?, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of Eu-
ralex, pages 105?116, Lorient, France. Reprinted
in Patrick Hanks (ed.). 2007. Lexicology: Critical
concepts in Linguistics. London: Routledge.
McCarthy, D. and Navigli, R. (2009). The English
lexical substitution task. Language Resources and
Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and
Beyond, 43(2):139?159.
Mihalcea, R., Corley, C., and Strapparava, C.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the American Association for Artificial Intelli-
gence (AAAI 2006), Boston, MA.
Mitchell, J. and Lapata, M. (2008). Vector-based
models of semantic composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Mohler, M., Bunescu, R., and Mihalcea, R. (2011).
Learning to grade short answer questions us-
ing semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 752?762, Portland, Oregon, USA. As-
sociation for Computational Linguistics.
Mohler, M. and Mihalcea, R. (2009). Text-to-text se-
mantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009),
pages 567?575, Athens, Greece. Association for
Computational Linguistics.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,
G., Ku?bler, S., Marinov, S., and Marsi, E. (2007).
Maltparser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95?135.
Patwardhan, S., Banerjee, S., and Pedersen, T.
(2003). Using measures of semantic relatedness
for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelli-
gent Text Processing and Computational Linguis-
tics (CICLing 2003), Mexico City.
Rychly?, P. (2008). A lexicographer-friendly associ-
ation score. In Proceedings of 2nd Workshop on
Recent Advances in Slavonic Natural Languages
Processing, RASLAN 2008, Brno.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49, Manchester,
UK.
Turney, P. D. (2002). Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR, cs.LG/0212033.
564
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 54?60,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exemplar-based Word-Space Model for Compositionality Detection: Shared
task system description
Siva Reddy
University of York, UK
siva@cs.york.ac.uk
Suresh Manandhar
University of York, UK
suresh@cs.york.ac.uk
Diana McCarthy
Lexical Computing Ltd, UK
diana@dianamccarthy.co.uk
Spandana Gella
University of York, UK
spandana@cs.york.ac.uk
Abstract
In this paper, we highlight the problems of
polysemy in word space models of compo-
sitionality detection. Most models represent
each word as a single prototype-based vec-
tor without addressing polysemy. We propose
an exemplar-based model which is designed
to handle polysemy. This model is tested for
compositionality detection and it is found to
outperform existing prototype-based models.
We have participated in the shared task (Bie-
mann and Giesbrecht, 2011) and our best per-
forming exemplar-model is ranked first in two
types of evaluations and second in two other
evaluations.
1 Introduction
In the field of computational semantics, to represent
the meaning of a compound word, two mechanisms
are commonly used. One is based on the distribu-
tional hypothesis (Harris, 1954) and the other is on
the principle of semantic compositionality (Partee,
1995, p. 313).
The distributional hypothesis (DH) states that
words that occur in similar contexts tend to have
similar meanings. Using this hypothesis, distribu-
tional models like the Word-space model (WSM,
Sahlgren, 2006) represent a target word?s meaning
as a context vector (location in space). The simi-
larity between two meanings is the closeness (prox-
imity) between the vectors. The context vector of a
target word is built from its distributional behaviour
observed in a corpus. Similarly, the context vector of
a compound word can be built by treating the com-
pound as a single word. We refer to such a vector as
a DH-based vector.
The other mechanism is based on the principle of
semantic compositionality (PSC) which states that
the meaning of a compound word is a function of,
and only of, the meaning of its parts and the way
in which the parts are combined. If the meaning of
a part is represented in a WSM using the distribu-
tional hypothesis, then the principle can be applied
to compose the distributional behaviour of a com-
pound word from its parts without actually using the
corpus instances of the compound. We refer to this
as a PSC-based vector. So a PSC-based is composed
of component DH-based vectors.
Both of these two mechanisms are capable of de-
termining the meaning vector of a compound word.
For a given compound, if a DH-based vector and
a PSC-based vector of the compound are projected
into an identical space, one would expect the vec-
tors to occupy the same location i.e. both the vectors
should be nearly the same. However the principle
of semantic compositionality does not hold for non-
compositional compounds, which is actually what
the existing WSMs of compositionality detection ex-
ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001). The DH-based and
PSC-based vectors are expected to have high simi-
larity when a compound is compositional and low
similarity for non-compositional compounds.
Most methods in WSM (Turney and Pantel, 2010)
represent a word as a single context vector built from
merging all its corpus instances. Such a representa-
tion is called the prototype-based modelling (Mur-
phy, 2002). These prototype-based vectors do not
54
distinguish the instances according to the senses of
a target word. Since most compounds are less am-
biguous than single words, there is less need for dis-
tinguishing instances in a DH-based prototype vec-
tor of a compound and we do not address that here
but leave ambiguity of compounds for future work.
However the constituent words of the compound are
more ambiguous. When DH-based vectors of the
constituent words are used for composing the PSC-
based vector of the compound, the resulting vec-
tor may contain instances, and therefore contexts,
that are not relevant for the given compound. These
noisy contexts effect the similarity between the PSC-
based vector and the DH-based vector of the com-
pound. Basing compositionality judgements on a
such a noisy similarity value is no longer reliable.
In this paper, we address this problem of pol-
ysemy of constituent words of a compound using
an exemplar-based modelling (Smith and Medin,
1981). In exemplar-based modelling of WSM (Erk
and Pado?, 2010), each word is represented by all its
corpus instances (exemplars) without merging them
into a single vector. Depending upon the purpose,
only relevant exemplars of the target word are acti-
vated and then these are merged to form a refined
prototype-vector which is less-noisy compared to
the original prototype-vector. Exemplar-based mod-
els are more powerful than prototype-based ones be-
cause they retain specific instance information.
We have evaluated our models on the validation
data released in the shared task (Biemann and Gies-
brecht, 2011). Based on the validation results, we
have chosen three systems for public evaluation and
participated in the shared task (Biemann and Gies-
brecht, 2011).
2 Word Space Model
In this section, construction of WSM for all our ex-
periments is described. We use Sketch Engine1 (Kil-
garriff et al, 2004) to retrieve all the exemplars for
a target word or a pattern using corpus query lan-
guage. Let w1 w2 be a compound word with con-
stituent words w1 and w2. Ew denotes the set of
exemplars of w. Vw is the prototype vector of the
word w, which is built by merging all the exemplars
in Ew
1Sketch Engine http://www.sketchengine.co.uk
For the purposes of producing a PSC-based vector
for a compound, a vector of a constituent word is
built using only the exemplars which do not contain
the compound. Note that the vectors are sensitive
to a compound?s word-order since the exemplars of
w1 w2 are not the same as w2 w1.
We use other WSM settings following Mitchell
and Lapata (2008). The dimensions of the WSM
are the top 2000 content words in the given corpus
(along with their coarse-grained part-of-speech in-
formation). Cosine similarity (sim) is used to mea-
sure the similarity between two vectors. Values at
the specific positions in the vector representing con-
text words are set to the ratio of the probability of
the context word given the target word to the overall
probability of the context word. The context window
of a target word?s exemplar is the whole sentence of
the target word excluding the target word. Our lan-
guage of interest is English. We use the ukWaC cor-
pus (Ferraresi et al, 2008) for producing out WSMs.
3 Related Work
As described in Section 1, most WSM models for
compositionality detection measure the similarity
between the true distributional vector Vw1w2 of the
compound and the composed vector Vw1?w2 , where
? denotes a compositionality function. If the simi-
larity is high, the compound is treated as composi-
tional or else non-compositional.
Giesbrecht (2009); Katz and Giesbrecht (2006);
Schone and Jurafsky (2001) obtained the compo-
sitionality vector of w1 w2 using vector addition
Vw1?w2 = aVw1 + bVw2 . In this approach, if
sim(Vw1?w2 , Vw1w2) > ?, the compound is clas-
sified as compositional, where ? is a threshold for
deciding compositionality. Global values of a and b
were chosen by optimizing the performance on the
development set. It was found that no single thresh-
old value ? held for all compounds. Changing the
threshold alters performance arbitrarily. This might
be due to the polysemous nature of the constituent
words which makes the composed vector Vw1?w2
filled with noisy contexts and thus making the judge-
ment unpredictable.
In the above model, if a=0 and b=1, the result-
ing model is similar to that of Baldwin et al (2003).
They also observe similar behaviour of the thresh-
55
old ?. We try to address this problem by addressing
the polysemy in WSMs using exemplar-based mod-
elling.
The above models use a simple addition based
compositionality function. Mitchell and Lapata
(2008) observed that a simple multiplication func-
tion modelled compositionality better than addi-
tion. Contrary to that, Guevara (2011) observed
additive models worked well for building composi-
tional vectors. In our work, we try using evidence
from both compositionality functions, simple addi-
tion and simple multiplication.
Bannard et al (2003); McCarthy et al (2003) ob-
served that methods based on distributional similar-
ities between a phrase and its constituent words help
when determining the compositionality behaviour of
phrases. We therefore also use evidence from the
similarities between each constituent word and the
compound.
4 Our Approach: Exemplar-based Model
Our approach works as follows. Firstly, given a
compound w1 w2, we build its DH-based proto-
type vector Vw1w2 from all its exemplars Ew1w2 .
Secondly, we remove irrelevant exemplars in Ew1
and Ew2 of constituent words and build the refined
prototype vectors Vwr1 and Vwr2 of the constituent
words w1 and w2 respectively. These refined vec-
tors are used to compose the PSC-based vectors 2 of
the compound. Related work to ours is (Reisinger
and Mooney, 2010) where exemplars of a word are
first clustered and then prototype vectors are built.
This work does not relate to compositionality but to
measuring semantic similarity of single words. As
such, their clusters are not influenced by other words
whereas in our approach for detecting composition-
ality, the other constituent word plays a major role.
We use the compositionality functions, sim-
ple addition and simple multiplication to build
Vwr1+wr2 and Vwr1?wr2 respectively. Based on
the similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),
sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1?wr2), we
decide if the compound is compositional or non-
compositional. These steps are described in a little
more detail below.
2Note that we use two PSC-based vectors for representing a
compound.
4.1 Building Refined Prototype Vectors
We aim to remove irrelevant exemplars of one con-
stituent word with the help of the other constituent
word?s distributional behaviour. For example, let
us take the compound traffic light. Light occurs
in many contexts such as quantum theory, optics,
lamps and spiritual theory. In ukWaC, light has
316,126 instances. Not all these exemplars are rel-
evant to compose the PSC-based vector of traffic
light. These irrelevant exemplars increases the se-
mantic differences between traffic light and light and
thus increase the differences between Vtraffic?light
and Vtraffic light. sim(Vlight, Vtraffic light) is found to be
0.27.
Our intuition and motivation for exemplar re-
moval is that it is beneficiary to choose only the
exemplars of light which share similar contexts of
traffic since traffic light should have contexts sim-
ilar to both traffic and light if it is compositional.
We rank each exemplar of light based on common
co-occurrences of traffic and also words which are
distributionally similar to traffic. Co-occurrences of
traffic are the context words which frequently occur
with traffic, e.g. car, road etc. Using these, the
exemplar from a sentence such as ?Cameras capture
cars running red lights . . .? will be ranked higher
than one which does not have contexts related to
traffic. The distributionally similar words to traffic
are the words (like synonyms, antonyms) which are
similar to traffic in that they occur in similar con-
texts, e.g. transport, flow etc. Using these distri-
butionally similar words helps reduce the impact of
data sparseness and helps prioritise contexts of traf-
fic which are semantically related. We use Sketch
Engine to compute the scores of a word observed
in a given corpus. Sketch Engine scores the co-
occurrences (collocations) using logDice motivated
by (Curran, 2003) and distributionally related words
using (Rychly? and Kilgarriff, 2007; Lexical Com-
puting Ltd., 2007). For a given word, both of these
scores are normalised in the range (0,1)
All the exemplars of light are ranked based on
the co-occurrences of these collocations and distri-
butionally related words of traffic using
strafficE ? Elight =
?
c ? E
xEc ? y
traffic
c (1)
where strafficE ? Elight stands for the relevance score of the
56
exemplar E w.r.t. traffic, c for context word in the
exemplar E, xEc is the coordinate value (contextual
score) of the context word c in the exemplar E and
ytrafficc is the score of the context word c w.r.t. traffic.
A refined prototype vector of light is then built by
merging the top n exemplars of light
Vlightr =
n?
ei?Etrafficlight ;i=0
ei (2)
where Etrafficlight are the set of exemplars of light
ranked using co-occurrence information from the
other constituent word traffic. n is chosen such that
sim(Vlightr , Vtraffic light) is maximised. This similar-
ity is observed to be greatest using just 2286 (less
than 1%) of the total exemplars of light. After ex-
emplar removal, sim(Vlightr , Vtraffic light) increased to
0.47 from the initial value of 0.27. Though n is cho-
sen by maximising similarity, which is not desirable
for non-compositional compounds, the lack of simi-
larity will give the strongest possible indication that
a compound is not compositional.
4.2 Building Compositional Vectors
We use the compositionality functions, simple ad-
dition and simple multiplication to build composi-
tional vectors Vwr1+wr2 and Vwr1?wr2 . These are as de-
scribed in (Mitchell and Lapata, 2008). In model ad-
dition, Vw1?w2 = aVw1 + bVw2 , all the previous ap-
proaches use static values of a and b. Instead, we use
dynamic weights computed from the participating
vectors using a =
sim(Vw1w2 ,Vw1 )
sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )
and b = 1?a. These weights differ from compound
to compound.
4.3 Compositionality Judgement
To judge if a compound is compositional or non-
compositional, previous approaches (see Section 3)
base their judgement on a single similarity value. As
discussed, we base our judgement based on the col-
lective evidences from all the similarity values using
a linear equation of the form
?(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)
+ a2.sim(Vw1w2 , Vwr2) (3)
+ a3.sim(Vw1w2 , Vwr1+wr2)
+ a4.sim(Vw1w2 , Vwr1?wr2)
Model APD Acc.
Exm-Best 13.09 88.0
Pro-Addn 15.42 76.0
Pro-Mult 17.52 80.0
Pro-Best 15.12 80.0
Table 1: Average Point Difference (APD) and Av-
erage Accuracy (Acc.) of Compositionality Judge-
ments
where the value of ? denotes the compositionality
score. The range of ? is in between 0-100. If ? ?
34, the compound is treated as non-compositional,
34 < ? < 67 as medium compositional and ? ?
67 as highly compositional. The parameters ai?s
are estimated using ordinary least square regression
by training over the training data released in the
shared task (Biemann and Giesbrecht, 2011). For
the three categories ? adjective-noun, verb-object
and subject-verb ? the parameters are estimated sep-
arately.
Note that if a1 = a2 = a4 = 0, the model bases
its judgement only on addition. Similarly if a1 =
a2 = a3 = 0, the model bases its judgement only on
multiplication.
We also experimented with combinations such as
?(Vwr1 , Vw2) and ?(Vw1 , Vwr2) i.e. using refined vec-
tor for one of the constituent word and the unrefined
prototype vector for the other constituent word.
4.4 Selecting the best model
To participate in the shared task, we have selected
the best performing model by evaluating the mod-
els on the validation data released in the shared task
(Biemann and Giesbrecht, 2011). Table 1 displays
the results on the validation data. The average point
difference is calculated by taking the average of the
difference in a model?s score ? and the gold score
annotated by humans, over all compounds. Table 1
also displays the overall accuracy of coarse grained
labels ? low, medium and high.
Best performance for verb(v)-object(o) com-
pounds is found for the combination ?(Vvr , Vor) of
Equation 3. For subject(s)-verb(v) compounds, it is
for ?(Vsr , Vvr) and a3 = a4 = 0. For adjective(j)-
noun(n) compounds, it is ?(Vjr , Vn). We are not
certain of the reason for this difference, perhaps
there may be less ambiguity of words within specific
grammatical relationships or it may be simply due to
57
TotPrd Spearman ? Kendalls ?
Rand-Base 174 0.02 0.02
Exm-Best 169 0.35 0.24
Pro-Best 169 0.33 0.23
Exm 169 0.26 0.18
SharedTaskNextBest 174 0.33 0.23
Table 2: Correlation Scores
the actual compounds in those categories. We leave
analysis of this for future work. We combined the
outputs of these category-specific models to build
the best model Exm-Best.
For comparison, results of standard mod-
els prototype addition (Pro-Addn) and prototype-
multiplication (Pro-Mult) are also displayed in Table
1. Pro-Addn can be represented as ?(Vw1 , Vw2) with
a1 = a2 = a4 = 0. Pro-Mult can be represented as
?(Vw1 , Vw2) with a1 = a2 = a3 = 0. Pro-Best is
the best performing model in prototype-based mod-
elling. It is found to be ?(Vw1 , Vw2). (Note: De-
pending upon the compound type, some of the ai?s
in Pro-Best may be 0).
Overall, exemplar-based modelling excelled in
both the evaluations, average point difference and
coarse-grained label accuracies. The systems Exm-
Best, Pro-Best and Exm ?(Vwr1 , Vwr2) were submit-
ted for the public evaluation in the shared task. All
the model parameters were estimated by regression
on the task?s training data separately for the 3 com-
pound types as described in Section 4.3. We perform
the regression separately for these classes to max-
imise performance. In the future, we will investigate
whether these settings gave us better results on the
test data compared to setting the values the same re-
gardless of the category of compound.
5 Shared Task Results
Table 2 displays Spearman ? and Kendalls ? corre-
lation scores of all the models. TotPrd stands for
the total number of predictions. Rand-Base is the
baseline system which randomly assigns a compo-
sitionality score for a compound. Our model Exm-
Best was the best performing system compared to
all other systems in this evaluation criteria. Shared-
TaskNextBest is the next best performing system
apart from our models. Due to lemmatization er-
rors in the test data, our models could only predict
judgements for 169 out of 174 compounds.
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 32.82 34.57 29.83 32.34
Zero-Base 23.42 24.67 17.03 25.47
Exm-Best 16.51 15.19 15.72 18.6
Pro-Best 16.79 14.62 18.89 18.31
Exm 17.28 15.82 18.18 18.6
SharedTaskBest 16.19 14.93 21.64 14.66
Table 3: Average Point Difference Scores
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 0.297 0.288 0.308 0.30
Zero-Base 0.356 0.288 0.654 0.25
Most-Freq-Base 0.593 0.673 0.346 0.65
Exm-Best 0.576 0.692 0.5 0.475
Pro-Best 0.567 0.731 0.346 0.5
Exm 0.542 0.692 0.346 0.475
SharedTaskBest 0.585 0.654 0.385 0.625
Table 4: Coarse Grained Accuracy
Table 3 displays average point difference scores.
Zero-Base is a baseline system which assigns a score
of 50 to all compounds. SharedTaskBest is the over-
all best performing system. Exm-Best was ranked
second best among all the systems. For ADJ-NN
and V-SUBJ compounds, the best performing sys-
tems in the shared task are Pro-Best and Exm-Best
respectively. Our models did less well on V-OBJ
compounds and we will explore the reasons for this
in future work.
Table 4 displays coarse grained scores. As above,
similar behaviour is observed for coarse grained ac-
curacies. Most-Freq-Base is the baseline system
which assigns the most frequent coarse-grained la-
bel for a compound based on its type (ADJ-NN, V-
SUBJ, V-OBJ) as observed in training data. Most-
Freq-Base outperforms all other systems.
6 Conclusions
In this paper, we examined the effect of polysemy
in word space models for compositionality detec-
tion. We showed exemplar-based WSM is effective
in dealing with polysemy. Also, we use multiple
evidences for compositionality detection rather than
basing our judgement on a single evidence. Over-
all, performance of the Exemplar-based models of
compositionality detection is found to be superior to
prototype-based models.
58
References
Baldwin, T., Bannard, C., Tanaka, T., and Widdows,
D. (2003). An empirical model of multiword ex-
pression decomposability. In Proceedings of the
ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, ac-
quisition and treatment - Volume 18, MWE ?03,
pages 65?72, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Biemann, C. and Giesbrecht, E. (2011). Distri-
butional semantics and compositionality 2011:
Shared task description and results. In Pro-
ceedings of DISCo-2011 in conjunction with ACL
2011.
Curran, J. R. (2003). From distributional to semantic
similarity. Technical report, PhD Thesis, Univer-
sity of Edinburgh.
Erk, K. and Pado?, S. (2010). Exemplar-based mod-
els for word meaning in context. In Proceed-
ings of the ACL 2010 Conference Short Papers,
ACLShort ?10, pages 92?97, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the WAC4 Workshop at
LREC 2008, Marrakesh, Morocco.
Giesbrecht, E. (2009). In search of semantic com-
positionality in vector spaces. In Proceedings
of the 17th International Conference on Concep-
tual Structures: Conceptual Structures: Leverag-
ing Semantic Technologies, ICCS ?09, pages 173?
184, Berlin, Heidelberg. Springer-Verlag.
Guevara, E. R. (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, IWCS ?2011.
Harris, Z. S. (1954). Distributional structure. Word,
10:146?162.
Katz, G. and Giesbrecht, E. (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underly-
ing Properties, MWE ?06, pages 12?19, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of EU-
RALEX.
Lexical Computing Ltd. (2007). Statistics used in
the sketch engine.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, MWE ?03,
pages 73?80, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Murphy, G. L. (2002). The Big Book of Concepts.
The MIT Press.
Partee, B. (1995). Lexical semantics and compo-
sitionality. L. Gleitman and M. Liberman (eds.)
Language, which is Volume 1 of D. Osherson (ed.)
An Invitation to Cognitive Science (2nd Edition),
pages 311?360.
Reisinger, J. and Mooney, R. J. (2010). Multi-
prototype vector-space models of word mean-
ing. In Proceedings of the 11th Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-
2010), pages 109?117.
Rychly?, P. and Kilgarriff, A. (2007). An efficient
algorithm for building a distributional thesaurus
(and other sketch engine developments). In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Ses-
sions, ACL ?07, pages 41?44, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sahlgren, M. (2006). The Word-Space Model: Us-
ing distributional analysis to represent syntag-
59
matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University.
Schone, P. and Jurafsky, D. (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01.
Smith, E. E. and Medin, D. L. (1981). Categories
and concepts / Edward E. Smith and Douglas L.
Medin. Harvard University Press, Cambridge,
Mass. :.
Turney, P. D. and Pantel, P. (2010). From frequency
to meaning: vector space models of semantics. J.
Artif. Int. Res., 37:141?188.
60
