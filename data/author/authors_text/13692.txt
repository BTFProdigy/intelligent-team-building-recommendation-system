Machine Translation of Very Close Languages 
Jan HAJI(~ 
Computer Science Dept. 
Johns Hopkins University 
3400 N. Charles St., Baltimore, 
MD 21218, USA 
hajic@cs.jhu.edu 
Jan HRIC 
KTI MFF UK 
Malostransk6 nfim.25 
Praha 1, Czech Republic, 11800 
hric@barbora.m ff.cuni.cz 
Vladislav KUBON 
OFAL MFF UK 
Malostransk6 mim.25 
Praha 1, Czech Republic, 11800 
vk@ufal.mff.cuni.cz 
Abstract 
Using examples of the transfer-based MT 
system between Czech and Russian 
RUSLAN and the word-for-word MT system 
with morphological disambiguation between 
Czech and Slovak (~ESILKO we argue that 
for really close languages it is possible to 
obtain better translation quality by means of 
simpler methods. The problem of translation 
to a group of typologically similar languages 
using a pivot language is also discussed here. 
Introduction 
Although the field of machine translation has a 
very long history, the number of really successful 
systems is not very impressive. Most of the funds 
invested into the development of various MT 
systems have been wasted and have not 
stimulated a development of techniques which 
would allow to translate at least technical texts 
from a certain limited domain. There were, of 
course, exceptions, which demonstrated that 
under certain conditions it is possible to develop 
a system which will save money and efforts 
invested into human translation. The main reason 
why the field of MT has not met the expectations 
of sci-fi literature, but also the expectations of 
scientific community, is the complexity of the 
task itself. A successful automatic translation 
system requires an application of techniques from 
several areas of computational inguistics 
(morphology, syntax, semantics, discourse 
analysis etc.) as a necessary, but not a sufficient 
condition. The general opinion is that it is easier 
to create an MT system for a pair of related 
languages. In our contribution we would like to 
demonstrate hat this assumption holds only for 
really very closely related languages. 
1. Czech-to-Russian MT system RUSLAN 
1.1 History 
The first attempt o verify the hypothesis that 
related languages are easier to translate started in 
mid 80s at Charles University in Prague. The 
project was called RUSLAN and aimed at the 
translation of documentation i the domain of 
operating systems for mainframe computers. It 
was developed in cooperation with the Research 
Institute of Mathematical Machines in Prague. At 
that time in former COMECON countries it was 
obligatory to translate any kind of documentation 
to such systems into Russian. The work on the 
Czech-to-Russian MT system RUSLAN (cf. Oliva 
(1989)) started in 1985. It was terminated in 1990 
(with COMECON gone) for the lack of funding. 
1.2 System description 
The system was rule-based, implemented in 
Colmerauer's Q-systems. It contained a full- 
fledged morphological and syntactic analysis of 
Czech, a transfer and a syntactic and 
morphological generation of Russian. There was 
almost no transfer at the beginning of the project 
due to the assumption that both languages are 
similar to the extent that does not require any 
transfer phase at all. This assumption turned to be 
wrong and several phenomena were covered by 
the transfer in the later stage of the project (for 
example the translation of the Czech verb "b~" 
\[to be\] into one of the three possible Russian 
equivalents: empty form, the form "byt6" in future 
7 
tense and the verb "javljat6sja"; or the translation 
of verbal negation). 
At the time when the work was terminated in 
1990, the system had a main translation 
dictionary of about 8000 words, accompanied by 
so called transducing dictionary covering another 
2000 words. The transducing dictionary was 
based on the original idea described in Kirschner 
(1987). It aimed at the exploitation of the fact 
that technical terms are based (in a majority of 
European languages) on Greek or Latin stems, 
adopted according to the particular derivational 
rules of the given languages. This fact allows for 
the "translation" of technical terms by means of a 
direct transcription of productive ndings and a 
slight (regular) adjustment of the spelling of the 
stem. For example, the English words 
localization and discrimination can be 
transcribed into Czech as "lokalizace" and 
"diskriminace" with a productive nding -ation 
being transcribed to -ace. It was generally 
assumed that for the pair Czech/Russian the 
transducing dictionary would be able to profit 
from a substantially greater number of productive 
rules. This hypothesis proved to be wrong, too 
(see B6mov~, Kubofi (1990)). The set of 
productive ndings for both pairs (English/Czech, 
as developed for an earlier MT system from 
English to Czech, and Czech/Russian) was very 
similar. 
The evaluation of results of RUSLAN showed 
that roughly 40% of input sentences were 
translated correctly, about 40% with minor errors 
correctable by a human post-editor and about 
20% of the input required substantial editing or 
re-translation. There were two main factors that 
caused a deterioration of the translation. The first 
factor was the incompleteness of the main 
dictionary of the system. Even though the system 
contained a set of so-called fail-soft rules, whose 
task was to handle such situations, an unknown 
word typically caused a failure of the module of  
syntactic analysis, because the dictionary entries 
contained - besides the translation equivalents 
and morphological information - very important 
syntactic information. 
The second factor was the module of syntactic 
analysis of Czech. There were several reasons of 
parsing failures. Apart from the common inability 
of most rule-based formal grammars to cover a 
particular natural anguage to the finest detail of 
its syntax there were other problems. One of  them 
was the existence of non-projective constructions, 
which are quite common in Czech even in 
relatively short sentences. Even though they 
account only for 1.7?/'o f syntactic dependencies, 
every third Czech sentence contains at least one, 
and in a news corpus, we discovered as much as 
15 non-projective dependencies; see also Haji6 et 
al. (1998). An example of a non-projective 
construction is "Soubor se nepodafilo otev~it." 
\[lit.: File Refl. was_not._possible to_open. - It was 
not possible to open the file\]. The formalism used 
for the implementation (Q-systems) was not meant 
to handle non-projective constructions. Another 
source of trouble was the use of so-called 
semantic features. These features were based on 
lexical semantics of individual words. Their main 
task was to support a semantically plausible 
analysis and to block the implausible ones. It 
turned out that the question of implausible 
combinations of  semantic features is also more 
complex than it was supposed to be. The practical 
outcome of the use of semantic features was a 
higher atio of parsing failures - semantic features 
often blocked a plausible analysis. For example, 
human lexicographers a signed the verb 'to run' a 
semantic feature stating that only a noun with 
semantic features of a human or other living being 
may be assigned the role of subject of this verb. 
The input text was however full of sentences with 
'programs' or 'systems' running etc. It was of 
course very easy to correct he semantic feature in 
the dictionary, but the problem was that there 
were far too many corrections required. 
On the other hand, the fact that both languages 
allow a high degree of word-order freedom 
accounted for a certain simplification of  the 
translation process. The grammar elied on the 
fact that there are only minor word-order 
differences between Czech and Russian. 
1.3 Lessons learned  f rom RUSLAN 
We have learned several lessons regarding the MT 
of closely related languages: 
? The transfer-based approach provides a 
similar quality of translation both for closely 
related and typologically different languages 
? Two main bottlenecks of full-fledged 
transfer-based systems are: 
8 
- complexity of the syntactic dictionary 
- relative unreliability of the syntactic 
analysis of the source language 
Even a relatively simple component 
(transducing dictionary) was equally complex 
for English-to-Czech and Czech-to-Russian 
translation 
Limited text domains do not exist in real life, 
it is necessary to work with a high coverage 
dictionary at least for the source language. 
2. Translation and localization 
2.1 A pivot language 
Localization of products and their documentation 
is a great problem for any company, which wants 
to strengthen its position on foreign language 
market, especially for companies producing 
various kinds of  software. The amounts of texts 
being localized are huge and the localization 
costs are huge as well. 
It is quite clear that the localization from one 
source language to several target languages, 
which are typologically similar, but different 
from the source language, is a waste of money 
and effort. It is of course much easier to translate 
texts from Czech to Polish or from Russian to 
Bulgarian than from English or German to any of 
these languages. There are several reasons, why 
localization and translation is not being 
performed through some pivot language, 
representing a certain group of closely related 
languages. Apart from political reasons the 
translation through a pivot language has several 
drawbacks. The most important one is the 
problem of the loss of translation quality. Each 
translation may to a certain extent shift the 
meaning of the translated text and thus each 
subsequent translation provides results more and 
more different from the original. The second 
most important reason is the lack of translators 
from the pivot to the target language, while this is 
usually no problem for the translation from the 
source directly to the target language. 
2.2 Translation memory is the key 
The main goal of this paper is to suggest how to 
overcome these obstacles by means of a 
combination of an MT system with commercial 
MAHT (Machine-aided human translation) 
systems. We have chosen the TRADOS 
Translator's Workbench as a representative 
system of a class of these products, which can be 
characterized as an example-based translation 
tools. IBM's Translation Manager and other 
products also belong to this class. Such systems 
uses so-called translation memory, which contains 
pairs of previously translated sentences from a 
source to a target language. When a human 
translator starts translating a new sentence, the 
system tries to match the source with sentences 
already stored in the translation memory. If it is 
successful, it suggests the translation and the 
human translator decides whether to use it, to 
modify it or to reject it. 
The segmentation f a translation memory is a key 
feature for our system. The translation memory 
may be exported into a text file and thus allows 
easy manipulation with its content. Let us suppose 
that we have at our disposal two translation 
memories - one human made for the source/pivot 
language pair and the other created by an MT 
system for the pivot/target language pair. The 
substitution of segments of a pivot language by 
the segments of a target language is then only a 
routine procedure. The human translator 
translating from the source language to the target 
language then gets a translation memory for the 
required pair (source/target). The system of 
penalties applied in TRADOS Translator's 
Workbench (or a similar system) guarantees that if 
there is already a human-made translation present, 
then it gets higher priority than the translation 
obtained as a result of the automatic MT. This 
system solves both problems mentioned above - 
the human translators from the pivot to the target 
language are not needed at all and the machine- 
made translation memory serves only as a 
resource supporting the direct human translation 
from the source to the target language. 
3. Mach ine  t rans lat ion of  (very) closely 
related Slavic languages 
In the group of Slavic languages, there are more 
closely related languages than Czech and Russian. 
Apart from the pair of Serbian and Croatian 
languages, which are almost identical and were 
9 
considered one language just a few years ago, the 
most closely related languages in this group are 
Czech and Slovak. 
This fact has led us to an experiment with 
automatic translation between Czech and Slovak. 
It was clear that application of a similar method 
to that one used in the system RUSLAN would 
lead to similar results. Due to the closeness of 
both languages we have decided to apply a 
simpler method. Our new system, (~ESILKO, 
aims at a maximal exploitation of the similarity 
of both languages. The system uses the method of 
direct word-for-word translation, justified by the 
similarity of syntactic constructions of both 
languages. 
Although the system is currently being tested on 
texts from the domain of documentation to 
corporate information systems, it is not limited to 
any specific domain. Its primary task is, however, 
to provide support for translation and localization 
of various technical texts. 
3.1 System (~ESiLKO 
The greatest problem of the word-for-word 
translation approach (for languages with very 
similar syntax and word order, but different 
morphological system) is the problem of 
morphological ambiguity of individual word 
forms. The type of ambiguity is slightly different 
in languages with a rich inflection (majority of 
Slavic languages) and in languages which do not 
have such a wide variety of forms derived from a 
single lemma. For example, in Czech there are 
only rare cases of part-of-speech ambiguities ( t~t 
\[to stay/the state\], zena \[woman/chasing\] or tri 
\[three/rub(imperative)\]), much more frequent is 
the ambiguity of gender, number and case (for 
example, the form of the adjective jam\[ \[spring\] 
is 27-times ambiguous). The main problem is that 
even though several Slavic languages have the 
same property as Czech, the ambiguity is not 
preserved. It is distributed in a different manner 
and the "form-for-form" translation is not 
applicable. 
Without he analysis of at least nominal groups it 
is often very difficult to solve this problem, 
because for example the actual morphemic 
categories of adjectives are in Czech 
distinguishable only on the basis of gender, 
number and case agreement between an adjective 
and its governing noun. An alternative way to the 
solution of this problem was the application of a 
stochastically based morphological disambiguator 
(morphological tagger) for Czech whose success 
rate is close to 92?/'0. Our system therefore consists 
of the following modules: 
1. Import of the input from so-called 'empty' 
translation memory 
2. Morphological analysis of Czech 
3. Morphological disambiguation 
4. Domain-related bilingual glossaries (incl. 
single- and multiword terminology) 
5. General bilingual dictionary 
6. Morphological synthesis of Slovak 
7. Export of the output o the original translation 
memory 
Letus now look in a more detail at the individual 
modules of the system: 
ad 1. The input text is extracted out of a 
translation memory previously exported into an 
ASCII file. The exported translation memory (of 
TRADOS) has a SGML-Iike notation with a 
relatively simple structure (cf. the following 
example): 
Example 1. - A sample of the exported translation 
memory 
<RTF Preamble>...</RTF Preamble> 
<TrU> 
<CrD>23051999 
<CrU>VK 
<Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete 
rychle a jednoduge vytv~i~et regerge. 
<Seg L=SK_01 >n/a 
</TrU> 
Our system uses only the segments marked by 
<Seg L=CS_01>, which contain one source 
language sentence ach, and <Seg L=SK_01>, 
which is empty and which will later contain the 
same sentence translated into the target language 
by CESiLKO. 
ad 2. The morphological analysis of Czech is 
based on the morphological dictionary developed 
by Jan Haji6 and Hana Skoumalov~i in 1988-99 
(for latest description, see Haji~ (1998)). The 
dictionary contains over 700 000 dictionary 
entries and its typical coverage varies between 
10 
99% (novels) to 95% (technical texts). The 
morphological analysis uses the system of 
positional tags with 15 positions (each 
morphological .category, such as Part-of-speech, 
Number, Gender, Case, etc. has a fixed, single- 
symbol place in the tag). 
Example 2 - tags assigned to the word-form 
"pomoci" (help/by means of) 
pomoci: 
NFP2 .... . .  A .... \]NFS7 ...... A .... I R--2 . . . . . . . . . . .  
where : 
N - noun; R - preposition 
F - feminine gender 
S - singular, P - plural 
7, 2 - case (7 - instrumental, 2 - genitive) 
A - affirmative (non negative) 
ad 3. The module of morphological 
disambiguation is a key to the success of  the 
translation. It gets an average number of 3.58 
tags per token (word form in text) as an input. 
The tagging system is purely statistical, and it 
uses a log-linear model of probability distribution 
- see Haji~, Hladkfi (1998). The learning is based 
on a manually tagged corpus of Czech texts 
(mostly from the general newspaper domain). 
The system learns contextual rules (features) 
automatically and also automatically determines 
feature weights. The average accuracy of tagging 
is between 91 and 93% and remains the same 
even for technical texts (if we disregard the 
unknown names and foreign-language t rms that 
are not ambiguous anyway). 
The lemmatization immediately follows tagging; 
it chooses the first lemma with a possible tag 
corresponding to the tag selected. Despite this 
simple lemmatization method, and also thanks to 
the fact that Czech words are rarely ambiguous in 
their Part-of-speech, it works with an accuracy 
exceeding 98%. 
ad 4. The domain-related bilingual glossaries 
contain pairs of individual words and pairs of 
multiple-word terms. The glossaries are 
organized into a hierarchy specified by the user; 
typically, the glossaries for the most specific 
domain are applied first. There is one general 
matching rule for all levels of glossaries - the 
longest match wins. 
The multiple-word terms are sequences of lemmas 
(not word forms). This structure has several 
advantages, among others it allows to minimize 
the size of the dictionary and also, due to the 
simplicity of the structure, it allows modifications 
of the glossaries by the linguistically naive user. 
The necessary morphological information is 
introduced into the domain-related glossary in an 
off-line preprocessing stage, which does not 
require user intervention. This makes a big 
difference when compared to the RUSLAN 
Czech-to-Russian MT system, when each 
multiword dictionary entry cost about 30 minutes 
of linguistic expert's time on average. 
ad 5. The main bilingual dictionary contains data 
necessary for the translation of  both lemmas and 
tags. The translation of tags (from the Czech into 
the Slovak morphological system) is necessary, 
because due to the morphological differences both 
systems use close, but slightly different tagsets. 
Currently the system handles the 1:1 translation of 
tags (and 2:2, 3:3, etc.). Different ratio of 
translation is very rare between Czech and Siovak, 
but nevertheless an advanced system of dictionary 
items is under construction (for the translation 1:2, 
2:1 etc.). It is quite interesting that the lexically 
homonymous words often preserve their 
homonymy even after the translation, so no 
special treatment of homonyms is deemed 
necessary. 
ad 6. The morphological synthesis of Slovak is 
based on a monolingual dictionary of SIovak, 
developed by J.Hric (1991-99), covering more 
than \]00,000 dictionary entries. The coverage of 
the dictionary is not as high as of  the Czech one, 
but it is still growing. It aims at a similar coverage 
of Slovak as we enjoy for Czech. 
ad 7. The export of  the output of the system 
(~ESILKO into the translation memory (of 
TRADOS Translator's Workbench) amounts 
mainly to cleaning of all irrelevant SGML 
markers. The whole resulting Slovak sentence is 
inserted into the appropriate location in the 
original translation memory file. The following 
example also shows that the marker <CrU> 
contains an information that the target language 
sentence was created by an MT system. 
11 
Example 3. -A  sample of the translation memory 
containing the results of MT 
<RTF Preamble>...</RTF Preamble> 
<TrU> 
<CRD>23051999 
<CrU>MT! 
<Seg L=CS_01>Pomoci v~kazu ad-hoc mfi~ete 
rychle a jednodu~e vytv~i~et re,erie. 
<Seg L=SK_01>Pomoci v~kazov ad-hoc m6~ete 
r~chio a jednoducho vytvhrat' re,erie. 
</TrU> 
3.2 Evaluation of results 
The problem how to evaluate results of automatic 
translation is very difficult. For the evaluation of 
our system we have exploited the close 
connection between our system and the 
TRADOS Translator's Workbench. The method 
is simple - the human translator eceives the 
translation memory created by our system and 
translates the text using this memory. The 
translator is free to make any changes to the text 
proposed by the translation memory. The target 
text created by a human translator is then 
compared with the text created by the mechanical 
application of translation memory to the source 
text. TRADOS then evaluates the percentage of 
matching in the same manner as it normally 
evaluates the percentage of matching of source 
text with sentences in translation memory. Our 
system achieved about 90% match (as defined by 
the TRADOS match module) with the results of 
human translation, based on a relatively large 
(more than 10,000 words) test sample. 
4. Conclusions 
The accuracy of the translation achieved by our 
system justifies the hypothesis that word-for- 
word translation might be a solution for MT of 
really closely related languages. The remaining 
problems to be solved are problems with the one- 
to many or many-to-many translation, where the 
lack of information in glossaries and dictionaries 
sometimes causes an unnecessary translation 
error. 
The success of the system CESILKO has 
encouraged the investigation of the possibility to 
use the same method for other pairs of Slavic 
languages, namely for Czech-to-Polish translation. 
Although these languages are not so similar as 
Czech and Slovak, we hope that an addition of a 
simple partial noun phrase parsing might provide 
results with the quality comparable to the full- 
fledged syntactic analysis based system RUSLAN 
(this is of course true also for the Czechoto-Slovak 
translation). The first results of Czech-to Polish 
translation are quite encouraging in this respect, 
even though we could not perform as rigorous 
testing as we did for Slovak. 
Acknowledgements 
This project was supported by the grant GAt~R 
405/96/K214 and partially by the grant GA(~R 
201/99/0236 and project of the Ministry of 
Education No. VS96151. 
References 
B6movfi, Alevtina and Kubofi, Vladislav (1990). Czech- 
to-Russian Transducing Dictionary; In: Proceedings 
of the Xlllth COLING conference, Helsinki 1990 
Haji~, Jan (1998). Building and Using a Syntactially 
Annotated Coprus: The Prague Dependency 
Treebank. In: Festschrifi for Jarmila Panevov~i, 
Karolinum Press, Charles Universitz, Prague. pp. 
106---132. 
Haji~, Jan and Barbora Hladk~t (1998). Tagging 
Inflective Languages. Prediction of Morphological 
Categories for a Rich, Structured Tagset. ACL- 
Coling'98, Montreal, Canada, August 1998, pp. 483- 
490. 
Haji~, Jan; Brill, Eric; Collins, Michael; Hladk~t 
Barbora; Jones, Douglas; Kuo, Cynthia; Ramshaw, 
Lance; Schwartz, Oren; Tillman, Christoph; and 
Zeman, Daniel: Core Natural Language Processing 
Technology Applicable to Multiple Languages. The 
Workshop'98 Final Report. CLSP JHU. Also at: 
http:llwww.clsp.jhu.edulws981projectslnlplreport. 
Kirschner, Zden~k (1987). APAC3-2: An English-to- 
Czech Machine Translation System; Explizite 
Beschreibung der Sprache und automatische 
Textbearbeitung XII1, MFF UK Prague 
Oliva, Karel (1989). A Parser for Czech Implemented 
in Systems Q; Explizite Beschreibung der Sprache 
und automatische Textbearbeitung XVI, MFF UK 
Prague 
12 
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 43?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Language Technology from a European Perspective 
 
 
Hans Uszkoreit, 
Valia Kordoni 
Vladislav Kubon Michael Rosner Sabine Kirchmeyer-
Andersen 
Dept. of Computational 
Linguistics 
UFAL MFF UK Dept. of Computer Sci-
ence and A.I. 
Dept. of Computational 
Linguistics 
Saarland University Charles University University of Malta Copenhagen Business 
School 
D-66041, Saarbruecken, 
Germany 
Prague, Czech Republic Msida, Malta Copenhagen, Denmark 
{uszkoreit, 
kordoni}@coli.uni-
sb.de 
vk@ufal.mff.cuni.cz mike.rosner 
@um.edu.mt 
ska.id@cbs.dk 
 
 
 
 
Abstract 
This paper describes the cooperation of 
four European Universities aiming at at-
tracting more students to European master 
studies in Language and Communication 
Technologies. The cooperation has been 
formally approved within the framework 
of the new European program ?Erasmus 
Mundus? as a Specific Support Action in 
2004. The consortium also aims at creat-
ing a sound basis for a joint master pro-
gram in the field of language technology 
and computer science. 
1 European higher education: Erasmus 
Mundus 
The Erasmus Mundus programme [1] is a co-
operation and mobility program in the field of 
higher education. It aims to enhance quality in 
European higher education and to promote inter-
cultural understanding through co-operation with 
non-EU countries. 
The program is intended to strengthen European 
co-operation and international links in higher edu-
cation by supporting high-quality European Mas-
ters Courses, by enabling students and visiting 
scholars from around the world to engage in post-
graduate study at European universities, as well as 
by encouraging the outgoing mobility of European 
students and scholars towards non-EU countries. 
The Erasmus Mundus program comprises four 
concrete actions: 
 
ACTION 1 - Erasmus Mundus Masters Courses: 
high-quality integrated courses at masters level 
offered by a consortium of at least three universi-
ties in at least three different European countries.  
 
ACTION 2 - Erasmus Mundus scholarships: a 
scholarship scheme for non-EU-country graduate 
students and scholars from the whole world. 
 
ACTION 3 - Partnerships: Erasmus Mundus Mas-
ters Courses selected under Action 1 also have the 
possibility of establishing partnerships with non-
EU-country higher education institutions.  
 
ACTION 4 - Enhancing attractiveness: projects 
aimed at enhancing the attractiveness of the Euro-
pean higher education.  
2 LATER 
One of the projects approved for funding (and the 
only one in the field of language technology) in the 
2004 call is called LATER ? Language Technol-
ogy Erasmus Mundus [2]. 
LATER falls under action 4 of the program and 
hence addresses the need to enhance the attractive-
ness of European higher education in Language 
43
Technology and Communication (LCT). This need 
will be met through dissemination of the combined 
LCT-related expertise in of a consortium of Uni-
versities whose members are as follows 
 
Saarland University in Saarbruecken (CoLi) 
The Department of Computational Linguistics 
and Phonetics (CoLi) of Saarland University (co-
ordinator) has an excellent international reputation 
for graduate training in Language Technologies, 
and for leading-edge basic research in this area. 
CoLi offers a new M.Sc. program in Language 
Science and Technology [3]. This is an active pro-
gram of basic, applied and cognitive research, 
which combines with state-of-the-art facilities to 
provide students with a rich and stimulating envi-
ronment for their research. Moreover, CoLi offers 
a European Ph.D. program in Language Technol-
ogy and Cognitive Systems. In the past 15 years, 
CoLi has provided postgraduate research training 
to 100 early-stage researchers [4]. 
 
Charles University, Prague (?FAL) 
The Institute of Formal and Applied Linguistics 
(?FAL) at the Faculty of Mathematics and Physics 
of the Charles University in Prague offers a five-
year master program in Computer Science with 
several specialized branches. One of the branches 
of this program is the masters in Computational 
and Formal Linguistics [7]. It focuses mainly on 
the following four topics: formal description of 
natural language, grammars and automata in lin-
guistics, methods of artificial intelligence in lin-
guistics, as well as methods of automatic natural 
language processing.  
 
University of Malta (UoM) 
The Department of Computer Science and Arti-
ficial Intelligence at the University of Malta, estab-
lished in 1993, teaches both Bachelors and Masters 
degree programs. The 4-year BSc. (Hons) scheme 
include several streams relevant to Language 
Technology including NLP and Computational 
Linguistics itself, Information Retrieval, Semantic 
Web, Internet and Agent technologies. The De-
partment also runs a, one-year research oriented 
M.Sc. program [10]. The areas of specialization 
include the development of computational tools, 
techniques and resources for Maltese, the only se-
mitic language to enjoy official EU status.  
 
Copenhagen Business School (CBS) 
The Department of Computational Linguistics 
is part of the Faculty of Modern Languages at the 
Copenhagen Business School. The Department is 
actively involved in research in the following four 
core fields: formal descriptions of the Danish lan-
guage, modeling of knowledge relevant for LSP, 
LSP databases, and Machine Translation. Embed-
ded in this context is the Master of Language Ad-
ministration (MLA) [9] that the Department of 
Computational Linguistics of the Copenhagen 
Business School offers in co-operation with the 
University of Southern Denmark in Roskilde  
3 Overall aims of the project 
The overall aim of the project is to export the 
common educational experience currently embod-
ied within existing Masters programs of the con-
sortium to scholars and students of non-EU 
countries. 
This aim will be realized by several different 
classes of activity under the rubrics of (i) work-
shops (ii) distance learning tools and (iii) coordina-
tion of a common Master program. We discuss 
these in the following sections. 
3.1 Workshops 
One of the most important types of activities of 
the project is organizing workshops and courses 
both for students from non-EU countries and for 
their teachers. The effect of these events is at least 
twofold ? the students from countries or regions 
which do not have an access to any higher degree 
education in LCT get a chance to broaden their 
perspective by listening to lectures of prominent 
scientists and lecturers. The courses will also help 
the consortium to establish better contacts with 
non-EU Universities, teachers, and students which 
will turn out to be invaluable when disseminating 
the common European Master program in Lan-
guage Technology discussed further below. 
Both ?FAL and CoLi have a long tradition in 
respect of offering such courses to students from 
the broadest possible range of countries. 
?FAL has devoted a huge effort in the past to 
raise funding for the organization, once or twice a 
year, of a series of lectures by prominent scientists 
and lecturers from all over the world. This series of 
lectures, the Vilem Mathesius courses [6], have 
become well-known, especially among the Central 
44
and East European students of computational and 
general linguistics.  
This year?s course, held in March under the aus-
pices of LATER, was able to support  the atten-
dance of 50 students from Russia, Ukraine, 
Albania, Bosnia, Serbia, Croatia and Georgia to 
lectures by prominent individuals including two 
ACL award winners. 
At CoLi, the Computational Linguistics Collo-
quium is also a traditional event attracting the at-
tention of both well-known lecturers and a number 
of master and postgraduate students from various 
countries. A second series of lectures in the frame 
of our project was held at the University of Saar-
landes in Saarbruecken in January. 
A third event, organized by the CBS, will take 
place in June. The first day consists of information 
seminar on content management and language 
technology to promote CBS? newly-launched In-
ternational Master of Language Administration, 
whilst the second will be devoted to diffusion of a 
various issues connected to the Erasmus Mundus 
course.  
Finally, a fourth event, in the form of a work-
shop with invited guest lecturers, is being organ-
ized at the University of Malta that will take place 
in September 2005. The theme of the workshop 
will be Machine Translation which is currently 
very topical given the newly-achieved official 
European status that the local language now enjoys.  
3.2 Coordination of Masters Programs 
A second important aim of the LATER project 
is the definition, coordination and implementation 
of an integrated European Masters Programme in 
LCT by creating a common basis that will appeal 
to both European and non-EU students. 
The rationale behind the creation of such a pro-
gramme is the assumption that LCT now occupies 
a central position in research and education in 
Europe, being a key enabling technology for nu-
merous applications related to the information so-
ciety, although the shortage of qualified 
researchers and developers is slowing down the 
speed of innovation in Europe. 
The proposed programme addresses this short-
age by creating a directed education and training 
opportunity for the next generation of LCT innova-
tors in that will in turn bring educational, social 
and economic benefits. Some specific aims of 
Erasmus Mundus are also addressed: European 
education in LCT will be promoted worldwide and 
its competitiveness increased, increasing at the 
same time the competitiveness of European IT in-
dustries, creating a multilingual information soci-
ety that is accessible for all, and turning the 
``information overload'' into a wealth of accessible 
and useful knowledge. 
3.3 Distance learning tools 
A third aim of LATER is the development of 
effective methods of hosting and integrating non-
EU students, for example by developing distance 
learning tools and joint distance education modules, 
in order to facilitate outreach by online dissemina-
tion of courses. An example of such modules, as 
well as for computer-based tools, is being devel-
oped on the basis of the virtual courses CoLi has 
developed in the last 3 years in the framework of 
the MiLCA project (Medienintensive Lehrmodule 
in der Computerlinguistik-Ausbildung1). 
We also plan to explore the use of collaboration 
technologies based on Sitescape [16], that have 
been developed at CBS for academic collaboration, 
for the management of certain aspects of the pro-
posed Masters programme.
The fruits of various initiatives already under 
way at UoM will be exploited and extended during 
the life of the proposed course. These include in-
teractive web based course delivery [13], just-in-
time support based on P2P architectures [14], 
XML-based frameworks for online courses [15], 
the latter being developed within as a part of the 
Mediterranean Virtual University (MVU) 
EUMEDIS project [17]. 
 
4 Integrated European LCT Masters 
Programme 
Whilst many agree with the above assessment of 
the importance of LCT, they disagree on the defi-
nition of ?integrated course?. Fortunately, we can 
turn to the comprehensive definition supplied by 
the EU call, the central element of which is ?a 
jointly developed curriculum or full recognition by 
the consortium of modules which are developed 
                                                          
1 for more see http://milca.sfs.uni-
tuebingen.de/index.html. 
45
and delivered separately, but make up a common 
standard Masters course.? 
Again, some turn away in horror at the notion of 
a standard curriculum in this area, the claim being 
that there is already enough standardization in the 
world, so why add to it? The point is, any pro-
gramme dealing with LCT has to address the fact 
that it is highly interdisciplinary, including, at the 
core, computer science, computational and theo-
retical linguistics, and mathematics, and at the pe-
riphery, a wide variety of other subjects including 
electrical engineering, psychology, cognitive sci-
ence artificial intelligence etc.  
With such a large number of disciplines in-
volved, it is practically impossible for a single 
University to excel in all of them. However if more 
than one University is involved, various kinds of 
curriculum sharing can be envisaged and so a 
much higher level of coverage becomes entirely 
achievable.  
Put another way, curriculum sharing, together 
with common admission and assessment proce-
dures envisaged, allows delivery of a complex 
course to be handled by what is effectively a ?su-
peruniversity?. 
4.1 Integration in practice 
To put this idea into practice we are proposing 
that students will get the chance to attend a two 
years? master program at two universities chosen 
from a larger consortium, which is currently being 
put together. It includes the four original partners 
of the LATER project and the following new part-
ners: University of Amsterdam (UvA) in the Neth-
erlands, Free University of Bolzano-Bozen (FUB) 
in Italy, the Universities of Nancy 1 and Nancy 2 
in France, Roskilde University in Denmark and 
Utrecht University in the Netherlands. 
Studying in multi-national groups at two uni-
versities in Europe, with English as instruction 
language, accompanied by language classes in an-
other European language, will contribute to the 
students' preparation for the increasing globaliza-
tion of science, commerce and industry. The 
course also will also prepare students for follow-up 
Ph.D. studies provided by the participating partners 
and others. 
The proposed programme follows the Bologna 
model for higher education in Europe and com-
prises 120 ECTS2 credits, 30 of which make up the 
Masters dissertation, and 90 of which are course-
work credits structured as follows: 
? Compulsory modules in Computer Science (28 
ECTS) 
? Compulsory modules in Language Technology 
(28 ECTS) 
? Advanced modules in Language Technology, 
Computational Linguistics and Computer Sci-
ence (34 ECTS) 
Coursework is distributed over three semesters, 
while the dissertation is supposed to be completed 
in the fourth semester  
It is important to underline that this structure 
permits a considerable degree of variation. First, a 
module might be ?implemented? by different set of 
courses at different Universities. Secondly, the ad-
vanced modules are electives, based on the specific 
strengths in research and teaching of individual 
partner institutions. There is no requirement that 
the advanced modules offered by different Univer-
sities should coincide. 
Let us now introduce individual modules in 
more detail. Parentheses indicate ECTS credits. 
Computer Science Modules 
The Computer Science Modules are as follows:  
 
? Logic, Computability and Complexity (? 9) 
Topics: Logic & inference; Computability the-
ory; Complexity theory; Discrete mathematics 
? Formal Languages and Algorithms (? 9) 
Topics: Formal grammars and languages hier-
archy; Parsing and compiler design; Search 
techniques and constraint resolution; Auto-
mated Learning 
? Data Structures, Data Organization and 
Processing (? 6) 
Topics: Algebraic data types; Relational data-
bases; Semi-structured data and XML; Informa-
tion retrieval; Digital libraries 
? Advanced Modules and Applications(? 6) 
Topics: Artificial Intelligence, Knowledge 
?epresentation, Automated Reasoning, 
Semantic Web, Neural Networks, Machine 
Learning etc. Students are expected to obtain at 
least 9 ECTS credits from each of the first two 
                                                          
2 European Credit Transfer System: a standard measure that is 
used in Europe for comparing the size of courses. 
46
modules and 6 ECTS credits from each of the 
remaining two modules.  
Language Technology Modules 
The Language Technology Modules are these: 
? Foundations of Language Technology (? 6) 
Topics: Statistical methods; Symbolic methods; 
Cognition; Corpus Linguistics; Text and 
speech; Foundations of Linguistics 
? Computational Syntax and Morphology (? 9) 
Topics: Finite state methods; Probabilistic ap-
proaches; Formal grammars; Tagging; Chunk-
ing; Parsing 
? Computational Semantics, Pragmatics and 
Discourse (? 6) 
Topics: Syntax-semantics interface; Semantic 
construction; Dialogue; Formal semantics 
? Advanced Modules and Applications 
(? 6) Topics: Machine Translation, Informa-
tion Retrieval, Speech Recognition, Question 
Answering, Psycholinguistics etc.. 
4.2 Main issues to be addressed  
Although it was not explicitly mentioned in the 
previous text, the integration of existing master 
programmes is done exclusively pair-wise. The 
students can?t study at three universities (although 
the rules of the Erasmus Mundus programme allow 
such triangular cooperation). The restrictions 
within our consortia go even further ? the students 
do not have a free choice of a combination of any 
two universities from within the consortium, they 
must choose one of the pairs offered by the consor-
tium. 
The reason for such a restriction is pretty simple 
- it turned out that although all members of the 
consortia in principle provide education both in 
Computer Science and in Computational Linguis-
tics, they differ in the balance between these two 
fields. Within the consortium, there are universities 
with a strong stress on a Computer Science courses, 
aiming at a complex education including the sound 
theoretical background in the field, while other 
universities offer a more practically oriented edu-
cational scheme, stressing the concepts attracting a 
wider audience, e.g. various types of web tech-
nologies, databases, data mining etc.  
As a result of this, each university participates in 
an average of four bilateral partnerships. We think 
that the fact that the consortium consists of univer-
sities which are not identical greatly increases the 
variety of options available. They have a chance to 
choose those universities which are best suited to 
their preferences whether these are in terms of sub-
ject area emphasis or geographical region.  
The preparation of the integrated Master pro-
gramme doesn?t stop at matching the universities 
and lectures offered. Erasmus Mundus is not just a 
cooperation, it is really a completely new scheme 
which must also address practical issues as grades, 
examination procedures, admission procedure, tui-
tion fees, defense of the thesis, local specialties 
existing at some partner universities etc.
The proposed Masters programme is something 
new. It is the first attempt to create a comprehen-
sive Masters degree in this subject area that con-
forms to all the legalistic requirements of each 
participating University. Students completing the 
course will possess a Masters degree delivered by 
two of the participant Universities. This is in con-
trast to the existing European Master in Language 
and Speech [11], which is implemented through a 
certification procedure that does not replace any 
legal degree that a student may obtain from a Uni-
versity.
 
5 Conclusion 
Although the process of establishing a new Euro-
pean Master programme in Language Technology 
was really very complicated, time consuming and 
painful, there are definitely already at this stage 
very positive results. 
In order to submit a proposal, our consortium 
has managed to overcome all formal and structural 
differences among all partners, it has found a rea-
sonable model of cooperation, it has developed a 
high-quality master programme open both to Euro-
pean and non-EU students. 
The wide variety of modules and topics offered 
combined with a relatively high degree of freedom 
of choice for students allows for individual pairs of 
partner universities to promote those courses and 
fields in which they excel. The students are of 
course offered individual guidance from consor-
tium members in order to allow them to identify 
that pair of universities which best suits their indi-
vidual needs and preferences 
47
The strategy we have chosen ? the initial coop-
eration of a smaller consortium in the LATER pro-
ject, promoting LTC education among the students 
from outside the EU and testing our ability both to 
offer a coordinated high-quality education and to 
attract a reasonable amount of interested students, 
has turned to be a sound one. It also helped to 
solve some issues in the larger consortium based 
on the experience from the smaller one. 
References  
[1] http://europa.eu.int/comm/education
/programmes/mundus/index_en.html 
(Erasmus Mundus web page) 
[2] http://europa.eu.int/comm/education
/programmes/mundus/projects/2004/47
.pdf (The description of the LATER pro-
ject) 
[3] http://www.coli.uni-
saarland.de/msc/ (the MSc website at 
the University of Saarlandes in Saar-
bruecken) 
[4] http://www.coli.uni-
saarland.de/kvv/ (courses at the Dept. 
of Computational Linguistics at the Uni-
versity of Saarlandes in Saarbruecken) 
[5] http://www.coli.uni-
saarland.de/courses/late2/ (the web 
page of the Language Technology II 
course in Saarbruecken) 
[6] http://ufal.mff.cuni.cz/vmc/vmc_ls2
0.html (the web page of the Vilem 
Mathesius Lecture Series) 
[7] http://www.mff.cuni.cz/toUTF8.en/st
udium/bcmgr/ok/i1b53.htm (the master 
programme in Mathematical Linguistics at 
the Charles University in Prague) 
[8] http://web.cbs.dk/stud_pro/clmdatau
k.shtml (the master program at the Co-
penhagen Business School) 
[9] http://uk.cbs.dk/mla (Master of Lan-
guage Administration at the Copenhagen 
Business School) 
[10] http://www.cs.um.edu.mt/rese
arch/pgEnquiries.html (the master 
program at the University of Malta) 
[11] http://www.cstr.ed.ac.uk/e
uromasters (European Masters in 
Language and Speech) 
[12] A.Burchardt, S. Walter and M. 
Pinkal. 2004. "MiLCA -- Distance Educa-
tion in Computational Linguistics". In 
Szucs, Andras and Bo, Ingeborg 
(eds.),  New Challenges and Partnerships 
in an Enlarged European Union ? Proc. 
2004 EDEN Conference, Budapest, pp. 
351-356. 
[13] Ellul, C., 2002, ?Just-in-Time Lec-
ture Delivery, Management and Student 
Support System?, BSc. Project report, 
Dept. CSAI, University of Malta. 
[14] Bezzina, R., 2002, ?Peer-to-Peer 
Just-in-Time Support for Curriculum based 
Learning?, BSc. Project report, Dept. 
CSAI, University of Malta. 
[15] Cachia, E., and Micallef, M., forth-
coming, ?A Universal XML/XSLT 
Framework for Online Courses?, Proc. In-
ternational Conference  on IT-Based 
Higher Education And Training (ITHET)?, 
Dominican Republic. 
[16] www.sitescape.com : SiteScape 
corporate website. 
[17] http://www.eumedis.net/en/project/
22: Mediterranean Virtual University 
(MVU) description. 
48
Proceedings of the Workshop on Linguistic Distances, pages 91?99,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Structural Similarity Measure
Petr Homola and Vladislav Kubon?
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25
110 00 Praha 1, Czech republic
{homola,vk}@ufal.mff.cuni.cz
Abstract
This paper outlines a measure of lan-
guage similarity based on structural
similarity of surface syntactic depen-
dency trees. Unlike the more tradi-
tional string-based measures, this mea-
sure tries to reflect ?deeper? correspon-
dences among languages. The develop-
ment of this measure has been inspired
by the experience from MT of syntac-
tically similar languages. This experi-
ence shows that the lexical similarity is
less important than syntactic similar-
ity. This claim is supported by a num-
ber of examples illustrating the prob-
lems which may arise when a measure
of language similarity relies too much
on a simple similarity of texts in differ-
ent languages.
1 Introduction
Although the similarity of natural languages is
in principal a very vague notion, the linguistic
literature seems to be full of claims classifying
two natural languages as being more or less
similar. These claims are in some cases a result
of a detailed comparative examination of lex-
ical and/or syntactic properties of languages
under question, in some cases they are based
on a very subjective opinion of the author, in
many other cases they reflect the application
of some mathematical formula on textual data
(a very nice example of such mathematical ap-
proach can be found at (Scannell, 2004)).
Especially in the last case the notion of lan-
guage similarity is very often confused with the
notion of text similarity. Even the well known
paper (Lebart and Rajman, 2000) deals more
with the text similarity than language similar-
ity. This general trend is quite understand-
able, the mathematical methods for measur-
ing text similarity are of a prominent impor-
tance especially for information retrieval and
similar fields. On the other hand, they con-
centrate too much on the surface similarity
of word forms and thus may not reflect the
similarity of languages properly. This paper
tries to advocate different approach, based on
the experience gained in MT experiments with
closely related (and similar) languages, where
it is possible to ?measure? the similarity indi-
rectly by a complexity of modules we have to
use in order to achieve a reasonable transla-
tion quality. This experience led us to formu-
lating an evaluation measure trying to capture
not only textual, but also syntactic similarities
between natural languages.
2 Imperfections of measures based
on string similarity
There are many application areas in the NLP
in which it is useful to apply the measures ex-
ploiting the similarity of word forms (strings).
They serve very well for example for tasks
like spellchecking (where the choice of the best
candidates for correction of a spelling error is
typically based upon the Levenshtein metrics)
or estimating the similarity of a new source
sentence to those stored in the translation
memory of a Machine Aided Translation sys-
tem. They are a bit controversial in a ?proper?
machine translation, where the popular BLEU
score (Papineni et al, 2002), although widely
accepted as a measure of translation accuracy,
seems to favor stochastic approaches based on
91
an n-gram model over other MT methods (see
the results in (Nist, 2001)).
The controversies the BLEU score seems to
provoke arise due to the fact that the evalua-
tion of MT systems can be, in general, per-
formed from two different viewpoints. The
first one is that of a developer of such a sys-
tem, who needs to get a reliable feedback in
the process of development and debugging of
the system. The primary interest of such a
person is the grammar or dictionary coverage
and system performance and he needs a cheap,
fast and simple evaluation method in order to
allow frequent routine tests indicating the im-
provements of the system during the develop-
ment of the system.
The second viewpoint is that of a user, who
is primarily concerned with the capability of
the system to provide fast and reliable trans-
lation requiring as few post-editing efforts as
possible. The simplicity, speed and low costs
are not of such importance here. If the eval-
uation is performed only once, in the mo-
ment when the system is considered to be
ready, the evaluation method may even be rel-
atively complicated, expensive and slow. A
good example of such a complex measure is the
FEMTI framework (Framework for the Evalu-
ation of Machine Translation). The most com-
plete description of the FEMTI framework can
be found in (Hovy et al, 2002). Such mea-
sures are much more popular among transla-
tors than among language engineers and MT
systems developers.
If we aim at measuring the similarity of lan-
guages or language distances, our point of view
should be much more similar to that of a hu-
man translator than of a system developer, if
we?ll stick to our MT analogy. When looking
for clues concerning the desirable properties
of a language similarity (or distance) measure,
we can first try to formulate the reasons why
we consider the simple string-based (or word-
form-based) measures inadequate.
If we take into account a number of lan-
guages existing in the world, the number of
word forms existing in each of those languages
and a simple fact that a huge percentage of
those word forms is not longer than five or
six characters, it is quite clear that there is a
huge number of overlapping word forms which
have completely different meaning in all lan-
guages containing that particular word form.
Let us take for illustration some language pairs
of non-related languages.
For example for Czech and English (the lan-
guages very different with regard both to the
lexicon and syntax) we can find several exam-
ples of overlapping word forms. The English
word house means a duckling in Czech, the En-
glish indefinite article a is in Czech also very
frequent, because it represents a coordinating
conjunction and, while an is an archaic form
of a pronoun in Czech. On the other hand, if
we look at the identical (or nearly identical)
word forms in similar languages, we can find
many examples of totally different meaning.
For example, the word form z?ivot means life
in Czech and belly in Russian; godina means
year in Serbo-Croatian while hodina is an hour
in Czech (by the way, an hour in Russian is c?as
? and the same word means time in Czech).
The overlapping word forms between rela-
tively distant languages are so frequent that it
is even possible to create (more or less) syntac-
tically correct sentences in one language con-
taining only word forms from the other lan-
guage. Again, let us look at the Czech-English
language pair. The English sentences Let my
pal to pile a lumpy paste on a metal pan. or
I had to let a house to a nosy patron. consist
entirely of word forms existing also in Czech,
while the Czech sentence Adept demise metal
hole pod led. ? [A resignation candidate was
throwing sticks under the ice.] consists of En-
glish word forms.
Creating such a Czech sentence is more com-
plicated ? as a highly inflected language it
uses a wide variety of endings, which make it
more difficult to create a syntactically correct
sentence from word forms of a language which
has incomparably smaller repertoire of end-
ings. This fact directly leads to another argu-
ment against the string similarity based mea-
sures ? even though two languages may have
very similar syntactic properties and their ba-
sic word forms may also be very similar, then if
the languages are highly inflective and the only
difference between those languages are differ-
ent endings used for expressing identical mor-
phosyntactic properties, the string similarity
based methods will probably show a substan-
92
tial difference between these languages.
This is highly probable especially for shorter
words ? the words with a basic form
only four or five characters long may have
endings longer or equal to the length of
the basic form, for example: nova?/novata
?new? (Cze/Mac), vide?ny?/vidimyj ?seen?
(Cze/Rus), fotografuj??c??/fotografuojantysis
?photographing? (Cze/Lit).
The last but not least indirect argument
against the use of string-based metrics can be
found in (Kubon? and Be?mova?, 1990). The pa-
per describes so called transducing dictionary,
a set of rules designed for a direct transcrip-
tion of a certain category of source language
words into a target language. The system has
been tested on two language pairs (English-
to-Czech and Czech-to-Russian) and although
there was a natural original assumption that
such a system will cover substantially more ex-
pressions when applied to a pair of related lan-
guages (which are not only related, but also
quite similar), this assumption turned to be
wrong. The system covered almost identical
set of words for both language pairs ? namely
the words with Greek or Latin origin. The
similarity of coverage even allowed to build an
English-to-Russian transducing dictionary us-
ing Czech as a pivot language with a negligible
loss of the coverage.
3 Experience from MT of similar
languages
The Machine Translation field is a good testing
ground for any theory concerning the similar-
ity of natural languages. The systems dealing
with related languages usually achieve higher
translation quality than the systems aiming at
the translation of more distant language pairs
? the average MT quality for a given system
and a given language pair might therefore also
serve as some kind of a very rough metrics of
similarity of languages concerned.
Let us demonstrate this idea using an ex-
ample of a multilingual MT system described
in several recently published papers (see e.g.
(Hajic? et al, 2003) or (Homola and Kubon?,
2004)). The system aims at the translation
from a single source language (Czech) into
multiple more or less similar target languages,
namely into Slovak, Polish, Lithuanian, Lower
Sorbian and Macedonian.
The system is very simple ? it doesn?t con-
tain any full-fledged parser, neither rule based,
nor stochastic one. It relies on the syntactic
similarity of the source and target languages.
It is transfer-based with the transfer being per-
formed as soon as possible, depending on the
similarity of both languages. In its simplest
form (Czech to Slovak translation) the system
consists of the following modules:
1. Morphological analysis of the source lan-
guage (Czech)
2. Morphological disambiguation of the
source language text by means of a
stochastic tagger
3. Transfer exploiting the domain-related
bilingual glossaries and a general (domain
independent) bilingual dictionary
4. Morphological synthesis of the target lan-
guage
The lower degree of similarity between Czech
and the remaining target languages led to
an inclusion of a shallow parsing module for
Czech for some of the language pairs. This
module directly follows the morphological dis-
ambiguation of Czech.
The evaluation results presented in (Homola
and Kubon?, 2004) indicate that even though
Czech and Lithuanian are much less similar
at the lexical and morphological level (e.g. at
both levels actually dealing with strings), the
translation quality is very similar due to the
syntactic similarity between all languages con-
cerned.
4 Typology of language similarity
The experience from the field of MT of closely
related languages presented in the previus sec-
tions shows that it is very useful to classify the
language similarity into several categories:
? typological
? morphological
? syntactic
? lexical
Let us now look at these categories from the
point of view of machine translation,
93
4.1 Typological similarity
The first type of similarity is probably the
most important one. If both the target and
the source language are of a different language
type, it is more difficult to obtain good MT
quality. The notions like word order, the ex-
istence or non-existence of articles, different
temporal system and several other properties
have direct consequences for the translation
quality. Let us take Czech and Lithuanian as
an example of the language pair, which doesn?t
belong to the same group of languages (Czech
is a Slavic and Lithuanian Baltic language).
Both languages have rich inflection and very
high degree of word order freedom, thus it is
not necessary to change the word order at the
constituent level. On the other hand, both
languages differ a lot in the lexics and mor-
phology.
For example, both (1) and (3) mean approx-
imately ?The father read a/the book?. What
these sentences differ in is the information
structure. (1) should be translated as ?The
father read a book?, whereas (3) means in
fact ?The book has been read by the father?.1
The category of voice differs in both sentences
because of strict word order in English, al-
though in both Czech equivalents, active voice
is used.2 We see that in the Lithuanian trans-
lation, the word order is exactly the same.
(1) Otec
father-nom
c?etl
read-3sg,past
knihu
book-acc
?The father read a book.? (Cze)
(2) Te?vas
father-nom
skaite?
read-3sg,past
knyg ?a
book-acc
?The father read a book.? (Lit)
(3) Knihu
book-acc
c?etl
read-3sg,past
otec
father-nom
?The father read a book.? (Cze)
1Note that in the first sentence, an indefinite article
is used, whereas in the latter one, a definite article
stands in front of ?book?. The reason is that in the first
sentence, the noun?book? is not contextually bound (it
belongs to the focus), in the latter one it belongs to the
topic.
2Passive voice (except of the reflexive one) occurs
rarely in Czech (and most other Slavonic languages).
It can be used if one would like to underline the di-
rect object or if there is no subject at all (for example,
Kniha byla c?tena ?The book has been read?).
(4) Knyg ?a
book-acc
skaite?
read-3sg,past
te?vas
father-nom
?The father read a book.? (Lit)
4.2 Lexical similarity
The lexical similarity does not mean that the
vocabulary has to have the same origin, i.e.,
that words have to be created from the same
(proto-)stem. What is important for shallow
MT (and for MT in general), is the seman-
tic correspondence (preferably one-to-one re-
lation).
Lexical similarity is the least important one
from the point of view of MT, because the lex-
ical differences are solved in the glossaries and
general dictionaries.
4.3 Syntactic similarity
Syntactic similarity is also very important es-
pecially on higher levels, in particular on the
verbal level. The differences in verbal va-
lences have negative influence on the quality
of translation due to the fact that the trans-
fer thus requires a large scale valence lexicon
for both languages, which is extremely difficult
to build. Syntactic structure of smaller con-
stituents, such as nominal and prepositional
phrases, is not that important, because it is
possible to analyze those constituents syntac-
tically using a shallow syntactic analysis and
thus it is possible to adapt locally the syntactic
structure of a target sentence.
4.4 Morphological similarity
Morphological similarity means similar struc-
ture of morphological hierarchy and paradigms
such as case system, verbal system etc. In
our understanding Baltic and Slavic languages
(except for Bulgarian and Macedonian) have
a similar case system and their verbal system
is quite similar as well. Some problems are
caused by synthetic forms, which have to be
expressed by analytical constructions in other
languages (e.g., future tense or conjunctive in
Czech and Lithuanian). The differences in
morphology can be relatively easily overcomed
by the exploitation of full-fledged morphology
of both languages (source and target).
Similar morphological systems simplify the
transfer. For example, Slavonic languages (ex-
cept of Bulgarian and Macedonian) have 6-7
94
cases. The case system of East Baltic lan-
guages is very similar, although it has been re-
duced formally in Latvian (instrumental forms
are equal as dative and accusative and the
function of instrumentral is expressed by the
preposition ar ?with?, similarly as in Upper
Sorbian). (Ambrazas, 1996) gives seven cases
for Lithuanian, but there are in fact at least
eight cases in Lithuanian (or ten cases but only
eight of them are productive3). Nevertheless
the case systems of Slavonic and East Baltic
languages are very similar which makes the
languages quite similar even across the border
of different language groups.
Significant differences occur only in the ver-
bal system, East Baltic languages have a huge
amount of participles and half-participles that
have no direct counterpart in Czech. The
Lithuanian translation of an example from
(Gamut, 1991) is given in (5):
(5) Gime?
was-born-3sg
vaikas,
child-nom
valdysiantis
ruling-fut,masc,sg,nom
pasauli?
world-acc
?A child was born which will rule the
world.? (Lit)
The participle valdysiantis is used instead
of an embedded sentence, because Lithuanian
has future participles. These participles have
to be expresses by an embedded sentence in
Slavonic languages.
5 An outline of a structural
similarity measure
In this section, we propose a comparatively
simple measure of syntactic (structural) sim-
ilarity. There are generally two levels which
may serve as a basis for such a structural mea-
sure, the surface or deep syntactic level. Let us
first explain the reasons supporting our choice
of surface syntactic level.
Compared to deep syntactic representation,
the surface syntactic trees are much more
3Although some Balticists argue that illative forms
are adverbs, it is a fact that this case is productive and
used quite often (Erika Rimkute?, personal communica-
tion), though it has been widely replaced by preposi-
tional phrases. Allative and adessive are used only in
some Lithuanian dialects, except of a few fixed allative
forms (e.g., vakarop(i) ?in the evening?, velniop(i) ?to
the hell?.)
closely related to the actual surface form of a
sentence. It is quite common that every word
form or punctuation sign is directly related to
a single node of a surface syntactic tree. The
deep syntactic trees, on the other hand, usu-
ally represent autosemantic words only, they
may even actually contain more nodes than
there are words in the input sentence (for ex-
ample, when the input sentence contains ellip-
sis). It is also quite clear that the deep syntac-
tic trees are much more closely related to the
meaning of the sentence than its original sur-
face form, therefore they may hide certain dif-
ferences between the languages concerned, it is
a generally accepted hypothesis that transfer
performed on the deep syntactic level is eas-
ier than the transfer at the surface syntactic
level, especially for syntactically and typolog-
ically less similar languages.
The second important decision we had to
make was to select the best type of surface
syntactic trees between the dependency and
phrase structure trees. For practical reasons
we have decided to use dependency trees. The
main motivation for this decision is the enor-
mous structural ambiguity of phrase structure
trees that represent sentences with identical
surface form. Let us have a look at the follow-
ing Polish sentence:
(6) Pawe l
Pawe l-nom
czyta
read-3sg
ksi ?az?k ?e
book-fem,sg,acc
?Pawe l is reading a/the book.?
The syntactic structure of this sentence can
be expressed by two phrase structure trees rep-
resenting different order of attaching nominal
phrases to a verb.4
4The full line denotes the head of the phrase, the
dotted line a dependent.
95
??
?



?



?



Pawe l czyta ksi ?az?k ?e
?
?
?



?



?



Pawe l czyta ksi ?az?k ?e
There is no linguistically relevant difference
between these two trees. Although generally
useful, the information hidden in both trees
is purely superfluous for our goal of designing
a simple structural metrics. The dependency
tree obtained from the phrase structure ones
by contraction of all head edges seem to be
much more appropriate for our purpose. In our
example, we therefore get the following form
of the dependency tree:
czyta
zzuu
u
u
u
u
u
u
u
$$J
J
J
J
J
J
J
J
J
Pawe l ksi ?az?k ?e
The nodes of the dependency trees repre-
senting surface syntactic level directly corre-
spond to word forms present in the sentence.
For the sake of simplicity, the punctuation
marks are not represented in our trees. They
would probably cause a lot of technical prob-
lems and might distort the whole similarity
measure. The node of a tree are ordered and
reflect the surface word-order of the sentence.
Different labels of nodes in both languages (see
the example below) don?t influence the value
of the measure, however they are important
for the identification of corresponding nodes
(a bilingual dictionary is used here).
The structural measure we are suggesting is
based on the analogy to the Levenshtein mea-
sure. It is therefore pretty simple ? the dis-
tance of two trees is the minimal amount of
elementary operations that transform one tree
to the other. We consider the following ele-
mentary operations:
1. adding a node,
2. removing a node,
3. changing the order of a node,
4. changing the father of a node.
The similarity of languages can be obtained
as an average distance of individual sentences
in a parallel corpus.
The following examples show the use of the
measure on individual trees. The correspon-
dence between individual nodes of both trees
can be handled by exploiting the bilingual dic-
tionary wherever necessary:
(7) Vesna
Vesna-nom
je
is-3sg
pri?sla
come-respart,fem,sg
?Vesna has come.? (Slo)
(8) Vesna
Vesna-nom
przysz la
come-respart,fem,sg
?Vesna has come.? (Pol)
The distance between (7) and (8) is equal 1,
since one node has been removed (the dotted
line gives the removed node).
pri?sla/przysz la
ttjj
jj
jj
jj
jj
jj
jj
jj
xx
Vesna je
(9) Grem
go-1sg
z
with
avtom
car-masc,sg,ins
?I am going by car.? (Slo)
(10) Jad ?e
go-1sg
samochodem
car-masc,sg,ins
?I am going by car.? (Pol)
96
The distance between (9) and (10) is equal
1, since one node has been removed (the dotted
line gives the removed node).
grem/jad ?e
**UU
UU
UU
UU
UU
UU
UU
UU
U
avtom/samochodem
wwz
5.1 Formalization
(11) On
he-nom
ra?d
with-pleasure
plave
swims-3sg
?He likes swimming.? (Cze)
plave
uujj
jj
jj
jj
jj
jj
jj
jj
jj
j
yy










on ra?d
likes
|| %%
he 11 swimming
The Czech-English example (11) shows two
sentences which have a mutual distance equal
to 3 ? if we start changing the Czech tree
into an English one, then the first elemen-
tary operation is the deletion of the node ra?d,
the second operation adds the new node cor-
responding to the English word likes and the
third and last operation is the change of the
father of the node corresponding to the per-
sonal pronoun on [he] from swimming to likes.
As mentioned above, the node labels are not
taken into account, the fact that the Czech fi-
nite verbal form plave changes into an English
gerund has no effect on the distance.
A similar case are sentences with a dative
agent, for example:
(12) Je
is
mi
me-dat
zima
cold-f,sg,nom
?I am cold? (Cze)
In this sentence, the Czech mi does not
match to I since it is no subject. Similarly,
the substantive zima does not match to cold,
since it is a different part of speech. Hence
two nodes are removed and two new nodes
are added, which gives us a distance of 4.
This example demonstrates that the measure
tends to behave naturally - even short sen-
tences containing syntactically different con-
structions get a relatively high score.
To formalize the process described above, let
us introduce a notion of lexical and analytical
equality of nodes in analytical trees:
? Two nodes equal lexically if and only if
they share the same meaning in the given
context. Nevertheless to simplify auto-
matic processing, we treat two nodes as
lexically equal if they share a particular
meaning (defined e.g. as a non-empty in-
tersection of Wordnet classes).
? Two nodes equal analytically if and only
if they have the same analytical label (e.g.
subject, spacial adverbial etc.).
As for the measure, two nodes match to each
other if they 1) occur at the same position in
the subtree of their parent and 2) equal lexi-
cally and analytically.
If a subtree (greater than 1) is added or re-
moved, the operation contributes to the mea-
sure with the size of the subtree (the amount
of its nodes), for example in the following id-
iomatic phrase:
(13) pus?cic?
leave-inf
z
with
dymem
smoke-masc,sg,ins
?burn down? (Pol)
(14) zapa?lit
burn-down-inf
?burn down? (Cze)
In the above example, the distance is
equal 2.
The automatic procedure can be described
as follows (given two trees):
1. Align all sons of the root node.
2. Count discrepancies.
3. For all matched nodes, go to step 1 to
process subtrees and sum up distances.
97
5.2 Discussion
It is obvious that our measure expresses the ty-
pological similarity of languages. We get com-
paratively high values even for genetically re-
lated languages if their typology is different.
Let us demonstrate this fact on Czech and
Macedonian examples.
(15) Ivan
Ivan-nom
dal
gave-respart,masc,sg
knihu
book-fem.sg,acc
Stojanovi
Stojan-dat
?Ivan gave the book to Stojan.? (Cze)
dal
||y
y
y
y
y
y
y
y
##F
F
F
F
F
F
F
F
F
**T
TT
TT
TT
TT
TT
TT
TT
TT
T
Ivan knihu Stojanovi
(16) Ivan
Ivan-nom
mu
him
ja
her-fem,sg,acc
ima
has-3sg
dadeno
given-ppart,neut,sg
knigata
book-fem.sg,def
na
on
Stojan
Stojan
?Ivan gave the book to Stojan.? (Mac)
The distance equals 5. The score is rela-
tively high, taken into account that both lan-
guages are related. It indicates again that for
a given purpose the measure seems to provide
consistent results.
The proposed measure takes into account
only the structure of the trees, completely ig-
noring node and edge labels. Let us analyze
the following example:
(17) Ta
this-fem,sg,nom
ksi ?az?ka
book-fem.sg,nom
si ?e
REFL
cz ?esto
well
czyta
read-3sg
?This book is read often.?
(18) T?e
this-fem,sg,acc
ksi ?az?k ?e
book-fem.sg,acc
si ?e
REFL
cz ?esto
well
czyta
read-3sg
?This book is read often.?
The syntactic trees of both sentences have
the same structure, but (17) is passive and
(18) active (with a general subject). This is
of course a significant difference and as such
it should be captured in the measure, never-
theless our simple measure doesn?t reflect it.
There are several reasons why a current ver-
sion of the measure doesn?t include morpho-
logical and morphosyntactic labels. One of the
reasons is a different nature of the problem ?
to design a reliable measure combining struc-
tural information with the information con-
tained in node labels is very difficult. From the
technical point of view, a great obstacle is also
the variety of systems of tags used for this pur-
pose for individual languages, which may not
be compatible. For example, Macedonian has
almost no cases at nouns, therefore it would
make no sense to use cases in the noun anno-
tation, while for other Slavic languages (and
not only for Slavic ones) is this information
very important. To find a good integration of
morphosyntactic features into the structural
measure is definitely a very interesting topic
for future research.
6 Conclusions
This paper contains an outline of a simple lan-
guage similarity measure based upon the sur-
face syntactic dependency trees. According to
our opinion, such a measure expresses more
adequately the similarity of languages than
simple string-based measures used for the text
similarity. The measure is defined on pairs of
trees from a parallel corpus. In its current
form it doesn?t account for differences in mor-
phosyntactic labels of corresponding nodes or
edges, although it is an important parameter
of language similarity. The proper combina-
tion of our basic structural similarity measure
with some measure reflecting the differences of
labels opens a wide range of options for a fu-
ture research. Equally important seems to be
a task of gathering properly syntactically an-
notated parallel corpora of a reasonable size.
The only corpus of such kind which we have
at our disposal, the Prague Czech-English De-
pendency Treebank (Cur???n et al, 2004) re-
lies on imperfect automatic annotation which
might distort the results. The human annota-
tion of the PCEDT is just starting, so there?s a
98
dadeno
rrffff
fff
fff
fff
fff
fff
fff
fff
fff
fff
f
ss uu
zz %%
L
L
L
L
L
L
L
L
L
L
++XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
XX
X
Ivan mu ja ima knigata Stojan
{{
na
Figure 1: The dependency tree of (16)
good chance that the measure will bring some
reliable results at least for those two lenguages
soon.
7 Acknowledgements
This research was supported by the Min-
istry of Education of the Czech Repub-
lic, project MSM0021620838, by the grant
No. GAUK 351/2005 and by the grant
No. 1ET100300517. We would like to thank
the anonymous reviewers for their valuable
comments and recommendations.
References
Vytautas Ambrazas. 1996. Dabartine?s lietuviu? kal-
bos gramatika. Mokslo ir enciklopediju? leidykla,
Vilnius.
Jan Cur???n, Martin C?mejrek, Ji?r?? Havelka, Jan Ha-
jic?, Vladislav Kubon?, and Zdene?k Z?abokrtsky?.
2004. Prague Czech-English Dependency Tree-
bank Version 1.0. Linguistic Data Consortium.
LTF Gamut. 1991. Login, loanguage and meaning
2: Intensional logic and logical grammar. Uni-
versity of Chicago Press, Chicago.
Jan Hajic?, Petr Homola, and Vladislav Kubon?.
2003. A simple multilinguale machine transla-
tion system. In Proceedings of the MT Summit
IX, New Orleans.
Petr Homola and Vladislav Kubon?. 2004. A trans-
lation model for languages of accessing coun-
tries. In Proceedings of the 9th EAMT Work-
shop, La Valetta, Malta.
Eduard Hovy, Margaret King, and Andrei
Popescu-Beli. 2002. Principles of Context-
Based Machine Translation Evaluation. Ma-
chine Translation, 1(17).
Vladislav Kubon? and Alevtina Be?mova?. 1990.
Czech-to-Russian Transducing Dictionary. In
Proceedings of the XIIIth conference COLING
?90, volume 3.
Ludovic Lebart and Martin Rajman, 2000. Hand-
book of Natural Language Processing, chapter
Computing similarity. Dekker, New York.
Nist. 2001. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. Technical report, NIST.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for
Automatic Evaluation of Machine Translation.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
Philadelphia.
Kevin P. Scannell. 2004. Cor-
pus building for minority languages.
http://borel.slu.edu/crubadan/index.html.
99
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards the Automatic Extraction of Definitions in Slavic
1Adam Przepio?rkowski
2?ukasz Dego?rski
8Beata Wo?jtowicz
Institute of Computer Science PAS
Ordona 21, Warsaw, Poland
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
4Kiril Simov
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
kivs@bultreebank.org
petya@bultreebank.org
3Miroslav Spousta
7Vladislav Kubon?
Charles University
Malostranske? na?me?st?? 25
Prague, Czech Republic
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
6Lothar Lemnitzer
University of Tu?bingen
Wilhelmstr. 19, Tu?bingen, Germany
lothar@sfs.uni-tuebingen.de
Abstract
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
1 Introduction
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
43
2 Related Work
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malaise? et al, 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malaise? et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Droz?dz?yn?ski et al, 2003), dis-
cusses shallow SProUT (Becker et al, 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al, 2004; Marciniak et al, 2005).1
3 Shallow Grammars for Definition
Extraction
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
k?adzie nacisk na (Eng. ?Constructivism puts em-
phasis on?) may be as follows:2
<s id="s9">
<markedTerm id="mt7" kw="y">
<tok base="konstruktywizm" ctag="subst"
id="t253"
msd="sg:nom:m3">Konstruktywizm</tok>
</markedTerm>
<tok base="klasc" ctag="fin" id="t254"
msd="sg:ter:imperf">kladzie</tok>
<tok base="nacisk" ctag="subst" id="t255"
msd="sg:acc:m3">nacisk</tok>
<tok base="na" ctag="prep" id="t256"
msd="acc">na</tok>
[...]
<tok base="." ctag="interp" id="t273">.
</tok>
</s>
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
<rule name="PP">
<seq>
<query match="tok[@ctag = ?prep?]"/>
<ref name="NP1">
<with-param name="case" value="??"/>
</ref>
</seq>
</rule>
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
2Part of the representation has been replaced by ?[...]?.
3Among the tools considered here were also CLaRK (Simov
et al, 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al,
2002), not used here because we found GATE?s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
44
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
3.1 Bulgarian
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
Pattern # %
NP is NP 140 34.2
NP verb NP 18 29.8
NP - NP 21 5.0
This is NP 15 3.7
It represents NP 4 1.0
other patterns 107 26.2
Table 1: Bulgarian definition types
In the second schema above, ?verb? is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ?????????????? (to repre-
sent), ????????? (to show), ?????????? (to mean),
???????? (to describe), ??? ????????? (to be used),
??????????? (to allow), ????? ?????????? ???
(to give opportunity), ??? ??????? (is called),
??????????? (to improve), ??????????? (to ensure),
?????? ??? (to serve as), ??? ???????? (to be under-
stood as), ???????????? (to denote), ????????? (to
contain), ?????????? (to determine), ?????????
(to include), ??? ???????? ????? (is defined as),
??? ???????? ??? (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
3.2 Czech
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
Pattern # %
NP is/are NP 52 21.2
NP verb NP 45 18.4
structural 39 15.9
NP (NP) 30 12.2
NP -/:/= NP 20 8.2
other patterns 59 24.1
Table 2: Czech definition types
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (?is/are?) category. The sec-
5The grammar was originally developed by Nguyen Thu
Trang.
45
ond most successful rule is the one using selected
verbs like ?definuje? (defines), ?znamen?? (means),
?vymezuje? (delimits), ?pr?edstavuje? (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
3.3 Polish
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may
describe/define as NP-ACC 5 2.0
other patterns
(not in the grammar) 94 36.7
Table 3: Polish definition types
The middle layer consists of rules catching pat-
terns such as ?simple NP in given case, followed by
a sequence of non-punctuation elements? or ?cop-
ula?.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
4 Results
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
7In general, F? = (1 + ?) ? (precision ? recall)/(? ?
precision+recall). Perhaps? larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
46
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
Table 4: Token-based evaluation of shallow gram-
mars
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
5 Evaluation and Possible Improvements
5.1 Interannotator Agreement
We calculated Cohen?s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
? =
Pr(a) ? Pr(e)
1 ? Pr(e)
(1)
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
Pr(e) = p1 ? p2 + (1 ? p1) ? (1 ? p2) (2)
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where ? for
each language ? the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of ?. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
5.2 Evaluation Methodology
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call ? as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
Table 5: Definition-based evaluation of shallow
grammars
5.3 Definitions and Sentence Boundaries
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
47
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
<par id="d1p2">
<s id="d1p2s1">
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
</s>
</par>
becomes:
<par id="Sd1p2"/>
<s id="Sd1p2s1"/>
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
<s id="Ed1p2s1"/>
<par id="Ed1p2"/>
11An example of a Polish manually annotated multi-sentence
definition is: . . . opracowano techniki antyspamowe. Tech-
niki te drastycznie zaniz?aja? wartos?c? strony albo ja? banuja?. . .
(Eng. ?. . . anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it. . . ?).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te. . . .
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
5.4 Problems Specific to Slavic
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like ?NP verb NP?,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
5.5 Levels of Linguistic Processing
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
48
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, ? 14.2); the
best of these parsers reach 80?85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Wolin?ski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al, 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
5.6 Postprocessing: Machine Learning and
Keyword Identification
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high ?probably due to the fact that the
current corpus consists of encyclopedic material only? (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
6 Conclusion
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
References
Markus Becker et al 2002. SProUT ? shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18?25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
49
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
V?ronique Malais?, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55?62, Geneva, Switzerland. COL-
ING.
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al, editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI 2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68?78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89?100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360?1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al, editors, Proceedings of the
Seventh Euralex International Congress, pp. 817?824,
G?teborg.
Jakub Piskorski et al 2004. Information extraction for
Polish using the SProUT platform. In M. A. K?opotek
et al, editors, Intelligent Information Processing and
Web Mining, pp. 227?236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517?526.
Kiril Simov et al 2001. CLaRK ? an XML-based sys-
tem for corpora development. In P. Rayson et al, edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558?560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20?28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
50
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 33?36,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Simple Automatic MT Evaluation Metric
Petr Homola
Charles University
Prague, Czech Republic
Vladislav Kubon?
Charles University
Prague, Czech Republic
{homola|vk|pecina}@ufal.mff.cuni.cz
Pavel Pecina
Charles University
Prague, Czech Republic
Abstract
This paper describes a simple evaluation
metric for MT which attempts to overcome
the well-known deficits of the standard
BLEU metric from a slightly different an-
gle. It employes Levenshtein?s edit dis-
tance for establishing alignment between
the MT output and the reference transla-
tion in order to reflect the morphological
properties of highly inflected languages. It
also incorporates a very simple measure
expressing the differences in the word or-
der. The paper also includes evaluation on
the data from the previous SMT workshop
for several language pairs.
1 Introduction
The problem of finding a reliable machine trans-
lation metrics corresponding with a human judg-
ment has recently returned to the centre of atten-
tion. After a brief period following the introduc-
tion of generally accepted and widely used met-
rics, BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002), when it seemed that this persistent
problem has finally been solved, the researchers
active in the field of machine translation (MT)
started to express their worries that although these
metrics are simple, fast and able to provide con-
sistent results for a particular system during its de-
velopment, they are not sufficiently reliable for the
comparison of different systems or different lan-
guage pairs.
The results of the NIST evaluation in 2005
(Le and Przybocki, 2005) have also strengthened
the suspicion that the correlation between human
judgment and the BLEU and NIST measures is not
as strong as it was widely believed. Both mea-
sures seem to favor the MT output created by sys-
tems based on n-gram architecture, they are un-
able to take into account certain factors which are
very important for the human judges of translation
quality.
The article (Callison-Burch et al, 2006) thor-
oughly discusses the deficits of the BLEU and
similar metrics. The authors claim that the existing
automatic metrics, including some of the new and
seemingly more reliable ones as e.g. Meteor (cf.
(Banerjee and Lavie, 2005)) ?. . . they are all quite
rough measures of translation similarity, and have
inexact models of allowable variation in transla-
tion.? This claim is supported by a construction of
translation variations which have identical BLEU
score, but which are very different for a human
judge. The authors identify three prominent fac-
tors which contribute to the inadequacy of BLEU ?
the failure to deal with synonyms and paraphrases,
no penalties for missing content, and the crudeness
of the brevity penalty.
Let us add some more factors based on our ex-
periments with languages typologically different
than English, Arabic or Chinese, which are prob-
ably the languages most frequently used in recent
shared-task MT evaluations. The highly inflected
languages and languages with a higher degree of
word-order freedom may provide additional ex-
amples of sentences in which relatively small al-
terations of correct word forms may have a dire
effect on the BLEU score while the sentence still
remains understandable and acceptable for human
evaluators.
The effect of rich inflection has been observed
for example in (Ty?novsky?, 2007), where the au-
thor mentions the fact that the BLEU score used
for measuring the improvements in his experimen-
tal Czech-German EBMT system penalized heav-
ily all subtle errors in Czech morphology arising
from an out-of-context combined partial transla-
tions taken from different examples.
The problem of the insensitivity of BLEU to the
variations of the order of n-grams identified in ref-
erence translations has already been mentioned in
33
the paper (Callison-Burch et al, 2006). The au-
thors showed examples where changing a good
word order into an unacceptable one did not af-
fect the BLEU score. We may add a different ex-
ample documenting the phenomenon that a pair
of syntactically correct Czech sentences with the
same word forms, differing only in the word order
whose n-gram score for n = 2, 3, and 4 differs
greatly. Let us take one of the sentences from the
2008 SMT workshop and its reference translation:
When Caligula appointed his horse to the Sen-
ate, the horse at least did not have blood on its
hoofs. ? Kdyz? Caligula zvolil do sena?tu sve?ho
kone?, neme?l jeho ku?n? aspon? na kopytech krev.
If we modify the Czech reference sentence into
Kdyz? sve?ho kone? do sena?tu zvolil Caligula, jeho
ku?n? aspon? neme?l na kopytech krev., we destroy 8
out of 15 bigrams, 11 out of 14 trigrams and 12
out of 13 quadrigrams while we still have sentence
with almost identical meaning and probably very
similar human evaluation. The BLEU score of the
modified sentence is, however, lower than it would
be for the identical copy of the reference transla-
tion.
2 The description of the proposed metric
There is one aspect of the problem of a MT
quality metric which tends to be overlooked but
which is very important from the practical point
of view. This aspect concerns the expected diffi-
culties when post-editing the MT output. It is very
important for everybody who really wants to use
the MT output and who faces the decision whether
it is better to post-edit the MT output or whether a
new translation made by human translators would
be faster and more efficient way towards the de-
sired quality. It is no wonder that such a met-
ric is mentioned only in connection with systems
which really aim at practical exploitation, not with
a majority of experimental MT system which will
hardly ever reach the stage of industrial exploita-
tion.
We have described one example of such practi-
cally oriented metric in (Hajic? et al, 2003). The
metric exploits the matching algorithm of Trados
Translator?s Workbench for obtaining the percent-
age of differences between the MT output and the
reference translation (created by post-editing the
MT output). The advantage of this measure is its
close connection to the real world of human trans-
lating by means of translation memory, the disad-
vantage concerns the use of a proprietary match-
ing algorithm which has not been made public and
which requires the actual use of the Trados soft-
ware.
Nevertheless, the matching algorithm of Trados
gives results which to a great extent correspond
to a much simpler traditional metric, to the Lev-
enshtein?s edit distance. The use of this metric
may help to refine a very strict treatment of word-
form differences by BLEU. A similar approach at
the level of unigram matching has been used by
the well-known METEOR metric (Agarwal and
Lavie, 2008), which proved its qualities during the
previous MT evaluation task in 2008 (Callison-
Burch et al, 2008). Meteor uses Porter stemmer
as one step in the word alignment algorithm. It
also relies on synonymy relations in WordNet.
When designing our metric, we have decided to
follow two general strategies ? to use as simple
means as possible and to avoid using any language
dependent tools or resources. Levenshtein metric
(or its modification for word-level edit distance)
therefore seemed to be the best candidate for sev-
eral aspects of the proposed measure.
The first aspect we have decided to include was
the inflection. The edit distance has one advan-
tage over the language independent stemmer ? it
can uniformly handle the differences regardless of
their position in the string. The stemmer will prob-
ably face certain problems with changes inside the
stem as e.g. in the Czech equivalent of the word
house in different cases du?m (nom.sg) ? domu
(gen., dat. or loc. sg.) or German Mann in differ-
ent numbers der Mann (sg.) ? die Ma?nner (pl.),
while the edit distance will treat them uniformly
with the variation of prefixes, suffixes and infixes.
As mentioned above, we have also intended to
aim at the treatment of the free word order in our
metric. However this seems to be one of the ma-
jor flaws of the BLEU score, it turned out that the
word order is extremely difficult if we stick to the
use of simple and language independent means. If
we take Czech as an example of a language with
relatively high degree of word-order freedom, we
can still find certain restrictions (e.g. the sentence-
second position of clitics, their mutual order, the
adjectives typically, but not always preceding the
nouns they depend upon etc.) which will defi-
nitely influence the human judgment of the accept-
ability of a particular sentence. These restrictions
are language dependent (for example Polish, the
34
language very closely related to Czech, has dif-
ferent rules for congruent attributes, the adjectives
stand much more often to the right of the govern-
ing noun) and they are also very difficult to capture
algorithmically. If the MT output is compared to
a single reference translation only, there is, in fact,
no way how the metric could account for the pos-
sible correct variations of the word order without
exploiting very deep language dependent informa-
tion. If there are more reference translations, it is
possible that they will provide the natural varia-
tions of the word order, but it, in fact, means that
if we want to stick to the above mentioned require-
ments, we have to give up the hope that our metric
will capture this important phenomenon.
2.1 Word alignment algorithm
In order to capture the word form variations
caused by the inflection, we have decided to em-
ploy the following alignment algorithm at the level
of individual word forms. Let us use the follow-
ing notation: Let the reference translation R be a
sequence of words ri, where i ?< 1, . . . , n >.
Let the MT output T be a sequence of words tj,
where j ?< 1, . . . ,m >. Let us also set a thresh-
old of similarity s ?< 0, 1 >. (s roughly ex-
presses how different the forms of a lemma may
be. The idea behind this criterion is that a mistake
in one morphological category (reflected mostly
by a different ending of the corresponding word
form) is not as serious as a completely different
lexeme. This holds especially for morphologically
rich languages that can have tens or even hun-
dreds of distinct word forms for a single lemma.)
Starting from t1, let us find for each tj the best
ri for i ?< 1, . . . , n > such that the edit dis-
tance dj from tj to ri normalized by the length
of tj is minimal and at the same time dj < s.
If the ri is already aligned to some tk, k < j
and the edit distance dk > dj , then align tj to
ri and re-calculate the alignment for tk to its sec-
ond best candidate, otherwise take the second best
candidate rl conforming with the above mentioned
conditions and align it to tj . As a result of this
process, we get the alignment score ATR from T
to R. ATR =
?
(1?di)
m (for i ?< 1, . . . , n >)
where di = 1 for those word forms ti which are
not aligned to any of the word forms rj from R.
Then we calculate the alignment score ART using
the same algorithm and aligning the words from R
to T. The similarity score S equals the minimum
from ATR and ART . The way how the similar-
ity score S is constructed ensures that the score
takes into account a difference in length between
T and R, therefore it is not necessary to include
any brevity penalty into the metric.
2.2 A structural metric
In order to express word-order difference between
the MT output and the reference translation we
have designed a structural part of the metric. It
is based on an algorithm similar to one of the stan-
dard sorting methods, an insert sort. The refer-
ence translation R represents the desired word or-
der and the algorithm counts the number of op-
erations necessary for obtaining the correct word
order from the word order of the MT output T by
inserting the words ti to their desired positions rj
(ti is aligned to rj). If a particular word ti is not
aligned to any rj , a penalty of 1 is added to the
number of operations.
2.3 A combination of both metrics
The overall score is computed as a weighted aver-
age of both metrics mentioned above. Let L be the
lexical similarity score and M the structural score
based on a word mapping. Then then overall score
S can be obtained as follows:
S = aL+ bM
The coefficients a and b must sum up to one.
They allow to capture the difference in the degree
of word-order freedom among target languages.
The coefficient b should be set lower for the tar-
get languages with more free word-order. Because
both then partial measures L andM have values in
the interval < 0, 1 >, the value of S will also fall
into this interval.
3 The experiment
We have performed a test of the proposed met-
ric using the data from the last year?s SMT work-
shop.1 The parameters a, b, and s have been set to
the same value for all evaluated language pairs, no
language dependent alterations were tested in this
experiment:
Parameter Value
s 0.15
a 0.9
b 0.1
1The data are available at http://www.statmt.org/wmt08.
35
The values for the parameters have been set up
empirically with special attention being paid to
Czech, the only language with really rich inflec-
tion among the languages being tested.
We have performed sentence-level and system-
level evaluation using the Spearman?s rank corre-
lation coefficient which is defined as follows:
? = 1?
6
?
d2i
n(n2 ? 1)
where di = xi?yi is the difference between the
ranks of corresponding values Xi and Yi and n is
the number of values in each data set.
The following scores express the correlation of
our automatic metric and the human judgements
for the language pairs English-Czech and English-
German. The sentence-level correlation ?sent is
the average of Spearman?s ? across all sentences.
Language pair Metric ?sent ?sys
English-Czech proposed 0.20 0.50
English-Czech BLEU 0.21 0.50
English-German proposed 0.91 0.37
English-German BLEU 0.90 0.20
3.1 Conclusions
The metric presented in this paper attempts to
combine some of the important factors which
seem to be neglected by some generally accepted
MT evaluation metrics. Inspired by the fact that
human judges tend to accept incorrect word-forms
of corectly translated lemmas, it employs a simi-
larity measure relaxing the requirements on iden-
tity (or similarity) of matching word forms in the
MT output and the reference translation. At the
same time, it also incorporates a penalty for dif-
ferent length of the MT output and the reference
translation. The second component of the metric
tackles the problem of incorrect word-order. The
constants used in the metric allow to set the weight
of its two components with regard to the target lan-
guage properties.
The experiments performed on the data from
the previous shared evaluation task are promising.
They indicate that the first component of the met-
ric succesfully replaces the strict unigram mea-
sure used in BLEU while the second component
may require certain alteration in order to achieve a
higher correlation with human judgement.
Acknowledgments
The presented research has been supported by the
grant No. 1ET100300517 of the GAAV C?R and
by Ministry of Education of the Czech Republic,
project MSM 0021620838.
References
Abhaya Agarwal and Alon Lavie. 2008. Meteor,
M-BLEU and M-TER: Evaluation metrics for high
correlation with human rankings of machine trans-
lation output. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 115-
118. Columbus, Ohio, Association for Computa-
tional Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments.. In Workshop
on Intrinsic and Extrinsic Evaluation Measures for
MT and/or Summarization, Ann Arbor, Michigan.
Chris Callison-Burch, Miles Osborne, Philipp Koehn.
2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research.. In Proceedings of the
EACL?06, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation..
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70-106, Columbus,
Ohio. Association for Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, San Diego, California,USA
Jan Hajic?, Petr Homola, Vladislav Kubon?. 2003. A
Simple Multilingual Machine Translation System..
In Proceedings of the MT Summit IX, New Orleans,
USA.
Kishore Papineni, Salim Roukos, ToddWard, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation.. In Proceedings of
ACL 2002.
Audrey Le and Mark Przybocki. 2005. NIST 2005
machine translation evaluation official results.. Of-
ficial release of automatic evaluation scores for all
submissions.
Miroslav Ty?novsky?. 2007. Exploitation of Linguis-
tic Information in EBMT.. Master thesis at Charles
University in Prague, Faculty of Mathematics and
Physics.
36
Problems Of Reusing An Existing MT System?
Ondr?ej Bojar, Petr Homola, Vladislav Kubon?
Institute of Formal and Applied Linguistics
?UFAL MFF UK, Malostranske? na?me?st?? 25, Praha 1, CZ-11800
Czech Republic
{bojar,homola,vk}@ufal.mff.cuni.cz
Abstract
This paper describes an attempt to recy-
cle parts of the Czech-to-Russian ma-
chine translation system (MT) in the
new Czech-to-English MT system. The
paper describes the overall architecture
of the new system and the details of
the modules which have been added.
A special attention is paid to the prob-
lem of named entity recognition and
to the method of automatic acquisition
of lexico-syntactic information for the
bilingual dictionary of the system. The
paper concentrates on the problems en-
countered in the process of reusing ex-
isting modules and their solution.
1 Introduction
The last decade has witnessed several attempts to
increase the quality of MT systems by introduc-
ing new methods. The strong stress on stochastic
methods in the NLP in general and in the MT in
particular, the attempts to develop hybrid systems,
a wide acceptance of translation-memory based
systems among the translation professionals, the
aim at limited domain speech-to-speech transla-
tion systems, all these (and many other) trends
have demonstrated encouraging results in recent
years.
Developing and using new methods definitely
moves the whole MT field forward, but one
?The work described in this paper has been supported by
the grant of the Grant Agency of the Czech Republic GACR
No.405/03/0914 and partially also by the grant of the Grant
Agency of the Charles University GAUK No. 351/2005
should not forget about all the effort invested into
the old systems. Reusing at least some parts of
those systems may help to decrease the costs of
new systems, especially when one of the lan-
guages is not a ?big? language and therefore there
is not such a wide range of tools, grammars, dic-
tionaries available as for example for English,
German, Japanese or Spanish. In this paper we
would like to describe one such attempt to reuse
the existing system for a new language pair.
2 The original system
One of the systems which was silently abandoned
in early nineties was the system for the translation
from Czech to Russian called RUSLAN (Oliva,
1989). It was being developed in the second half
of eighties with the aim to translate texts from a
relatively closed thematic domain, the domain of
operating systems of mainframes.
The system used transfer-based architecture.
The implementation of the system was almost
completely done in Q-systems, a formalism cre-
ated by Alain Colmerauer (Colmerauer, 1969)
for the TAUM-METEO project. The Czech-to
Russian system also relied upon a set of dictio-
naries containing all data exploited by individ-
ual modules of the system. Each lexical item
in the main (bilingual) dictionary contained not
only lexico-syntactic data (valency frames etc.),
but also a set of semantic features.
The work on the system RUSLAN has been ter-
minated in 1990, in the final phase of system test-
ing and debugging. The reason was quite sim-
ple - after the political changes in 1989 there was
no more any commercial demand for Czech to
179
Russian MT system.
The demand for Czech-English translation has
grown dramatically during the years following the
abandonment of the system RUSLAN. On the
other hand, also the range of methods, tools and
resources for MT has grown substantially. Sev-
eral corpora were created for Czech, the most
prominent ones being the morphologically anno-
tated Czech National Corpus and syntactically an-
notated Prague Dependency Treebank. In 2002
we have started the work on the parallel bilin-
gual Prague Czech English Dependency Treebank
(PCEDT) (Cur???n et al, 2004), which contains
about a half of the texts from PennTreebank 3
translated into Czech by native speakers. A large
morphological dictionary of Czech has been de-
veloped (Hajic?, 2001), allowing for a good quality
morphological analysis of Czech, which has been
tested in numerous commercial applications and
scientific projects since then.
3 The background of the project
The main motivation for our Czech-English MT
experiment was to test several hypotheses. The
most prominent of these hypotheses concerns the
level, at which it is reasonable to perform the
transfer. Due to the differences between both lan-
guages it is not sufficient to perform the transfer
immediately after the morphological analysis or
shallow parsing, as it has been done in the MT
system eslko aiming at the translation between
closely related (and similar) languages [cf (Hajic?
et al, 2003)]. On the other hand, it is a ques-
tion whether the typological differences between
Czech and English justify the transfer being per-
formed at the tectogrammatical (deep syntactic)
level.
Last but not least, one of our aims was to de-
velop a rule-based MT system with minimal pos-
sible costs, either reusing the existing modules or
trying to use (semi)automatic methods whenever
possible, concentrating on areas where using the
human labor would be extremely expensive (for
example building a large coverage bilingual dic-
tionary, cf. the following paragraphs.)
4 Czech-English MT system
The main goal of our project is to develop an ex-
perimental MT system for the translation of texts
from the PCEDT from Czech to English. The sys-
tem investigates the possibility of reusing the ex-
isting resources (grammar, dictionary) in order to
decrease the development time. It also exploits
the parallel bilingual corpus of syntactically anno-
tated texts, although not as a direct learning ma-
terial, more like an additional source of linguis-
tic data especially for the dictionary development
and for the testing of the system.
The task is complicated by the fact that this
translation direction is according to our opinion
more complicated than the reverse one. There are
several reasons for this claim; the most prominent
one is the free word-order nature of the source
language. It generally means that it is very of-
ten necessary to make substantial changes of the
word order if we want to get a grammatical Eng-
lish sentence, while when translating from Eng-
lish to Czech the results are more or less gram-
matically correct and comprehensible even if we
don?t change the word order at all.
Another problem of the Czech-English transla-
tion is the insertion of articles. Czech doesn?t use
any articles and it is of course much easier to re-
move them from the text (when translating from
English) than to insert a proper article on a proper
place (when translating from Czech).
Let us now look at the individual modules of
the new system.
4.1 Morphological analysis
Due to the limited size of the original morpho-
syntactic dictionary of the system it was neces-
sary to replace the original module by a new one.
The new module of morphological analysis of
Czech (Hajic?, 2001) has been already exploited in
numerous applications. It covers almost the entire
Czech language, with very few exceptions (it is
estimated that it contains about 800 000 lemmas).
It is very reliable, due to a really large coverage
there are almost no unknown words in the whole
PCEDT. The only problem was the incorporation
of the new module into the system - the original
module of syntactic analysis of Czech from the
system RUSLAN was very closely bound to a dic-
tionary lookup and to the morphological module.
The new module also uses a different tagset.
180
4.2 Bilingual dictionary
The bilingual dictionary of the system RUSLAN
contained approximately 8000 lexical items with
a rich lexico-syntactic information. We have orig-
inally assumed that the information contained in
the dictionary might be transformed and reused in
the new system, but this assumption turned to be
false. Although the information contained in the
original bilingual dictionary is extremely valuable
for the module of syntactic analysis of Czech, we
have decided to sacrifice it. The mere 8000 lex-
ical items constitute too small part of the new
bilingual dictionary and we have decided to prefer
handling the dictionary in a uniform way.
At the moment there are no Czech-English dic-
tionaries exploitable in an MT system. The avail-
able machine-readable dictionaries built mainly
for a human user (such as WinGED1 or
Svoboda (2001)) suffer from important limita-
tions:
? Sometimes, several variants of translation
are combined in one entry2.
? No clear annotation of meta-language is
present, although the entries contain valu-
able morphological or syntactic information
to some extent. (E.g. valency frames are
encoded by means of rather inconsistent ab-
breviations in plain text: accession to = vs-
toupen?? do or adjudge sb. to be guilty = uz-
nat vinny?m koho.)
? Usually, no morphological information is
given along the entries, although the mor-
phological information can be vital for cor-
rectly recognizing an occurrence of the entry
in a text. For example, an expression kniha
u?c?etn?? can be translated as either an account-
ing book or a book of an accountant depend-
ing whether the Czech word u?c?etn?? is an ad-
jective or a noun.
? No syntactic information is available and no
consistent rules have been adopted by the
1http://www.rewin.cz/
2Throughout the text, we use the term ENTRY as a syn-
onym to translation pair, i.e. a pair of Czech and English
expressions.
lexicographers to annotate syntactic proper-
ties in plain text (such as putting the head of
the clause as the first word).
From the point of view of structural machine
translation, the lack of syntactic information in
the translation dictionary is crucial. In the course
of translation, the input sentence is syntactically
analyzed before searching for foreign language
equivalents. In order to check for presence of
multi-word expressions in the input, the dictio-
nary must encode the structural shape of such en-
tries, otherwise the system does not know how to
traverse the relevant part of the tree. Similarly,
some expressions require some constraints to be
met (such as an agreement in case or number) in
the input text. If these constraints are not fulfilled,
the proposed foreign language equivalent is not
applicable.
The importance of valency (subcategorization)
frames and their equivalents should be stressed,
too. In the described system, already the syntac-
tic analyzer requires verb and adjective valency
frames in order to allow for specific syntactic con-
structions. In general, knowledge of translation
equivalents of valencies is important to preserve
the meaning (pr?ij??t na ne?jaky? na?pad = come at an
idea, literal translation: come on an idea; chodit
na housle = attend violin lessons, lit. walk on vi-
olin) or to handle auxiliary words properly (c?ekat
na ne?hoko = wait for somebody, lit. wait on sb.;
r???ci ne?co = tell something but pr?ejet ne?co = run
over something).
4.2.1 Dictionary cleanup
In order to handle the problems mentioned
above, we performed an extensive cleanup of the
data from available machine-readable dictionar-
ies. The core steps of the cleanup are as follows:
Identifying meta-information.
We manually processed all the entries and
searched for frequent words that typically encode
some meta-information, such as sth., st., oneself.
We also checked all entries ending with a word
that is potentially a preposition. Based on the ex-
pression in the other language, we were able to
recognize the meaning and identify, whether the
suspicious word expresses a ?slot? in the expres-
sion or whether it is a fixed part of the expression.
(E.g. m??t o sobe? vysoke? m??ne?n?? = think something
181
of oneself, only the word oneself encodes a slot,
the word something is a fixed part of the expres-
sion.)
During this phase, entries encoding several
translation variants at once were disassembled
into separate translation pairs, too.
Part-of-speech disambiguation.
We processed the Czech part of each entry with
a morphological analyzer (Hajic?, 2001) and we
performed manual part-of-speech disambiguation
of expressions with ambiguity. It should be noted
that automatic tagging would not provide us with
satisfactory results due to the lack of sentential
context around the expressions.
Adding morphological constraints.
Morphological constraints on word entries de-
scribe which values of morphological features are
valid for each word of the entry or have to be
shared among some words of the entry. Once
identified, morphological constraints can be used
to check whether a word group in the input text
represents an entry or not. With respect to our fi-
nal task (translation from Czech to English), we
aim at Czech constraints only.
We decided to induce morphological con-
straints automatically, based on corpus examples
of the entries. For each entry, we look up sen-
tences that contain all the lemmas of the entry
in a close neighborhood (but irrespective to the
word order and possible presence of inserted extra
words). We weight the instances to promote those
with no intervening words and those with con-
nected dependency graph. The list of weighted
instances is scanned for both unary (such as ?case
is accusative?, ?number is singular?) and binary
(?the case of the first and second words match?)
pre-defined constraints selecting those that are
satisfied by at least 75% of total weight.
Most of the expressions with at least 10 corpus
instances obtain a valid set of constraints. Only
expressions containing very common words (so
that the words do appear quite often close together
without actually forming the expression) obtain
too weak constraints. For instance, no case and
gender agreement constraints are selected for the
expression bohaty? c?love?k (wealthy man).
Adding syntactic information.
Syntactic information (dependency relations
among words in the expression) is needed mainly
during the analysis of input sentences, therefore
we focused on adding the information to the
Czech part of entries first. For most of the en-
tries, it was possible to add the dependency struc-
ture manually, based on the part-of-speech pattern
of the entry. For instance all the entries contain-
ing an adjective followed by a noun get the same
structure: the noun governs the preceeding adjec-
tive. For the remaining entries (with very varied
POS patterns), we employ a corpus-based search
similar to the automatic procedure of identifying
morphological constraints.
4.3 Named entity recognition module
Named entities (NE) are atomic units such as
proper names, temporal expressions (e.g., dates)
and quantities (e.g., monetary expressions). They
occur quite often in various texts and carry impor-
tant information. Hence, proper analysis of NEs
and their translation has an enormous impact on
MT quality (Babych and Hartley, 2004). In our
system they are extremely important due to the
nature of input texts. The Wall Street Journal sec-
tion of PennTreebank shows much higher density
of named entities than ordinary texts. Their cor-
rect recognition therefore has a tremendous im-
pact on the performance of the whole system, es-
pecially if the evaluation of the translation quality
is based on golden standard translations.
NE translation involves both semantic transla-
tion and phonetic transliteration. Each type of NE
is handled in a different way. For instance, person
names do not undergo semantic translation (only
transliteration is required), while certain titles and
part of names do (e.g., prvn?? da?ma Laura Bushova?
? first lady Laura Bush). In case of organiza-
tions, application of regular transfer rules for NPs
seems to be sufficient (e.g., ?Ustav forma?ln?? a ap-
likovane? lingvistiky ? Institute of formal and ap-
plied linguistics), although an idiomatic transla-
tion may be probably preferable sometimes. With
respect to geographical places we apply bilingual
glossaries and a set of regular transfer rules as
well.
For NE-recognition, we have developed a
grammar based on regular expressions that
processes typed feature structures. The gram-
mar framework, similarly as the formally a bit
weaker platform SProUT (Bering et al, 2003),
182
uses finite-state techniques and unification, i.e., a
grammar consists of pattern/action rules, where
the left-hand side is a regular expression over
typed feature structures (TFS) with variables, rep-
resenting the recognition pattern, and the right-
hand side is a TFS specification of the output
structure.
The NE grammar is based on the experiment
described in (Piskorski et al, 2004). An example
of a simple rule is:
#subst[LEMMA: ministerstvo]$s1
+ #top[CASE: gen, PHRASE: $phr]$s2
== $s1#ministry[ATTR: $s2,
PHRASE: &(?ministerstvo ? + phr)]
(1)
The first TFS matches any morphological vari-
ant of the word ministerstvo (ministry), followed
by a genitive NP. The variables $s1, $s2 and $phr
create dynamic value assignments and allow to
transport these values to the slots in the output
structure of type ministry. The output structure
contains a new attribute called PHRASE with the
lemmatized value of the whole phrase.
If the input phrase is
informace ministerstva zahranic???
o cestova?n?? do ohroz?eny?ch oblast?? (2)
then the phrase ?ministerstva zahranic???? will be
recognized as a NE and handled as an atomic unit
in the whole MT process:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ministry
LEMMA ministerstvo
FORM ministerstva
PHRASE ministerstvo zahranic???
ATTR
?
?
?
?
?
subst
LEMMA zahranic???
PHRASE zahranic???
FORM zahranic???
CASE gen
NUMBER sg
GENDER n
?
?
?
?
?
CASE gen
NUMBER sg
GENDER n
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(3)
Lemmatization of NEs is crucial in the context
of MT. However, it might pose a serious problem
in case of languages with rich inflection due to
structural ambiguities, e.g., internal bracketing of
complex noun phrases might be difficult to ana-
lyze. The core of the framework is based on gram-
mars that have been developed for the MT system
?Ces??lko (Hajic? et al, 2003).
4.4 Syntactic analysis of Czech
Although we have originally assumed that the
module of syntactic analysis of Czech will re-
quire only small modifications and its reuse in the
new system was one of the goals of our system,
it turned out that this module is one of the main
sources of problems.
In the course of testing and debugging of the
system we had to create a number of new gram-
mar rules covering the phenomena which were
not properly accounted for in the original system
due to the different nature of the original domain.
The texts from PCEDT show for example much
higher number of numerals and numeric expres-
sions, some of which require either special gram-
matical or transfer rules than operating systems
manuals from the system RUSLAN. The com-
plexity of input sentences with regard to the num-
ber of clauses and their mutual relationship is also
much higher. This, of course, decreases the num-
ber of sentences which are completely syntacti-
cally analyzed and thus degrades the translation
quality.
One of the biggest problems of the grammar
are the properties of Q-systems. It was quite
clear since the start of the project that it is im-
possible to extract only the knowledge encoded
into the grammar, the grammar rules written in
Q-systems are so complicated that rewriting them
into a different (even chart-parser based) formal-
ism would actually mean to write a completely
new grammar. Although we have at our disposal
a new, modernized and reimplemented version of
a Q-systems compiler and interpreter which over-
comes the technical problems of the original ver-
sion, the nature of the formalism is of course pre-
served.
4.5 Transfer
The main task of this module is to transform the
syntactic structure (syntactic tree) of the input
Czech sentence into the syntactic structure (tree)
of the corresponding English sentence. The trans-
fer module does not handle the translation of reg-
ularly translated lexical units, it is handled by the
bilingual dictionary in the earlier phases of the
system. The transfer concentrates on three main
tasks:
183
? The transformation of the Czech syntactic
tree into the English one reflecting the dif-
ferences in the word order between both lan-
guages.
? The identification and translation of those
constructions in Czech, which require spe-
cific (irregular) translation into English.
? The insertion of articles (which do not exist
in Czech) into the target language sentences.
The development of this module still continues,
the initial tests confirmed that a substantial im-
provement can be achieved in the future.
4.6 Syntactic synthesis of English
The syntactic synthesis of Russian in RUSLAN is
very closely bound to transfer, therefore we have
tried to use as big portion of the grammar as possi-
ble, but of course, substantial modifications of the
grammar were necessary. As well as the work on
the transfer module, also the work on this module
still continues.
4.7 Morphological synthesis of English
Due to the simplicity of English morphology this
module has a very limited role in our system. It
handles plurals, 3rd persons and irregular words.
5 Conclusion
The problems mentioned in this paper do not al-
low to formulate an answer to the crucial ques-
tion - does it really pay off to recycle the old sys-
tem or not? The integration of existing parts into
a new system is so complicated that we are still
not able to perform evaluation of results on texts
of a reasonable size. One way out of this situa-
tion would be the combination of the new mod-
ules mentioned in this paper with one of the ex-
isting stochastic parsers of Czech instead of the
rule-based grammar.
Another possible direction for the future re-
search might be the exploitation of two new mod-
ules. The first one will contain partial, but error-
free disambiguation of the results of morpholog-
ical analysis of Czech, which will substantially
decrease the morphological ambiguity of individ-
ual Czech word forms. This ambiguity (the aver-
age number of morphological tags per word form
exceeds four in Czech) also negatively influences
the performance of the syntactic analysis.
The second way how to decrease the ambigu-
ity is the exploitation of a special module resolv-
ing the lexical ambiguity in those cases when the
bilingual dictionary provides more than one lexi-
cal equivalent. This stochastic module would ex-
ploit the context and would suggest the best trans-
lation.
References
B. Babych and A. Hartley. 2004. Selecting transla-
tion strategies in MT using automatic named en-
tity recognition. In Proceedings of the Ninth EAMT
Workshop, Valetta, Malta.
C. Bering, W. Droz?dz?yn?ski, G. Erbach, C. Guasch,
P. Homola, S. Lehmann, H. Li, H.-U. Krieger,
J. Piskorski, U. Schaefer, A. Shimada, M. Siegel,
F. Xu, and D. Ziegler-Eisele. 2003. Corpora
and evaluation tools for multilingual named entity
grammar development.
Alain Colmerauer. 1969. Les Systemes Q ou un for-
malisme pour analyser et synthetiser des phrases sur
ordinateur.
Jan Cur???n, Martin ?Cmejrek, Jir??? Havelka, and
Vladislav Kubon?. 2004. Building a Parallel Bilin-
gual Syntactically Annotated Corpus. In Proceed-
ings of the 1st International Joint Conference on
NLP.
Jan Hajic?. 2001. Disambiguation of Rich Inflection
- Computational Morphology of Czech, volume I.
Prague Karolinum, Charles University Press. 334
pp.
J. Hajic?, P. Homola, and V. Kubon?. 2003. A sim-
ple multilingual machine translation system. In In:
Proceedings of the MT Summit IX, New Orleans.
Karel Oliva. 1989. A Parser for Czech Implemented
in Systems Q. Explizite Beschreibung der Sprache
und automatische Textbearbeitung.
J. Piskorski, P. Homola, M. Marciniak,
A. Mykowiecka, A. Przepio?rkowski, and
M. Wolin?ski. 2004. Information extraction
for Polish using the SProUT platform. In Pro-
ceedings of the International IIS:IIP WM?04
Conference, Zakopane, Poland.
Milan Svoboda. 2001. GNU/FDL English-Czech
Dictionary. http://slovnik.zcu.cz/.
184
