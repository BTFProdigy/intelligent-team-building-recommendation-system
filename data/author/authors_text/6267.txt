TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 17?24,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Extractive Automatic Summarization: Does more linguistic knowledge 
make a difference? 
 
Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
http://www.nilc.icmc.usp.br 
1Departamento de Computa??o, UFSCar  
CP 676, 13565-905 S?o Carlos - SP, Brazil 
2Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
CP 668, 13560-970 S?o Carlos - SP, Brazil 
{daniel_leite; lucia}@dc.ufscar.br , {taspardo,gracan}@icmc.usp.br 
 
 
Abstract 
In this article we address the usefulness of 
linguistic-independent methods in extrac-
tive Automatic Summarization, arguing 
that linguistic knowledge is not only useful, 
but may be necessary to improve the in-
formativeness of automatic extracts. An as-
sessment of four diverse AS methods on 
Brazilian Portuguese texts is presented to 
support our claim. One of them is Mihal-
cea?s TextRank; other two are modified 
versions of the former through the inclusion 
of varied linguistic features. Finally, the 
fourth method employs machine learning 
techniques, tackling more profound and 
language-dependent knowledge. 
1 Introduction 
Usually, automatic summarization involves produc-
ing a condensed version of a source text through 
selecting or generalizing its relevant content. As a 
result, either an extract or an abstract will be pro-
duced. An extract is produced by copying text seg-
ments and pasting them into the final text preserving 
the original order. An abstract instead is produced 
by selecting and restructuring information from the 
source text. The resulting structure is thus linguisti-
cally realized independently of the surface choices 
of the source text. This comprises, thus, a rewriting 
task. 
This article focuses solely on extracts of source 
texts written in Brazilian Portuguese. For extrac-
tive Automatic Summarization (AS), several meth-
ods have been suggested that are based upon 
statistics or data readily available in the source 
text. Word frequency (Luhn, 1958) and sentence 
position (Edmundson, 1969) methods are classic 
examples of that. Usually, extractive AS does not 
take into account linguistic and semantic knowl-
edge in order to be portable to distinct domains or 
languages (Mihalcea, 2005). Graph-based methods 
aim at the same and have been gaining a lot of in-
terest because they usually do not rely on any lin-
guistic resource and run pretty fast. Exemplars of 
those are LexRank (Erkan and Radev, 2004) and 
TextRank (Mihalcea and Tarau, 2004). In spite of 
their potentialities, we claim that there is a com-
promise in pursuing a language-free setting: how-
ever portable a system may be, it may also produce 
extracts that lack the degree of informativeness 
needed for use. Informativeness, in the current 
context, refers to the ability of an automatic sum-
marizer to produce summaries that convey most 
information of reference, or ideal, summaries. Our 
assessment thus aimed at verifying if parsimonious 
use of linguistic knowledge could improve extrac-
tive AS. 
We argue that the lack of linguistic knowledge 
in extractive AS can be the reason for weak per-
formance regarding informativeness. This argu-
ment follows from acknowledging that 
improvements on the scores usually obtained in 
that field have not been expressive lately. The most 
common metrics used to date, precision and recall, 
signal average results, suggesting that it is not 
enough to pursue completely language-free sys-
tems, no matter the current demands for portability 
in the global communication scenario. We focus 
here on TextRank, which can be used for summa-
17
rizing Brazilian Portuguese texts due to its lan-
guage independence. To show that linguistic 
knowledge does make a difference in extractive 
AS, we compared four automatic summarizers: 
TextRank itself, two other modified versions of 
that, and SuPor-2 (Leite and Rino, 2006). 
TextRank works in a completely unsupervised 
way. Our two variations, although still 
unsupervised, include diverse linguistic knowledge 
in the preprocessing phase. SuPor-2 is the only 
machine learning-based system amongst the four 
ones, and it was built to summarize texts in 
Brazilian Portuguese, although it may be 
customized to other languages. Unlike the others, it 
embeds more sophisticated decision features that 
rely on varied linguistic resources. Some of them 
correspond to full summarization methods by 
themselves: Lexical Chaining (Barzilay and 
Elhadad, 1997), Relationship Mapping (Salton et 
al., 1997), and Importance of Topics (Larocca Neto 
et al, 2000). This is its unique and distinguishing 
characteristic.   
In what follows we first review the different lev-
els of processing in extractive AS (Section 2), then 
we describe TextRank and its implementation to 
summarize Brazilian Portuguese texts (Section 3). 
Our suggested modifications of TextRank are pre-
sented in Section 4, whilst SuPor-2 is described in 
Section 5. Finally, we compare the results of the 
four automatic summarizers when running on Bra-
zilian Portuguese texts (Section 6), and make some 
remarks on linguistic independence for extractive 
AS in Section 7. 
2 A Review of Automatic Summarization 
Mani (2001) classifies AS methods based upon 
three levels of linguistic processing to summarize a 
text, namely: 
 
? Shallow level. At this level only features at the 
surface of the text are explored. For example, 
location (Edmunson, 1969), sentence length 
and presence of signaling phrases (e.g., Kupiec 
et al, 1995). Combined, such features may 
yield a salience function that drives selection 
of sentences of the source text to include in a 
summary. 
 ? Entity level. The aim here is to build an inter-
nal representation of the source text that con-
veys its entities and corresponding 
relationships. These amount to the information 
that allows identifying important text seg-
ments. Examples of such relations are word 
cooccurrence (e.g., Salton et al, 1997), syno-
nyms and antonyms (e.g., Barzilay and Elha-
dad, 1997), logical relations, such as 
concordance or contradiction, and syntactic 
relations. 
 ? Discourse level. At this level the whole struc-
ture of the source text is modeled, provided 
that its communicative goals can be grasped 
from the source text. The discourse structure is 
intended to help retrieving, e.g., the main top-
ics of the document (e.g, Barzilay and Elha-
dad, 1997; Larocca Neto et al, 2000) or its 
rhetorical structure (e.g., Marcu, 1999), in or-
der to provide the means for AS. 
 
In this work we mainly focus on the entity level. 
Special entities and their relations thus provide the 
means to identify important sentences for building 
an extract. In turn, there is a loss of independence 
from linguistic knowledge, when compared to shal-
lower approaches. Actually, apart from TextRank, 
the other systems described in this paper target en-
tity level methods, as we shall see shortly.  
3 The TextRank Method 
The unsupervised TextRank method (Mihalcea and 
Tarau, 2004) takes after Google?s PageRank (Brin 
and Page, 1998), a graph-based system that helps 
judge the relevance of a webpage through incoming 
and outgoing links. PageRank directed graphs repre-
sent webpages as nodes and their linking to other 
webpages as edges. A random walk model is thus 
applied to build a path between the nodes, in order 
to grade the importance of a webpage in the graph. 
Similarly to grading webpages through travers-
ing a graph, TextRank attempts to weight sentences 
of a text by building an undirected graph. Nodes are 
now sentences, and edges express their similarity 
degrees to other sentences in the text. Actually, the 
degree of similarity is based upon content overlap. 
As such, similarity degrees help assess the overall 
cohesive structure of a text. The more content over-
lap a sentence has with other sentences, the more 
important it is and more likely it is to be included in 
the extract.. Similarity is calculated through equa-
tion [1] (Mihalcea and Tarau, 2004), where Si and Sj 
are sentences and wk is a common token between 
18
them. The numerator is the sum of common words 
between Si and Sj. To reduce bias, normalization of 
the involved sentences length takes place, as shows 
the denominator. 
 
|)log(||)log(|
|}|{|),(
ji
jkikk
ji SS
SwSwwSSSim +
???=  [1] 
 
Once the graph and all similarity degrees are 
produced, sentence importance is calculated by the 
random walk algorithm shown in equation [2]. 
TR(Vi) signals sentence importance, d is an arbitrary 
parameter in the interval [0,1], and N is the number 
of sentences in the text. Parameter d integrates the 
probability of jumping from one vertex to another 
randomly chosen. Thus, it is responsible for random 
walking. This parameter is normally set to 0.85 (this 
value is also used in TextRank). 
 
? ?
-
= -
=
??
??
?
?
??
??
?
?
??+-=
1
0
1
0
),(
),()()1()(
N
j
N
k
kj
ji
ji
SSSim
SSSimVTRddVTR  [2] 
 
Initial TR similarity values are randomly set in 
the [0,1] interval. After successive calculations, 
those values converge to the targeted importance 
value. After calculating the importance of the verti-
ces, the sentences are sorted in reverse order and the 
top ones are selected to compose the extract. As 
usual, the number of sentences of the extract is de-
pendent upon a given compression rate. 
  Clearly, TextRank is not language dependent. 
For this reason Mihalcea (2005) could use it to 
evaluate AS on texts in Brazilian Portuguese, be-
sides reporting results on texts in English. She also 
explored distinct means of representing a text with-
out considering linguistic knowledge, emphasizing 
TextRank language and domain independence. She 
varies, e.g., the ways the graphs could be traversed 
using both directed and undirected graphs. Once a 
sentence is chosen to compose an extract, having 
undirected graphs makes possible, to look forward ? 
from the sentence to its outgoing edges (i.e., focus-
ing on the set of its following sentences in the text) 
? or to look backward, considering that sentence 
incoming edges and, thus, the set of its preceding 
sentences in the text. 
Another variation proposed by Mihalcea is to 
replace the PageRank algorithm (Equation [2]) by 
HITS (Kleinberg, 1999). This works quite simi-
larly to PageRank. However, instead of aggregat-
ing the scores for both incoming and outgoing 
links of a node in just one final score, it produces 
two independent scores. These are correspondingly 
named ?authority? and ?hub? scores. 
4 Improving TextRank through varia-
tions on linguistic information 
To improve the similarity scores between sen-
tences in TextRank we fed it with more linguistic 
knowledge, yielding its two modified versions. The 
first variation focused just upon basic preprocess-
ing; the second one, on the use of a thesaurus to 
calculate semantic similarity to promote AS deci-
sions. However, we did not modify the main ex-
tractive algorithm of TextRank: we kept the graph 
undirected and used PageRank as the score deter-
miner. Actually, we modified only the method of 
computing the edges weights. 
4.1 Using Basic Preprocessing Methods 
In applying Equation 1 for similarity scores, only 
exact matches between two words are allowed. 
Since in Brazilian Portuguese there are many mor-
phological and inflexional endings for most words, 
this process becomes troublesome: important 
matches may be ignored. To overcome that, we used 
a stemmer for Brazilian Portuguese (Caldas Jr. et 
al., 2001) based upon Porter?s algorithm (1980). We 
also removed stopwords from the source text, be-
cause they are not useful in determining similarity. 
The resulting version of TextRank is named hereaf-
ter ?TextRank+Stem+StopwordsRem?. 
4.2 Using a Thesaurus 
Our second TextRank variation involved plugging 
into the system a Brazilian Portuguese thesaurus 
(Dias-da-Silva et al, 2003). Our hypothesis here is 
that semantic similarity of the involved words is 
also important to improve the informativeness of 
the extracts under production. Thus, an extractive 
summarizer should consider not only word repeti-
tion in the source text, but also synonymy and an-
tonymy.  
Although plugging the thesaurus into the 
automatic summarizer did not imply changing its 
main method of calculating similarity, there were 
some obstacles to overcome concerning the follow-
ing:  
19
 
 
 
 
 
 Figure 1. SuPor-2 training phase 
 
  
Figure 2. SuPor-2 extraction phase 
 
a) Should we consider only synonyms or both 
synonyms and antonyms in addition to term 
repetition (reiteration)? 
 
b) How to acknowledge, and disentangle, se-
mantic similarity, when polissemy, for ex-
ample, is present? 
 
c) Once the proper relations have been 
determined, how should they be weighted? 
Just considering all thesaural relations to be 
equally important might not be the best ap-
proach. 
Concerning (a), synonyms, antonyms, and term 
repetition were all considered, as suggested by oth-
ers (e.g., Barzilay and Elhadad, 1997). We did not 
tackle (b) to choose the right sense of a word be-
cause of the lack of an effective disambiguation 
procedure for Brazilian Portuguese. Finally, in 
tackling (c) and, thus, grading the importance of 
the relations for sentence similarity, we adopted 
the same weights proposed by Barzilay and Elha-
dad (1997) in their lexical chaining method, which 
is discussed in more detail below. For both reitera-
tion and synonymy, they assume a score of 10 for 
the considered lexical chain; for antonymy, they 
suggest a score of 7. The resulting version of Tex-
tRank is named here ?TextRank+Thesaurus?. 
5 The SuPor-2 System 
SuPor-2 is an extractive summarizer built from 
scratch for Brazilian Portuguese. It embeds differ-
ent features in order to identify and extract relevant 
sentences of a source text. To configure SuPor-2 
for an adequate combination of such features we 
employ a machine learning approach. Figures 1 
and 2 depict the training and extraction phases, 
respectively. 
For training, machine learning is carried out by a 
Na?ve-Bayes classifier that employs Kernel meth-
ods for numeric feature handling, known as Flexi-
ble Bayes (John and Langley, 1995). This 
environment is provided by WEKA1 (Witten and 
Frank, 2005), which is used within SuPor-2 itself. 
The training corpus comprises both source texts 
and corresponding reference extracts. Every sen-
tence from a source text is represented in the train-
                                                        
1 Waikato Environment for Knowledge Analysis. Available at 
http://www.cs.waikato.ac.nz/ml/weka/ (December, 2006) 
20
ing dataset as a tuple of the considered features. 
Each tuple is labeled with its class, which signals if 
the sentence appears in a reference extract. The 
class label will be true if the sentence under focus 
matches a sentence of the reference extract and 
false otherwise. 
Once produced, the training dataset is used by 
the Bayesian classifier to depict the sentences that 
are candidates to compose the extract (Figure 2). In 
other words, the probability for the ?true? class is 
computed and the top-ranked sentences are se-
lected, until reaching the intended compression 
rate. 
When computing features, three full methods 
(M) and four corpus-based parameters (P) are con-
sidered. Both methods and parameters are mapped 
onto the feature space and are defined as follows: 
 
(M) Lexical Chaining (Barzilay and Elhadad, 
1997). This method computes the connectedness 
between words aiming at determining lexical 
chains in the source text. The stronger a lexical 
chain, the more important it is considered for ex-
traction. Both an ontological resource and Word-
Net (Miller et al, 1990) are used to identify 
different relations, such as synonymy or antonym, 
hypernymy or hyponymy, that intervene to com-
pute connectedness. The lexical chains are then 
used to produce three sets of sentences. To identify 
and extract sentences from those sets, three heuris-
tics are  made available, namely: (H1) selecting 
every sentence s of the source text based on each 
member m of every strong lexical chain of the text. 
In this case, s is the sentence that contains the first 
occurrence of m; (H2) this heuristics is similar to 
the former one, but instead of considering all the 
members of a strong lexical chain, it uses only the 
representative ones. A representative member is 
one whose frequency is greater than the average 
frequency of all words in the chain; (H3) a sen-
tence s is chosen by focusing only on representa-
tive lexical chains of every topic of the source text. 
In SuPor-2, the mapping of this method onto a 
nominal feature is accomplished by signaling 
which heuristics have recommended the sentence. 
Thus, features in the domain may range over the 
values {?None?, ?H1?, ?H2?, ?H3?, ?H1H2?, 
?H1H3?, ?H2H3?, ?H1H2H3?}. 
 
(M) Relationship Mapping (Salton et al, 
1997). This method performs similarly to the pre-
vious one and also to TextRank in that it builds up 
a graph interconnecting text segments. However, it 
considers paragraphs instead of sentences as verti-
ces. Hence, graph edges signal the connectiveness 
of the paragraphs of the source text. Similarity 
scores between two paragraphs are thus related to 
the degree of connectivity of the nodes. Similarly 
to Lexical Chaining, Salton et al also suggest three 
different ways of producing extracts. However, 
they now depend on the way the graph is traversed. 
The so-called dense or bushy path (P1), deep path 
(P2), and segmented path (P3) aim at tackling dis-
tinct textual problems that may damage the quality 
of the resulting extracts. The dense path considers 
that paragraphs are totally independent from each 
other, focusing on the top-ranked ones (i.e., the 
ones that are denser). As a result, it does not guar-
antee that an extract will be cohesive. The deep 
path is intended to overcome the former problem 
by choosing paragraphs that may be semantically 
inter-related. Its drawback is that only one topic, 
even one that is irrelevant, may be conveyed in the 
extract. Thus, it may lack proper coverage of the 
source text. Finally, the segmented path aims at 
overcoming the limitations of the former ones, ad-
dressing all the topics at once. Similarly to Lexical 
Chaining, features in the Relationship method 
range over the set {?None?,?P1?,?P2?,?P3?, ?P1P2?, 
?P1P3?, ?P2P3?, ?P1P2P3?}. 
 
(M) Importance of Topics (Larocca Neto et 
al., 2000). This method also aims at identifying the 
main topics of the source text, however through the 
TextTiling algorithm (Hearst, 1993). Once the top-
ics of the source text have been determined, the 
first step is to select sentences that better express 
the importance of each topic. The amount of sen-
tences, in this case, is proportional to the topic im-
portance. The second step is to determine the 
sentences that will actually be included in the ex-
tract. This is carried out by measuring their simi-
larity to their respective topic centroids (Larocca 
Neto et al, 2000). The method thus signals how 
relevant a sentence is to a given topic. In SuPor-2 
this method yields a numeric feature whose value 
conveys the harmonic mean between the sentence 
similarity to the centroid of the topic in which it 
appears and the importance of that topic.  
(P) Sentence Length (Kupiec et al, 1995). 
This parameter just signals the normalized count of 
words of a sentence. 
21
(P) Sentence Location (Edmundson, 1969). 
This parameter takes into account the position of a 
sentence in the text. It is valued, thus, in 
{?II?,?IM?,?IF?,?MI?,?MM?,?MF?,?FI?,?FM?,?FF?}. 
In this set the first letter of each label signals the 
position of the sentence within a paragraph (Initial, 
Medium, or Final). Similarly, the second letter sig-
nals the position of the paragraph within the text. 
(P) Occurrence of proper nouns (e.g., Kupiec 
et al, 1995). This parameter accounts for the num-
ber of proper nouns in a sentence.  
(P) Word Frequency (Luhn, 1958). This pa-
rameter mirrors the normalized sum of the word 
frequency in a sentence. 
SuPor-2 provides a flexible way of combining 
linguistic and non-linguistic features for extraction. 
There are profound differences from TextRank. 
First, it is clearly language-dependent. Also, its 
graph-based methods do not assign weights to their 
vertices in order to select sentences for extraction. 
Instead, they traverse a graph in very specific  and 
varied ways that mirror both linguistic interde-
pendencies and important connections between the 
nodes. 
6 Assessing the Four Systems 
To assess the degree of informativeness of the sys-
tems previously described, we adopt ROUGE2 (Lin 
and Hovy, 2003), whose recall rate mirrors the in-
formativeness degree of automatically generated 
extracts by correlating automatic summaries with 
ideal ones. 
The two modified versions of TextRank require 
linguistic knowledge but at a low cost. This is cer-
tainly due to varying only preprocessing, while the 
main decision procedure is kept unchanged and 
language-independent. Those three systems do not 
need training, one of the main arguments in favor 
of TextRank (Mihalcea and Tarau, 2004). In con-
trast, SuPor-2 relies on training and this is certainly 
one of its main bottlenecks. It also employs lin-
guistic knowledge for both preprocessing and ex-
traction, which TextRank purposefully avoids. 
However, using WEKA has made its adjustments 
less demanding and more consistent, indicating 
that scaling up the system is feasible.  
                                                        
2 Recall-Oriented Understudy for Gisting Evaluation. Avail-
able at http://haydn.isi.edu/ROUGE/ (January, 2007). 
In our assessment, the same single-document 
summarization scenario posed by Mihalcea (2005) 
was adopted, namely: (a) we considered the Brazil-
ian Portuguese TeM?rio corpus (Pardo and Rino, 
2003); (b) we used the same baseline, which se-
lects top-first sentences to include in the extract; 
(c) we adopted a 70-75% compression rate, making 
it compatible with the compression rate of the ref-
erence summaries; and (d) ROUGE was used for 
evaluation in its Ngram(1,1) 95% confidence rate 
setting, without stopwords removal. TeM?rio com-
prises 100 newspaper articles from online Brazilian 
newswire. A set of corresponding manual summa-
ries produced by an expert in Brazilian Portuguese 
is also included in TeM?rio. These are our refer-
ence summaries. 
For training and testing SuPor-2, we avoided 
building an additional training corpus by using a 
10-fold cross-validation procedure. Finally, we 
produced three sets of extracts using ?TextRank +  
Stem + StopwordsRem?, ?TextRank + Thesaurus?, 
and SuPor-2 on the TeM?rio source texts. Results 
for informativeness are shown in Table 1. Since 
Mihalcea?s setting was kept unchanged, we just 
included in that table the same results presented in 
(Mihalcea, 2005), i.e., we did not run her systems 
all over again. We also reproduced for comparison 
the TextRank variations reported by Mihalcea, es-
pecially regarding graph-based walks by PageRank 
and HITS. Shaded lines correspond to our sug-
gested methods presented in Sections 4 and 5, 
which involve differing degrees of dependence on 
linguistic knowledge. 
It can be seen that ?TextRank+Thesaurus? and 
?TextRank+Stem+StopwordsRem? considerably 
outperformed all other versions of TextRank. 
Compared with Mihalcea's best version, i.e., with 
'TextRank (PageRank - backward)', those two 
methods represented a 6% and 9% improvement, 
respectively. We can conclude that neither the way 
the graph is built nor the choice of the graph-based 
ranking algorithm affects the results as signifi-
cantly as do the linguistic-based methods. Clearly, 
both variations proposed in this paper signal that 
linguistic knowledge, even if only used at the pre-
processing stage, provides more informative ex-
tracts than those produced when no linguistic 
knowledge at all is considered. Moreover, at that 
stage little modeling and computational effort is 
demanded, since lexicons, stoplists, and thesauri 
22
are quite widely available nowadays for several 
Romance languages. 
Even the baseline outperformed most versions 
of TextRank, showing that linguistic independence 
in a random walk model for extractive AS should 
be reconsidered. Actually, this shows that linguis-
tic knowledge does make a difference, at least for 
summarizing newswire texts in Brazilian Portu-
guese. 
In addition, SuPor-2 performance exceeds the 
best version of TextRank that uses no linguistic 
knowledge ? ?TextRank (PageRank - backward)? ? 
by about 14%. 
 
 
System ROUGE NGram(1,1) 
SuPor-2 0,5839 
TextRank+Thesaurus 0,5603 
TextRank+Stem+StopwordsRem 0,5426 
TextRank (PageRank - backward) 0,5121 
TextRank (HIT hub - forward) 0,5002 
TextRank (HITS authority - backward) 0,5002 
Baseline 0,4963 
TextRank (PageRank - undirected) 0,4939 
TextRank (HITS authority - forward) 0,4834 
TextRank (HIT hub - backward) 0,4834 
TextRank (HITS authority - undirected) 0,4814 
TextRank (HIT hub - undirected) 0,4814 
TextRank (PageRank - forward) 0,4574 
 
Table 1. Informativeness comparison between ex-
tractive summarizers 
7 Final Remarks 
A critical issue in the comparison presented above 
is the contrast between having an unsupervised or 
supervised summarizer, which is related to the is-
sue on having linguistic-independent extractive 
summarizers. Perhaps the question that we should 
pose here is how interesting and useful an extrac-
tive automatic summarizer that is totally independ-
ent from linguistic knowledge can actually be. To 
our view, the more non-informative an extract, the 
less useful it may be. So, summarizers that do not 
reach a minimum threshold concerning informa-
tiveness are deemed to failure nowadays. Clearly, 
SuPor-2 requires language-dependent resources, 
but its main extraction procedure is still general 
enough to make it portable and adaptable to new 
domains and languages. Hence, SuPor-2 assess-
ment suggests that it may be interesting to scale up 
SuPor-2. 
Considering that SuPor-2 is one of the best ex-
tractive summarizers for Brazilian Portuguese texts 
(Leite and Rino, 2006) and ?TextRank+Thesaurus? 
performed only 4% below it, we can also argue  in 
favor of providing even simple linguistic proce-
dures for extractive AS. The latter system shows 
that TextRank can yield extracts nearly as informa-
tive as those produced by the former, when em-
bedding stemming and stopwords removal. It can 
also perform AS with little computational effort 
and no training, when compared to the supervised 
SuPor-2. As a conclusion, we see that some lin-
guistic knowledge may boost TextRank perform-
ance without too much effort, since language-
dependent resources for preprocessing texts in 
natural language are usually available and easy to 
handle, concerning our addressed approach. 
There are many experiments that may be derived 
from our discussion in this paper (1) Although the 
reported results suggest that linguistic knowledge 
does make a difference when embedded in lan-
guage-free extractive summarizers, the perform-
ance of the top systems assessed through ROUGE 
should be more comprehensively licensed through 
additional assessment tasks. (2) These could also 
incorporate other graph-based algorithms than 
TextRank, such as the LexRank one, aiming at re-
assuring our claim and scaling up graph-based ap-
proaches. (3) Since we addressed language-
independence (thus portability) versus language-
dependence for informativeness, it would also be 
interesting to explore other domains or languages 
to support our claim or, at least, to look for other 
findings to confirm if linguistic knowledge indeed 
makes a difference. (4) Other TextRank variations 
could also be explored, to see if adding more fea-
tures would make TextRank closer to SuPor-2. 
Acknowledgements 
This work has been supported by the Brazilian re-
search funding agencies CNPq, CAPES and 
FAPESP.
23
References 
B. C. Dias-da-Silva, M. F. Oliveira, H. R. Moraes, C. 
Paschoalino, R. Hasegawa, D. Amorin and A. C. 
Nascimento. 2000. Constru??o de um Thesaurus Ele-
tr?nico para o Portugu?s do Brasil. In Proceedings of 
the V Encontro para o Processamento Computacio-
nal da L?ngua Portuguesa Escrita e Falada 
(PROPOR 2000), S?o Carlos, Brasil , 1-11. 
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-occurrence Statistics. 
In Proceedings of Language Technology Conference 
(HLT-NAACL 2003), Edmonton, Canada.  
D. Marcu. 1999. Discourse Trees Are Good Indicators 
of Importance in Text. In Mani, I., Maybury, M. T. 
(Eds.). 1999. Advances in Automatic Text Summari-
zation. MIT Press. 
D. S. Leite and L. H. M. Rino. 2006. Selecting a Feature 
Set to Summarize Texts in Brazilian Portuguese. In J. 
S. Sichman et al (eds.): Proceedings of 18th. Brazil-
ian Symposium on Artificial Intelligence (SBIA'06) 
and 10th. Ibero-American Artificial Intelligence Con-
ference (IBERAMIA'06). Lecture Notes on Artificial 
Intelligence, No. 4140, Springer-Verlag, 462-471. 
G. Erkan and D R. Radev. 2004. LexRank: Graph-based 
Lexical Centrality as Salience in Text Summariza-
tion. Journal of Artificial Intelligence Research 
22:457-479 
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross and 
K. Miller. 1990. Introduction to WordNet: An On-
line Lexical Database. International Journal of Lexi-
cography 3(4):235-244 
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing & Management 24 : 513-523.. Reprinted 
in: K. Sparck-Jones and P. Willet (eds.). 1997. Read-
ings in Information Retrieval, Morgan Kaufmann, 
323-328.  
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development 
2:159-165 
H. P. Edmundson. 1969. New methods in automatic 
extracting. Journal of the Association for Computing 
Machinery 16:264-285. 
I. Witten and E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques, 2nd ed. Mor-
gan Kaufmann, San Francisco. 
I. Mani. 2001. Automatic Summarization. John Benja-
min?s Publishing Company.  
I. Mani and M. T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press. 
J. Caldas Junior, C. Y. M. Imamura and S. O. Rezende. 
Avalia??o de um Algoritmo de Stemming para a 
L?ngua Portuguesa. In Proceedings of the 2nd Con-
gress of Logic Applied to Technology 
(LABTEC?2001), vol. II. Faculdade SENAC de Ci?n-
cias Exatas e Tecnologia, S?o Paulo, Brasil (2001), 
267-274. 
J. M. Kleinberg. 1999. Authoritative sources in hyper-
linked environment. Journal of the ACM, 46(5):604-
632. 
J. Kupiec, J. Pedersen and F. Chen. 1995. A trainable 
document summarizer. In Proceedings of the 18th 
ACM-SIGIR Conference on Research & Develop-
ment in Information Retrieval, 68-73. 
J. Larocca Neto, A. D. Santos, C. A. A. Kaestner and A. 
A. Freitas. 2000. Generating Text Summaries 
through the Relative Importance of Topics. Lecture 
Notes in Artificial Intelligence, No. 1952. Springer-
Verlag, 200-309 
M. A. Hearst. 1993. TextTiling: A Quantitative Ap-
proach to Discourse Segmentation. Technical Report 
93/24. University of California, Berkeley. 
M. F. Porter. 1980. An Algorithm for Suffix Stripping. 
Program, 14 (3) : 130-137 
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In  Proceedings of the Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), Barcelona, Spain, July.  
R. Mihalcea. 2005. Language Independent Extractive 
Summarization. In Proceedings of the 43th Annual 
Meeting of the Association for Computational Lin-
guistics, Companion Volume (ACL2005), Ann Ar-
bor, MI, June. 
R. Barzilay and M. Elhadad. 1997. Using lexical chains 
for text summarization. In Proceedings of the Intelli-
gent Scalable Text Summarization Workshop 
(ISTS'97), ACL, Madrid, Spain.  
S. Brin and L. Page. 1998. The anatomy of a large-scale 
hypertextual Web search engine. Computer Networks 
and ISDN Systems 30:1-7. 
T. A. S. Pardo and L.H.M. Rino. 2003. TeM?rio: A cor-
pus for automatic text summarization (in Portu-
guese). NILC Tech. Report NILC-TR-03-09  
24
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 34?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supporting the Adaptation of Texts for Poor Literacy Readers: a Text 
Simplification Editor for Brazilian Portuguese 
 
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, Thiago A. S. Pardo, Lucia Specia, and Sandra M. Aluisio  
 
Center of Computational Linguistics (NILC) / Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
arnaldoc@icmc.usp.br, egmaziero@gmail.com, {cgasperin,taspardo,lspecia,sandra}@icmc.usp.br 
 
Abstract 
In this paper we investigate the task of text 
simplification for Brazilian Portuguese. Our 
purpose is three-fold: to introduce a 
simplification tool for such language and its 
underlying development methodology, to 
present an on-line authoring system of 
simplified text based on the previous tool, and 
finally to discuss the potentialities of such 
technology for education. The resources and 
tools we present are new for Portuguese and 
innovative in many aspects with respect to 
previous initiatives for other languages.  
1 Introduction 
In Brazil, according to the index used to measure 
the literacy level of the population (INAF - National 
Indicator of Functional Literacy), a vast number of 
people belong to the so called rudimentary and basic 
literacy levels. These people are only able to find 
explicit information in short texts (rudimentary 
level) or process slightly longer texts and make 
simple inferences (basic level). INAF reports that 
68% of the 30.6 million Brazilians between 15 and 
64 years who have studied up to 4 years remain at 
the rudimentary literacy level, and 75% of the 31.1 
million who studied up to 8 years remain at the 
rudimentary or basic levels. 
Reading comprehension entails three elements: 
the reader who is meant to comprehend; the text that 
is to be comprehended and the activity in which 
comprehension is a part of (Snow, 2002). In 
addition to the content presented in the text, the 
vocabulary load of the text and its linguistic 
structure, discourse style, and genre interact with the 
reader?s knowledge. When these factors do not 
match the reader?s knowledge and experience, the 
text becomes too complex for the comprehension to 
occur. In this paper we will focus on the text and the 
aspects of it that make reading difficult or easy. One 
solution to ease the syntactic structure of a text is 
via Text Simplification (TS) facilities.  
TS aims to maximize the comprehension of 
written texts through the simplification of their 
linguistic structure. This may involve simplifying 
lexical and syntactic phenomena, by substituting 
words that are only understood by a few people with 
words that are more usual, and by breaking down 
and changing the syntactic structure of the sentence, 
respectively. As a result, it is expected that the text 
can be more easily understood both by humans and 
computer systems (Mapleson, 2006; Siddharthan, 
2003, Max, 2006). TS may also involve dropping 
parts or full sentences and adding some extra 
material to explain a difficult point. This is the case, 
for example, of the approach presented by Petersen 
and Ostendorf (2007), in which abridged versions of 
articles are used in adult literacy learning. 
It has already been shown that long sentences, 
conjoined sentences, embedded clauses, passives, 
non-canonical word order, and use of low-frequency 
words, among other things, increase text complexity 
for language-impaired readers (Siddharthan, 2002; 
Klebanov et al, 2004; Devlin and Unthank, 2006). 
The Plain English initiative makes available 
guidelines to make texts easier to comprehend: the 
Plain Language1. In principle, its recommendations 
can be applied to any language. Although some of 
them are directly useful for TS systems (e.g., 
subject-verb-object order and active voice), others 
are difficult to specify (e.g., how simple each 
syntactic construction is and which words are 
simple). 
In this paper we present the results of a study of 
syntactic simplification for Brazilian Portuguese 
(BP) and a rule-based syntactic simplification 
system for this language that was developed based 
on this study ? the first of this kind for BP. We also 
present an on-line authoring tool for creating 
simplified texts. One possible application of this 
tool is to help teachers to produce instructional texts 
                                                 
1
 http://www.plainlanguage.gov 
34
to be used in classrooms. The study is part of the 
PorSimples project2 (Simplification of Portuguese 
Text for Digital Inclusion and Accessibility), which 
aims at producing text simplification tools for 
promoting digital inclusion and accessibility for 
people with different levels of literacy, and possibly 
other kinds of reading disabilities. 
This paper is organized as follows. In Section 2 
we present related approaches for text simplification 
with educational purposes. In Section 3 we describe 
the proposed approach for syntactic simplification, 
which is used within an authoring tool described in 
Section 4. In Section 5 we discuss possible uses of 
text simplification for educational purposes.  
2 Related work 
Burstein (2009) presents an NLP-based application 
for educational purposes, named Text Adaptor, 
which resembles our authoring tool. It includes 
complex sentence highlighting, text elaboration 
(word substitutions by easier ones), text 
summarization and translation. The system does not 
perform syntactic simplification, but simply 
suggests, using a shallow parser, that some 
sentences might be too complex. Specific hints on 
the actual source of complexity are not provided. 
Petersen (2007) addresses the task of text 
simplification in the context of second-language 
learning. A data-driven approach to simplification is 
proposed using a corpus of paired articles in which 
each original sentence does not necessarily have a 
corresponding simplified sentence, making it 
possible to learn where writers have dropped or 
simplified sentences. A classifier is used to select 
the sentences to simplify, and Siddharthan?s 
syntactic simplification system (Siddharthan, 2003) 
is used to split the selected sentences. In our 
approach, we do not drop sentences, since we 
believe that all the content must be kept in the text. 
Siddharthan proposes a syntactic simplification 
architecture that relies on shallow text analysis and 
favors time performance. The general goal of the 
architecture is to make texts more accessible to a 
broader audience; it has not targeted any particular 
application. The system treats apposition, relative 
clauses, coordination and subordination. Our 
method, on the other hand, relies on deep parsing 
(Bick, 2000). We treat the same phenomena as 
                                                 
2
 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
Siddharthan, but also deal with Subject-Verb-Object 
ordering (in Portuguese sentences can be written in 
different orders) and passive to active voice 
conversion. Siddharthan's system deals with non-
finite clauses which are not handled by our system 
at this stage. 
Lal and Ruger?s (2002) created a bayesian 
summarizer with a built-in lexical simplification 
module, based on WordNet and MRC psycho-
linguistic database3. The system focuses on 
schoolchildren and provides background 
information about people and locations in the text, 
which are retrieved from databases. Our rule-based 
simplification system only replaces discourse 
markers for more common ones using lexical 
resources built in our project, instead of inserting 
additional information in the text. 
Max (2005, 2006) applies text simplification in 
the writing process by embedding an interactive text 
simplification system into a word processor. At the 
user?s request, an automatic parser analyzes an 
individual sentence and the system applies 
handcrafted rewriting rules. The resulting suggested 
simplifications are ranked by a score of syntactic 
complexity and potential change of meaning. The 
writer then chooses their preferred simplification. 
This system ensures accurate output, but requires 
human intervention at every step. Our system, on 
the other hand, is autonomous, even though the user 
is able to undo any undesirable simplification or to 
choose alternative simplifications. These alternative 
simplifications may be produced in two cases: i) to 
compose a new subject in simplifications involving 
relatives and appositions and ii) to choose among 
one of the coordinate or subordinate simplifications 
when there is ambiguity regarding to conjunctions. 
Inui et al (2003) proposes a rule-based system 
for text simplification aimed at deaf people. The 
authors create readability assessments based on 
questionnaires answered by teachers about the deaf. 
With approximately one thousand manually created 
rules, the authors generate several paraphrases for 
each sentence and train a classifier to select the 
simpler ones. Promising results are obtained, 
although different types of errors on the paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency. In our work we 
produce alternative simplifications only in the two 
cases explained above. 
                                                 
3
 http://www.psych.rl.ac.uk/ 
35
Caseli et al (2009) developed an annotation 
editor to support the building of parallel corpora of 
original and simplified texts in Brazilian 
Portuguese. The tool was used to build a corpus of 
simplified texts aimed at people with rudimentary 
and basic literacy levels. We have used the parallel 
corpus to evaluate our rule-based simplification 
system. The on-line authoring system presented in 
this paper evolved from this annotation editor. 
There are also commercial systems like Simplus4 
and StyleWriter5, which aim to support Plain 
English writing.  
3 A rule-based syntactic simplification 
system 
Our text simplification system comprises seven 
operations (see Sections 3.1 and 3.2), which are 
applied to a text in order to make its syntactic 
structure simpler. These operations are applied 
sentence by sentence, following the 3-stage 
architecture proposed by Siddharthan (2002), which 
includes stages of analysis, transformation and 
regeneration. In Siddharthan?s work, the analysis 
stage performs the necessary linguistic analyses of 
the input sentences, such as POS tagging and 
chunking; the transformation stage applies 
simplification rules, producing simplified versions 
of the sentences; the regeneration stage performs 
operations on the simplified sentences to make them 
readable, like referring expressions generation, cue 
words rearrangement, and sentence ordering. 
Differently from such architecture, currently our 
regeneration stage only includes the treatment of 
cue words and a surface forms (GSF) generator, 
which is used to adjust the verb conjugation and 
regency after some simplification operations. 
As a single sentence may contain more than 
one complex linguistic phenomenon, simplification 
operations are applied in cascade to a sentence, as 
described in what follows. 
3.1 Simplification cases and operations 
As result of a study on which linguistic phenomena 
make BP text complex to read and how these 
phenomena could be simplified, we elaborated a 
manual of BP syntactic simplification (Aluisio et al, 
2008). The rule-based text simplification system 
                                                 
4
 http://www.linguatechnologies.com/english/home.html  
5
 http://www.editorsoftware.com/writing-software 
developed here is based on the specifications in this 
manual. According to this manual, simplification 
operations should be applied when any of the 22 
linguistic phenomena presented in Table 1 is 
detected. 
The possible operations suggested to be applied 
in order to simplify these phenomena are: (a) split 
the sentence, (b) change a discourse marker by a 
simpler and/or more frequent one (the indication is 
to avoid the ambiguous ones), (c) change passive to 
active voice, (d) invert the order of the clauses, (e) 
convert to subject-verb-object ordering, (f) change 
topicalization and detopicalization of adverbial 
phrases and (g) non-simplification.  
Table 1 shows the list of all simplification 
phenomena covered by our manual, the clues used 
to identify the phenomena, the simplification 
operations that should be applied in each case, the 
expected order of clauses in the resulting sentence, 
and the cue phrases (translated here from 
Portuguese) used to replace complex discourse 
markers or to glue two sentences. In column 2, we 
consider the following clues: syntactic information 
(S), punctuation (P), and lexicalized clues, such as 
conjunctions (Cj), relative pronouns (Pr) and 
discourse markers (M), and semantic information 
(Sm, and NE for named entities). 
3.2 Identifying simplification cases and 
applying simplification rules 
Each sentence is parsed in order to identify cases for 
simplification. We use parser PALAVRAS (Bick, 
2000) for Portuguese. This parser provides lexical 
information (morphology, lemma, part-of-speech, 
and semantic information) and the syntactic trees for 
each sentence. For some operations, surface 
information (such as punctuation or lexicalized cue 
phrases) is used to identify the simplification cases, 
as well as to assist simplification process. For 
example, to detect and simplify subjective non-
restrictive relative clauses (where the relative 
pronoun is the subject of the relative clause), the 
following steps are performed: 
1. The presence of a relative pronoun is verified. 
2. Punctuation is verified in order to distinguish it 
from restrictive relative clauses: check if the 
pronoun occurs after a comma or semicolon.  
3. Based on the position of the pronoun, the next 
punctuation symbol is searched to define the 
boundaries of the relative clause. 
36
4. The first part of the simplified text is generated, 
consisting of the original sentence without the 
embedded relative clause. 
5. The noun phrase in the original sentence to 
which the relative clause refers is identified. 
6. A second simplified sentence is generated, 
consisting of the noun phrase (as subject) and 
the relative clause (without the pronoun). 
The identification of the phenomena and the 
application of the operations are prone to errors 
though. Some of the clues that indicate the 
occurrence of the phenomena may be ambiguous. 
For example, some of the discourse markers that are 
used to identify subordinate clauses can indicate 
more than one type of these: for instance, ?como? 
(in English ?like?, ?how? or ?as?) can indicate 
reason, conformative or concessive subordinate 
clauses. Since there is no other clue that can help us 
disambiguate among those, we always select the 
case that occurs more frequently according to a 
corpus study of discourse markers and the rhetoric 
relations that they entitle (Pardo and Nunes, 2008). 
However, we can also treat all cases and let the user 
decide the simplifications that is most appropriate. 
 
Phenomenon Clues Op Clause Order Cue phrase Comments 
1.Passive voice S c   Verb may have to be adapted 
2.Embedded appositive S a Original/ 
App. 
 Appositive: Subject is the head of original + 
to be in present tense + apposition 
3.Asyndetic coordinate clause S a Keep order   New sentences: Subjects are the head of the 
original subject 
4.Additive coordinate clause S, Cj a Keep order Keep marker Marker appears in the beginning of the new 
sentence 
5.Adversative coordinate clause M a, b Keep order But  
6.Correlated coordinate clause M a, b Keep order Also Original markers disappear 
7.Result coordinate clause S, M a, b Keep order As a result  
8.Reason coordinate clause S, M a, b Keep order This happens 
because 
May need some changes in verb 
9.Reason subordinate clause M a, b, 
d 
Sub/Main With this To keep the ordering cause, result 
M a, b Main/Sub Also Rule for such ... as, so ... as markers  10.Comparative subordinate clause 
M g   Rule for the other markers or short sentences 
M a, b, 
d 
Sub/Main But ?Clause 1 although clause 2? is changed to 
?Clause 2. But clause 1? 
11.Concessive subordinate clause 
M a, b Main/Sub This happens 
even if 
Rule for hypothetical sentences 
12.Conditional subordinate clause S, M d Sub/Main  Pervasive use in simple accounts 
13. Result subordinate clause M a, b Main/Sub Thus May need some changes in verb 
14.Final/Purpose subordinate clause S, M a, b Main/Sub The goal is  
15.Confirmative subordinate clause M a, b, 
d 
Sub/Main Confirms 
that 
May need some changes in verb 
M a Sub/Main  May need some changes in verb 16.Time subordinate clause 
M a, b  Then Rule for markers: after that, as soon as  
17. Proportional Subordinate Clause M g    
18. Non-finite subordinate clause S g    
19.Non-restrictive relative clause S, P, Pr a Original/ 
Relative 
 Relative: Subject is the head of original + 
relative (subjective relative clause) 
20.Restrictive relative clause S, Pr a Relative/ 
Original 
 Relative: Subject is the head of original + 
relative  (subjective relative clause) 
21.Non Subject-Verb-Object order S e   Rewrite in Subject-Verb-Object order 
22. Adverbial phrases in theme 
position 
S, NE, 
Sm  
f In study  In study 
Table 1: Cases, operations, order and cue phrases 
Every phenomenon has one or more 
simplification steps associated with it, which are 
applied to perform the simplification operations. 
Below we detail each operation and discuss the 
challenges involved and our current limitations in 
their implementing. 
a) Splitting the sentence - This operation is the 
most frequent one. It requires finding the split point 
37
in the original sentence (such as the boundaries of 
relative clauses and appositions, the position of 
coordinate or subordinate conjunctions) and the 
creation of a new sentence, whose subject 
corresponds to the replication of a noun phrase in 
the original sentence. This operation increases the 
text length, but decreases the length of the 
sentences. With the duplication of the term from the 
original sentence (as subject of the new sentence), 
the resulting text contains redundant information, 
but it is very helpful for people at the rudimentary 
literacy level. 
When splitting sentences due to the presence of 
apposition, we need to choose the element in the 
original sentence to which it is referring, so that this 
element can be the subject of the new sentence. At 
the moment we analyze all NPs that precede the 
apposition and check for gender and number 
agreement. If more than one candidate passes the 
agreement test, we choose the closest one among 
these; if none does, we choose the closest among all 
candidates. In both cases we can also pass the 
decision on to the user, which we do in our 
authoring tool described in Section 4. 
For treating relative clauses we have the same 
problem as for apposition (finding the NP to which 
the relative clause is anchored) and an additional 
one: we need to choose if the referent found should 
be considered the subject or the object of the new 
sentence. Currently, the parser indicates the 
syntactic function of the relative pronoun and that 
serves as a clue. 
b) Changing discourse marker - In most cases 
of subordination and coordination, discourse 
markers are replaced by most commonly used ones, 
which are more easily understood. The selection of 
discourse markers to be replaced and the choice of 
new markers (shown in Table 1, col. 4) are done 
based on the study of Pardo and Nunes (2008). 
c) Transformation to active voice - Clauses in 
the passive voice are turned into active voice, with 
the reordering of the elements in the clause and the 
modification of the tense and form of the verb. Any 
other phrases attached to the object of the original 
sentence have to be carried with it when it moves to 
the subject position, since the voice changing 
operation is the first to be performed. For instance, 
the sentence: 
?More than 20 people have been bitten by gold piranhas 
(Serrasalmus Spilopleura), which live in the waters of the 
Sanchuri dam, next to the BR-720 highway, 40 km from 
the city.? 
is simplified to: 
?Gold piranhas (Serrasalmus Spilopleura), which live in 
the waters of the Sanchuri dam, next to the BR-720 
highway, 40 km from the city, have bitten more than 20 
people.? 
After simplification of the relative clause and 
apposition, the final sentence is: 
?Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. Gold piranhas 
are Serrasalmus Spilopleura.? 
d) Inversion of clause ordering - This operation 
was primarily designed to handle subordinate 
clauses, by moving the main clause to the beginning 
of the sentence, in order to help the reader 
processing it on their working memory (Graesser et 
al., 2004). Each of the subordination cases has a 
more appropriate order for main and subordinate 
clauses (as shown in Table 1, col. 3), so that 
?independent? information is placed before the 
information that depends on it. In the case of 
concessive subordinate clauses, for example, the 
subordinate clause is placed before the main clause. 
This gives the sentence a logical order of the 
expressed ideas. See the example below, in which 
there is also a change of discourse marker and 
sentence splitting, all operations assigned to 
concessive subordinate clauses:  
?The building hosting the Brazilian Consulate was also 
evacuated, although the diplomats have obtained 
permission to carry on working.? 
Its simplified version becomes:  
?The diplomats have obtained permission to carry on 
working. But the building hosting the Brazilian Consulate 
was also evacuated.? 
e) Subject-Verb-Object ordering - If a sentence 
is not in the form of subject-verb-object, it should be 
rearranged. This operation is based only on 
information from the syntactic parser. The example 
below shows a case in which the subject is after the 
verb (translated literally from Portuguese, 
preserving the order of the elements): 
?On the 9th of November of 1989, fell the wall that for 
almost three decades divided Germany.? 
Its simplified version is: 
?On the 9th of November of 1989, the wall that for almost 
three decades divided Germany fell.? 
Currently the only case we are treating is the non-
canonical order Verb-Object-Subject. We plan to 
treat other non-canonical orderings in the near 
future. Besides that, we still have to define how to 
deal with elliptic subjects and impersonal verbs 
(which in Portuguese do not require a subject). 
38
When performing this operation and the previous 
one, a generator of surface forms (GSF) is used to 
adjust the verb conjugation and regency. The GSF is 
compiled from the Apertium morphological 
dictionaries enhanced with the entries of Unitex-BP 
(Muniz et al, 2005), with an extra processing to 
map the tags of the parser to those existing in 
morphological dictionaries (Caseli et al, 2007) to 
obtain an adjusted verb in the modified sentence. 
f) Topicalization and detopicalization - This 
operation is used to topicalize or detopicalize an 
adverbial phrase. We have not implemented this 
operation yet, but have observed that moving 
adverbial phrases to the end or to the front of 
sentences can make them simpler in some cases. For 
instance, the sentence in the last example would 
become: 
?The wall that for almost three decades divided Germany fell 
on the 9th of November of 1989.? 
We are still investigating how this operation 
could be applied, that is, which situations require 
(de)topicalization. 
3.3 The cascaded application of the rules 
As previously mentioned, one sentence may contain 
several phenomena that could be simplified, and we 
established the order in which they are treated. The 
first phenomenon to be treated is passive voice. 
Secondly, embedded appositive clauses are 
resolved, since they are easy to simplify and less 
prone to errors. Thirdly, subordinate, non-restrictive 
and restrictive relative clauses are treated, and only 
then the coordinate clauses are dealt with.  
As the rules were designed to treat each case 
individually, it is necessary to apply the operations 
in cascade, in order to complete the simplification 
process for each sentence. At each iteration, we (1) 
verify the phenomenon to be simplified following 
the standard order indicated above; (2) when a 
phenomenon is identified, its simplification is 
executed; and (3) the resulting simplified sentence 
goes through a new iteration. This process continues 
until there are no more phenomena. The cascade 
nature of the process is crucial because the 
simplified sentence presents a new syntactic 
structure and needs to be reparsed, so that the 
further simplification operations can be properly 
applied. However, this process consumes time and 
is considered the bottleneck of the system.  
3.4 Simplification evaluation 
We have so far evaluated the capacity of our rule-
based simplifier to identify the phenomena present 
in each sentence, and to recommend the correct 
simplification operation. We compared the 
operations recommended by the system with the 
ones performed manually by an annotator in a 
corpus of 104 news articles from the Zero Hora 
newspaper, which can be seen in our Portal of 
Parallel Corpora of Simplified Texts6. Table 2 
presents the number of occurrences of each 
simplification operation in this corpus. 
Simplification Operations # Sentences 
Non-simplification 2638 
Subject-verb-object ordering 44 
Transformation to active voice 154 
Inversion of clause ordering 265 
Splitting sentences 1103 
Table 2. Statistics on the simplification operations 
The performance of the system for this task is 
presented in Table 3 in terms of precision, recall, 
and F-measure for each simplification operation.  
Operation P R F 
Splitting sentences 64.07 82.63 72.17 
Inversion of clause ordering 15.40 18.91 16.97 
Transformation to active voice 44.29 44.00 44.14 
Subject-verb-object ordering 1.12 4.65 1.81 
ALL 51.64 65.19 57.62 
Non-simplification 64.69 53.58 58.61 
Table 3. Performance on defining simplification 
operations according to syntactic phenomena 
These results are preliminary, since we are still 
refining our rules. Most of the recall errors on the 
inversion of clause ordering are due to the absence 
of a few discourse markers in the list of markers that 
we use to identify such cases. The majority of recall 
errors on sentence splitting are due to mistakes on 
the output of the syntactic parser and to the number 
of ordering cases considered and implemented so 
far. The poor performance for subject-verb-object 
ordering, despite suffering from mistakes of the 
parser, indicates that our rules for this operation 
need to be refined. The same applies to inversion of 
clause ordering. 
We did not report performance scores related to 
the ?changing discourse marker? operation because 
in our evaluation corpus this operation is merged 
with other types of lexical substitution. However, in  
                                                 
6
 http://caravelas.icmc.usp.br/portal/index.php 
39
order to assess if the sentences were correctly 
simplified, it is necessary to do a manual evaluation, 
since it is not possible to automatically compare the 
output of the rule-based simplifier with the 
annotated corpus, as the sentences in the corpus 
have gone through operations that are not performed 
by the simplifier (such as lexical substitution). We 
are in the process of performing such manual 
evaluation. 
4 Simplifica editor: supporting authors 
We developed Simplifica7 (Figure 1), an authoring 
system to help writers to produce simplified texts. It 
employs the simplification technology described in 
the previous section. It is a web-based WYSIWYG 
editor, based on TinyMCE web editor8.  
The user inputs a text in the editor, customizes 
the simplification settings where one or more 
simplifications can be chosen to be applied in the 
text and click on the ?simplify? button. This triggers 
the syntactic simplification system, which returns an 
XML file containing the resulting text and tags 
indicating the performed simplification operations. 
After that, the simplified version of the text is 
shown to the user, and he/she can revise the 
automatic simplification. 
4.1 The XML representation of simplification 
operations 
Our simplification system generates an XML file 
                                                 
7
 http://www.nilc.icmc.usp.br/porsimples/simplifica/ 
8
 http://tinymce.moxiecode.com/ 
describing all simplification operations applied to a 
text. This file can be easily parsed using standard 
XML parsers. Table 5 presents the XML annotation 
to the ?gold piranhas? example in Section 3.2. 
  
<simplification type="passive"> 
<simplification type="appositive"> 
<simplification type="relative"> 
Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. 
</simplification> 
Gold piranhas are Serrasalmus Spilopleura. 
</simplification> 
</simplification> 
Table 5. XML representation of a simplified text 
In our annotation, each sentence receives a 
<simplification> tag which describes the simplified 
phenomena (if any); sentences that did not need 
simplification are indicated with a <simplification 
type=?no?> tag. The other simplification types refer 
to the eighteen simplification cases presented in 
Table 1. Nested tags indicate multiple operations 
applied to the same sentence. 
4.2 Revising the automatic simplification 
Once the automatic simplification is done, a review 
screen shows the user the simplified text so that 
he/she can visualize all the modifications applied 
and approve or reject them, or select alternative 
simplifications. Figure 1 shows the reviewing screen 
and a message related to the simplification 
performed below the text simplified. 
The user can revise simplified sentences one at a 
time; the selected sentence is automatically 
highlighted. The user can accept or reject a 
 
Figure 1: Interface of the Simplifica system 
40
simplified sentence using the buttons below the text. 
In the beginning of the screen ?Mais op??es?, 
alternative simplifications for the sentence are 
shown: this facility gives the user the possibility to 
resolve cases known to be ambiguous (as detailed in 
Sections 2 and 3.2) for which the automatic 
simplification may have made a mistake. In the 
bottom of the same screen we can see the original 
sentence (?Senten?a original?) to which the 
highlighted sentence refers.  
For the example in Figure 1, the tool presents 
alternative simplifications containing different 
subjects, since selecting the correct noun phrase to 
which an appositive clause was originally linked 
(which becomes the subject of the new sentence) 
based on gender and number information was not 
possible.  
At the end of the process, the user returns to the 
initial screen and can freely continue editing the text 
or adding new information to it. 
5 Text Simplification for education 
Text simplification can be used in several 
applications. Journalists can use it to write simple 
and straightforward news texts. Government 
agencies can create more accessible texts to a large 
number of people. Authors of manuals and technical 
documents can also benefit from the simplification 
technology. Simplification techniques can also be 
used in an educational setting, for example, by a 
teacher who is creating simplified texts to students. 
Classic literature books, for example, can be quite 
hard even to experienced readers. Some genres of 
texts already have simplified versions, even though 
the simplification level can be inadequate to a 
specific target audience. For instance, 3rd and 7th 
grade students have distinct comprehension levels. 
In our approach, the number and type of 
simplification operations applied to sentences 
determine its appropriateness to a given literacy 
level, allowing the creation of multiple versions of 
the same text, with different levels of complexity, 
targeting special student needs. 
The Simplifica editor allows the teacher to adopt 
any particular texts to be used in the class, for 
example, the teacher may wish to talk about current 
news events with his/her students, which would not 
be available via any repository of simplified texts. 
The teacher can customize the text generating 
process and gradually increase the text complexity 
as his/her students comprehension skills evolve. The 
use of the editor also helps the teacher to develop a 
special awareness of the language, which can 
improve his/her interaction with the students.  
Students can also use the system whenever they 
have difficulties to understand a text given in the 
classroom. After a student reads the simplified text, 
the reading of the original text becomes easier, as a 
result of the comprehension of the simplified text. In 
this scenario, reading the original text can also help 
the students to learn new and more complex words 
and syntactic structures, which would be harder for 
them without reading of the simplified text. 
6 Conclusions 
The potentialities of text simplification systems for 
education are evident. For students, it is a first step 
for more effective learning. Under another 
perspective, given the Brazilian population literacy 
levels, we consider text simplification a necessity. 
For poor literacy people, we see text simplification 
as a first step towards social inclusion, facilitating 
and developing reading and writing skills for people 
to interact in society. The social impact of text 
simplification is undeniable. 
In terms of language technology, we not only 
introduced simplification tools in this paper, but also 
investigated which linguistic phenomena should be 
simplified and how to simplify them. We also 
developed a representation schema and designed an 
on-line authoring system. Although some aspects of 
the research are language dependent, most of what 
we propose may be adapted to other languages. 
Next steps in this research include practical 
applications of such technology and the 
measurement of its impact for both education and 
social inclusion. 
Acknowledgments 
We thank the Brazilian Science Foundation FAPESP 
and Microsoft Research for financial support. 
References 
Alu?sio, S.M., Specia, L., Pardo, T.A.S., Maziero, E.G., 
Fortes, R. 2008. Towards Brazilian Portuguese 
Automatic Text Simplification Systems. In the 
Proceedings of the 8th ACM Symposium on Document 
Engineering, pp. 240-248. 
Bick, E. 2000. The parsing system ?Palavras?: 
41
Automatic grammatical analysis of Portuguese in a 
constraint grammar framework. PhD Thesis 
University of ?rhus, Denmark. 
Burstein, J. 2009. Opportunities for Natural Language 
Processing Research in Education. In the  Proceedings 
of CICLing, pp. 6-27.  
Caseli, H., Pereira, T.F., Specia, L., Pardo, T.A.S., 
Gasperin, C., Aluisio, S. 2009. Building a Brazilian 
Portuguese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Caseli, H.M.; Nunes, M.G.V.; Forcada, M.L. 2008. 
Automatic induction of bilingual resources from 
aligned parallel corpora: application to shallow-
transfer machine translation. Machine Translation, V. 
1, p. 227-245. 
Devlin, S., Unthank, G. 2006. Helping aphasic people 
process online information. In the Proceedings of the 
ACM SIGACCESS Conference on Computers and 
Accessibility, pp. 225-226. 
Graesser, A., McNamara, D. S., Louwerse, M., Cai, Z. 
2004. Coh-Metrix: Analysis of text on cohesion and 
language. Behavioral Research Methods, Instruments, 
and Computers, V. 36, pp. 193-202. 
Inui, K., Fujita, A., Takahashi, T., Iida, R., Iwakura, T. 
2003. Text Simplification for Reading Assistance: A 
Project Note. In the Proceedings of the Second 
International Workshop on Paraphrasing, 9 -16.  
Klebanov, B., Knight, K., Marcu, D. 2004. Text 
Simplification for Information-Seeking Applications. 
On the Move to Meaningful Internet Systems. LNCS, 
V.. 3290, pp. 735-747. 
Lal, P., Ruger, S. 2002. Extract-based summarization with 
simplification. In the Proceedings of DUC.  
Mapleson, D.L. 2006. Post-Grammatical Processing for 
Discourse Segmentation. PhD Thesis. School of 
Computing Sciences, University of East Anglia, 
Norwich. 
Max, A. 2005. Simplification interactive pour la 
production de textes adapt es aux personnes souffrant 
de troubles de la compr ehension. In the Proceedings 
of Traitement Automatique des Langues Naturelles 
(TALN).  
Max, A. 2006. Writing for language-impaired readers. In 
the  Proceedings of CICLing, pp. 567-570.  
Muniz, M.C., Laporte, E. Nunes, M.G.V. 2005. UNITEX-
PB, a set of flexible language resources for Brazilian 
Portuguese. In Anais do III Workshop em Tecnologia 
da Informa??o e da Linguagem Humana, V. 1, pp. 1-
10. 
Pardo, T.A.S.  and Nunes, M.G.V. 2008. On the 
Development and Evaluation of a Brazilian 
Portuguese Discourse Parser. Journal of Theoretical 
and Applied Computing, V. 15, N. 2, pp. 43-64. 
Petersen, S.E. 2007. Natural Language Processing Tools 
for Reading Level Assessment and Text Simplification 
for Bilingual Education. PhD Thesis, University of 
Washington.  
Petersen, S.E. and Ostendorf, M. 2007. Text 
Simplification for Language Learners: A Corpus 
Analysis. In the Proceedings of the Speech and 
Language Technology for Education Workshop, pp. 
69-72. 
Specia, L., Alu?sio, S.M., Pardo, T.A.S. 2008. Manual de 
simplifica??o sint?tica para o portugu?s. Technical 
Report NILC-TR-08-06, NILC. 
Siddharthan, A. 2002. An Architecture for a Text 
Simplification System. In the Proceedings of the 
Language Engineering Conference, pp. 64-71. 
Siddharthan, A. 2003. Syntactic Simplification and Text 
Cohesion. PhD Thesis. University of Cambridge. 
Snow, C. 2002. Reading for understanding: Toward an 
R&D program in reading comprehension. Santa 
Monica, CA. 
 
42
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 16?23,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
A Machine Learning Approach to Automatic Term Extraction
using a Rich Feature Set?
Merley da Silva Conrado, Thiago A. Salgueiro Pardo, and Solange Oliveira Rezende
Laboratory of Computational Intelligence,
An Interinstitutional Center for Research and Development in Computational Linguistic,
Institute of Mathematical and Computer Sciences,
University of Sao Paulo (USP),
P.O. Box 668, 13561-970, Sao Carlos-SP, Brazil
{merleyc,taspardo,solange}@icmc.usp.br
Abstract
In this paper we propose an automatic term
extraction approach that uses machine learn-
ing incorporating varied and rich features of
candidate terms. In our preliminary experi-
ments, we also tested different attribute se-
lection methods to verify which features are
more relevant for automatic term extraction.
We achieved state of the art results for uni-
gram extraction in Brazilian Portuguese.
1 Introduction
Terms are terminological units from specialised
texts (Castellv?? et al, 2001). A term may be: (i) sim-
ple1 (a single element), such as ?biodiversity?, or (ii)
complex (more than one element), such as ?aquatic
ecosystem? and ?natural resource management?.
Automatic term extraction (ATE) methods aim to
identify terminological units in specific domain cor-
pora (Castellv?? et al, 2001). Such information is ex-
tremely useful for several tasks, from the linguistic
perspective of building dictionaries, taxonomies and
ontologies, to computational applications as infor-
mation retrieval, extraction, and summarisation.
Although ATE has been researched for more than
20 years, there is still room for improvement. There
are four major ATE problems. The first one is that
the ATE approaches may extract terms that are not
actual terms (?noise?) or do not extract actual terms
(?silence?). Considering the ecology domain, an ex-
ample of silence is when a term (e.g., pollination),
?This research was supported by FAPESP (Proc. No.
2009/16142-3 and 2012/09375-4), Brazil.
1When we refer to unigrams, we mean simple terms.
with low frequency, is not considered a candidate
term (CT), and, therefore, it will not appear in the
extracted term list if we consider its frequency. Re-
garding noise, if we consider that nouns may be
terms and that adjectives may not, if an adjective
(e.g., ecological) is mistakenly tagged as a noun, it
will be wrongly extracted as a term. The second
problem is the difficulty in dealing with extremely
high number of candidates (called the high dimen-
sionality of candidate representation) that requires
time to process them. Since the ATE approaches ge-
nerate large lists of TCs, we have the third problem
that is the time and human effort spent for validat-
ing the TCs, which usually is manually performed.
The fourth problem is that the results are still not sa-
tisfactory and there is a natural ATE challenge since
the difficulty in obtaining a consensus among the ex-
perts about which words are terms of a specific do-
main (Vivaldi and Rodr??guez, 2007).
Our proposed ATE approach uses machine learn-
ing (ML), since it has been achieving high precision
values (Zhang et al, 2008; Foo and Merkel, 2010;
Zhang et al, 2010; Loukachevitch, 2012). Although
ML may also generate noise and silence, it facili-
tates the use of a large number of TCs and their fea-
tures, since ML techniques learn by themselves how
to recognize a term and then they save time extract-
ing them.
Our approach differs from others because we
adopt a rich feature set using varied knowledge lev-
els. With this, it is possible to decrease the silence
and noise and, consequently, to improve the ATE
results. Our features range from simple statistical
(e.g., term frequency) and linguistic (e.g., part of
16
speech - POS) knowledge to more sophisticated hy-
brid knowledge, such as the analysis of the term
context. As far as we know, the combined use of
this specific knowledge has not been applied before.
Another difference is that we apply 3 statistical fea-
tures (Term Variance (Liu et al, 2005), Term Vari-
ance Quality (Dhillon et al, 2003), and Term Con-
tribution (Liu et al, 2003)) that to date have only
been used for attribute selection and not for term ex-
traction. As far as we know, the combined use of
this specific knowledge and feature feedback has not
been applied before. We also propose 4 new linguis-
tic features for ATE. All these features are detailed in
Section 4. Finally, for the first time, ML is being ap-
plied in the task of ATE in Brazilian Portuguese (BP)
corpora. Our approach may also be easily adapted to
other languages.
We focus on extracting only unigram terms, since
this is already a complex task. We run our experi-
ments on 3 different corpora. Our main contribution
is the improvement of precision (in the best case, we
improve the results 11 times) and F-measure (in the
best case, we improve 2 times).
Section 2 presents the main related work. Section
3 describes our ATE approach. Section 4 details the
experiments, and Section 5 reports the results. Con-
clusions and future work are presented in Section 6.
2 Related Work
There are several recent and interesting studies that
are not focused on extracting unigrams (Estopa` et
al., 2000; Almeida and Vale, 2008; Zhang et al,
2008; Zhang et al, 2010; Nazar, 2011; Vivaldi et al,
2012; Lopes, 2012). Normally, ATE studies use cor-
pora of different domain and language and, in some
cases, the authors use different evaluation measures.
Regardless of variation (e.g., the size of the test cor-
pora), we mention studies that have highlighted re-
sults for unigrams2. When possible, we show the
best precision (P) of the related work and its recall
(R).
(Ventura and Silva, 2008) extracted terms using
statistical measures that consider the predecessors
and successors of TCs. They achieved, for English,
P=81.5% and R=55.4% and, for Spanish, P=78.2%
2It is not specified if (Zhang et al, 2010) extracted simple or
complex terms.
and R=60.8%. For Spanish, the Greek forms of a
candidate and their prefix may help to extract terms
(e.g., the Greek formant laring that belongs to the
term laringoespasm in the medical domain) (Vivaldi
and Rodr??guez, 2007), achieving about P=55.4%
and R=58.1%. For Spanish, (Gelbukh et al, 2010)
compared TCs of a domain with words of a general
corpus using Likelihood ratio based distance. They
achieved P=92.5%. For Brazilian Portuguese, the
ExPorTer methods are the only previous work that
uniquely extract unigrams (Zavaglia et al, 2007).
Therefore, they are the state of the art for unigrams
extraction for BP. The linguistic ExPorTer consid-
ers terms that belong to some POS patterns and uses
indicative phrases (such as is defined as) that may
identify where terms are. It achieved P=2.74% and
R=89.18%. The hybrid ExPorTer used these lin-
guistic features with frequency and Likelihood ratio.
The latter one obtained P=12.76% and R=23.25%.
3 Term Extraction Approach based on
Machine Learning
In order to model the ATE task as a machine learn-
ing solution, we consider each word in the input
texts3 of a specific domain (except the stopwords)
as a learning instance (candidate term). For each in-
stance, we identify a set of features over which the
classification is performed. The classification pre-
dicts which words are terms (unigrams) of a specific
domain. We test different attribute selection meth-
ods in order to verify which features are more rele-
vant to classify a term.
We start by preprocessing the input texts, as
shown in Figure 1. This step consists of POS tag-
ging the corpora and normalizing4 the words of the
texts. The normalization minimizes the second ATE
problem because it allows working with a lower CT
representation dimensionality. When working with
a lower dimensionality, the words that do not help
identify terms are eliminated. Consequently, fewer
candidates should to be validated or refuted as terms
(it would minimize the third ATE problem). When
working with fewer candidates it also may improve
the result quality (it handles the fourth ATE prob-
3When we refer to texts, we mean documents.
4Normalization consists of standardizing the words by re-
ducing their variations.
17
lem), and, definitely, it spends less time and fewer
resources to carry out the experiments. By improv-
ing the results, consequently, we minimize silence
and noise, which handles the first ATE problem.
Afterwards, we remove stopwords.
In order to identify a set of features over which
the classification is performed, we studied and tested
several measures. The feature identification is the
most important step of our approach. We divide the
features into two types: (i) the features that obtain
statistical, linguistic, and hybrid knowledge from the
input corpus, such as TFIDF and POS, and (ii) the
features that obtain these knowledge from measures
that use other corpora besides the input corpus. The
corpora belong to another domain that is different of
the input corpus domain (called contrastive corpora)
or not belong to any specific domain (called general
corpora). Our hypothesis is that, with the joining of
features of different levels of knowledge, it is possi-
ble to improve the ATE.
Figure 1: Term extraction approach proposed.
4 Experimental Setup
At this point, for obtaining the knowledge in order
to extract terms, we tested 17 features that do not
depend on general or contrastive corpora and 2 fea-
tures that depend on these corpora. We intend to
explore more features (and we will possibly propose
new measures) that use contrastive or general cor-
pora or any taxonomic structure. The experiments
that expand the number of features are ongoing now.
We used 3 corpora of different domains in the
Portuguese language. The EaD corpus (Souza and
Di Felippo, 2010) has 347 texts about distance edu-
cation and has a gold standard with 118 terms5 (Gi-
5(Gianoti and Di Felippo, 2011) stated that the EaD unigram
gold standard has 59 terms, but in this paper we used 118 uni-
grams that the authors provided us prior to their work.
anoti and Di Felippo, 2011). The second one is the
ECO6 corpus (Zavaglia et al, 2007). It contains 390
texts of ecology domain and its gold standard has
322 unigrams. The latter is the Nanoscience and
Nanotechnology (N&N) corpus (Coleti et al, 2008)
that contains 1,057 texts. Its gold standard has 1,794
unigrams (Coleti et al, 2008; Coleti et al, 2009).
In order to preprocess these corpora, we POS
tagged them using the PALAVRAS parser (Bick,
2000) and normalized their words using a stem-
ming7 technique. Stemming was chosen because of
its capacity to group similar word meanings, and its
use decreases representation dimensionality of can-
didate terms, which minimizes the second and third
ATE problems. Afterwards, we removed the stop-
words8, the conjugation of the verb ?to be?, punctu-
ation, numbers, accents, and the words composed of
only one character are removed.
We identify and calculate 19 features in which 11
features are used for ATE in the literature, 3 features
are normally applied to the attribute selection tasks
(identified by *), 1 normally used for Named Entity
Recognition (identified by **), and we created 4 new
features (identified by ?). These features are shown
in Table 1, accompanied by the hypotheses that un-
derlie their use. They are also divided into 3 levels
of knowledge: statistical, linguistic, and hybrid.
For the S feature, we removed stopwords at the
beginning and at the end of these phrases. For
POS, we assumed that terms may also be adjectives
(Almeida and Vale, 2008), besides nouns and verbs.
For GC and Freq GC, we used the NILC Corpus9 as
a general corpus, which contains 40 million words.
We created and used 40 indicative phrases (NPs).
For example, considering are composed of as an IP
in All organisms are composed of one or more cells,
we would consider organisms and cells as TCs. For
features related to CT stem, we analyzed, e.g., the
words educative, educators, education and educate
that came from the stem educ. Therefore, educ may
6ECO corpus - http://www.nilc.icmc.usp.br/
nilc/projects/bloc-eco.htm
7PTStemmer: A Stemming toolkit for the Portuguese lan-
guage - http://code.google.com/p/ptstemmer/
8Stoplist and Indicative Phrase list are avaiable in
http://www2.icmc.usp.br/ merleyc/
9NILC Corpus - http://www.nilc.icmc.usp.br/
nilc/tools/corpora.htm
18
Table 1: Features of candidate terms.
Feature Description Hypothesis
The eight linguistic features
S noun and prepositional phrases terms are noun phrases and, sometimes, prepositional phrases
N S head of phrases heads of noun and prepositional phrases
POS noun, proper noun, and adjective terms follow some patterns
IP indicative phrases IPs may identify definitions/descriptions that may be terms
N noun ? number of nouns
N adj ? number of adjectives stemmed terms come from
N verb ? number of verbs higher number of nouns
N PO ? total of words from which stemmed TCs come from than adjectives or verbs
The seven statistical features
SG** n-gram length each domain has a term pattern
TF Term Frequency terms have neither low nor very high frequencies
DF Document Frequency terms appear in at least certain number of documents
TFIDF Term Frequency Inverse Document Frequency terms are very common in the corpus(Salton and Buckley, 1987) but they occur in few documents in this corpus
TCo* Term Contribution (Liu et al, 2003) terms help to distinguish the different documents
TV* Term Variance (Liu et al, 2005) terms do not have low frequency in documents and maintain a
TVQ* Term Variance Quality (Dhillon et al, 2003) non-uniform distribution throughout corpus (higher variance)
The four hybrid features
GC CT occurrence in general corpus terms do not occur with high frequency in a general corpusFreq GC CT frequency in GC
C-value the potential of a CT to be a term (Frantzi et al, 1998) the C-value helps to extract terms
NC-value CT context (Frantzi et al, 1998) candidate context helps to extract terms
have as features N Noun = 2 (educators and educa-
tion), N Adj = 1 educative, N Verb = 1 (educate),
and N PO = 4 (total number of words). Our hy-
pothesis is that stemmed candidates that were origi-
nated from a higher number of nouns than adjectives
or verbs will be terms. Finally, we used NC-Value
adapted to unigrams (Barro?n-Ceden?o et al, 2009).
After calculating the features for each unigram
(candidate term), the CT representation has high di-
mensionality (it is the second ATE problem) and,
hence, the experiments may take a considerable
amount of time to be executed. To decrease this di-
mensionality and, consequently, the number of TCs
(which corresponds to the second and third ATE
problems, respectively), we tested two different cut-
offs, which preserve only TCs that occur in at least
two documents in the corpus. The first cut-off is
called C1. In the second one (called C2), the can-
didates must be noun and prepositional phrases and
also follow some of these POS: nouns, proper nouns,
verbs, and adjectives. The number of obtained can-
didates (stems) was 10,524, 14,385, and 46,203,
for the ECO, EaD, and N&N corpora, respectively.
When using the C1 cut-off, we decreased to 55,15%,
45,82%, and 57,04%, and C2 decreased 63.10%,
63.18%, 66.94% in relation to the number of all the
obtained candidates (without cutt-offs).
5 Experimental Evaluation and Results
The first evaluation aimed to identify which fea-
tures must be used for ATE (see Section 3). For
that, we applied 2 methods that select attributes by
evaluating the attribute subsets. Their evaluation is
based on consistency (CBF) and correlation (CFS).
We also tested search methods. The combination
of these methods, available in WEKA (Hall et al,
2009), is: CFS SubsetEval using the RankSearch
Filter as search method (CFS R), CFS SubsetEval
using the BestFirst as search method (CFS BF),
CBF SubsetEval using the Ranking Filter (C R),
and CBF SubsetEval using the Greedy Stepwise
(C G). These methods return feature sets that are
considered the most representative for the term clas-
sification (Table 2). For the EaD corpus, the CG at-
tribute selection method did not select any feature.
For our experiments, we also considered all the fea-
tures (referred by All). Additionally, we compared
the use of two cut-off types for each feature set, C1
and C2, detailed in Section 4.
For both evaluations8, we chose largely known
inductors in the machine learning area. They rep-
resent different learning paradigms: JRip (Rule In-
duction), Na??ve Bayes (Probabilistic), J48 (Decision
Tree) with confidence factor of 25%, and SMO (Sta-
tistical Learning). All of these algorithms are avail-
19
Table 2: Features chosen by the attribute selection meth-
ods.
Methods CorporaEaD ECO N&N
CFS R
TFIDF, TV, TVQ, TFIDF, TV, TVQ, Freq, TFIDF, TVQ,
IP, N Noun, N Adj POS, N Noun IP, Cvalue, N Noun,
POS, N Adj, N PO
CFS BF
Same as in the TFIDF, TVQ, Freq, TFIDF, TV,
CFS R method. TCo, POS IP, Cvalue, N Noun,
POS, N Adj, N PO
C R
Freq, DF, TFIDF, Freq, DF, TFIDF, Freq, DF, TFIDF,
TV, TVQ, TCo, IP, TV, TVQ, TCo, GC, TV, TVQ, TCo, GC,
GC, POS, FreqGC, Cvalue, NCvalue, IP, S, Cvalue, POS,
NCvalue, Cvalue, IP, S, N S, POS, NCvalue, N S,
N Adj, N Noun, N Noun, N Adj, N Noun, N Adj,
N Verb, N PO N Verb, N PO N Verb, N PO
C G
Method did Freq, DF, TFIDF, Freq, DF, TFIDF, S,
not select any TV, TVQ, GC, IP, TV, TVQ, TCo, IP,
feature. N S, NCvalue, NCvalue, N S, POS,
S, N Noun, POS, GC, N Noun, N PO,
N Adj, N PO N Verb, N Adj
able in WEKA and described in (Witten and Frank,
2005). We run the experiments on a 10 fold cross-
validation and calculated the precision, recall, and
F-measure scores of term classification according to
the gold standard of unigrams of each corpus. Using
default parameter values for SMO, the results were
lower than the other inductors. Due to this fact and
the lack of space in the paper, we do not present the
SMO results here.
The best precision obtained for the EaD corpus
using the term classification, 66.66%, was achieved
by the C R attribute selection method with the C2
cut-off (C R-C2) using the JRIP inductor. The best
recall score, 20.96%, was obtained using Na??ve
Bayes with the CFS R-C1 method. The best F-
measure was 17.58% using the J48 inductor with
C R-C2. For the ECO corpus, the best precision
was 60% obtained with the J48 inductor with con-
fidence factor of 25% and the C R-C1 method. The
best recall was 21.40% with JRIP and the C G-C1
method. Our best F-measure was 24.26% obtained
with Na??ve Bayes using the CFS R-C1 method.
For the N&N corpus, the best precision score was
61.03% using JRIP. The best recall was 52.53% and
the best F-measure score was 54.04%, both using
J48 inductor with confidence factor of 25%. The
three results used the All-C2 method.
Table 3 shows the comparison of our best results
with 2 baselines, which are the well-known term fre-
quency and TFIDF, using our stoplist. We also con-
sidered all the stemmed words of these corpora as
CT, except the stopwords, and we calculated the pre-
cision, recall, and F-measure scores for these words
as well. Finally, we compared our results with the
third baseline, which is the only previous work that
uniquely extracts unigrams (Zavaglia et al, 2007),
described in Section 2. Therefore, this is the state
of the art for unigrams extraction for Portuguese. In
order to compare this work with our results of the
EaD and N&N corpora, we implemented the ATE
method of Zavaglia et al We have to mention that
this method uses the normalization technique called
lemmatization instead of stemming, which we used
in our method. The only difference between our im-
plementation descriptions and the original method is
that we POS tagged and lemmatizated the texts using
the same parser (PALAVRAS10 (Bick, 2000)) used
in our experiments instead of the MXPOST tagger
(Ratnaparkhi, 1996).
For all used corpora, we obtained better results of
precision and F-measure comparing with the base-
lines. In general, we improve the ATE precision
scores, for the EaD corpus, eleven times (from 6.1%
to 66.66%) and, for the N&N corpus, one and a half
times (from 35.4% to 61.03%), both comparing our
results with the use of TFIDF. For the ECO corpus,
we improve four and a half times (from 12.9% to
60%), by comparing with the use of frequency. We
improve the ATE F-measure scores, for the EaD cor-
pus, one and a half times (from 10.93% to 17.58%);
for the ECO corpus, we slightly improve the results
(from 20.64% to 24.26%); and, for the N&N cor-
pus, two times (from 28.12% to 54.04%). The last
three cases are based on the best F-measure values
obtained using TFIDF. Regarding recall, on the one
hand, the linguistic ExPorTer method (detailed in
Section 2), to which we also compare our results,
achieved better recall for all used corpora, about
89%. On the other hand, its precision (about 2%)
and F-measure (about 4%) were significantly lower
than our results.
Finally, if we compare our results with the results
of all stemmed words, with the exception of the stop-
words, the recall values of the latter are high (about
76%) for all used corpora. However, the precision
scores are extremely low (about 1.26%), because it
used almost all words of the texts.
10As all NLP tools for general domains, PALAVRAS is not
excellent for specific domains. However, as it would be expen-
sive (time and manual work) to customize it for each specific
domain that we presented in this paper, we decided use it, even
though there are error tagging.
20
Table 3: Comparison with baselines.
Method Precision Recall F-Measure(%) (%) (%)
The EaD corpus
JRIP with C R-C2 66.66 8.06 14.38
Na??ve Bayes 13.19 20.96 16.19with CFS R-C1
J48 with F.C. of 27.58 12.9 17.580.25 with C R-C2
Ling. ExPorTer 0.33 89.70 0.66
Hyb. ExPorTer 0.07 17.64 0.15
Frequency 5.9 50.86 10.57
TFIDF 6.1 52.58 10.93
All the corpus 0.52 62.9 1.04
The ECO corpus
J48 with F.C. of 60.00 6.02 10.94
0.25 with C R-C1
JRIP with C G-C1 23.44 21.40 22.38
Na??ve Bayes 33.33 19.06 24.26
with CFS R-C1
Ling. ExPorTer 2.74 89.18 5.32
Hyb. ExPorTer 12.76 23.25 16.48
Frequency 12.9 43.28 19.87
TFIDF 13.4 44.96 20.64
All the corpus 1.48 99.07 2.92
The N&N corpus
JRIP with All-C2 61.03 27.73 38.14
J48 with F.C. of 55.64 52.53 54.04
0.25 with All-C2
Ling. ExPorTer 3.75 89.40 7.20
Hyb. ExPorTer 1.68 35.35 3.22
Frequency 31.6 20.83 25.1
TFIDF 35.4 23.33 28.12
All the corpus 1.83 66.99 3.57
6 Conclusions and Future Work
This paper described ongoing experiments about
unigrams extraction usingML. Our first contribution
regarding the experiments was to create 4 features
and to test 4 features that normally are applied to
other tasks and not for automatic term extraction.
Our second contribution is related to the first and
fourth ATE problems, which are the existence of si-
lence and noise and low ATE results, respectively.
We achieved state of art results for unigrams in
Brazilian Portuguese. We improved, for all used cor-
pora, precision (in the best case, we improve the re-
sults 11 times using the EaD corpus) and F-measure
(in the best case, 2 times using the N&N corpus)
and, consequently, we minimized silence and noise.
The third contribution is about the features that
are better for extracting domain terms. All the tested
attribute selection methods indicated the TFIDF as
an essential feature for ATE. 90.9% of the meth-
ods selected N Noun and TVQ, and 81.81% selected
TV, IP, N adj, and POS as relevant features. How-
ever, only one of these methods chose Freq GC, and
none of them chose the SG feature. Regarding the
levels of knowledge - statistical, linguistic, and hy-
brid - in which each feature was classified, at least
45.45% of the methods chose 6 statistical, 5 linguis-
tic, and 3 hybrid features. We also observed that the
best F-measures (see Tables 2 and 3) were obtained
when using at least linguistic and statistical features
together. This fact proves that our main hypothesis is
true, because we improved the ATE results by join-
ing features of different levels of knowledge. Addi-
tionally, we allow the user to choose the features that
are better for term extraction.
As the fourth contribution, we minimized the
problem of high dimensionality (as mentioned, the
second ATE problem) by means of the use of two
different cut-offs (C1 and C2). By reducing the
number of TCs, fewer candidates were validated or
refuted as terms and, consequently, we minimized
the third ATE problem, which is the time and human
effort for validating the TCs. However, we still per-
ceived the need to reduce more the number of can-
didates. Therefore, for future work, we intend to use
instance selection techniques to reduce the term rep-
resentation.
We believe to have achieved significant results for
the experiments realized to date. Experiments using
more features that dependent on general corpus are
ongoing. We will also possibly propose new features
and will use taxonomic structure in order to improve
more the results. For using the taxonomic structure,
we intend to create a conventional taxonomy (Mi-
iller and Dorre, 1999) is created using the input cor-
pus. Therefore, we may identify more features for
the instances considering this taxonomy. For exam-
ple, normally in a taxonomy?s leaf specific words
of a domain happen, consequently, terms should ap-
pear there. Additionally, we are encouraged to adapt
these features for bigram and trigram terms as well.
References
G. M. B. Almeida and O. A. Vale. 2008. Do texto
ao termo: interac?a?o entre terminologia, morfologia e
21
lingu??stica de corpus na extrac?a?o semi-automa?tica de
termos. In A. N. Isquerdo and M. J. B. Finatto, edi-
tors, As Cie?ncias do Le?xico: Lexicologia, Lexicografia
e Terminologia, volume IV, pages 483?499. UFMS,
MS, Brazil, 1 edition.
A. Barro?n-Ceden?o, G. Sierra, P. Drouin, and S. Anani-
adou. 2009. An improved automatic term recogni-
tion method for spanish. In Proc of the 10th Int. CNF
on Computational Linguistics and Intelligent Text Pro-
cessing, pages 125?136, Berlin, Heidelberg. Springer-
Verlag.
E. Bick. 2000. The Parsing System ?PALAVRAS?. Auto-
matic Grammatical Analysis of Portuguese in a Con-
straint Grammar Framework. University of Arhus,
Arhus.
M. T. Cabre? Castellv??, R. Estopa` Bagot, and Jordi Vivaldi
Palatresi. 2001. Automatic term detection: a review
of current systems. In D. Bourigault, C. Jacquemin,
and M-C. L?Homme, editors, Recent Advances in
Computational Terminology, pages 53?88, Amster-
dam/Philadelphia. John Benjamins.
J. S. Coleti, D. F. Mattos, L. C. Genoves Junior, A. Can-
dido Junior, A. Di Felippo, G. M. B. Almeida,
S. M. Alu??sio, and O. N. Oliveira Junior. 2008.
Compilac?a?o de Corpus em L??ngua Portuguesa na
a?rea de Nanocie?ncia/Nanotecnologia: Problemas e
soluc?o?es, volume 1. Tagnin and Vale., SP, Brazil, 192
edition.
J. S. Coleti, D. F. Mattos, and G. M. B. Almeida. 2009.
Primeiro diciona?rio de nanocie?ncia e nanotecnolo-
gia em l??ngua portuguesa. In Marcelo Fila Pecenin,
Valdemir Miotello, and Talita Aparecida Oliveira, ed-
itors, II Encontro Acade?mico de Letras (EALE), pages
1?10. Caderno de Resumos do II EALE.
I. Dhillon, J. Kogan, and C. Nicholas. 2003. Feature
selection and document clustering. In M. W. Berry,
editor, Survey of Text Mining, pages 73?100. Springer.
R. Estopa`, J. Vivaldi, and M. T. Cabre?. 2000. Use of
greek and latin forms for term detection. In Proc of
the 2nd on LREC, pages 855?861, Greece. ELRA.
J. Foo and M. Merkel. 2010. Using machine learning
to perform automatic term recognition. In N. Bel,
B. Daille, and A. Vasiljevs, editors, Proc of the 7th
LREC - Wksp on Methods for automatic acquisition
of Language Resources and their Evaluation Methods,
pages 49?54.
K. T. Frantzi, S. Ananiadou, and J. I. Tsujii. 1998.
The C-value/NC-value method of automatic recogni-
tion for multi-word terms. In Proc of the 2nd ECDL,
pages 585?604, London, UK. Springer-Verlag.
A. F. Gelbukh, G. Sidorov, E. Lavin-Villa, and
L. Chanona-Herna?ndez. 2010. Automatic term ex-
traction using log-likelihood based comparison with
general reference corpus. In NLDB, pages 248?255.
A. C. Gianoti and A. Di Felippo. 2011. Extrac?a?o de con-
hecimento terminolo?gico no projeto TermiNet. Tech-
nical Report NILC-TR-11-01, Instituto de Cie?ncias
Matema?ticas e de Computac?a?o (ICMC) - USP, SP,
Brazil.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: An update. In SIGKDD-ACM, vol-
ume 11, pages 10?18.
T. Liu, S. Liu, and Z. Chen. 2003. An evaluation on
feature selection for text clustering. In Proceedings of
the 10th Int. CNF on Machine Learning, pages 488?
495, San Francisco, CA, USA. Morgan Kaufmann.
L. Liu, J. Kang, J. Yu, and Z. Wang. 2005. A compar-
ative study on unsupervised feature selection methods
for text clustering. In Proc of IEEE NLP-KE, pages
597?601.
L. Lopes. 2012. Extrac?a?o automa?tica de conceitos a par-
tir de textos em l??ngua portugesa. Ph.D. thesis, Porto
Alegre, RS. Pontif??cia Universidade do Rio Grande do
Sul (PUCRS).
N. Loukachevitch. 2012. Automatic term recognition
needs multiple evidence. In N. Calzolari, K. Choukri,
T. Declerck, M. Dogan, B. Maegaard, J. Mariani,
Odijk, and S. Piperidis, editors, Proc of the 8th on
LREC, pages 2401?2407, Istanbul, Turkey. ELRA.
A. Miiller and J. Dorre. 1999. The taxgen frame-
work: Automating the generation of a taxonomy for
a large document collection. In Proceedings of the
Thirty-Second Annual Hawaii International Confer-
ence on System Sciences (HICSS), volume 2, pages
2034?2042, Washington, DC, USA. IEEE Computer
Society.
R. Nazar. 2011. A statistical approach to term extraction.
Int. Journal of English Studies, 11(2).
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. Proc of the CNF on EMNLP,
pages 491?497.
G. Salton and C. Buckley. 1987. Term weighting ap-
proaches in automatic text retrieval. Technical report,
Ithaca, NY, USA.
J. W. C. Souza and A. Di Felippo. 2010. Um exerc??cio
em lingu?istica de corpus no a?mbito do projeto Ter-
miNet. Technical Report NILC-TR-10-08, ICMC -
USP, SP, Brazil.
J. Ventura and J. F. Silva. 2008. Ranking and extrac-
tion of relevant single words in text. In Cesare Rossi,
editor, Brain, Vision and AI, pages 265?284. InTech,
Education and Publishing.
J. Vivaldi and H. Rodr??guez. 2007. Evaluation of terms
and term extraction systems: A practical approach.
Terminology, 13(2):225?248.
22
J. Vivaldi, L. A. Cabrera-Diego, G. Sierra, and M. Pozzi.
2012. Using wikipedia to validate the terminology
found in a corpus of basic textbooks. In N. Calzolari,
K. Choukri, T. Declerck, M. U. Dogan, B. Maegaard,
J. Mariani, J. Odijk, and S. Piperidis, editors, Proc of
the 8th Int. CNF on LREC, Istanbul, Turkey. ELRA.
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition (Morgan Kaufmann Series in Data Manage-
ment Systems). Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
C. Zavaglia, L. H. M. Oliveira, M. G. V. Nunes, and
S. M. Alu??sio. 2007. Estrutura ontolo?gica e unidades
lexicais: uma aplicac?a?o computacional no dom??nio da
ecologia. In Proc. of the 5th Wksp em Tecnologia da
Informac?a?o e da Linguagem Humana, pages 1575?
1584, RJ, Brazil. SBC.
Z. Zhang, J. Iria, C. Brewster, and F. Ciravegna. 2008.
A comparative evaluation of term recognition algo-
rithms. In N. Calzolari (CNF Chair), K. Choukri,
B. Maegaard, J. Mariani, J. Odjik, S. Piperidis, and
D. Tapias, editors, Proc of the 6th on LREC, pages
2108?2113, Marrakech, Morocco. ELRA.
X. Zhang, Y. Song, and A. Fang. 2010. Term recogni-
tion using conditional random fields. In Proc of IEEE
NLP-KE, pages 333?336.
23
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 568?572, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NILC USP: A Hybrid System for Sentiment Analysis in Twitter Messages
Pedro P. Balage Filho and Thiago A. S. Pardo
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Science, University of Sa?o Paulo
Sa?o Carlos - SP, Brazil
{balage, taspardo}@icmc.usp.br
Abstract
This paper describes the NILC USP system
that participated in SemEval-2013 Task 2:
Sentiment Analysis in Twitter. Our system
adopts a hybrid classification process that
uses three classification approaches: rule-
based, lexicon-based and machine learning
approaches. We suggest a pipeline architec-
ture that extracts the best characteristics from
each classifier. Our system achieved an F-
score of 56.31% in the Twitter message-level
subtask.
1 Introduction
Twitter and Twitter messages (tweets) are a modern
way to express sentiment and feelings about aspects
of the world. In this scenario, understanding the sen-
timent contained in a message is of vital importance
in order to understand users behavior and for mar-
ket analysis (Java et al, 2007; Kwak et al, 2010).
The research area that deals with the computational
treatment of opinion, sentiment and subjectivity in
texts is called sentiment analysis (Pang et al, 2002).
Sentiment analysis is usually associated with a
text classification task. Sentiment classifiers are
commonly categorized in two basic approaches:
lexicon-based and machine learning (Taboada et al,
2011). A lexicon-based classifier uses a lexicon to
provide the polarity, or semantic orientation, of each
word or phrase in the text. A machine learning clas-
sifier learns features (usually the vocabulary) from
annotated corpus or labeled examples.
In this paper, we present a hybrid system for senti-
ment classification in Twitter messages. Our system
combines three different approaches: rule-based,
lexicon-based and machine learning. The purpose of
our system is to better understand the use of a hybrid
system in Twitter text and to verify the performance
of this approach in an open evaluation contest.
Our system participated in SemEval-2013 Task
2: Sentiment Analysis in Twitter (Wilson et al,
2013). The task objective was to determine the sen-
timent contained in Twitter messages. The task in-
cluded two sub-tasks: a expression-level classifi-
cation (Task A) and a message-level classification
(Task B). Our system participated in Task B. In this
task, for a given message, our system should classify
it as positive, negative, or neutral.
Our system was coded using Python and the
CLiPS Pattern library (De Smedt and Daelemans,
2012). This last library provides the part-of-speech
tagger and the SVM algorithm used in this work1.
2 Related work
Despite the significant number of works in senti-
ment analysis, few works have approached Twit-
ter messages. Agarwal et al (2011) explored new
features for sentiment classification of twitter mes-
sages. Davidov et al (2010) studied the use of
hashtags and emoticons in sentiment classification.
Diakopoulos and Shamma (2010) analyzed the peo-
ple?s sentiment on Twitter for first U.S. presidential
debate in 2008.
The majority of works in sentiment analysis uses
either machine learning techniques or lexicon-based
1Our system code is freely available at
http://github.com/pedrobalage/SemevalTwitterHybridClassifier
568
techniques. However, some few works have pre-
sented hybrid approaches. Ko?nig and Brill (2006)
propose a hybrid classifier that utilizes human rea-
soning over automatically discovered text patterns to
complement machine learning. Prabowo and Thel-
wall (2009) evaluates the effectiveness of different
classifiers. This study showed that the use of multi-
ple classifiers in a hybrid manner could improve the
effectiveness of sentiment analysis.
3 System architecture
Our system is organized in four main components:
normalization, rule-based classifier, lexicon-based
classifier and machine learning classifier. These
components are connected in a pipeline architecture
that extracts the best characteristics from each com-
ponent. The Figure 1 shows the system architecture.
Figure 1: System architecture
In this pipeline architecture, each classifier, in a
sequential order, evaluates the Twitter message. In
each step, the classifier may determine the polarity
class of the message if a certain degree of confidence
is achieved. If the classifier may not achieve this
confidence threshold, the classifier in the next step
is called. The machine learning classifier is the last
step in the process. It is responsible to determine the
polarity if the previous classifiers failed to achieve
the confidence level required to classification. The
normalization component is responsible to correct
and normalize the text before the classifiers use it.
This architecture improves the classification pro-
cess because it takes advantage of the multiple ap-
proaches. For example, the rule-based classifier is
the most reliable classifier. It achieves good results
when the text is matched by a high-confidence rule.
However, due the freedom of language, rules may
not match 100% of the unseen examples, conse-
quently it has a low recall rate.
Lexicon-based classifiers, for example, are very
confident in the process to determine if a text is polar
or neutral. Using sentiment lexicons, we can deter-
mine that sentences containing sentiment words are
polar and sentences that do not contain such words
are neutral. Moreover, the presence of a high num-
ber of positive or negative words in the text may be
a strong indicative of the polarity.
Finally, machine learning is known to be highly
domain adaptive and to be able to find deep corre-
lations (Taboada et al, 2011). This last classifier
might provide the final decision when the previous
methods failed. In the following sub-sections, we
describe in more details the components in which
our system is based on. In the next section, we ex-
plain how the confidence level was determined.
3.1 Normalization and rule-based classifier
The normalization module is in charge of correcting
and normalizing the texts. This module performs the
following operations:
? Elements such as hashtags, urls and mentions
are transformed into a consistent set of codes;
? Emoticons are grouped into representative
categories (such as happy, sad, laugh) and con-
verted to particular codes;
? Signals of exaltation (such as repetitive excla-
mation marks) are recognized;
? A simple misspelling correction is performed;
? Part-of-speech tagging is performed.
The rule-based classifier is very simple. The only
rules applied here are concerned to the emoticons
found in the text. Empirically, we evidenced that
positive emoticons are an important indicative of
positiveness in texts. Likewise, negative emoticons
569
indicate negativeness tendency. This module re-
turns the number of positive and negative emoticons
matched in the text.
3.2 Lexicon-based classifier
The lexicon-based classifier is based on the idea that
the polarity of a text can be summarized by the sum
of the individual polarity values of each word or
phrase present in the text. In this assumption, a
sentiment lexicon identifies polar words and assigns
polarity values to them (known as semantic orienta-
tions).
In our system, we used the sentiment lexicon pro-
vided by SentiStrength (Thelwall et al, 2010). This
lexicon provides an emotion vocabulary, an emoti-
cons list, a negation list and a booster word list.
In our algorithm, we sum the semantic orienta-
tions of each individual word in the text. If the word
is negated, the polarity is inverted. If the word is in-
tensified (boosted), we increase its polarity by a fac-
tor determined in the sentiment lexicon. A lexicon-
based classifier usually assumes the signal of the fi-
nal score as the sentiment class: positive, negative
or neutral (score zero).
3.3 Machine learning classifier
The machine learning classifier uses labeled exam-
ples to learn how to classify new instances. The
algorithm learns by using features extracted from
these examples. In our classifier, we used the SVM
algorithm provided by CLiPS Pattern. The features
used by the classifier are bag-of-words, the part-of-
speech set, and the existence of negation in the sen-
tence.
4 Hybrid approach and tuning
The organization from SemEval-2013 Task 2: Senti-
ment Analysis in Twitter provided three datasets for
the task (Wilson et al, 2013). A training dataset
(TrainSet), with 6,686 messages2, a development
dataset (DevSet), with 1,654 messages, and two test-
ing datasets (TestSets), with 3,813 (Twitter TestSet)
and 2,094 (SMS TestSet) messages each.
As we said in the previous section, our system is
a pipeline of classifiers where each classifier may
2The number of messages may differ from other participants
because the data was collected by crawling
assign a sentiment class if it achieves a particular
confidence threshold. This confidence threshold is a
fixed value we set for each system in order to have
a decision boundary. This decision was made by in-
specting the results table obtained with the develop-
ment set, as shown below.
Table 1 shows how the rule-based classifier per-
formed in the development dataset. The classifier
score consists in the difference between the num-
ber of positive emoticons and the number of nega-
tive emoticons found in the message. For example,
for score of -1 we had 22 negative, 4 neutral and 2
positive messages.
Table 1: Correlation between the rule-based classifier
scores and the gold standard classes in the DevSet
Rule-based Gold Standard Class
classifier score Negative Neutral Positive
-1 22 4 2
0 311 708 496
1 7 24 71
2 2 4
3 to 6 1 2
Inspecting the Table 1 we adjusted the rule-based
classifier boundary to decide when the score is dif-
ferent from zero. For values greater than zero, the
classifier will assign the positive class and, for val-
ues below zero, the classifier will assign the negative
class. For values equal zero, the classifier will call
the lexicon-based classifier.
Table 2 is similar to the Table 1, but it now shows
the scores obtained by the lexicon-based classifier
for the development set. This score is the message
semantic orientation computed by the sum of the se-
mantic orientation for each individual word.
Inspecting Table 2, we adjusted the lexicon-based
classifier to assign the positive class when the total
score is greater than 3 and negative class when the
total score is below -3. Moreover, we evidenced that,
compared to the other classifiers, the lexicon-based
classifier had better performance to determine the
neutral class. Therefore, we adjusted the lexicon-
based classifier to assign the neutral class when the
total score is zero. For any other values, the machine
learning classifier is called.
Finally, Table 3 shows the confusion matrix for
the machine learning classifier in the development
570
Table 2: Correlation between the lexicon-based classifier
score and the gold standard classes in the DevSet
Lexicon-based Gold Standard Class
classifier scores Negative Neutral Positive
-11 to -6 26 8 4
-5 15 6 4
-4 31 20 9
-3 32 24 5
-2 57 86 22
-1 25 31 20
0 74 354 115
1 26 70 42
2 28 87 103
3 12 29 81
4 8 9 56
5 2 6 42
6 to 13 4 9 72
dataset. The machine learning classifier does not
operate with a confidence threshold, so no decisions
were made for this classifier. We see that machine
learning classifier does not have a good accuracy
in general. Our hybrid approach proposed aims to
overcome this problem. Next section shows the re-
sults achieved for the Semeval test dataset.
Table 3: Confusion matrix for the machine learning clas-
sifier in the DevSet
Machine learning Gold Standard Class
classifier class Negative Neutral Positive
negative 35 6 11
neutral 232 595 262
positive 73 138 302
5 Results
Table 4 shows the results obtained by each individ-
ual classifier and the hybrid classifier for the test
dataset. In the task, the systems were evaluated with
the average F-Score obtained for positive and nega-
tive classes3. We see that the Hybrid approach could
improve in relation to each classifier score, confirm-
ing our hypothesis.
3Semeval-2013 Task 2: Sentiment Analysis in Twitter com-
pares the systems by the average F-score for positive and nega-
tive classes. For more information see Wilson et al (2013)
Table 4: Average F-score (positive and negative) obtained
by each classifier and the hybrid approach
Classifier Twitter TestSet SMS TestSet
Rule-based 0.1437 0.0665
Lexicon-Based 0.4487 0.4282
Machine Learning 0.4999 0.4029
Hybrid Approach 0.5631 0.5012
Table 5 shows the results in terms of precision,
recall and F-score for each class by the hybrid clas-
sifier in the Twitter dataset. Inspecting our algo-
rithm for the Twitter dataset, we had 277 examples
classified by the rule-based classifier, 2,312 by the
lexicon-based classifier and 1,224 the by machine
learning classifier. The results for the SMS dataset
had similar values.
Table 5: Results for Twitter TestSet
Class Precision Recall F-Score
positive 0.6935 0.6145 0.6516
negative 0.5614 0.4110 0.4745
neutral 0.6152 0.7427 0.6729
6 Conclusion
We described a hybrid classification system used for
Semeval-2013 Task 2: Sentiment Analysis in Twit-
ter. This paper showed how a hybrid classifier might
take advantage of multiple sentiment analysis ap-
proaches and how these approaches perform in a
Twitter dataset.
A future direction of this work would be im-
proving each individual classifier. In our system,
we used simple methods for each employed classi-
fier. Thus, we believe the hybrid classification tech-
nique applied might achieve even better results. This
strengthens our theory that hybrid techniques might
outperform the current state-of-art in sentiment anal-
ysis.
Acknowledgments
We would like to thank the organizers for their work
constructing the dataset and overseeing the task. We
also would like to thank FAPESP and CNPq for fi-
nancial support.
571
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Tom De Smedt and Walter Daelemans. 2012. Pattern for
python. The Journal of Machine Learning Research,
13:2063?2067.
Nicholas A. Diakopoulos and David A. Shamma. 2010.
Characterizing debate performance via aggregated
twitter sentiment. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI
?10, pages 1195?1198, New York, NY, USA. ACM.
Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.
2007. Why we twitter: understanding microblogging
usage and communities. In Proceedings of the 9th We-
bKDD and 1st SNA-KDD 2007 workshop on Web min-
ing and social network analysis, WebKDD/SNA-KDD
?07, pages 56?65, New York, NY, USA. ACM.
Arnd Christian Ko?nig and Eric Brill. 2006. Reducing the
human overhead in text categorization. In Proceed-
ings of the 12th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?06, pages 598?603, New York, NY, USA. ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - EMNLP ?02, pages 79?86,
Morristown, NJ, USA, July. Association for Computa-
tional Linguistics.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Informet-
rics, 3(2):143?157.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267?307, June.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in short
strength detection informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558, December.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
572
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 428?432,
Dublin, Ireland, August 23-24, 2014.
NILC USP: An Improved Hybrid System for Sentiment Analysis in
Twitter Messages
Pedro P. Balage Filho, Lucas Avanc?o, Thiago A. S. Pardo, Maria G. V. Nunes
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Sciences, University of S?ao Paulo
S?ao Carlos - SP, Brazil
{balage, taspardo, gracan}@icmc.usp.br avanco@usp.br
Abstract
This paper describes the NILC USP sys-
tem that participated in SemEval-2014
Task 9: Sentiment Analysis in Twitter, a
re-run of the SemEval 2013 task under the
same name. Our system is an improved
version of the system that participated in
the 2013 task. This system adopts a hybrid
classification process that uses three clas-
sification approaches: rule-based, lexicon-
based and machine learning. We sug-
gest a pipeline architecture that extracts
the best characteristics from each classi-
fier. In this work, we want to verify how
this hybrid approach would improve with
better classifiers. The improved system
achieved an F-score of 65.39% in the Twit-
ter message-level subtask for 2013 dataset
(+ 9.08% of improvement) and 63.94% for
2014 dataset.
1 Introduction
Twitter is an important platform of social com-
munication. The analysis of the Twitter messages
(tweets) offers a new possibility to understand so-
cial behavior. Understanding the sentiment con-
tained in such messages showed to be very impor-
tant to understand user behavior and also to as-
sist market analysis (Java et al., 2007; Kwak et al.,
2010).
Sentiment analysis, the area in charge of study-
ing how sentiments and opinions are expressed in
texts, is usually associated with text classification
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tasks. Sentiment classifiers are commonly cate-
gorized in two basic approaches: lexicon-based
and machine learning approaches (Taboada et al.,
2011). A lexicon-based classifier uses a lexicon
to provide the polarity, or semantic orientation, of
each word or phrase in the text. A machine learn-
ing classifier uses features (usually the vocabulary
in the texts) obtained from labeled examples to
classify the texts according to their polarity.
In this paper, we present a hybrid system for
sentiment classification in Twitter messages. Our
system combines the lexicon-based and machine
learning approaches, as well as uses simple rules
to aid in the process. Our system participated in
SemEval-2014 Task 9: Sentiment Analysis in Twit-
ter (Rosenthal et al., 2014), a re-run for the Se-
mEval 2013 task under the same name (Nakov et
al., 2013). The task goal was to determine the sen-
timent contained in tweets. The task included two
sub-tasks: a expression-level classification (Task
A) and a message-level classification (Task B).
Our system participated only in Task B, where, for
a given message, it should classify it as positive,
negative, or neutral.
The system presented is an improved version of
the system submitted for Semeval 2013. Our pre-
vious system had demonstrated that a hybrid ap-
proach could achieve good results (F-measure of
56.31%), even if we did not use the state-of-the-
art algorithms for each approach (Balage Filho and
Pardo, 2013). In this way, this work aims to ver-
ify how much this hybrid system could improve in
relation to the previous one by including modifica-
tions on both lexicon-based and machine learning
approaches.
428
2 Related work
The analysis of Tweets has gained lots of interest
recently. One evidence is the expressive number
of participants in the SemEval-2013 Task 2: Sen-
timent Analysis in Twitter (Nakov et al., 2013).
There were a total of 149 submissions from 44
teams. The best performing system on twitter
dataset for task B was reported by Mohammad et
al. (2013) with an F-mesaure of 69.02%. Their
system used a machine learning approach and a
very rich feature set. They showed that the best
results were achieved using a built-in positive and
negative lexicon and a bag-of-words as features.
Other important system in Semeval 2013 was
reported by Malandrakis et al. (2013). The authors
presented a hybrid system for twitter sentiment
analysis combining two approaches: a hierarchi-
cal model based on an affective lexicon and a lan-
guage modeling approach. The system achieved
an F-mesaure of 60.14%.
Most work in sentiment analysis uses either ma-
chine learning or lexicon-based techniques. How-
ever, few studies have shown promising results
with the hybrid approach. K?onig and Brill (2006)
proposed a hybrid classifier that uses human rea-
soning over automatically discovered text patterns
to complement machine learning. Prabowo and
Thelwall (2009) evaluated the effectiveness of dif-
ferent classifiers. Their study showed that the use
of multiple classifiers in a hybrid manner could
improve the effectiveness of sentiment analysis.
3 System Architecture
Our system is described as a pipeline solution of
four main processes: normalization, rule-based
classification, lexicon-based classification and ma-
chine learning classification. This is the same ar-
chitecture presented by our system in 2013.
This pipeline architecture works as a back-off
model. In this model, each classifier tries to clas-
sify the tweets by using the underlying approach.
If a certain degree of confidence is achieved, the
classifier will provide the final sentiment class for
the message. Otherwise, the next classifier will
continue the classification task. The last possibil-
ity is the machine learning classifier, responsible
to deliver the class when the previous two could
not achieve the confidence level. We decided to
use this back-off model instead of a voting system,
for example, due to the high precision achieved for
the rule-based and the lexicon-based classifiers.
The aim of this pipeline architecture is to im-
prove the classification process. In Balage Filho
and Pardo (2013), we have shown that this hybrid
classification approach may outperform the indi-
vidual approaches.
In the following subsections, we detail the com-
ponents of our system. In the next section, we ex-
plain how the confidence level was determined.
3.1 Normalization and Rule-based Classifier
The normalization module is responsible for nor-
malizing and tagging the texts. This module per-
forms the following operations:
? Hashtags, urls and mentions are transformed
into codes;
? Emoticons are grouped into representative
categories (such as ?happy?, ?sad?, ?laugh?)
and are converted to particular codes;
? Part-of-speech tagging is performed by using
the Ark-twitter NLP (Owoputi et al., 2013)
The rule-based classifier is designed to provide
rules that better impact the precision than the re-
call. In our 2014 system, we decided to use the
same rule-based classifier from the 2013 system.
The rules in this classifier only verify the pres-
ence of emoticons in the text. Empirically, we
evidenced that the use of emoticons indicates the
actual polarity of the message. In this module,
we consider the number of positive and negative
emoticons found in the text to determine its clas-
sification.
3.2 Lexicon-based Classifier
The lexicon-based classifier is based on the idea
that the polarity of a text can be given by the sum
of the individual polarity values of each word or
phrase present in the text. For this, a sentiment lex-
icon identifies polarity words and assigns polarity
values to them (known as semantic orientations).
In the 2013 system, we had used SentiStrength
lexicon (Thelwall et al., 2010). In 2014, we
improved our lexicon-based classifier by using
a larger sentiment lexicon. We used the senti-
ment lexicon provided by Opinion-Lexicon (Hu
and Liu, 2004) and a list of sentiment hashtags
provided by the NRC Hashtag Sentiment Lexicon
(Mohammad et al., 2013). For dealing with nega-
tion, we used a handcrafted list of negative words.
429
In our algorithm, the semantic orientations of
each individual word in the text are added up.
In this approach, the algorithm searches for each
word in the lexicon and only the words that were
found are returned. We associate the value +1 to
the positive words, and -1 to the negative words.
If a polarity word is negated, its value is inverted.
This lexicon-based classifier assumes the signal of
the final score as the sentiment class (positive or
negative) and the score zero as neutral.
3.3 Machine Learning Classifier
The machine learning classifier uses labeled ex-
amples to learn how to classify new instances.
The features used for this 2014 system were com-
pletely changed from 2013 system. We inspired
our machine learning module in the work reported
by Mohammad et al. (2013). The features used by
the classifier are:
1. unigrams, bigrams and trigrams
2. the presence of negation
3. the presence of three or more characters in
the words
4. the sequence of three or more punctuation
marks
5. the number of words with all letters in upper-
case
6. the total number of each tag present in the
text
7. the number of positive words computed by
the lexicon-based method
8. the number of negative words computed by
the lexicon-based method
We use a Linear Kernel SVM classifier provided
by the python sckit-learn library with C=0.005
1
.
4 Hybrid Approach and Tuning
The organization from SemEval-2014 Task 9: Sen-
timent Analysis in Twitter provided four datasets
for the task: a training dataset (TrainSet) with
9675 messages directly retrieved from Twitter; a
development dataset (DevSet), with 1654 mes-
sages; the testing dataset from 2013 run, which
was not used; and the testing dataset for 2014
1
Available at http://scikit-learn.org/
with 8987 messages. The 2014 testing dataset was
composed of 5 different sources:
? Twitter2013: Twitter test data from 2013 run
? SMS2013: SMS test data from 2013 run
? Twitter2014: 2000 tweets
? LiveJournal2014: 2000 sentences from Live-
Journal blogs
? Twitter2014Sarcasm: 100 tweets that contain
sarcasm
As we said in the previous section, our system is
a pipeline of classifiers where each classifier may
assign a sentiment class if it achieves a particu-
lar confidence threshold score. This confidence
score is a fixed value set for each system in or-
der to have a decision boundary. This decision
was made by inspecting the results obtained for the
development set. Tables 1 and 2 shows how the
rule-based and lexicon-based classifiers perform
for the development dataset in terms of score. The
score obtained by the rule-based classifier consists
of the difference between the number of positive
emoticons and the number of negative emoticons
found in the messages. The score obtained by the
lexicon-based classifier represents the total seman-
tic orientation obtained by the algorithm by adding
up the semantic orientation for their lexicon.
Inspecting Table 1, for the best threshold, we
adjusted the rule-based classifier boundary to de-
cide when the score is different from zero. For
values greater than zero, the classifier will assign
the positive class and, for values below zero, the
classifier will assign the negative class. For values
equal to zero, the classifier will call the lexicon-
based classifier.
Table 1: Correlation between the rule-based clas-
sifier scores and the gold standard classes in the
DevSet
Rule-based Gold Standard Class
classifier score Negative Neutral Positive
-1 22 3 3
0 311 709 495
1 7 26 73
2 0 0 2
3 to 6 0 1 2
Inspecting Table 2, for the best threshold, we
adjusted the lexicon-based classifier to assign the
430
positive class when the total score is greater than
1 and negative class when the total score is below
-2. For any other values, the classifier will call the
machine learning classifier.
Table 2: Correlation between the lexicon-based
classifier score and the gold standard classes in the
devset
Lexicon-based Gold Standard Class
classifier scores Negative Neutral Positive
-7 to -4 2 0 0
-3 10 4 0
-2 48 18 7
-1 111 99 35
0 108 432 178
1 48 143 210
2 11 39 104
3 to 5 3 4 47
As the machine learning classifier is responsible
for the final stage, we did not have to decide any
threshold for this classifier. However, we empiri-
cally identified a bias toward the positive class (the
negative class was barely chosen). In order to cor-
rect this problem, we setup the machine learning
classifier to decide for the negative class whenever
the SVM score for this class is bigger than -0.4.
Next section shows the results achieved for the Se-
meval test dataset.
5 Results
Table 3 shows the results obtained by each individ-
ual classifier and by the hybrid classifier for the
Twitter2014 messages in the testset. In the task,
the systems were evaluated with the average F-
score obtained for positive and negative classes.
Table 3: Average F-score (positive and negative)
obtained by each classifier and the hybrid ap-
proach for the Twitter2014 testset
Classifier Twitter2014 Testset
Rule-based 14.03
Lexicon-Based 47.55
Machine Learning 63.36
Hybrid Approach 63.94
Table 4 shows the improvement of the system
over the 2013 run. Unlike last year, we notice that
the performance of this hybrid system is very close
to the performance of the machine-learning.
Table 4: Comparison of the average F-score (pos-
itive and negative) obtained by each classifier and
the hybrid approach for the Twitter2013 testset for
2013 and 2014 versions
Classifier 2013 system 2014 system
Rule-based 14.37 13.31
Lexicon-Based 44.87 46.80
Machine Learning 49.99 63.75
Hybrid Approach 56.31 65.39
Table 5 shows the scores for each source in the
testset. Last column shows our system rank among
the 50 systems that participated in the competition.
For the entire testing dataset, our algorithm had
503 (5%) examples classified by the rule-based
classifier, 3204 (36%) by the lexicon-based classi-
fier and 5280 (59%) by the machine learning clas-
sifier.
6 Conclusion
We described our improved hybrid classification
system used for Semeval-2014 Task 9: Sentiment
Analysis in Twitter. This work showed that this
hybrid classifier can be improved as its modules
are too. However, we noticed that, improving the
lexicon and machine learning modules, the overall
score tends towards the machine learning score.
The source code produced for the experiment is
available at https://github.com/pedrobalage.
Acknowledgments
We would like to thank the organizers for their
work in constructing the dataset and in the over-
seeing of the task. We also would like to
thank FAPESP and SAMSUNG for supporting
this work.
References
Pedro Balage Filho and Thiago Pardo. 2013.
NILC USP: A Hybrid System for Sentiment Analy-
sis in Twitter Messages. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 568?572, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
431
Table 5: Results for Twitter TestSet
TestSet Source Majority Baseline Our Score Best Result Our Rank
Twitter2013 29.2 65.39 72.12 15th
SMS2013 19.0 61.35 70.28 16th
Twitter2014 34.6 63.94 70.96 19th
LiveJournal2014 27.2 69.02 74.84 18th
Twitter2014Sarcasm 27.7 42.06 58.16 34th
Akshay Java, Xiaodan Song, Tim Finin, and Belle
Tseng. 2007. Why we twitter: understanding mi-
croblogging usage and communities. In Proceed-
ings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network anal-
ysis, WebKDD/SNA-KDD ?07, pages 56?65, New
York, NY, USA. ACM.
Arnd Christian K?onig and Eric Brill. 2006. Reducing
the human overhead in text categorization. In Pro-
ceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?06, pages 598?603, New York, NY, USA.
ACM.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is twitter, a social network
or a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Nikolaos Malandrakis, Abe Kazemzadeh, Alexan-
dros Potamianos, and Shrikanth Narayanan. 2013.
SAIL: A hybrid approach to sentiment analysis. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 438?442, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-
the-Art in Sentiment Analysis of Tweets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321?327, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143?157.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in
short strength detection informal text. Journal of the
American Society for Information Science and Tech-
nology, 61(12):2544?2558, December.
432
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 433?436,
Dublin, Ireland, August 23-24, 2014.
NILC USP: Aspect Extraction using Semantic Labels
Pedro P. Balage Filho and Thiago A. S. Pardo
Interinstitutional Center for Computational Linguistics (NILC)
Institute of Mathematical and Computer Sciences, University of S?ao Paulo
S?ao Carlos - SP, Brazil
{balage, taspardo}@icmc.usp.br
Abstract
This paper details the system NILC USP
that participated in the Semeval 2014: As-
pect Based Sentiment Analysis task. This
system uses a Conditional Random Field
(CRF) algorithm for extracting the aspects
mentioned in the text. Our work added se-
mantic labels into a basic feature set for
measuring the efficiency of those for as-
pect extraction. We used the semantic
roles and the highest verb frame as fea-
tures for the machine learning. Overall,
our results demonstrated that the system
could not improve with the use of this se-
mantic information, but its precision was
increased.
1 Introduction
Sentiment analysis, or opinion mining, has gained
lots of attention lately. The importance of this
field of study is linked with the grown of informa-
tion in the internet and the commercial attention it
brought.
According to Liu et al. (2010), there are two
kinds of information available in the internet: facts
and opinions. Facts are objective statements about
entities and events in the world. Opinions are sub-
jective statements that reflect people?s sentiments
or perceptions about the entities and events. Ac-
cording to Liu, by that time, there was a lot of at-
tention on the processing of facts but little work
had been done on the processing of opinions.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Three levels of analysis for sentiment analysis
are known (Liu, 2012): document level, sentence
level and aspect level. The aspect-based sentiment
analysis is the name of the research topic that aims
to extract the sentiments about the aspects present
in the text.
This work presents a system evaluated in the
SemEval Task4: Aspect Based Sentiment Analy-
sis shared task (Pontiki et al., 2014). Our system
participated only in subtask 1: Aspect Term Ex-
traction. In this subtask, given a text, the system
should extract all aspects that are present. There
were two different domains for this task: restau-
rants and laptops.
The goal of our system was to verify how se-
mantic labels used in machine learning classifica-
tion would improve the aspect extraction task. For
this goal, we used two kinds of semantic labels:
the semantic roles (Palmer et al., 2010) and the se-
mantic frames (Baker et al., 1998).
Liu et al. (2012) categorizes the works for as-
pect extraction in four types, regarding the ap-
proach they follow, using: frequent terms, infre-
quent terms, machine learning, and topic model-
ing. This work uses a machine learning approach
that consists in training a sequential labeling algo-
rithm for aspect detection and extraction.
In what follows, we present some related work
in Section 2. Section 3 and 4 introduce our system
and report the achieved results. Some conclusions
are presented in Section 5.
2 Related work
Jin and Hovy (2009) reported one the first works
using sequential labeling for aspect extraction. In
this work, the authors used a Lexicalized Hidden
Markov Model to learn patterns to extract aspects
and opinions. Jakob and Gurevych (2010) trained
433
a Conditional Random Field for aspect extraction.
In this work, the authors report the results for a sin-
gle domain and a cross domain experiment. They
show that even in other domains the method could
be good.
Kim and Hovy (2006) explored the semantic
structure of a sentence, anchored to an opinion
bearing verb or adjective. Their method uses se-
mantic role labeling as an intermediate step to la-
bel an opinion holder and topic using data from
FrameNet.
Houen (2011) presented a system for opinion
mining with semantic analysis. The author ex-
plores the use of the semantic frame-based ana-
lyzer FrameNet (Baker et al., 1998) for modeling
features in a machine learning approach. The au-
thor found that the FrameNet information was not
helpful in this classifier.
3 System Description
Our system uses a sequential labeling algorithm.
In our work, we use the Conditional Random Field
(Lafferty et al., 2001) algorithm provided by the
CRF++ tool
1
.
For training the sequential labeling algorithm,
we give as input features for each word in the cor-
pus. The algorithm will then learn how to classify
those words. In our approach, the possible classes
are: True, representing an aspect word; and False,
representing the remaining words.
The goal of our system was to evaluate the per-
formance of the semantic labels for the task. In
order to model our system, we built a feature set
consisting of 6 features.
1. the word
2. the part-of-speech
3. the chunk
4. the named-entity category
5. the semantic role label (SRL)
6. the most generic frame in FrameNet
The use of the first four features is consistent
with the best approaches in aspect-based senti-
ment analysis. The last two features are the ones
we are testing in our work.
In order to extract the features, we used two im-
portant tools: the Senna (Collobert et al., 2011), a
1
Available at http://crfpp.googlecode.com/
semantic role labeling system, and the ARK SE-
MAFOR, a Semantic Analyzer of Frame Repre-
sentations (Das et al., 2010).
The Senna system uses a deep learning neural
network (Collobert, 2011) to provide several pre-
dictions for natural language processing. The sys-
tem output is represented in the CONLL format,
the same used in CRF++.
Our first 5 features were directly provided by
the Senna output. In these features, we decided to
keep the IOBE information since the initial exper-
iments showed the results were better with it than
without.
Our fifth feature, the semantic role label, was
retrieved from Senna as well. In the corresponding
paper, they reported Senna could achieve a F1 of
75% for the SRL task.
The example below shows how the features
were represented. In this example, we are only
showing four features: the word, the part-of-
speech, the chunk and the SRL. The classes are
in the last column.
WORD POS CHUNK SRL IS_ASPECT?
Great JJ B-NP B-A0 False
laptop NN E-NP E-A0 False
that WDT S-NP S-R-A0 False
offers VBZ S-VP S-V False
many JJ B-NP B-A1 False
great JJ I-NP I-A1 False
features NNS E-NP E-A1 True
! . O - False
The last feature was retrieved by ARK SE-
MAFOR tool. ARK SEMAFOR uses a probabilis-
tic frame-semantic parsing using the FrameNet re-
source. The ARK SEMAFOR output is the anal-
ysis of the frames present in the text for a given
verb. As our feature set has only word related fea-
tures, we decided to use the most upper level struc-
ture in the frame. In case of multiple verbs in the
sentence, we used the structure for the verb that is
closest to the word of interest.
The following example shows how the frames
were added into the training model. We limit to
show only the word, frame and the class. For train-
ing, we used the full training set with the six fea-
tures plus the class.
WORD FRAME IS_ASPECT?
I Shopping False
shopped Shopping False
around Relational_quantity False
before Relational_quantity False
434
buying Relational_quantity False
. O False
The organization from SemEval-2014 Task 4:
Aspect Based Sentiment Analysis provided two
domains for evaluation: restaurants and laptops.
For each domain, the organization provided three
datasets: a trainset, a devset and a testset.
We executed our algorithm with C pa-
rameter equal to 4.0. The experiment
code is fully available at the weblink
https://github.com/pedrobalage/
4 Results
Tables 1 and 2 show our system results for the
restaurants and laptops domains respectively. In
these tables, the results are discriminated by the
feature sets that were used. The reader may see
that a ?+ Frame? system, for example, stands for
all the features discriminated above (Word, POS,
Chunk, NR, SRL) plus the Frame feature. The last
line shows the results scored by our system in the
SemEval shared task with all the features. We also
show the results for the baseline system provided
by the shared task (Pontiki et al., 2014).
Table 1: Results for restaurants domain
System Precision Recall F1-mesaure
Baseline 52.54 42.76 47.15
Word + POS 83.76 68.69 75.48
+ Chunk 83.38 68.16 75.01
+ NE 83.45 68.07 74.98
+ SRL 82.79 67.46 74.34
+ Frame 87.72 34.03 49.04
Table 2: Results for laptops domain
System Precision Recall F1-mesaure
Baseline 44.31 29.81 35.64
Word + POS 80.87 39.44 53.03
+ Chunk 78.83 39.29 52.44
+ NE 79.93 39.60 52.96
+ SRL 78.22 38.99 52.04
+ Frame 83.62 14.83 25.19
Comparing with the baseline, we may noticed
that our submitted system (+Frame) outperformed
the baseline for the restaurants domain but it did
not outperformed the baseline for the laptops do-
main (considering F1 mesaure).
When we look in detail for the inclusion of fea-
tures in our feature set, we may notice that, at ev-
ery new feature, the precision goes up, but the re-
call goes down. We believe this is due to the be-
haviour of the conditional random field algorithm
for compensating for a sparser feature set.
In general, the semantic labels (SRL and Frame)
could not improve the results. However, if we
are interested only on precision, these features are
helpful. This may be the case in scenarios where a
lot of information is available, as in the web, and
we want to be sure about the retrieved informa-
tion. Certainly, there is a conflict between preci-
sion and computational complexity, since the se-
mantic features are more expensive to be achieved
(in relation to the usual simpler features that may
be used).
Despite of that, we judge to be necessary to con-
duct more experiments in order to better evaluate
the impact of semantic labels in the aspect extrac-
tion task.
5 Conclusion
We presented an aspect extraction system built on
a conditional random field algorithm. We used
a rich feature set with the semantic roles and
the FrameNet upper frames for each word. We
have showed that the semantic labels may help to
achieve a more precise classifier, but it did not help
to improve the overall F-measure of the system.
Regarding the shared task, our system achieved
the second best precision value among the com-
peting systems, but the lowest recall value. Future
work should investigate ways of also improving
recall without penalty for the achieved precision.
Acknowledgments
We would like to thank the organizers for their
work constructing the dataset and overseeing the
task. We also would like to thank FAPESP for the
financial support.
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1, pages 86?90. Association for Computa-
tional Linguistics.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
435
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Geoffrey J. Gordon and
David B. Dunson, editors, Proceedings of the Four-
teenth International Conference on Artificial Intel-
ligence and Statistics (AISTATS-11), volume 15,
pages 224?232. Journal of Machine Learning Re-
search - Workshop and Conference Proceedings.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Semafor 1.0: A probabilistic
frame-semantic parser. Technical report, Language
Technologies Institute, School of Computer Science,
Carnegie Mellon University.
S?ren Houen. 2011. Opinion Mining with Seman-
tic Analysis. Ph.D. thesis, Department of Computer
Science, University of Copenhagen.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1035?1045.
Wei Jin and Hung Hay Ho. 2009. A Novel Lexicalized
HMM-based Learning Framework for Web Opinion
Mining. In L?eon Bottou and Michael Littman, ed-
itors, Proceedings of International Conference on
Machine Learning (ICML-2009), ICML ?09, pages
1?8. ACM, ACM Press.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ?06,
pages 1?8, Stroudsburg, PA, USA. Association for
Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Bing Liu. 2010. Sentiment Analysis and Subjectivity.
In N Indurkhya and F J Damerau, editors, Handbook
of Natural Language Processing, number 1, chap-
ter 28, pages 627?666. CRC Press, Taylor and Fran-
cis Group, Boca Raton.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1?103.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation, SemEval 2014, Dublin, Ireland.
436
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 1?7, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Computational Linguistics in Brazil: An Overview 
 
Thiago A. S. Pardo1, Caroline V. Gasperin1, Helena M. Caseli2,  
Maria das Gra?as V. Nunes1 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
1 Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Av. Trabalhador S?o-carlense, 400 - Centro 
P.O.Box 668. 13560-970 - S?o Carlos/SP, Brazil 
2
 Departamento de Computa??o, Universidade Federal de S?o Carlos 
Rod. Washington Lu?s, Km 235 
P.O.Box 676. 13565-905 - S?o Carlos/SP, Brazil 
{taspardo,cgasperin}@icmc.usp.br, helenacaseli@dc.ufscar.br, 
gracan@icmc.usp.br 
 
 
 
Abstract 
In this paper we give an overview of 
Computational Linguistics / Natural 
Language Processing in Brazil, describing the 
general research scenario, the main research 
groups, existing events and journals, and the 
perceived challenges, among other relevant 
information. We also identify opportunities 
for collaboration. 
1 Brazilian Research Scenario 
Computational Linguistics (CL) / Natural 
Language Processing (NLP) is an emerging and 
growing area in Brazil. Although there is no 
consensus, it is traditionally understood as a 
research field within Artificial Intelligence, 
gathering researchers mainly from Computer 
Science/Engineering and Linguistics. There is also 
modest interaction with Information Sciences area. 
In general the CL/NLP area in Brazil started 
with researchers that finished their PhD abroad 
and, after coming back, initiated the first CL/NLP 
projects. Since then, but mainly more recently, the 
area has experienced some internationalization due 
to the fact that the number of undergraduate and 
graduate students that undergo internships on 
renowned foreign NLP research centers has 
increased. In Brazil, PhD students have the 
possibility to take their complete PhD course 
abroad or, alternatively, only a part of it. In both 
cases, students may count on Brazilian funding 
agencies. 
The area is more strongly represented and 
promoted by Brazilian Computer Society (SBC)1, 
particularly by its Special Interest Group on NLP 
(CEPLN)2, created in 2007. It is interesting that 
most researchers in Brazil (independent from their 
background area) do not differentiate CL from 
NLP, using both terms interchangeably. 
Research in Brazil is carried out mainly at 
public universities and at a few private universities 
and business companies. Differently from most 
countries, in Brazil public universities are 
generally considered the top ones, although 
exceptions do exist. 
Currently, there are no undergraduate courses on 
CL/NLP in Brazil, therefore researchers in this 
field come mainly from Computer Science and 
Linguistics courses. However, there are a few 
graduate courses on CL/NLP, with both computing 
and language emphases, such as the MSc and PhD 
programs at USP/S?o Carlos3, UFSCar4, 
UNESP/Araraquara5, PUC-RS6, and UFRGS7, 
among others.  
                                                           
1
 http://www.sbc.org.br 
2
 http://www.sbc.org.br/cepln 
3
 http://www.icmc.usp.br/~posgrad/computacao.html 
4
 http://ppgcc.dc.ufscar.br and http://www.ppgl.ufscar.br 
5
 http://www.fclar.unesp.br/poslinpor 
6
 http://www.pucrs.br/inf/pos 
7
 http://www.ufrgs.br 
1
Funding for research comes mainly from 
governmental agencies. Nowadays Brazil has 4 
agencies that significantly support research in the 
country (in this order): CNPq8 (National Council 
for Scientific and Technological Development), 
FAPESP9 (S?o Paulo Research Foundation), 
CAPES10 (Coordena??o de Aperfei?oamento de 
Pessoal de N?vel Superior), and FINEP11 (Research 
and Projects Financing). Private funding is still 
modest, which reflects the limited interaction 
between universities and companies. Some of the 
above agencies have tried to change this scenario 
by providing special joint university-industry 
funding programs. For instance, FAPESP and 
Microsoft Research recently formed a partnership 
to fund socially relevant projects in the state of S?o 
Paulo, e.g., the PorSimples12 text simplification 
project. FAPESP also funds special university-
company programs, where the research to be 
developed must be of interest to a company, which, 
in turn, has to support the research and work 
together with the researchers. 
NLP research in Brazil is varied and deals not 
only with Portuguese processing, but also with 
English and Spanish mainly. Given that Portuguese 
is among the most spoken languages in the world 
(it is estimated that almost 250 million people 
speak some variant of Portuguese in the world13), 
research interests on Portuguese processing is 
shared with other countries, mainly Portugal. In 
this sense, Portugal has launched an initiative to 
create and maintain a unified information storage 
center that indexes resources and publications 
for/on Portuguese processing. The initiative is the 
Linguateca project14, which was officially created 
in 2002, but initial works date back to 1998. Santos 
(2009) presents and evaluates the work carried out 
by Linguateca. 
Brazil and Portugal have a history of partnership 
on Portuguese processing, which formally started 
in 1993 with the first PROPOR conference 
(PROPOR event series is introduced in Section 4). 
                                                           
8
 http://www.cnpq.br 
9
 http://www.fapesp.br 
10
 http://www.capes.gov.br 
11
 http://www.finep.gov.br 
12
 http://caravelas.icmc.usp.br 
13
 Besides Brazil and Portugal, Portuguese is an official 
language in Angola, Cape Verde, East Timor, Equatorial 
Guinea, Guinea-Bissau, Macau, Mozambique, and S?o Tom? 
and Pr?ncipe. 
14
 http://www.linguateca.pt 
We maintain this partnership active by having 
collaborative projects and promoting joint events.  
As far as we know, other Portuguese speaking 
countries do not have a tradition of CL/NLP 
research. However, curiously, there are researchers 
from other non-Portuguese speaking countries that 
develop relevant research on Portuguese language. 
For example, to the best of our knowledge, 
currently the best syntactical parsers for 
Portuguese were developed by researchers from 
Denmark and the USA. These researchers actively 
work with the Brazilian research community. 
In what follows, we briefly present the Brazilian 
research profile (Section 2), the main research 
groups (Section 3), and the Brazilian events and 
journals (Section 4). We also report the main 
challenges for research in Brazil (Section 5) and 
the collaboration opportunities with other 
American researchers that we envision (Section 6). 
2 Research Profile 
In 2009 CEPLN proposed a survey of the status of 
CL/NLP research in Brazil and published the 
results during the 7th Brazilian Symposium in 
Information and Human Language Technology 
(Pardo et al, 2009). The survey aimed at gathering 
information both about researchers (such as their 
location, education level, number of students, etc.) 
and their research (main research topics, number of 
funded projects, main challenges, etc.). 
The survey was carried out mainly on-line. A 
call for participation was sent to all known e-mail 
lists from scientific associations from varied areas. 
Data was also obtained from the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics15. 
148 researchers responded to the survey: 35% of 
these were academic staff with a PhD degree, 16% 
academic staff with a Master's degree, 1% 
academic staff with a Bachelors degree, 9% PhD 
students, 26% Master's students, 14% 
undergraduate students, and 5% others. Table 1 
summarizes the main results of the survey, 
showing the percentage of answers for each issue. 
One may see that CL/NLP research is mainly 
carried out in the south and southeast regions of 
Brazil. 
 
                                                           
15
 http://ww.d.umn.edu/~tpederse/registry/registry.cgi 
2
Table 1. CEPLN survey 
Issues Results 
Geographic distribution 48% S?o Paulo state 
18% Rio Grande do Sul state 
8% Paran? state 
7% Rio de Janeiro state 
19% Other states 
National collaboration 52% Yes, 48% No 
International Collaboration 25% Yes, 75% No 
Background area 62% Computer Science 
29% Linguistics 
9% Other 
Supervision of postgraduate students 28% Yes, 72% No 
Funded projects 28% Yes, 72% No 
Source of funding 43% Federal government agencies 
25% S?o Paulo state government agency 
31% Other state government agencies 
 
. 
  
 
Figure 1. Research topics 
 
The survey also inquired the participants about 
their research topics. Figure 1 shows the 
distribution of topics among researchers who 
responded to the survey. Researchers could mark 
as many research topics as they wanted. Some 
topics subsume others, so these were marked more 
often by respondents. 
Ontologies and semantics were the topics 
marked by most respondents. We believe that there 
is indeed a significant number of researchers 
working on them, but we also believe that they are 
not the main topic of research of most people who 
3
listed them. For example, the statistics for 
?Ontologies? probably also include researchers 
who simply make use of ontologies in their work 
and not necessarily develop ontologies or ontology 
generation methods. Other researchers believe that 
we are in a changing period, moving from syntax-
centered research to semantics-centered research, 
due to the fact that more recently the community 
has produced more robust semantic tools and 
resources, e.g., the first versions of Portuguese 
language wordnets, as TeP 2.016, Wordnet.PT17, 
and MWN.PT18, as well as named entities 
recognizers, e.g., REMBRANDT19. 
Interestingly, corpus linguistics is one of the 
hottest topics but, at the same time, it is not seen as 
a genuine CL/NLP topic: most researchers that 
indicated corpus linguistics as a research topic 
marked it as ?other area of interest?. Some 
researchers have advocated that CL/NLP area and 
corpus linguistics should be considered a unique 
area, while others argue that these areas have 
different purposes and, therefore, different 
scientific methods, what would avoid such 
unification. Text mining is another curious case: 
research on this theme is mostly carried out by 
non-CL/NLP researchers, but instead by 
researchers on general AI and database areas 
Based on the publications on the last Brazilian 
scientific events and on the fact that we personally 
know most of the CL/NLP researchers in Brazil, 
we dare to indicate the following topics as the most 
recurrent ones (in no particular order): text 
summarization, machine translation, text 
simplification, automatic discourse analysis, 
coreference and anaphora resolution, information 
retrieval, text mining, terminology/lexicon 
research, ontologies and semantic tagging, and 
corpus linguistics. 
Based on the survey, we estimate that Brazil has 
about 250 researchers (including students) with 
interest in CL/NLP area. Although only 148 
researchers attended the CEPLN survey, we 
computed other researchers in the Registry of Latin 
American Researchers in Natural Language 
Processing and Computational Linguistics and in 
the CEPLN e-mail list that did not attend the 
survey. In general, we estimate that about 35-40 of 
                                                           
16
 http://www.nilc.icmc.usp.br/tep2/index.htm 
17
 http://www.clul.ul.pt/clg/wordnetpt 
18
 http://mwnpt.di.fc.ul.pt 
19
 http://xldb.di.fc.ul.pt/Rembrandt 
these are active researchers, whose main topic of 
research is CL/NLP, and who supervise 
undergraduate and graduate students on the 
subject. We also estimate that there are 5-10 
researchers on speech processing that actively 
collaborate with the CL/NLP community. 
3 Main research groups 
The largest CL/NLP research group in Brazil is 
NILC (Interinstitutional Center for Research and 
Development in Computational Linguistics)20, 
which includes researchers mainly from University 
of S?o Paulo (USP; Computer Science and Physics 
departments), Federal University of S?o Carlos 
(UFSCar; Computer Science and Linguistics 
departments) and State University of S?o Paulo 
(UNESP; Linguistics department). The group was 
created at 1993. 
NILC has a long history of research in CL/NLP, 
which has thrived since the ReGra21 project, in 
which the grammar checker for Portuguese that is 
currently used within Microsoft Word since its 
2000 version was built. In fact, ReGra project was 
born from a university-industry collaboration, one 
of the few successful ones in CL/NLP area in 
Brazil. At the moment most of the research at 
NILC is concentrated on the following topics: 
automatic summarization, text simplification, 
coreference resolution, and terminology. NILC has 
hosted STIL 2009 (STIL event series is introduced 
in the next section). NILC also currently holds the 
presidency of CEPLN. 
The NLP group at the Computer Science 
department at the Catholic University of Rio 
Grande do Sul (PUC-RS)22 also has a tradition of 
research on CL/NLP. Their current projects focus 
on information retrieval, ontology engineering and 
anaphora resolution. The group also has research 
on multi-agent systems applied to NLP tasks and, 
more recently, on text categorization. The group 
hosts PROPOR 2010 (PROPOR event series is 
also introduced in the next section). The group has 
held the presidency of CEPLN from its creation 
(2007) until 2009. 
The above research group and NILC form the 
main CL/NLP research vein in Brazil. They have 
joint research projects and have strong 
                                                           
20
 http://www.nilc.icmc.usp.br 
21
 http://www.nilc.icmc.usp.br/nilc/projects/regra.htm 
22
 http://www.inf.pucrs.br/~linatural 
4
collaboration, constantly hosting graduate students 
from each other in internship research periods. 
There are also other very relevant NLP groups 
in Brazil that regularly carry out projects on the 
area. We may cite the Catholic University of Rio 
de Janeiro (PUC-Rio)23, Federal University of Rio 
Grande do Sul (UFRGS), State University of 
Campinas (UNICAMP), University of the Sinos 
River Valley (Unisinos), and State University of 
Maring? (UEM), among others. 
4 Events and Journals 
The Brazilian Symposium on Information and 
Human Language Technology (STIL) is the main 
event on CL/NLP in South America and is in its 
seventh edition. It is promoted by CEPLN and is 
carried out since 1993. It is intended to be a forum 
for gathering everyone with interest in CL/NLP. It 
happens regularly (every one or two years) and 
accepts contributions in Portuguese, Spanish and 
English. Details about the event are available at 
www.nilc.icmc.usp.br/til. 
The International Conference on Computational 
Processing of Portuguese Language (PROPOR) is 
an international conference jointly promoted by 
Brazil and Portugal and is in its ninth edition. It is 
the main conference with focus on Portuguese 
language, giving equal space to research on text 
and speech processing. It is carried out in Brazil 
and in Portugal interchangeably (every two or 
three years) and accepts submissions in English 
only. PROPOR?s proceedings are published as part 
of Springer Lecture Notes series. Details about the 
event are available at 
www.nilc.icmc.usp.br/cgpropor. 
STIL and PROPOR are the most relevant 
conferences for researchers in CL/NLP in Brazil. 
Their last editions received support from NAACL. 
AI events are also recurrent forums for CL/NLP 
researchers. The Brazilian AI events are the 
Brazilian Symposium on Artificial Intelligence 
(SBIA)24 and the National Meeting on Artificial 
Intelligence (ENIA)25, also promoted by SBC. 
They are already in their twentieth and seventh 
editions, respectively. 
Other related events in Brazil are the Corpus 
                                                           
23
 www.letras.puc-rio.br/Clic/ogrupo.htm 
24
 http://www.jointconference.fei.edu.br/ 
25
 http://csbc2009.inf.ufrgs.br/ 
Linguistics Meeting (ELC)26 and Brazilian School 
on Computational Linguistics (EBRALC)27, which 
are in their eighth and third editions, respectively. 
These events are mainly organized by the 
Linguistics research community. EBRALC is 
mainly intended for new students in the area and 
has been held together with ELC. 
Brazilian researchers count mainly on the 
following journals for national periodical 
publications: 
 JBCS28 (Journal of the Brazilian Computer 
Society), which is published by SBC and 
covers all Computer Science areas, including 
CL/NLP; 
 RITA29 (Journal of Theoretical and Applied 
Computing), also of general scope. 
 
It is important to cite Linguam?tica30, which is an 
European initiative to publish CL/NLP research on 
the Iberian languages. 
CEPLN is also organizing a joint journal with 
other SBC AI-related special interest groups. 
5 Challenges 
At STIL 2009, the research community discussed 
challenging issues (raised by respondents of the 
CEPLN survey) that hamper research on CL/PLN 
in Brazil. The main issues raised were: 
 Lack of large and robust language resources 
for Portuguese; 
 Lack of formal models for linguistic 
description and analysis of Portuguese; 
 Difficulty in attracting students and researchers 
to the area; 
 Lack of multidisciplinary collaboration; 
 CL/NLP marginalization in both Computer 
Science and Linguistics. 
 Poor interaction between universities and 
industry; 
 Insufficient funding. 
 
Here we discuss some of these points. Although 
Portuguese has got state of the art tools (as POS 
taggers and syntactic parsers) and comprehensive 
corpora of contemporary written language, there is 
                                                           
26
 http://www.corpuslg.org/elc/Inicial.html 
27
 http://www.corpuslg.org/ebralc/Inicial.html 
28
 http://www.springer.com/computer+science/journal/13173 
29
 http://www.seer.ufrgs.br/index.php/rita 
30
 http://linguamatica.pt 
5
still a need for resources for particular applications 
or domains. Many researchers feel that Portuguese 
syntactic parsers (which are considered basic NLP 
tools) and wordnet-like resources are still too 
limited, not attending their demands. Brazil also 
lacks representative spoken corpora, what may be 
explained by the fact that, in Brazil, written and 
spoken language processing communities have 
modest interaction. While written language 
processing research is reported at SBC events, 
spoken language processing is mainly conducted 
under SBrT (Brazilian Telecommunications 
Society)31. PROPOR series have tried to bring 
together these two communities, fostering joint 
research and mutual awareness of both research 
lines.  
The lack of formal models for Portuguese 
linguistic description and analysis was mainly 
perceived by linguists that work with CL/NLP. In 
fact, they acknowledge that Brazil has no tradition 
in carrying out events on these themes, what would 
eventually harm CL/NLP research. This goes along 
with Sp?rck Jones (2007) opinion paper. One first 
step towards overcoming this lack of formal 
models for Portuguese description was the 
Workshop on Portuguese Description32, carried out 
together with the last edition of STIL. 
Another point that deserves attention is the 
sentiment that CL/NLP research suffers from 
marginalization in both Computer Science and 
Linguistics areas, as it is usually the case for 
multidisciplinary subjects. We believe this might 
be fueled by the way research is assessed in Brazil. 
In Brazil, the quality of research is mainly assessed 
by the publications generated from it, and 
publication vehicles from Linguistics are usually 
rated worse in Computer Science, and vice versa. It 
is expected that different areas may have different 
scientific methods and perspectives, as well as it is 
natural that such differences are mirrored in any 
evaluation instrument. However, such factors lead 
some researchers to feel uncomfortable with the 
multidisciplinary nature of CL/NLP field and the 
way they are recognized in their own major areas. 
Many researchers (not only from Brazil, but also 
from Portugal) have supported that CL/NLP should 
become a new ?major? area, instead of being part 
of Computer Science or Linguistics. 
                                                           
31
 http://www.sbrt.org.br 
32
 http://www.ppgl.ufscar.br/jdp/index.html 
Concerning insufficient funding, we believe that 
the main complaints came from Brazilian regions 
other than south and southeast, which currently 
concentrate CL/NLP research. In fact, during a 
lengthy discussion at STIL 2009 about the raised 
challenges, this issue was dismissed by many 
participants as non-representative. We believe that 
the funding situation in each region of Brazil 
contributes to the status of research on all topics, 
not particularly CL/NLP, in these regions. While 
in most Brazilian states researchers have to 
compete for funding from national agencies, some 
states (mainly in the southeast region) can rely on 
strong state-based funding agencies, such as 
FAPESP, in the state of S?o Paulo. 
6 Opportunities for Collaboration 
We believe that there are many opportunities for 
collaboration on CL/NLP with other researchers in 
the Americas, mainly due to the fact that the 
research community in Brazil works not only with 
Portuguese, but also with English and Spanish. 
One first step towards collaboration in Latin 
America was given in the event CHARLA 2008 
(Grand Challenges in Computer Science Research 
in Latin America Workshop). Organized by several 
scientific societies (including SBC), the event 
aimed at contributing to the definition of a long-
term research agenda in Latin America with the 
potential to significantly advance science and 
motivate the networking of abilities and 
competencies in Latin America. One of the 
recognized challenges was ?multilinguism?, which 
involves several CL/NLP topics. CHARLA 
immediate impact in Brazil was the adaptation of 
Brazilian CL/NLP events to receive contributions 
in Spanish, which has a vast number of speakers in 
Latin America. Contributions in English were 
already traditionally considered in Brazilian 
events. 
We believe that another important source of 
collaboration comes from awareness of the 
ongoing research projects in the Americas. 
Workshops such as this seem to be a channel for 
the exchange of information. We envision that 
initial collaborations may arise within machine 
translation projects, which naturally already deal 
with the representative languages of the Americas. 
Letting aside technical collaboration, we believe 
there is room for higher-level concrete actions that 
6
could foster collaboration in the Americas. These 
are actions that may increase the visibility of the 
research done in Latin America, as well as 
motivate new research. One first action that we 
envisage is the opening of evaluation challenges 
and shared tasks to the languages of the Americas 
other than English. For instance, 
contests/conferences such as TAC33, 
Senseval/SemEval34, and TREC35, among others, 
might make Portuguese/Spanish datasets available, 
as CLEF36 has done in its last editions. This has 
certainly an organizational cost, but it may turn out 
to be a valuable investment.  
Another action that could stimulate the progress 
of CL/NLP research in Latin America consists of 
including the proceedings of other American 
CL/NLP conferences in the ACL Anthology37, for 
example, the proceedings of STIL and PROPOR, 
to mention the Brazilian examples. This could be 
restricted to conferences that received 
ACL/NAACL endorsement and/or sponsorship.  
While the first action we proposed would make 
it feasible for more countries to participate in the 
evaluation contests, the second action would allow 
the works carried out in these countries to be better 
known. 
In a different strategy, we imagine that it must 
be possible for regional scientific associations to 
establish formal partnerships, granting some 
advantages to associated researchers from the 
corresponding countries, such as: registration 
discounts in the CL/NLP conferences from the 
countries (for instance, ACL/NAACL members 
would have discounts for registering in Brazilian 
events, as well as SBC members for ACL/NAACL 
events); and distribution of relevant publications 
for members of the associations (for instance, SBC 
traditionally distributes to its members the JBCS 
journal, which is considered a prestigious 
international publication). 
Our last idea would be to create a fund (possibly 
through the associations? partnership) for funding 
visits for knowledge transfer (1-2 weeks) for 
researchers and mainly students. These could be an 
opportunity for studying/working with researchers 
from other countries that work on topics of 
                                                           
33
 http://www.nist.gov/tac 
34
 http://www.senseval.org 
35
 http://trec.nist.gov 
36
 http://www.clef-campaign.org 
37
 http://aclweb.org/anthology-new 
interest, as well as for renowned researchers to 
visit research groups in order to stimulate work on 
a particular topic. Such opportunities would be 
very positive for Brazilian students. 
We believe that the actions suggested above can 
lead to a more integrated research scenario in the 
Americas. 
Acknowledgments 
The authors are grateful to SBC, CEPLN, 
FAPESP, and CAPES for supporting this work and 
the realization of STIL 2009, where part of the data 
shown in this paper was presented. 
References  
Pardo, T.A.S.; Caseli, H.M.; Nunes, M.G.V. (2009). 
Mapeamento da Comunidade Brasileira de 
Processamento de L?nguas Naturais. In the 
Proceedings of the 7th Brazilian Symposium in 
Information and Human Language Technology - 
STIL, pp. 1-21. September 8-10, S?o Carlos/SP, 
Brazil. 
Santos, D. (2009). Caminhos percorridos no mapa da 
portuguesifica??o: A Linguateca em perspectiva. 
Linguam?tica, N. 1, pp. 25-58. 
Sp?rck Jones, K. (2007). Computational Linguistics: 
What About the Linguistics? Computational 
Linguistics, Last Words Section, Vol. 33, N. 3, pp. 
437-441. 
7
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 74?82,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Experiments with CST-based Multidocument Summarization 
 
 
Maria Luc?a del Rosario Castro Jorge, Thiago Alexandre Salgueiro Pardo 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Avenida Trabalhador s?o-carlense, 400 - Centro 
P.O.Box 668. 13560-970, S?o Carlos/SP, Brazil 
{mluciacj,taspardo}@icmc.usp.br 
 
 
 
 
Abstract 
Recently, with the huge amount of growing 
information in the web and the little 
available time to read and process all this 
information, automatic summaries have 
become very important resources. In this 
work, we evaluate deep content selection 
methods for multidocument summarization 
based on the CST model (Cross-document 
Structure Theory). Our methods consider 
summarization preferences and focus on the 
overall main problems of multidocument 
treatment: redundancy, complementarity, and 
contradiction among different information 
sources. We also evaluate the impact of the 
CST model over superficial summarization 
systems. Our results show that the use of 
CST model helps to improve informativeness 
and quality in automatic summaries. 
1 Introduction 
In the last years there has been a considerable 
increase in the amount of online information and 
consequently the task of processing this 
information has become more difficult. Just to 
have an idea, recent studies conducted by IDC 
showed that 800 exabytes of information were 
produced in 2009, and it is estimated that in 2012 
it will be produced 3 times more. Among all of 
this information, there is a lot of related content 
that comes from different sources and that 
presents similarities and differences. Reading 
and dealing with this is not straightforward. In 
this scenario, multidocument summarization has 
become an important task.  
Multidocument summarization consists in 
producing a unique summary from a set of 
documents on the same topics (Mani, 2001). A 
multidocument summary must contain the most 
relevant information from the documents. For 
example, we may want to produce a 
multidocument summary from all the documents 
telling about the recent world economical crisis 
or the terrorism in some region. As an example, 
Figure 1 reproduces a summary from Radev and 
Mckeown (1998), which contains the main facts 
from 4 news sources.  
 
 
 
 
 
 
 
 
 
Figure 1: Example of multidocument summary 
(Radev and Mckeown, 1998, p. 478) 
 
Multidocument summarization has to deal not 
only with the fact of showing relevant 
information but also with some multidocument 
phenomena such as redundancy, 
complementarity, contradiction, information 
ordering, source identification, temporal 
resolution, etc. It is also interesting to notice that, 
instead of only generic summaries (as the one in 
the example), summaries may be produced 
considering user preferences. For example, one 
may prefer summaries including information 
attributed to particular sources (if one trusts more 
in some sources) or more context information 
(considering a reader that has not accompanied 
some recent important news), among other 
possibilities.  
Reuters reported that 18 people were killed in a 
Jerusalem bombing Sunday. The next day, a bomb 
in Tel Aviv killed at least 10 people and wounded 
30 according to Israel radio. Reuters reported that 
at least 12 people were killed and 105 wounded. 
Later the same day, Reuters reported that the 
radical Muslim group Hamas had claimed 
responsibility for the act. 
74
There are two main approaches for 
multidocument summarization (Mani and 
Maybury, 1999): the superficial and the deep 
approaches. Superficial approach uses little 
linguistic knowledge to produce summaries. This 
approach usually has low cost and is more 
robust, but it produces poor results. On the other 
hand, deep approaches use more linguistic 
knowledge to produce summaries. In general 
terms, in this approach it is commonly used 
syntactical, semantic and discourse parsers to 
analyze the original documents. A very common 
way to analyze documents consists in 
establishing semantic relations among the 
documents parts, which helps identifying 
commonalities and differences in information. 
Within this context, discourse models as CST 
(Cross-document Structure Theory) (Radev, 
2000) are useful (see, e.g., Afantenos et al, 
2004; Afantenos, 2007; Jorge and Pardo, 2009, 
2010; Radev and Mckeown, 1998; Radev et al, 
2001; Zhang et al, 2002). 
It was proposed in Mani and Maybury (1999) 
a general architecture for multidocument 
summarization, with analysis, transformation, 
and synthesis stages. The first stage consists in 
analyzing and formally representing the content 
of the original documents. The second stage 
consists mainly in transforming the represented 
content into a condensed content that will be 
included in the final summary. One of the most 
important tasks in this stage is the content 
selection process, which consists in selecting the 
most relevant information. Finally, the third 
stage expresses the condensed content in natural 
language, producing the summary. 
In this paper, we explore a CST-based 
summarization method and evaluate the 
corresponding prototype system for 
multidocument summarization. Our system, 
called CSTSumm (CST-based SUMMarizer), 
produces multidocument summaries from input 
CST-analyzed news documents. We mainly 
investigate content selection methods for 
producing both generic and preference-based 
summaries. Particularly, we formalize and codify 
our content selection strategies as operators that 
perform the previously cited transformation 
stage. We run our experiments with Brazilian 
Portuguese news texts (previously analyzed 
according to CST by human experts) and show 
that we produce more informative summaries in 
comparison with some superficial summarizers 
(Pardo, 2005; Radev et al, 2000). We also use 
CST to enrich these superficial summarizers, 
showing that the results also improve. Our 
general hypothesis for this work is that the deep 
knowledge provided by CST helps to improve 
information and quality in summaries. 
This work is organized as follows. In Section 
2, the main concepts of the CST model are 
introduced and the works that have already used 
CST for multidocument summarization are 
reviewed.  In Section 3, we present CSTSumm, 
while its evaluation is reported in Section 4. 
Some final remarks are presented in Section 5. 
2 Related Work 
2.1 Cross-document Structure Theory 
Radev (2000) proposed CST model with a set of 
24 relations for multidocument treatment in any 
domain. Table 1 lists these relations. 
 
Table 1: CST original relations 
Identity Judgment 
Equivalence Fulfillment 
Translation Description 
Subsumption Reader profile 
Contradiction Contrast 
Historical background Parallel 
Modality Cross-reference 
Attribution Citation 
Summary Refinement 
Follow-up Agreement 
Elaboration Generalization 
Indirect speech Change of perspective 
 
The established relations may have (or not) 
directionality, e.g., the equivalence relation 
(which states that two text segments have similar 
content) has no directionality while the historical 
background relation (which states that a segment 
provides historical information about other) has. 
Figure 2 shows examples of these two relations 
among sentences from different sources. 
As part of the model, the author proposes a 
general schema that reveals the possibility of 
relationship at any level of linguistic analysis. 
Figure 3 (reproduced from Radev, 2000) 
illustrates this schema. According to this schema, 
the documents with CST relations are 
represented as a graph, whose nodes are text 
segments (of possibly any level) and the edges 
are relations. This graph is possibly 
disconnected, since not all segments present 
relations with other segments. It is important to 
say that, in general, only one analysis level is 
treated. In this work, we only deal with sentences 
from the input documents, since sentences are 
75
well delimited and are standard segments in 
discourse analysis. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Examples of CST relations 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: CST general schema (Radev, 2000, p. 78) 
2.2 Multidocument Summarization 
A few works explored CST for multidocument 
summarization. A 4-stage multidocument 
summarization methodology was proposed in 
Radev (2000). In this methodology, the first 
stage consists in clustering documents according 
to their topics. In the second stage, internal 
analysis (syntactical and semantic, for instance) 
of the texts may be performed. In the third stage, 
CST relations are established among texts. 
Finally, in the fourth stage, information is 
selected to produce the final summary. For this 
methodology the author suggests using operators 
activated by user summarization preferences 
such as authorship (i.e., reporting the information 
sources) or contradictory information preference. 
The author also says that it may be possible to 
produce generic summaries without considering 
a particular preference. In this case the criterion 
used to select information is based on the number 
of CST relations that a segment has. This 
criterion is based on the idea that relevant 
information is more repeated/elaborated and 
related to other segments across documents. This 
may be easily verified in practice. In this paper 
we follow such ideas. 
A methodology for enriching multidocument 
summaries produced by superficial summarizers 
was proposed by Zhang et al (2002). The 
authors incorporated the information given by 
CST relations to MEAD (Radev et al, 2000) 
summarization process, showing that giving 
preference to segments with CST relations 
produces better summaries. Otterbacher et al 
(2002) investigated how CST relations may 
improve cohesion in summaries, which was 
tested by ordering sentences in summaries 
according to CST relations. The idea used behind 
this ordering is that sentences related by CST 
relations should appear closer in the final 
summaries as well as should respect possible 
temporal constraints indicated by some relations. 
Afantenos et al (2004) proposed another 
summarization methodology that extracts 
message templates from the texts (using 
information extraction tools) and, according to 
the type of CST relation between two templates, 
produces a unified message that would represent 
the summary content. The authors did not fully 
implement this method. 
3 CSTSumm 
In this paper we evaluate a CST-based 
multidocument summarization method by 
implementing and testing a prototype system, 
called CSTSumm. It performs content selection 
operations over a group of texts on the same 
topic that were previously annotated according to 
CST. For the moment, we are using manually 
annotated texts, i.e., the analysis stage of 
multidocument summarization is only simulated. 
In the future, texts may be automatically 
annotated, since a CST parser is under 
development for Brazilian Portuguese language 
(Maziero et al, 2010). 
Initially, the system receives as input the 
CST-annotated texts, which are structured as a 
graph. An initial rank of sentences is then built: 
the sentences are ordered according to the 
number of CST relations they present; the more 
Equivalence relation 
Sentence 1: Nine people died, three of them 
children, and 25 others were wounded last 
Monday in a blast at a market in Moscow, 
police said. 
Sentence 2: Nine people died, including three 
children, and 25 others were injured last 
Monday in an explosion that happened at a 
market in Moscow, police of Moscow 
informed. 
Historical background relation 
(directionality: from Sentence 2 to 1) 
Sentence 1: An airplane accident in Bukavu, 
east of Democratic Republic of Congo, killed 
13 people this Thursday in the afternoon. 
Sentence 2: Congo has a history of more than 
30 airplane tragedies. 
76
relations a sentence presents, better ranked it will 
be. Having the initial rank, content selection is 
performed. In this work, following the idea of 
Jorge and Pardo (2010), we represent and codify 
each content selection strategy as an operator. A 
content selection operator tells how to rearrange 
the sentences in the rank in order to produce 
summaries that better satisfy the corresponding 
user preferences. For instance, if a user requires 
more context information in the summary, the 
corresponding operator is activated. Such 
operator will (i) select in the rank all the 
sentences that present historical background and 
elaboration CST relations with better ranked 
sentences and (ii) improve their position in the 
rank by putting them immediately after the better 
ranked sentences with which they are related. 
This final action would give to these 
?contextual? sentences more preference for being 
in the summary, since they are better positioned 
in the refined rank. Figure 4 shows an example 
of a hypothetical CST graph (derived from a 
group of texts), the corresponding initial rank 
(with relations preserved for clarification) and 
the transformation that the context operator 
would do for producing the new/refined rank. It 
is possible to see that sentence 1, that presents 
historical information about the sentence 4, gets 
a better position in the rank (immediately after 
sentence 4), receiving some privilege to be in the 
summary. 
Besides the context operator, we also have 
other 3 operators: the contradiction operator 
(which looks for the contradiction CST relation 
in order to include in the summary every 
contradiction in the texts), the authorship 
operator (which looks for the citation and 
attribution CST relations in order to include in 
the summary possible sources that provided the 
available information), and the evolving events 
operator (which looks for historical background 
and follow-up CST relations in order to present 
the development of the events during a time 
period). 
Independently from the user preference, an 
extra operator is always applied: the redundancy 
operator. It removes from the rank all sentences 
whose information is already expressed in other 
better ranked sentences. Redundancy is 
represented by the identity, equivalence, and 
subsumption CST relations. 
After the content selection process, in the last 
stage ? the synthesis stage ? the system selects as 
many sentences from the rank as allowed by the 
specified compression rate. The compression rate 
(provided by the user) informs the size of the 
summary. For instance, a 70% rate indicates that 
the summary must have at most 30% of the 
number of words in a text. In this work, given the 
multidocument nature of the task, we compute 
the compression rate over the size of the longest 
text in the group. 
 
Hypothetical CST graph 
 
 
Initial rank 
 
 
Refined rank (after applying the operator) 
 
Figure 4: Example of context operator application 
 
Synthesis stage also orders the selected sentences 
according to a simple criterion that only 
considers the position of the sentences in the 
original documents: first sentences appear first in 
the summary. If two sentences have the same 
position but in different documents, then the 
sentences are ordered according to the document 
number. Finally, we apply a sentence fusion 
system (Seno and Nunes, 2009) to some selected 
sentences. This is done when sentences with 
overlap CST relation among them are selected to 
the summary. The overlap relation indicates that 
the sentences have similar content, but also that 
both present unique content. In this case, it is 
desired that the sentences become only one with 
the union of their contents. The fusion system 
that we use does that. Figure 5 illustrates the 
fusion process, with the original sentences and a 
resulting fusion. 
Figure 6 shows the general architecture of 
CSTSumm, which summarizes the whole process 
described before. Each operator is codified in 
77
XML, where it is specified which relations 
should be looked in the rank in order to have the 
correspondent sentences better ranked. It is 
important to notice that, excepting the 
redundancy operator, our system was designed to 
allow the application of only one content 
selection operator at a time. If more than one 
operator is applied, the application of the 
following operator may probably rewrite the 
modifications in the rank that the previous 
operator has done. For instance, the application 
of the contradiction operator after the context 
operator might include sentences with 
contradiction above sentences with context 
information in the rank, altering therefore the 
rank produced by the context operator. One 
simple alternative to this design choice is to ask 
the user to rank his preferences and, then, to 
apply the corresponding operators in the opposite 
order, so that the rank produced by the most 
important preference will not be further altered. 
Other alternative is to produce more complex 
operators that combine preferences (and the 
corresponding CST relations), but some 
preference on the relations should still be 
specified. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 5: Example of sentence fusion 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: CSTSumm architecture 
 
In Figure 7 we show the algorithm for the 
application of operators during content selection 
process. It is important to notice that the selected 
operator looks for its relations in all pairs of 
sentences in the rank. Once it finds the relations, 
it rearranges the rank appropriately, by putting 
the related sentence more above in the rank. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Algorithm for application of content 
selection operators  
 
As an illustration of the results of our system, 
Figure 8 shows an automatic summary produced 
from a group of 3 texts with the application of 
the context operator (after redundancy operator 
was applied) and a 70% compression rate. The 
summary was translated from Portuguese, the 
language with which the summarizer was tested. 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 8: Example of multidocument summary with 
context information 
4 Evaluation 
Our main research question in this work was how 
helpful CST would be for producing better 
summaries. CSTSumm enables us to assess the 
summaries and content selection strategies, but a 
comparison of these summaries with summaries 
produced by superficial methods is still 
necessary. In fact, we not only proceeded to such 
Sentence 1: According to a spokesman from 
United Nations, the plane was trying to land at 
the airport in Bukavu in the middle of a storm. 
Sentence 2: Everyone died when the plane, 
hampered by bad weather, failed to reach the 
runway and crashed in a forest 15 kilometers 
from the airport in Bukavu. 
Fusion: According to a spokesman for the United 
Nations, everyone died when a plane that was 
trying to land at Bukavu airport, hampered by bad 
weather, failed to reach the runway and crashed 
in a forest 15 kilometers from the airport. 
procedure for application of content selection 
operators 
input data: initial rank, user summarization 
preference, operators 
output data: refined rank 
apply the redundancy operator 
select one operator according to the user 
summarization preference 
for i=sentence at the first position in the rank to the 
last but one sentence 
for j=sentence at position i+1 in the rank to the 
last sentence 
if the operator relations happen among 
sentences i and j, rearrange the rank 
appropriately 
The Brazilian volleyball team has won on Friday 
the seventh consecutive victory in the World 
League, defeating Finland by 3 sets to 0 - partials 
of 25/17, 25/22 and 25/21 - in a match in the 
Tampere city, Finland. The first set remained 
balanced until the middle, when Andr? Heller 
went to serve. In the last part, Finland again 
paired the game with Brazil, but after a sequence 
of Brazilians points Finland failed to respond and 
lost by 25 to 21. The Brazilian team has won five 
times the World League in 1993, 2001, 2003, 
2004 and 2005. 
78
comparison, but also improved the superficial 
methods with CST knowledge. 
As superficial summarizers, we selected 
MEAD (Radev et al, 2000) and GistSumm 
(Pardo et al, 2003; Pardo, 2005) summarizers. 
MEAD works as follows. Initially, MEAD builds 
an initial rank of sentences according to a score 
based on three parameters: position of the 
sentence in the text, lexical distance of the 
sentence to the centroid of the text, and the size 
of the sentence. These three elements are linearly 
combined for producing the score. GistSumm, on 
the other side, is very simple: the system 
juxtaposes all the source texts and gives a score 
to each sentence according to the presence of 
frequent words (following the approach of Luhn, 
1958) or by using TF-ISF (Term Frequency ? 
Inverse Sentence Frequency, as proposed in 
Larroca et al, 2000). Following the work of 
Zhang et al (2002), we decided to use CST to 
rearrange (and supposedly improve) the sentence 
ranks produced by MEAD and GistSumm. We 
simply add to each sentence score the number of 
CST relations that the sentence presents: 
 
new sentence score = old sentence score + number of 
CST relations 
 
The number of sentences is retrieved from the 
CST graph. This way, the sentence positions in 
the rank are changed. 
For our experiments, we used the CSTNews 
corpus (Aleixo and Pardo, 2008), which is a 
corpus of news texts written in Brazilian 
Portuguese. The corpus contains 50 clusters of 
texts. Each group has from 2 to 4 texts on the 
same topic annotated according to CST by 
human experts, as well as a manual generic 
summary with 70% compression rate (in relation 
to the longest text). The annotation process was 
carried out by 4 humans, with satisfactory 
agreement, which demonstrated that the 
annotation task was well defined and performed. 
More details about the corpus and its annotation 
process are presented by Maziero et al (2010). 
For each cluster of CSTNews corpus, it was 
produced a set of automatic summaries 
corresponding to each method that was explored 
in this work. To evaluate the informativity and 
quality of the summaries, we used two types of 
evaluation: automatic evaluation and human 
evaluation. For the automatic evaluation we used 
ROUGE (Lin, 2004) informativity measure, 
which compares automatic summaries with 
human summaries in terms of the n-grams that 
they have in common, resulting in precision, 
recall and f-measure numbers between 0 (the 
worst) and 1 (the best), which indicate how much 
information the summary presents. Precision 
indicates the amount of relevant information that 
the automatic summary contains; recall indicates 
how much information from the human summary 
is reproduced in the automatic summary; f-
measure is a unique performance measure that 
combines precision and recall. Although it looks 
simple, ROUGE author has showed that it 
performs as well as humans in differentiating 
summary informativeness, which caused the 
measure to be widely used in the area. In 
particular, for this work, we considered only 
unigram comparison, since the author of the 
measure demonstrated that unigrams are enough 
for differentiating summary quality. For 
computing ROUGE, we compared each 
automatic summary with the corresponding 
human summary in the corpus. 
We computed ROUGE for every summary we 
produced through several strategies: using only 
the initial rank, only the redundancy operator, 
and the remaining preference operators (applied 
after the redundancy operator). Is is important to 
notice that it is only fair to use ROUGE to 
evaluate the summaries produced by the initial 
rank and by the redundancy operator, since the 
human summary (to which ROUGE compares 
the automatic summaries) are generic, produced 
with no preference in mind. We only computed 
ROUGE for the preference-biased summaries in 
order to have a measure of how informative they 
are. Ideally, these preference-biased summaries 
should not only mirror the user preference, but 
also contain the main information from the 
source texts. 
On the other hand, we used human evaluation 
to measure the quality of the summaries in terms 
of coherence, cohesion and redundancy, factors 
that ROUGE is not sensitive enough to capture. 
By coherence, we mean the characteristic of a 
text having a meaning and being understandable. 
By cohesion, we mean the superficial makers of 
coherence, i.e., the sequence of text elements that 
connect the ideas in the text, as punctuation, 
discourse markers, anaphors, etc. 
For each one of the above evaluation factors, 
a human evaluator was asked to assign one of 
five values: very bad (score 0), bad (score 1), 
regular (score 2), good (score 3), and excellent 
(score 4). We also asked humans to evaluate 
informativity in the preference-biased summaries 
produced by our system, which is a more fair 
79
evaluation than the automatic one described 
above. The user should score each summary 
(using the same values above) according to how 
much he was satisfied with the actual content of 
the summary in face of the preference made. The 
user had access to the source texts for performing 
the evaluation. 
Table 2 shows the ROUGE scores for the 
summaries produced by the initial rank, by the 
application of the operators, by the superficial 
summarizers, and by the CST-enriched 
superficial summarizers. It is important to say 
that these results are the average results obtained 
for the automatic summaries generated for all the 
clusters in the CSTNews corpus. 
 
Table 2: ROUGE results 
Content selection method Precision Recall F-measure 
Initial rank 0.5564 0.5303 0.5356 
Redundancy treatment (only) 0.5761 0.5065 0.5297 
Context information 0.5196 0.4938 0.4994 
Authorship information 0.5563 0.5224 0.5310 
Contradiction information 0.5503 0.5379 0.5355 
Evolving events information 0.5159 0.5222 0.5140 
MEAD without CST 0.5242 0.4602 0.4869 
MEAD with CST 0.5599 0.4988 0.5230 
GistSumm without CST 0.3599 0.6643 0.4599 
GistSumm with CST 0.4945 0.5089 0.4994 
 
As expected, it may be observed that the best 
results were achieved by the initial rank (since it 
produces generic summaries, as happens to the 
human summaries to which they are compared), 
which does not consider any summarization 
preference at all. It is also possible to see that: (a) 
the superficial summarizers are outperformed by 
the CST-based methods and (b) CST-enriched 
superficial summarizers produced better results 
than the superficial summarizers.  
Results for human evaluation are shown in 
Table 3. These results show the average value for 
each factor evaluated for a sample group of 48 
texts randomly selected from the corpus. We also 
associated to each value the closest concept in 
our evaluation. We could not perform the 
evaluation for the whole corpus due to the high 
cost and time-demanding nature of the human 
evaluation. Six humans carried out this 
evaluation. Each human evaluated eight 
summaries, and each summary was evaluated by 
three humans. 
 
Table 3: Results for human evaluation 
Content selection method Coherence Cohesion Redundancy Informativity 
Initial rank 3.6  
Excellent 
3.2 
Good 
1.8 
Regular 
3.6 
Excellent 
Context  2.1 
Regular 
2.7 
Good 
3.6 
Excellent 
2.2 
Regular 
Authorship  
 
3.3 
Good 
2.4 
Regular 
2.8 
Good 
3 
Good 
Contradiction  2.4 
Regular 
2.7 
Good 
2.5 
Regular 
3.7 
Excellent 
Evolving events  2.1 
Regular 
2.5 
Regular 
2.6 
Good 
3.2 
Good 
 
It may be observed that informativity factor 
results are quite satisfactory, since more than 
50% of the judges considered that the 
performance was excellent. For coherence, 
cohesion and redundancy factors, results were 
not excellent in all the cases, but they were not 
bad either. We consider that one of the things 
that could have had an influence in this case is 
the performance of the fusion system, since it 
may generate sentences with some problems of 
coherence and cohesion. There are also other 
things that may influence these results, such as 
80
the method for ordering sentences that we used 
in this work. This method does not follow any 
deep criteria to order sentences and may also 
lead to coherence and cohesion problems. 
These results show that CSTSumm is capable 
of producing summaries with good informativity 
and quality. In fact, the results validate our 
hypothesis that deep knowledge may improve the 
results, since it deals better with the 
multidocument phenomena, as the presence of 
redundant, complementary and contradictory 
information. 
5 Final Remarks 
Although we consider that very good results 
were achieved, there is still room for 
improvements. Future works include the 
investigation of better sentence ordering 
methods, as well as more investigation on how to 
jointly apply more than one content selection 
operator. 
For the moment, CSTSumm assumes that the 
texts to be summarized must be already 
annotated with CST. In the future, as soon as an 
automatic CST parser is available for 
Portuguese, it should provide the suitable input 
to the summarizer. 
Finally, it is interesting to notice that, 
although we have tested our methods with 
Brazilian Portuguese texts, they are robust and 
generic enough to be applied to any other 
language, since both our methods and CST 
model are language independent. 
Acknowledgments 
The authors are grateful to FAPESP and CNPq 
for supporting this work. 
References  
Afantenos, S.D.; Doura, I.; Kapellou, E.; 
Karkaletsis, V. 2004. Exploiting Cross-
Document Relations for Multi-document 
Evolving Summarization. In the Proceedings 
of SETN, pp. 410-419. 
Afantenos, S.D. 2007. Reflections on the Task of 
Content Determination in the Context of 
Multi-Document Summarization of Evolving 
Events. In Recent Advances on Natural 
Language Processing. Borovets, Bulgaria. 
Aleixo, P. and Pardo, T.A.S. 2008. CSTNews: 
Um C?rpus de Textos Journal?sticos Anotados 
segundo a Teoria Discursiva CST (Cross-
Document Structure Theory). S?rie de 
Relat?rios T?cnicos do Instituto de Ci?ncias 
Matem?ticas e de Computa??o, Universidade 
de S?o Paulo no. 326. S?o Carlos, Brazil . 
Jorge, M.L.C and Pardo, T.A.S. 2009. Content 
Selection Operators for Multidocument 
Summarization based on Cross-document 
Structure Theory. In the Proceedings of the 7th 
Brazilian Symposium in Information and 
Human Language Technology. S?o Carlos, 
Brazil. 
Jorge, M.L.C. and Pardo, T.A.S. 2010. 
Formalizing CST-based Content Selection 
Operations. In the Proceedings of the 9th 
International Conference on Computational 
Processing of Portuguese Language (Lecture 
Notes in Artificial Intelligence 6001), pp. 25-
29.  Porto Alegre, Brazil. 
Larocca Neto, J.; Santos, A.D.; Kaestner, A.A.; 
Freitas, A.A. 2000. Generating Text 
Summaries through the Relative Importance of 
Topics. In M.C. Monard and J.S. Sichman 
(eds.), Lecture Notes in Artificial Intelligence, 
N. 1952, pp. 300-309. Springer, Verlag. 
Lin, C-Y. 2004. ROUGE: a Package for 
Automatic Evaluation of Summaries. In the 
Proceedings of the Workshop on Text 
Summarization Branches Out. Barcelona, 
Spain.  
Luhn, H.P. 1958. The automatic creation of 
literature abstracts. IBM Journal of Research 
and Development, Vol. 2, pp. 159-165. 
Barcelona, Spain. 
Mani, I. and Maybury, M. T. 1999. Advances in 
automatic text summarization. MIT Press, 
Cambridge, MA. 
Mani, I. 2001. Automatic Summarization. John 
Benjamins Publishing Co. Amsterdam. 
Maziero, E.G.; Jorge, M.L.C.; Pardo, T.A.S. 
2010. Identifying Multidocument Relations. In 
the Proceedings of the 7th International 
Workshop on Natural Language Processing 
and Cognitive Science. June 8-12, 
Funchal/Madeira, Portugal. 
Otterbacher, J.C.; Radev, D.R.; Luo, A. 2002. 
Revisions that improve cohesion in multi-
document summaries: a preliminary study. In 
the Proceedings of the Workshop on 
Automatic Summarization, pp 27-36. 
Philadelphia. 
81
Pardo, T.A.S.; Rino, L.H.M.; Nunes, M.G.V. 
2003. GistSumm: A Summarization Tool 
Based on a New Extractive Method. In N.J. 
Mamede, J. Baptista, I. Trancoso, M.G.V. 
Nunes (eds.), 6th Workshop on Computational 
Processing of the Portuguese Language - 
Written and Spoken (Lecture Notes in 
Artificial Intelligence 2721), pp. 210-
218. Faro, Portugal. 
Pardo, T.A.S. 2005. GistSumm - GIST 
SUMMarizer: Extens?es e Novas 
Funcionalidades. S?rie de Relat?rios do 
NILC. NILC-TR-05-05. S?o Carlos, Brazil. 
Radev, D. and McKeown, K. 1998. Generating 
natural language summaries from multiple on-
line sources. Computational Linguistics, Vol. 
24, N. 3, pp. 469-500. 
Radev, D.R. 2000. A common theory of 
information fusion from multiple text sources, 
step one: Cross-document structure. In the 
Proceedings of the 1st ACL SIGDIAL 
Workshop on Discourse and Dialogue. Hong 
Kong. 
Radev, D.R.; Jing, H.; Budzikowska, M. 2000. 
Centroid-based summarization of multiple 
documents: sentence extraction, utility-based 
evaluation and user studies. In the 
Proceedings of the ANLP/NAACL Workshop, 
pp. 21-29. 
Radev, D.R.; Blair-Goldensohn, S.; Zhang, Z. 
2001. Experiments in single and multi-
document summarization using MEAD. In the 
Proceedings of the 1st Document 
Understanding Conference. New Orleans, LA. 
Seno, E.R.M. and Nunes, M.G.V. 2009. 
Reconhecimento de Informa??es Comuns para 
a Fus?o de Senten?as Compar?veis do 
Portugu?s. Linguam?tica, Vol. 1, pp. 71-87. 
Zhang, Z.; Goldenshon, S.B.; Radev, D.R. 2002. 
Towards CST-Enhanced Sumarization. In the 
Proceedings of the 18th National Conference 
on Artificial Intelligence. Edmonton. 
82
Proceedings of the SIGDIAL 2013 Conference, pages 92?96,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
On the contribution of discourse structure to topic segmentation 
 
Paula C. F. Cardoso1, Maite Taboada2, Thiago A. S. Pardo1 
1N?cleo Interinstitucional de Lingu?stica Computacional (NILC) 
Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
Av. Trabalhador S?o-carlense, 400 - Centro 
Caixa Postal: 668 ? CEP: 13566-970 ? S?o Carlos/SP 
2Department of Linguistics ? Simon Fraser University  
8888 University Dr., Burnaby, B.C., V5A 1S6 - Canada 
pcardoso@icmc.usp.br, mtaboada@sfu.ca, taspardo@icmc.usp.br 
 
 
Abstract 
In this paper, we describe novel methods for 
topic segmentation based on patterns of dis-
course organization. Using a corpus of news 
texts, our results show that it is possible to use 
discourse features (based on Rhetorical Struc-
ture Theory) for topic segmentation and that 
we outperform some well-known methods. 
1 Introduction 
Topic segmentation aims at finding the bounda-
ries among topic blocks in a text (Chang and 
Lee, 2003). This task is useful for a number of 
important applications such as information re-
trieval (Prince and Labadi?, 2007), automatic 
summarization (Wan, 2008) and question-
answering systems (Oh et al, 2007). 
In this paper, following Hearst (1997), we as-
sume that a text or a set of texts develop a main 
topic, exposing several subtopics as well. We 
also assume that a topic is a particular subject 
that we write about or discuss (Hovy, 2009), and 
subtopics are represented in pieces of text that 
cover different aspects of the main topic (Hearst, 
1997; Hennig, 2009). Therefore, the task of topic 
segmentation aims at dividing a text into topical-
ly coherent segments, or subtopics. The granular-
ity of a subtopic is not defined, as a subtopic may 
contain one or more sentences or paragraphs. 
Several methods have been tested for topic 
segmentation. There are, however, no studies on 
how discourse structure directly mirrors topic 
boundaries in texts and how they may contribute 
to such task, although such possible correlation 
has been suggested (e.g., Hovy and Lin, 1998). 
In this paper, we follow this research line, 
aiming at exploring the relationship of discourse 
and subtopics. In particular, our interest is main-
ly on the potential of Rhetorical Structure Theory 
(RST) (Mann and Thompson, 1987) for this task. 
We propose and evaluate automatic topic seg-
mentation strategies based on the rhetorical 
structure of a text. We also compare our results 
to some well-known algorithms in the area, 
showing that we outperform these algorithms. 
Our experiments were performed using a corpus 
of news texts manually annotated with RST and 
subtopics. 
The remainder of this paper is organized as 
follows. Section 2 gives a brief background on 
text segmentation. Section 3 describes our auto-
matic strategies to find the subtopics. The corpus 
that we use is described in Section 4. Section 5 
presents some results and Section 6 contains the 
conclusions and future work. 
2 Related work 
Several approaches have tried to measure the 
similarity across sentences and to estimate where 
topic boundaries occur. One well-known ap-
proach, that is heavily used for topic segmenta-
tion, is TextTiling (Hearst, 1997), which is based 
on lexical cohesion. For this strategy, it is as-
sumed that a set of lexical items is used during 
the development of a subtopic in a text and, 
when that subtopic changes, a significant propor-
tion of vocabulary also changes.  
Passoneau and Litman (1997), in turn, have 
combined multiple linguistic features for topic 
segmentation of spoken text, such as pause, cue 
words, and referential noun phrases. Hovy and 
Lin (1998) have used various complementary 
92
techniques for topic segmentation, including 
those based on text structure, cue words and 
high-frequency indicative phrases for topic iden-
tification in a summarization system. Although 
the authors do not mention an evaluation of these 
features, they suggested that discourse structure 
might help topic identification. For this, they 
suggested using RST.  
RST represents relations among propositions 
in a text and discriminates nuclear and satellite 
information. In order to present the differences 
among relations, they are organized in two 
groups: subject matter and presentational rela-
tions. In the former, the text producer intends 
that the reader recognizes the relation itself and 
the information conveyed, while in the latter the 
intended effect is to increase some inclination on 
the part of the reader (Taboada and Mann, 2006). 
The relationships are traditionally structured in a 
tree-like form (where larger units ? composed of 
more than one proposition ? are also related in 
the higher levels of the tree).  
To the best of our knowledge, we have not 
found any proposal that has directly employed 
RST for topic segmentation purposes. Following 
the suggestion of the above authors, we investi-
gated how discourse structure mirrors topic shifts 
in texts. Next section describes our approach to 
the problem. 
3 Strategies for topic segmentation  
For identifying and partitioning the subtopics of 
a text, we developed four baseline algorithms 
and six other algorithms that are based on dis-
course features. 
The four baseline algorithms segment at para-
graphs, sentences, random boundaries (randomly 
selecting any number of boundaries and where 
they are in a text) or are based on word reitera-
tion. The word reiteration strategy is an adapta-
tion of TextTiling1 (Hearst, 1997) for the charac-
teristics of the corpus that we used (introduced 
latter in this paper). 
The algorithms based on discourse consider 
the discourse structure itself and the RST rela-
tions in the discourse tree. The first algorithm 
(which we refer to as Simple Cosine) is based on 
Marcu?s idea (2000) for measuring the ?good-
ness? of a discourse tree. He assumes that a dis-
course tree is ?better? if it exhibits a high-level 
structure that matches as much as possible the 
                                                 
1  We have specifically used the block comparison 
method with block size=2. 
topic boundaries of the text for which that struc-
ture was built. Marcu associates a clustering 
score to each node of a tree. For the leaves, this 
score is 0; for the internal nodes, the score is giv-
en by the lexical similarity between the immedi-
ate children. The hypothesis underlying such 
measurements is that better trees show higher 
similarity among their nodes. We have adopted 
the same idea using the cosine measure. We have 
proposed that text segments with similar vocabu-
lary are likely to be part of the same topic seg-
ment. In our case, nodes with scores below the 
average score are supposed to indicate possible 
topic boundaries. 
The second algorithm (referred to as Cosine 
Nuclei) is also a proposal by Marcu (2000). It is 
assumed that whenever a discourse relation holds 
between two textual spans, that relation also 
holds between the most salient units (nuclei) as-
sociated with those spans. We have used this 
formalization and measured the similarity be-
tween the salient units associated with two spans 
(instead of measuring among all the text spans of 
the relation, as in the previous algorithm).  
The third (Cosine Depth) and fourth (Nuclei 
Depth) algorithms are variations of Simple Co-
sine and Cosine Nuclei. For these new strategies, 
the similarity for each node is divided by the 
depth where it occurs, traversing the tree in a 
bottom-up way. These should guarantee that 
higher nodes are weaker and might better repre-
sent topic boundaries. Therefore, we have the 
assumption that topic boundaries are more likely 
to be mirrored at the higher levels of the dis-
course structure. We also have used the average 
score to find out less similar nodes. Figure 1 
shows a sample RST tree. The symbols N and S 
indicate the nucleus and satellite of each rhetori-
cal relation. For this tree, the score between 
nodes 3 and 4 is divided by 1 (since we are at the 
leaf level); the score between Elaboration and 
node 5 is divided by 2 (since we are in a higher 
level, 1 above the leaves on the left); and the 
score between Sequence and Volitional-result is 
divided by 3 (1 above the leaves on the right). 
 
 
Figure 1. Example of an RST structure 
93
The next algorithms are based on the idea that 
some relations are more likely to represent topic 
shifts. For estimating this, we have used the 
CSTNews (described in next section), which is 
manually annotated with subtopics and RST. 
In this corpus, there are 29 different types of 
RST relations that may connect textual spans. In 
an attempt to characterize topic segmentation 
based on rhetorical relations, we recorded the 
frequency of those relations in topic boundaries. 
We realized that some relations were more fre-
quent on topic boundaries, whereas others never 
occurred at the boundaries of topics. Out of the 
29 relations, 16 appeared in the reference annota-
tion. In topic boundaries, Elaboration was the 
most frequent relation (appearing in 60% of the 
boundaries), followed by List (20%) and Non-
Volitional Result (5%). Sequence and Evidence 
appeared in 2% of the topic boundaries, and 
Background, Circumstance, Comparison, Con-
cession, Contrast, Explanation, Interpretation, 
Justify, and Non-Volitional Cause in 1% of the 
boundaries. 
We used this knowledge about the relations? 
frequency and attributed a weight associated with 
the possibility that a relation indicates a bounda-
ry, in accordance with its frequency on topic 
boundaries in the reference corpus. Figure 2 
shows how the 29 relations were distributed. One 
relation is weak if it usually indicates a bounda-
ry; in this case, its weight is 0.4. One relation is 
medium because it may indicate a boundary or 
not; therefore, its weight is 0.6. On the other 
hand, a strong relation almost never indicates a 
topic boundary; therefore, its weight is 0.8. Such 
values were empirically determined. Another 
factor that may be observed is that all presenta-
tional relations are classified as strong, with the 
exception of Antithesis. This is related to the def-
inition of presentational relations, and Antithesis 
was found in the reference segmentation with a 
low frequency. 
 
Class Relations 
Weak 
(0.4) 
Elaboration, Contrast, Joint, List 
Medium 
(0.6) 
Antithesis, Comparison, Evaluation 
Means, Non-Volitional Cause, Non-
Volitional Result, Solutionhood, Voli-
tional Cause, Volitional Result, Sequence 
Strong 
(0.8) 
Background, Circumstance, Concession, 
Conclusion, Condition, Enablement, Evi-
dence, Explanation, Interpretation, Justi-
fy, Motivation, Otherwise, Purpose, Re-
statement, Summary 
Figure 2. Classification of RST relations 
From this classification we created two more 
strategies: Relation_Depth and Nu-
clei_Depth_Relation. Relation_Depth associates 
a score to the nodes by dividing the relations 
weight by the depth where it occurs, in a bottom-
up way of traversing the tree. We also have used 
the average score to find out nodes that are less 
similar. As we have observed that some im-
provement might be achieved every time nuclei 
information was used, we have tried to combine 
this configuration with the relations? weight. 
Hence, we computed the scores of the Nuclei 
Depth strategy times the proposed relations 
weight. This was the algorithm that we called 
Nuclei_Depth_Relation. Therefore, these two 
last algorithms enrich the original Cosine Depth 
and Nuclei Depth strategies with the relation 
strength information.  
The next section presents the data set we have 
used for our evaluation. 
4 Overview of the corpus 
We used the CSTNews corpus2 that is composed 
of 50 clusters of news articles written in Brazili-
an Portuguese, collected from several sections of 
mainstream news agencies: Politics, Sports, 
World, Daily News, Money, and Science. The 
corpus contains 140 texts altogether, amounting 
to 2,088 sentences and 47,240 words. On aver-
age, the corpus conveys in each cluster 2.8 texts, 
41.76 sentences and 944.8 words. All the texts in 
the corpus were manually annotated with RST 
structures and topic boundaries in a systematic 
way, with satisfactory annotation agreement val-
ues (more details may be found in Cardoso et al, 
2011; Cardoso et al, 2012). Specifically for topic 
boundaries, groups of trained annotators indicat-
ed possible boundaries and the ones indicated by 
the majority of the annotators were assumed to 
be actual boundaries. 
5 Evaluation 
This section presents comparisons of the results 
of the algorithms over the reference corpus. 
The performance of topic segmentation is usu-
ally measured using Recall (R), Precision (P), 
and F-measure (F) scores. These scores quantify 
how closely the system subtopics correspond to 
the ones produced by humans. Those measures 
compare the boundary correspondences without 
considering whether these are close to each oth-
er: if they are not the same (regardless of wheth-
                                                 
2 www2.icmc.usp.br/~taspardo/sucinto/cstnews.html 
94
er they are closer or farther from one another), 
they score zero. However, it is also important to 
know how close the identified boundaries are to 
the expected ones, since this may help to deter-
mine how serious the errors made by the algo-
rithms are. We propose a simple measure to this, 
which we call Deviation (D) from the reference 
annotations. Considering two algorithms that 
propose the same amount of boundaries for a text 
and make one single mistake each (having, there-
fore, the same P, R, and F scores), the best one 
will be the one that deviates the least from the 
reference. The best algorithm should be the one 
with the best balance among P, R, F, and D 
scores.  
The results achieved for the investigated 
methods are reported in Table 1. The first 4 rows 
show the results for the baselines. The algorithms 
based on RST are in the last 6 rows. The last row 
represents the human performance, which we 
refer by topline. It is interesting to have a topline 
because it possibly indicates the limits that au-
tomatic methods may achieve in the task. To find 
the topline, a human annotator of the corpus was 
randomly selected for each text and his annota-
tion was compared with the reference one. 
As expected, the paragraph baseline was very 
good, having the best F values of the baseline 
set. This shows that, in most of the texts, the sub-
topics are organized in paragraphs. Although the 
sentence baseline has the best R, it has the worst 
D. This is due to the fact that not every sentence 
is a subtopic, and to segment all of them be-
comes a problem when we are looking for major 
groups of subtopics. TextTiling is the algorithm 
that deviates the least from the reference seg-
mentation. This happens because it is very con-
servative and detects only a few segments, some-
times only one (the end of the text), causing it to 
have a good deviation score, but penalizing R. 
 
Algorithm R P F D 
TextTiling 0.405 0.773 0.497 0.042 
Paragraph 0.989 0.471 0.613 0.453 
Sentence 1.000 0.270 0.415 1.000 
Randomly 0.674 0.340 0.416 0.539 
Simple Cosine 0.549 0.271 0.345 0.545 
Cosine Nuclei 0.631 0.290 0.379 0.556 
Cosine Depth 0.873 0.364 0.489 0.577 
Nuclei Depth 0.899 0.370 0.495 0.586 
Relation_Depth 0.901 0.507 0.616 0.335 
Nuclei_Depth 
Relation 
0.908 0.353 0.484 0.626 
Topline 0.807 0.799 0.767 0.304 
Table 1. Evaluation of algorithms 
In the case of the algorithms based on RST, we 
may notice that they produced the best results in 
terms of R, P, and F, with acceptable D values. 
We note too that every time the salient units 
were used, R and P increase, except for Nu-
clei_Depth_Relation. Examining the measures, 
we notice that the best algorithm was Rela-
tion_Depth. Although its F is close to the one of 
the Paragraph baseline, the Relation_Depth algo-
rithm shows a much better D value. One may see 
that the traditional TextTiling was also outper-
formed by Relation_Depth.  
As expected, the Topline (the human, there-
fore) has the best F with acceptable D. Its F val-
ue is probably the best that an automatic method 
may expect to achieve. It is 25% better than our 
best method (Relation_Depth). There is, there-
fore, room for improvements, possibly using oth-
er discourse features. 
We have run t-tests for pairs of algorithms for 
which we wanted to check the statistical differ-
ence. As expected, the F difference is not signifi-
cant for Relation_Depth and the Paragraph algo-
rithms, but it was significant with 95% confi-
dence for the comparison of Relation_Depth with 
Nuclei_Depth and TextTiling (also regarding the 
F values). Finally, the difference between Rela-
tion_Depth and the Topline was also significant. 
6 Conclusions and future work 
In this paper we show that discourse structures 
mirror, in some level, the topic boundaries in the 
text. Our results demonstrate that discourse 
knowledge may significantly help to find bound-
aries in a text. In particular, the relation type and 
the level of the discourse structure in which the 
relation happens are important features. To the 
best of our knowledge, this is the first attempt to 
correlate RST structures with topic boundaries, 
which we believe is an important theoretical ad-
vance. 
At this stage, we opted for a manually anno-
tated corpus, because we believe an automatic 
RST analysis would surely decrease the corre-
spondence that was found. However, better dis-
course parsers have arisen and this may not be a 
problem anymore in the future. 
Acknowledgments 
The authors are grateful to FAPESP, CAPES, 
CNPq and Natural Sciences and Engineering Re-
search Council of Canada (Discovery Grant 
261104-2008) for supporting this work. 
 
95
References  
Paula C.F. Cardoso, Erick G. Maziero, Maria L.R. 
Castro Jorge, Eloize M.R. Seno, Ariani Di Fellipo, 
L?cia H.M. Rino, Maria G.V. Nunes, Thiago A.S. 
Pardo. 2011. CSTNews ? A discourse-annotated 
corpus for single and multidocument summariza-
tion of texts in Brazilian Portuguese. In: Proceed-
ings of the 3rd RST Brazilian Meeting, pp. 88-105. 
Paula C.F. Cardoso, Maite Taboada, Thiago A.S. Par-
do. 2013. Subtopics annotation in a corpus of news 
texts: steps towards automatic subtopic segmenta-
tion. In: Proceedings of the Brazilian Symposium 
in Information and Human Language Technology. 
T-H Chang and C-H Lee. 2003. Topic segmentation 
for short texts. In: Proceedings of the 17th Pacific 
Asia Conference Language, pp. 159-165. 
Marti Hearst. 1997. TextTiling: Segmenting Text into 
Multi-Paragraph Subtopic Passages. Computation-
al Linguistics 23(1), pp. 33-64. 
Leonhard Hennig. 2009. Topic-based multi-document 
summarization with probabilistic latent semantic 
analysis. In: Recent Advances in Natural Language 
Processing, pp. 144-149. 
Eduard Hovy and C-Y Lin. 1998. Automated Text 
Summarization and the SUMMARIST system. In: 
Proceedings of TIPSTER, pp. 197-214. 
Eduard Hovy. 2009. Text Summarization. In: Ruslan 
Mitkov. The Oxford Handbook of Computational 
Linguistics, pp. 583-598. United States: Oxford 
University. 
Anna Kazantseva and Stan Szpakowicz. 2012. Topi-
cal Segmentation: a study of human performance 
and a new measure of quality. In:  Proceedings of 
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pp. 211-220. 
Willian C. Mann and Sandra A. Thompson. 1987. 
Rhetorical Structure Theory: A Theory of Text Or-
ganization. Technical Report ISI/RS-87-190. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT 
Press. Cambridge, Massachusetts. 
Hyo-Jung Oh, Sung Hyon Myaeng and Myung-Gil 
Jang. 2007. Semantic passage on sentence topics 
for question answering. Information Sciences 
177(18), pp. 3696-3717. 
Rebecca J. Passonneau and Diane J. Litman. 1997. 
Discourse segmentation by human and automated 
means. Computational Linguistics 23(1), pp. 103-
109. 
Violaine Prince and Alexandre Labadi?. 2007. Text 
segmentation based on document understanding for 
information retrieval. In: Proceedings of the 12th 
International Conference on Applications of Natu-
ral Language to Information Systems, pp. 295-304. 
Maite Taboada and William C. Mann. 2006. Rhetori-
cal Structure Theory: Looking back and moving 
ahead. Discourse Studies 8(3), pp.423-459. 
Xiaojun Wan. 2008. An exploration of document im-
pact on graph-based multi-document summariza-
tion. In: Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pp. 
755-762. 
 
96
Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 22?28,
Gothenburg, Sweden, April 26 2014. c?2014 Association for Computational Linguistics
Some issues on the normalization of a corpus of products reviews in 
Portuguese 
 
Magali S. Duran 
NILC-ICMC 
University of S?o Paulo 
Brazil 
 magali.duran@gmail.com 
Lucas V. Avan?o 
NILC-ICMC 
University of S?o Paulo 
Brazil  
avanco89@gmail.com 
Sandra M. Alu?sio 
NILC-ICMC 
University of S?o Paulo 
Brazil 
sandra@icmc.usp.br 
 
Thiago A. S. Pardo 
NILC-ICMC 
University of S?o Paulo 
Brazil 
taspardo@icmc.usp.br 
Maria G. V. Nunes 
NILC-ICMC 
University of S?o Paulo 
Brazil 
gracan@icmc.usp.br 
 
Abstract 
This paper describes the analysis of different 
kinds of noises in a corpus of products 
reviews in Brazilian Portuguese. Case 
folding, punctuation, spelling and the use of 
internet slang are the major kinds of noise we 
face. After noting the effect of these noises 
on the POS tagging task, we propose some 
procedures to minimize them. 
1. Introduction 
 
Corpus normalization has become a common 
challenge for everyone interested in processing a 
web corpus. Some normalization tasks are 
language and genre independent, like boilerplate 
removal and deduplication of texts. Others, like 
orthographic errors correction and internet slang 
handling, are not.  
Two approaches to web corpus normalization 
have been discussed in Web as a Corpus (WAC)  
literature. One of them is to tackle the task as a 
translation problem, being the web texts the 
source language and the normalized texts the 
target language (Aw et al., 2006; Contractor et 
al., 2010; Schlippe et al., 2013). Such approach 
requires a parallel corpus of original and 
normalized texts of reasonable size for training a 
system with acceptable accuracy. The other 
approach is to tackle the problem as a number of 
sub problems to be solved in sequence 
(Ringlstetter et al., 2006; Bildhauer & Sch?fer, 
2013; Sch?fer et al., 2013). 
The discussion we engage herein adopts the 
second approach and is motivated by the  
demand of preprocessing a Brazilian Portuguese 
web corpus constituted of products reviews for 
the specific purpose of building an opinion 
mining classifier and summarizer. Our project 
also includes the task of adding a layer of 
semantic role labeling to the corpus. The roles 
will be assigned to nodes of the syntactic trees 
and, therefore, SRL subsumes the existence of 
layers of morphosyntactic and syntactic 
annotations. The annotated corpus will be used 
as training corpus for a SRL classifier. The aim 
of SRL classifier, on its turn, is to provide deep 
semantic information that may be used as 
features by the opinion miner. If the text is not 
normalized, the POS tagger does not perform 
well and compromise the parsing result, which, 
as consequence, may generate defective trees, 
compromising the assignment of role labels to 
their nodes. 
In fact, mining opinions from a web corpus is 
a non-trivial NLP task which often requires some 
language processing, such as POS tagging and 
parsing. Most of taggers and parsers are made to 
handle error-free texts; therefore they may 
jeopardize the application results when they face 
major noises. What constitutes a major noise and 
which noise may be removed or corrected in 
such a corpus is the challenge we are facing in 
this project. 
22
 2. Related Work 
 
Depending on the point of view, there are 
several studies that face problems similar to 
those faced by us. The general issue is: how to 
convert a non-standard text into a standard one? 
By non-standard text we mean a text produced 
by people that have low literacy level or by 
foreign language learners or by speech-to-text 
converters, machine translators or even by  
digitization process. Also included in this class 
are the texts produced in special and informal 
environments such as the web. Each one of these 
non-standard texts has its own characteristics. 
They may differ in what concerns spelling, non-
canonical use of case, hyphen, apostrophe, 
punctuation, etc. Such characteristics are seen as 
?noise? by NLP tools trained in well written texts 
that represent what is commonly known as 
standard language. Furthermore, with the 
widespread use of web as corpus, other types of 
noise need to be eliminated, as for example 
duplication of texts and boilerplates.  
The procedures that aim to adapt texts to 
render them more similar to standard texts are 
called normalization. Some normalization 
procedures like deduplication and boilerplate 
removal are less likely to cause destruction of 
relevant material. The problem arises when the 
noise category contains some forms that are 
ambiguous to other forms of the standard 
language. For example, the words ?Oi? and 
?Claro? are the names of two Brazilian mobile 
network operators, but they are also common 
words (?oi? = hi; ?claro? = clear). Cases like 
these led Lita et al. (2003) to consider case 
normalization as a problem of word sense 
disambiguation. Proper nouns which are derived 
from common nouns (hence, distinguished only 
by case) are one of the challenges for case 
normalization reported by Manning et al. (2008). 
Similar problem is reported by Bildhauer and 
Sch?fer (2013) regarding dehyphenation, that is, 
the removal of hyphens used in typeset texts and 
commonly found in digitized texts. In German, 
there are many hyphenated words and the 
challenge is to remove noisy hyphens without 
affecting the correct ones. There are situations, 
however, in which both the corrected and the 
original text are desired. For example, social 
media corpora are plain of noises that express 
emotions, a rich material for sentiment analysis. 
For these cases, the non-destructive strategy 
proposed by Bildhauer and Sch?fer (2013), 
keeping the corrected form as an additional 
annotation layer, may be the best solution.  
 
3. Corpus of Products Reviews 
 
To build the corpus of products reviews, we 
have crawled  a products reviews database of one 
of the most traditional online services in Brazil, 
called Buscap?, where customers post their 
comments about several products. The comments 
are written in a free format within a template 
with three sections: Pros, Cons, and Opinion. We 
gathered 85,910 reviews, totaling 4,088,718 
tokens and 90,513 types. After removing stop 
words, numbers and punctuation, the frequency 
list totaled 63,917 types. 
Customers have different levels of literacy 
and some reviews are very well written whereas 
others present several types of errors. In addition, 
some reviewers adopt a standard language style, 
whereas others incorporate features that are 
typical of the internet informality, like abusive 
use of abbreviations, missing or inadequate 
punctuation; a high percentage of named entities 
(many of which are misspelled); a high 
percentage of foreign words; the use of internet 
slang; non-conventional use of uppercase; 
spelling errors and missing of diacritic signals. 
A previous work (Hartmann et al. 2014) 
investigated the nature and the distribution of the 
34,774 words of the corpus Buscap? not 
recognized by Unitex, a Brazilian Portuguese 
lexicon (Muniz et. al. 2005). The words for 
which only the diacritic signals were missing 
(3,652 or 10.2%) have been automatically 
corrected. Then, all the remaining words with 
more than 2 occurrences (5775) were classified 
in a double-blind annotation task, which obtained 
0,752 of inter-annotator agreement (Kappa 
statistics, Carletta, 1996). The results obtained 
are shown in Table 1.  
 
Table 1. Non-Recognized Words with more 
than 2 occurrences in the corpus 
Common Portuguese misspelled words 44% 
Acronyms 5% 
Proper Nouns 24% 
Abbreviations 2% 
Internet Slang 4% 
Foreign words used in Portuguese 8% 
Units of Measurement 0% 
Other problems  13% 
Total 100% 
23
The study reported herein aims to investigate 
how some of these problems occur in the corpus 
and to what extent they may affect POS tagging. 
Future improvements remain to be done in the 
specific tools that individually tackle these 
problems.  
 
4. Methodology  
 
As the same corpus is to be used for different 
subtasks ? semantic role labeling, opinion 
detection, classification and summarization ? the 
challenge is to normalize the corpus but also 
keep some original occurrences that may be 
relevant for such tasks. Maintaining two or more 
versions of the corpus is also being considered. 
To enable a semi-automatic qualitative and 
quantitative investigation, a random 10-reviews 
sample (1226 tokens) of the original corpus was 
selected and POS tagged by the MXPOST tagger 
which was trained on MAC-Morpho, a 1.2 
million tokens corpus of Brazilian Portuguese 
newspaper articles (Alu?sio et al., 2003).  
It is worthwhile to say that the sampling did 
not follow statistical principles. In fact, we 
randomly selected 10 texts (1226 tokens from a 
corpus of 4,088,718 tokens), which we 
considered a reasonable portion of text to 
undertake the manual tasks required by the first 
diagnosis experiments. Our aim was to explore 
tendencies and not to have a precise statistical 
description of the percentage of types of errors in 
the corpus. Therefore, the probabilities of each 
type of error may not reflect those of the entire 
corpus.  
We manually corrected the POS tagged 
version to evaluate how many tags were 
correctly assigned. The precision of MXPOST in 
our sample is 88.74%, while its better precision, 
of 96.98%, has been obtained in its training 
corpus. As one may see, there was a decrease of 
8.49% in performance, which is expected in such 
change of text genre. 
In the sequence, we created four manually 
corrected versions of the sample, regarding each 
of the following normalization categories: 
spelling (including foreign words and named 
entities); case use; punctuation; and use of 
internet slang. This step produced four golden 
corpus samples which were used for separate 
evaluations. The calculation of the difference 
between the original corpus sample and each of 
the golden ones led us to the following 
conclusions.  
The manual corrections of the sample were 
made by a linguist who followed some rules  
established in accordance with the project goals 
and the MXPOST annotation guidelines1. As a 
result, only the punctuation correction allowed 
some subjective decisions; the other kinds of 
correction were very objective. 
 
5. Results of diagnosing experiments 
 
Regarding to spelling, 2 foreign words, 3 
named entities and 19 common words were 
detected as misspelled. A total of 24 (1.96%) 
words have been corrected. There are 35 words 
(2.90%) for which the case have been changed (6 
upper to lower and 29 in the reverse direction). 
Punctuation has showed to be a relevant 
issue: 48 interventions (deletions, insertions or 
substitutions) have been made to turn the texts 
correct, representing 3.92% of the sample. 
Regarding internet slang, only 3 occurrences 
(0.24%) were detected in the sample, what 
contradicted our expectation that such lexicon 
would have a huge impact in our corpus. 
However due to the size of our sample, this may 
have occurred by chance.  
The precision of the POS tagged sample has 
been compared with the ones of the POS tagged 
versions of golden samples. The results showed 
us the impact of the above four normalization 
categories on the tagger performance.  
We have verified that there was improvement 
after the correction of each category, reducing 
the POS tagger errors as shown in Table 2. When 
we combine all the categories of correction 
before tagging the sample, the cumulative result 
is an error reduction of 19.56%.  
 
Table 2. Improvement of the tagger precision 
in the sample 
Case Correction + 15.94% 
Punctuation Correction + 4.34% 
Spelling + 2.90% 
Internet Slang Convertion + 1.45% 
Cumulative Error Reduction 19.56% 
 
These first experiments revealed that case 
correction has major relevance in the process of 
normalizing our corpus of products reviews. It is 
important to note that case information is largely 
                                                          
1
 Available at 
http://www.nilc.icmc.usp.br/lacioweb/manuais.htm 
24
used as feature by Named Entities Recognizers 
(NER), POS taggers and parsers. 
To evaluate whether the case use distribution 
is different from that of a corpus of well written 
texts, we compared the statistics of case use in 
our corpus with those of a newspaper corpus 
(http://www.linguateca.pt/CETENFolha/), as 
shown in Table 3. 
 
Table 3. Percentage of case use in newspaper 
and products reviews corpus genres 
CORPUS Newspaper 
 
Products 
Reviews 
Uppercase words 6.41% 5.30% 
Initial uppercase 
words 
20.86% 7.30% 
Lowercase words 70.79% 85.37% 
 
The differences observed led us to conclude 
that the tendency observed in our sample (proper 
names and acronyms written in lower case) is 
probably a problem for the whole corpus.  
To confirm such conclusion, we searched in 
the corpus the 1,339 proper nouns identified in 
our previous annotation task. They occurred 
40,009 times with the case distribution shown in 
Table 4. 
 
Table 4. Case distribution of Proper Nouns 
Initial uppercase words 15,148 38% 
Uppercase words 7,392 18% 
Lower case words 17,469 44% 
Total 40,009 100% 
 
The main result of these experiments is the 
evidence that the four kind of errors investigated 
do affect POS tagging. In the next section we 
will detail the procedures envisaged to provide 
normalization for each one of the four categories 
of errors.  
 
6. Towards automatic normalization 
procedures 
 
After diagnosing the needs of text 
normalization of our corpus, we started to test 
automatic procedures to meet them. The 
processing of a new genre always poses a 
question: should we normalize the new genre to 
make it similar to the input expected by available 
automatic tools or should we adapt the existing 
tools to process the new genre? This is not a 
question of choice, indeed. We argue that both 
movements are needed. Furthermore, the 
processing of a new genre is an opportunity not 
only to make genre-adaptation, but also to 
improve general purpose features of NLP tools. 
 
6.1 Case normalization: truecasing 
 
In NLP the problem of case normalization is 
usually called ?truecasing? (Lita et al, 2003, 
Manning et al., 2008). The challenge is to decide 
when uppercase should be changed into lower 
case and when lower case should be changed into 
upper case. In brief, truecasing is the process of 
correcting case use in badly-cased or non-cased 
text. 
The problem is particularly relevant in two 
scenarios; speech recognition and informal web 
texts. 
We prioritized the case normalization for two 
reasons: first, badly-cased text seems to be a 
generalized problem in the genre of products 
reviews and, second, it is important to make case 
normalization before using a spell checker. This 
is crucial to ?protect? Named Entities from 
spelling corrections because when non-
recognized lowercase words are checked by 
spellers, there is the risk of wrong correction. 
Indeed, the more extensive is the speller lexicon, 
the greater is the risk of miscorrection. 
The genre under inspection presents a 
widespread misuse of case. By one side, lower 
case is used in place of uppercase in the initial 
letter of proper names. On the other side, upper 
case is used to emphasize any kind of word.  
Our first tentative to tackle the problem of 
capitalization was to submit the samples to a 
Named Entity Recognizer. We chose Rembrandt2 
(Cardoso, 2012), a Portuguese NER that 
enhances both lexical knowledge extracted from 
Wikipedia and statistical knowledge.  
The procedure was: 1) to submit the sample 
to Rembrandt; 2) to capitalize the recognized 
entities written in lower case; 3) to change all the 
words capitalized, except the named entities, to 
lower case. Then we tagged the sample with 
MXPOST to evaluate the effect on POS tagging 
accuracy.  
The number of errors of POS tagging 
increased (149) when compared to the one of the 
sample without preprocessing (138). The 
                                                          
2
 The Portuguese named entity recognition is made by 
system Rembrandt (http://xldb.di.fc.ul.pt/Rembrandt/) 
25
explanation for this is that among the words not 
recognized as named entities there were 
capitalized named entities which were lost by 
this strategy. 
Next we tried a new version of this same 
experiment: we only changed into lower case the 
words not recognized as named entities that were 
simultaneously recognized by Unitex. The results 
were slightly better (143 errors) compared to the 
first version of the experiment, but still worse 
than those of the sample without preprocessing.  
Our expectation was to automatically 
capitalize the recognized entities written in lower 
case. In both experiments, however, no word was 
changed from lower to upper case because all the 
entities recognized by the NER were already 
capitalized.  
The sample contains 57 tokens of named 
entities (corresponding to proper nouns and 
acronyms) from which 24 were written in lower 
case. The NER recognized 22 of the 57 or 18 of 
the 38 types of named entities (a performance of 
47.4%). Unfortunately the NER is strongly based 
on the presence of capitalized initial letters and 
was of no aid in the procedure we tested. 
We argue that a finite list of known proper 
nouns and acronyms, although useful for 
improving evaluation figures, is of limited use 
for an application such as an opinion miner. In 
real scenarios this constitutes an open class and 
new entities shall be recognized as well.  
We observed that many of the named entities 
found in the reviews relate to the product being 
reviewed and to the company that produces it. 
Then we realized an advantage of the source 
from which we have crawled the reviews: the 
customers are only allowed to review products 
that have been previously registered in the site 
database. The register of the name of the product 
is kept in our corpus as metadata for each review. 
This situation gave us the opportunity to 
experiment another strategy: to identify named 
entities of each review in its respective metadata 
file. We first gathered all the words annotated as 
Proper Nouns and Acronyms in our previous 
annotation task3. Then we search for the matches. 
The result is promising: from 1,334 proper nouns 
and from 271 acronyms, respectively 676 
                                                          
3
 Confusion matrix of our double annotated data show 
that annotators diverged in what concerns Proper Nouns and 
Acronyms. For our purposes, however, all of them are 
named entities and need to be capitalized, so that this kind 
of disagreement did not affect the use we have made of the 
annotated words. 
(50.67%) and 44 (16.23%) were found in the 
metadata. Adding both types of named entities, 
we have a match of 44.85% (720 of 1605). This 
is pretty good mainly because the named entities 
recognized are precisely the names of products 
for which opinions will be mined. 
However, we still need to solve the 
recognition of the other named entities in order 
to support the truecasing strategies.  
Following Lita et al. (2003) and Beaufays and 
Strope (2013), we are considering using a 
language model. Lita et al. developed a truecaser 
for news articles, a genre more ?stable? than 
products reviews. Beaufays and Strope, on their 
turn, developed a truecaser to tackle texts 
generated from speech recognition. Language 
modeling may be a good approach to our 
problem because many named entities of 
products domain do not sound as Portuguese 
words. For example, they frequently have the 
consonants k, y and w, which are only used in 
proper names in Portuguese. Other approaches to 
truecasing reported in the literature include finite 
state transducers automatically built from 
language models and maximum entropy models 
(Batista et al. 2008). 
 
6.2 Punctuation problems 
 
Many reviews have no punctuation at all. 
This prevents processing the text by most of NLP 
tools which processes sentences. Some 
grammatical rules may be used to correct the use 
of comma, but the problem is more complex in 
what concerns full stop. We are now training a 
machine learning based program with a corpus of 
well written texts by using features related to n-
grams. We aim at building a sentence 
segmentation tool which does not depend on the 
presence of punctuation or case folding, since 
these are major noises in the corpus.  
 
6.3 Spelling correction 
 
The common Portuguese words in the corpus 
which were not recognized by Unitex have been 
spell checked. Manual analysis is being 
undertaken to determine whether the word has 
been accurately corrected or not. Early results 
evidenced opportunity to extend Unitex and to 
improve our spellers with more phonetic rules in 
order to suggest more adequate alternatives. As 
we have already mentioned, product reviewers 
have several levels of literacy and those of lower 
level frequently swap the consonant letters that 
26
conveys the same phonetic value. For example, 
in Portuguese the letters ?s?, ?c?, ?xc? ?ss? and 
??? can have the same sound: /s/. Therefore, it is 
a common mistake to employ one instead of the 
other. These rules shall be incorporated in spell 
checker. In addition, there are many words which 
were correctly spelled, but were not part of 
Unitex or of the speller?s dictionary or both. 
Both lexicons will be extended with the missing 
words. 
In the same way, the foreign words of current use 
in Brazilian Portuguese shall be incorporated in 
the spell checkers in order to improve their 
suggestions of correction. As a matter of fact, 
foreign words are frequently misspelled. For 
example, ?touchscreen? appeared as 10 different 
spelling forms in our corpus with more than 2 
occurrences (?toch escreen?, ?touch screem?, 
?touch sreen?, ?touche?, ?touch scream?, 
?touchscream?, ?touchscreem?, ?touch-screen?, 
?touchsren?, ?touch screen"). 
 
6.4 Internet slang normalization 
 
Internet slang is a class that combines: 1) 
words written in a different way and 
abbreviations of recurrent expressions, for which 
there is an equivalent in the standard language 
(in this case the procedure is to substitute one for 
another); 2) repeated letters and punctuation (e.g. 
!!!!!!!!!!!!, and ameiiiiiiiiiiiiiiiiiiiiiii, in which the 
word "amei" = ?love? is being emphasized), 
which may be normalized by eliminating 
repetitions; and 3) sequences of letters related to 
emotion expression, like emoticons (e.g. ?:)?, 
?:=(?), laughing (e.g. rsrsrsrs, heheheh, 
kkkkkkkk), which for some purposes shall be 
eliminated and for others shall not. The 
procedures relating to internet slang will be 
implemented carefully  to allow the user to 
activate each one of the three procedures 
separately, depending on his/her interest in 
preserving emotion expression or not.  
  
7. Final Remarks 
 
This preliminary investigation about the 
needs of text normalization for the genre of 
products reviews led us to deep understand our 
challenges and to envisage some solutions.  
We have opened some avenues for future 
works and established an agenda for the next 
steps towards corpus normalization.  
 
Acknowledgments  
This research work is being carried on as part of 
an academic agreement between University of 
S?o Paulo and Samsung Eletr?nica da Amaz?nia 
Ltda.  
References  
Alu?sio, S. M.; Pelizzoni, J. M.; Marchi, A. R.; 
Oliveira, L. H.; Manenti, R.; Marquivaf?vel, V. 
(2003). An account of the challenge of tagging a 
reference corpus of Brazilian Portuguese. In: 
Proceedings of PROPOR?2003. Springer Verlag, 
2003, pp. 110-117. 
Aw, A.; Zhang, M.; Xiao, J.; Su, J. (2006).  A Phrase-
based Statistical Model for SMS Text 
Normalization. In: Proceedings of the COLING-
2006 .ACL, Sydney, 2006, pp. 33?40. 
Batista, F.; Caseiro, D. A.;  Mamede, N. J.; Trancoso, 
I. (2008). Recovering Capitalization and 
Punctuation Marks for Automatic Speech 
Recognition: Case Study for the Portuguese 
Broadcast News, Speech Communication, vol. 50, 
n. 10, pages 847-862, doi: 
10.1016/j.specom.2008.05.008, October 2008 
Beaufays, F.; Strope, B. (2013) Language Model 
Capitalization. In:  2013 IEEE International 
Conference on Acoustics, Speech and Signal 
Processing (ICASSP), p. 6749 ? 6752. 
Bildhauer, F.; Sch?fer, R. (2013) Token-level noise in 
large Web corpora and non-destructive 
normalization for linguistic applications. In: 
Proceedings of Corpus Analysis with Noise in the 
Signal (CANS 2013) . 
Cardoso, N. (2012). Rembrandt - a named-entity 
recognition framework. In: Proceedings of the 
Eight International Conference on Language 
Resources and Evaluation (LREC'12). May, 23-25, 
Istanbul, Turkey. 
Carletta, J.: Assessing Agreement on Classification 
Tasks: The Kappa Statistic. Computational 
Linguistics, vol. 22, n. 2, pp. 249--254. (1996) 
Contractor, D.; Tanveer A.; Faruquie; L.; 
Subramaniam, V. (2010). Unsupervised cleansing 
of noisy text. Coling 2010: Poster Volume, pages 
189?196, Beijing, August 2010. 
Hartmann, N. S.; Avan?o. L.; Balage, P. P.; Duran, 
M. S.; Nunes, M. G. V.; Pardo, T.; Alu?sio, S. 
(2014). A Large Opinion Corpus in Portuguese - 
Tackling Out-Of-Vocabulary Words. In: 
Proceedings of the Ninth International Conference 
27
on Language Resources and Evaluation (LREC 
2014). Forthcoming. 
Lita, L., Ittycheriah, A., Roukos, S. & Kambhatla,N. 
(2003), Truecasing, In: Proceedings of the 41st 
Annual Meeting of the Association for 
Computational Linguistics, Japan. 
Manning, C. D., Raghavan, P., & Sch?tze, H. (2008). 
Introduction to information retrieval (Vol. 1). 
Cambridge: Cambridge university press. 
Muniz, M.C.M.; Nunes, M.G.V.; Laporte. E. (2005) 
"UNITEX-PB, a set of flexible language resources 
for Brazilian Portuguese", Proceedings of the 
Workshop on Technology of Information and 
Human Language (TIL), S?o Leopoldo (Brazil): 
Unisinos. 
Ringlstetter, C.; Schulz, K. U. and Mihov, S. (2006). 
Orthographic Errors in Web Pages: Toward 
Cleaner Web Corpora. In: Computational 
Linguistics Volume 32, Number 3, p. 295-340.  
Sch?fer, R.; Barbaresi, A.; Bildhauer, F. (2013) The 
Good, the Bad, and the Hazy: Design Decisions in 
Web Corpus Construction. In:  Proceedings of the 
8th Web as Corpus Workshop (WAC-8). 
Schlippe, T.; Zhu, C.; Gebhardt J.;, Schultz, T.(2013). 
Text Normalization based on Statistical Machine 
Translation and Internet User Support. In: 
Proceedings of The 38th International Conference 
on Acoustics, Speech, and Signal Processing  
(ICASSP-2013) p. 8406 ? 841. 
 
 
28
