Authorship Attribution with Topic Models
Yanir Seroussi?
Monash University
Ingrid Zukerman??
Monash University
Fabian Bohnert?
Monash University
Authorship attribution deals with identifying the authors of anonymous texts. Traditionally,
research in this field has focused on formal texts, such as essays and novels, but recently
more attention has been given to texts generated by on-line users, such as e-mails and blogs.
Authorship attribution of such on-line texts is a more challenging task than traditional author-
ship attribution, because such texts tend to be short, and the number of candidate authors is
often larger than in traditional settings. We address this challenge by using topic models to
obtain author representations. In addition to exploring novel ways of applying two popular topic
models to this task, we test our new model that projects authors and documents to two disjoint
topic spaces. Utilizing our model in authorship attribution yields state-of-the-art performance on
several data sets, containing either formal texts written by a few authors or informal texts gener-
ated by tens to thousands of on-line users. We also present experimental results that demonstrate
the applicability of topical author representations to two other problems: inferring the sentiment
polarity of texts, and predicting the ratings that users would give to items such as movies.
1. Introduction
Authorship attribution has attracted much attention due to its many applications in,
for example, computer forensics, criminal law, military intelligence, and humanities
research (Juola 2006; Stamatatos 2009; Argamon and Juola 2011). The traditional prob-
lem, which is the focus of this article, is to attribute anonymous test texts to one of
a set of known candidate authors, whose training texts are supplied in advance (i.e.,
supervised classification). Whereas most of the early work on authorship attribution
focused on formal texts with only a few candidate authors, researchers have recently
? Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.
E-mail: yanir.seroussi@monash.edu.
?? Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.
E-mail: ingrid.zukerman@monash.edu.
? Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.
E-mail: fabian.bohnert@monash.edu.
Submission received: 30 December 2012; revised submission received: 9 May 2013; accepted for
publication: 23 June 2013.
doi:10.1162/COLI a 00173
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
turned their attention to scenarios involving informal texts and tens to thousands of
authors (Koppel, Schler, and Argamon 2011; Luyckx and Daelemans 2011). In parallel,
topic models have gained popularity as a means of discovering themes in such large
text corpora (Blei 2012). This article explores authorship attribution with topic models,
extending the work presented by Seroussi and colleagues (Seroussi, Zukerman, and
Bohnert 2011; Seroussi, Bohnert, and Zukerman 2012) by reporting additional experi-
mental results and applications of topic-based author representations that go beyond
traditional authorship attribution.
Topic models work by defining a probabilistic representation of the latent structure
of corpora through latent factors called topics, which are commonly associated with
distributions over words (Blei 2012). For example, in the popular Latent Dirichlet
Allocation (LDA) topic model, each document is associated with a distribution over
topics, and each word in the document is generated according to its topic?s distribution
over words (Blei, Ng, and Jordan 2003). The word distributions often correspond to
a human-interpretable notion of topics, but this is not guaranteed, as interpretability
depends on the corpus used for training the model. Indeed, when we ran LDA on a data
set of movie reviews and message board posts, we found that some word distributions
correspond to authorship style as reflected by authors? vocabulary, with netspeak
words such as ?wanna,? ?alot,? and ?haha? assigned to one topic, and words such as
?compelling? and ?beautifully? assigned to a different topic. This finding motivated
our use of LDA for authorship attribution (Seroussi, Zukerman, and Bohnert 2011).
One limitation of LDA is that it does not model authors explicitly. This led us to
use Rosen-Zvi et al.?s (2004) Author-Topic (AT) model to obtain improved authorship
attribution results (Seroussi, Bohnert, and Zukerman 2012). However, AT is also limited
in that it does not model documents. We addressed this limitation through the Disjoint
Author-Document Topic (DADT) model?a topic model that draws on the strengths of
LDA and AT, while addressing their limitations by integrating them into a single model.
Our DADT model extends the model introduced by Seroussi, Bohnert, and Zukerman
(2012), which could only be trained on single-authored texts. In this article, we provide
a detailed account of the extended model. In addition, we offer experimental results for
five data sets, extending the results by Seroussi, Bohnert, and Zukerman (2012), which
were restricted to two data sets of informal texts with many authors. Our experiments
show that DADT yields state-of-the-art performance on these data sets, which contain
either formal texts written by a few authors or informal texts where the number of
candidate authors ranges from 62 to about 20,000.
Although our evaluation is focused on single-authored texts, AT and DADT can
also be used to model authors based on multi-authored texts, such as research papers. To
demonstrate the potential utility of this capability of the models, we present the results
of a preliminary study, where we use AT and DADT to identify anonymous reviewers
based on publicly available information (reviewer lists and the reviewers? publications,
which are often multi-authored). Our results indicate that reviewers may be identified
with moderate accuracy, at least in small conference tracks and workshops. We hope
that these results will help fuel discussions on the issue of reviewer anonymity.
Our finding that topic models yield good authorship attribution performance
indicates that they capture aspects of authorship style, which is known to be indicative
of author characteristics such as demographic information and personality traits
(Argamon et al. 2009). This is in addition to the well-established result that topic models
can be used to represent authors? interests (Rosen-Zvi et al. 2004). An implication of
these results is that topic models may be used to obtain text-based representations of
users in scenarios where user-generated texts are available. We demonstrate this by
270
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
showing how topic models can be utilized to improve the performance of methods we
developed to address the popular tasks of polarity inference and rating prediction.
This article is structured as follows. Section 2 surveys related work. Section 3
discusses LDA, AT, and DADT and the author representations they yield. Section 4
introduces authorship attribution methods, which are evaluated in Section 5. Section 6
presents applications of our topic-based approach, and Section 7 concludes the article.
2. Related Work
Authorship attribution has a long history that predates modern computing. For exam-
ple, Mendenhall (1887) suggested that word length can be used to distinguish works by
different authors. Modern interest in authorship attribution is commonly traced back
to Mosteller and Wallace?s (1964) study on applying Bayesian statistical analysis of
function word frequencies to uncover the authors of the Federalist Papers (Juola 2006;
Koppel, Schler, and Argamon 2009; Stamatatos 2009). The interest in authorship attri-
bution is due to its many applications in areas such as computer forensics, criminal law,
military intelligence, and humanities research. In recent years, authorship attribution
research has been fuelled by advances in natural language processing, text mining,
machine learning, information retrieval, and statistical analysis. This has motivated
the organization of workshops and competitions to facilitate the development and
comparison of authorship attribution methods (Juola 2004; Argamon and Juola 2011).
In this article, we focus on the closed-set attribution task, where training texts by
the candidate authors are supplied in advance, and for each test text, the goal is to
attribute the text to the correct author out of the candidate authors (Argamon and Juola
2011). Related tasks include open-set attribution, where some test texts may not have
been written by any of the candidate authors, and verification, where texts by only one
candidate author are supplied in advance, and the task is to verify whether test texts
were written by the candidate author (Koppel and Schler 2004; Sanderson and Guenter
2006; Koppel, Schler, and Argamon 2011).
Regardless of the task, a challenge currently faced by researchers in the field is
addressing scenarios with many candidate authors and varying amounts of data per
author (Argamon and Juola 2011; Koppel, Schler, and Argamon 2011; Luyckx and
Daelemans 2011). This challenge is illustrated by the corpus chosen for the PAN?11
competition (Argamon and Juola 2011), which contains short e-mails by tens of authors.
Other examples are Koppel, Schler, and Argamon?s (2011) work on a corpus of blog
posts by thousands of authors, and Luyckx and Daelemans?s (2011) study of the effect
of the number of authors and training set size on authorship attribution performance
on data sets of student essays. Our approach to authorship attribution addresses this
challenge by using topic models, which are known to successfully deal with varying
amounts of text (Blei 2012).
We know of only one previous case where topic models were used for authorship
attribution of single-authored texts: Rajkumar et al. (2009) reported preliminary results
on using LDA topic distributions as feature vectors for support vector machines (SVM),
but they did not compare the results obtained with LDA-based SVM to those obtained
with SVM trained on tokens only (we present the results of such a comparison in
Section 5). We know of two related studies that followed the publication of our
initial LDA-based results (Seroussi, Zukerman, and Bohnert 2011): Wong, Dras and
Johnson?s (2011) work on native language identification with LDA, and Pearl and
Steyvers?s (2012) study of authorship verification where some of the features are topic
distributions. Although Wong, Dras and Johnson reported only limited success (perhaps
271
Computational Linguistics Volume 40, Number 2
because an author?s native language may manifest itself in only a few words, or maybe
due to data-set-specific issues), Pearl and Steyvers found that topical representations
helped them achieve state-of-the-art verification accuracy. Pearl and Steyvers?s
findings further strengthen our hypothesis that topic models yield meaningful author
representations. We take this observation one step further by defining our DADT
model, and applying it to several authorship attribution scenarios, where it yields
better performance than LDA-based approaches and methods based on the AT model
(Section 5).
A line of research that has garnered much interest in recent years is the definition of
generic topic models that incorporate metadata labels (Blei 2012). These models can
be divided into two types: upstream models, which use the labels to constrain the
topics, and downstream models, which generate the labels from the topics (Mimno
and McCallum 2008). Generic models have the appealing advantage of obviating the
need to define a new model for each new task (e.g., they may be used to obtain
author representations by defining a metadata label for each author). However, this
advantage may come at the price of increased computational complexity or poorer
performance than that of task-specific models (Mimno and McCallum 2008). As the
focus of our work is on modeling authors, we experimented only with LDA and with
the task-specific topic models discussed in Section 3 (AT and DADT, which model
authors explicitly). The applicability of generic models to authorship attribution is an
open question that would be interesting to investigate in the future. Nonetheless, most
of the generic models surveyed here have properties that make them unsuitable for
our purposes.
Examples of generic upstream models include DiscLDA (Lacoste-Julien, Sha, and
Jordan 2008), Labeled LDA (Ramage et al. 2009), and DMR (Mimno and McCallum
2008). The former two dedicate at least one topic to each metadata label, making them
too computationally expensive to use on data sets with thousands of authors, such
as the Blog and IMDb1M data sets (Section 5.1). In contrast to DiscLDA and Labeled
LDA, DMR uses less topics by sharing them between labels. Mimno and McCallum
(2008) showed that DMR outperformed AT on authorship attribution of multi-authored
documents. Despite this, we decided to use AT, because we found in preliminary exper-
iments that AT performs better than DMR on authorship attribution of single-authored
texts. Such texts are the main focus of this article. Nonetheless, it is worth noting that
Mimno and McCallum?s experiments were performed on a data set of research papers
where stopwords were filtered out. We do not discard stopwords in most experiments,
because they are known to be indicators of authorship (Koppel, Schler, and Argamon
2009).1
A representative example of a generic downstream model is sLDA (Blei and
McAuliffe 2007), which generates labels from each document?s topic assignments via
a generalized linear model. This model was extended by Zhu, Ahmed, and Xing (2009),
who introduced MedLDA, where training is done in a way that maximizes the margin
between labels, which is ?arguably more suitable? for inferring document labels. Zhu
1 Following Salton (1981), we use the term stopwords to denote the most common words?we use his
English stopword list from the SMART information retrieval system (Salton 1971), which is available
from www.lextek.com/manuals/onix/stopwords2.html. Stopword lists typically include a set of
non-content words, which is a superset of the function words in a given language. Although Koppel,
Schler, and Argamon (2009) found that good performance can be obtained by relying only on function
words, they also showed that the data-driven approach of relying on the most common words in a
corpus yields superior performance in most cases. Either way, discarding stopwords is likely to yield
poor results, as we show in Section 5.3.
272
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
and Xing (2010) further extended that work by introducing sCTRF, which combines
sLDA with conditional random fields to accommodate arbitrary types of features. Zhu
and Xing applied these models to the polarity inference task, and found that support
vector regression outperformed sLDA and performed comparably to MedLDA (these
three models used only unigrams), whereas sCTRF yielded the best performance by
incorporating additional feature types (e.g., part-of-speech tags and a lexicon of positive
and negative words). Based on these results, we decided to leave experiments with
downstream models for future work, as it seems unlikely that we would obtain good
results on the authorship attribution task without considering other feature types in
addition to token unigrams (which is beyond the scope of this article).
3. Topic Models and Author Representations
This section introduces notation, provides a discussion of the meaning of the parameters
used by the topic models, and describes the three topic models considered in this
article (LDA, AT, and DADT), focusing on the author representations that they yield.
3.1 Notation and Preliminaries
We denote matrices and stacked vectors in uppercase boldface italics (e.g., M), and
vectors in lowercase boldface italics (e.g., v). The element at the i-th row and j-th column
of a matrix M is denoted mij, and vector elements are denoted in lowercase italics with
a subscript index (e.g., vi). Sets are denoted with calligraphic font (e.g., S). In addition,
Dir and Cat denote the Dirichlet and categorical distributions, respectively.
The values of the parameters that are given as input to the models are either deter-
mined by the corpus (Section 3.1.1) or configured when using the models (Sections 3.1.2
and 3.1.3). Table 1 shows the models? corpus-dependent and configurable parameters
with their lengths and meanings (scalars have length 1). Corpus-dependent parameters
are at the top, configurable document-related parameters are in the middle (for LDA
Table 1
Corpus-dependent and configurable parameters.
Symbol Length Meaning
A 1 Number of authors
D 1 Number of documents
V 1 Vocabulary size
Nd 1 Number of words in document d
T(D) 1 Number of document topics
? (D) T(D) Document topic prior
? (D) V Word in document topic prior
?(D) 1 Document words in document prior
T(A) 1 Number of author topics
? (A) T(A) Author topic prior
? (A) V Word in author topic prior
? A Author in corpus prior
?(A) 1 Author words in document prior
273
Computational Linguistics Volume 40, Number 2
and DADT), and configurable author-related parameters are at the bottom (for AT
and DADT).
3.1.1 Corpus-Dependent Parameters. The following parameters depend on the corpus,
and are thus considered to be observed:
A: Number of authors. We use a ? {1, . . . , A} to denote an author identifier.
D: Number of documents. We use d ? {1, . . . , D} to denote a document identifier.
V: Vocabulary size. We use v ? {1, . . . , V} to denote a unique word identifier.
Nd: Number of words in document d. We use i ? {1, . . . , Nd} to denote a word index
in document d.
A: Document authors. This is a D-dimensional vector of vectors, where the d-th
element ad contains the authors of the d-th document. In cases where the corpus
contains only single-authored texts, we use the scalar ad to denote the author of
the d-th document, since ad is always of unit length.
W : Document words. This is a D-dimensional vector of vectors, where the d-th
element wd contains the words of the d-th document. The vector wd is of length Nd,
and wdi ? {1, . . . , V} is the i-th word in the d-th document.
3.1.2 Number of Topics. We make a distinction between document topics and author topics.
In both cases, ?topics? describe distributions over all the words in the vocabulary. The
difference is that document topics are word distributions that arise from documents,
while author topics are word distributions that characterize the authors. LDA uses only
document topics, whereas AT uses only author topics. DADT, our hybrid model, uses
both document topics and author topics.
All three models take the number of topics as a configurable parameter, denoted
by T(D) for the number of document topics and by T(A) for the number of author
topics. Although the models have other configurable parameters (introduced subse-
quently), we found that the number of topics has the largest impact on model perfor-
mance because it controls the overall model complexity. For example, setting T(D) = 1
in LDA means that all the words in all the documents are drawn from the same
topic (i.e., a single distribution for all the words), whereas setting T(D) = 200 gives LDA
much more freedom to adapt to the corpus, as each word can be drawn from one of
200 distributions.
It is worth noting that techniques for determining the optimal number of topics
have been suggested. For example, Teh et al. (2006) used hierarchical Dirichlet processes
to learn the number of topics while inferring the LDA model. We did not experiment
with such techniques as they tend to complicate model inference, and we found that
using a constant number of topics yields good performance. Nonetheless, we note that
utilizing such techniques may be a worthwhile future research direction, especially to
determine the balance between document topics and author topics for DADT.
3.1.3 Distribution Priors. The following parameters are the priors of the Dirichlet and beta
distributions used by the models. In contrast to the number of topics, which controls
model complexity, the priors allow users of the models to specify their prior knowledge
and beliefs about the data. In addition, the number of topics imposes a rigid constraint
on the inferred model, whereas the effect of the priors on the model is expected to dimin-
ish as the amount of observed data increases (Equations (3), (7), and (11)). Indeed, we
found in our experiments that varying prior values had a small effect on performance
compared to varying the number of topics (Figure 8b, Section 5.3.4).
274
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
The priors are defined as follows (all vector elements and scalars are positive):
? (D): Document topic prior ? a vector of length T(D).
? (D): Prior for words in document topics ? a vector of length V.
? (A): Author topic prior ? a vector of length T(A).
? (A): Prior for words in author topics ? a vector of length V.
?(D): Document words in document prior.
?(A): Author words in document prior.
? : Author in corpus prior ? a vector of length A.
The support of a K-dimensional Dirichlet distribution Dir(?) is the set of K-
dimensional vectors with elements in the range [0, 1] whose sum is 1 (the Dirichlet
distribution is a multivariate generalization of the beta distribution). Hence, each draw
from the Dirichlet distribution can be seen as defining the parameters of a categorical
distribution. This is illustrated by Figure 1, which shows the Dirichlet distribution
density in the three-dimensional case for three different prior vectors ? (the density
is triangular because the drawn vector elements have to sum to 1?each corner of
the triangle corresponds to a dimension of the distribution, denoted 1, 2, and 3 in
Figure 1). When the prior vector is symmetric (i.e., all its elements have the same
value), the density is also symmetric (Figures 1a and 1b). Symmetric priors with element
values that are greater than 1 yield densities that are concentrated in the middle of the
triangle, meaning that categorical vectors with relatively uniform values are likely to
be drawn (Figure 1a). On the other hand, symmetric priors with element values that
are less than 1 yield sparse densities with high values in the corners of the triangle,
meaning that the categorical vectors are likely to have one element whose value is
greater than the other elements (Figure 1b). Finally, when the prior is asymmetric,
vectors that give higher probabilities to the elements with higher prior values are likely
to be drawn (Figure 1c).
The document and author topic priors (? (D) and ? (A), respectively) encode our
beliefs about the document and author topic distributions, respectively. They are often
set to be symmetric, because we have no reason to favor one topic over the other before
we have seen the data (Steyvers and Griffiths 2007). Wallach, Mimno, and McCallum
(2009) argue that using asymmetric priors in LDA is beneficial, and suggest a method
that learns such priors as part of model inference (by placing another prior on the ? (D)
prior). We implemented Wallach, Mimno and McCallum?s method for all the models
we considered, but found that it did not improve authorship attribution accuracy in
preliminary experiments. Thus, in all our experiments we set the elements of ? (D)
and ? (A) to min{0.1, 5/T(D)} and min{0.1, 5/T(A)}, respectively, yielding relatively
sparse topic distributions, since we expect each document and author to be sufficiently
1
2 3
1
2 3
1
2 3
High
Low
(a) ? = (2, 2, 2) (b) ? = (0.5, 0.5, 0.5) (c) ? = (0.9, 0.5, 0.5)
Figure 1
Three-dimensional Dirichlet probability density, given three prior vectors.
275
Computational Linguistics Volume 40, Number 2
represented by only a few topics. This choice follows the recommendations from
LingPipe?s documentation (alias-i.com/lingpipe), which are based on empirical
evidence from several corpora.
The priors for words in document and author topics (? (D) and ? (A), respectively)
encode our beliefs about the word distributions. As for the topic distribution priors,
symmetric priors are often used, with a default value of 0.01 for all the vector ele-
ments (yielding sparse word distributions, as indicated earlier), meaning that each topic
is expected to assign high probabilities to only a few top words (Steyvers and Griffiths
2007). In contrast to the topic distribution priors, Wallach, Mimno, and McCallum (2009)
found in their experiments on LDA that using an asymmetric ? (D) was of no benefit.
This is because using an asymmetric ? (D) means that we encode a prior preference
for a certain word to appear in all topics (e.g., a word represented by corner 1 in
Figure 1c). For the same reason, using a symmetric ? (A) is a sensible choice for AT. In
contrast to LDA and AT, our DADT model distinguishes between document words and
author words, and thus uses both ? (D) and ? (A) as priors. This allows us to encode our
prior knowledge that stopword use is indicative of authorship. Thus, for DADT we set
?
(D)
v = 0.01? e and ?(A)v = 0.01 + e for all v, where v is a stopword (e can be set to zero
to obtain symmetric priors).
DADT?s ?(D) and ?(A) priors encode our prior belief about the balance between
document words and author words in a given document. Document words (drawn
from document topics) are expected to be representative of the documents in the
corpus, whereas author words (drawn from author topics) characterize the authors
in the corpus. For example, if we asked two different authors to write a report about
LDA, both reports are likely to contain content words like Dirichlet, topic, and prior,
but the frequencies of non-content words (i.e., function words and other indicators of
authorship style) are likely to vary across the reports. In this case, the content words
are expected to be allocated to document topics, and the non-content words whose
usage varies across authors would be allocated to author topics. In cases where the
authors write about different issues, DADT may allocate some content words to author
topics (i.e., the meaning of DADT?s topics is expected to be corpus-specific). According
to DADT?s definition (Section 3.4.1), which uses the beta distribution, the prior expected
value of the portion of each document that is composed of author words is
?(A)
?(A) + ?(D) (1)
with a variance of
?(A)?(D)
(
?(A) + ?(D)
)2 (
?(A) + ?(D) + 1
) (2)
In our experiments, we chose values for ?(D) and ?(A) by deciding on the expected value
and variance, and solving these equations for ?(D) and ?(A).
Finally, DADT?s ? prior determines the prior belief about an author having written a
document (without looking at the actual words in the document). This prior is only used
on documents with unobserved authors (i.e., when attributing authors to anonymous
texts). Because we have no reason to favor one author over the other, we use a uniform
prior, setting ?a = 1 for each author a.
276
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
wdi DNd
zdi? D?d
(D)(D)
? T?t (D)
(D) (D)
Figure 2
Latent Dirichlet allocation (LDA).
3.2 LDA
3.2.1 Model Definition. LDA was originally defined by Blei, Ng, and Jordan (2003). Here
we describe Griffiths and Steyvers?s (2004) extended version. The idea behind LDA is
that each document in a corpus is described by a distribution over topics, and each word
in the document is drawn from its topic?s word distribution. Figure 2 presents LDA in
plate notation, where observed variables are in shaded circles, unobserved variables are
in unshaded circles, and each box represents repeated sampling, with the number of
repetitions at the bottom-right corner. Formally, the generative process is: (1) for each
topic t, draw a word distribution? (D)t ? Dir
(
? (D)
)
; (2) for each document d, draw a topic
distribution ? (D)d ? Dir
(
? (D)
)
; and (3) for each word index i in each document d, draw a
topic zdi ? Cat (?
(D)
d ), and the word wdi ? Cat (?
(D)
zdi ).
3.2.2 Model Inference. Topic models are commonly inferred using either collapsed Gibbs
sampling (Griffiths and Steyvers 2004; Rosen-Zvi et al. 2004) or methods based on vari-
ational inference (Blei, Ng, and Jordan 2003). We use collapsed Gibbs sampling to infer
all models due to its efficiency and ease of implementation. This involves repeatedly
sampling from the conditional distribution of the latent parameters, which is obtained
analytically by marginalizing over the topic and word distributions, and using the prop-
erties of conjugate priors. This conditional distribution is given in Equation (3) (Griffiths
and Steyvers 2004; Steyvers and Griffiths 2007):
p
(
zdi = t|W ,Z?di;? (D),? (D)
)
?
?
(D)
t + c
(DT)
dt
?T(D)
t?=1
(
?
(D)
t? + c
(DT)
dt?
)
?
(D)
wdi + c
(DTV)
twdi
?V
v=1
(
?
(D)
v + c(DTV)tv
) (3)
where W is the corpus; Z?di contains all the topic assignments, excluding the assign-
ment for the i-th word of the d-th document; c(DT)dt is the count of topic t in document d;
and c(DTV)twdi is the count of word wdi in document topic t. Here, these counts exclude
the di-th topic assignment (i.e., zdi).
Commonly, several Gibbs sampling chains are run, and several samples are retained
from each chain after a burn-in period, which allows the chain to reach its stationary
distribution. For each sample, the topic distributions and the word distributions are
estimated using their expected values, given the topic assignments Z. These expected
values are given in Equations (4) and (5):
E[?(D)dt |Z] =
?
(D)
t + c
(DT)
dt
?T(D)
t?=1
(
?
(D)
t? + c
(DT)
dt?
) (4)
277
Computational Linguistics Volume 40, Number 2
E[?(D)tv |Z] =
?
(D)
v + c(DTV)tv
?V
v?=1
(
?
(D)
v? + c
(DTV)
tv?
) (5)
where in this case the counts are over the full topic and author assignments. The
two equations take a similar form due to the fact that the Dirichlet distribution is
the conjugate prior of the categorical distribution (Griffiths and Steyvers 2004). Note
that these values cannot be averaged across samples due to the exchangeability of the
topics (Steyvers and Griffiths 2007) (e.g., topic 1 in one sample is not necessarily the
same as topic 1 in another sample).
The examined authorship attribution problem follows a supervised classification
setup, where training texts with known candidate authors are given in advance. Test
texts are classified one by one, and the goal is to attribute each test text to one of the
candidate authors. As the word distributions of the LDA model inferred in the training
phase are unlikely to change much due to the addition of a single test document, in the
classification phase we consider each topic?s word distribution to be observed, setting
it to its expected value according to Equation (5). This yields the following sampling
equation for a given test text w? (w? is a word vector of length N?):
p
(
z?i = t|w?, z??i;? (D),? (D)
)
?
?
(D)
t + c?
(DT)
t
?T(D)
t?=1
(
?
(D)
t? + c?
(DT)
t?
)?
(D)
tw?i
(6)
where z?i is the topic assignment for the i-th word in w?, z??i contains all of w??s topic
assignments except for the i-th assignment, and c?(DT)t is the count of words assigned to
topic t, excluding the i-th assignment.
As done in the training phase, we set the test text?s topic distribution ?? (D) to its
expected value according to Equation (4), where c(DT)dt is replaced with c?
(DT)
t (which now
contains the counts over the full vector of topic assignments z?). Note that because we
assume that the ? (D)t values are observed in the classification phase, the topics are not
exchangeable. This means that we can average the E[?? (D)t |z?] values across test samples
obtained from the same sampling chain.
3.2.3 Author Representations. LDA does not directly model authors, but it can still be used
to obtain valuable information about them. The output of LDA consists of distributions
over topics ? (D)d for each document d. As the number of topics T
(D) is commonly much
smaller than the size of the vocabulary V, these topical representations form a lower-
dimensional representation of the corpus. The LDA-based author representation we
consider in this article is LDA-M (LDA with multiple documents per author), where
each author a is represented as the set of distributions over topics of their documents,
namely, the set {? (D)d |ad = a}, where ad is the author of document d. An alternative
approach is LDA-S (LDA with a single document per author), where each author?s
documents are concatenated into a single document in a preprocessing step, LDA is run
on the concatenated documents, and each author is represented by a single distribution
over topics (the distribution of the concatenated document).
An advantage of LDA-S over LDA-M is that LDA-S yields a much more compact
author representation than LDA-M, especially for authors who wrote many documents.
However, this compactness may come at the price of accuracy, as markers that may
be present only in a few short documents by one author may lose their prominence
278
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
if these documents are concatenated with longer documents. It is worth noting that
concatenating each author?s documents into one document has been named the
profile-based approach in previous authorship attribution studies, in contrast to the
instance-based approach, where each document is considered separately (Stamatatos
2009).
A limitation of these representations is that they apply only to corpora of single-
authored documents, and there is no straightforward way of extending them to consider
multi-authored documents. This limitation is addressed by AT, which we present in
the next section. Note that when analyzing single-authored documents, the author
representations yielded by AT are equivalent to LDA-S?s representations. Therefore, we
do not report results obtained with LDA-S. Nonetheless, practitioners may find it easier
to use LDA-S than AT due to the relative prevalence of LDA implementations (in fact,
our initial modeling approach was LDA-S for exactly this reason).
3.3 AT
3.3.1 Model Definition. AT was introduced by Rosen-Zvi et al. (2004) to model author
interests in corpora of multi-authored texts (e.g., research papers). The main idea
behind AT is that each document is generated from the topic distributions of its
observed authors, rather than from a document-specific topic distribution. Figure 3
presents AT in plate notation. Formally, the generative process is: (1) for each topic t,
draw a word distribution ? (A)t ? Dir
(
? (A)
)
; (2) for each author a, draw a topic
distribution ? (A)a ? Dir
(
? (A)
)
; and (3) for each word index i in each document d, draw
an author xdi uniformly from the document?s set of authors ad, a topic zdi ? Cat
(
?
(A)
xdi
)
,
and the word wdi ? Cat
(
?
(A)
zdi
)
.
3.3.2 Model Inference. As for LDA, we use collapsed Gibbs sampling to infer AT. This
involves repeatedly sampling from Equation (7) (Rosen-Zvi et al. 2004, 2010):
p
(
xdi = a,
zdi = t
?
?
?
?
A,W ,X?di,Z?di;
? (A),? (A)
)
?
?
(A)
t + c
(AT)
at
?T(A)
t?=1
(
?
(A)
t? + c
(AT)
at?
)
?
(A)
wdi + c
(ATV)
twdi
?V
v=1
(
?
(A)
v + c(ATV)tv
) (7)
where X?di and Z?di are all the author and topic assignments, respectively, excluding
the assignment for the i-th word of the d-th document; c(AT)at is the count of topic t
assignments to author a; and c(ATV)twdi is the count of word wdi in author topic t. Here,
all the counts exclude the di-th assignments (i.e., xdi and zdi). We sample xdi and zdi
wdi DNd
zdi
xdiDad
A
T?t (A)
(A)
?a(A)?(A)
?(A)
Figure 3
The Author-Topic (AT) model.
279
Computational Linguistics Volume 40, Number 2
jointly because this yields faster convergence than separate sampling (Rosen-Zvi et al.
2010).
Similarly to LDA, we estimate the topic and word distributions using their expected
values given the author assignments X and the topic assignments Z:
E[?(A)at |X,Z] =
?
(A)
t + c
(AT)
at
?T(A)
t?=1
(
?
(A)
t? + c
(AT)
at?
) (8)
E[?(A)tv |Z] =
?
(A)
v + c(ATV)tv
?V
v?=1
(
?
(A)
v? + c
(ATV)
tv?
) (9)
where in this case the counts are over the full author and topic assignments.
In the classification phase, we do not know the author a? of the test text w? (we
assume that test texts are single-authored). If we did, no sampling would be required to
obtain a??s topic distribution because it is already inferred in the training phase (Equa-
tion (8)). Hence, we assume that a? is a ?new,? previously unknown author, and utilize
Gibbs sampling to infer this author?s topic distribution ?? (A) by repeatedly sampling
from Equation (10) (as for LDA, the word distributions are assumed to be observed and
set to their expected values according to Equation (9)):
p
(
z?i = t|w?, z??i;? (A),? (A)
)
?
?
(A)
t + c?
(AT)
t
?T(A)
t?=1
(
?
(A)
t? + c?
(AT)
t?
)?
(A)
tw?i
(10)
where z?i is the topic assignment for the i-th word in w?, z??i contains all of w??s topic
assignments except for the i-th assignment, and c?(AT)t is the count of topic t assignments
to author a? (excluding the i-th assignment). Similarly to LDA, we then set ?? (A) to its
expected value according to Equation (8), where c(AT)at is replaced with c?
(AT)
t over the full
assignment vector z?.
3.3.3 Author Representations. AT naturally yields author representations in the form of
distributions over topics. That is, each author a is represented as a distribution over
topics ? (A)a . However, AT is limited because all the documents by the same authors
are generated in an identical manner (Section 3.3.1). To address this limitation, Rosen-
Zvi et al. (2010) introduced ?fictitious? authors, adding a unique ?author? to each
document. This allows AT to adapt itself to each document without changing the
model specification. Therefore, we consider the two following variants: (1) AT: ?Pure?
AT, without fictitious authors; and (2) AT-FA: AT, when run with the additional pre-
processing step of adding a fictitious author to each document.
3.4 DADT
3.4.1 Model Definition. Our DADT model can be seen as a combination of LDA and
AT, which is meant to address the weaknesses of both models while retaining their
strengths. The main idea behind DADT is that words are generated from two disjoint
sets of topics: document topics and author topics. Words generated from document
topics follow the same generation process as in LDA, whereas words generated from
280
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
author topics are generated in an AT-like fashion. This approach has the potential ben-
efit of separating ?document? words from ?author? words. That is, words whose use
varies across documents are expected to be found in document topics, whereas words
whose use varies between authors are expected to be assigned to author topics. Figure 4
presents the graphical representation of the model, where the document-dependent
parameters appear on the left-hand side, and the author-dependent parameters appear
on the right-hand side. Formally, the generative process is as follows (we mark each
step as coming from either LDA or AT, or as new in DADT).
Corpus level:
L. For each document topic t, draw a word distribution ? (D)t ? Dir
(
? (D)
)
.
A. For each author topic t, draw a word distribution ? (A)t ? Dir
(
? (A)
)
.
A. For each author a, draw an author topic distribution ? (A)a ? Dir
(
? (A)
)
.
D. Draw an author distribution ? ? Dir
(
?
)
.
Document level. For each document d:
L. Draw d?s document topic distribution ? (D)d ? Dir
(
? (D)
)
.
D. Draw d?s author set ad by repeatedly sampling without replacement from Cat (?).
D. Draw d?s author/document topic ratio pid ? Beta
(
?(A), ?(D)
)
.
Word level. For each word index i ? {1, . . . , Nd}:
D. Draw the author/document topic indicator ydi ? Bernoulli(pid).
L. If ydi = 0, use document topics: draw a topic zdi ? Cat
(
?
(D)
d
)
, and the
word wdi ? Cat
(
?
(D)
zdi
)
.
A. If ydi = 1, use author topics: Draw an author xdi uniformly from ad,
a topic zdi ? Cat
(
?
(A)
xdi
)
, and the word wdi ? Cat
(
?
(A)
zdi
)
.
It is worth noting that drawing the document?s author set can also be modeled as
sampling from Wallenius?s noncentral hypergeometric distribution (Fog 2008) with a
weight vector ? and a parameter vector whose elements are all equal to 1. In this article,
we consider only situations where ad is observed when the model is inferred. When
handling documents with unknown authors in our authorship attribution experiments,
we assume that all anonymous texts are single-authored.
(D) (A)
wdi? T DNd
zdi
?t
? D?d
ydi
A
Dad ?
??d?
(D)
(D)
(D)
(D) (D)
?
?(A)
?(A)T?t (A)
(A)
?a(A)
xdi
Figure 4
The Disjoint Author-Document Topic (DADT) model.
281
Computational Linguistics Volume 40, Number 2
3.4.2 Model Inference. We infer DADT using collapsed Gibbs sampling, as done for LDA
and AT. This involves repeatedly sampling from the following conditional distribution
of the latent parameters:
p
(
xdi = a,
ydi = y, zdi = t
?
?
?
?
A,W ,X?di,Y?di,Z?di;
? (D),? (D), ?(D),? (A),? (A), ?(A)
)
? (11)
?
?????
?????
(
?(D) + c(DD)d
)
?
(D)
t +c
(DT)
dt
?T(D)
t?=1
(
?
(D)
t?
+c(DT)
dt?
)
?
(D)
wdi
+c(DTV)twdi
?V
v=1
(
?
(D)
v +c
(DTV)
tv
) if y = 0
(
?(A) + c(DA)d
)
?
(A)
t +c
(AT)
at
?T(A)
t?=1
(
?
(A)
t?
+c(AT)
at?
)
?
(A)
wdi
+c(ATV)twdi
?V
v=1
(
?
(A)
v +c
(ATV)
tv
) if y = 1
where Y?di contains the topic indicators, excluding the di-th value; and c
(DD)
d and c
(DA)
d
are the counts of words assigned to document or author topics in document d, respec-
tively. The other variables are defined as for LDA and AT. Here, all the counts exclude
the di-th assignments (i.e., xdi, ydi, and zdi).
The building blocks of our DADT model are clearly visible in Equation (11). LDA?s
Equation (3) is contained in the y = 0 case, where the word is drawn from document
topics, and AT?s Equation (7) is contained in the y = 1 case, where the word is drawn
from author topics. However, Equation (11) also demonstrates the main difference
between DADT and its building blocks, as DADT considers both documents and authors
during the inference process by assigning each word to either a document topic or an
author topic, where document topics and author topics come from disjoint sets.
As for LDA and AT, we ran several sampling chains in our experiments, retaining
samples from each chain after a burn-in period (a sample consists of X, Y, and Z).
For each sample, the topic and word distributions are estimated using their expected
values given the latent variable assignments. The expected values for the topic and
word distributions are the same as for LDA and AT, and the expected values for the
author/document ratio and the corpus author distribution are:
E[pid|Y] =
?(A) + c(DA)d
?(D) + ?(A) + Nd
(12)
E[?a|W] =
?a + c(AD)a
?A
a?=1
(
?a? + c(AD)a?
) (13)
where the counts are now over the full assignments X, Y, and Z. As in LDA and AT,
these equations were straightforward to obtain, because the Dirichlet distribution is the
conjugate prior of the categorical distribution and the beta distribution is the conjugate
prior of the Bernoulli distribution. It is worth noting that because we assume that the
documents? authors are observed during model inference, the expected value of each
element of the corpus distribution over authors ?a does not vary across samples, as it
only depends on the prior ?a and on author a?s count of documents in the corpus c
(AD)
a .
In the classification phase, we do not know the author a? of the test text w?. As for
AT, we assume that a? is a previously unknown author. We set the word distributions to
their expected values from the training phase, and infer a??s author topic distribution ?? (A)
282
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
together with the test text?s document topic distribution ?? (D) and author/document
topic ratio p?i by repeatedly sampling from
p
(
y?i = y, z?i = t|w?, y??i, z??i;? (D),? (A),? (D),? (A), ?(D), ?(A)
)
? (14)
?
?????
?????
(
?(D) + c?(DD)
) ?(D)t +c?
(DT)
t
?T(D)
t?=1
(
?
(D)
t?
+c?(DT)
t?
)?
(D)
tw?i
if y = 0
(
?(A) + c?(DA)
) ?(A)t +c?
(AT)
t
?T(A)
t?=1
(
?
(A)
t?
+c?(AT)
t?
)?
(A)
tw?i
if y = 1
where y?i is the topic indicator for the i-th word, y??i contains all of w??s topic indicators
except for the i-th indicator, and c?(DD) and c?(DA) are the counts of words assigned
to document and author topics, respectively, excluding the i-th assignment (the other
variables are defined as in Equations (6) and (10)). The expected values of ?? (D) and ?? (A)
are the same as for LDA and AT, respectively. The expected value of p?i is obtained by
replacing c(DA)d and Nd with c?
(DA) and N? in Equation (12) (where c?(DA) now contains the
counts over the full vector of indicators y?).
3.4.3 Author Representations and Comparison to LDA and AT. DADT can be seen as a gen-
eralization of LDA and AT?setting DADT?s number of author topics T(A) to zero yields
a model that is equivalent to LDA, and setting the number of document topics T(D) to
zero yields a model that is equivalent to AT. An advantage of DADT over LDA and
AT is that both documents and authors are accounted for in the model?s definition, and
are represented via distributions over document and author topics, respectively. Hence,
preprocessing steps such as concatenating each author?s documents or adding fictitious
authors?as done in LDA-S and AT-FA to obtain author and document representations,
respectively?are unnecessary.
Of the LDA and AT variants presented in Sections 3.2.3 and 3.3.3, DADT might
seem most similar to AT-FA. However, there are several key differences between DADT
and AT-FA.
First, in DADT, author topics are disjoint from document topics, with different priors for
each topic set. Thus, the number of author topics T(A) can be different from the number
of document topics T(D), which enables us to vary the number of author and document
topics according to the number of authors and documents in the corpus. For example,
in the judgment data set (Section 5.1.1), which includes only a few authors that wrote
many long documents, we expect that small values of T(A) compared to T(D) would
suffice to get good author representations. By contrast, modeling the 19,320 authors of
the Blog data set (Section 5.1.5) is expected to require many more author topics. On such
large data sets, using more than a few hundred topics may become too computationally
expensive, because adding topics increases model complexity and thus adds to the
runtime of the inference algorithm. Hence, being able to specify the balance between
document and author topics in such cases is beneficial (Section 5.4).
Second, DADT places different priors on the word distributions for author topics and
document topics (? (A) and ? (D), respectively). We know from previous work that stop-
words are strong indicators of authorship (Koppel, Schler, and Argamon 2009). Our
model allows us to encode this prior knowledge by giving elements that correspond
to stopwords in ? (A) higher weights than such elements in ? (D). We found that this
property of DADT has practical benefits, as it improved the accuracy of DADT-based
authorship attribution methods in our experiments (Section 5).
283
Computational Linguistics Volume 40, Number 2
Third, DADT learns the ratio between document words and author words on a per-
document basis, and makes it possible to specify a prior belief of what this ratio
should be. We show that this has practical benefits in our authorship attribution
experiments (Section 5): Specifying a prior belief that on average about 80% of each
document is composed of author words can yield better results than using AT?s fic-
titious author approach that evenly splits each document into author and document
words.
Fourth, DADT defines the process that generates authors. This allows us to consider the
number of texts by each author when performing authorship attribution. In addition,
this enables the use of DADT in a semi-supervised setup by training on documents
with unknown authors?an extension that is left for future work.
4. Authorship Attribution Methods
This section introduces the authorship attribution methods considered in this article. In
Section 4.1, we discuss our baseline method (SVM trained on tokens), and Sections 4.2,
4.3, 4.4, and 4.5 introduce methods based on LDA, AT, AT-FA, and DADT, respectively.
These methods are summarized in Table 2.
We consider two approaches to using topic models in authorship attribution:
dimensionality reduction and probabilistic.
Under the dimensionality reduction approach, the original documents are con-
verted to topic distributions, and the topic distributions are used as input to a classifier.
Generally, this approach makes it possible to use classifiers that are too computationally
expensive to use with a large feature set, e.g., Webb, Boughton and Wang?s (2005) AODE
classifier, whose time complexity is quadratic in the number of features. We use the
reduced document representations as input to SVM, and compare their performance
with the performance obtained with SVM trained directly on tokens (denoted Token
SVM). This allows us to roughly gauge how much information is lost by converting texts
from token representations to topic representations. However, this approach ignores the
probabilistic nature of the underlying topic model, and thus does not fully test the utility
of the author representations yielded by the model?these are better tested by the next
approach.
Table 2
Summary of authorship attribution methods.
Method Description
Token SVM Baseline: SVM trained on token frequencies
LDA-SVM SVM trained on LDA document topic distributions
AT-SVM SVM trained on AT author topic distributions
AT-P Probabilistic attribution with AT
AT-FA-SVM SVM trained on AT-FA author topic distributions (real and fictitious)
AT-FA-P1 Probabilistic attribution with AT-FA (classification without fictitious authors)
AT-FA-P2 Probabilistic attribution with AT-FA (classification with fictitious authors)
DADT-SVM SVM trained on DADT document and author topic distributions
DADT-P Probabilistic attribution with DADT
284
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
In contrast to dimensionality reduction methods, probabilistic methods utilize the
underlying model?s definitions directly to estimate the probability that a given author
wrote a given test text. These methods require the model to be aware of authors, which
means that LDA cannot be used in this case. We expect this approach to outperform
the dimensionality reduction approach because the probabilistic approach considers the
structure of the topic model.
An alternative approach that we considered uses a distance measure (e.g., Hellinger
distance) to find the author whose topic distributions are closest to the distributions
inferred from the test text. We do not describe distance-based methods in this article
because we found that they yield poor results in most cases (Seroussi 2012), probably
because they do not fully consider the underlying structure of the topic model.
4.1 Baseline: Token SVM
Our baseline method is SVM trained on token frequency features (i.e., token counts
divided by the total number of tokens in the document). This method is known to yield
state-of-the-art authorship attribution performance on this feature set; that is, when
comparing methods without any further feature engineering, Token SVM is expected
to yield good performance with minimal tuning (Koppel, Schler, and Argamon 2009).
We use the one-versus-all setup to handle non-binary authorship attribution scenarios.
This setup scales linearly in the number of authors and was shown to be at least as
effective as other multi-class SVM approaches in many cases (Rifkin and Klautau 2004).
It is worth noting that unlike the topic models, the Token SVM baseline is trained
with the goal of maximizing the authorship attribution accuracy, which may give Token
SVM an advantage over topic-based methods. Further, as a discriminative classification
approach, SVM may yield better performance than probabilistic topic-based methods,
which are generative classifiers (Ng and Jordan 2001). However, as demonstrated by
Ng and Jordan?s comparison of discriminative and generative classifiers, this better
performance may only be obtained in the presence of ?enough? training data (just how
much data is ?enough? depends on the data set).
4.2 Methods Based on LDA
4.2.1 Dimensionality Reduction: LDA-SVM. Using LDA for dimensionality reduction is
relatively straightforward?all it entails is converting the training and test texts to
topic distributions as described in Section 3.2.2, and using these topic distributions as
classifier features. Because we use SVM, it is possible to directly compare the results
obtained with the LDA-SVM method to the baseline results obtained by running SVM
trained directly on token frequencies.
This LDA-SVM approach was utilized by Blei, Ng, and Jordan (2003) to demonstrate
the dimensionality reduction capabilities of LDA on the task of classifying articles
according to a set of predefined categories. To the best of our knowledge, only Rajkumar
et al. (2009) have previously applied LDA-SVM to authorship attribution?they pub-
lished preliminary results obtained by running LDA-SVM, but did not compare their
results to a Token SVM baseline. In Section 5, we present the results of more extensive
experiments on the applicability of this approach to authorship attribution.
4.3 Methods Based on AT
4.3.1 Dimensionality Reduction: AT-SVM. We cannot use AT to obtain document topic
distributions, because AT only infers author topic distributions (Section 3.3). Hence,
we train the SVM component on the author topic representations (each document is
285
Computational Linguistics Volume 40, Number 2
converted to its author topic distribution). For each test text, we assume that it was
written by a previously unknown author, infer this author?s topic distribution ??
(A)
(Sec-
tion 3.3.2), and classify this distribution. This may be seen as very radical dimensionality
reduction, because each author?s entire set of training documents is reduced to a single
author topic distribution.
4.3.2 Probabilistic: AT-P. For each author a, AT-P calculates the probability of the test text
words given the AT model inferred from the training texts, under the assumption that
the test text was written by a. It returns the author for whom this probability is the
highest:
arg max
a?{1,...,A}
p
(
w?|a? = a,? (A),? (A)
)
? arg max
a?{1,...,A}
N??
i=1
T(A)?
t=1
?
(A)
at ?
(A)
tw?i
(15)
This method does not require any topic inference in the classification phase, because the
author topic distributions? (A) and topic word distributions ? (A) are already inferred at
training time. It is worth noting that we use the log of this probability for reasons of
numerical stability.
As mentioned at the beginning of this section, we expect AT-P to outperform
AT-SVM because AT-P relies directly on the probabilistic structure of the AT model.
In addition, AT-P has the advantage of not requiring any topic inference in the classifi-
cation phase.
We also performed preliminary experiments with a method that: (1) assumes that
the test text was co-written by all the candidate authors, (2) infers the word-to-author
assignments for the test text, and (3) returns the author that was attributed the most
words. However, we found that this method performs poorly in comparison with other
AT-based approaches in three-way authorship attribution. In addition, this method was
too computationally expensive to run in cases with many authors, as it requires iterating
through all the authors for every test text word in each sampling iteration.
4.4 Methods Based on AT-FA
AT-FA is the same model as AT, but it is run with the preprocessing step of adding
an additional fictitious author to each training document. Hence, different constraints
apply to AT-FA in the classification phase. This is because in this phase, we cannot
conserve AT-FA?s assumption that all the texts are written by a real author together
with a fictitious author, since we do not know who wrote the test text. Hence, if we
were to assume that the real author is a previously unknown author, as done for AT,
we would have no way of telling the previously unknown author from the fictitious
author, because they are both unique to the test text. We consider two possible ways of
addressing this:
1. Assume that the test text was written only by a real, previously unknown,
author (without a fictitious author), and infer this author?s topic distribu-
tion ?? (A) (as in AT).
2. For each training author a, assume that the test text was written by a together with
a fictitious author fa and infer the fictitious author?s topic distribution ??
(A)
fa
. This
results in a set of fictitious author topic distributions, each matching a training
author.
286
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
Although the second alternative may appear more attractive because it does not violate
the fictitious author assumption of AT-FA, we cannot use it with the dimensionality
reduction method (AT-FA-SVM, as described in the following section), as this method
requires inferring the topic distribution of the previously unknown author ?? (A).
4.4.1 Dimensionality Reduction: AT-FA-SVM. AT-FA yields a topic distribution for each
training document (i.e., the topic distribution of the fictitious author associated with
the document), and a topic distribution for each real author (all the distributions are
over the same topic set). We convert each training document to the concatenation of
these two distributions, and use this concatenation as input to the SVM component. In
the classification phase, we assume that the test text was written by a single previously
unknown author, and represent the test text as the concatenation of the inferred topic
distribution ?? (A) to itself.
It is worth noting that our DADT model offers a more elegant solution than con-
catenating the same distribution to itself, because DADT differentiates between author
topics and document topics?a distinction that AT-FA attempts to capture through
fictitious authors. Hence, we expect the DADT-SVM approach, which we define in
Section 4.5, to perform better than AT-FA-SVM. Nonetheless, we also experiment with
AT-FA-SVM for the sake of completeness.
4.4.2 Probabilistic: AT-FA-P. For the probabilistic approach, we consider two variants,
matching the two alternatives outlined earlier.
1. AT-FA-P1. This variant is identical in the classification phase to AT-P?it returns
the author that maximizes the probability of the test text?s words according to
Equation (15), assuming that the test text was not co-written by a fictitious author.
2. AT-FA-P2. This variant performs the following steps for each author a: (1) assume
that the test text was written by a and a fictitious author fa; (2) infer the topic
distribution of the fictitious author ?? (A)fa ; (3) calculate the probability of the test
text words under the assumption that it was written by a and fa, and given the
inferred ?? (A)fa ; and (4) return the author for whom the probability of the test text
words is maximized:
arg max
a?{1,...,A}
p
(
w?|a?, ?? (A)fa ,?
(A),? (A)
)
? arg max
a?{1,...,A}
N??
i=1
T(A)?
t=1
(
?
(A)
at ?
(A)
tw?i
+ ??(A)fat ?
(A)
tw?i
)
(16)
where a? = {a, fa} is the test text?s set of authors.
The problem with this approach is that it is too computationally expensive to
use on data sets with many candidate authors, as it requires running a separate
inference procedure for each author. Nonetheless, in cases where AT-FA-P2 can be
run, we expect it to perform better than AT-FA-P1 because it does not violate the
fictitious author assumption of AT-FA.
4.5 Methods Based on DADT
4.5.1 Dimensionality Reduction: DADT-SVM. DADT yields a document topic distribu-
tion ? (D)d for each document d, and an author topic distribution ?
(A)
a for each author a.
Similarly to AT-FA-SVM, we convert each training document to the concatenation of
these two distributions, and use this concatenation as input to the SVM component.
287
Computational Linguistics Volume 40, Number 2
In contrast to AT-FA, DADT?s document topic distributions are defined over a
topic set that is disjoint from the author topic set. This makes it possible to assume
that the test text was written by a previously unknown author, and obtain the test
text?s document distribution ?? (D) together with the previously unknown author?s topic
distribution ?? (A) (following the procedure described in Section 3.4.2). As in the training
phase, test texts are represented as the concatenation of these two distributions.
We expect DADT-SVM to outperform AT-FA-SVM, because we are able to main-
tain the assumptions of DADT in the classification phase, which we cannot do in
AT-FA-SVM. Further, DADT-SVM should perform better than AT-SVM, because DADT-
SVM accounts for differences between individual documents, whereas AT-SVM repre-
sents each author using a single training instance. Hypothesizing about the expected
performance of DADT-SVM in comparison to LDA-SVM is harder: We expect per-
formance to be corpus-dependent to a certain degree?in data sets where differences
between individual documents are important, LDA-SVM may have an advantage, as all
the words are allocated to document topics. On the other hand, in data sets where the
differences between authors are more important, DADT-SVM may outperform LDA-
SVM because it represents the authors explicitly.
4.5.2 Probabilistic: DADT-P. This method assumes that the test text was written by a
previously unknown author, infers the test text?s document topic distribution ?? (D) and
the author/document topic ratio p?i, and returns the most probable author according to
the following equation:
arg max
a?{1,...,A}
p
(
a? = a|w?, p?i, ?? (D), ? (A)a ,? (D),? (A),?a
)
? (17)
arg max
a?{1,...,A}
?a
N??
i=1
?
?p?i
T(A)?
t=1
?
(A)
at ?
(A)
tw?i
+ (1? p?i)
T(D)?
t=1
??
(D)
t ?
(D)
tw?i
?
?
It is worth noting that in preliminary experiments, we found that an alternative
approach that avoids sampling p?i and ?? (D) by setting p?i = 1 yields poor performance,
probably because it ?forces? all the words to be author words, including words that are
very likely to be document words. In addition, we found that following an approach
where p?i and ?? (D) are sampled separately for each author (similarly to AT-FA-P2) yields
comparable performance to sampling only once by following the previously-unknown
author assumption. However, the former approach is too computationally expensive
to run on data sets with many candidate authors. Hence, we present only the results
obtained with the approach that performs sampling only once.
5. Evaluation
This section presents the results of our evaluation. We first describe the data sets we
used (Section 5.1) and our experimental setup (Section 5.2), followed by the results of
our experiments on the Judgment and PAN?11 data sets (Section 5.3). Then, we present
the results of a more restricted set of experiments on the larger IMDb62, IMDb1M, and
Blog data sets (Section 5.4) and summarize our key findings (Section 5.5).
288
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
5.1 Data Sets
We experimented with five data sets: Judgment, PAN?11, IMDb62, IMDb1M, and
Blog. Judgment, IMDb62, and IMDb1M were collected and introduced by us, and
are freely available for research use (Judgment can be downloaded from www.csse.
monash.edu.au/research/umnl/data, and IMDb62 and IMDb1M are available upon
request). The two other data sets were introduced by other researchers, are publicly
available, and were used to facilitate comparison between our methods and previous
work. Table 3 presents some data set statistics.
5.1.1 Judgment. The Judgment data set contains judgments by three judges who served
on the Australian High Court from 1913 to 1975: Dixon, McTiernan, and Rich (ab-
breviated to D, M, and R, respectively, in Table 3). We created this data set to verify
rumors that Dixon ghost-wrote some of the judgments attributed to McTiernan and
Rich (Seroussi, Smyth, and Zukerman 2011). This data set is an example of a traditional
authorship attribution data set, as it contains only three authors who wrote relatively
long texts in a formal language. In this article, we only use judgments with undisputed
authorship, which were written in periods when only one of the three judges served
on the High Court (Dixon?s 1929?1964 judgments, McTiernan?s 1965?1975 judgments,
and Rich?s 1913?1928 judgments). We removed numbers from the texts to ensure that
dates cannot be used to discriminate between judges. We also removed quotes to ensure
that the classifiers take into account only the actual authors? language use (removal was
done automatically by matching regular expressions for numbers and text in quotation
marks). Because all three judges dealt with various topics, it is likely that successful
methods would have to consider each author?s style, rather than rely solely on content
features in the texts.
As Table 3 shows, the Judgment data set contains the smallest number of authors of
the data sets we considered, but these authors wrote more texts than the average author
in PAN?11, IMDb1M, and Blog. Judgments are also substantially longer than the texts in
all the other data sets, which should make authorship attribution on the Judgment data
set relatively easy.
Table 3
Data set statistics.
Judgment PAN?11 IMDb62 IMDb1M Blog
Authors 3 72 62 22,116 19,320
Texts 1,342 Trn: 9,335 79,550 271,625 678,161
Vld: 1,296
Tst: 1,300
Texts per D: 902 Trn: 129.7 (139.3) 1283.1 12.3 35.1
author M: 253 Vld: 19.9 (19.0) (685.8) (92.1) (105.0)
mean (stddev) R: 187 Tst: 20.3 (18.9)
Tokens per D: 2,858.6 (2,456.9) Trn: 60.8 (109.4) 281.8 124.2 248.4
text M: 1,310.7 (1,248.4) Vld: 65.3 (98.9) (234.8) (166.6) (510.8)
mean (stddev) R: 783.0 (878.5) Tst: 71.0 (115.1)
289
Computational Linguistics Volume 40, Number 2
5.1.2 PAN?11. The PAN?11 data sets were introduced as part of the PAN 2011 compe-
tition (available from pan.webis.de) (Argamon and Juola 2011). These data sets were
extracted from the Enron e-mail corpus (www.cs.cmu.edu/~enron), and were designed
to emulate closed-class and open-class authorship attribution and authorship verifica-
tion scenarios (Section 2). These data sets represent authorship attribution scenarios
that may arise in computer forensics, such as the case noted by Chaski (2005), where an
employee who was terminated for sending a racist e-mail claimed that any person with
access to his computer could have sent the e-mail.
In our experiments, we used the largest PAN?11 data set, with e-mails by 72 authors.
Unlike the other data sets we used, this data set is split into training, validation, and
testing subsets (abbreviated to Trn, Vld, and Tst, respectively, in Table 3). We focused
on the closed-class problem, using the validation and testing sets that contain texts only
by training authors. The only change we made to the original data set was dropping
two training and two validation texts that were automatically generated, which were
detected by length and content. This had a negligible effect on method accuracy, but
made the statistics in Table 3 more representative of the data (e.g., the mean count of
tokens per text is 65.3 in the validation set without the two automatically generated
texts, compared with 338.3 in the full validation set).
Using this data set allows us to test our methods on short and informal texts with
more authors than in traditional authorship attribution. As Table 3 shows, the PAN?11
data set contains the shortest texts of the data sets we considered. This fact, together
with the training/validation/testing structure of the data set, make it possible to run
many experiments on this data set before moving on to larger data sets.
5.1.3 IMDb62. IMDb62 contains 62,000 movie reviews and 17,550 message board posts
by 62 prolific users of the Internet Movie database (IMDb, www.imdb.com). We intro-
duced this data set (Seroussi, Zukerman, and Bohnert 2010) to test our author-aware
polarity inference approach (Section 6.2). Each user wrote 1,000 reviews (sampled from
their full set of reviews), and a variable number of message board posts, which are
mostly movie-related, but may also be about television, music, and other topics. This
data set allows us to test our approach in a setting where all the texts have similar
themes, and the number of authors is relatively small, but is already much larger than
the number of authors considered in traditional authorship attribution settings. Unlike
the other data sets of informal texts, IMDb62 consists only of prolific authors, allowing
us to test our approach in a scenario where training data is plentiful.
5.1.4 IMDb1M. Although the IMDb62 data set is useful for testing our methods on small-
to medium-scale problems, it cannot be seen as an adequate representation of large-
scale problems. This is especially relevant to the task of rating prediction, in which
typical data sets contain thousands of users (Section 6.3). Hence, we created IMDb1M
by randomly generating one million valid IMDb user IDs and downloading the reviews
and message board posts written by these users (Seroussi, Bohnert, and Zukerman
2011). Unfortunately, most of the randomly generated IDs led to users who submit-
ted neither reviews nor posts?we found that about 5% of the entire user population
submitted posts, and less than 3% wrote reviews. After filtering out users who have not
submitted any rated reviews, we were left with 22,116 users. These users, who make up
the IMDb1M data set, submitted 204,809 posts and 66,816 rated reviews.
IMDb1M can be seen as complementary to the IMDb62 data set, as IMDb62 allows
us to test scenarios in which the user population is made up of prolific users, whereas
290
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
IMDb1M contains a more varied sample of the population. However, because we did
not impose a minimum threshold on the number of reviews or posts, the IMDb1M
population is very challenging as it includes many users with few texts (e.g., about
56% of the users in IMDb1M wrote only one text). It is worth noting that three users
appear in both IMDb62 and IMDb1M. In IMDb62 these three users authored 3,000
reviews and 268 posts in total (about 4.8% of the total number of reviews and 1.5% of the
posts), and in IMDb1M they authored 5,695 reviews and 358 posts (about 8.5% of the re-
views and 0.2% of the posts). The difference in the number of reviews is due to the
sampling we performed when we created IMDb62, and the difference in the number
of posts is due to the time difference between the creation of the two data sets.
5.1.5 Blog. The Blog data set is the largest data set we consider, containing 678,161
blog posts by 19,320 authors (available from u.cs.biu.ac.il/~koppel). It was created
by Schler et al. (2006) to learn about the relation between language use and demographic
characteristics, such as age and gender. We use this data set to test how our authorship
attribution methods scale to handle thousands of authors. As blog posts can be about
any topic, this data set is less restricted than the Judgment, PAN?11, and IMDb data sets.
Further, the large number of authors ensures that every topic is likely to interest at least
several authors, meaning that methods that rely only on content are unlikely to perform
as well as methods that also take author style into account.
5.2 Experimental Setup
We used different experimental setups, depending on the data set. PAN?11 experiments
followed the setup of the PAN?11 competition (Argamon and Juola 2011): We trained all
the methods on the given training data set, tuned the parameters according to results
obtained for the given validation data set, and ran the tuned methods on the given
testing data set. For all the other data sets we utilized ten-fold cross validation. In all
cases, we report the overall classification accuracy, that is, the percentage of test texts
correctly attributed to their author. Statistically significant differences are reported when
p < 0.05 according to McNemar?s test (when reporting results in a table, the best result
for each column is in boldface, and several boldface results mean that the differences
between them are not statistically significant).
In our experiments, we used the L2-regularized linear SVM implementation of
LIBLINEAR (Fan et al. 2008), which is well suited for large-scale text classification. We
experimented with cost parameter values from the set {. . . , 10?1, 100, 101, . . .}, until no
accuracy improvement was obtained (starting from 100 = 1 and going in both direc-
tions). We report the results obtained with the value that yielded the highest accuracy,
which gives an optimistic estimate for the performance of the Token SVM baseline.
We used collapsed Gibbs sampling to train all the topic models, running four chains
with a burn-in of 1,000 iterations. In the Judgment, PAN?11, and IMDb62 experiments,
we retained eight samples per chain with a spacing of 100 iterations. In the IMDb1M
and Blog experiments, we retained one sample per chain due to runtime constraints.
Because we cannot average topic distribution estimates obtained from training samples
due to topic exchangeability (Steyvers and Griffiths 2007), we averaged the probabilities
calculated from the retained samples. In the dimensionality reduction experiments, we
used the topic distributions from a single training sample to ensure that the number of
features is substantially reduced (an alternative approach would be to use the concate-
nation of all the samples, but this may result in a large number of features, and em-
ploying this alternative approach did not improve results in preliminary experiments).
291
Computational Linguistics Volume 40, Number 2
For test text sampling, we used a burn-in of 10 iterations and averaged the parameter
estimates over the next 10 iterations in a similar manner to the procedure used by Rosen-
Zvi et al. (2010). We found that these settings yield stable results across different random
seed values.
To enable a fair comparison between the topic-based methods and the Token SVM
baseline, all methods were trained on the same token representations of the texts. In
most experiments, we did not apply any filters and simply used all the tokens as they
appear in the text. In some cases, as indicated throughout this section, we either retained
only stopwords or discarded the stopwords in a preprocessing step that was applied
before running the methods. This allowed us to obtain rough estimates of the effect
of considering only style words, considering only content words, and considering both
style and content. However, we note that this is only a crude way of separating style and
content, because some stopwords may contain content clues, whereas some words that
do not appear in the stopword list may be seen as indicators of personal style, regardless
of content.
We found that the number of topics has a large impact on performance, and
the effect of other configurable parameters is smaller (Section 3.1). Hence, we used
symmetric topic priors, setting all the elements of ? (D) and ? (A) to min{0.1, 5/T(D)}
and min{0.1, 5/T(A)}, respectively. For all models, we set ?w = 0.01 for each word w as
the base measure for the prior of words in topics. Because DADT allows us to encode our
prior knowledge that stopword use is indicative of authorship, we set ?(D)w = 0.01? e
and ?(A)w = 0.01 + e for all w, where w is a stopword. Unless otherwise specified, we
set e = 0.009, based on tuning experiments on Judgment and PAN?11 (Section 5.3). Sim-
ilarly, we set ?(D) = 1.222 and ?(A) = 4.889 for DADT, based on the same experiments.
In addition, we set ?a = 1 for each author a, yielding smoothed estimates for the corpus
distribution of authors ?.
5.3 Experiments on Small Data Sets
In this section, we present the results of our experiments on the Judgment data set,
which contains judgments by three judges, and on the PAN?11 data set, which con-
tains e-mails by 72 authors. Authorship attribution on the PAN?11 data set is more
challenging than on the Judgment data set, because PAN?11 texts are shorter than
judgments, and some of the PAN?11 authors wrote only a few e-mails. We first present
the results obtained with LDA, followed by the results obtained with AT (with and
without fictitious authors), and with our DADT-based methods, which yielded the best
performance. We end this section with experiments that explore the effect of applying
stopword filters to the corpus in a preprocessing step. These experiments demonstrate
that our DADT-based approach models authorship indicators other than content words.
As discussed in Section 5.2, we ran ten-fold cross validation on the Judgment
data set. On PAN?11, we tuned the methods on the validation subset and report the
results obtained on the testing subset with the settings that yielded the best validation
results (i.e., each method was run multiple times on the validation subset and only once
on the testing subset). We present some tuning results together with testing results to
illustrate the behavior of the various methods. It is worth noting that for most methods,
PAN?11 testing results are better than the best validation results. This may be because
on average testing texts are about 10% longer than validation texts (Section 5.1.2).
5.3.1 LDA. Figure 5 presents the results of the LDA experiment, with Judgment results in
Figure 5a, and PAN?11 validation and testing results in Figures 5b and 5c, respectively.
292
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVM Majority LDA-SVM  0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVM Majority LDA-SVM
(a) Judgment data set (b) PAN?11 validation
Method T(D) Accuracy
Majority ? 7.15%
Token SVM ? 53.31%
LDA-SVM 200 31.92%
(c) PAN?11 testing
Figure 5
LDA results (data sets: Judgment and PAN?11).
On the Judgment data set, the best performance obtained by training an SVM classi-
fier on LDA topic distributions (LDA-SVM with 100 topics) was somewhat worse than
that obtained by training directly on tokens (Token SVM), but was still much better than
a majority baseline (the differences between LDA-SVM and both the Token SVM and
majority baselines are statistically significant in all cases). This indicates that although
some authorship indicators are lost when using LDA for dimensionality reduction,
many are retained despite the fact that LDA?s document representations are much more
compact than the raw token representations.
The ranking of methods on PAN?11 is similar to the ranking on the Judgment data
set, though on Judgment the difference between LDA-SVM and Token SVM is much
smaller. The reason for this difference may be that LDA does not consider authors in
the model-building stage. Although this had a relatively small effect on performance
in the three-way judgment attribution scenarios, it appears that accounting for authors
is important in scenarios with many authors. As the rest of this article deals with such
scenarios, we decided not to use LDA for modeling authors in subsequent sections.
5.3.2 AT. Figure 6 presents the results of the AT experiment, with Judgment results in
Figure 6a and PAN?11 validation and testing results in Figures 6b and 6c, respectively.
In contrast to LDA-SVM, AT-SVM was very sensitive to the number of topics. This
is probably because AT-SVM?s dimensionality reduction is more radical than LDA-
SVM?s: In AT-SVM, each document is reduced to the same distribution over author
topics because AT does not model individual documents (Section 4.3). Notably, AT-
SVM?s performance was very poor when 200 and 400 topics were used, possibly because
the more fine-grained topic distributions yielded by using more topics resulted in
sparser author representations (where some topics were allocated only a few words),
which may have caused the SVM component to overfit. This trend is more pronounced
in the Judgment results than in the PAN?11 results: On Judgment, AT-SVM with 200 and
293
Computational Linguistics Volume 40, Number 2
400 topics yielded poorer results than the majority baseline, probably because the effect
of sparsity is larger when considering three authors than when modeling 72 authors.
The probabilistic AT-P method significantly outperformed AT-SVM and the ma-
jority baseline on both data sets. Although AT-P performed comparably to Token
SVM on the PAN?11 data set, it was significantly outperformed by Token SVM on the
Judgment data set. Nonetheless, these results indicate that AT captures many of the
indicators required for authorship attribution. This is despite the fact that AT was not
designed with authorship attribution in mind. Hence, it represents each author with
a single distribution over topics while ignoring differences and similarities between
documents (which may be important for the authorship attribution task). This stands in
contrast to the Token SVM baseline, which attempts to build a document-based model
that is optimized for the classification goal of authorship attribution (Section 4.1).
5.3.3 AT-FA. Figure 7 presents the results of the AT-FA experiment, with Judgment
results in Figure 7a and PAN?11 validation and testing results in Figures 7b and 7c,
respectively.
On both data sets, the highest accuracy yielded by AT-FA-SVM and AT-FA-P1 was
significantly lower than that obtained by the corresponding methods in the AT case
without fictitious authors (AT-SVM and AT-P, respectively). This may seem surprising,
because the only difference between AT and AT-FA is the addition of a fictitious author
for each document, which was shown to improve AT?s ability to predict unseen por-
tions of documents (Rosen-Zvi et al. 2010). However, the reason for AT-FA-SVM and
AT-FA-P1?s poor performance may be that they do not conserve the underlying as-
sumption of fictitious authors in the classification stage, i.e., they do not assume that
the test text was written by a fictitious author together with a previously unseen author
(Section 4.4). This is probably also the reason why the probabilistic AT-FA-P2 signifi-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVMMajority AT-SVMAT-P  0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVMMajority AT-SVMAT-P
(a) Judgment data set (b) PAN?11 validation
Method T(A) Accuracy
Majority ? 7.15%
Token SVM ? 53.31%
AT-SVM 50 39.23%
AT-P 100 53.08%
(c) PAN?11 testing
Figure 6
AT results (data sets: Judgment and PAN?11).
294
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVMMajority AT-FA-SVMAT-FA-P1 AT-FA-P2  0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Acc
ura
cy [
%]
Number of Topics
Token SVMMajority AT-FA-SVMAT-FA-P1
(a) Judgment data set (b) PAN?11 validation
Method T(A) Accuracy
Majority ? 7.15%
Token SVM ? 53.31%
AT-FA-SVM 50 23.15%
AT-FA-P1 400 29.69%
PAN?11 testing
Figure 7
AT-FA results (data set: Judgment and PAN?11).
cantly outperformed AT-FA-P1 by a large margin on the Judgment data set?AT-FA-P2
conserves the fictitious author assumption, whereas AT-FA-P1 ignores it (we did not run
the AT-FA-P2 method on PAN?11 because it requires running a separate sampling chain
for each candidate author and test text, which makes it too computationally expensive
to run in cases with many candidate authors and test texts).
When comparing AT-FA-P2 to the baselines (on Judgment), we see that it was
significantly outperformed by Token SVM for all topic numbers, but yielded signifi-
cantly better performance than the majority baseline. Despite the fact that AT-FA-P2
was outperformed by Token SVM, the margin was not large when enough topics were
used (AT-FA-P2 yielded its best accuracy of 89.60% with 100 topics, in comparison with
Token SVM?s accuracy of 91.15%). This indicates that representing both documents and
authors in the topic model may have advantages in terms of authorship attribution. This
further motivates the use of our DADT model, which considers documents and authors
without requiring the preprocessing step of adding fictitious authors.
5.3.4 DADT. Figure 8a presents the results of the DADT experiment on the Judg-
ment data set, obtained with 10 author topics, 90 document topics, and prior settings
of ?(D) = 1.222, ?(A) = 4.889, and e = 0.009 (other parameter settings are discussed
subsequently). These results are compared to the baselines (majority and Token SVM),
and to the best topic-based result obtained on this data set thus far (by AT-FA-P2 with
100 topics). As we can see, the best DADT-based result was obtained with the proba-
bilistic DADT-P method, which significantly outperformed all the other methods. This
demonstrates the effectiveness of our DADT model in capturing author characteristics
that are relevant to authorship attribution.
Notably, DADT-SVM yielded significantly poorer results than DADT-P. DADT-
SVM?s relatively weak performance may be because its use of document topics
295
Computational Linguistics Volume 40, Number 2
Method Accuracy
Majority 67.21%
Token SVM 91.15%
AT-FA-P2 89.60%
DADT-SVM 85.49%
DADT-P 93.64%
T(D) T(A) ?(D) ?(A) e Accuracy
90 10 1 1 0 93.81%
90 10 1.222 4.889 0 93.49%
90 10 1.222 4.889 0.009 93.64%
50 50 1.222 4.889 0.009 92.88%
10 90 1.222 4.889 0.009 88.62%
(a) Tuned DADT methods (b) DADT-P tuning
Figure 8
DADT results (data set: Judgment).
introduces noise that causes the SVM component to underperform, as DADT?s
document topics are not expected to be indicative of authorship.
The separation of document words from author words that is obtained by using
DADT on the Judgment data set is illustrated by Figure 9, which presents three docu-
ment topics and three author topics in word-cloud form. The top 50 tokens from each
topic are shown, where the size and shade of each token indicates its frequency in the
topic. This anecdotal sample of topics reflects the general trend that we noticed in this
data set, where document topics represent different types of cases, and the top tokens
in author topics do not carry content information and are dominated by stopwords
(a) Document topics (b) Author topics
Figure 9
DADT topic examples.
296
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
and punctuation (LDA and AT topics were similar to DADT?s author topics due to the
prevalence of stopwords and the lack of document?author separation in these models).
This trend is in line with what we expected, because all three judges handled cases
of different types, and thus content words are unlikely to carry enough information to
adequately represent the judges. As discussed in Section 3.1.3, this separation of content
and style is corpus-dependent and is expected to occur only in cases where content is
independent of author identity. Indeed, we did not observe such a clear separation in
our experiments on other data sets.
Our choice of DADT settings reflects the following insights:
r We used 100 topics overall based on the results of the other topic-based methods,
which showed that good results are obtained with this number of overall topics.
We chose the 90/10 document/author topic split because in the case of the Judg-
ment data set, DADT models only three authors who wrote many documents.
r Setting ?(D) = 1.222 and ?(A) = 4.889 encodes our prior belief that the portion of
each document that is composed of author words is 80% on average, with 15%
standard deviation (obtained as described in Section 3.1.3).
r Setting e = 0.009 encodes our prior belief that stopword choice is more likely to
be influenced by the identity of the author than by the content of the documents
(Section 5.2).
Somewhat surprisingly, these settings did not have a large effect on the performance of
the methods in most cases. This is demonstrated by the results presented in Figure 8b,
which were obtained by varying the values of these parameters and running the DADT-
P method. As Figure 8b shows, the results obtained with a setting of ?(D) = ?(A) = 1,
which can be seen as encoding no strong prior belief about the document/author word
balance in each document (it is equivalent to setting a uniform prior on this balance),
were comparable to the results obtained with ?(D) = 1.222 and ?(A) = 4.889. Likewise,
changing e from 0 to 0.009 only had a minor effect on the results. The only setting that
made a relatively large difference is the document/author topic split: Changing it from
90/10 to 10/90 yielded poorer results. However, the 50/50 split yielded close results to
the 90/10 split, which shows that in this case, the document/author topic split setting
is only sensitive to relatively large variations.
It is likely that performing an exhaustive grid search for the optimal parameter
settings for each method would allow us to obtain somewhat improved results. How-
ever, such a search would be computationally expensive, as the model needs to be
retrained and tested for each fold, parameter set, and method. Therefore, we decided
to present the results obtained with the non-optimized settings, which are sufficient to
demonstrate the merits of our DADT approach, as DADT-P outperformed all the other
methods discussed so far.
On PAN?11, we ran the DADT experiments with 100 topics overall, as this number
of topics yielded the best topic-based results of the models and methods whose results
we presented thus far (AT-P with 100 topics yielded the best results of the methods
based on LDA, AT, and AT-FA). Figure 10b shows the results of tuning DADT?s settings
and running DADT-P on the PAN?11 validation set. The PAN?11 tuning experiment
shows a clearer picture in terms of accuracy differences between different parameter
settings than the Judgment experiments. Specifically, when we used uninformed uni-
form priors on the document/author word split (?(D) = ?(A) = 1), and the same word-
in-topic priors for both document and author words (e = 0), the obtained accuracy was
comparable to AT-P?s accuracy. On the other hand, setting ?(D) = 1.222 and ?(A) = 4.889,
which encodes our prior belief that on average 80% (with a standard deviation of 15%)
297
Computational Linguistics Volume 40, Number 2
of each document is composed of author words, significantly improved performance.
Setting e = 0.009 to encode our prior knowledge that stopwords are indicators of au-
thorship yielded an additional improvement. Finally, the last two results in Figure 10b
demonstrate the importance of having enough topics to model the authors: Accuracy
dropped by about 4 percentage points when we used 50 author topics and 50 document
topics, and by about 24 percentage points when we used only 10 author topics and 90
document topics, rather than 90 author topics and 10 document topics. This leads us to
conjecture that it would be beneficial to pursue a future extension that learns the topic
balance automatically, e.g., in a similar manner to Teh et al.?s (2006) method of inferring
the number of topics in LDA.
Figure 10a presents the PAN?11 results obtained with the DADT-based methods,
using the best setting from Figure 10b: 10 document topics, 90 author topics, ?(D) =
1.222, ?(A) = 4.889, and e = 0.009. As Figure 10a shows, DADT-P, which obtained the
best performance of all the methods tested in this section, is the only method that
outperformed Token SVM. This implies that our DADT model is the most suitable of
the models we considered for capturing patterns in the data that are important for
authorship attribution, at least in scenarios that are similar to the PAN?11 case.
DADT-P?s testing result is comparable to the third-best accuracy (out of 17) obtained
in the PAN?11 competition (Argamon and Juola 2011) (competitors were ranked accord-
ing to macro-averaged and micro-averaged precision, recall, and F1; the micro-averaged
measures are all equivalent to the accuracy measure in this case, because each of the test
texts is assigned to a single candidate author). However, to the best of our knowledge,
DADT-P obtained the best accuracy for a fully supervised method that uses only uni-
gram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest ac-
curacy (65.8%), assumed that all the test texts are given to the classifier at the same time
and used this additional information with a semi-supervised method, whereas Kern
et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-
best (59.4%) accuracies, respectively, used various feature types (e.g., features obtained
from parse trees). In addition, preprocessing differences make it hard to compare the
methods on a level playing field. Nonetheless, we note that extending DADT to enable
semi-supervised classification and additional feature types are promising directions for
future work.
5.3.5 Testing the Effect of Stopwords. The results reported up to this point were all obtained
by running the methods on document representations that include all the tokens. As
discussed in Section 5.2, discarding or retaining stopwords provides a crude way of
separating style from content. We ran a set of experiments where we either discarded
Method Accuracy
Majority 7.15%
Token SVM 53.31%
AT-P 53.08%
DADT-SVM 39.69%
DADT-P 59.38%
T(D) T(A) ?(D) ?(A) e Accuracy
10 90 1 1 0 48.53%
10 90 1.222 4.889 0 53.40%
10 90 1.222 4.889 0.009 54.86%
50 50 1.222 4.889 0.009 50.31%
90 10 1.222 4.889 0.009 30.48%
(a) Tuned DADT methods (testing subset) (b) DADT-P tuning (validation subset)
Figure 10
DADT results (data set: PAN?11).
298
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
Table 4
Stopword experiment results (data sets: Judgment and PAN?11).
Method Judgment PAN?11 Testing
All Discard Retain only All Discard Retain only
words stopwords stopwords words stopwords stopwords
Majority 67.21% 67.21% 67.21% 7.15% 7.15% 7.15%
Token SVM 91.15% 86.18% 92.76% 53.31% 46.46% 28.38%
DADT-P 93.64% 89.28% 90.85% 59.38% 54.69% 18.54%
stopwords in a preprocessing step or retained only stopwords, and then ran the Token
SVM baseline and the DADT-P method, which obtained the best performance when all
the tokens were used (DADT was run with the same settings used to obtain the tuned
results from the previous section).
The results of this experiment are presented in Table 4. As the results show, dis-
carding stopwords caused the Token SVM baseline to yield poorer performance than
when all the tokens were used, but retaining only stopwords significantly improved
Token SVM?s performance on Judgment and yielded a substantial drop in performance
on PAN?11. Interestingly, this was not the case with DADT-P, where either discarding or
retaining stopwords caused a statistically significant drop in performance in compari-
son with using all the tokens. The reason why DADT-P?s performance dropped when
only stopwords were used may be that DADT was designed under the assumption that
all the tokens in the corpus are retained. However, we are encouraged by the fact that
DADT-P?s performance drop on Judgment was not very large when only stopwords
were retained, as it indicates that DADT captures stylistic elements in the authors? texts.
Another encouraging result is that DADT-P yielded significantly better perfor-
mance than Token SVM when using feature sets that included all the tokens or all the
tokens without stopwords. DADT-P appears to harness the extra information from non-
stopword tokens more effectively than Token SVM, despite the fact that such tokens
tend to occur less frequently in the texts than stopwords. Further, the vocabulary size of
these two feature sets is larger than that of the stopword-only feature set, which suggests
that DADT-P is more resilient to noise than Token SVM.
It is worth noting that some content-independent information is lost when only
stopwords are retained. For example, the phrase ?in my opinion? appears in texts by all
three authors in the Judgment data set, but is used more frequently by McTiernan (it
occurs in about 82% of his judgments) than by Dixon (69%) or Rich (58%). As the
frequency of this phrase is apparently dependent on author style and independent of
the specific content of a given judgment, it is probably safe to assume that it would
be beneficial to retain the word ?opinion? (this is also evidenced by the dominance of
this word in the third author topic in Figure 9). However, this word does not appear
in our stopword list. This problem is more pronounced in the PAN?11 data set, where
it appears that other words beyond stopwords are also indicative of authorship. For
instance, Tanguy et al. (2011) used the openings and closings of the e-mails in the data
set as separately weighted features. Openings can start with words such as ?hello,? ?hi,?
?hey,? and ?dear,? but only the first two words appear in our stopword list, meaning
that even when only stopwords are retained some stylistic features are lost. These exam-
ples highlight the difficulties in extracting words that are truly content-independent?a
problem that would be especially relevant when trying to adapt an authorship classifier
299
Computational Linguistics Volume 40, Number 2
Table 5
Large-scale experiment results (data sets: IMDb62, IMDb1M, and Blog).
Method IMDb62 IMDb1M Blog (prolific) Blog (full)
Majority 7.37% 3.00% 1.28% 0.62%
Token SVM 92.52% 43.85% 32.96% 24.13%
AT-P 89.62% 40.82% 37.59% 23.03%
DADT-P 91.79% 44.23% 43.65% 28.62%
trained on texts from one domain to texts from a completely different domain (this
problem is beyond the scope of this study). A possible solution is to obtain corpus-
specific stopwords?for example, by extracting a list of frequent words?but this gives
rise to new problems, such as determining a frequency threshold. We decided not to
pursue such a solution because the PAN?11 results show that improved performance
is not guaranteed when only stopwords are retained, even when Token SVM is used.
Hence, in the remainder of this article we use all the words, that is, we neither discard
stopwords nor retain only stopwords.
5.4 Experiments on Large Data Sets
In this section, we report the results of our experiments on the IMDb62, IMDb1M, and
Blog data sets. Both IMDb data sets contain movie reviews and message board posts,
with IMDb62 consisting of texts by 62 prolific authors (with at least 1,000 texts each),
and IMDb1M consisting of texts by 22,116 authors, who are mostly non-prolific. The
Blog data set contains blog posts by 19,320 authors, and is the largest of the data sets
we considered in terms of token count?it contains about 168 million tokens, whereas
IMDb62 and IMDb1M contain about 22 and 34 million tokens, respectively. In addition
to running experiments on the full Blog data set, we considered a subset that contains all
the texts by the 1,000 most prolific authors (this subset contains about 69 million tokens
overall in 332,797 posts?about 49% of the posts in the full Blog data set).
Due to resource constraints, we performed a more restricted set of experiments
on IMDb62, IMDb1M, and Blog than on the Judgment and PAN?11 data sets (which
contain about 3 and 0.74 million tokens, respectively). We ran only the Token SVM
baseline, AT-P, and DADT-P, as these methods yielded the best performance in the
PAN?11 experiments. We set the overall number of topics of AT and DADT to 200 topics
for IMDb62, and 400 topics for IMDb1M and Blog. We set DADT?s document/author
topic split to 50/150 for IMDb62 and 50/350 for IMDb1M and Blog, and used the prior
setting that yielded the best PAN?11 results (?(D) = 1.222, ?(A) = 4.889, and e = 0.009).
As in the PAN?11 experiments, we determined the overall number of topics based on
AT-P?s performance with 25, 50, 100, 200, and 400 topics. The document/author topic
splits we tested were 10/190, 50/150, and 100/100 for IMDb62, and 10/390, 50/350,
and 100/300 for IMDb1M and Blog.
Table 5 shows the results of this set of experiments. As in our previous experiments,
DADT-P consistently outperformed AT-P, which indicates that using disjoint sets of
document and author topics yields author representations that are more suitable for
authorship attribution than using only author topics. In contrast to the previous exper-
iments, Token SVM outperformed DADT-P in one case: the IMDb62 data set. This may
be because discriminative methods (such as Token SVM) tend to outperform generative
300
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
methods (such as DADT-P) in scenarios where training data is abundant (Ng and Jordan
2001), which is the case with IMDb62?it contains at least 900 texts per author in each
training fold.
A notable result is that although all the methods yielded relatively low accuracies
on the full Blog data set, the topic-based methods experienced a larger drop in accuracy
than Token SVM when transitioning from the prolific author subset to the full data
set. This may be because topic-based methods use a single model, making them more
sensitive to the number of authors than Token SVM?s one-versus-all setup that uses
one model per author (this sensitivity may also explain why DADT-P outperformed
Token SVM by a relatively small margin on IMDb1M). This result suggests a direction
for future work in the form of an ensemble of Token SVM and DADT-P. The potential of
this direction is demonstrated by the fact that a perfect oracle, which chooses the correct
answer between Token SVM and DADT-P when they disagree, yields an accuracy of
37.36% on the full Blog data set.
5.5 Summary of Key Findings
In summary, we found that the DADT-based probabilistic approach (DADT-P) yielded
strong performance on the five data sets we considered, outperforming the Token
SVM baseline in four out of the five cases. We showed that DADT-P is more suitable
for authorship attribution than methods based on LDA and AT (with or without fic-
titious authors), and than using DADT for dimensionality reduction. Although our
results demonstrate that separating document words from author words is a good
approach to authorship attribution, relying only on unigrams is a limitation (which
is shared by LDA, AT, and DADT). We discuss ways of addressing this limitation in
Section 7.
DADT?s improved performance in comparison with methods based on LDA and AT
comes at a price of more parameters to tune. However, the most important parameter is
the number of topics?we found that the prior values that yielded good results on the
small data sets also obtained good performance on the large data sets without further
tuning. We offered a simple recipe to determine the number of topics for DADT-P: First
run AT-P to find the overall number of topics (which is equivalent to running DADT-
P without document topics), and then tune the document/author topic balance. As
mentioned in Section 3.1.2, this procedure can be obviated by automatically learning
the topic balance, which is left for future work.
6. Applications
This section presents three applications of topic-based author representations: identi-
fying anonymous reviewers (Section 6.1), author-aware polarity inference (Section 6.2),
and text-aware rating prediction (Section 6.3).
6.1 Reviewer Identification
AT and DADT can potentially be used to identify anonymous reviewers based on pub-
licly available data?the reviewer list (which is commonly available), and the reviewers?
published papers. The main question in this case is whether authorship markers learned
from (often multi-authored) texts in one domain (the papers) can be used to classify
single-authored texts from a related domain (the reviews).
301
Computational Linguistics Volume 40, Number 2
To start answering this question, we considered a small conference track, which
attracted 18 submissions that were each reviewed by two reviewers. We collected the
bodies of 10 papers (without references, author names, acknowledgments, etc.) by each
of the 18 reviewers that were listed in the proceedings, which resulted in a training cor-
pus of 171 documents with 196 authors overall (some of the reviewers have co-authored
papers with other reviewers). We omitted authors with only one paper, because their
presence is equivalent to having fictitious authors, which may hurt performance
(Section 5.3). This resulted in a total of 77 authors. Our test data set consisted of 19
reviews by the 9 reviewers who gave us permission to use their reviews.
We trained AT and DADT on the paper corpus under the setup described in
Section 5.2, and used AT-P and DADT-P to classify the reviews. The best accuracy,
8/19, was obtained by DADT-P with 10 document topics and 90 author topics. The
accuracy of AT-P (with 100 topics) was slightly worse, at 7/19. In addition, the correct
reviewer appeared in the top-five list of probable authors for 15/19 of the reviews with
DADT-P and 11/19 with AT-P (the list of probable authors included all 18 reviewers?
we considered all the reviewers as candidates because this did not require using any
private information and it made our experimental setup more realistic). We obtained
better results by eliminating non-reviewers from the training corpus (thus training on
the 171 documents with 18 authors overall). DADT-P required only 25 document topics
and 25 author topics in this case, and its accuracy rose to 10/19 (AT-P again performed
worse with an accuracy of 7/19). In 16/19 of the cases the correct reviewer appeared in
DADT-P?s top-five list, compared to 12/19 with AT-P.
These results were obtained on a very small data set. Still, they indicate that re-
viewer identification is feasible (note that it is unlikely that DADT-P?s performance
is only due to content words, as interest areas are often shared between reviewers).
To verify this, a fully fledged study should be done on a corpus of reviews from
a large conference, with a training corpus that includes each author?s full body of
publications (perhaps dropping very old publications, which we did not do). As far
as we know, such a study is yet to be performed. The closest work we know of is by
Nanavati et al. (2011), who considered the question of whether ?insiders,? who served
as program committee members and thus had access to non-anonymous reviews, can
use these reviews as training data to identify reviewers. Although they found that they
could identify reviewers with high accuracy, the main limitation of their approach is
that it relies on private data.
Nonetheless, we believe that reviewer anonymity needs to be addressed. One
approach is to use tools that obfuscate author identity, as developed by, for example,
Kacmarcik and Gamon (2006) and Brennan and Greenstadt (2009). However, as this may
lead to an ?arms race? between such tools and authorship analysis methods, perhaps
the best approach is to forgo anonymity completely, as advocated by some researchers
and editors (Groves 2010). This is an open question with no simple answers, but we
hope that our results will help motivate the search for solutions.
6.2 Author-Aware Polarity Inference
Sentiment analysis deals with inferring people?s sentiments and opinions from
texts (Pang and Lee 2008; Liu and Zhang 2012). One of the main tasks in this field is
polarity inference, where the goal is to infer the degree of positive or negative sentiment
of texts (Pang and Lee 2008). Even though the way polarity is expressed often depends
on the author, most of the work in this field ignores authors. We addressed this gap
(Seroussi, Zukerman, and Bohnert 2010; Seroussi 2012) by introducing a framework
302
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
that considers authors when performing polarity inference, by combining the outputs of
author-specific inference models in a manner that makes it possible to consider author
similarity. We showed that our framework outperforms two state-of-the-art baselines
introduced by Pang and Lee (2005): one that ignores authorship information, and
another that considers only the model learned for the author of the text whose polarity
we want to infer. These results support our hypothesis that the way sentiment is ex-
pressed is often author-dependent, and shows that our approach successfully harnesses
this dependency to improve polarity inference performance.
Topic-based representations of authors suggest a way of measuring similarity be-
tween authors based on their texts, which can be used by our polarity inference frame-
work. Such measures are expected to capture authors? interests and aspects of their
authorship style, which is indicative of demographic attributes and personality traits.
We hypothesize that compact representation of authors using topic distributions would
help handle the inherent noisiness of large data sets of user-generated texts without
losing much information, as it did on the authorship attribution task.
To test this hypothesis, we experimented with a simple variant of our polarity
inference framework, which infers the polarity rating of a sentiment-bearing text q
written by author a according to a weighted average2
?
a??Na waa? r?a?q?
a??Na waa?
(18)
where Na is the set of neighbors of author a, waa? is a non-negative similarity weight
assigned to each neighbor a?, and r?a?q is the polarity inferred by the inferrer of a?
for q (each inferrer is a support vector regression model trained on the labeled texts
by a?). The neighborhood Na is obtained for each author a by learning a threshold on
the number of similar authors to consider. This is done by performing five-fold cross
validation on a?s set of labeled texts to find the threshold that minimizes the root mean
squared error (RMSE) out of a set of candidate thresholds.
We compare the results obtained with baselines of equal weights (i.e., an un-
weighted average) and token frequency similarity with those obtained with similarity
measures based on the AT and DADT topic models. The token frequency similarity
measure is the cosine similarity of the frequency vectors of all the tokens in the authors?
vocabularies. The AT and DADT similarity measures are calculated as one minus the
Hellinger distance between the author topic distributions.
We ran this experiment on the IMDb62 data set. To test our approach in a variety
of scenarios, we utilized the GivenX protocol, where each target author has exactly X
training samples (Breese, Heckerman, and Kadie 1998). Specifically, we performed ten-
fold cross validation over authors, where we partitioned the authors into ten folds and
iterated over the folds, using nine folds as the training folds and the remaining fold
as the test fold. The model was trained on all the labeled texts (IMDb62 reviews, each
with a polarity rating assigned by its author) by the authors in the training folds, and
exactly X labeled texts by each target author in the test fold. The model was then tested
on the remaining samples by each target author. This process was repeated five times
2 This is not the strongest variant of those explored by Seroussi, Zukerman, and Bohnert (2010) and
Seroussi (2012), where we found that normalizing the inferences from the neighborhood and considering
a model trained on author a?s texts improves performance. Using a simple weighted average allows us to
compare similarity measures independently of these enhancements.
303
Computational Linguistics Volume 40, Number 2
with different random seeds, and the RMSE was averaged across folds (here and in
the next section we use a paired two-tailed t-test to measure statistical significance, as
polarity inference and rating prediction are regression problems). Note that the GivenX
protocol cannot be used to reliably compare the performance of the same method
across different X values (e.g., testing how the performance of a method varies from
Given1 to Given100), because the test samples vary across X values. Rather, we use this
protocol to compare different methods under the same conditions, e.g., by comparing
the performance of using DADT to that of employing equal weights under the Given10
scenario.
Figure 11 presents the results of this experiment. As the figure shows, the AT and
DADT similarity measures outperformed the baselines and performed comparably to
each other (the differences between either AT or DADT and the baselines are statisti-
cally significant in all cases except for Given5 and Given10 with the token frequency
measure, whereas the differences between DADT and AT are not statistically significant
in all cases). It is worth noting that we did not tune AT?s and DADT?s parameters.
Instead, we used the settings that yielded the best authorship attribution performance
on the IMDb62 data set (Section 5.4). It appears that in this case DADT?s approach of
de-noising the author representations by modeling authors and documents over two
disjoint sets of topics is of little benefit in comparison with AT?s approach of using only
author topics. This may appear to stand in contrast to the results of our authorship
attribution experiments (Section 5), but it could be because the similarity measures do
not require the models? full discriminatory power, which is where DADT?s strengths
lie (Section 3.4.3). Nonetheless, we are encouraged by the fact that using either AT or
DADT yielded better results than both the equal weights and token frequency baselines
in most cases. This is despite the fact that these models operate in a space of lower
dimensionality than the token frequency measure, which demonstrates the strength of
topic-based approaches for author representation.
 1.78
 1.8
 1.82
 1.84
 1.86
 1.88
 1.9
 1.92
 1.94
 1.96
 0  20  40  60  80  100
R
M
SE
 300  500  700  900
Labelled Texts per Target Author
Equal weights
Token frequencies
AT
DADT
Figure 11
Author-aware polarity inference with topic models (data set: IMDb62).
304
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
6.3 Text-Aware Rating Prediction
Recommender systems help users deal with information overload by finding and
recommending items of personal interest (Resnick and Varian 1997). Rating prediction
is a core component of many recommender systems (Herlocker et al. 1999). Recently,
rating prediction algorithms that are based on matrix factorization have become
increasingly popular, due to their high accuracy and scalability (Koren, Bell, and
Volinsky 2009). However, such algorithms often deliver inaccurate rating predictions
for users with few ratings (this is known as the new user problem). We introduced
an extension to the matrix factorization algorithm that considers user attributes
when generating rating predictions (Seroussi, Bohnert, and Zukerman 2011; Seroussi
2012). We showed that using either demographic attributes or text-based attributes
extracted with the LDA-S model, which is equivalent to AT (Section 3.2.3), outperforms
state-of-the-art baselines that consider only ratings, thereby enabling more accurate
generation of personalized rating predictions for new users. In the case of AT, these
predictions are generated without requiring users to explicitly supply any information
about themselves and their preferences.
Our framework predicts the rating user u would give to item i by switching between
our attribute-based model and Koren, Bell, and Volinsky?s (2009) ratings-only model
according to an empirically set threshold n on the size of user u?s known rating setRu:
r?ui =
{
? + b(I)i +
?T
t=1 p(t|u)
(
b(A)t + z>?t y?i
)
|Ru| < n
? + b(U)u + b(I)i + x>?uy?i otherwise
(19)
where ? is the global rating mean; b(U)u , b
(I)
i , and b
(A)
t are the user, item, and attribute
biases, respectively; and x?u, y?i, and z?t denote the u-th, i-th, and t-th columns of the
user, item, and attribute factor matrices X, Y, and Z, respectively. The probability of a
user u having one of the T attributes t is denoted by p(t|u), which in the case of AT
and DADT is the user?s probability of using the author topic t, i.e., ?(A)ut (each topic
model is inferred from the texts written by the user). We infer the biases and factor
matrices using gradient descent in two stages (all the available ratings are used in both
stages): (1) infer the ratings-only part of the model; and (2) infer the attribute-based
part of the model, assuming that the item biases and factor matrix are given (Seroussi,
Bohnert, and Zukerman 2011).
We ran Given0 and Given1 experiments on the IMDb1M data set, where the training
set consisted of message board posts and rated reviews, and calculated the RMSE on the
test ratings (the reviews associated with these ratings were hidden from the models).
The baseline methods were non-personalized prediction (? + b(I)i , which is roughly
equivalent to item i?s rating mean), and the personalized, ratings-only model, which
could only be used in the Given1 case. We set the number of author topics to 75 for
Given0 and 125 for Given1, as this yielded the best results for AT (out of 5, 10, 25, 50,
75, 100, 125, and 150). In DADT?s case, we used additional five document topics, which
yielded the best results (out of 1, 5, 10, and 25). As the attribute-based model is sensitive
only to the number of author topics, this enabled us to perform a fair comparison
between the two models.
The results of this experiment are presented in Table 6. Both AT and DADT out-
performed the baselines, which supports our hypothesis that considering user texts
by using topic-based author representations can yield personalized and accurate rating
305
Computational Linguistics Volume 40, Number 2
Table 6
Text-aware rating prediction with AT and DADT (data set: IMDb1M).
Method Given0 Given1
Non-personalized 2.733 2.691
Personalized (only ratings) ? 2.734
Personalized (AT) 2.719 2.668
Personalized (DADT) 2.719 2.678
predictions, potentially leading to improved recommendations. The reason DADT did
not outperform AT may be that DADT tends to yield user representations that help
discriminate between texts by individual users (as shown in our authorship attribution
experiments), but such representations are not as useful when utilized as attributes, be-
cause the attribute-based model requires a representation that captures commonalities
between users.
7. Conclusion and Future Work
In this article, we extended and added detail to the work of Seroussi, Zukerman, and
Bohnert (2011) and Seroussi, Bohnert, and Zukerman (2012) by reporting additional
experimental results and applications of topic-based author representations that
go beyond traditional authorship attribution. We provided experimental results for
authorship attribution methods that are based on three topic models (LDA, AT, and
DADT) for several scenarios where the number of authors varies from three to about
20,000. Specifically, we showed that in most cases, a probabilistic approach that is
based on our DADT model (DADT-P) yields the best results, outperforming methods
based on LDA and AT, as well as a Token SVM baseline. This indicates that our
topic-based approach successfully captures indicators of authors? style (which is
indicative of author characteristics such as demographic attributes and personality
traits) as reflected by their texts. We harnessed this property when we used AT and
DADT to uncover the authors of anonymous reviews where the training texts are multi-
authored, improve performance when measuring similarity between authors based on
their texts in our polarity inference framework, and obtain compact representations
of users for our rating prediction framework.
The work presented in this article can be extended in many ways. One direction
would be to address the limitation of relying only on unigrams as features by consider-
ing word order. This can possibly be pursued by adding author-awareness to Griffiths
et al.?s (2004) HMM-LDA model, which considers word order by combining LDA with a
Hidden Markov Model. Author awareness can also be introduced into the models sug-
gested by Wallach (2006) and Wang, McCallum, and Wei (2007), who made each word
dependent on both its topic and on the previous word (at a considerable computational
cost). A more general alternative would be to enable the use of various feature types, for
example, by incorporating conditional random fields into DADT in a manner similar to
Zhu and Xing?s (2010) model. This direction can also be pursued by using DADT-P in
an ensemble with SVMs that can be trained on feature types other than token unigrams,
which may also have the added value of combining the strengths of DADT with those
of the SVM approach (Section 5.4). Testing these approaches with character n-grams
would be of particular interest, as they often deliver strong performance, sometimes
outperforming token unigrams (Koppel, Schler, and Argamon 2009).
306
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
Our DADT-P method can be extended to handle situations where the test texts may
have not been written by any of the candidate authors (i.e., open-set attribution and ver-
ification, described in Section 2). A fairly straightforward approach consists of setting
a threshold on the probability assigned to the selected author based on performance
on held-out data?if the probability of the selected author is below the threshold, then
?unknown author? is returned. This approach was successfully used by Tanguy et al.
(2011) in conjunction with a maximum entropy classifier.
Another potential extension would be to automatically infer the optimal number
of author and document topics. This is likely to yield improved results, because the
number of topics had the largest impact on performance among the parameters con-
sidered in our experiments (Section 5.5). In addition, our models can be extended to
address semi-supervised authorship attribution, and may potentially be applied to
any scenario where user-generated texts are available, going beyond the applications
presented in Section 6.
Acknowledgments
This research was supported in part by grant
LP0883416 from the Australian Research
Council. The authors thank Russell Smyth
for the collaboration on initial results on the
Judgment data set, Mark Carman for fruitful
discussions on topic modeling, and the
anonymous reviewers for their insightful
comments.
References
Argamon, Shlomo and Patrick Juola.
2011. Overview of the international
authorship identification competition at
PAN-2011. In CLEF 2011: Proceedings
of the 2011 Conference on Multilingual and
Multimodal Information Access Evaluation
(Lab and Workshop Notebook Papers),
Amsterdam.
Argamon, Shlomo, Moshe Koppel, James W.
Pennebaker, and Jonathan Schler. 2009.
Automatically profiling the author of an
anonymous text. Communications of the
ACM, 52(2):119?123.
Blei, David M. 2012. Probabilistic topic
models. Communications of the ACM,
55(4):77?84.
Blei, David M. and Jon D. McAuliffe. 2007.
Supervised topic models. In NIPS 2007:
Proceedings of the 21st Annual Conference
on Neural Information Processing Systems,
pages 121?128, Vancouver.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1022.
Breese, John S., David Heckerman, and
Carl Kadie. 1998. Empirical analysis of
predictive algorithms for collaborative
filtering. In UAI 1998: Proceedings
of the 14th Conference on Uncertainty
in Artificial Intelligence, pages 43?52,
Madison, WI.
Brennan, Michael and Rachel Greenstadt.
2009. Practical attacks against authorship
recognition techniques. In IAAI 2009:
Proceedings of the 21st Conference on
Innovative Applications of Artificial
Intelligence, pages 60?65, Pasadena, CA.
Chaski, Carole E. 2005. Who?s at the
keyboard? Authorship attribution
in digital evidence investigations.
International Journal of Digital Evidence, 4(1).
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A library for large
linear classification. Journal of Machine
Learning Research, 9:1871?1874.
Fog, Agner. 2008. Calculation methods for
Wallenius? noncentral hypergeometric
distribution. Communications in Statistics,
Simulation and Computation, 37(2):258?273.
Griffiths, Thomas L. and Mark Steyvers.
2004. Finding scientific topics. Proceedings
of the National Academy of Sciences,
101(Suppl. 1):5228?5235.
Griffiths, Thomas L., Mark Steyvers,
David M. Blei, and Joshua B. Tenenbaum.
2004. Integrating topics and syntax. In
NIPS 2004: Proceedings of the 18th Annual
Conference on Neural Information Processing
Systems, pages 537?544, Vancouver.
Groves, Trish. 2010. Is open peer review the
fairest system? Yes. BMJ, 341:c6424.
Herlocker, Jonathan L., Joseph A. Konstan,
Al Borchers, and John Riedl. 1999. An
algorithmic framework for performing
collaborative filtering. In SIGIR 1999:
Proceedings of the 22nd International
ACM SIGIR Conference on Research and
307
Computational Linguistics Volume 40, Number 2
Development in Information Retrieval,
pages 230?237, Berkeley, CA.
Juola, Patrick. 2004. Ad-hoc authorship
attribution competition. In ALLC-ACH
2004: Proceedings of the 2004 Joint
International Conference of the Association for
Literary and Linguistic Computing and the
Association for Computers and the
Humanities, pages 175?176, Go?teborg.
Juola, Patrick. 2006. Authorship attribution.
Foundations and Trends in Information
Retrieval, 1(3):233?334.
Kacmarcik, Gary and Michael Gamon.
2006. Obfuscating document stylometry
to preserve author anonymity. In
COLING-ACL 2006: Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics
(Main Conference Poster Sessions),
pages 444?451, Sydney.
Kern, Roman, Christin Seifert, Mario
Zechner, and Michael Granitzer. 2011.
Vote/veto meta-classifier for authorship
identification. In CLEF 2011: Proceedings of
the 2011 Conference on Multilingual and
Multimodal Information Access Evaluation
(Lab and Workshop Notebook Papers),
Amsterdam.
Koppel, Moshe and Jonathan Schler. 2004.
Authorship verification as a one-class
classification problem. In ICML 2004:
Proceedings of the 21st International
Conference on Machine Learning,
pages 62?68, Banff.
Koppel, Moshe, Jonathan Schler, and
Shlomo Argamon. 2009. Computational
methods in authorship attribution.
Journal of the American Society for
Information Science and Technology,
60(1):9?26.
Koppel, Moshe, Jonathan Schler, and
Shlomo Argamon. 2011. Authorship
attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Koren, Yehuda, Robert Bell, and Chris
Volinsky. 2009. Matrix factorization
techniques for recommender systems.
IEEE Computer, 42(8):30?37.
Kourtis, Ioannis and Efstathios Stamatatos.
2011. Author identification using
semi-supervised learning. In CLEF 2011:
Proceedings of the 2011 Conference on
Multilingual and Multimodal Information
Access Evaluation (Lab and Workshop
Notebook Papers), Amsterdam.
Lacoste-Julien, Simon, Fei Sha, and
Michael I. Jordan. 2008. DiscLDA:
Discriminative learning for dimensionality
reduction and classification. In NIPS 2008:
Proceedings of the 22nd Annual Conference
on Neural Information Processing Systems,
pages 897?904, Vancouver.
Liu, Bing and Lei Zhang. 2012. A survey of
opinion mining and sentiment analysis. In
Charu C. Aggarwal and ChengXiang Zhai,
editors, Mining Text Data. Springer US,
pages 415?463.
Luyckx, Kim and Walter Daelemans. 2011.
The effect of author set size and data size
in authorship attribution. Literary and
Linguistic Computing, 26(1):35?55.
Mendenhall, Thomas C. 1887. The
characteristic curves of composition.
Science, 9(214S):237?246.
Mimno, David and Andrew McCallum. 2008.
Topic models conditioned on arbitrary
features with Dirichlet-multinomial
regression. In UAI 2008: Proceedings
of the 24th Conference on Uncertainty in
Artificial Intelligence, pages 411?418,
Helsinki.
Mosteller, Frederick and David L. Wallace.
1964. Inference and Disputed Authorship:
The Federalist. Addison-Wesley.
Nanavati, Mihir, Nathan Taylor, William
Aiello, and Andrew Warfield. 2011.
Herbert West?deanonymizer.
In HotSec?11: Proceedings of the 6th
USENIX Workshop on Hot Topics in Security,
San Francisco, CA.
Ng, Andrew Y. and Michael I. Jordan. 2001.
On discriminative vs. generative
classifiers: A comparison of logistic
regression and naive Bayes. In NIPS 2001:
Proceedings of the 15th Annual Conference on
Neural Information Processing Systems,
pages 841?848, Vancouver.
Pang, Bo and Lillian Lee. 2005. Seeing stars:
Exploiting class relationships for sentiment
categorization with respect to rating
scales. In ACL 2005: Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 115?124,
Ann Arbor, MI.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pearl, Lisa and Mark Steyvers. 2012.
Detecting authorship deception: A
supervised machine learning approach
using author writeprints. Literary and
Linguistic Computing, 27(2):183?196.
Rajkumar, Arun, Saradha Ravi,
Venkatasubramanian Suresh,
M. Narasimha Murthy, and C. E. Veni
Madhavan. 2009. Stopwords and
308
Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models
stylometry: A latent Dirichlet allocation
approach. In Proceedings of the NIPS
2009 Workshop on Applications for Topic
Models: Text and Beyond (Poster Session),
Whistler.
Ramage, Daniel, David Hall, Ramesh
Nallapati, and Christopher D. Manning.
2009. Labeled LDA: A supervised
topic model for credit attribution in
multi-labeled corpora. In EMNLP 2009:
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 248?256, Singapore.
Resnick, Paul and Hal R. Varian. 1997.
Recommender systems. Communications
of the ACM, 40(3):56?58.
Rifkin, Ryan and Aldebaro Klautau. 2004.
In defense of one-vs-all classification.
Journal of Machine Learning Research,
5(Jan):101?141.
Rosen-Zvi, Michal, Chaitanya
Chemudugunta, Thomas Griffiths,
Padhraic Smyth, and Mark Steyvers.
2010. Learning author-topic models from
text corpora. ACM Transactions on
Information Systems, 28(1):1?38.
Rosen-Zvi, Michal, Thomas Griffiths,
Mark Steyvers, and Padhraic Smyth.
2004. The author-topic model for authors
and documents. In UAI 2004: Proceedings
of the 20th Conference on Uncertainty in
Artificial Intelligence, pages 487?494, Banff.
Salton, Gerard. 1971. The SMART Retrieval
System?Experiments in Automatic
Document Processing. Prentice-Hall, Inc.,
Upper Saddle River, NJ.
Salton, Gerard. 1981. A blueprint for
automatic indexing. SIGIR Forum,
16(2):22?38.
Sanderson, Conrad and Simon Guenter.
2006. Short text authorship attribution via
sequence kernels, Markov chains and
author unmasking: An investigation.
In EMNLP 2006: Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, pages 482?491,
Sydney.
Schler, Jonathan, Moshe Koppel, Shlomo
Argamon, and James W. Pennebaker.
2006. Effects of age and gender on
blogging. In Proceedings of AAAI Spring
Symposium on Computational Approaches
for Analyzing Weblogs, pages 199?205,
Stanford, CA.
Seroussi, Yanir. 2012. Text Mining and Rating
Prediction with Topical User Models. Ph.D.
thesis, Faculty of Information Technology,
Monash University, Clayton, Victoria,
Australia.
Seroussi, Yanir, Fabian Bohnert, and Ingrid
Zukerman. 2011. Personalized rating
prediction for new users using latent
factor models. In HT 2011: Proceedings of
the 22nd International ACM Conference on
Hypertext and Hypermedia, pages 47?56,
Eindhoven.
Seroussi, Yanir, Fabian Bohnert, and
Ingrid Zukerman. 2012. Authorship
attribution with author-aware topic
models. In ACL 2012: Proceedings of the
50th Annual Meeting of the Association for
Computational Linguistics (Volume 2:
Short Papers), pages 264?269,
Jeju Island.
Seroussi, Yanir, Russell Smyth, and
Ingrid Zukerman. 2011. Ghosts from
the High Court?s past: Evidence from
computational linguistics for Dixon
ghosting for McTiernan and Rich.
University of New South Wales Law
Journal, 34(3):984?1005.
Seroussi, Yanir, Ingrid Zukerman, and
Fabian Bohnert. 2010. Collaborative
inference of sentiments from texts.
In UMAP 2010: Proceedings of the
18th International Conference on User
Modeling, Adaptation and Personalization,
pages 195?206, Waikoloa, HI.
Seroussi, Yanir, Ingrid Zukerman, and Fabian
Bohnert. 2011. Authorship attribution with
latent Dirichlet allocation. In CoNLL 2011:
Proceedings of the 15th International
Conference on Computational Natural
Language Learning, pages 181?189,
Portland, OR.
Stamatatos, Efstathios. 2009. A survey of
modern authorship attribution methods.
Journal of the American Society for
Information Science and Technology,
60(3):538?556.
Steyvers, Mark and Tom Griffiths. 2007.
Probabilistic topic models. In Thomas K.
Landauer, Danielle S. McNamara,
Simon Dennis, and Walter Kintsch,
editors, Handbook of Latent Semantic
Analysis. Lawrence Erlbaum Associates,
pages 427?448.
Tanguy, Ludovic, Assaf Urieli, Basilio
Calderone, Nabil Hathout, and
Franck Sajous. 2011. A multitude of
linguistically-rich features for authorship
attribution. In CLEF 2011: Proceedings
of the 2011 Conference on Multilingual and
Multimodal Information Access Evaluation
(Lab and Workshop Notebook Papers),
Amsterdam.
Teh, Yee Whye, Michael I. Jordan,
Matthew J. Beal, and David M. Blei. 2006.
309
Computational Linguistics Volume 40, Number 2
Hierarchical Dirichlet processes. Journal
of the American Statistical Association,
101(476):1566?1581.
Wallach, Hanna M. 2006. Topic modeling:
Beyond bag-of-words. In ICML 2006:
Proceedings of the 23rd International
Conference on Machine Learning,
pages 977?984, Pittsburgh, PA.
Wallach, Hanna M., David Mimno,
and Andrew McCallum. 2009.
Rethinking LDA: Why priors matter.
In NIPS 2009: Proceedings of the
23rd Annual Conference on Neural
Information Processing Systems,
pages 1,973?1,981, Vancouver.
Wang, Xuerui, Andrew McCallum, and
Xing Wei. 2007. Topical N-grams: Phrase
and topic discovery, with an application
to information retrieval. In ICDM 2007:
Proceedings of the 7th IEEE International
Conference on Data Mining, pages 697?702,
Omaha, NE.
Webb, Geoffrey I., Janice R. Boughton, and
Zhihai Wang. 2005. Not so naive Bayes:
Aggregating one-dependence estimators.
Machine Learning, 58(1):5?24.
Wong, Sze-Meng Jojo, Mark Dras,
and Mark Johnson. 2011. Topic modeling
for native language identification.
In ALTA 2011: Proceedings of the
Australasian Language Technology
Association Workshop, pages 115?124,
Canberra.
Zhu, Jun, Amr Ahmed, and Eric P. Xing.
2009. MedLDA: Maximum margin
supervised topic models for regression and
classification. In ICML 2009: Proceedings of
the 26th International Conference on Machine
Learning, pages 1,257?1,264, Montreal.
Zhu, Jun and Eric P. Xing. 2010. Conditional
topic random fields. In ICML 2010:
Proceedings of the 27th International
Conference on Machine Learning,
pages 1,239?1,246, Haifa.
310
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 264?269,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Authorship Attribution with Author-aware Topic Models
Yanir Seroussi Fabian Bohnert
Faculty of Information Technology, Monash University
Clayton, Victoria 3800, Australia
firstname.lastname@monash.edu
Ingrid Zukerman
Abstract
Authorship attribution deals with identifying
the authors of anonymous texts. Building on
our earlier finding that the Latent Dirichlet Al-
location (LDA) topic model can be used to
improve authorship attribution accuracy, we
show that employing a previously-suggested
Author-Topic (AT) model outperforms LDA
when applied to scenarios with many authors.
In addition, we define a model that combines
LDA and AT by representing authors and doc-
uments over two disjoint topic sets, and show
that our model outperforms LDA, AT and sup-
port vector machines on datasets with many
authors.
1 Introduction
Authorship attribution (AA) has attracted much at-
tention due to its many applications in, e.g., com-
puter forensics, criminal law, military intelligence,
and humanities research (Stamatatos, 2009). The
traditional problem, which is the focus of our work,
is to attribute test texts of unknown authorship to
one of a set of known authors, whose training texts
are supplied in advance (i.e., a supervised classifi-
cation problem). While most of the early work on
AA focused on formal texts with only a few pos-
sible authors, researchers have recently turned their
attention to informal texts and tens to thousands of
authors (Koppel et al, 2011). In parallel, topic mod-
els have gained popularity as a means of analysing
such large text corpora (Blei, 2012). In (Seroussi et
al., 2011), we showed that methods based on Latent
Dirichlet Allocation (LDA) ? a popular topic model
by Blei et al (2003) ? yield good AA performance.
However, LDA does not model authors explicitly,
and we are not aware of any previous studies that
apply author-aware topic models to traditional AA.
This paper aims to address this gap.
In addition to being the first (to the best of
our knowledge) to apply Rosen-Zvi et al?s (2004)
Author-Topic Model (AT) to traditional AA, the
main contribution of this paper is our Disjoint
Author-Document Topic Model (DADT), which ad-
dresses AT?s limitations in the context of AA. We
show that DADT outperforms AT, LDA, and linear
support vector machines on AA with many authors.
2 Disjoint Author-Document Topic Model
Background. Our definition of DADT is motivated
by the observation that when authors write texts on
the same issue, specific words must be used (e.g.,
texts about LDA are likely to contain the words
?topic? and ?prior?), while other words vary in fre-
quency according to author style. Also, texts by the
same author share similar style markers, indepen-
dently of content (Koppel et al, 2009). DADT aims
to separate document words from author words by
generating them from two disjoint topic sets of T (D)
document topics and T (A) author topics.
Lacoste-Julien et al (2008) and Ramage et al
(2009) (among others) also used disjoint topic sets
to represent document labels, and Chemudugunta
et al (2006) separated corpus-level topics from
document-specific words. However, we are unaware
of any applications of these ideas to AA. The clos-
est work we know of is by Mimno and McCallum
(2008), whose DMR model outperformed AT in AA
264
(D) (A)
wdi? T DNd
zdi
?t
? D?d
ydi
A
Dad ?
??d?
(D)
(D)
(D)
(D) (D)
?
?(A)
?(A)T?t (A)
(A)
?a(A)
Figure 1: The Disjoint Author-Document Topic Model
of multi-authored texts (DMR does not use disjoint
topic sets). We use AT rather than DMR, since we
found that AT outperforms DMR in AA of single-
authored texts, which are the focus of this paper.
The Model. Figure 1 shows DADT?s graphical rep-
resentation, with document-related parameters on
the left (the LDA component), and author-related
parameters on the right (the AT component). We de-
fine the model for single-authored texts, but it can be
easily extended to multi-authored texts.
The generative process for DADT is described be-
low. We use D and C to denote the Dirichlet and
categorical distributions respectively, and A, D and
V to denote the number of authors, documents, and
unique vocabulary words respectively. In addition,
we mark each step as coming from either LDA or
AT, or as new in DADT.
Global level:
L. For each document topic t, draw a word dis-
tribution ?(D)t ? D
(
?(D)
)
, where ?(D) is a
length-V vector.
A. For each author topic t, draw a word distribu-
tion ?(A)t ? D
(
?(A)
)
, where ?(A) is a length-
V vector.
A. For each author a, draw the author topic dis-
tribution ?(A)a ? D
(
?(A)
)
, where ?(A) is a
length-T (A) vector.
D. Draw a distribution over authors ? ? D (?),
where ? is a length-A vector.
Document level: For each document d:
L. Draw d?s topic distribution ?(D)d ? D
(
?(D)
)
,
where ?(D) is a length-T (D) vector.
D. Draw d?s author ad ? C (?).
D. Draw d?s topic ratio pid ? Beta
(
?(A), ?(D)
)
,
where ?(A) and ?(D) are scalars.
Word level: For each word index i in document d:
D. Draw di?s topic indicator ydi ? Bernoulli(pid).
L. If ydi = 0, draw a document topic zdi ?
C
(
?(D)d
)
and word wdi ? C
(
?(D)zdi
)
.
A. If ydi = 1, draw an author topic zdi ? C
(
?(A)ad
)
and word wdi ? C
(
?(A)zdi
)
.
DADT versus AT. DADT might seem similar to
AT with ?fictitious? authors, as described by Rosen-
Zvi et al (2010) (i.e., AT trained with an additional
unique ?fictitious? author for each document, allow-
ing it to adapt to individual documents and not only
to authors). However, there are several key differ-
ences between DADT and AT.
First, in DADT author topics are disjoint from
document topics, with different priors for each topic
set. Thus, the number of author topics can be differ-
ent from the number of document topics, enabling
us to vary the number of author topics according to
the number of authors in the corpus.
Second, DADT places different priors on the
word distributions for author topics and document
topics (?(A) and ?(D) respectively). Stopwords are
known to be strong indicators of authorship (Kop-
pel et al, 2009), and DADT allows us to use this
knowledge by assigning higher weights to the ele-
ments of ?(A) that correspond to stopwords than to
such elements in ?(D).
Third, DADT learns the ratio between document
words and author words on a per-document basis,
and makes it possible to specify a prior belief of
what this ratio should be. We found that specify-
ing a prior belief that about 80% of each document
is composed of author words yielded better results
than using AT?s approach, which evenly splits each
document into author and document words.
Fourth, DADT defines the process that generates
authors. This allows us to consider the number
of texts by each author when performing AA. This
also enables the potential use of DADT in a semi-
supervised setting by training on unlabelled texts,
which we plan to explore in the future.
3 Authorship Attribution Methods
We experimented with the following AA methods,
using token frequency features, which are good pre-
dictors of authorship (Koppel et al, 2009).
265
Baseline: Support Vector Machines (SVMs).
Koppel et al (2009) showed that SVMs yield
good AA performance. We use linear SVMs in a
one-versus-all setup, as implemented in LIBLIN-
EAR (Fan et al, 2008), reporting results obtained
with the best cost parameter values.
Baseline: LDA + Hellinger (LDA-H). This ap-
proach uses the Hellinger distances of topic dis-
tributions to assign test texts to the closest author.
In (Seroussi et al, 2011), we experimented with two
variants: (1) each author?s texts are concatenated be-
fore building the LDA model; and (2) no concate-
nation is performed. We found that the latter ap-
proach performs poorly in cases with many candi-
date authors. Hence, we use only the former ap-
proach in this paper. Note that when dealing with
single-authored texts, concatenating each author?s
texts yields an LDA model that is equivalent to AT.
AT. Given an inferred AT model (Rosen-Zvi et al,
2004), we calculate the probability of the test text
words for each author a, assuming it was written
by a, and return the most probable author. We do not
know of any other studies that used AT in this man-
ner for single-authored AA. We expect this method
to outperform LDA-H as it employs AT directly,
rather than relying on an external distance measure.
AT-FA. Same as AT, but built with an additional
unique ?fictitious? author for each document.
DADT. Given our DADT model, we assume that the
test text was written by a ?new? author, and infer
this author?s topic distribution, the author/document
topic ratio, and the document topic distribution. We
then calculate the probability of each author given
the model?s parameters, the test text words, and the
inferred author/document topic ratio and document
topic distribution. The most probable author is re-
turned. We use this method to avoid inferring the
document-dependent parameters separately for each
author, which is infeasible when many authors ex-
ist. A version that marginalises over these parame-
ters will be explored in future work.
4 Evaluation
We compare the performance of the methods on
two publicly-available datasets: (1) PAN?11: emails
with 72 authors (Argamon and Juola, 2011);
and (2) Blog: blogs with 19,320 authors (Schler et
al., 2006). These datasets represent realistic scenar-
ios of AA of user-generated texts with many can-
didate authors. For example, Chaski (2005) notes
a case where an employee who was terminated for
sending a racist email claimed that any person with
access to his computer could have sent the email.
Experimental Setup. Experiments on the PAN?11
dataset followed the setup of the PAN?11 competi-
tion (Argamon and Juola, 2011): We trained all the
methods on the given training subset, tuned the pa-
rameters according to the results on the given valida-
tion subset, and ran the tuned methods on the given
testing subset. In the Blog experiments, we used ten-
fold cross validation as in (Seroussi et al, 2011).
We used collapsed Gibbs sampling to train all the
topic models (Griffiths and Steyvers, 2004), run-
ning 4 chains with a burn-in of 1,000 iterations. In
the PAN?11 experiments, we retained 8 samples per
chain with spacing of 100 iterations. In the Blog
experiments, we retained 1 sample per chain due to
runtime constraints. Since we cannot average topic
distribution estimates obtained from training sam-
ples due to topic exchangeability (Steyvers and Grif-
fiths, 2007), we averaged the distances and probabil-
ities calculated from the retained samples. For test
text sampling, we used a burn-in of 100 iterations
and averaged the parameter estimates over the next
100 iterations in a similar manner to Rosen-Zvi et
al. (2010). We found that these settings yield stable
results across different random seed values.
We found that the number of topics has a larger
impact on accuracy than other configurable pa-
rameters. Hence, we used symmetric topic pri-
ors, setting all the elements of ?(D) and ?(A)
to min{0.1, 5/T (D)} and min{0.1, 5/T (A)} respec-
tively.1 For all models, we set ?w = 0.01 for each
word w as the base measure for the prior of words in
topics. Since DADT allows us to encode our prior
knowledge that stopword use is indicative of author-
ship, we set ?(D)w = 0.01 ?  and ?(A)w = 0.01 + 
for all w, where w is a stopword.2 We set  = 0.009,
which improved accuracy by up to one percentage
point over using  = 0. Finally, we set ?(A) = 4.889
and ?(D) = 1.222 for DADT. This encodes our prior
1We tested Wallach et al?s (2009) method of obtaining
asymmetric priors, but found that it did not improve accuracy.
2We used the stopword list from www.lextek.com/
manuals/onix/stopwords2.html.
266
PAN?11 PAN?11 Blog Blog
Method Validation Testing Prolific Full
SVM 48.61% 53.31% 33.31% 24.13%
LDA-H 34.95% 42.62% 21.61% 7.94%
AT 46.68% 53.08% 37.56% 23.03%
AT-FA 20.68% 24.23% ? ?
DADT 54.24% 59.08% 42.51% 27.63%
Table 1: Experiment results
belief that 0.8 ? 0.15 of each document is com-
posed of author words. We found that this yields
better results than an uninformed uniform prior of
?(A) = ?(D) = 1 (Seroussi et al, 2012). In addition,
we set ?a = 1 for each author a, yielding smoothed
estimates for the corpus distribution of authors ?.
To fairly compare the topic-based methods, we
used the same overall number of topics for all the
topic models. We present only the results obtained
with the best topic settings: 100 for PAN?11 and 400
for Blog, with DADT?s author/document topic splits
being 90/10 for PAN?11, and 390/10 for Blog. These
splits allow DADT to de-noise the author represen-
tations by allocating document words to a relatively
small number of document topics. It is worth not-
ing that AT can be seen as an extreme version of
DADT, where all the topics are author topics. A fu-
ture extension is to learn the topic balance automat-
ically, e.g., in a similar manner to Teh et al?s (2006)
method of inferring the number of topics in LDA.
Results. Table 1 shows the results of our experi-
ments in terms of classification accuracy (i.e., the
percentage of test texts correctly attributed to their
author). The PAN?11 results are shown for the val-
idation and testing subsets, and the Blog results are
shown for a subset containing the 1,000 most prolific
authors and for the full dataset of 19,320 authors.
Our DADT model yielded the best results in all
cases (the differences between DADT and the other
methods are statistically significant according to a
paired two-tailed t-test with p < 0.05). We attribute
DADT?s superior performance to the de-noising ef-
fect of the disjoint topic sets, which appear to yield
author representations of higher predictive quality
than those of the other models.
As expected, AT significantly outperformed
LDA-H. On the other hand, AT-FA performed much
worse than all the other methods on PAN?11, prob-
ably because of the inherent noisiness in using the
same topics to model both authors and documents.
Hence, we did not run AT-FA on the Blog dataset.
DADT?s PAN?11 testing result is close to the
third-best accuracy from the PAN?11 competi-
tion (Argamon and Juola, 2011). However, to the
best of our knowledge, DADT obtained the best
accuracy for a fully-supervised method that uses
only unigram features. Specifically, Kourtis and
Stamatatos (2011), who obtained the highest accu-
racy (65.8%), assumed that all the test texts are
given to the classifier at the same time, and used
this additional information with a semi-supervised
method; while Kern et al (2011) and Tanguy et al
(2011), who obtained the second-best (64.2%) and
third-best (59.4%) accuracies respectively, used var-
ious feature types (e.g., features obtained from parse
trees). Further, preprocessing differences make it
hard to compare the methods on a level playing
field. Nonetheless, we note that extending DADT to
enable semi-supervised classification and additional
feature types are promising future work directions.
While all the methods yielded relatively low accu-
racies on Blog due to its size, topic-based methods
were more strongly affected than SVM by the transi-
tion from the 1,000 author subset to the full dataset.
This is probably because topic-based methods use a
single model, making them more sensitive to corpus
size than SVM?s one-versus-all setup that uses one
model per author. Notably, an oracle that chooses
the correct answer between SVM and DADT when
they disagree yields an accuracy of 37.15% on the
full dataset, suggesting it is worthwhile to explore
ensembles that combine the outputs of SVM and
DADT (we tried using DADT topics as additional
SVM features, but this did not outperform DADT).
5 Conclusion
This paper demonstrated the utility of using author-
aware topic models for AA: AT outperformed LDA,
and our DADT model outperformed LDA, AT and
SVMs in cases with noisy texts and many authors.
We hope that these results will inspire further re-
search into the application of topic models to AA.
Acknowledgements
This research was supported in part by Australian
Research Council grant LP0883416. We thank Mark
Carman for fruitful discussions on topic modelling.
267
References
Shlomo Argamon and Patrick Juola. 2011. Overview of
the international authorship identification competition
at PAN-2011. In CLEF 2011: Proceedings of the 2011
Conference on Multilingual and Multimodal Informa-
tion Access Evaluation (Lab and Workshop Notebook
Papers), Amsterdam, The Netherlands.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3(Jan):993?1022.
David M. Blei. 2012. Probabilistic topic models. Com-
munications of the ACM, 55(4):77?84.
Carole E. Chaski. 2005. Who?s at the keyboard? Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4(1).
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2006. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In NIPS 2006: Proceedings of the 20th Annual Confer-
ence on Neural Information Processing Systems, pages
241?248, Vancouver, BC, Canada.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9(Aug):1871?1874.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Roman Kern, Christin Seifert, Mario Zechner, and
Michael Granitzer. 2011. Vote/veto meta-classifier
for authorship identification. In CLEF 2011: Pro-
ceedings of the 2011 Conference on Multilingual and
Multimodal Information Access Evaluation (Lab and
Workshop Notebook Papers), Amsterdam, The Nether-
lands.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Ioannis Kourtis and Efstathios Stamatatos. 2011. Au-
thor identification using semi-supervised learning. In
CLEF 2011: Proceedings of the 2011 Conference
on Multilingual and Multimodal Information Access
Evaluation (Lab and Workshop Notebook Papers),
Amsterdam, The Netherlands.
Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan.
2008. DiscLDA: Discriminative learning for dimen-
sionality reduction and classification. In NIPS 2008:
Proceedings of the 22nd Annual Conference on Neu-
ral Information Processing Systems, pages 897?904,
Vancouver, BC, Canada.
David Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
Dirichlet-multinomial regression. In UAI 2008: Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 411?418, Helsinki, Finland.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In EMNLP 2009: Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 248?256, Singapore.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI 2004: Proceedings of
the 20th Conference on Uncertainty in Artificial Intel-
ligence, pages 487?494, Banff, AB, Canada.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers. 2010.
Learning author-topic models from text corpora. ACM
Transactions on Information Systems, 28(1):1?38.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and gen-
der on blogging. In Proceedings of AAAI Spring Sym-
posium on Computational Approaches for Analyzing
Weblogs, pages 199?205, Stanford, CA, USA.
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
2011. Authorship attribution with latent Dirichlet alo-
cation. In CoNLL 2011: Proceedings of the 15th Inter-
national Conference on Computational Natural Lan-
guage Learning, pages 181?189, Portland, OR, USA.
Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.
2012. Authorship attribution with author-aware topic
models. Technical Report 2012/268, Faculty of Infor-
mation Technology, Monash University, Clayton, VIC,
Australia.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In Thomas K. Landauer, Danielle S.
McNamara, Simon Dennis, and Walter Kintsch, ed-
itors, Handbook of Latent Semantic Analysis, pages
427?448. Lawrence Erlbaum Associates.
Ludovic Tanguy, Assaf Urieli, Basilio Calderone, Nabil
Hathout, and Franck Sajous. 2011. A multitude
of linguistically-rich features for authorship attribu-
tion. In CLEF 2011: Proceedings of the 2011 Con-
ference on Multilingual and Multimodal Information
Access Evaluation (Lab and Workshop Notebook Pa-
pers), Amsterdam, The Netherlands.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
268
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Hanna M. Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter. In
NIPS 2009: Proceedings of the 23rd Annual Confer-
ence on Neural Information Processing Systems, pages
1973?1981, Vancouver, BC, Canada.
269
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 181?189,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Authorship Attribution with Latent Dirichlet Allocation
Yanir Seroussi Ingrid Zukerman
Faculty of Information Technology, Monash University
Clayton, Victoria 3800, Australia
firstname.lastname@monash.edu
Fabian Bohnert
Abstract
The problem of authorship attribution ? at-
tributing texts to their original authors ? has
been an active research area since the end of
the 19th century, attracting increased interest
in the last decade. Most of the work on au-
thorship attribution focuses on scenarios with
only a few candidate authors, but recently con-
sidered cases with tens to thousands of can-
didate authors were found to be much more
challenging. In this paper, we propose ways
of employing Latent Dirichlet Allocation in
authorship attribution. We show that our ap-
proach yields state-of-the-art performance for
both a few and many candidate authors, in
cases where these authors wrote enough texts
to be modelled effectively.
1 Introduction
The problem of authorship attribution ? attributing
texts to their original authors ? has received con-
siderable attention in the last decade (Juola, 2006;
Stamatatos, 2009). Most of the work in this field fo-
cuses on cases where texts must be attributed to one
of a few candidate authors, e.g., (Mosteller and Wal-
lace, 1964; Gamon, 2004). Recently, researchers
have turned their attention to scenarios with tens to
thousands of candidate authors (Koppel et al, 2011).
In this paper, we study authorship attribution with
few to many candidate authors, and introduce a new
method that achieves state-of-the-art performance in
the latter case.
Our approach to authorship attribution consists of
building models of authors and their texts using La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003).
We compare these models to models built from texts
with unknown authors to find the most likely authors
of these texts (Section 3.2). Our evaluation shows
that our approach yields a higher accuracy than the
method recently introduced by Koppel et al (2011)
in several cases where prolific authors are consid-
ered, while requiring less runtime (Section 4).
This paper is structured as follows. Related work
is surveyed in Section 2. Our LDA-based approach
to authorship attribution is described in Section 3,
together with the baselines we considered in our
evaluation. Section 4 presents and discusses the re-
sults of our evaluation, and Section 5 discusses our
conclusions and plans for future work.
2 Related Work
The field of authorship attribution predates modern
computing. For example, in the late 19th century,
Mendenhall (1887) suggested that word length can
be used to distinguish works by different authors. In
recent years, increased interest in authorship attribu-
tion was fuelled by advances in machine learning,
information retrieval, and natural language process-
ing (Juola, 2006; Stamatatos, 2009).
Commonly used features in authorship attribu-
tion range from ?shallow? features, such as token
and character n-gram frequencies, to features that
require deeper analysis, such as part-of-speech and
rewrite rule frequencies (Stamatatos, 2009). As in
other text classification tasks, Support Vector Ma-
chines (SVMs) have delivered high accuracy, as
they are designed to handle feature vectors of high
dimensionality (Juola, 2006). For example, one-
vs.-all (OVA) is an effective approach to using bi-
nary SVMs for multi-class (i.e., multi-author) prob-
lems (Rifkin and Klautau, 2004). Given A authors,
181
OVA trains A binary classifiers, where each classi-
fier is trained on texts by one author as positive ex-
amples and all the other texts as negative examples.
However, ifA is large, each classifier has many more
negative than positive examples, often yielding poor
results due to class imbalance (Raskutti and Kowal-
czyk, 2004). Other setups, such as one-vs.-one or
directed acyclic graph, require training O(A2) clas-
sifiers, making them impractical where thousands of
authors exist. Multi-class SVMs have also been sug-
gested, but they generally perform comparably to
OVA while taking longer to train (Rifkin and Klau-
tau, 2004). Hence, using SVMs for scenarios with
many candidate authors is problematic (Koppel et
al., 2011). Recent approaches to employing binary
SVMs consider class similarity to improve perfor-
mance (Bickerstaffe and Zukerman, 2010; Cheng
et al, 2007). We leave experiments with such ap-
proaches for future work (Section 5).
In this paper, we focus on authorship attribution
with many candidate authors. This problem was pre-
viously addressed by Madigan et al (2005) and Luy-
ckx and Daelemans (2008), who worked on datasets
with texts by 114 and 145 authors respectively. In
both cases, the reported results were much poorer
than those reported in the binary case. More re-
cently, Koppel et al (2011) considered author sim-
ilarity to handle cases with thousands of candidate
authors. Their method, which we use as our base-
line, is described in Section 3.1.
Our approach to authorship attribution utilises La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
to build models of authors from their texts. LDA
is a generative probabilistic model that is tradition-
ally used to find topics in textual data. The main
idea behind LDA is that each document in a cor-
pus is generated from a distribution of topics, and
each word in the document is generated according
to the per-topic word distribution. Blei et al (2003)
showed that using LDA for dimensionality reduction
can improve performance for supervised text clas-
sification. We know of only one case where LDA
was used in authorship attribution: Rajkumar et al
(2009) reported preliminary results on using LDA
topic distributions as feature vectors for SVMs, but
they did not compare the results obtained with LDA-
based SVMs to those obtained with SVMs trained
on tokens directly. Our comparison shows that both
methods perform comparably (Section 4.3).
Nonetheless, the main focus of our work is
on authorship attribution with many candidate au-
thors, where it is problematic to use SVMs. Our
LDA+Hellinger approach employs LDA without
SVM training (Section 3.2), yielding state-of-the-art
performance in several scenarios (Section 4).
3 Authorship Attribution Methods
This section describes the authorship attribution
methods considered in this paper. While all these
methods can employ various representations of doc-
uments, e.g., token frequencies or part-of-speech n-
gram frequencies, we only experimented with token
frequencies.1 This is because they are simple to ex-
tract, and can achieve good performance (Section 4).
Further, the focus of this paper is on comparing the
performance of our methods to that of the baseline
methods. Thus, we leave experiments on other fea-
ture types for future work (Section 5).
3.1 Baselines
We consider two baseline methods, depending on
whether there are two or many candidate authors.
If there are only two, we use Support Vector Ma-
chines (SVMs), which have been shown to de-
liver state-of-the-art performance on this task (Juola,
2006). If there are many, we follow Koppel et
al.?s (2011) approach, which we denote KOP.
The main idea behind KOP is that different pairs
of authors may be distinguished by different sub-
sets of the feature space. Hence, KOP randomly
chooses k1 subsets of size k2F (k2 < 1) from a set
of F features; for each of the k1 subsets, it calcu-
lates the cosine similarity between a test document
and all the documents by one author (each author is
represented by one feature vector); it then outputs
the author who had most of the top matches. KOP
also includes a threshold ?? to handle cases where
a higher level of precision is required, at the cost
of lower recall. If the top-matching author was the
top match less than ?? times, then KOP outputs ?un-
known author?. In our experiments we set ?? = 0 to
obtain full coverage, as this makes it easier to inter-
pret the results using a single measure of accuracy.
1Token frequency is the token count divided by the total
number of tokens.
182
3.2 Authorship Attribution with LDA
In this work, we follow the extended LDA model de-
fined by Griffiths and Steyvers (2004). Under the as-
sumptions of the extended model, given a corpus of
M documents, a document iwithN tokens is gener-
ated by choosing a document topic distribution ?i ?
Dir(?), where Dir(?) is a T -dimensional symmet-
ric Dirichlet distribution, and ? and T are parame-
ters of the model. Then, each token in the document
wij is generated by choosing a topic from the docu-
ment topic distribution zij ? Multinomial(?i), and
choosing a token from the token topic distribution
wij ? Multinomial(?zij ), where ?zij ? Dir(?), and
? is a parameter of the model. The model can be
inferred from the data using Gibbs sampling, as out-
lined in (Griffiths and Steyvers, 2004) ? an approach
we follow in our experiments.
Note that the topics obtained by LDA do not have
to correspond to actual, human-interpretable topics.
A more appropriate name may be ?latent factors?,
but we adopt the convention of calling these fac-
tors ?topics? throughout this paper. The meaning of
the factors depends on the type of tokens that are
used as input to the LDA inference process. For
example, if stopwords are removed from the cor-
pus, the resulting factors often, but not necessarily,
correspond to topics. However, if only stopwords
are retained, as is commonly done in authorship at-
tribution studies, the resulting factors lose their in-
terpretability as topics; rather, they can be seen as
stylistic markers. Note that even if stopwords are
discarded, nothing forces the factors to stand for ac-
tual topics. Indeed, in a preliminary experiment on a
corpus of movie reviews and message board posts,
we found that some factors correspond to topics,
with words such as ?noir? and ?detective? consid-
ered to be highly probable for one topic. However,
other factors seemed to correspond to authorship
style as reflected by authors? vocabulary, with net-
speak words such as ?wanna?, ?alot? and ?haha? as-
signed to one topic, and words such as ?compelling?
and ?beautifully? assigned to a different topic.
We consider two ways of using LDA in authorship
attribution: (1) Topic SVM, and (2) LDA+Hellinger.
The LDA part of both approaches consists of apply-
ing a frequency filter to the features in the training
documents,2 and then using LDA to reduce the di-
mensionality of each document to a topic distribu-
tion of dimensionality T .
Topic SVM. The topic distributions are used as
features for a binary SVM classifier that discrimi-
nates between authors. This approach has been em-
ployed in the past for document classification, e.g.,
in (Blei et al, 2003), but it has been applied to au-
thorship attribution only in a limited study that con-
sidered just stopwords (Rajkumar et al, 2009). In
Section 4.3, we present the results of more thorough
experiments in applying this approach to binary au-
thorship attribution. Our results show that the per-
formance of this approach is comparable to that ob-
tained without using LDA. This indicates that we
do not lose authorship-related information when em-
ploying LDA, even though the dimensionality of the
document representations is greatly reduced.
LDA+Hellinger. This method is our main contri-
bution, as it achieves state-of-the-art performance in
authorship attribution with many candidate authors,
where it is problematic to use SVMs (Section 2).
The main idea of our approach is to use the
Hellinger distance between document topic distribu-
tions to find the most likely author of a document:3
D(?1, ?2) =
?
1
2
?T
t=1
(?
?1,t ?
?
?2,t
)2
where ?i
is a T -dimensional multinomial topic distribution,
and ?i,t is the probability of the t-th topic.
We propose two representations of an author?s
documents: multi-document and single-document.
? Multi-document (LDAH-M). The LDA model
is built based on all the training documents.
Given a test document, we measure the
Hellinger distance between its topic distribu-
tion and the topic distributions of the training
documents. The author with the lowest mean
distance for all of his/her documents is returned
as the most likely author of the test document.
2We employed frequency filtering because it has been shown
to be a scalable and effective feature selection method for au-
thorship attribution tasks (Stamatatos, 2009). We leave experi-
ments with other feature selection methods for future work.
3We considered other measures for comparing topic dis-
tributions, including Kullback-Leibler divergence and Bhat-
tacharyya distance. From these measures, only Hellinger dis-
tance satisfies all required properties of a distance metric.
Hence, we used Hellinger distance.
183
? Single-document (LDAH-S). Each author?s
documents are concatenated into a single doc-
ument (the profile document), and the LDA
model is learned from the profile documents.4
Given a test document, the Hellinger distance
between the topic distributions of the test docu-
ment and all the profile documents is measured,
and the author of the profile document with the
shortest distance is returned.
The time it takes to learn the LDA model de-
pends on the number of Gibbs samples S, the num-
ber of tokens in the training corpusW , and the num-
ber of topics T . For each Gibbs sample, the al-
gorithm iterates through all the tokens in the cor-
pus, and for each token it iterates through all the
topics. Thus, the time complexity of learning the
model is O(SWT ). Once the model is learned, in-
ferring the topic distribution of a test document of
length N takes O(SNT ). Therefore, the time it
takes to classify a document when using LDAH-S
isO(SNT+AT ), whereA is the number of authors,
and O(T ) is the time complexity of calculating the
Hellinger distance between two T -dimensional dis-
tributions. The time it takes to classify a docu-
ment when using LDAH-M is O(SNT + MT ),
where M is the total number of training documents,
and M ? A, because every candidate author has
written at least one document.
An advantage of LDAH-S over LDAH-M is that
LDAH-S requires much less time to classify a test
document when many documents per author are
available. However, this improvement in runtime
may come at the price of accuracy, as authorship
markers that are present only in a few short doc-
uments by one author may lose their prominence
if these documents are concatenated to longer doc-
uments. In our evaluation we found that LDAH-
M outperforms LDAH-S when applied to one of
the datasets (Section 4.3), while LDAH-S yields
a higher accuracy when applied to the other two
datasets (Sections 4.4 and 4.5). Hence, we present
the results obtained with both variants.
4Concatenating all the author documents into one document
has been named the profile-based approach in previous studies,
in contrast to the instance-based approach, where each docu-
ment is considered separately (Stamatatos, 2009).
4 Evaluation
In this section, we describe the experimental setup
and datasets used in our experiments, followed
by the evaluation of our methods. We evaluate
Topic SVM for binary authorship attribution, and
LDA+Hellinger on a binary dataset, a dataset with
tens of authors, and a dataset with thousands of au-
thors. Our results show that LDA+Hellinger yields
a higher accuracy than Koppel et al?s (2011) base-
line method in several cases where prolific authors
are considered, while requiring less runtime.
4.1 Experimental Setup
In all the experiments, we perform ten-fold cross
validation, employing stratified sampling where pos-
sible. The results are evaluated using classification
accuracy, i.e., the percentage of test documents that
were correctly assigned to their author. Note that
we use different accuracy ranges in the figures that
present our results for clarity of presentation. Sta-
tistically significant differences are reported when
p < 0.05 according to a paired two-tailed t-test.
We used the LDA implementation from Ling-
Pipe (alias-i.com/lingpipe) and the SVM im-
plementation from Weka (www.cs.waikato.ac.
nz/ml/weka). Since our focus is on testing the
impact of LDA, we used a linear SVM kernel and
the default SVM settings. For the LDA param-
eters, we followed Griffiths and Steyvers (2004)
and the recommendations in LingPipe?s documenta-
tion, and set the Dirichlet hyperparameters to ? =
min(0.1, 50/T ) and ? = 0.01, varying only the
number of topics T . We ran the Gibbs sampling
process for S = 1000 iterations, and based the doc-
ument representations on the last sample. While
taking more than one sample is generally consid-
ered good practice (Steyvers and Griffiths, 2007),
we found that the impact of taking several samples
on accuracy is minimal, but it substantially increases
the runtime. Hence, we decided to use only one sam-
ple in our experiments.
4.2 Datasets
We considered three datasets that cover different
writing styles and settings: Judgement, IMDb62 and
Blog. Table 1 shows a summary of these datasets.
The Judgement dataset contains judgements by
three judges who served on the Australian High
184
Judgement IMDb62 Blog
Authors 3 62 19,320
Texts 1,342 62,000 678,161
Texts per
Author
Dixon: 902
McTiernan: 253
Rich: 187
1,000
Mean: 35.10
Stddev.: 104.99
Table 1: Dataset Statistics
Court from 1913 to 1975: Dixon, McTiernan and
Rich (available for download from www.csse.
monash.edu.au/research/umnl/data). In
this paper, we considered the Dixon/McTiernan and
the Dixon/Rich binary classification cases, using
judgements from non-overlapping periods (Dixon?s
1929?1964 judgements, McTiernan?s 1965?1975,
and Rich?s 1913?1928). We removed numbers from
the texts to ensure that dates could not be used to dis-
criminate between judges. We also removed quotes
to ensure that the classifiers take into account only
the actual author?s language use.5 Employing this
dataset in our experiments allows us to test our meth-
ods on formal texts with a minimal amount of noise.
The IMDb62 dataset contains 62,000 movie re-
views by 62 prolific users of the Internet Movie
database (IMDb, www.imdb.com, available upon
request from the authors of (Seroussi et al, 2010)).
Each user wrote 1,000 reviews. This dataset is nois-
ier than the Judgement dataset, since it may con-
tain spelling and grammatical errors, and the reviews
are not as professionally edited as judgements. This
dataset alows us to test our approach in a setting
where all the texts have similar themes, and the num-
ber of authors is relatively small, but is already much
larger than the number of authors considered in tra-
ditional authorship attribution settings.
The Blog dataset is the largest dataset we consid-
ered, containing 678,161 blog posts by 19,320 au-
thors (Schler et al, 2006) (available for download
from u.cs.biu.ac.il/?koppel). In contrast to
IMDb reviews, blog posts can be about any topic,
but the large number of authors ensures that every
topic is likely to interest at least some authors. Kop-
pel et al (2011) used a different blog dataset con-
sisting of 10,240 authors in their work on authorship
5We removed numbers and quotes by matching regular ex-
pressions for numbers and text in quotation marks, respectively.
attribution with many candidate authors. Unfortu-
nately, their dataset is not publicly available. How-
ever, authorship attribution is more challenging on
the dataset we used, because they imposed some re-
strictions on their dataset, such as setting a minimal
number of words per author, and truncating the train-
ing and testing texts so that they all have the same
length. The dataset we use has no such restrictions.
4.3 LDA in Binary Authorship Attribution
In this section, we present the results of our experi-
ments with the Judgement dataset (Section 4.2), test-
ing the use of LDA in producing feature vectors for
SVMs and the performance of our LDA+Hellinger
methods (Section 3.2).
In all the experiments, we employed a classifier
ensemble to address the class imbalance problem
present in the Judgement dataset, which contains 5
times more texts by Dixon than by Rich, and over 3
times more texts by Dixon than by McTiernan (Ta-
ble 1). Dixon?s texts are randomly split into 5 or
3 subsets, depending on the other author (Rich or
McTiernan respectively), and the base classifiers are
trained on each subset of Dixon?s texts together with
all the texts by the other judge. Given a text by an
unknown author, the classifier outputs are combined
using majority voting. We found that the accuracies
obtained with an ensemble are higher than those ob-
tained with a single classifier. We did not require
the vote to be unanimous, even though this increases
precision, because we wanted to ensure full cover-
age of the test dataset. This enables us to compare
different methods using only an accuracy measure.6
Experiment 1. Figure 1 shows the results of an
experiment that compares the accuracy obtained us-
ing SVMs with token frequencies as features (Token
SVMs) with that obtained using LDA topic distribu-
tions as features (Topic SVMs). We experimented
with several filters on token frequency, and differ-
ent numbers of LDA topics (5, 10, 25, 50, . . ., 250).
The x-axis labels describe the frequency filters: the
minimum and maximum token frequencies, and the
approximate number of unique tokens left after fil-
tering (in thousands). We present only the results
obtained with 10, 25, 100 and 200 topics, as the re-
6For all our experiments, the results for the Dixon/McTier-
nan case are comparable to those for Dixon/Rich. Therefore,
we omit the Dixon/McTiernan results to conserve space.
185
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
0
1E-5
9.2
0
5E-5
12.2
0
1E-4
13
0
5E-4
13.8
0
1
14
1E-5
5E-5
3
1E-5
1E-4
3.8
1E-5
5E-4
4.6
1E-5
1
4.8
5E-5
1E-4
0.7
5E-5
5E-4
1.5
5E-5
1
1.7
1E-4
5E-4
0.8
1E-4
1
1
5E-4
1
0.2
Ac
cur
acy
Token Frequency Filter
Min
Max
Tokens (K)
Token SVM
Majority Baseline
10 Topic SVM
25 Topic SVM
100 Topic SVM
200 Topic SVM
Figure 1: LDA Features for SVMs in Binary Authorship
Attribution (Judgement dataset, Dixon/Rich)
sults obtained with other topic numbers are consis-
tent with the presented results, and the results ob-
tained with 225 and 250 topics are comparable to
the results obtained with 200 topics.
Our results show that setting a maximum bound
on token frequency filters out important authorship
markers, regardless of whether LDA is used or
not (performance drops). This shows that it is un-
likely that discriminative LDA topics correspond to
actual topics, as the most frequent tokens are mostly
non-topical (e.g., punctuation and function words).
An additional conclusion is that using LDA for
feature reduction yields results that are comparable
to those obtained using tokens directly. While Topic
SVMs seem to perform slightly better than Token
SVMs, the differences between the best results ob-
tained with the two approaches are not statistically
significant. However, the number of features that
the SVMs consider when topics are used is usually
much smaller than when tokens are used directly, es-
pecially when no token filters are used (i.e., when
the minimum frequency is 0 and the maximum fre-
quency is 1). This makes it easy to apply LDA to dif-
ferent datasets, since the token filtering parameters
may be domain-dependent, and LDA yields good re-
sults without filtering tokens.
Experiment 2. Figure 2 shows the results of
an experiment that compares the performance of
the single profile document (LDAH-S) and multi-
ple author documents (LDAH-M) variants of our
LDA+Hellinger approach to the results obtained
with Token SVMs and Topic SVMs. As in Exper-
iment 1, we employ classifier ensembles, where the
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  25  50  75  100  125  150  175  200
Ac
cur
acy
Number of Topics
Token SVM
Majority Baseline
Topic SVM
LDAH-S
LDAH-M
Figure 2: LDA+Hellinger in Binary Authorship Attribu-
tion (Judgement dataset, Dixon/Rich)
base classifiers are either SVMs or LDA+Hellinger
classifiers. We did not filter tokens, since Experi-
ment 1 indicates that filtering has no advantage over
not filtering tokens. Instead, Figure 2 presents the
accuracy as a function of the number of topics.
Note that we did not expect LDA+Hellinger to
outperform SVMs, since LDA+Hellinger does not
consider inter-class relationships. Indeed, Figure 2
shows that this is the case (the differences between
the best Topic SVM results and the best LDAH-
M results are statistically significant). However,
LDA+Hellinger still delivers results that are much
better than the majority baseline (the differences be-
tween LDA+Hellinger and the majority baseline are
statistically significant). This leads us to hypothe-
sise that LDA+Hellinger will perform well in cases
where it is problematic to use SVMs due to the large
number of candidate authors. We verify this hypoth-
esis in the following sections.
One notable result is that LDAH-S delivers high
accuracy even when only a few topics are used,
while LDAH-M requires about 50 topics to outper-
form LDAH-S (all the differences between LDAH-S
and LDAH-M are statistically significant). This may
be because there are only two authors, so LDAH-
S builds the LDA model based only on two profile
documents. Hence, even 5 topics are enough to ob-
tain two topic distributions that are sufficiently dif-
ferent to discriminate the authors? test documents.
The reason LDAH-M outperforms LDAH-S when
more topics are considered may be that some impor-
tant authorship markers lose their prominence in the
profile documents created by LDAH-S.
186
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  50  100  150  200  250  300  350  400
Ac
cur
acy
Number of Topics
KOP: k1 = 400, k2 = 0.2LDAH-S
LDAH-M
Figure 3: LDA+Hellinger with Tens of Authors (IMDb62
dataset)
4.4 LDA+Hellinger with Tens of Authors
In this section, we apply our LDA+Hellinger ap-
proaches to the IMDb62 dataset (Section 4.2), and
compare the obtained results to those obtained with
Koppel et al?s (2011) method (KOP). To this effect,
we first established a KOP best-performance base-
line by performing parameter tuning experiments for
KOP. Figure 3 shows the results of the comparison
of the accuracies obtained with our LDA+Hellinger
methods to the best accuracy yielded by KOP (ob-
tained in the parameter tuning experiment).
For this experiment, we ran our LDA+Hellinger
variants with 5, 10, 25, 50, . . ., 300, 350 and 400
topics. The highest LDAH-M accuracy was ob-
tained with 300 topics (Figure 3). However, LDAH-
S yielded a much higher accuracy than LDAH-M.
This may be because the large number of training
texts per author (900) may be too noisy for LDAH-
M. That is, the differences between individual texts
by each author may be too large to yield a meaning-
ful representation of the author if they are considered
separately. Finally, LDAH-S requires only 50 topics
to outperform KOP, and outperforms KOP by about
15% for 150 topics. All the differences between the
methods are statistically significant.
This experiment shows that LDAH-S models the
authors in IMDb62 more accurately than KOP. The
large improvement in accuracy shows that the com-
pact author representation employed by LDAH-S,
which requires only 150 topics to obtain the highest
accuracy, has more power to discriminate between
authors than KOP?s much heavier representation, of
400 subsets with more than 30,000 features each. In
addition, the per-fold runtime of the KOP baseline
was 93 hours, while LDAH-S required only 15 hours
per fold to obtain the highest accuracy.
4.5 LDA+Hellinger with Thousands of Authors
In this section, we compare the performance of our
LDA+Hellinger variants to the performance of KOP
on several subsets of the Blog dataset (Section 4.2).
For this purpose, we split the dataset according to
the prolificness of the authors, i.e., we ordered the
authors by the number of blog posts, and considered
subsets that contain all the posts by the 1000, 2000,
5000 and 19320 most prolific authors.7 Due to the
large number of posts, we could not run KOP for
more than k1 = 10 iterations on the smallest subset
of the dataset and 5 iterations on the other subsets,
as the runtime was prohibitive for more iterations.
For example, 10 iterations on the smallest subset re-
quired about 90 hours per fold (the LDA+Hellinger
runtimes were substantially shorter, with maximum
runtimes of 56 hours for LDAH-S and 77 hours for
LDAH-M, when 200 topics were considered). Inter-
estingly, running KOP for 5 iterations on the larger
subsets decreased performance compared to running
it for 1 iteration. Thus, on the larger subsets, the
most accurate KOP results took less time to obtain
than those of our LDA+Hellinger variants.
Figure 4 shows the results of this experiment.
For each author subset, it compares the results ob-
tained by LDAH-S and LDAH-M to the best re-
sult obtained by KOP. All the differences between
the methods are statistically significant. For up to
2000 prolific authors (Figures 4(a), 4(b)), LDAH-S
outperforms KOP by up to 50%. For 5000 prolific
users (figure omitted due to space limitations), the
methods perform comparably, and KOP outperforms
LDAH-S by a small margin. However, with all the
authors (Figure 4(c)), KOP yields a higher accuracy
than both LDA+Hellinger variants. This may be
because considering non-prolific authors introduces
noise that results in an LDA model that does not cap-
ture the differences between authors. However, it is
encouraging that LDAH-S outperforms KOP when
less than 5000 prolific authors are considered.
7These authors make up about 5%, 10%, 25% and exactly
100% of the authors, but they wrote about 50%, 65%, 80% and
exactly 100% of the texts, respectively.
187
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 10, k2 = 0.6LDAH-SLDAH-M
(a) 1,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M
(b) 2,000 Prolific Authors
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  25  50  75  100  125  150  175  200
Acc
urac
y
Number of Topics
KOP: k1 = 1, k2 = 1.0LDAH-SLDAH-M
(c) 19,320 (all) Authors
Figure 4: LDA+Hellinger with Thousands of Authors (Blog dataset)
The accuracies obtained in this section are rather
low compared to those obtained in the previous
sections. This is not surprising, since the author-
ship attribution problem is much more challenging
with thousands of candidate authors. This chal-
lenge motivated the introduction of the ?? thresh-
old in KOP (Section 3.1). Our LDA+Hellinger vari-
ants can also be extended to include a threshold: if
the Hellinger distance of the best-matching author is
greater than the threshold, the LDA+Hellinger algo-
rithm would return ?unknown author?. We leave ex-
periments with this extension to future work, as our
focus in this paper is on comparing LDA+Hellinger
to KOP, and we believe that this comparison is
clearer when no thresholds are used.
5 Conclusions and Future Work
In this paper, we introduced an approach to author-
ship attribution that models texts and authors using
Latent Dirichlet Allocation (LDA), and considers
the distance between the LDA-based representations
of the training and test texts when classifying test
texts. We showed that our approach yields state-of-
the-art performance in terms of classification accu-
racy when tens or a few thousand authors are consid-
ered, and prolific authors exist in the training data.
This accuracy improvement was achieved together
with a substantial reduction in runtime compared to
Koppel et al?s (2011) baseline method.
While we found that our approach performs well
on texts by prolific authors, there is still room for
improvement on authors who have not written many
texts ? an issue that we will address in the future.
One approach that may improve performance on
such authors involves considering other types of fea-
tures than tokens, such as parts of speech and char-
acter n-grams. Since our approach is based on LDA,
it can easily employ different feature types, which
makes this a straightforward extension to the work
presented in this paper.
In the future, we also plan to explore ways of ex-
tending LDA to model authors directly, rather than
using it as a black box. Authors were considered by
Rosen-Zvi et al (2004; 2010), who extended LDA
to form an author-topic model. However, this model
was not used for authorship attribution, and was
mostly aimed at topic modelling of multi-authored
texts, such as research papers.
Another possible research direction is to improve
the scalability of our methods. Our approach, like
Koppel et al?s (2011) baseline, requires linear time
in the number of possible authors to classify a single
document. One possible way of reducing the time
needed for prediction is by employing a hierarchi-
cal approach that builds a tree of classifiers based on
class similarity, as done by Bickerstaffe and Zuker-
man (2010) for the sentiment analysis task. Under
this framework, class similarity (in our case, author
similarity) can be measured using LDA, while small
groups of classes can be discriminated using SVMs.
In addition to authorship attribution, we plan to
employ text-based author models in user modelling
tasks such as rating prediction ? a direction that we
already started working on by successfully applying
our LDA-based approach to model users for the rat-
ing prediction task (Seroussi et al, 2011).
Acknowledgements
This research was supported in part by grant
LP0883416 from the Australian Research Council.
The authors thank Russell Smyth for the collabora-
tion on initial results on the judgement dataset.
188
References
Adrian Bickerstaffe and Ingrid Zukerman. 2010. A hier-
archical classifier applied to multi-way sentiment de-
tection. In COLING 2010: Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 62?70, Beijing, China.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3(Jan):993?1022.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2007. Lo-
calized support vector machine and its efficient algo-
rithm. In SDM 2007: Proceedings of the 7th SIAM
International Conference on Data Mining, pages 461?
466, Minneapolis, MN, USA.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analysis
features. In COLING 2004: Proceedings of the 20th
International Conference on Computational Linguis-
tics, pages 611?617, Geneva, Switzerland.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Kim Luyckx and Walter Daelemans. 2008. Authorship
attribution and verification with many authors and lim-
ited data. In COLING 2008: Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 513?520, Manchester, UK.
David Madigan, Alexander Genkin, David D. Lewis,
Shlomo Argamon, Dmitriy Fradkin, and Li Ye. 2005.
Author identification on the large scale. In Proceed-
ings of the Joint Annual Meeting of the Interface and
the Classification Society of North America, St. Louis,
MO, USA.
Thomas C. Mendenhall. 1887. The characteristic curves
of composition. Science, 9(214S):237?246.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Addison-Wesley.
Arun Rajkumar, Saradha Ravi, Venkatasubramanian
Suresh, M. Narasimha Murthy, and C. E. Veni Mad-
havan. 2009. Stopwords and stylometry: A latent
Dirichlet alocation approach. In Proceedings of the
NIPS 2009 Workshop on Applications for Topic Mod-
els: Text and Beyond (Poster Session), Whistler, BC,
Canada.
Bhavani Raskutti and Adam Kowalczyk. 2004. Extreme
re-balancing for SVMs: A case study. ACM SIGKDD
Explorations Newsletter, 6(1):60?69.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5(Jan):101?141.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for
authors and documents. In UAI 2004: Proceedings of
the 20th Conference on Uncertainty in Artificial Intel-
ligence, pages 487?494, Banff, AB, Canada.
Michal Rosen-Zvi, Chaitanya Chemudugunta, Thomas
Griffiths, Padhraic Smyth, and Mark Steyvers. 2010.
Learning author-topic models from text corpora. ACM
Transactions on Information Systems, 28(1):1?38.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and gen-
der on blogging. In Proceedings of AAAI Spring Sym-
posium on Computational Approaches for Analyzing
Weblogs, pages 199?205, Stanford, CA, USA.
Yanir Seroussi, Ingrid Zukerman, and Fabian Bohnert.
2010. Collaborative inference of sentiments from
texts. In UMAP 2010: Proceedings of the 18th In-
ternational Conference on User Modeling, Adaptation
and Personalization, pages 195?206, Waikoloa, HI,
USA.
Yanir Seroussi, Fabian Bohnert, and Ingrid Zukerman.
2011. Personalised rating prediction for new users us-
ing latent factor models. In Hypertext 2011: Proceed-
ings of the 22nd ACM Conference on Hypertext and
Hypermedia, Eindhoven, The Netherlands.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. In Thomas K. Landauer, Danielle S.
McNamara, Simon Dennis, and Walter Kintsch, ed-
itors, Handbook of Latent Semantic Analysis, pages
427?448. Lawrence Erlbaum Associates.
189
