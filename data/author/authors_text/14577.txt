Proceedings of the ACL 2010 Conference Short Papers, pages 142?146,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Better Filtration and Augmentation for Hierarchical Phrase-Based
Translation Rules
Zhiyang Wang ? Yajuan Lu? ? Qun Liu ? Young-Sook Hwang ?
?Key Lab. of Intelligent Information Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
wangzhiyang@ict.ac.cn yshwang@sktelecom.com
Abstract
This paper presents a novel filtration cri-
terion to restrict the rule extraction for
the hierarchical phrase-based translation
model, where a bilingual but relaxed well-
formed dependency restriction is used to
filter out bad rules. Furthermore, a new
feature which describes the regularity that
the source/target dependency edge trig-
gers the target/source word is also pro-
posed. Experimental results show that, the
new criteria weeds out about 40% rules
while with translation performance im-
provement, and the new feature brings an-
other improvement to the baseline system,
especially on larger corpus.
1 Introduction
Hierarchical phrase-based (HPB) model (Chiang,
2005) is the state-of-the-art statistical machine
translation (SMT) model. By looking for phrases
that contain other phrases and replacing the sub-
phrases with nonterminal symbols, it gets hierar-
chical rules. Hierarchical rules are more powerful
than conventional phrases since they have better
generalization capability and could capture long
distance reordering. However, when the train-
ing corpus becomes larger, the number of rules
will grow exponentially, which inevitably results
in slow and memory-consuming decoding.
In this paper, we address the problem of reduc-
ing the hierarchical translation rule table resorting
to the dependency information of bilingual lan-
guages. We only keep rules that both sides are
relaxed-well-formed (RWF) dependency structure
(see the definition in Section 3), and discard others
which do not satisfy this constraint. In this way,
about 40% bad rules are weeded out from the orig-
inal rule table. However, the performance is even
better than the traditional HPB translation system.
Source
Target 
f f? 
e
Figure 1: Solid wire reveals the dependency rela-
tion pointing from the child to the parent. Target
word e is triggered by the source word f and it?s
head word f ?, p(e|f ? f ?).
Based on the relaxed-well-formed dependency
structure, we also introduce a new linguistic fea-
ture to enhance translation performance. In the
traditional phrase-based SMT model, there are
always lexical translation probabilities based on
IBM model 1 (Brown et al, 1993), i.e. p(e|f),
namely, the target word e is triggered by the source
word f . Intuitively, however, the generation of e
is not only involved with f , sometimes may also
be triggered by other context words in the source
side. Here we assume that the dependency edge
(f ? f ?) of word f generates target word e (we
call it head word trigger in Section 4). Therefore,
two words in one language trigger one word in
another, which provides a more sophisticated and
better choice for the target word, i.e. Figure 1.
Similarly, the dependency feature works well in
Chinese-to-English translation task, especially on
large corpus.
2 Related Work
In the past, a significant number of techniques
have been presented to reduce the hierarchical rule
table. He et al (2009) just used the key phrases
of source side to filter the rule table without taking
advantage of any linguistic information. Iglesias
et al (2009) put rules into syntactic classes based
on the number of non-terminals and patterns, and
applied various filtration strategies to improve the
rule table quality. Shen et al (2008) discarded
142
found
The
girl 
lovely 
house
a beautiful
Figure 2: An example of dependency tree. The
corresponding plain sentence is The lovely girl
found a beautiful house.
most entries of the rule table by using the con-
straint that rules of the target-side are well-formed
(WF) dependency structure, but this filtering led to
degradation in translation performance. They ob-
tained improvements by adding an additional de-
pendency language model. The basic difference
of our method from (Shen et al, 2008) is that we
keep rules that both sides should be relaxed-well-
formed dependency structure, not just the target
side. Besides, our system complexity is not in-
creased because no additional language model is
introduced.
The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-
ond word can come from any part of the sentence,
there may be a prohibitively large number of pa-
rameters involved. Besides, He et al (2008) built
a maximum entropy model which combines rich
context information for selecting translation rules
during decoding. However, as the size of the cor-
pus increases, the maximum entropy model will
become larger. Similarly, In (Shen et al, 2009),
context language model is proposed for better rule
selection. Taking the dependency edge as condi-
tion, our approach is very different from previous
approaches of exploring context information.
3 Relaxed-well-formed Dependency
Structure
Dependency models have recently gained consid-
erable interest in SMT (Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008). Depen-
dency tree can represent richer structural infor-
mation. It reveals long-distance relation between
words and directly models the semantic structure
of a sentence without any constituent labels. Fig-
ure 2 shows an example of a dependency tree. In
this example, the word found is the root of the tree.
Shen et al (2008) propose the well-formed de-
pendency structure to filter the hierarchical rule ta-
ble. A well-formed dependency structure could be
either a single-rooted dependency tree or a set of
sibling trees. Although most rules are discarded
with the constraint that the target side should be
well-formed, this filtration leads to degradation in
translation performance.
As an extension of the work of (Shen et
al., 2008), we introduce the so-called relaxed-
well-formed dependency structure to filter the hi-
erarchical rule table. Given a sentence S =
w1w2...wn. Let d1d2...dn represent the position of
parent word for each word. For example, d3 = 4
means that w3 depends on w4. If wi is a root, we
define di = ?1.
Definition A dependency structure wi...wj is
a relaxed-well-formed structure, where there is
h /? [i, j], all the words wi...wj are directly or
indirectly depended on wh or -1 (here we define
h = ?1). If and only if it satisfies the following
conditions
? dh /? [i, j]
? ?k ? [i, j], dk ? [i, j] or dk = h
From the definition above, we can see that
the relaxed-well-formed structure obviously cov-
ers the well-formed one. In this structure, we
don?t constrain that all the children of the sub-root
should be complete. Let?s review the dependency
tree in Figure 2 as an example. Except for the well-
formed structure, we could also extract girl found
a beautiful house. Therefore, if the modifier The
lovely changes to The cute, this rule also works.
4 Head Word Trigger
(Koehn et al, 2003) introduced the concept of
lexical weighting to check how well words of
the phrase translate to each other. Source word
f aligns with target word e, according to the
IBM model 1, the lexical translation probability
is p(e|f). However, in the sense of dependency
relationship, we believe that the generation of the
target word e, is not only triggered by the aligned
source word f , but also associated with f ?s head
word f ?. Therefore, the lexical translation prob-
ability becomes p(e|f ? f ?), which of course
allows for a more fine-grained lexical choice of
143
the target word. More specifically, the probabil-
ity could be estimated by the maximum likelihood
(MLE) approach,
p(e|f ? f ?) = count(e, f ? f
?)
?
e? count(e?, f ? f ?)
(1)
Given a phrase pair f , e and word alignment
a, and the dependent relation of the source sen-
tence dJ1 (J is the length of the source sentence,
I is the length of the target sentence). Therefore,
given the lexical translation probability distribu-
tion p(e|f ? f ?), we compute the feature score of
a phrase pair (f , e) as
p(e|f, dJ1 , a)
= ?|e|i=1
1
|{j|(j, i) ? a}|
?
?(j,i)?a
p(ei|fj ? fdj) (2)
Now we get p(e|f, dJ1 , a), we could obtain
p(f |e, dI1, a) (dI1 represents dependent relation of
the target side) in the similar way. This new fea-
ture can be easily integrated into the log-linear
model as lexical weighting does.
5 Experiments
In this section, we describe the experimental set-
ting used in this work, and verify the effect of
the relaxed-well-formed structure filtering and the
new feature, head word trigger.
5.1 Experimental Setup
Experiments are carried out on the NIST1
Chinese-English translation task with two differ-
ent size of training corpora.
? FBIS: We use the FBIS corpus as the first
training corpus, which contains 239K sen-
tence pairs with 6.9M Chinese words and
8.9M English words.
? GQ: This is manually selected from the
LDC2 corpora. GQ contains 1.5M sentence
pairs with 41M Chinese words and 48M En-
glish words. In fact, FBIS is the subset of
GQ.
1www.nist.gov/speech/tests/mt
2It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08, LDC2005T06.
For language model, we use the SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram model on the first 1/3 of the Xinhua portion
of GIGAWORD corpus. And we use the NIST
2002 MT evaluation test set as our development
set, and NIST 2004, 2005 test sets as our blind
test sets. We evaluate the translation quality us-
ing case-insensitive BLEU metric (Papineni et
al., 2002) without dropping OOV words, and the
feature weights are tuned by minimum error rate
training (Och, 2003).
In order to get the dependency relation of the
training corpus, we re-implement a beam-search
style monolingual dependency parser according
to (Nivre and Scholz, 2004). Then we use the
same method suggested in (Chiang, 2005) to
extract SCFG grammar rules within dependency
constraint on both sides except that unaligned
words are allowed at the edge of phrases. Pa-
rameters of head word trigger are estimated as de-
scribed in Section 4. As a default, the maximum
initial phrase length is set to 10 and the maximum
rule length of the source side is set to 5. Besides,
we also re-implement the decoder of Hiero (Chi-
ang, 2007) as our baseline. In fact, we just exploit
the dependency structure during the rule extrac-
tion phase. Therefore, we don?t need to change
the main decoding algorithm of the SMT system.
5.2 Results on FBIS Corpus
A series of experiments was done on the FBIS cor-
pus. We first parse the bilingual languages with
monolingual dependency parser respectively, and
then only retain the rules that both sides are in line
with the constraint of dependency structure. In
Table 1, the relaxed-well-formed structure filtered
out 35% of the rule table and the well-formed dis-
carded 74%. RWF extracts additional 39% com-
pared to WF, which can be seen as some kind
of evidence that the rules we additional get seem
common in the sense of linguistics. Compared to
(Shen et al, 2008), we just use the dependency
structure to constrain rules, not to maintain the tree
structures to guide decoding.
Table 2 shows the translation result on FBIS.
We can see that the RWF structure constraint can
improve translation quality substantially both at
development set and different test sets. On the
Test04 task, it gains +0.86% BLEU, and +0.84%
on Test05. Besides, we also used Shen et al
(2008)?s WF structure to filter both sides. Al-
though it discard about 74% of the rule table, the
144
System Rule table size
HPB 30,152,090
RWF 19,610,255
WF 7,742,031
Table 1: Rule table size with different con-
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means
relaxed-well-formed constraint and WF represents
the well-formed structure.
System Dev02 Test04 Test05
HPB 0.3285 0.3284 0.2965
WF 0.3125 0.3218 0.2887
RWF 0.3326 0.3370** 0.3050
RWF+Tri 0.3281 / 0.2965
Table 2: Results of FBIS corpus. Here Tri means
the feature of head word trigger on both sides. And
we don?t test the new feature on Test04 because of
the bad performance on development set. * or **
= significantly better than baseline (p < 0.05 or
0.01, respectively).
over-all BLEU is decreased by 0.66%-0.78% on
the test sets.
As for the feature of head word trigger, it seems
not work on the FBIS corpus. On Test05, it gets
the same score with the baseline, but lower than
RWF filtering. This may be caused by the data
sparseness problem, which results in inaccurate
parameter estimation of the new feature.
5.3 Result on GQ Corpus
In this part, we increased the size of the training
corpus to check whether the feature of head word
trigger works on large corpus.
We get 152M rule entries from the GQ corpus
according to (Chiang, 2007)?s extraction method.
If we use the RWF structure to constrain both
sides, the number of rules is 87M, about 43% of
rule entries are discarded. From Table 3, the new
System Dev02 Test04 Test05
HPB 0.3473 0.3386 0.3206
RWF 0.3539 0.3485** 0.3228
RWF+Tri 0.3540 0.3607** 0.3339*
Table 3: Results of GQ corpus. * or ** = sig-
nificantly better than baseline (p < 0.05 or 0.01,
respectively).
feature works well on two different test sets. The
gain is +2.21% BLEU on Test04, and +1.33% on
Test05. Compared to the result of the baseline,
only using the RWF structure to filter performs the
same as the baseline on Test05, and +0.99% gains
on Test04.
6 Conclusions
This paper proposes a simple strategy to filter the
hierarchal rule table, and introduces a new feature
to enhance the translation performance. We em-
ploy the relaxed-well-formed dependency struc-
ture to constrain both sides of the rule, and about
40% of rules are discarded with improvement of
the translation performance. In order to make full
use of the dependency information, we assume
that the target word e is triggered by dependency
edge of the corresponding source word f . And
this feature works well on large parallel training
corpus.
How to estimate the probability of head word
trigger is very important. Here we only get the pa-
rameters in a generative way. In the future, we we
are plan to exploit some discriminative approach
to train parameters of this feature, such as EM al-
gorithm (Hasan et al, 2008) or maximum entropy
(He et al, 2008).
Besides, the quality of the parser is another ef-
fect for this method. As the next step, we will
try to exploit bilingual knowledge to improve the
monolingual parser, i.e. (Huang et al, 2009).
Acknowledgments
This work was partly supported by National
Natural Science Foundation of China Contract
60873167. It was also funded by SK Telecom,
Korea under the contract 4360002953. We show
our special thanks to Wenbin Jiang and Shu Cai
for their valuable suggestions. We also thank
the anonymous reviewers for their insightful com-
ments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
145
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
extended lexicon models in search and rescoring for
smt. In NAACL ?09: Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 17?20.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet lexicon models
for statistical machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 372?
381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 321?328.
Zhongjun He, Yao Meng, Yajuan Lu?, Hao Yu, and Qun
Liu. 2009. Reducing smt rule table with monolin-
gual key phrase. In ACL-IJCNLP ?09: Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 121?124.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1222?1231.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 380?388.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
?04: Proceedings of the 20th international confer-
ence on Computational Linguistics, pages 64?70.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL ?05: Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 72?80.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901?904.
146
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364?369,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stem Translation with Affix-Based Rule Selection
for Agglutinative Languages
Zhiyang Wang?, Yajuan Lu??, Meng Sun?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{wangzhiyang,lvyajuan,sunmeng,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Current translation models are mainly de-
signed for languages with limited mor-
phology, which are not readily applicable
to agglutinative languages as the differ-
ence in the way lexical forms are gener-
ated. In this paper, we propose a nov-
el approach for translating agglutinative
languages by treating stems and affixes
differently. We employ stem as the atomic
translation unit to alleviate data spare-
ness. In addition, we associate each stem-
granularity translation rule with a distri-
bution of related affixes, and select desir-
able rules according to the similarity of
their affix distributions with given spans to
be translated. Experimental results show
that our approach significantly improves
the translation performance on tasks of
translating from three Turkic languages to
Chinese.
1 Introduction
Currently, most methods on statistical machine
translation (SMT) are developed for translation
of languages with limited morphology (e.g., En-
glish, Chinese). They assumed that word was the
atomic translation unit (ATU), always ignoring the
internal morphological structure of word. This
assumption can be traced back to the original
IBM word-based models (Brown et al, 1993) and
several significantly improved models, including
phrase-based (Och and Ney, 2004; Koehn et al,
2003), hierarchical (Chiang, 2005) and syntac-
tic (Quirk et al, 2005; Galley et al, 2006; Liu et
al., 2006) models. These improved models worked
well for translating languages like English with
large scale parallel corpora available.
Different from languages with limited morphol-
ogy, words of agglutinative languages are formed
mainly by concatenation of stems and affixes.
Generally, a stem can attach with several affixes,
thus leading to tens of hundreds of possible inflect-
ed variants of lexicons for a single stem. Modeling
each lexical form as a separate word will generate
high out-of-vocabulary rate for SMT. Theoretical-
ly, ways like morphological analysis and increas-
ing bilingual corpora could alleviate the problem
of data sparsity, but most agglutinative languages
are less-studied and suffer from the problem of
resource-scarceness. Therefore, previous research
mainly focused on the different inflected variants
of the same stem and made various transformation
of input by morphological analysis, such as (Lee,
2004; Goldwater and McClosky, 2005; Yang and
Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza
and Federico, 2009; Wang et al, 2011). These
work still assume that the atomic translation unit
is word, stem or morpheme, without considering
the difference between stems and affixes.
In agglutinative languages, stem is the base
part of word not including inflectional affixes.
Affix, especially inflectional affix, indicates dif-
ferent grammatical categories such as tense, per-
son, number and case, etc., which is useful for
translation rule disambiguation. Therefore, we
employ stem as the atomic translation unit and
use affix information to guide translation rule
selection. Stem-granularity translation rules have
much larger coverage and can lower the OOV
rate. Affix based rule selection takes advantage
of auxiliary syntactic roles of affixes to make a
better rule selection. In this way, we can achieve
a balance between rule coverage and matching
accuracy, and ultimately improve the translation
performance.
364
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
(B)Translation rules with affix distribution
zunyi yighin ||| ????? ||| i:0 gha:0.09 zunyi yighin ||| ?????? ||| i:0 da:0.24
zunyi
/STM
i
/SUF
yighin
/STM
da
/SUF
zunyi yighin ||| ?????? ||| i da
(A) Instances of translation rule
(1) (2)
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
(3)
Original:zunyi yighin+i+da
Meaning:on zunyi conference
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
Figure 1: Translation rule extraction from Uyghur to Chinese. Here tag ?/STM? represents stem and
?/SUF? means suffix.
2 Affix Based Rule Selection Model
Figure 1 (B) shows two translation rules along
with affix distributions. Here a translation rule
contains three parts: the source part (on stem lev-
el), the target part, and the related affix distribution
(represented as a vector). We can see that, al-
though the source part of the two translation rules
are identical, their affix distributions are quite
different. Affix ?gha? in the first rule indicates
that something is affiliated to a subject, similar to
?of? in English. And ?da? in second rule implies
location information. Therefore, given a span
?zunyi/STM yighin/STM+i/SUF+da/SUF+...? to
be translated, we hope to encourage our model to
select the second translation rule. We can achieve
this by calculating similarity between the affix
distributions of the translation rule and the span.
The affix distribution can be obtained by keep-
ing the related affixes for each rule instance during
translation rule extraction ((A) in Figure 1). After
extracting and scoring stem-granularity rules in a
traditional way, we extract stem-granularity rules
again by keeping affix information and compute
the affix distribution with tf-idf (Salton and Buck-
ley, 1987). Finally, the affix distribution will be
added to the previous stem-granularity rules.
2.1 Affix Distribution Estimation
Formally, translation rule instances with the same
source part can be treated as a document collec-
tion1, so each rule instance in the collection is
1We employ concepts from text classification to illustrate
how to estimate affix distribution.
some kind of document. Our goal is to classify the
source parts into the target parts on the document
collection level with the help of affix distribu-
tion. Accordingly, we employ vector space model
(VSM) to represent affix distribution of each rule
instance. In this model, the feature weights are
represented by the classic tf-idf (Salton and Buck-
ley, 1987):
tf i,j =
ni,j?
k nk,j
idf i,j = log
|D|
|j : ai ? rj|
tfidf i,j = tf i,j ? idf i,j
(1)
where tfidf i,j is the weight of affix ai in transla-
tion rule instance rj . ni,j indicates the number of
occurrence of affix ai in rj . |D| is the number
of rule instance with the same source part, and
|j : ai ? rj| is the number of rule instance which
contains affix ai within |D|.
Let?s take the suffix ?gha? from (A1) in Figure
1 as an example. We assume that there are only
three instances of translation rules extracted from
parallel corpus ((A) in Figure 1). We can see that
?gha? only appear once in (A1) and also appear
once in whole instances. Therefore, tfgha,(A1) is
0.5 and idfgha,(A1) is log(3/2). tfidfgha,(A1) is
the product of tfgha,(A1) and idfgha,(A1) which
is 0.09.
Given a set of N translation rule instances with
the same source and target part, we define the
centroid vector dr according to the centroid-based
classification algorithm (Han and Karypis, 2000),
dr =
1
N
?
i?N
di (2)
365
Data set #Sent. #Type #Tokenword stem morph word stem morph
UY-CH-Train. 50K 69K 39K 42K 1.2M 1.2M 1.6M
UY-CH-Dev. 0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5K
UY-CH-Test. 0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8K
KA-CH-Train. 50K 62K 40K 42K 1.1M 1.1M 1.3M
KA-CH-Dev. 0.7K*4 5.3K 4.2K 4.5K 15K 15K 18K
KA-CH-Test. 0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8K
KI-CH-Train. 50K 53K 27K 31K 1.2M 1.2M 1.5M
KI-CH-Dev. 0.5K*4 4.1K 3.1K 3.5K 12K 12K 15K
KI-CH-Test. 0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8K
Table 1: Statistics of data sets. ?N means the number of reference, morph is short to morpheme. UY,
KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.
dr is the final affix distribution.
By comparing the similarity of affix distribu-
tions, we are able to decide whether a translation
rule is suitable for a span to be translated. In
this work, similarity is measured using the cosine
distance similarity metric, given by
sim(d1,d2) =
d1 ? d2
?d1? ? ?d2?
(3)
where di corresponds to a vector indicating affix
distribution, and ??? denotes the inner product of
the two vectors.
Therefore, for a specific span to be translated,
we first analyze it to get the corresponding stem
sequence and related affix distribution represented
as a vector. Then the stem sequence is used to
search the translation rule table. If the source part
is matched, the similarity will be calculated for
each candidate translation rule by cosine similarity
(as in equation 3). Therefore, in addition to the
traditional translation features on stem level, our
model also adds the affix similarity score as a
dynamic feature into the log-linear model (Och
and Ney, 2002).
3 Related Work
Most previous work on agglutinative language
translation mainly focus on Turkish and Finnish.
Bisazza and Federico (2009) and Mermer and
Saraclar (2011) optimized morphological analysis
as a pre-processing step to improve the translation
between Turkish and English. Yeniterzi and Oflaz-
er (2010) mapped the syntax of the English side
to the morphology of the Turkish side with the
factored model (Koehn and Hoang, 2007). Yang
and Kirchhoff (2006) backed off surface form to
stem when translating OOV words of Finnish.
Luong and Kan (2010) and Luong et al (2010)
focused on Finnish-English translation through
improving word alignment and enhancing phrase
table. These works still assumed that the atomic
translation unit is word, stem or morpheme, with-
out considering the difference between stems and
affixes.
There are also some work that employed the
context information to make a better choice of
translation rules (Carpuat and Wu, 2007; Chan et
al., 2007; He et al, 2008; Cui et al, 2010). all the
work employed rich context information, such as
POS, syntactic, etc., and experiments were mostly
done on less inflectional languages (i.e. Chinese,
English) and resourceful languages (i.e. Arabic).
4 Experiments
In this work, we conduct our experiments on
three different agglutinative languages, including
Uyghur, Kazakh and Kirghiz. All of them are
derived from Altaic language family, belonging to
Turkic languages, and mostly spoken by people in
Central Asia. There are about 24 million people
take these languages as mother tongue. All of
the tasks are derived from the evaluation of Chi-
na Workshop of Machine Translation (CWMT)2.
Table 1 shows the statistics of data sets.
For the language model, we use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
a 5-gram model with the target side of training
corpus. And phrase-based Moses3 is used as our
2http://mt.xmu.edu.cn/cwmt2011/en/index.html.
3http://www.statmt.org/moses/
366
UY-CH KA-CH KI-CH
word 31.74+0.0 28.64+0.0 35.05+0.0
stem 33.74+2.0 30.14+1.5 35.52+0.47
morph 32.69+0.95 29.21+0.57 34.97?0.08
affix 34.34+2.6 30.19+2.27 35.96+0.91
Table 2: Translation results from Turkic languages
to Chinese. word: ATU is surface form,
stem: ATU is represented stem, morph: ATU
denotes morpheme, affix: stem translation with
affix distribution similarity. BLEU scores in
bold means significantly better than the baseline
according to (Koehn, 2004) for p-value less than
0.01.
baseline SMT system. The decoding weights are
optimized with MERT (Och, 2003) to maximum
word-level BLEU scores (Papineni et al, 2002).
4.1 Using Unsupervised Morphological
Analyzer
As most agglutinative languages are resource-
poor, we employ unsupervised learning method
to obtain the morphological structure. Follow-
ing the approach in (Virpioja et al, 2007), we
employ the Morfessor4 Categories-MAP algorith-
m (Creutz and Lagus, 2005). It applies a hierar-
chical model with three categories (prefix, stem,
and suffix) in an unsupervised way. From Table 1
we can see that vocabulary sizes of the three lan-
guages are reduced obviously after unsupervised
morphological analysis.
Table 2 shows the translation results. All the
three translation tasks achieve obvious improve-
ments with the proposed model, which always per-
forms better than only employ word, stem and
morph. For the Uyghur to Chinese translation
(UY-CH) task in Table 2, performances after unsu-
pervised morphological analysis are always better
than the baseline. And we gain up to +2.6 BLEU
points improvements with affix compared to the
baseline. For the Kazakh to Chinese translation
(KA-CH) task, the improvements are also signifi-
cant. We achieve +2.27 and +0.77 improvements
compared to the baseline and stem, respectively.
As for the Kirghiz to Chinese translation (KI-CH)
task, improvements seem relative small compared
to the other two language pairs. However, it also
gains +0.91 BLEU points over the baseline.
4http://www.cis.hut.fi/projects/morpho/
UY Unsup Sup
stem #Type 39K 21K#Token 1.2M 1.2M
affix #Type 3.0K 0.3K#Token 0.4M 0.7M
Table 3: Statistics of training corpus after unsuper-
vised(Unsup) and supervised(Sup) morphological
analysis.
 31.5 32
 32.5 33
 33.5 34
 34.5 35
 35.5 36
 36.5
word morph stem affix
B
L
E
U
 
s
c
o
r
e
(
%
)
UnsupervisedSupervised
Figure 2: Uyghur to Chinese translation results
after unsupervised and supervised analysis.
4.2 Using Supervised Morphological
Analyzer
Taking it further, we also want to see the effect of
supervised analysis on our model. A generative
statistical model of morphological analysis for
Uyghur was developed according to (Mairehaba
et al, 2012). Table 3 shows the difference of
statistics of training corpus after supervised and
unsupervised analysis. Supervised method gen-
erates fewer type of stems and affixes than the
unsupervised approach. As we can see from
Figure 2, except for the morph method, stem
and affix based approaches perform better after
supervised analysis. The results show that our
approach can obtain even better translation per-
formance if better morphological analyzers are
available. Supervised morphological analysis gen-
erates more meaningful morphemes, which lead to
better disambiguation of translation rules.
5 Conclusions and Future Work
In this paper we propose a novel framework for
agglutinative language translation by treating stem
and affix differently. We employ the stem se-
quence as the main part for training and decod-
ing. Besides, we associate each stem-granularity
translation rule with an affix distribution, which
could be used to make better translation decisions
by calculating the affix distribution similarity be-
367
tween the rule and the instance to be translated.
We conduct our model on three different language
pairs, all of which substantially improved the
translation performance. The procedure is totally
language-independent, and we expect that other
language pairs could benefit from our approach.
Acknowledgments
The authors were supported by 863 State
Key Project (No. 2011AA01A207), and
National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge
Innovation Program of Chinese Academy of
Sciences (No. KGZD-EW-501). Qun Liu?s work
is partially supported by Science Foundation
Ireland (Grant No.07/CE/I1142) as part of the
CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their
insightful comments and those who helped to
modify the paper.
References
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English
statistical machine translation. In Proceedings of
IWSLT, pages 129?135.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
Marine Carpuat and Dekai Wu. 2007. Improving
statistical machine translation using word sense
disambiguation. In Proceedings of EMNLP-CoNLL,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves
statistical machine translation. In Proceedings of
ACL, pages 33?40.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proceedings of ACL, pages 263?270.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR, pages
106?113.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2010. A joint rule selection
model for hierarchical phrase-based translation. In
Proceedings of ACL, Short Papers, pages 6?11.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of COLING/ACL, pages 961?968.
Sharon Goldwater and David McClosky. 2005.
Improving statistical MT through morphological
analysis. In Proceedings of HLT-EMNLP, pages
676?683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of NAACL, Short Papers, pages 49?
52.
Eui-Hong Sam Han and George Karypis. 2000.
Centroid-based document classification: analysis
experimental results. In Proceedings of PKDD,
pages 424?431.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008.
Improving statistical machine translation using
lexicalized rule selection. In Proceedings of
COLING, pages 321?328.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of EMNLP-
CoNLL, pages 868?876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of NAACL, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT-NAACL, Short Papers, pages 57?60.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609?616.
Minh-Thang Luong and Min-Yen Kan. 2010.
Enhancing morphological alignment for translating
highly inflected languages. In Proceedings of
COLING, pages 743?751.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich
languages. In Proceedings of EMNLP, pages 148?
157.
Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibu-
layin Tuergen, and Qun Liu. 2012. Directed graph
model of Uyghur morphological analysis. Journal
of Software, 23(12):3115?3129.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish morphological segmentation for
statistical machine translation. In Workshop of MT
and Morphologically-rich Languages.
368
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to statistical machine
translation. Comput. Linguist., pages 417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: syntactically
informed phrasal SMT. In Proceedings of ACL,
pages 271?279.
Gerard Salton and Chris Buckley. 1987. Term
weighting approaches in automatic text retrieval.
Technical report.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Proceedings of
ICSLP, pages 311?318.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs
induced in an unsupervised manner. In Proceedings
of MT SUMMIT, pages 491?498.
Zhiyang Wang, Yajuan Lu?, and Qun Liu. 2011.
Multi-granularity word alignment and decoding for
agglutinative language translation. In Proceedings
of MT SUMMIT, pages 360?367.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly
inflected languages. In Proceedings of EACL, pages
1017?1020.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-morphology mapping in factored phrase-based
statistical machine translation from English to
Turkish. In Proceedings of ACL, pages 454?464.
369
