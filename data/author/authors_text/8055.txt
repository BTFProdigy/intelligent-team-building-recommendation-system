Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 492?495,
Prague, June 2007. c?2007 Association for Computational Linguistics
XRCE-T: XIP temporal module for TempEval campaign 
Caroline Hag?ge 
XEROX Research Centre Europe 
6, chemin de Maupertuis 
38240 MEYLAN, FRANCE 
Caroline.Hagege@xrce.xerox.com
Xavier Tannier 
XEROX Research Centre Europe 
6, chemin de Maupertuis 
38240 MEYLAN, FRANCE 
Xavier.Tannier@xrce.xerox.com 
 
Abstract 
We present the system we used for the 
TempEval competition. This system relies 
on a deep syntactic analyzer that has been 
extended for the treatment of temporal ex-
pressions, thus making temporal processing 
a complement to a better general purpose 
text understanding system.  
1 General presentation and system over-
view 
Although interest in temporal and aspectual phe-
nomena is not new in NLP and AI, temporal proc-
essing of real texts is a topic that has been of grow-
ing interest in the last years (Mani et al 2005).  
The work we have done concerning temporal 
processing of texts is part of a more general proc-
ess in text understanding, integrated into a more 
generic tool.   
In this article, we present briefly our general 
purpose analyzer XIP and explain how we perform 
our three-level temporal processing. TempEval 
experiments of our system are finally described 
and results we obtained are discussed.  
1.1 XIP ? a general purpose deep syntactic 
analyzer 
Our temporal processor, called XTM, is an exten-
sion of XIP (Xerox Incremental Parser (A?t Mok-
htar et al, 2002). XIP extracts basic grammatical 
relations and also thematic roles in the form of de-
pendency links. See (Brun and Hag?ge 2003) for 
details on deep linguistic processing using XIP. 
XIP is rule-based and its architecture can roughly 
be divided into the three following parts: 
? A pre-processing stage handling tokeniza-
tion, morphological analysis and POS tag-
ging. 
? A surface syntactic analysis stage consist-
ing in chunking the input and dealing with 
Named Entity Recognition (NER).  
? A deep syntactic analysis  
1.2 Intertwining temporal processing and 
linguistic processing 
The underlying idea is that temporal processing is 
one of the necessary steps in a more general task of 
text understanding. All temporal processing at the 
sentence level is performed together with other 
tasks of linguistic analysis. Association between 
temporal expressions and events is considered as a 
particular case of the more general task of attach-
ing thematic roles to predicates (the TIME and 
DURATION roles). We will detail in sections 3.1 
and 3.2 how low-level temporal processing is com-
bined with the rest of the linguistic processing.  
 
2 Three levels of temporal processing 
Temporal processing has the following purposes:  
1) Recognizing and interpreting temporal expres-
sions, 2) Attaching these expressions to the corre-
sponding events1 they modify, 3) Ordering these 
events using a set of temporal expressions we pre-
sent above.  
We deliberately decided not to change our sys-
tem?s output in order to match TempEval gold-
standard EVENTs and TIMEX3s. This would have 
                                                 
1 We consider as events: verbs, deverbal nouns or any 
kind of non-deverbal nouns from a pre-defined list (e.g.: 
?sunrise? or ?war?). 
492
implied to change our parser?s behavior. As linking 
events and temporal expressions is only a part of a 
general syntactico-semantic process, changing this 
part would have had bad consequences for the 
other aspects of the parsing. 
4 years ago 
- DURATION 
4Y
- TEMPORAL RELATION 
BEFORE 
- REFERENT 
ST (Speech Time) 
4Y, BEFORE, ST 
(4 years before ST) 
ADV[tempexpr:+,anchor:+] =  
#1[dur], adv#2[temp_rel,temp_ref], 
where(merge anchor and dur(#2,#1,#0))
Figure 1: Local level processing, anchor date. 
 
2.1 Local level 
Recognition of temporal expressions is performed 
by local rules that can make use of left and/or right 
context. Together with contextual rules, some ac-
tions are associated. These actions are meant to 
attribute a value to the resulting temporal expres-
sion. Figure 1 illustrates this stage for a simple an-
chor date. An ADV (adverbial) node with associ-
ated Boolean features is built from linguistic ex-
pressions such as ?4 years ago?. Note that there is 
a call to a Python function (Roux, 2006) 
?merge_anchor_and_dur? whose parameters are 
three linguistic nodes (#0 represents the resulting 
left-hand expression). The representation of the 
values is close to TimeML format (Saur? et al 
2006). 
 
2.2 Sentence level 
The sentence level is the place where some links 
between temporal expressions and the events they 
modify are established, as well as temporal rela-
tions between events in a same sentence.  
 
Attaching temporal expressions to events 
As a XIP grammar is developed in an incremental 
way, at a first stage, any prepositional phrase (PP, 
included temporal PP) is attached to the predicate 
it modifies through a very general MOD (modifier) 
dependency link. Then, in a later stage, these de-
pendency links are refined considering the nature 
and the linguistic properties of the linked constitu-
ents.   
In the case of temporal expressions, a specific 
relation TEMP links the temporal expression and 
the predicate it is attached to. 
For instance, in the following sentence (ex-
tracted from trial data): 
People began gathering in Abuja 
Tuesday for the two day rally. 
 
The following dependencies are extracted  
TEMP(began, Tuesday) 
TEMP(rally, two day) 
 
?Tuesday?is recognized as a date and ?two day? 
as a duration. 
 
Temporal relations between events in the 
same sentence 
Using the temporal relations presented above, 
the system can detect in certain syntactic configu-
rations if predicates in the sentence are temporally 
related and what kind of relations exist between 
them. When it is explicit in the text, a temporal 
distance between the two events is also calculated. 
The following example illustrates these temporal 
dependencies: 
This move comes a month after 
Qantas suspended a number of 
services. 
 
In this sentence, the clause containing the verb 
?suspended? is embedded into the main clause 
headed by ?comes?. These two events have a tem-
poral distance of one month, which is expressed by 
the expression ?a month after?. We obtain the fol-
lowing dependencies: 
ORDER[before](suspended, comes) 
DELTA(suspended, comes, a month) 
 
Verbal tenses and aspect 
Morphological analysis gives some information 
about tenses. But the final tense of a complex ver-
bal chain is calculated considering not only mor-
phological clues, but also aspectual information.  
Tenses of complex verbal chains may be under-
specified when there is insufficient context. 
493
For instance, for the chain ?has been taken?, we 
extract ?take? as the semantic head of the verbal 
chain. The aspect is perfective and the tense of the 
auxiliary ?has? is present.  
From this information, we deduce that this form is 
either in present or in past. This is expressed the 
following way: 
PRES-OR-PAST(taken). 
 
2.3 Document level 
Beyond sentence-level, the system is at the first 
stage of development. We are only able to com-
plete relative dates when it refers to the document 
creation time, and to infer new relations with the 
help of composition rules, by saturating the graph 
of temporal relations (Muller and Tannier, 2004). 
 
3 Adapting XTM to TempEval specifica-
tions 
The TempEval track consists of three different 
tasks described in (Verhagen et al 2007). Tem-
pEval guidelines present several differences with 
respect to our own methodology. These differences 
concern definitions of relations and events, as well 
as choices about linking. 
 
3.1 TIMEX3 definition 
TimeML definition of a temporal expression 
(TIMEX3) is slightly different from what we con-
sider to be a temporal expression in XTM: 
?  First, we incorporate signals (in, at?) into 
temporal expressions boundaries. But, as 
TIMEX3s are provided in the test collection, 
a simple mapping is quite easy to perform. 
?  We also have a different tokenization for 
complex temporal expressions. This tokeni-
zation is based on syntactic and semantic 
properties of the whole expression.  
For example, our criteria make that we consider 
?ten days ago yesterday" as a single temporal 
expression, while "during 10 days in Decem-
ber" should be split into "during 10 days" and 
"in December".  
3.2 TIMEX3 linking 
XTM does not handle temporal relations be-
tween events and durations. In our temporal model, 
an event can have duration. However, this is not 
represented by a temporal relation, but by an at-
tribute of the event. Durations included in a larger 
temporal expression (like in ?two days later?) in-
troduce an interval for the temporal relation: AF-
TER(A, B, interval: two days). Here again no tem-
poral relation is attributed with respect to the dura-
tion.  
Therefore, we had to adapt our system so that it 
is able to infer at least some relations between 
events and durations. We used two ways to do so: 
? An event having an explicit duration at-
tributed by XTM gets the relation OVER-
LAP with this duration. 
? An event occurring, for example, ?two 
days after another one? (resp. ?two days be-
fore?) gets the relation AFTER (resp. BE-
FORE) with this duration. 
Other relations are found (or not) by composi-
tion rules. 
 
3.3 TIMEX3 values 
TempEval test collection provides a "value" attrib-
ute for each TIMEX3. However we did not use this 
value, because we wanted to obtain an evaluation 
as close as possible to a real world application. The 
only value we used was the given Document Crea-
tion Time. 
 
3.4 EVENTs mapping 
Event lists do not match either between  
TempEval corpus and our system analysis. Unfor-
tunately, when a TempEval EVENT is not consid-
ered as an event by XTM, we did not find any suc-
cessful way to map this EVENT to another event 
of the sentence. 
 
3.5 Temporal relation mapping 
The set of temporal relations we use is the follow-
ing: AFTER, BEFORE, DURING, INCLUDES, 
OVERLAPS, IS_OVERLAPPED AND EQUALS. 
494
This choice is explained in more details in (Muller 
and Tannier, 2004). 
Obtaining TempEval relations from our own re-
lations is straightforward: AFTER and BEFORE 
are kept just as they are. The other relations or dis-
junctions of these relations are turned into OVER-
LAP. Disjunctions of relations containing AFTER 
(resp. BEFORE) and OVERLAP-like relations are 
turned into OVERLAP-OR-AFTER (resp. BE-
FORE-OR-OVERLAP). 
4 Results  
The trial, training and test sets of document pro-
vided were all subsets of the annotated TimeBank 
corpus. For each task, two metrics are used, the 
strict measure and the relaxed measure (see also 
(Muller and Tannier, 2004)). 
Our rule-based analyzer is designed to favor 
precision. As our system is intended for use in in-
formation extraction, finding correct relations is 
more important than finding a large number of re-
lations. That is why, at least for tasks A and B, we 
do not assign a temporal relation when the parser 
does not find any link. For the same reason, in our 
opinion, the strict measure is not as valuable as the 
relaxed one. We would argue that it does not really 
make sense to use a strict metric in combination 
with disjunctive relations. 
Tasks A and B were evaluated together. We ob-
tained the best precision for relaxed matching 
(0.79), but with a low recall (respectively 0.50). 
Strict matching is not very different. Another inter-
esting figure is that less than 10% of the relations 
are totally incorrect (e.g.: BEFORE instead of AF-
TER). As we said, this was our main aim. 
Note that if we choose a default behavior 
(OVERLAP for task A, BEFORE for task B, 
which are respectively the most frequent relations) 
for every undefined relation, we obtain precision 
and recall of 0.69, which is lower than but not far 
from the best team results. 
Task C was more exploratory. Even more than 
for task AB, the fact that we chose not to use the 
provided TIMEX3 values makes the problem 
harder. Our gross results are quite low. We used a 
default OVERLAP for each unfound relation2 and 
finally got equal precision and recall of 0.57. 
                                                 
2 The OVERLAP relation is the most frequent for task C 
training data. 
However, assigning OVERLAP to all 258 links 
led to precision and recall of 0.508; no team man-
aged to bring a satisfying trade-off in this task. 
5 Conclusion 
We described in this paper the system that we 
adapted in order to participate to TempEval 2007 
evaluation campaign. We obtained a good preci-
sion score and a very low rate of incorrect relations, 
which makes the tool robust enough for informa-
tion extraction applications. Errors and low recall 
are mostly due to parsing errors or underspecifica-
tion and to the fact that we gave priority to our 
own theoretical choices concerning event and tem-
poral expression definitions and event-temporal 
expression linking.  
References 
James Allen, 1984. Toward a general theory of action 
and time. Artificial Intelligence, 23:123-154. 
Salah A?t-Mokhtar, Jean-Pierre Chanod and Claude 
Roux. 2002. Robustness beyond Shallowness: Incre-
mental Deep  Parsing. Natural Language Engineer-
ing, 8 :121-144 
Caroline Brun and Caroline Hagege, 2003. Normaliza-
tion and Paraphrasing using Symbolic Methods, 2nd 
Workshop on Paraphrasing, ACL 2003. 
Inderjeet Mani, James Pustejovsky and Robert Gai-
zauskas (ed.) 2005. The Language of Time A reader.  
Philippe Muller and Xavier Tannier 2004. Annotating 
and measuring temporal relations in texts. In Pro-
ceedings of COLING 2004. 
James Pustejovsky, Patrick Hanks, Roser Saur?, Andrew 
See, Robert Gaizauskas, Andrea Setzer and Beth 
Sundheim. 2003. The TIMEBANK Corpus. Corpus 
Linguistics. Lancaster, U.K. 
Claude Roux. 2006. Coupling a linguistic formalism 
and a script language. CSLP-06, Coling-ACL. 
Roser Saur?, Jessica  Littman, Bob Knippen, Robert 
Gaizauskas, Andrea Setzer and James Pustejovsky. 
TimeML Annotation Guidelines. 2006. 
Marc Verhagen, Robert Gaizauskas, Frank Schilder, 
Mark Hepple, Graham Katz and James Pustejovsky. 
2007. SemEval-2007 ? Task 15: TempEval Temporal 
Relation Identification. SemEval workshop in ACL 
2007. 
495
Annotating and measuring temporal relations in texts
Philippe Muller Xavier Tannier
IRIT, Universit? Paul Sabatier IRIT, Universit? Paul Sabatier
Toulouse, France Toulouse, France
muller@irit.fr tannier@emse.fr
Abstract
This paper focuses on the automated processing
of temporal information in written texts, more
specifically on relations between events intro-
duced by verbs in finite clauses. While this
problem has been largely studied from a the-
oretical point of view, it has very rarely been
applied to real texts, if ever, with quantified re-
sults. The methodology required is still to be
defined, even though there have been proposals
in the strictly human annotation case. We pro-
pose here both a procedure to achieve this task
and a way of measuring the results. We have
been testing the feasibility of this on newswire
articles, with promising results.
1 Annotating temporal information
This paper focuses on the automated annotation of
temporal information in texts, more specifically re-
lations between events introduced by finite verbs.
While the semantics of temporal markers and the
temporal structure of discourse are well-developed
subjects in formal linguistics (Steedman, 1997),
investigation of quantifiable annotation of unre-
stricted texts is a somewhat recent topic. The is-
sue has started to generate some interest in com-
putational linguistics (Harper et al, 2001), as it is
potentially an important component in information
extraction or question-answer systems. A few tasks
can be distinguished in that respect:
? detecting dates and temporal markers
? detecting event descriptions
? finding the date of events described
? figuring out the temporal relations between
events in a text
The first task is not too difficult when looking for
dates, e.g. using regular expressions (Wilson et
al., 2001), but requires some syntactic analysis in a
larger framework (Vazov, 2001; Shilder and Habel,
2001). The second one raises more difficult, onto-
logical questions; what counts as an event is not un-
controversial (Setzer, 2001): attitude reports, such
as beliefs, or reported speech have an unclear status
in that respect. The third task adds another level of
complexity: a lot of events described in a text do
not have an explicit temporal stamp, and it is not
always possible to determine one, even when tak-
ing context into account (Filatova and Hovy, 2001).
This leads to an approach more suited to the level of
underspecification found in texts: annotating rela-
tions between events in a symbolic way (e.g. that an
event e1 is before another one e2). This is the path
chosen by (Katz and Arosio, 2001; Setzer, 2001)
with human annotators. This, in turn, raises new
problems. First, what are the relations best suited to
that task, among the many propositions (linguistic
or logical) one can find for expressing temporal lo-
cation ? Then, how can an annotation be evaluated,
between annotators, or between a human annotator
and an automated system ? Such annotations can-
not be easy to determine automatically anyway, and
must use some level of discourse modeling (cf. the
work of (Grover et al, 1995)).
We want to show here the feasibility of such an
effort, and we propose a way of evaluating the suc-
cess or failure of the task. The next section will
precise why evaluating this particular task is not a
trivial question. Section 3 will explain the method
used to extract temporal relations, using also a form
of symbolic inference on available temporal infor-
mation (section 4). Then section 5 discusses how
we propose to evaluate the success of the task, be-
fore presenting our results (section 6).
2 Evaluating annotations
What we want to annotate is something close to the
temporal model built by a human reader of a text; as
such, it may involve some form of reasoning, based
on various cues (lexical or discursive), and may be
expressed in several ways. As was noticed by (Set-
zer, 2001), it is difficult to reach a good agreement
between human annotators, as they can express re-
lations between events in different, yet equivalent,
ways. For instance, they can say that an event e1
happens during another one e2, and that e2 happens
before e3, leaving implicit that e1 too is before e3,
while another might list explicitly all relations. One
option could be to ask for a relation between all
pairs of events in a given text, but this would be
demanding a lot from human subjects, since they
would be asked for n? (n? 1)/2 judgments, most
of which would be hard to make explicit. Another
option, followed by (Setzer, 2001) (and in a very
simplified way, by (Katz and Arosio, 2001)) is to
use a few rules of inference (similar to the exam-
ple seen in the previous paragraph), and to compare
the closures (with respect to these rules) of the hu-
man annotations. Such rules are of the form "if r1
holds between x and y, and r2 holds between y and
z, then r3 holds between x and z". Then one can
measure the agreement between annotations with
classical precision and recall on the set of triplets
(event x,event y,relation). This is certainly an im-
provement, but (Setzer, 2001) points out that hu-
mans still forget available information, so that it is
necessary to help them spell out completely the in-
formation they should have annotated. Setzer esti-
mates that an hour is needed on average for a text
with a number of 15 to 40 events.
Actually, this method has two shortcomings.
First, the choice of temporal relations proposed to
annotators, i.e. "before", "after", "during", and "si-
multaneously". The latter is all the more difficult
to judge as it lacks a precise semantics, and is de-
fined as "roughly at the same time" ((Setzer, 2001),
p.81). The second problem is related to the infer-
ential model considered, as it is only partial. Even
though the exact mental processing of such infor-
mation is still beyond reach, and thus any claim to
cognitive plausibility is questionable, there are more
precise frameworks for reasoning about temporal
information. For instance the well-studied Allen?s
relations algebra (see Figure 2). Here, relations be-
tween two time intervals are derived from all the
possibilities for the respective position of those in-
tervals endpoints (before, after or same), yielding
13 relations. What this framework can also express
are more general relations between events, such as
disjunctive relations (relation between event 1 and
event 2 is relation A or relation B), and reasoning
on such knowledge. We think it is important at
least to relate annotation relations to a clear tem-
poral model, even if this model is not directly used.
Besides, we believe that measuring agreement on
the basis of a more complete "event calculus" will
be more precise, if we accept to infer disjunctive re-
lation. Then we want to give a better score to the
annotation "A or B" when A is true, than to an an-
notation where nothing is said. Section 5 gives more
details about this problem.
We will now present our method to achieve the
task of annotating automatically event relations.
This has been tested on a small set of French
newswire texts from the Agence France Press.
3 A method for annotating temporal
relations
We will now present our method to achieve the task
of annotating automatically event relations. This
has been tested on a small set of French newswire
texts from the Agence France Press. The starting
point was raw text plus its broadcast date. We then
applied the following steps:
? part of speech tagging with Treetagger
(Schmid, 1994), with some post-processing to
locate some lexicalised prepositional phrases;
? partial parsing with a cascade of regular ex-
pressions analyzers (cf. (Abney, 1996); we
also used Abney?s Cass software to apply the
rules)1. This was done to extract dates, tem-
poral adjuncts, various temporal markers, and
to achieve a somewhat coarse clause-splitting
(one finite verb in each clause) and to attach
temporal adjuncts to the appropriate clause
(this is of course a potentially large source of
errors). Relative clauses are extracted and put
at the end of their sentence of origin, in a way
similar to (Filatova and Hovy, 2001). Table
1 gives an idea of the kind of temporal infor-
mation defined and extracted at this step and
for which potentially different temporal inter-
pretations are given (for now, temporal focus
is always the previously detected event; this is
obviously an over-simplification).
? date computation to precise temporal locations
of events associated with explicit, yet impre-
cise, temporal information, such as dates rela-
tive to the time of the text (e.g. last Monday).
? for each event associated to a temporal adjunct,
a temporal relation is established (with a date
when possible).
? a set of discourse rules is used to establish
possible relations between two events appear-
ing consecutively in the text, according to
the tenses of the verbs introducing the events.
These rules for French are similar to rules for
English proposed in (Grover et al, 1995; Song
and Cohen, 1991; Kameyama et al, 1993), but
1We have defined 89 rules, divided in 29 levels.
are expressed with Allen relations instead of a
set of ad hoc relations (see Table 1 for a sub-
set of the rules). These rules are only applied
when no temporal marker indicates a specific
relation between the two events.
? the last step consists in computing a fixed point
on the graph of relations between events recog-
nized in the text, and dates. We used a classi-
cal path-consistency algorithm (Allen, 1984).
More explanation is given section 4.
Allen relations are illustrated Figure 2. In the fol-
lowing (and Table 1) they will be abbreviated with
their first letters, adding an "i" for their inverse re-
lations. So, for instance, "before" is "b" and "after"
is "bi" (b(x,y)? bi(y,x)). Table 1 gives the disjunc-
tion of possible relations between an event e1 with
tense X and a event e2 with tense Y following e1 in
the text. This is considered as a first very simplified
discourse model. It only tries to list plausible rela-
tions between two consecutive events, when there is
no marker than could explicit that relation. For in-
stance a simple past e1 can be related with e, b, m,
s, d, f, o to a following simple past event e2 in such
a context (roughly saying that e1 is before or dur-
ing e2 or meets or overlaps it). This crude model is
only intended as a basis, which will be refined once
we have a larger set of annotated texts. This will be
enriched later with a notion of temporal focus, fol-
lowing for instance (Kameyama et al, 1993; Song
and Cohen, 1991), and a notion of temporal per-
spective necessary to capture more complex tense
interactions.
The path consistency algorithm is detailed in the
next section.
4 Inferring relations between events
X
Y
X
X
X
Y
Y
Yfinishes
before
meets
overlaps
X
X
Y
Y
equals
during
starts
X
Y
Figure 2: Allen Relations between two intervals X
and Y (Time flies from left to right)
We have argued in favor of the use of Allen rela-
tions for defining annotating temporal relations, not
only because they have a clear semantics, but also
because a lot of work has been done on inference
procedures over constraints expressed with these re-
lations. We therefore believe that a good way of
avoiding the pitfalls of choosing relations for hu-
man annotation and of defining inference patterns
for these relations is to define them from Allen rela-
tions and use relational algebra computation to infer
all possible relations between events of a text (that is
saturate the constraint graph, see below), both from
a human annotation and an annotation given by a
system, and then to compare the two. In this per-
spective, any event is considered to correspond to a
convex time interval.
The set of all relations between pairs of events is
then seen as a graph of constraints, which can be
completed with inference rules. The saturation of
the graph of relations is not done with a few hand-
crafted rules of the form (relation between e1 and
e2) + (relation between e2 and e3) gives (a simple
relation between e1 and e3) (Setzer, 2001; Katz and
Arosio, 2001) but with the use of the full algebra of
Allen relation. This will reach a more complete de-
scription of temporal information, and also gives a
way to detect inconsistencies in an annotation.
An algebra of relation can be defined on any set of
relations that are mutually exclusive (two relations
cannot hold at the same time between two entities)
and exhaustive (at least one relation must hold be-
tween two given entities). The algebra starts from a
set of base relations U= {r1, r2, ...}, and a general
relation is a subset of U, interpreted as a disjunction
of the relations it contains. From there we can de-
fine union and intersection of relations as classical
set union and intersection of the base relations they
consist of. Moreover, one can define a composition
of relations as follows:
(r1 ? r2)(x, z) ? ?y r1(x, y) ? r2(y, z)
By computing beforehand the 13?13 compositions
of base relations of U, we can compute the composi-
tion of any two general relations (because r?r ? =?
when r, r? are basic and r6= r?):
{r1, r2, ...rk} ? {s1, s2, ...sm} =
?
i,j
(ri ? sj)
Saturating the graph of temporal constraints means
applying these rules to all compatible pairs of
constraints in the graph and iterating until a fix-
point is reached. The following, so-called "path-
consistency" algorithm (Allen, 1984) ensures this
fixpoint is reached:
date(1/2) : non absolute date ("march 25th", "in June").
dateabs : absolute date "July 14th, 1789".
daterelST : date, relative to utterance time ("two years
ago").
daterelTF : date, relative to temporal focus ("3 days later").
datespecabs : absolute date, with imprecise reference ("in
the beginning of the 80s").
datespecrel : relative date, special forms (months, seasons).
dur : basic duration ("during 3 years").
dur2 : duration with two dates (from February, 11 to Octo-
ber, 27. . . ).
durabs : absolute duration ("starting July 14").
durrelST : relative duration, w.r.t utterance time ("for a
year").
durrelTF : relative duration, w.r.t temporal focus ("since").
tatom : temporal atom (three days, four years, . . . ).
Figure 1: Temporal elements extracted by shallow parsing (with examples translated from French)
e1/e2 imp pp pres sp
imp o, e, s, d, f, si, di, fi bi, mi, oi e, b o, d, s, f, e, si, di, fi
pp b, m, o, e, s, d, f b, m, o, e, s, d, f, bi, mi e, b b, m, o
pres U U b, m, o, si, di, fi, e U
sp b, m, o, e, s, d, f e, s, d, f, bi, mi e, b e, b, m, s, d, f, o
Table 1: Some Discursive temporal constraints for the main relevant tenses, sp=simple past and perfect,
imp=French imparfait, pp=past perfect, pres=present
Let
{
A = the set of all edges of the graph
N = the set of vertices of the graph
U = the disjunction of all 13 Allen relations
Rm,n = the current relation between
nodes m and n
1. changed = 0
2. for all pair of nodes (i, j) ? N ?N and for all
k ? N such that ((i, k) ? A ? (k, j) ? A)
(a) R1i,j = (Ri,k ? Rk,j)
(b) if no edge (a relation R2i,j) existed before
between i and j, then R2i,j = U
(c) intersect: Ri,j = R1i,j ? R2i,j
(d) if Ri,j = ? (inconsistency detected)
then : error
(e) if Ri,j = U (=no information) do nothing
else update edge
changed = 1
3. if changed = 1, then go back to 1.
It is to be noted that this algorithm is correct: if
it detects an inconsistency then there is really one,
but it is incomplete in general (it does not neces-
sarily detect an inconsistent situation). There are
sub-algebras for which it is also complete, but it re-
mains to be seen if any of them can be enough for
our purpose here.
5 Measuring success
In order to validate our method, we have compared
the results given by the system with a "manual" an-
notation. It is not really realistic to ask humans
(whether they are experts or not) for Allen relations
between events. They are too numerous and some
are too precise to be useful alone, and it is prob-
ably dangerous to ask for disjunctive information.
But we still want to have annotation relations with a
clear semantics, that we could link to Allen?s alge-
bra to infer and compare information about tempo-
ral situations. So we have chosen relations similar
to that of (Bruce, 1972) (as in (Li et al, 2001)), who
inspired Allen; these relations are equivalent to cer-
tain sets of Allen relations, as shown Table 2. We
thought they were rather intuitive, seem to have an
appropriate level of granularity, and since three of
them are enough to describe situations (the other 3
being the converse relations), they are not to hard to
use by naive annotators.
To abstract away from particulars of a given an-
notation for some text, and thus to be able to com-
pare the underlying temporal model described by an
annotation, we try to measure a similarity between
annotations given by a system and human annota-
tions, from the saturated graph of detected tempo-
ral relations in each case (the human graph is satu-
rated after annotation relations have been translated
as equivalent disjunctions of Allen relations). We do
not want to limit the comparison to "simple" (base)
relations, as in (Setzer, 2001), because it makes the
evaluation very dependent on the choice of rela-
tions, and we also want to have a gradual measure
of the imprecision of the system annotation. For in-
stance, finding there is a "before or during" relation
between two events is better than proposing "after"
is the human put down "before", and it is less good
BEFORE ? i ? j (i before j ? ((i b j) ? (i m j)))
AFTER ? i ? j (i after j ? ((i bi j) ? (i mi j)))
OVERLAPS ? i ? j (i overlaps j ? ((i o j)))
IS_OVERLAPPED ? i ? j (i is_overlapped j ? ((i oi j)))
INCLUDES ? i ? j (i includes j ? ((i di j) ? (i si j) ? (i fi j) ? (i e j)))
IS_INCLUDED ? i ? j (i is_included j ? ((i d j) ? (i s j) ? (i f j) ? (i e j)))
Table 2: Relations proposed for annotation
than the correct answer "before".
Actually we are after two different notions. The
first one is the consistency of the system?s annota-
tion with the human?s: the information in the text
is compatible with the system?s annotation, i.e. the
former implies the latter. The second notion is how
precise the information given by the system is. A
very disjunctive information is less precise than a
simple one, for instance (a or b or c) is less precise
than (a or b) if a correct answer is (a).
In order to measure these, we propose two ele-
mentary comparison functions between two sets of
relations S and H, where S is the annotation pro-
posed by the system and H is the annotation inferred
from what was proposed by the human.
finesse = |S?H||S| coherence =
|S?H|
|H|
The global finesse score of an annotation is the aver-
age of a measure on all edges that have information
according to the human annotation (this excludes
edges with the universal disjunction U) once the
graph is saturated, while coherence is averaged on
the set of edges that bear information according to
the system annotation.
Finesse is intended to measure the quantity of in-
formation the system gets, while coherence gives
an estimate of errors the system makes with re-
spect to information in the text. Finesse and coher-
ence thus are somewhat similar respectively to re-
call and precision, but we decided to use new terms
to avoid confusion ("precision" being an ambigu-
ous term when dealing with gradual measures, as
it could mean how close the measure is to the max-
imum 1).
Obviously if S=H on all edges, all measures are
equal to 1. If the system gives no information at
all, S is a disjunction of all relations so H ? S,
H ? S = H and coherence=1, but then finesse is
very low.
These measures can of course be used to estimate
agreement between annotators.
6 Results
In order to see whether the measures we propose
are meaningful, we have looked at how the mea-
sures behave on a text "randomly" annotated in the
following way: we have selected at random pairs of
events in a text, and for each pair we have picked a
random annotation relation. Then we have saturated
the graph of constraints and compared with the hu-
man annotation. Results are typically very low, as
shown on a newswire message taken as example Ta-
ble 3.
We have then made two series of measures: one
on annotation relations (thus disjunctions of Allen
relations are re-expressed as disjunctions of annota-
tion relations that contains them), and one on equiv-
alent Allen relations (which arguably reflects more
the underlying computation, while deteriorating the
measure of the actual task). In the first case, an
Allen relation answer equals to b or d or s between
two events is considered as ?before or is_included?
(using relations used by humans) and is compared
to an annotation of the same form.
We then used finesse and coherence to estimate
our annotation made according to the method de-
scribed in the previous sections. We tried it on a
still limited2 set of 8 newswire texts (from AFP),
for a total of 2300 words and 160 events, compar-
ing to the English corpus of (Setzer, 2001), which
has 6 texts for less than 2000 words and also about
160 events. Each one of these texts has between 10
and 40 events. The system finds them correctly with
precision and recall around 97%. We made the com-
parison only on the correctly recognized events, in
order to separate the problems. This course limits
the influence of errors on coherence, but handicaps
finesse as less information is available for inference.
The measures we used were then averaged on the
number of texts. This departs from what could be
considered a more standard practice, summing ev-
erything and dividing by the number of comparisons
made. The reason behind this is we think compar-
ing two graphs as comparing two temporal models
of a text, not just finding a list of targets in a set
of texts. It might be easier to accept this if one re-
members that the number of possible relations be-
tween n events is n(n?1)/2. A text t1 with k more
2We are still annotating more texts manually to give more
significance to the results.
Finesse Coherence
annotation relations 0.114 0.011
Allen relations 0.083 0.094
Table 3: Example of evaluation on a "random" annotation
events than a text t2 will thus have about k2 times
more importance in a global score, and we find con-
fusing this non-linear relation between the size of a
text and its weight in the evaluation process. There-
fore, both finesse and coherence are generalized as
global measure of a temporal model of a text. It
could then be interesting to relate temporal infor-
mation and other features of a given text (size being
only one factor).
Results are shown Table 4. These results seem
promising when considering the simplifications we
have made on every step of the process. Caution is
necessary though, given the limited number of texts
we have experimented on, and the high variation we
have observed between texts. At this stage we be-
lieve the quality of our results is not that important.
Our main objective, above all, was to show the fea-
sibility of a robust method to annotate temporal re-
lations, and provide useful tools to evaluate the task,
in order to improve each step separately later. Our
focus was on the design of a good methodology.
If we try a first analysis of the results, sources
of errors fall on various categories. First, a number
of temporal adverbials were attached to the wrong
event, or were misinterpreted. This should be fine-
tuned with a better parser than what we used. Then,
we have not tried to take into account the specific
narrative style of newswire texts. In our set of texts,
the present tense was for instance used in a lot of
places, sometimes to refer to events in the past,
sometimes to refer to events that were going to hap-
pen at the time the text was published. However,
given the method we adopted, one could have ex-
pected better coherence results than finesse results.
It means we have made decisions that were not cau-
tious enough, for reasons we still have to analyze.
One potential reason is that relations offered to hu-
mans are maybe too vague in the wrong places: a
lot of information in a text can be asserted to be
"strictly before" something else (based on dates for
instance), while human annotators can only say that
events are "before or meets" some other event; each
time this is the case, coherence is only 0.5.
It is important to note that there are few points
of comparison on this problem. To the best of our
knowledge, only (Li et al, 2001) and (Mani and
Wilson, 2000) mention having tried this kind of an-
notation, as a side job for their temporal expressions
mark-up systems. The former considers only rela-
tions between events within a sentence, and the lat-
ter did not evaluate their method.
Finally, it is worth remembering that human an-
notation itself is a difficult task, with potentially a
lot of disagreement between annotators. For now,
our texts have been annotated by the two authors,
with an a posteriori resolution of conflicts. We
therefore have no measure of inter-annotator agree-
ment which could serve as an upper bound of the
performance of the system, although we are plan-
ning to do this at a later stage.
7 Conclusion
The aim of this study was to show the feasibility of
annotating temporal relations in a text and to pro-
pose a methodology for the task. We thus define a
way of evaluating the results, abstracting away from
variations of human descriptions for similar tempo-
ral situations. Our preliminary results seem promis-
ing in this respect. Obviously, parts of the method
need some polishing, and we need to extend the
study to a larger data set. It remains to be seen how
improving part of speech tagging, syntactic analy-
sis and discourse modeling can influence the out-
come of the task. Specifically, some work needs to
be done to evaluate the detection of temporal ad-
juncts, a major source of information in the process.
We could also try to mix our symbolic method with
some empirical learning. Provided we can collect
more annotated data, it would be easy to improve
the discourse model by (at least local) optimization
on the space of possible rules, starting with our own
set. We hope that the measures of temporal infor-
mation we have used will help in all these aspects,
but we are also planning to further investigate their
properties and that of other candidate measures not
considered here.
References
Steven Abney, 1996. Corpus-Based Methods in
Language and Speech, chapter Part-of-Speech
Tagging and Partial Parsing. Kluwer Academic
Publisher.
J. Allen. 1984. Towards a general theory of action
and time. Artificial Intelligence, 23:123?154.
B. Bruce. 1972. A model for temporal references
Finesse Standard Deviation Coherence SD
annotation relations 0.477499 0.286781 0.449899 0.175922
Allen relations 0.448222 0.289856 0.495755 0.204974
Table 4: Evaluation
and its application in a question answering pro-
gram. Artificial Intelligence, 3(1-3):1?25.
Elena Filatova and Eduard Hovy. 2001. Assign-
ing time-stamps to event-clauses. In Harper et al
(Harper et al, 2001).
Claire Grover, Janet Hitzeman, and Marc Moens.
1995. Algorithms for analysing the temporal
structure of discourse. In Sixth International
Conference of the European Chapter of the As-
sociation for Computational Linguistics. ACL.
Lisa Harper, Inderjeet Mani, and Beth Sundheim,
editors. 2001. ACL Workshop on Temporal
and Spatial Information Processing, 39th Annual
Meeting and 10th Conference of the European
Chapter. Association for Computational Linguis-
tics.
M. Kameyama, R. Passonneau, and M. Poesio.
1993. Temporal centering. In Proceedings of
ACL 1993, pages 70?77.
Graham Katz and Fabrizio Arosio. 2001. The an-
notation of temporal information in natural lan-
guage sentences. In Harper et al (Harper et al,
2001), pages 104?111.
W. Li, K-F. Wong, and C. Yuan. 2001. A model
for processing temporal reference in chinese. In
Harper et al (Harper et al, 2001).
I. Mani and G. Wilson. 2000. Robust temporal pro-
cessing of news. In Proceedings of ACL 2000.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods in
Language Processing.
Andrea Setzer. 2001. Temporal Information in
Newswire Articles: an Annotation Scheme and
Corpus Study. Ph.D. thesis, University of
Sheffield, UK.
Franck Shilder and Christopher Habel. 2001. From
temporal expressions to temporal information:
Semantic tagging of news messages. In Harper
et al (Harper et al, 2001), pages 65?72.
F. Song and R. Cohen. 1991. Tense interpretation
in the context of narrative. In Proceedings of
AAAI?91, pages 131?136.
Mark Steedman. 1997. Temporality. In J. Van Ben-
them and A. ter Meulen, editors, Handbook of
Logic and Language. Elsevier Science B.V.
Nikolai Vazov. 2001. A system for extraction of
temporal expressions from french texts based on
syntactic and semantic constraints. In Harper
et al (Harper et al, 2001).
George Wilson, Inderjeet Mani, Beth Sundheim,
and Lisa Ferro. 2001. A multilingual approach to
annotating and extracting temporal information.
In Harper et al (Harper et al, 2001).
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1208?1217, Dublin, Ireland, August 23-29 2014.
Ranking Multidocument Event Descriptions
for Building Thematic Timelines
Kiem-Hieu Nguyen
?
, Xavier Tannier
?$
, and Veronique Moriceau
?$
?
LIMSI-CNRS
$
Univ. Paris-Sud
Orsay, France
{nguyen,xtannier,moriceau}@limsi.fr
Abstract
This paper tackles the problem of timeline generation from traditional news sources. Our sys-
tem builds thematic timelines for a general-domain topic defined by a user query. The system
selects and ranks events relevant to the input query. Each event is represented by a one-sentence
description in the output timeline.
We present an inter-cluster ranking algorithm that takes events from multiple clusters as input and
that selects the most salient and relevant events. A cluster, in our work, contains all the events
happening in a specific date. Our algorithm utilizes the temporal information derived from a
large collection of extensively temporal analyzed texts. Such temporal information is combined
with textual contents into an event scoring model in order to rank events based on their salience
and query-relevance.
1 Introduction
We aim at building thematic timelines from multiple documents relevant to a specific, user-generated
query. For instance, for the query ?Libya conflict?, our system will return important events related to
the Libya conflict in 2011 involving Kadhafi forces, rebels, NATO intervention, etc. (Figure 1). Such
a timeline can then be visualized as a textual, event-based summary, or through any existing graphical
timeline visualization tool.
The main contribution of this paper is a two-step inter-cluster ranking algorithm aimed at selecting
salient and non-redundant events from temporal clusters, which are sets of sentences describing events
related to the query and that occurred at the same day. In the first step, a scoring model is proposed to
rank sentences describing events, according to their relevance and salience to the topic. In the second
step, the ranked events are iteratively reranked based on their content in order to reduce information
redundancy. We finally obtain an extendable, chronological summary of important events concerning the
query.
This paper is organized as follows: ?2 introduces related work. ?3 presents the resources used and
gives an overview of the system. The salient date algorithm proposed by Kessler et al. (2012), that we
used to build our temporal clusters, is briefly summarized in ?4. ?5 and ?6 describe our ranking approach
to event selection and a content-based reranking algorithm, respectively. The evaluations are presented
in ?7. ?8 is dedicated to the conclusion and future work.
2 Related Work
Our work is closely related to event detection and tracking (EDT) and multidocument summarization
(MDS). This section introduces some important work in these fields.
2.1 Event Detection and Tracking
EDT on news streams has been intensively studied. Early work concentrates on detecting events from
article texts using vector-based techniques (Allan et al., 1998; Petrovi?c et al., 2010) or graphical models
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1208
Mar 19 2011. The UN Security Council agrees a resolution authorising ?all necessary measures? to protect civilians in Libya, where a revolt is under way
against the regime of Moamer Kadhafi.
Mar 19 2011. French, US and British forces attack Kadhafi?s forces from the air.
Mar 19 2011. They retreat from the rebel stronghold of Benghazi.
Mar 26 2011. Benefiting from the Western air strikes, rebels take the towns of Ajdabiya and Brega, moving on to the oil town of Ras Lanuf.
Mar 29 2011. International powers meet in London but fail to agree on a strategy against Kadhafi.
Mar 30 2011. Kadhafi?s forces retake Ras Lanuf and Brega.
Mar 30 2011. Libyan foreign minister Mussa Kussa defects to Britain.
Mar 31 2011. NATO takes full command of the coalition campaign.
Apr 01 2011. In the first of several ?friendly fire? incidents, NATO attacks kill nine rebels and civilians.
Apr 06 2011. Washington rejects a letter from Kadhafi calling for an end to air strikes, and repeats that Kadhafi must go.
Apr 07 2011. A world food program aid ship arrives at rebel-held Misrata, where shelling by Kadhafi?s forces has killed or wounded hundreds.
Apr 10 2011. An African Union delegation headed by South African president Jacob Zuma meets Kadhafi and the rebels.
Apr 10 2011. The former accepts their peace plan, but the latter refuse, saying Kadhafi and his sons must step down.
Apr 12 2011. Britain and France call on their NATO allies to step up operations against Kadhafi?s forces.
...
Figure 1: A chronology about ?Libya conflict? written by journalists.
(Sayyadi et al., 2009). These papers do not consider time, which is an essential dimension of event
timelines.
Attempts to use temporal information for EDT are significant in the literature. To name but a few,
Alonso et al. (2009) apply time-based clustering on search results. Yan et al. (2011) use document
timestamps to calculate temporal proximity for timeline generation from web documents. Similarly,
Zhao et al. (2007) use text similarity and time intensity for event clustering on social streams. Kessler
et al. (2012) exploit temporal analysis to detect salient dates of an event from raw text. Following this
direction, Battistelli et al. (2013) apply sequential pattern mining to select a one-sentence description for
each salient date of an event.
2.2 Multidocument Summarization
Sentence extraction is essential in extractive text summarization. In the unsupervised approach, sentences
are scored using term weight and term proximity induced from a document collection (Goldstein et al.,
2000). In the supervised approach, training data generated from reference summaries are used to learn
classification or ranking models. New sentences are selected based on their confidence value on learned
models (Wan et al., 2007). As information comes from documents on the same topic, it should be noticed
that it is also important to reduce redundancy in MDS (Carbonell and Goldstein, 1998).
Filippova (2010) builds a co-occurrence word graph from a collection of related sentences and gener-
ates a generic summary from the graph based on shortest path finding. Her algorithm is a hybrid method
between extractive and abstractive approaches to MDS.
3 Resources and System Overview
3.1 Corpus and Chronologies
For this work, we use a corpus of newswire texts provided by the AFP French news agency. The English
AFP corpus is composed of 1.3 million texts that span the 2004-2011 period (511 documents/day in
average and 426 millions words). Each document is an XML file containing title, document creation
time (DCT), set of keywords, and textual content split into paragraphs.
AFP ?chronologies? (textual event timelines) are a specific type of articles written by AFP journalists
in order to contextualize current events. These chronologies may concern any topic discussed in the
media, and consist in a list of dates (typically between 10 and 20) associated with a text describing the
related event(s). Figure 1 shows an example of such a chronology. Note that several important events
can occur at the same date.
3.2 System Overview
Figure 2 shows the general architecture of the system. When the user submits a query, sentences are
retrieved by the Lucene search engine and are clustered by the dates appearing in those sentences (step?
1209
  
RANK RERANK
RANKED EVENTS
TEMPORAL CLUSTERS
EXTENDABLE TIMELINETHEMATIC CLUSTERS
March 22 April 2nd
March 11
1.2.3.4.?..............
1.2.3.4.?..............
QUERY Salientdates
(Kessler et al., 2012)
Relevance and Salience
Redundancy
?
? ?
Figure 2: System overview.
in the Figure (Kessler et al., 2012)).
Then, all sentences are ranked by the relevance and salience of described events. This is done by
modeling event relevance and salience as a scoring function (step ?). Thematic clusters are created by
applying clustering on the set of events on the same date. Finally, sentences are reranked by an iterative
algorithm aiming at reducing redundancy from the initial list (step ?) to achieve an extendable timeline.
4 Temporal Clusters
As stated in the introduction, our main contribution in this paper is to rank and select salient and non-
redundant sentences from clusters, in order to build query-based timelines. We rely on the algorithm
proposed by Kessler et al. (2012) for building temporal clusters. This section is a quick overview of their
approach.
4.1 Preprocessing
A temporal analysis is performed on all documents from the AFP corpus (see ?3.1) with the Heidel-
time (Str?otgen and Gertz, 2013) parser. The main purpose is to collect as much temporal information as
possible. Absolute dates and DCT-relative dates are extracted and normalized (full dates represented in
a common format). DCT-relative dates are those which are relative to the date on which the document
is published, such as ?Yesterday? (day before DCT), ?next Friday? (first Friday following the DCT) or
?on Friday? (can be first Friday preceding or following the DCT, depending on the tense of the verb that
governs the temporal expression).
In a corpus containing 426 millions words, 845,000 absolute dates and 4.6 millions relative dates were
detected and normalized.
4.2 Temporal Cluster Building
At query time, temporal clusters (or ?salient date sets?) are then built with the help of a search engine
(Lucene in that case
1
). Articles are indexed by Lucene at sentence-level (a document = a sentence).
1
http://lucene.apache.org
1210
...launched the first air strike on March 19 , has deployed around 20 Rafale and Mirage...
...last week, said they were arrested on Saturday along with Getty photographer Joe Raedle...
...since coalition air strikes began Saturday , a figure that could not be confirmed...
...United State to launch air raids on March 19 , are in a hurry to get out of whether NATO...
Figure 3: All the temporal expressions in the following sentences were normalized at date level as ?Mar
19 2011?.
Given the query, a number of sentences are retrieved by search engine. Dates are extracted from these
sentences. These dates are then ranked by their ?salience? in the set of documents. The idea behind the
notion of salient date is that if a date is important in a sub-corpus (Lucene output), then we can say that
important events occurred at this date, and then that these events must appear in a timeline.
In practice, salience is mostly defined by the number of occurrences of the date in the documents from
the search engine, as well as some other features that are used to feed a machine learning classifier.
The output of this salient date algorithm is then a ranked list of dates, where each date comes together
with a set of sentences that contain this date and that are relevant to the query. We call temporal clusters
these sets of sentences linked to a specific date (see Figures 2 and 3).
5 Event Ranking
Our ranking mechanism relies on the mutual relation between relevance and salience. It aims at ranking
events based on these two factors. The problem of information redundancy will be addressed by a
reranking step in ?6. Our principal motivation is that an event has more chance to be selected into a
timeline if it is both relevant to the topic and important, or in other words, salient w.r.t other related
events. The concepts of relevance and salience are realized in our ranking function by considering term
proximity and date frequency, respectively.
Previous works in event detection normally formalize events as individual terms or syntactic patterns,
which facilitates the use of text content. Instead, as our method utilizes both time and text content, we
come to a formalization of an event as a pair of its mentioned date and its one-sentence description.
Given an input query, the aim of ranking is to select the most relevant and salient events. The relevance
of an event is calculated by vector-based query similarity, and augmented by the average relevance of its
containing thematic cluster. Salience is contributed by date frequency and averaged term weight. As a
result, the overall score of an event e given a query q is the multiplication of the following four factors:
score(e|q) =rel
e
(e|q)
? rel
cl
(cl|q)
? salience
e
(e|d, q)
? salience
d
(d|q),
(1)
where:
? rel
e
(e|q) is the relevance of e to q (see ?5.1).
? rel
cl
(cl|q) is the relevance of a thematic cluster cl to q, which is the averaged relevance of its
members (see ?5.2).
? salience
e
(e|d, q) is the salience of e w.r.t the date d that the event happens. It is calculated as the
average salience of the terms in its one-sentence description. Term salience, in turn, is calculated
based on term frequency in the date cluster (see ?5.3).
? salience
d
(d|q) is the salience of d w.r.t to q. Date salience is the averaged salience of all the events
in that date (see ?5.4).
1211
5.1 Event Relevance: rel
e
(e|q)
The motivation behind considering relevance is that if an event is relevant to the query then it is an
important event. We use the conventional TFIDF vector space model with bag-of-word assumption to
represent document and query vectors. For relevance, the similarity between document and query vectors
is the built-in Lucene score formula
2
,
rel
e
(e|q) = cosine(~e, ~q) ? norm
L
(~e, ~q). (2)
5.2 Thematic Cluster Relevance: rel
cl
(cl|q)
Date salience does not always correctly reflect the importance of event. For instance, the date of Haiti
earthquake considers the earthquake itself as the main event. However, related events such as the sorrow
expression of UN Secretary General also happen immediately after the earthquake but still in the same
date. Such satellite events will have the same date salience as the central event. In another case, a date
when there is no central event but there are many ?consequent? events will also have a high salience value.
E.g., on the day after the earthquake, international aids are planned; number of victims is estimated;
aftermath events are invoked, etc.
Those examples show that the ?one event per date? assumption is weak in reality. To overcome
this weakness, we apply an hierarchical clustering technique, in which two clusters are merged if their
normalized Manhattan distance is lower than a threshold ?, to generate thematic sub-clusters inside a
date cluster
3
. In in-house experiments, we observed that different values of ? did not significantly vary
performance. We hence selected ? = 0.5 for our system.The score of each thematic cluster is then
calculated as averaged document relevance,
rel
cl
(cl|q) =
?
e
rel
e
(e|q)
|cl|
. (3)
5.3 Event Salience: salience
e
(e|d, q)
An important event tends to contain salient terms. Those terms, in turn, tend to occur frequently on a
date. We hence come to measure term salience as its frequency of occurrence on the date f(t|d, q), and
event salience as the averaged salience of its terms. For term normalization, stopwords are removed and
tokens are normalized by the Porter stemming algorithm (Porter, 1997).
salience
e
(e|d, q) =
?
t?e
f(t|d, q)
|e|
?
t
?
?d
f(t
?
|d, q)
(4)
5.4 Date Salience: salience
d
(d|q)
The use of temporal clusters, i.e. date clusters, is motivated by the observation that an important event
happens on a salient date. Date salience is the total relevance of all events happening on that date (the
numerator):
salience
d
(d|q) =
?
e
rel
e
(e|q)
?
d
?
e
rel
e
(e|q)
. (5)
The denominator is used to normalize date salience so that it is comparable to other factors in (1).
6 Event Reranking
The score described in previous section leads to a ranked list of salient and relevant events. However, it
does not consider the fact that some information can be redundant between events. The reranking algo-
rithm presented in this section strives to reduce such redundancy. In principal, information redundancy is
2
https://lucene.apache.org/core/3_6_2/api/core/org/apache/lucene/search/
Similarity.html
3
In our implementation, for each one-sentence document, we used the whole texts of its containing article to create its
document vector. Manhattan distance is the sum of the absolute difference of term weight between two clusters
1212
Rank Date Event Description
NO RERANK
1 Mar 31 2011 The North Atlantic Treaty Organisation takes over formal command of the military operation.
2 Mar 31 2011 NATO took command of operations over Libya on March 31.
3 Mar 31 2011 NATO takes command of the coalition campaign.
4 Mar 19 2011 [...] French, US and British forces launch UN-mandated air strikes and push them back.
5 Mar 30 2011 Libyan foreign minister Mussa Kussa defects.
... ... ...
RERANK
1 Mar 31 2011 The North Atlantic Treaty Organisation takes over formal command of the military operation.
2 Mar 19 2011 [...] French, US and British forces launch UN-mandated air strikes and push them back.
3 Mar 30 2011 Libyan foreign minister Mussa Kussa defects.
4 Mar 23 2011 US Defence Secretary Robert Gates on Wednesday held talks in Cairo on the conflict in Libya [...]
5 Apr 04 2011 [...] photographer Manu Brabo disappeared on April 4 while covering the conflict in Libya.
... ... ...
{2} Mar 31 2011 NATO took command of operations over Libya on March 31.
... ... ...
{3} Mar 31 2011 NATO takes command of the coalition campaign.
... ... ...
Figure 4: The effect of reranking on the order of events (by score).
Algorithm 1 Reranking algorithm
1: out? ?
2: while (!terminate) do
3: for e ? S(q) \ out do
4: score(e|q)
5: end for
6: e
?
= argmax
e?S(q)\out
score(e|q)
7: out? out ? e
?
8: d
?
= date(e
?
)
9: for t ? e
?
do
10: used(d
?
)? used(d
?
) ? t
11: end for
12: end while
estimated by the distinction between used and unused terms. The algorithm iteratively recomputes event
salience (hence the overall event score) based on used/unused terms as follows:
salience
?
e
(e|d, q) =
?
t
?
?e
f(t
?
|d, q)
|e|
?
t
?
?d
f(t
?
|d, q)
, (6)
where t
?
is an unused term on the date d. A used term is the one that already occurred in better-ranked
sentences. This formula is different from (4) in the distinction between used and unused terms. Each
time a new event is selected, its appropriate list of used terms is updated with the terms in the one-
sentence description of the selected event. Each date has its own list of used/unused terms.
The algorithm for reranking is provided in Algorithm 1. At first, the score of all sentences related to
the query S(q) is calculated using the formula (1) with event salience defined in (6) (lines 3-5). Then,
the highest scored sentence is selected into the output (lines 6-7) and is removed from the pool. In line
8, d
?
is the date when the event e happens: d
?
= date(e
?
). The list of used terms on its date is updated
with the terms from that selected sentence (lines 9-11). A new iteration restarts by recalculating score of
unselected sentences according to new lists of used terms. The algorithm terminates after K iterations,
i.e. when K events have been selected into timeline.
Figure 4 illustrates the effect of reranking on the order of events in a timeline. The upper shows the
top events ranked by score without reranking. The date ?Mar 31 2011? appears three times in 1
st
, 2
nd
,
and 3
rd
events. The lower shows the ranking of events after the highest scored event has been selected.
As an effect of reranking, the two events previously ranked 2
nd
and 3
rd
now fall down the list.
1213
Mar 19 2011. (2) With the forces of Libyan leader Moamer Kadhafi threatening rebel-held Benghazi, French, US and British forces launch UN-mandated
air attacks and push them back.
Mar 19 2011. (9) Norwegian Prime Minister Jens Stoltenberg said Saturday Norway would contribute six F-16 warplanes to the international military
operation ?led by the United States, France and Britain? to enforce a no-fly zone over Libya.
Mar 19 2011. (11) Residents of another western town, Yafran, say nine people died there in an offensive that began on Monday.
Mar 21 2011. (4) Kadhafi?s forces retreat from the rebel stronghold of Benghazi.
Mar 22 2011. (1) In Western Libya fighting intensifies in Misrata, which has been in the hands of rebels for a month.
Mar 24 2011. (10) When I ask: What is the next stage? Do you have a road map? I see they do not, he said Thursday.
Mar 25 2011. (12) Ping returned early Friday from Europe after meeting with French Foreign Minister Alain Juppe and an envoy sent by the European
Union?s Chief Diplomat Catherine Ashton.
Mar 28 2011. (6) Qatar follows France in recognising the rebel shadow government.
Mar 29 2011. (3) Kadhafi loyalists push the rebels back.
Mar 30 2011. (8) Kadhafi?s forces push back.
Mar 31 2011. (5) NATO takes command of the coalition campaign.
Apr 04 2011. (13) Italy joins France and Qatar in recognising the rebel Transitional National Council.
Apr 13 2011. (7) A Libya contact group of 20 countries and organisations, including the rebels, meets in Qatar.
Apr 23 2011. (14) The United States carried out its first predator drone strike in Libya on Saturday, the Pentagon said, declining to give details on the
targets or location.
...
Figure 5: Timeline for the query ?Libya conflict? created by the Rank-Rerank method. Events are shown
in chronological order, each accompanied with its rank starting from 1, displayed as a number between
().
7 Evaluations
Our system for building timelines is named as RaRE, as short for ?Rank and RErank?. We use a set
of 91 chronologies manually written by expert journalists from the AFP news agency (Figure 1) as
golden reference summaries for evaluation. As our generated timelines are extendable, we need to define
its length for evaluation. Considering the characteristics of reference summaries, we decide that if a
reference summary of a timeline contains k events, we appropriately use only the k highest ranked events
in the timeline for evaluation (Figure 5). The evaluations of the date selection and summary generation
are presented in ?7.1 and ?7.2, respectively.
7.1 Evaluate Date Selection
We evaluate the dates selected by timelines returned by our system. The purposes of this evaluation
are two-fold: i) Since time (as date in our case) is an essential dimension of chronological timeline, it
is necessary to evaluate the time selected by timelines; ii) The novelty of this work w.r.t Kessler et al.
(2012) is the mixture of content and temporal information. We need to show empirical evidences that at
least, this mixture does not break the performance of date selection.
The dates occurring in a timeline are compared with the dates occurring in its reference timeline using
Mean Average Precision (MAP) metric. It should be noted that by using MAP@k as evaluation metric, a
date with higher rank has more impact than another date with lower rank. We use two systems presented
in Kessler et al. (2012), named as DFIDF and ML in Table 1, for comparison as follows:
? DFIDF is an unsupervised system solely relying on date frequency with a tfidf-like scoring function.
This method uses the AFP corpus, the same as the one used in our work. As the AFP corpus is
temporally analyzed, the method indexes all the occurrences of dates in the corpus. Dates are then
scored and ranked with so-called DFIDF, a tfidf-like scoring mechanism.
? ML is a supervised system that learns a classifier and ranks unseen dates based on classification
System MAP
DFIDF 71.46
ML 79.18
RaRE 77.83
Table 1: Comparison of salient date detection using MAP.
1214
System P R
DFIDF
?
27.24 25.50
ML
?
29.93 27.54
RaRE-no-rerank 28.82 24.47
RaRE 31.23 26.63
Table 2: Comparison of MDS using ROUGE at 95% confidence interval.
confidence. The method leverages the dates in reference summaries to create training data with
salient/non-salient examples. Temporal features such as date frequency, DCT, novelty, etc., are
extracted to learn an adaptive boosting classifier.
As shown in Table 1, our method is close to ML. This result is encouraging as ML requires training
data; and on the other hand, our system is not designed to directly solve the task of date selection. As
expected, our system beats the unsupervised system DFIDF by a large margin. This superiority shows
that the mixture of temporal information and content leads to an improvement on date selection over
using only the former.
7.2 Evaluate Summary Generation
In order to evaluate timelines as text summaries, we ignore dates and consider all the entries in a timeline
as one summary. We use ROUGE metric (Lin, 2004) to evaluate generated timelines against reference
summaries.
The following baselines are implemented (Table 2): In DFIDF
?
, salient dates are taken from the
outputs of the DFIDF system described in previous section. Each salient date is equivalent to a cluster
containing all the events happening in that date. We then select the event the most relevant to the query,
i.e. the event with the highest Lucene score, as representative of that salient date. Note that consequently,
DFIDF
?
makes an assumption, which is not assumed in RaRE, that there is only one event happens in
a particular date. The same assumption is presumed in Battistelli et al. (2013). However, because their
system is particularly designed for French and is intended to parse small corpora, we could not conduct
a direct comparison with their method. ML
?
is built similarly to DFIDF
?
, except that salient dates are
instead taken from the ML system. The RaRE-no-rerank system is identical to RaRE in the ranking step,
but the reranking step is omitted.
Our system is superior to DFIDF
?
as expected. Moreover, it outperforms ML
?
, even though ML
?
performs better on the task of date selection. Among these three systems that combine temporal infor-
mation and textual contents for summary generation, our system is the most successful. Furthermore,
RaRE outperforms RaRE-no-rerank, which shows that reducing redundancy by reranking improves the
performance of summary generation.
8 Conclusion and Future Work
We presented a two-step inter-cluster ranking algorithm for event selection. The rank step sorts events
based on their salience and query relevance. The event scoring function is based on both date frequency
induced from temporal analyzed texts and term weighting induced from contents to reflect these two
factors. The rerank step allows to reduce information redundancy by using inter-sentence dependency
between the descriptions of events happening in the same time period (i.e. the same date in this work).
Ranking based on sentences may be sensitive to sparsity. In the future, we will expand local contexts,
for instance, to neighboring sentences, to acquire richer textual representation of events. One remaining
issue is that reference chronologies, written by the journalists, are very subjective, and that we have only
one example of chronology per topic. In the future, we will conduct a manual evaluation in order to
complete results from this automatic evaluation. With the help of a validation interface, journalists will
be provided ranked list of events w.r.t. their queries. They will then be able to select and edit the events
that they wish to validate for their future timelines. Such an interface will both help journalists to produce
new timelines, and bring a new evaluation methodology for our system.
1215
Acknowledgements
This work has been partially funded by French National Research Agency (ANR) under project Chrono-
lines (ANR-10-CORD-010). We would like to thank the French News Agency (AFP) for providing us
with the corpus. We would like to thank anonymous reviewers for comments and suggestions on the
paper.
References
James Allan, Ron Papka, and Victor Lavrenko. 1998. On-line new event detection and tracking. In Proceedings
of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,
SIGIR ?98, pages 37?45, New York, NY, USA. ACM.
Omar Alonso, Michael Gertz, and Ricardo Baeza-Yates. 2009. Clustering and exploring search results using
timeline constructions. In Proceedings of the 18th ACM conference on Information and knowledge management,
CIKM ?09, pages 97?106, New York, NY, USA. ACM.
Delphine Battistelli, Thierry Charnois, Jean-Luc Minel, and Charles Teissedre. 2013. Detecting salient events in
large corpora. In Proceedings of the 14th international conference on Computational Linguistics and Intelligent
Text Processing, CICLing?13, Berlin, Heidelberg. Springer-Verlag.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, SIGIR ?98, pages 335?336, New York, NY, USA. ACM.
Katja Filippova. 2010. Multi-sentence compression: finding shortest paths in word graphs. In Proceedings of the
23rd International Conference on Computational Linguistics, COLING ?10, pages 322?330, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization
by sentence extraction. In Proceedings of the 2000 NAACL-ANLPWorkshop on Automatic summarization -
Volume 4, NAACL-ANLP-AutoSum ?00, pages 40?48, Stroudsburg, PA, USA. Association for Computational
Linguistics.
R?emy Kessler, Xavier Tannier, Caroline Hag`ege, V?eronique Moriceau, and Andr?e Bittar. 2012. Finding salient
dates for building thematic timelines. In Proceedings of the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages 730?739, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-
Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74?81,
Barcelona, Spain, July. Association for Computational Linguistics.
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10, pages 181?189, Stroudsburg, PA, USA. Association for
Computational Linguistics.
M. F. Porter. 1997. An algorithm for suffix stripping. In Readings in information retrieval, pages 313?316.
Morgan Kaufmann Publishers Inc.
Hassan Sayyadi, Matthew Hurst, and Alexey Maykov. 2009. Event detection and tracking in social streams.
In Proceedings of the Third International Conference on Weblogs and Social Media, ICWSM 2009, San Jose,
California, USA, May 17-20, 2009.
Jannik Str?otgen and Michael Gertz. 2013. Multilingual and Cross-domain Temporal Tagging. Language Re-
sources and Evaluation, 47(2):269?298.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document
summarization. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI?07,
pages 2903?2908, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011. Timeline generation
through evolutionary trans-temporal summarization. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages 433?443, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1216
Qiankun Zhao, Prasenjit Mitra, and Bi Chen. 2007. Temporal and information flow based event detection from so-
cial text streams. In Proceedings of the 22nd national conference on Artificial intelligence - Volume 2, AAAI?07,
pages 1501?1506. AAAI Press.
1217
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 958?967,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Building Event Threads out of Multiple News Articles
Xavier Tannier
LIMSI-CNRS
Univ. Paris-Sud
Orsay, France
xavier.tannier@limsi.fr
Ve?ronique Moriceau
LIMSI-CNRS
Univ. Paris-Sud
Orsay, France
moriceau@limsi.fr
Abstract
We present an approach for building multi-
document event threads from a large corpus
of newswire articles. An event thread is basi-
cally a succession of events belonging to the
same story. It helps the reader to contextual-
ize the information contained in a single arti-
cle, by navigating backward or forward in the
thread from this article. A specific effort is
also made on the detection of reactions to a
particular event.
In order to build these event threads, we use a
cascade of classifiers and other modules, tak-
ing advantage of the redundancy of informa-
tion in the newswire corpus.
We also share interesting comments con-
cerning our manual annotation procedure for
building a training and testing set1.
1 Introduction
In this paper, we explore a new way of dealing
with temporal relations between events. Our task
is somewhat between multidocument summariza-
tion and classification of temporal relations between
events. We work with a large collection of En-
glish newswire articles, where each article relates
an event: the main topic of the article is a specific
event, and other older events are mentioned in order
to put it into perspective. Thus, we consider that an
event is associated with an article and that defining
temporal relations between articles is a way to define
temporal relations between events.
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.
The task is to build a temporal graph of arti-
cles, linked between each other by the following re-
lations:
? Same event, when two documents relate the
same event, or when a document is an update
of another one.
? Continuation, when an event is the continua-
tion or the consequence of a previous one.
We also define a subset of continuation, called
reaction, concerning a document relating the reac-
tion of someone to another event.
Some examples of these three classes will be
given in Section 3.
These relations can be represented by a directed
graph where documents are vertices and relations
are edges (as illustrated in all figures of this article).
Figure 1 shows an example of such a graph.
Press articles, and especially newswire articles,
are characterized by an important redundancy of re-
lated events. An important event2 is likely to be
treated by several successive articles, which will
give more and more details and update some num-
bers (mainly, tragedy casualty updates, as shown in
Figure 2). On the one hand, this redundancy is an
issue since a system must not show duplicate infor-
mation to the user; on the other hand, we show in
this article that it can also be of great help in the
process of extracting temporal graphs.
In what follows, we first review some of the re-
lated work in Section 2. Section 3 presents the anno-
tation procedure and the resulting annotated corpus
2Note that we do not focus intentionally on ?important?
events. However, the fact is that minor events do hardly lead
to dense temporal graphs.
958
Figure 1: Example of ?temporal graph?: around the
Pope?s death. The associated text is the title of each ar-
ticle. Relations that can be obtained by transitivity have
been hidden for clarity?s sake.
used for developing, learning and evaluating the sys-
tem. The simple modules used to predict the same
event, continuation and, possibly, reaction relations
are described in Section 4, and results are given in
Section 5.
We also propose an end-user application to this
work. When a user reads an article, the system will
then be able to provide her with a thread of events
having occurred before or after, helping her to con-
textualize the information she is reading. This appli-
cation is described in Section 6.
2 Related work
The identification of temporal relations between
events in texts has been the focus of increasing atten-
tion because of its importance in NLP applications
such as information extraction, question-answering
or summarization. The evaluation campaigns Tem-
pEval 2007 (Verhagen et al, 2007) and TempEval
2010 (Verhagen et al, 2010) focused on temporal
relation identification, mainly on temporal relations
between events and times in the same sentence or
in consecutive sentences and between events and the
creation time of documents. In this context, the goal
is to identify the type of a temporal relation which is
Figure 2: Example of ?temporal graph?: Madrid attacks,
with many updates of the initial information. Note that
articles gathered in this main pool of articles can be pos-
terior to the continuations and reactions to the described
event.
known to be present. Systems having the best results
(accuracy about 0.6) use statistical learning based
on temporal features (modality, tense, aspect, etc.)
(Mani et al, 2006; Chambers et al, 2007). More re-
cently, Mirroshandel and Ghassem-Sani (2012) pro-
posed a new method for temporal relation extraction
by using a bootstrapping method on annotated data
and have a better accuracy than state-of-the-art sys-
tems. Their method is based on the assumption that
similar event pairs in topically related documents
are likely to have the same temporal relations. For
this work, the authors had already some collections
of topically related documents and did not need to
identify them.
In the 2012 i2b2 challenge (i2b, 2012), the
problem was not only to identify the type of tempo-
ral relations, but also to decide whether a temporal
relation existed or not between two elements, either
clinical concepts or temporal expressions. But, as
in TempEval, the temporal analysis were only to be
performed within a single document.
Other works focus on event ordering. For ex-
ample, Fujiki et al (2003) and Talukdar et al (2012)
proposed methods for automatic acquisition of event
sequences from texts. They did not use tempo-
ral information present in texts and extracted se-
quences of events (e.g. arrest/escape) from sen-
tences which were already arranged in chronologi-
959
cal order. Chambers and Jurafsky (2008) proposed
a method to learn narrative chains of events related
to a protagonist in a single document. The first
step consists in detecting narrative relations between
events sharing coreferring arguments. Then, a tem-
poral classifier orders partially the connected events
with the before relation.
Concerning the identification of the reaction re-
lation, to our knowledge, there is no work on the
detection of reaction between several documents.
Pouliquen et al (2007), Krestel et al (2008) and
Balahur et al (2009) focused on the identification of
reported speech or opinions in quotations in a docu-
ment, but not on the identification of an event which
is the source of a reaction and which can possibly be
in another document.
As we can see, all these approaches, as well as
traditional information extraction approaches, lean
on information contained by a single document, and
consider an event as a word or a phrase. However,
Ahmed et al (2011) proposed a framework to group
temporally and tocipally related news articles into
same story clusters in order to reveal the temporal
evolution of stories. But in these topically related
clusters of documents, no temporal relation is de-
tected between articles or events except chronologi-
cal order. On this point of view, our task is closer
to what is done in multidocument summarization,
where a system has to detect redundant excerpts
from various texts on the same topic and present
results in a relevant chronological order. For ex-
ample, Barzilay et al (2002) propose a system for
multidocument summarization from newswire arti-
cles describing the same event. First, similar text
units from different documents are identified using
statistical techniques and shallow text analysis and
grouped into thematic clusters. Then, in each theme,
sentences which are selected as part of the summary
are ordered using the publication date of the first oc-
currence of events to order sentences.
3 Resources
We built an annotated collection of English articles,
taken from newswire texts provided by the French
news agency (AFP), spreading over the period 2004-
2012. The entire collection contains about 1.5 mil-
lion articles. Each document is an XML file contain-
ing a title, a creation time (DCT), a set of keywords
and textual content split into paragraphs.
3.1 Selection of Article Pairs
Pairs of documents were automatically selected ac-
cording to the following constraints:
? The article describes an event. Articles such
as timelines, fact files, agendas or summaries
were discarded (all these kinds of articles were
tagged by specific keywords, making the filter-
ing easy).
? The distance between the two DCTs does not
exceed 7 days.
? There are at least 2 words in common in the set
of keywords and/or 2 proper nouns in common
in the first paragraph of each article.
These last two restrictions are important, but
necessary, in order to give annotators a chance to
find some related articles. Pure random selection of
pairs over a collection of 1.5 million articles would
be impractical.
We assume that the title and the first paragraph
describe the event associated with the document.
This is a realistic hypothesis, since the basic rules
of journalism impose that the first sentence should
summarize the event by informing on the ?5 Ws?
(What, Who, When, Where, Why). However, reading
more than the first paragraph is sometimes necessary
to determine whether a relation exists between two
events.
3.2 Relation Annotation
Two annotators were asked to attribute the following
relations between each pair of articles presented by
the annotation interface system.
In a first annotation round, 7 types of relations
were annotated:
? Three relations concerning cases where the two
articles relate the same event or an update:
? number update, when a document is an
update of numerical data (see top of Fig-
ure 5),
? form update, when the second document
brings only minor corrections,
960
Figure 3: Examples of relation continuation between two
documents.
Figure 4: Examples of relation continuation-reaction be-
tween two documents.
? details, when the second document gives
more details about the events (see bottom
of Figure 5).
? development of same story, when the two docu-
ments relate two events which are included into
a third one;
? continuation, when an event is the continuation
or the consequence of a previous one. Figure 3
shows two examples of such a relation. It is
important to make clear that a continuation re-
lation is more than a simple thematic relation,
it implies a natural prolongation between two
events. For example, two sport events of the
same Olympic Games, or two different attacks
in Iraq, shall not be linked together unless a di-
rect link between both is specified in the arti-
cles.
? reaction, a subset of continuation, when a doc-
ument relates the reaction of someone to an-
other event, as illustrated by the example in
Figure 4.
Figure 5: Example of relations same-event between two
documents: update on casualties (top) or details (bottom).
? nil, when no relation can be identified between
the two documents.
The inter-annotator agreement was calculated
with Cohen?s Kappa measure (Cohen, 1960) across
150 pairs: ? ? 0.68. The agreement was low for
the first 4 types of relations mostly because the dif-
ference between relations was not clear enough. We
therefore aggregated the number update, form up-
date and details relations into a more generic and
consensual same-event relation (see Figure 5). We
also discarded the development of same story rela-
tion, leaving only same-event, continuation and re-
action.
Annotation guidelines were modified and a sec-
ond annotation round was carried out: only the
same-event, continuation, reaction and nil relations
were annotated. Inter-annotator agreement across
150 pairs was then ? ? 0.83, which is a good agree-
ment.
3.3 Relation Set Extension
This manual annotation would have led to very
sparse temporal graphs without the two following
additional processes:
? When the annotator attributed a ?non-nil? rela-
tion to a pair of documents, the annotation sys-
tem suggested other pairs to annotate around
the concerned articles.
? Same-event and continuation relations are tran-
sitive: if A same-event B and B same-event
C, then A same-event C (and respectively for
961
Pair number Learning Evaluation
Same event 762 458 304
Continuation 1134 748 386
Reaction 182 123 59
Nil 918 614 304
TOTAL 2996 1943 1053
Table 1: Characteristics of the corpus.
continuation). Then, when the annotation was
done, a transitive closure was performed on the
entire graph, in order to get more relations with
low effort (and to detect and correct some in-
consistencies in the annotations).
Finally, almost 3,000 relations were annotated.
2{3 of the annotated pairs were used for development
and learning phases, while 1{3 were kept for evalua-
tion purpose (cf. Table 1).
4 Building Temporal Graphs
As we explained in the introduction, the main pur-
pose of this paper is to show that it is possible to ex-
tract temporal graphs of events from multiple docu-
ments in a news corpus. This is achieved with the
help of redundancy of information in this corpus.
Therefore, we will use a cascade of classifiers and
other modules, each of them using the relations de-
duced by the previous one. All modules predict a
relation between two documents (i.e., two events).
We did not focus on complex algorithms or
classifiers for tuning our results, and most of our fea-
tures are very simple. The idea here is to show that
good results can be obtained in this original and use-
ful task. The process can be separated into 3 main
stages, illustrated in Figure 6:
A. Filtering out pairs that have no relation at all, i.e.
classifying between nil and non-nil relations;
B. Classifying between same-event and continua-
tion relations;
C. Extracting reactions from the set of continuation
relations.
All described classifiers use SMO (Platt, 1998),
the SVM optimization implemented into Weka (Hall
et al, 2009), with logistic models fitting (option ?-
M?). With this option, the confidence score of each
Figure 6: A 3-step classification.
prediction can be used, while SMO alone provides a
constant probability for all instances.
From now on, when considering a pair of doc-
uments, we will refer to the older document as doc-
ument 1, and to the more recent one as document 2.
The relations found between documents will be rep-
resented by a directed graph where documents are
vertices and relations are edges.
4.1 A. Nils versus non-nils
We first aim at separating nil relations (no relation
between two events) from other relations. This step
is achieved by two successive classifiers: the first
one (A.1) uses mainly similarity measures between
documents, while the second one (A.2) uses the re-
lations obtained by the first one.
4.1.1 Step A.1: Nil classifier, level 1
Features provided to the SMO classifier at this
first step are based on 3 different similarity measures
applied to pairs of titles, pairs of first sentences,
and pairs of entire documents: cosine similarity (as
implemented by Lucene search engine3), inclusion
similarity (rate of words from element 1 present in
element 2) and overlap similarity (number of words
present in both elements). This classifier is therefore
based on only 9 features.
4.1.2 Step A.2: Nil classifier, level 2
Finding relations on a document implies that the
described event is important enough to be addressed
by several articles (same-event) or to have conse-
quences (continuation). Consequently, if we find
such relations concerning a document, we are more
likely to find more of them, because this means that
3http://lucene.apache.org
962
the document has some importance. A typical exam-
ple is shown in Figure 7, where an event described
by several documents (on the left) has many contin-
uations. For this reason, we build a second classifier
A.2 using additional features related to the relations
found at step A.1:
? Number of non-nil edges, incoming to or out-
going from document 1 (2 features); the sum of
both numbers (1 extra feature);
? Number of non-nil edges, incoming to or out-
going from document 2 (2 features); the sum of
both numbers (1 extra feature);
? Number of non-nil edges found involving one
of the two documents (i.e., the sum of all edges
described above ? 1 feature).
These figures have been computed on training
set for training, and on result of step A.1 classifier
for testing. This new information will basically help
the classifier to be more optimistic toward non-nil
relations for documents having already non-nil rela-
tions.
4.2 B. Same-event versus Continuation
We are now working only with non-nil relations
(even if some relations may switch between nil and
non-nil during the transitive closure).
4.2.1 Step B.1: Relation classifier, level 1
Distinction between same-event and continua-
tion is made by the following sets of features:
? Date features:
? Difference between the two document cre-
ation times (DCTs): difference in days, in
hours, in minutes (3 features);
? Whether the creation time of doc. 1 is
mentioned in doc. 2. For this purpose,
we use the date normalization system de-
scribed in Kessler et al (2012).
? Cosine similarity between the first sen-
tence of doc. 1 and sentences of doc. 2
containing the DCT of doc. 1.
? Cosine similarity between the first sen-
tence of doc. 1 and the most similar sen-
tence of doc. 2.
Figure 7: An example of highly-connected subgraph,
corresponding to the development of an important story.
Same events are grouped by cliques (see Section 4.2.3)
and some redundant relations are not shown for clarity?s
sake.
These last three features come from the
idea that a continuation relation can be
made explicit in text by mentioning the
first event in the second document.
? Temporal features: whether words introducing
temporal relations occur in document 1 or doc-
ument 2. These manually-collected words can
be prepositions (after, before, etc.) or verbs
(follow, confirm, prove, etc.).
? Reaction features: whether verbs introducing
reactions occur in document 1 or document 2
(25 manually-collected verbs as approve, ac-
cept, welcome, vow, etc.).
? Opinion features: whether opinion words occur
in document 1 or document 2. The list of opin-
ion words comes from the MPQA subjectivity
lexicon (Wilson et al, 2005).
Only same-event relations classified with more
than 90% confidence by the classifier are kept, in
order to ensure a high precision (recall will be im-
proved at next step). This threshold has been set up
on development set.
4.2.2 Step B.2: Relation classifier, level 2
As for step A.2, a second classifier is im-
plemented, using the results of step B.1 with the
same manner as A.2 uses A.1 (collecting numbers
of same-event and continuation relations that have
been found by the previous classifier).
4.2.3 Steps B.3 and B.4: Transitive closure by
vote
As already stated, same-event and continuation
relations are transitive. Same-event is also symmet-
ric (A same-event B ? B same-event A). In the
963
graph formed by documents (vertices) and relations
(edges), it is then possible to find all cliques, i.e. sub-
sets of vertices such that every two vertices in the
subset are connected by a same-event relation, as il-
lustrated by Figure 7.
This step does not involve any learning phase.
Starting from the result of last step, we find all same-
event cliques in the graph by using the Bron and
Kerbosch (1973) algorithm. The transitive closure
process is then illustrated by Figure 8. If the classi-
fier proposed a relation between some documents of
a clique and some other documents (as D1, D2 and
D3), then a vote is necessary:
? If the document is linked to half or more of the
clique, then all missing links are created (Fig-
ure 8.a);
? Otherwise, the document is entirely discon-
nected from the clique (Figure 8.b).
This vote is done for same-event and contin-
uation relations (resp. steps B.3 and B.4). Only
cliques containing at least 3 nodes are used. A draw-
back of this voting procedure is that the final result
may not be independent of the voting order, in some
cases. However, it is assured that the result is con-
sistent, i.e. that no document will sit in two different
cliques, or that two documents from the same clique
will not have two different relations toward a third
document.
Note that this vote leads to improvements only
if the precision of the initial classifier is sufficiently
good. As we will see in Section 5.2, this is the case
in our situation, but one must keep in mind that a
vote leaning on too imprecise material would lead to
even worse results. Some experiments on the devel-
opment set show us that at least 70% precision was
necessary. Another way to ensure robustness of the
vote would be to apply the transitive closure only
on bigger cliques (e.g., containing more than 3 or
4 nodes).
4.3 C. Continuation versus Reaction
The approach for reaction extraction is different. We
first try to determine which documents describe re-
actions, regardless of which event it is a reaction to.
In the training set, all documents having at least one
incoming reaction edge are considered as reaction
?
?
Figure 8: Vote for same-event transitive closure. At the
top (a.), four nodes from the 5-node clique are linked to
document D1, which is enough to add D1 to the clique.
At the bottom (b.), only two nodes from the clique are
linked to documents D2 and D3, which is not enough to
add them into the clique. All edges from the clique to D2
and D3 are then deleted.
documents, all others are not. This distinction is
then learned with the same model and features as
for step B.1 (Section 4.2.1).
Once reaction documents have been selected,
the question is how to decide to which other doc-
ument(s) it must be linked. For example, in Fig-
ure 1, ?Queen Elizabeth expresses deep sorrow? is
a reaction to pope?s death, not to other documents in
the temporal thread (for example, not to other reac-
tions or to ?Pope in serious condition?). We did not
manage to build any classifier leading to satisfying
results at this point. We then proposed the two fol-
lowing basic heuristics, applied on all continuation
relations found after step B:
? A reaction reacts to only one event.
? A reaction reacts to an important event. Then,
among all continuation edges incoming to
the reaction document, we choose the biggest
same-event clique and create reaction edges
instead of continuations. If there is no
clique (only single nodes) or several same-size
cliques, all of them are tagged as reactions.
This module is called step C.1. Finally, a transitive
closure is performed for reactions (C.2).
964
Relation Precision Recall F1
NIL 0.754 0.821 0.786
same-event 0.832 0.812 0.822
continuation 0.736 0.696 0.715
? reaction 0.273 0.077 0.120
Table 2: Results obtained by the baseline system. Con-
tinuation scores do not consider reactions, only the last
row makes the distinction.
5 Results
5.1 Baseline
As a baseline, we propose a single classifier deter-
mining all classes at once, based on the same SMO
classifier with the exact same parameters and all
similarity-based features (on titles, first sentences
and entire documents) described in Section 4.1.1.
Table 2 shows results for this baseline. Unsur-
prisingly, same-event relations are quite well clas-
sified by this baseline, since similarity is the major
clue for this class. Continuation is much lower and
only 3 reactions are well detected.
5.2 System Evaluation
Results for all successive steps described in previous
section are shown in Figure 3. The final result of the
entire system is the last one. The first observation
is that redundancy-based steps improve performance
in a significant manner:
? Classifiers A.2 and B.2, using the number of
incoming or outgoing edges found at previous
steps, lead to very significant improvement.
? Among transitivity closure algorithms (B.3,
B.4, C.2), only same-event transitivity B.3
leads to significant improvement. Furthermore,
as we already noticed, these algorithms must be
used only when a good precision is guaranteed
at previous step. Otherwise, there is a risk of
inferring mostly bad relations. This is why we
biased classifier at step B.1 towards precision.
Finally, if this condition on precision is true,
transitivity closure is a robust way to get new
relations for free.
Results also tell that classification of relations
same-event and continuation is encouraging. Reac-
tion level gets a fair precision but a bad recall. This
Step Relation Precision Recall F1
A. NIL vs non-NIL classifier
A.1 NIL 0.764 0.815 0.788
non-NIL 0.921 0.896 0.910
A.2 NIL 0.907 0.811 0.857
??? non-NIL 0.925 0.966 0.945
B. Same-event vs continuation classifier
B.1 NIL 0.907 0.811 0.857
same-event 0.870 0.553 0.676
continuation 0.664 0.867 0.752
B.2 NIL 0.947 0.831 0.885
??? same-event 0.894 0.724 0.800
continuation 0.744 0.911 0.819
B.3 NIL 0.884 0.831 0.857
?? same-event 0.943 0.819 0.877
continuation 0.797 0.906 0.848
B.4 NIL 0.890 0.831 0.860
? same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
C. Reaction vs continuation
C.1 NIL 0.890 0.831 0.860
C.2 same-event 0.943 0.819 0.877
continuation 0.798 0.911 0.851
? reaction 0.778 0.359 0.491
Table 3: Results obtained at each step of the classifica-
tion process. The significance of the improvement wrt
previous step (when relevant) is indicated by the Student
t-test (?: non significant; ??: p ? 0.05 (significant); ???:
p ? 0.01 (highly significant)). Steps C.1 and C.2 are
aggregated, since their results are exactly the same.
is not catastrophic since most of the missed reactions
are tagged as continuation, which is still true (only
10% of the reaction relations are mistagged as same-
event). However, there is big room for improvement
on this point.
6 Application
As we showed in previous section, results for classi-
fication of same-event and continuation relations be-
tween documents are good enough to use this system
in an application that builds ?event threads? around
an input document. The use case is the following:
? The reader reads an article (let?s say, about the
death of John Paul II, article published on Feb.
4th, 2005 (UT) ? see Figure 1).
? A link in the page suggests the user to visualize
the event thread around this article.
965
Figure 9: An example of temporal thread obtained on the death of John Paul II for user visualization (see corresponding
relation graph in Figure 1).
? All articles within a period of 7 days around
the event, sharing at least two keywords with
the current document, are collected. All pairs
are given to the system4.
? When same-event cliques are found, only the
longest article (often, the most recent one) of
each clique is presented to the user. However,
the date and time presented to the user are those
of the first article relating the event.
? This leads to a graph with only continuation
and reaction relations. Edges are ?cleaned? so
that a unique thread is visible: relations that can
be obtained by transitivity are removed, edges
between two documents are kept only if no doc-
ument can be inserted in-between.
? Nodes are presented in chronological order.
The user can visualize and navigate through
this graph (the event thread shows only titles
but full articles can be accessed by clicking on
the node).
? When found, reactions are isolated from the
main thread.
? Such a temporal thread is potentially infinite. If
the user navigates through the end of the 7-day
window, the system must be run again on the
next time span.
4In case of very important events where ?all pairs? would be
too much, the temporal window is restrained. However, there is
no real time performance issue in this system.
Figure 9 presents the result of this process on
the partial temporal graph shown in Figure 1.
7 Conclusion
This article presents a task of multidocument tem-
poral graph building. We make the assumption that
each news article (after filtering) relates an event,
and we present a system extracting relations be-
tween articles. This system uses simple features and
algorithms but takes advantage of the important re-
dundancy of information in a news corpus, by in-
corporating redundancy information in a cascade of
classifiers, and by using transitivity of relations to
infer new links.
Finally, we present an application presenting
?event threads? to the user, in order to contextual-
ize the information and recomposing the story of an
event.
Now that the task is well defined and that en-
couraging results have been obtained, we envisage to
enrich classifiers by more fine-grained temporal and
lexical information, such as narrative chains (Cham-
bers and Jurafsky, 2008) for continuation relation
or event clustering (Barzilay et al, 2002) for same-
event relation. There is no doubt that reaction de-
tection can be improved a lot, by going beyond sim-
ple lexical features and discovering specific patterns.
We also intend to adapt the described system to other
languages than English.
966
References
A. Ahmed, Q. Ho, J. Eisenstein, E.P. Xing, A.J. Smola,
and C.H. Teo. 2011. Unified Analysis of Streaming
News. In Proceedings of WWW, Hyderabad, India.
A. Balahur, R. Steinberger, E. van der Goot,
B. Pouliquen, and M. Kabadjov. 2009. Opinion
Mining on Newspaper Quotations. In Proceedings
of International Joint Conference on Web Intelli-
gence and Intelligent Agent Technologies, Milano,
Italy.
R. Barzilay, N. Elhadad, and K.R. McKeown. 2002. In-
ferring Strategies for Sentence Ordering in Multi-
document News Summarization. Journal of Artifi-
cial Intelligence Research, 17:35?55.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications
of the ACM, 16(9):575?577.
N. Chambers and D. Jurafsky. 2008. Unsupervised
Learning of Narrative Event Chains. In Proceedings
of the 46th Annual Meeting of the ACL, Columbus,
USA.
N. Chambers, S. Wang, and D. Jurafsky. 2007. Classify-
ing temporal relations between events. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, Prague,
Czech Republic, June.
J. Cohen. 1960. A Coefficient of Agreement for Nom-
inal Scales. Educational and Psychological Mea-
surement, 43(6):551?558.
T. Fujiki, H. Nanba, and M. Okumura. 2003. Automatic
Acquisition of Script Knowledge from a Text Col-
lection. In Proceedings of EACL, Budapest, hun-
gary.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explo-
rations, 11(1).
2012. Proceedings of i2b2/VA Shared-Tasks and Work-
shop on Challenges in Natural Language Process-
ing for Clinical Data, Chicago, USA.
R. Kessler, X. Tannier, C. Hage`ge, V. Moriceau, and
A. Bittar. 2012. Finding Salient Dates for Build-
ing Thematic Timelines. In Proceedings of the 50th
Annual Meeting of the ACL, Jeju Island, Republic of
Korea.
R. Krestel, S. Bergler, and R. Witte. 2008. Minding the
Source: Automatic Tagging of Reported Speech in
Newspaper Articles. In Proceedings of LREC, Mar-
rakech, Morocco.
I. Mani, M. Verhagen, B. Wellner, C. Lee, and J. Puste-
jovsky. 2006. Machine learning of temporal rela-
tions. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the ACL, Sydney, Australia.
S.A. Mirroshandel and G. Ghassem-Sani. 2012. Towards
Unsupervised Learning of Temporal Relations be-
tween Events. In Journal of Artificial Intelligence
Research, volume 45.
J.C. Platt, 1998. Advances in Kernel Methods - Support
Vector Learning, chapter Fast Training of Support
Vector Machines Using Sequential Minimal Opti-
mization. MIT Press.
B. Pouliquen, R. Steinberger, and C. Best. 2007. Auto-
matic Detection of Quotations in Multilingual News.
In Proceedings of RANLP, Borovets, Bulgaria.
P.P. Talukdar, D. Wijaya, and T. Mitchell. 2012. Ac-
quiring Temporal Constraints between Relations. In
Proceedings of the 21st ACM international confer-
ence on Information and knowledge management,
Hawaii.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007 -
15: TempEval Temporal Relation Identification. In
Proceedings of SemEval workshop at ACL, Prague,
Czech Republic.
M. Verhagen, R. Sauri, T. Caselli, and J. Pustejovsky.
2010. SemEval-2010 - 13: TempEval-2. In Pro-
ceedings of SemEval workshop at ACL, Uppsala,
Sweden.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In Proceedings of HLT-EMNLP.
967
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 730?739,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Finding Salient Dates for Building Thematic Timelines
Re?my Kessler
LIMSI-CNRS
Orsay, France
kessler@limsi.fr
Xavier Tannier
Univ. Paris-Sud,
LIMSI-CNRS
Orsay, France
xtannier@limsi.fr
Caroline Hage`ge
Xerox Research Center Europe
Meylan, France
hagege@xrce.xerox.com
Ve?ronique Moriceau
Univ. Paris-Sud, LIMSI-CNRS
Orsay, France
moriceau@limsi.fr
Andre? Bittar
Xerox Research Center Europe
Meylan, France
bittar@xrce.xerox.com
Abstract
We present an approach for detecting salient
(important) dates in texts in order to auto-
matically build event timelines from a search
query (e.g. the name of an event or person,
etc.). This work was carried out on a corpus
of newswire texts in English provided by the
Agence France Presse (AFP). In order to ex-
tract salient dates that warrant inclusion in an
event timeline, we first recognize and normal-
ize temporal expressions in texts and then use
a machine-learning approach to extract salient
dates that relate to a particular topic. We fo-
cused only on extracting the dates and not the
events to which they are related.
1 Introduction
Our aim here was to build thematic timelines for
a general domain topic defined by a user query.
This task, which involves the extraction of important
events, is related to the tasks of Retrospective Event
Detection (Yang et al, 1998), or New Event Detec-
tion, as defined for example in Topic Detection and
Tracking (TDT) campaigns (Allan, 2002).
The majority of systems designed to tackle this
task make use of textual information in a bag-of-
words manner. They use little temporal informa-
tion, generally only using document metadata, such
as the document creation time (DCT). The few sys-
tems that do make use of temporal information (such
as the now discontinued Google timeline), only ex-
tract absolute, full dates (that feature a day, month
and year). In our corpus, described in Section 3.1,
we found that only 7% of extracted temporal expres-
sions are absolute dates.
We distinguish our work from that of previous re-
searchers in that we have focused primarily on ex-
tracted temporal information as opposed to other
textual content. We show that using linguistic tem-
poral processing helps extract important events in
texts. Our system extracts a maximum of temporal
information and uses only this information to detect
salient dates for the construction of event timelines.
Other types of content are used for initial thematic
document retrieval. Output is a list of dates, ranked
from most important to least important with respect
to the given topic. Each date is presented with a set
of relevant sentences.
We can see this work as a new, easily evaluable
task of ?date extraction?, which is an important com-
ponent of timeline summarization.
In what follows, we first review some of the re-
lated work in Section 2. Section 3 presents the re-
sources used and gives an overview of the system.
The system used for temporal analysis is described
in Section 4, and the strategy used for indexing and
finding salient dates, as well as the results obtained,
are given in Section 51.
2 Related Work
The ISO-TimeML language (Pustejovsky et al,
2010) is a specification language for manual anno-
tation of temporal information in texts, but, to the
best of our knowledge, it has not yet actually been
used in information retrieval systems. Neverthe-
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.
730
less, (Alonso et al, 2007; Alonso, 2008; Kanhabua,
2009) and (Mestl et al, 2009), among others, have
highlighted that the analysis of temporal informa-
tion is often an essential component in text under-
standing and is useful in a wide range of informa-
tion retrieval applications. (Harabagiu and Bejan,
2005; Saquete et al, 2009) highlight the importance
of processing temporal expressions in Question An-
swering systems. For example, in the TREC-10 QA
evaluation campaign, more than 10% of questions
required an element of temporal processing in order
to be correctly processed (Li et al, 2005a). In multi-
document summarization, temporal processing en-
ables a system to detect redundant excerpts from
various texts on the same topic and to present re-
sults in a relevant chronological order (Barzilay and
Elhadad, 2002). Temporal processing is also useful
for aiding medical decision-making. (Kim and Choi,
2011) present work on the extraction of temporal in-
formation in clinical narrative texts. Similarly, (Jung
et al, 2011) present an end-to-end system that pro-
cesses clinical records, detects events and constructs
timelines of patients? medical histories.
The various editions of the TDT task have given
rise to the development of different systems that de-
tect novelty in news streams (Allan, 2002; Kumaran
and Allen, 2004; Fung et al, 2005). Most of these
systems are based on statistical bag-of-words mod-
els that use similarity measures to determine prox-
imity between documents (Li et al, 2005b; Brants
et al, 2003). (Smith, 2002) used spatio-temporal in-
formation from texts to detect events from a digital
library. His method used place/time collocations and
ranked events according to statistical measures.
Some efforts have been made for automatically
building textual and graphical timelines. For ex-
ample, (Allan et al, 2001) present a system that
uses measures of pertinence and novelty to con-
struct timelines that consist of one sentence per date.
(Chieu and Lee, 2004) propose a similar system that
extracts events relevant to a query from a collection
of documents. Important events are those reported
in a large number of news articles and each event is
constructed according to one single query and rep-
resented by a set of sentences. (Swan and Allen,
2000) present an approach to generating graphical
timelines that involves extracting clusters of noun
phrases and named entities. More recently, (Yan et
al., 2011b; Yan et al, 2011a) used a summarization-
based approach to automatically generate timelines,
taking into account the evolutionary characteristics
of news.
3 Resources and System Overview
3.1 AFP Corpus
For this work, we used a corpus of newswire texts
provided by the AFP French news agency. The En-
glish AFP corpus is composed of 1.3 million texts
that span the 2004-2011 period (511 documents/day
in average and 426 millions words). Each document
is an XML file containing a title, a date of creation
(DCT), set of keywords, and textual content split
into paragraphs.
3.2 AFP Chronologies
AFP ?chronologies? (textual event timelines) are a
specific type of articles written by AFP journal-
ists in order to contextualize current events. These
chronologies may concern any topic discussed in the
media, and consist in a list of dates (typically be-
tween 10 and 20) associated with a text describing
the related event(s). Figure 1 shows an example of
such a chronology. Further examples are given in
Figure 2. We selected 91 chronologies satisfying the
following constraints:
? All dates in the chronologies are between 2004
and 2011 to be sure that the related events
are described in the corpus. For example,
?Chronology of climax to Vietnam War? was
excluded because its corresponding dates do
not appear in the content of the articles.
? All dates in the chronology are anterior to the
chronology?s creation date. For example, the
chronology ?Space in 2005: A calendar?, pub-
lished in January 2005 and listing scheduled
events, was not selected (because almost no
rocket launches finally happened on the ex-
pected day).
? The temporal granularity of the chronology is
the day. For example, ?A timeline of how the
London transport attacks unfolded?, relating
the events hour by hour, is not in our focus.
731
<NewsML Version="1.2">
<NewsItem xml:lang="en">
<HeadLine>Key dates in Thai-
land?s political crisis</HeadLine>
<DateId>20100513T100519Z</DateId>
<NameLabel>Thailand-politics</NameLabel>
<DataContent>
<p>The following is a timeline of events since
the protests began, soon after Thailand?s Supreme
Court confiscated 1.4 billion dollars of Thaksin?s
wealth for abuse of power.</p>
<p>March 14: Tens of thousands of Red Shirts
demonstrate in the capital calling for Abhisit?s gov-
ernment to step down, [...]</p>
<p>March 28: The government and the Reds en-
ter into talks but hit a stalemate after two days
[...]</p>
<p>April 3: Tens of thousands of protesters move
from Bangkok?s historic district into the city?s com-
mercial heart [...]</p>
<p>April 7: Abhisit declares state of emergency
in capital after Red Shirts storm parliament.</p>
<p>April 8: Authorities announce arrest warrants
for protest leaders.</p>
. . .
</DataContent>
</NewsItem>
</NewsML>
Figure 1: Example of an AFP manual chronology.
For learning and evaluation purposes, all
chronologies were converted to a single XML
format. Each document was manually associated
with a user search query made up of the keywords
required to retrieve the chronology.
3.3 System Overview
Figure 3 shows the general architecture of the sys-
tem. First, pre-processing of the AFP corpus tags
and normalizes temporal expressions in each of the
articles (step ? in the Figure). Next, the corpus is
indexed by the Lucene search engine2 (step ?).
Given a query, a number of documents are re-
trieved by Lucene (?). These documents can be fil-
tered (?), and dates are extracted from the remain-
ing documents. These dates are then ranked in order
to show the most important ones to the user (?), to-
2http://lucene.apache.org
- Chronology of 18 months of trouble in Ivory Coast
- Chechen rebels? history of hostage-takings
- Iraqi political wrangling since March 7 election
- Athletics: Timeline of men?s 800m world record
- Major accidents in Chinese mines
- Space in 2005: A calendar
- Developments in Iranian nuclear standoff
- Chronology of climax to Vietnam War
- Timeline of ex-IMF chief?s sex attack case
- A timeline of how the London transport attacks un-
folded
Figure 2: Examples of AFP chronologies.
Figure 3: System overview.
gether with the sentences that contain them.
4 Temporal and Linguistic Processing
In this section, we describe the linguistic and tempo-
ral information extracted during the pre-processing
phase and how the extraction is carried out. We
rely on the powerful linguistic analyzer XIP (A??t-
Mokhtar et al, 2002), that we adapted for our pur-
poses.
4.1 XIP
The linguistic analyzer we use performs a deep syn-
tactic analysis of running text. It takes as input
XML files and analyzes the textual content enclosed
in the various XML tags in different ways that are
specified in an XML guide (a file providing instruc-
tions to the parser, see (Roux, 2004) for details).
XIP performs complete linguistic processing rang-
ing from tokenization to deep grammatical depen-
dency analysis. It also performs named entity recog-
732
nition (NER) of the most usual named entity cat-
egories and recognizes temporal expressions. Lin-
guistic units manipulated by the parser are either
terminal categories or chunks. Each of these units
is associated with an attribute-value matrix that con-
tains the unit?s relevant morphological, syntactic and
semantic information. Linguistic constituents are
linked by oriented and labelled n-ary relations de-
noting syntactic or semantic properties of the input
text. A Java API is provided with the parser so that
all linguistic structures and relations can be easily
manipulated by Java code.
In the following subsections, we give details of
the linguistic information that is used for the detec-
tion of salient dates.
4.2 Named Entity Recognition
Named Entity (NE) Recognition is one of the out-
puts provided by XIP. NEs are represented as unary
relations in the parser output. We used the exist-
ing NE recognition module of the English grammar
which tags the following NE types: location names,
person names and organization names. Ambigu-
ous NE types (ambiguity between type location or
organization for country names for instance) are
also considered.
4.3 Temporal Analysis
A previous module for temporal analysis was de-
veloped and integrated into the English grammar
(Hage`ge and Tannier, 2008), and evaluated during
TempEval campaign (Verhagen et al, 2007). This
module was adapted for tagging salient dates. Our
goal with temporal analysis is to be able to tag and
normalize3 a selected subset of temporal expressions
(TEs) which we consider to be relevant for our task.
This subset of expressions is described in the follow-
ing sections.
4.3.1 Absolute Dates
Absolute dates are dates that can be normalized
without external or contextual knowledge. This is
the case, for instance, of ?On January 5th 2003?.
In these expressions, all information needed for nor-
malization is contained in the linguistic expression.
3We call normalization the operation of turning a temporal
expression into a formated, fully specified representation. This
includes finding the absolute value of relative dates.
However, absolute dates are relatively infrequent in
our corpus (7%), so in order to broaden the cover-
age for the detection of salient dates, we decided to
consider relative dates, which are far more frequent.
4.3.2 DCT-relative Dates
DCT-relative temporal expressions are those
which are relative to the creation date of the docu-
ment. This class represents 40% of dates extracted
from the AFP corpus. Unlike the absolute dates, the
linguistic expression does not provide all the infor-
mation needed for normalization. External informa-
tion is required, in particular, the date which corre-
sponds to the moment of utterance. In news articles,
this is the DCT. Two sub-classes of relative TEs can
be distinguished. The first sub-class only requires
knowledge of the DCT value to perform the normal-
ization. This is the case of expressions like next Fri-
day, which correspond to the calendar date of the
first Friday following the DCT. The second sub-class
requires further contextual knowledge for normal-
ization. For example, on Friday will correspond ei-
ther to last Friday or to next Friday depending on
the context where this expression appears (e.g. He
is expected to come on Friday corresponds to next
Friday while He arrived on Friday corresponds to
last Friday). In such cases, the tense of the verb
that governs the TE is essential for normalization.
This information is provided by the linguistic analy-
sis carried out by XIP.
4.3.3 Underspecified Dates
Considering the kind of corpus we deal with
(news), we decided to consider TEs whose granu-
larity is at least equal to a day. As a result, TEs
were normalized to a numerical YYYYMMDD for-
mat (where YYYY corresponds to the year, MM to
the month and DD to the day). In case of TEs with
a granularity superior to the day or month, DD and
MM fields remain unspecified accordingly. How-
ever, these underspecified dates are not used in our
experiments.
4.4 Modality and Reported Speech
An important issue that can affect the calculation of
salient dates is the modality associated with time-
stamped events in text. For instance, the status of a
salient date candidate in a sentence like ?The meet-
733
ing takes place on Friday? has to be distinguished
from the one in ?The meeting should take place on
Friday? or ?The meeting will take place on Friday,
Mr. Hong said?. The time-stamped event meeting
takes place is factual in the first example and can
be taken as granted. In the second and third exam-
ples, however, the event does not necessarily occur.
This is expressed by the modality introduced by the
modal auxiliary should (second example), or by the
use of the future tense or reported speech (third ex-
ample). We annotate TEs with information regard-
ing the factuality of the event they modify. More
specifically, we consider the following features:
Events that are mentioned in the future: If a
time-stamped event is in the future tense, we add a
specific attribute MODALITY with value FUTURE to
the corresponding TE annotation.
Events used with a modal verb: If a time-
stamped event is introduced by a modal verb such
as should or would, then attribute MODALITY to the
corresponding TE annotation has the value MODAL.
Reported speech verbs: Reported speech verbs
(or verbs of speaking) introduce indirect or reported
speech. We dealt with time-stamped events gov-
erned by a reported speech verb, or otherwise ap-
pearing in reported speech. Once again, XIP?s lin-
guistic analysis provided the necessary information,
including the marking of reported speech verbs and
clause segmentation of complex sentences. If a rel-
evant TE modifies a reported speech verb, the anno-
tation of this TE contains a specific attribute, DE-
CLARATION=?YES?. If the relevant TE modifies
a verb that appears in a clause introduced by a re-
ported speech verb then the annotation contains the
attribute REPORTED=?YES?.
Note that the different annotations can be com-
bined (e.g. modality and reported speech can occur
for a same time-stamped event). For example, the
TE Friday in ?The meeting should take place on Fri-
day, Mr. Hong said? is annotated with both modality
and reported speech attributes.
4.5 Corpus-dependent Special Cases
While we developed the linguistic and temporal an-
notators, we took into account some specificities of
our corpus. We decided that the TEs today and
<DCT value="20050105"/>
<EC TYPE="TIMEX" value="unknown">The year
2004</EC> was the deadliest <EC TYPE="TIMEX"
value="unknown">in a decade</EC> for journalists
around the world, mainly because of the number of reporters
killed in <EC TYPE="LOCORG">Iraq</EC>, the
media rights group <EN TYPE="ORG">Reporters
Sans Frontieres</EN> (Reporters Without Bor-
ders) said <EC TYPE="DATE" SUBTYPE="REL"
REF="ST" DECLARATION="YES" value
="20050105">Wednesday</EC>.
Figure 4: Example of XIP output for a sample article.
now were not relevant for the detection of salient
dates. In the AFP news corpus, these expressions
are mostly generic expressions synomymous with
nowadays and do not really time-stamp an event
with respect to the DCT. Another specificity of the
corpus is the fact that if the DCT corresponds to a
Monday, and if an event in a past tense is described
with the associated TE on Monday or Monday, it
means that this event occurs on the DCT day itself,
and not on the Monday before. We adapted the TE
normalizer to these special cases.
4.6 Implementation and Example
As said previously, a NER module is integrated into
the XIP parser, which we used ?as is?. The TE tag-
ger and normalizer was adapted from (Hage`ge and
Tannier, 2008). We used the Java API provided with
the parser to perform the annotation and normal-
ization of TEs. The output for the linguistic and
temporal annotation consists in XML files where
only selected information is kept (structural infor-
mation distinguishing headlines from news content,
DCT), and enriched with the linguistic annotations
described before (NEs and TEs with relevant at-
tributes corresponding to the normalization and typ-
ing). Information concerning modality, future tense
and reported speech, appears as attributes on the TE
tag. Figure 4 shows an example of an analyzed ex-
cerpt of a news article.
In this news excerpt, only one TE (Wednesday) is
normalized as both The year 2004 and in a decade
are not considered to be relevant. The first one being
a generic TE and the second one being of granular-
ity superior to a year. The annotation of the relevant
TE has the attribute indicating that it time-stamps an
event realized by a reported speech verb. The nor-
734
malized value of the TE corresponds to the 5th of
January 2005, which is a Wednesday. NEs are also
annotated.
In the entire AFP corpus, 11.5 millions temporal
expressions were detected, among which 845,000
absolute dates (7%) and 4.6 millions normalized
relative dates (40%). Although we have not yet
evaluated our tagging of relative dates, the system
on which our current date normalization is based
achieved good results in the TempEval (Verhagen et
al., 2007) campaign.
5 Experiments and Results
In Section 5.1, we propose two baseline approaches
in order to give a good idea of the difficulty of the
task (Section 5.4 also discusses this point). In Sec-
tion 5.2, we present our experiments using simple
filtering and statistics on dates calculated by Lucene.
Finally, Section 5.3 gives details of our experiments
with a learning approach. In our experiments, we
used three different values to rank dates:
? occ(d) is the number of textual units (docu-
ments or sentences) containing the date d.
? Lucene provides ranked documents together
with their relevance score. luc(d) is the sum of
Lucene scores for textual units containing the
date d.
? An adaptation of classical tf.idf for dates:
tf.idf(d) = f(d).log
N
df(d)
where f(d) is the number of occurrences of
date d in the sentence (generally, f(d) = 1), N
is the number of indexed sentences and df(d)
is the number of sentences containing date d.
In all experiments (including baselines), timelines
have been built by considering only dates between
the first and the last dates of the corresponding man-
ual chronology. Processing runs were evaluated on
manually-written chronologies (see Section 3.2) ac-
cording to Mean Average Precision (MAP), which
is a widely accepted metric for ranked lists. MAP
gives a higher weight to higher ranked elements than
lower ranked elements. Significance of evaluation
results are indicated by the p-value results of the Stu-
dent?s t-test (t(90) = 1.9867).
Baselines ?only DCTs?
Model BLoccDCT BL
luc
DCT BL
tf.idf
DCT
MAP Score 0.5036 0.5521 0.5523
Baselines ?only absolute dates?
Model BLoccabs BL
luc
abs BL
tf.idf
abs
MAP Score 0.2627 0.2782 0.2778
Baselines ?absolute dates or alternatively DCTs?
Model BLoccmix BL
luc
mix BL
tf.idf
mix
MAP Score 0.4005 0.4110 0.4135
Table 1: MAP results for baseline runs.
5.1 Baseline Runs
BLDCT . Indexing and search were done at docu-
ment level (i.e. each AFP article, with its title
and keywords, is a document). Given a query,
the top 10,000 documents were retrieved. In
these runs, only the DCT for each document
was considered. Dates were ranked by one of
the three values described above (occ, luc or
tf.idf ) leading to runs BLoccDCT , BL
luc
DCT and
BLtfidfDCT .
BLabs. Indexing and search were done at sentence
level (document title and keywords are added
to sentence text). Given a query, the top 10,000
sentences were retrieved. Only absolute dates
in these sentences were considered. We thus
obtained runs BLoccabs, BL
luc
abs and BL
tfidf
abs .
Note that in this baseline, as well as in all the
subsequent runs, the information unit was the
sentence because a date was associated to a
small part of the text. The rest of the document
generally contained text that was not related to
the specific date.
BLmix. Same as BLabs, except that sentences con-
taining no absolute dates were considered and
associated to the DCT.
Table 1 shows results for these baseline runs.
Using only DCTs with Lucene scores or tf.idf(d)
already yielded interesting results, with MAP
around 0.55.
5.2 Salient Date Extraction with XIP Results
and Simple Filtering
In these experiments, we considered a Lucene index
to be built as follows: each document was taken to
735
Model MAP Score Model MAP Score
Salient date runs with all dates
SDluc 0.6962 SDtf.idf 0.6982
Salient dates runs with filtering
SDlucR 0.6975 SD
tf.idf
R 0.6996
SDlucF 0.6967 SD
tf.idf
F 0.6993
??
SDlucM 0.6978 SD
tf.idf
M 0.7005
?
SDlucD 0.7066
?? SDtf.idfD 0.7091
??
SDlucFMD 0.7086
?? SDtf.idfFMD 0.7112
??
SDlucRFMD 0.7127
?? SDtf.idfRFMD 0.7146
??
Table 2: MAP results for salient date extraction with XIP
and simple filtering. The significance of the improvement
due to filtering wrt no filtering is indicated by the Student
t-test (?: p < 0.05 (significant); ??: p < 0.01 (highly
significant)). The improvement due to using tf.idf(d) as
opposed to occ(d) is also highly significant.
be a sentence containing a normalized date. This
sentence was indexed with the title and keywords of
the AFP article containing it. Given a query, the top
10,000 documents were retrieved. Combinations be-
tween the following filtering operations were pos-
sible, by removing all dates associated with a re-
ported speech verb (R), a modal verb (M ) and/or
a future verb (F ). All these filtering operations were
intended to remove references to events that were
not certain, thereby minimizing noise in results.
These processing runs are named SD runs, with
indices representing the filtering operations. For ex-
ample, a run obtained by filtering modal and future
verbs is called SDM,F . In all combinations, dates
were ranked by the sum of Lucene scores for these
sentences (luc) or by tf.idf4.
Table 2 presents the results for this series of ex-
periments. MAP values are much higher than for
baselines. Using tf.idf(d) is only very slightly bet-
ter than luc. Filtering operations bring significant
improvement but the benefits of these different tech-
niques have to be further investigated.
5.3 Machine-Learning Runs
We used our set of manually-written chronologies
as a training corpus to perform machine learning
experiments. We used IcsiBoost5, an implementa-
4We do not present runs where dates are ranked by the num-
ber of times they appear in retrieved sentences (occ), as we did
for baselines, since results are systematically lower.
5http://code.google.com/p/icsiboost/
tion of adaptative boosting (AdaBoost (Freund and
Schapire, 1997)).
In our approach, we consider two classes: salient
dates are dates that have an entry in the manual
chronologies, while non-salient dates are all other
dates. This choice does, however, represent an im-
portant bias. The choices of journalists are indeed
very subjective, and chronologies must not exceed a
certain length, which means that relevant dates can
be thrown away. These issues will be discussed in
Section 5.4.
The classifier instances were not all sentences re-
trieved by the search engine. Using all sentences
would not yield a useful feature set. We rather ag-
gregated all sentences corresponding to the same
date before learning the classifier. Therefore, each
instance corresponded to a single date, and features
were figures concerning the set of sentences contain-
ing this date.
Features used in this series of runs are as follows:
1. Features representing the fact that the more
a date is mentioned, the more important it is
likely to be: 1) Sum of the Lucene scores for
all sentences containing the date 2) Number of
sentences containing the date 3) Ratio between
the total weights of the date and weights of all
returned dates 4) Ratio between the frequency
of the date and frequency of all returned dates;
2. Features representing the fact that an important
event is still written about, a long time after it
occurs: 1) Distance between the date and the
most recent mention of this date 2) Distance be-
tween the date and the DCT;
3. Other features: 1) Lucene?s best ranking of the
date 2) Number of times where the date is ab-
solute in the text 3) Number of times where
the date is relative (but normalized) in the text
4) Total number of keywords of the query in the
title, sentence and named entities of retrieved
documents 5) Number of times where the date
modifies a reported speech verb or is extracted
from reported speech.
We did not aim to classify dates, but rather to rank
them. Instead, we used the predicted probability
P (d) returned by the classifier, and mixed it with
the Lucene score of sentences, or with date tf.idf :
736
Model MAP Score
Machine-Learning Runs
MLlucbase 0.7033
MLluc 0.7905 ??
MLtf.idf 0.7918 ??
Table 3: MAP results for salient date extraction with
machine-learning. MLlucbase used Lucene scores and only
the first set of features described above. MLluc and
MLtf.idf used the three sets of features. They are both
highly significant under the t-test (p ? 6.10?4) wrt re-
spectively SDluc and SDtf.idf .
score(d) = P (d)? val(d)
where val(d) is either luc(d) or tf.idf(d).
Because the task is very subjective and (above
all) because of the low quantity of learning data, we
prefered not to opt for a ?learning to rank? approach.
We evaluated this approach with a classic 4-fold
cross-validation. Our 91 chronologies were ran-
domly divided into 4 sub-samples, each of them be-
ing used once as test data. The final scores, pre-
sented in Table 3, are the average of these 4 pro-
cesses. As shown in this table, the learning approach
improves MAP results by about 0.05 point.
5.4 Discussion and Final Experiment
Chronologies hand-written by journalists are a very
useful resources for evaluation of our system, as they
are completely dissociated from our research and are
an exact representation of the output we aim to ob-
tain. However, assembling such a chronology is a
very subjective task, and no clear method for evalu-
ation agreement between two journalists seems im-
mediately apparent. Only experts can build such
chronologies, and calculating this agreement would
require at least two experts from each domain, which
are hard to come by. One may then consider our sys-
tem as a useful tool for building a chronology more
objectively.
To illustrate this point, we chose four specific top-
ics6 and showed one of our runs on each topic to an
AFP expert for these subjects. We asked him to as-
sess the first 30 dates of these runs.
6Namely, ?Arab revolt timeline for Morocco?, ?Kyrgyzs-
tan unrest timeline?, ?Lebanon?s new government: a timeline?,
?Libya timeline?.
Topic APC APE
Morocco 0.5847 0.5718
Kyrgyzstan 0.6125 0.9989
Libya 0.7856 1
Lebanon 0.4673 0.7652
Table 4: Average precision results for manual evaluation
on 4 topics, against the original chronologies (APC), and
the expert assessment (APE).
Table 4 presents results for this evaluation, com-
paring average precision values obtained 1) against
the original, manual chronologies (APC), and 2)
against the expert assessment (APE). These values
show that, for 3 runs out of 4, many dates returned
by the system are considered as valid by the expert,
even if not presented in the original chronology.
Even if this experiment is not strong enough to
lead to a formal conclusion (post-hoc evaluation
with only 4 topics and a single assessor), this tends
to show that our system produces usable outputs and
that our system can be of help to journalists by pro-
viding them with chronologies that are as useful and
objective as possible.
6 Conclusion and Future Work
This article presents a task of ?date extraction? and
shows the importance of taking temporal informa-
tion into consideration and how with relatively sim-
ple temporal processing, we were able to indirectly
point to important events using the temporal infor-
mation associated with these events. Of course, as
our final goal consists in the detection of important
events, we need to take into account the textual con-
tent. In future work, we envisage providing, together
with the detection of salient dates, a semantic analy-
sis that will help determine the importance of events.
Another interesting direction in which we soon aim
to work is to consider all textual excerpts that are as-
sociated with salient dates, and use clustering tech-
niques to determine if textual excerpts correspond to
the same event or not. Finally, as our news corpus
is available both for English and French (compara-
ble corpus, not necessarily translations), we aim to
investigate cross-lingual extraction of salient dates
and salient events.
737
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond Shallowness: Incre-
mental Deep Parsing. Natural Language Engineering,
8:121?144.
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?01, pages 10?18.
James Allan, editor. 2002. Topic Detection and Tracking.
Springer.
Omar Alonso, Ricardo Baeza-Yates, and Michael Gertz.
2007. Exploratory Search Using Timelines. In
SIGCHI 2007 Workshop on Exploratory Search and
HCI Workshop.
Omar Rogelio Alonso. 2008. Temporal information re-
trieval. Ph.D. thesis, University of California at Davis,
Davis, CA, USA. Adviser-Gertz, Michael.
Regina Barzilay and Noemie Elhadad. 2002. Infer-
ring Strategies for Sentence Ordering in Multidocu-
ment News Summarization. Journal of Artificial In-
telligence Research, 17:35?55.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, SIGIR ?03, pages 330?337, New York, NY,
USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432.
Yoav Freund and Robert E. Schapire. 1997. A Decision-
Theoretic Generalization of On-Line Learning and an
Application to Boosting. Journal of Computer and
System Sciences, 55(1):119?139.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB ?05: Proceedings
of the 31st international conference on Very large data
bases, pages 181?192.
Caroline Hage`ge and Xavier Tannier. 2008. XTM: A Ro-
bust Temporal Text Processor. In Computational Lin-
guistics and Intelligent Text Processing, proceedings
of 9th International Conference CICLing 2008, pages
231?240, Haifa, Israel, February. Springer Berlin /
Heidelberg.
Sanda Harabagiu and Cosmin Adrian Bejan. 2005.
Question Answering Based on Temporal Inference. In
Proceedings of the Workshop on Inference for Textual
Question Answering, Pittsburg, Pennsylvania, USA,
July.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop,
BioNLP ?11, pages 146?154, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Nattiya Kanhabua. 2009. Exploiting temporal infor-
mation in retrieval of archived documents. In Pro-
ceedings of the 32nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR 2009, Boston, MA, USA, July 19-
23, 2009, page 848.
Youngho Kim and Jinwook Choi. 2011. Recogniz-
ing temporal information in korean clinical narra-
tives through text normalization. Healthc Inform Res,
17(3):150?5.
Giridhar Kumaran and James Allen. 2004. Text clas-
sification and named entities for new event detection.
In SIGIR ?04: Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 297?304.
ACM.
Wei Li, Wenjie Li, Qin Lu, and Kam-Fai Wong. 2005a.
A Preliminary Work on Classifying Time Granulari-
ties of Temporal Questions. In Proceedings of Second
international joint conference in NLP (IJCNLP 2005),
Jeju Island, Korea, oct.
Zhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma.
2005b. A Probabilistic Model for Restrospective
News Event Detection. In Proceedings of the 28th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, Sal-
vador, Brazil. ACM Press, New York City, NY, USA.
Thomas Mestl, Olga Cerrato, Jon ?lnes, Per Myrseth,
and Inger-Mette Gustavsen. 2009. Time Challenges -
Challenging Times for Future Information Search. D-
Lib Magazine, 15(5/6).
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. Iso-timeml: An international
standard for semantic annotation. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Claude Roux. 2004. Annoter les documents XML avec
un outil d?analyse syntaxique. In 11e`me Confrence
annuelle de Traitement Automatique des Langues Na-
turelles, Fe`s, Morocco, April. ATALA.
738
Estela Saquete, Jose L. Vicedo, Patricio Mart??nez-Barco,
Rafael Mun?oz, and Hector Llorens. 2009. Enhancing
QA Systems with Complex Temporal Question Pro-
cessing Capabilities. Journal of Articifial Intelligence
Research, 35:775?811.
David A. Smith. 2002. Detecting events with date and
place information in unstructured text. In JCDL ?02:
Proceedings of the 2nd ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 191?196, New York,
NY, USA. ACM.
Russell Swan and James Allen. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?00, pages 49?56, New York, NY, USA. ACM.
Marc Verhagen, Robert Gaizauskas, Franck Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 - 15: TempEval Temporal Rela-
tion Identification. In Proceedings of SemEval work-
shop at ACL 2007, Prague, Czech Republic, June. As-
sociation for Computational Linguistics, Morristown,
NJ, USA.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xi-
aoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, 27-31 July 2011, Edinburgh, UK, pages
433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceeding
of the 34th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR 2011, Beijing, China, July 25-29, 2011, pages
745?754.
Y. Yang, T. Pierce, and J. G. Carbonell. 1998. A study on
retrospective and on-line event detection. In Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, Melbourne, Australia, August. ACM Press,
New York City, NY, USA.
739
Proceedings of the TextGraphs-6 Workshop, pages 37?41,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
GrawlTCQ: Terminology and Corpora Building by Ranking Simultaneously
Terms, Queries and Documents using Graph Random Walks
Cle?ment de Groc
Syllabs
Univ. Paris Sud
LIMSI-CNRS
cdegroc@limsi.fr
Xavier Tannier
Univ. Paris Sud
LIMSI-CNRS
xtannier@limsi.fr
Javier Couto
Syllabs
MoDyCo (UMR 7114, CNRS-UPX)
jcouto@syllabs.com
Abstract
In this paper, we present GrawlTCQ, a new
bootstrapping algorithm for building special-
ized terminology, corpora and queries, based
on a graph model. We model links be-
tween documents, terms and queries, and use
a random walk with restart algorithm to com-
pute relevance propagation. We have evalu-
ated GrawlTCQ on an AFP English corpus of
57,441 news over 10 categories. For corpora
building, GrawlTCQ outperforms the Boot-
CaT tool, which is vastly used in the domain.
For 1,000 documents retrieved, we improve
mean precision by 25%. GrawlTCQ has also
shown to be faster and more robust than Boot-
CaT over iterations.
1 Introduction
Specialized terminology and corpora are key re-
sources in applications such as machine translation
or lexicon-based classification, but they are expen-
sive to develop because of the manual validation re-
quired. Bootstrapping is a powerful technique for
minimizing the cost of building these resources.
In this paper, we present GrawlTCQ1, a bootstrap-
ping algorithm for building specialized terminol-
ogy, corpora and queries: from a small set of user-
provided terms, GrawlTCQ builds the resources via
automated queries to a search engine. The algorithm
relies on a graph that encodes the three kinds of enti-
ties involved in the procedure (terms, documents and
queries) and relations between them. We model the
1GrawlTCQ stands for Graph RAndom WaLk for Terminol-
ogy, Corpora and Queries.
relevance propagation in our graph by using a ran-
dom walk with restart algorithm.
We use BootCaT (Baroni and Bernardini, 2004)
as our baseline because it is a similar algorithm that
has been vastly used and validated experimentally
in the domain. We have evaluated GrawlTCQ and
BootCaT on an AFP (Agence France Presse) En-
glish corpus of 57,441 news over 10 categories. Re-
sults show that, for corpora building, GrawlTCQ
significantly outperforms the BootCaT algorithm.
As this is an on-going work, further work is needed
to evaluate terminology and query results.
The article is structured as follows: in Section 2,
we review the related work in terminology and cor-
pora construction using bootstrapping techniques, as
well as random walk applications. In Section 3,
we describe GrawlTCQ. In Section 4, we evaluate
GrawlTCQ and compare its results with those pro-
vided by BootCaT. We conclude in Section 5.
2 Related Work
Several works using bootstrapping techniques have
been carried out in terminology and corpora cre-
ation. For example, (Ghani et al, 2005) has built mi-
nority language corpora from the web. The Web-as-
Corpus WaCky initiative (Baroni et al, 2009; Fer-
raresi et al, 2008; Sharoff, 2006) has built very large
web-derived corpus in various languages. They used
previously mentioned BootCaT tool to do this. As
the quality of the results is strongly dependent on
the quality of seed terms and the underlying search
engine, manual filtering is usually mandatory to en-
hance performance. GrawlTCQ uses a graph to au-
tomatically filter out erroneous terms and documents
37
Seeds
Queries
Combinations
Ranked documents
Search engine
Graph model
Filter and keep N-best
.....
.....
.....
.....
.....
.....
-----
-----
-----
-----
-----
-----
-----
-----
-----
Ranked
terms
Ranked
documents
Ranked
queries
User
Figure 1: Components of the GrawlTCQ algorithm.
and improve the system?s overall performance. The
manual filtering cost is therefore drastically reduced.
Graph modeling and random walks have been
applied with success to many different domains
of NLP, such as keyword and sentence extraction
(Mihalcea and Tarau, 2004), computer-science arti-
cles ranking (Nie et al, 2005), web pages ranking
(Haveliwala, 2002; Page et al, 1999; Richardson
and Domingos, 2002), WordNet-based word sense
disambiguation (Agirre and Soroa, 2009) and lexical
semantic relatedness (Hughes and Ramage, 2007),
or set expansion (Wang and Cohen, 2007). In this
paper, we confirm the relevance of this approach to
terminology and corpora bootstrapping.
3 Ranking simultaneously Terms, Queries
and Documents
3.1 The GrawlTCQ bootstrapping algorithm
Figure 1 shows the components of the GrawlTCQ
algorithm. Starting from user provided seed terms2,
GrawlTCQ iteratively creates queries, finds docu-
ments and extracts new terms. We model this boot-
strapping procedure with a graph that keeps all links
between documents, terms and queries. Our hypoth-
2These terms may be easily computed from a list of seed urls
or documents, using terminology extraction techniques.
Boxoffice Grammys BBC
Boxoffice
AND
Grammys
Grammys
AND
BBC
DOC 1 DOC 2
Jackson Beatles
Album
in query
(-1)
leads to
(-1)
contains
(-1)
Terms
Queries
Documents
Figure 2: Sample subgraph using ?boxoffice?, ?Gram-
mys? and ?BBC? as seed terms.
esis is that the information added will increase the
procedure?s robustness and overall performances.
The graph model (see figure 2) is built online. As
common terms will occur in many documents and
thus have high centrality, they will end with high
scores. In order to avoid this effect, document-
term edges are weighted with a TermHood measure
(Kageura and Umino, 1996) such as tfidf or log odds
ratio.
By using a random walk with restart algorithm,
also known as personalized PageRank (Haveliwala,
2002), terms, queries and documents are weighted
globally and simultaneously. At the end of each it-
eration of GrawlTCQ, a random walk is computed
and the resulting stationary distribution is used to
rank documents and terms3. If more documents are
needed, then the algorithm executes one more step.
Several parameters can be specified by the user,
such as the number of seed terms, the number of
terms composing a query, as well as the number of
documents retrieved for each query. In addition, the
algorithm may use the Internet (with search engines
as Google, Yahoo! or Bing), an Intranet, or both,
as data sources. When using the web as source, spe-
cific algorithms must be used to remove HTML boil-
erplate (Finn et al, 2001) and filter un-useful docu-
ments (duplicates (Broder, 2000), webspam and er-
ror pages (Fletcher, 2004)).
3As an additional result, we also obtain a ranked list of
queries.
38
3.2 Graph Walk
Considering a directed graph G = (V,E), the score
of a vertex Vi is defined as
PR(Vi) = (1? ?)?0 + ??
?
j?In(Vi)
PR(Vj)
|Out(Vj)|
where In(Vi) (resp. Out(Vi)) are Vi predecessors
(resp. successors). In the original PageRank algo-
rithm, a damping factor ? of 0.85 has been used and
the personalization vector (or teleportation vector)
?0 is distributed uniformly over V . On the contrary,
(Richardson and Domingos, 2002) and (Haveliwala,
2002) have proposed to personalize the PageRank
according to a user query or a chosen topic. Follow-
ing previous work (Page et al, 1999; Mihalcea and
Tarau, 2004), we have fixed the damping factor to
0.854 and the convergence threshold to 10?8.
As we have different types of edges carrying dif-
ferent relations, we slightly modify the PageRank
formula, as in (Wang and Cohen, 2007): when walk-
ing away from a node, the random surfer first picks
randomly a relation type and then chooses uniformly
between all edges of the chosen relation type. Bias-
ing the algorithm to insist more on seed terms is a
legitimate lead as these nodes represent the strong
base of our model. We thus use a custom ?0 distri-
bution that spreads weights uniformly over the seed
terms instead of the whole set of vertices.
4 Evaluation
Evaluating the proposed method on the web can
hardly be done without laborious manual annota-
tion. Moreover, web-based evaluations are not re-
producible as search engines index and ranking
functions change over time. This is especially a
problem when evaluating the impact of different pa-
rameters of our algorithm. In this article, we have
chosen to carry out an objective and reproducible
evaluation based on a stable and annotated document
collection.
The AFP has provided us an English corpus com-
posed of 57,441 news documents written between
January 1st and March 31, 2010. We have con-
sidered the 17 top-level categories from the IPTC
4During our experiments, we haven?t observed any signifi-
cant change when modifying this parameter.
Id Category #docs
01 Arts, culture and entertainment 3074
02 Crime, law and justice 5675
03 Disaster and accident 4602
04 Economy, business and finance 13321
08 Human interest 1300
11 Politics 17848
12 Religion and belief 1491
14 Social issue 1764
15 Sport 15089
16 Unrest, conflicts and war 8589
Table 1: AFP corpus categories distribution.
standard (http://www.iptc.org). Documents are cat-
egorized in one or more of those categories and are
annotated with various metadata, such as keywords.
As some categories contained too few documents,
we have only kept the 10 largest ones (see table 1).
The corpus was then indexed using Apache Lucene
(http://lucene.apache.org) in order to create a basic
search engine5. This setup has several advantages:
first, the document collection is stable and quantifi-
able. Documents are clean text written in a journal-
istic style. As they are already annotated, several
automatic evaluations can be run with different pa-
rameters. Finally, querying the search engine and
retrieving documents can be done efficiently. How-
ever, note that, as the document collection is lim-
ited, queries might return few or no results (which is
rarely the case on the web).
We have used the BootCaT algorithm as our base-
line. To the best of our knowledge this is the first at-
tempt to rigorously evaluate BootCaT performances.
We have compared both algorithms in exactly the
same conditions, on a task-based experiment: to re-
trieve 50, 100, 300, 500 and 1000 documents for
each category, independently of the number of itera-
tions done.
To be as close as possible to the original BootCaT
algorithm, we have weighted document-term edges
by log odds ratio. This measure allows us to dis-
tinguish common terms by using a reference back-
ground corpus. In all our experiments, we have used
the ukWac corpus (Ferraresi et al, 2008), a very
large web-derived corpus, for this purpose.
In order to select initial seed terms we have used
documents? metadata. We have computed the fre-
5All normalization features except lower-casing were dis-
abled to allow ease of reproducibility.
39
50 150 250 350 450 550 650 750 850 950Number of documents
0.1
0.2
0.3
0.4
0.5
0.6
Me
an 
Pre
cisi
on 
/ Re
cal
l
GrawlTCQ Precision
GrawlTCQ Recall
BootCaT Precision
BootCaT Recall
1 5 10# iter
0
1000
2000
3000
# d
ocs
GR
BC
Figure 3: Mean precision and recall at 50, 100, 300, 500
and 1000 documents (inset: Mean number of documents
/ number of iterations)
quency of occurrences of a keyword in a category
and have then divided this score by the sum of oc-
currences in all other categories. This strategy leads
to relevant seed terms that are not necessarily ex-
clusive to a category. For instance, selected seeds
for the 4th category are: economics, summary, rate,
opec, distress, recession, zain, jal, gold, and spyker.
We have fixed a number of parameters for our ex-
periments: at each iteration, the top-10 seeds are se-
lected (either from the initial set or from newly ex-
tracted terms). Queries are composed of 2 seeds, all
45 possible combinations6 are used and a total of 10
documents are retrieved for each query.
All scores are averaged over the 10 categories.
As can be seen in figure 3, GrawlTCQ shows much
more robustness and outperforms BootCaT by 25%
precision at 1000 documents. Detailed results for
each category are shown in table 2 and confirm the
relevance of our approach. Interestingly, BootCaT
and GrawlTCQ have very low precisions for the 14th
category (Social issue). Documents found in this
category are often ambiguous and both algorithms
fail to extract the domain terminology. We have also
plotted the number of documents in function of the
number of iterations as shown in figure 3 (inset).
The curve clearly shows that GrawlTCQ yields more
6When running the same experiment with randomly selected
tuples several times, we have found similar results when aver-
aging all runs output.
CatId
P@50 P@100 P@300 P@500 P@1000
GR BC GR BC GR BC GR BC GR BC
01 0.58 0.50 0.57 0.30 0.43 0.12 0.35 0.08 0.23 0.05
02 0.44 0.60 0.45 0.33 0.46 0.17 0.44 0.10 0.34 0.07
03 0.82 0.82 0.99 0.81 0.89 0.41 0.66 0.26 0.54 0.14
04 0.86 0.80 0.82 0.85 0.84 0.55 0.78 0.34 0.79 0.19
08 0.79 0.79 0.44 0.48 0.23 0.42 0.17 0.40 0.20 0.39
11 0.76 0.78 0.79 0.81 0.87 0.71 0.57 0.64 0.57 0.56
12 0.46 0.54 0.35 0.27 0.20 0.10 0.17 0.06 0.15 0.03
14 0.08 0.24 0.13 0.10 0.06 0.04 0.04 0.02 0.04 0.02
15 1.0 1.0 1.0 1.0 0.92 0.78 0.87 0.67 0.81 0.39
16 0.82 0.56 0.81 0.49 0.71 0.21 0.72 0.15 0.70 0.13
Table 2: Precision at various cutoffs by category
documents at a faster rate. This is due to the seed se-
lection process: GrawlTCQ?s queries lead to many
documents while BootCaT queries often lead to few
or no documents. Moreover, as we can see in figure
3, while fetching more documents faster, the mean
precision of GrawlTCQ is still higher than the Boot-
CaT one which shows that selected seeds are, at the
same time, more prolific and more relevant.
5 Conclusion
In this paper, we have tackled the problem of ter-
minology and corpora bootstrapping. We have pro-
posed GrawlTCQ, an algorithm that relies on a
graph model including terms, queries, and docu-
ments to track each entity origin. We have used a
random walk algorithm over our graph in order to
globally and simultaneously compute a ranking for
each entity type. We have evaluated GrawlTCQ on a
large news dataset and have shown interesting gain
over the BootCaT baseline. We have especially ob-
tained better results without any human intervention,
reducing radically the cost of manual filtering. We
are considering several leads for future work. First,
we must evaluate GrawlTCQ for query and term
ranking. Then, while preliminary experiments have
shown very promising results on the web, we would
like to setup a large scale rigorous evaluation. Fi-
nally, we will conduct further experiments on edges
weighting and seed terms selection strategies.
Acknowledgments
We would like to thank the AFP for providing us
the annotated news corpus. This work was par-
tially funded by the ANR research project ANR-08-
CORD-013.
40
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics on -
EACL, 2009.
Marco Baroni and Silvia Bernardini. 2004. BootCaT:
Bootstrapping Corpora and Terms from the Web. In
Proceedings of the LREC 2004 conference.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web : A Col-
lection of Very Large Linguistically Processed Web-
Crawled Corpora. In Proceedings of the LREC 2009
conference, volume 43, pages 209?226.
Andrei Z Broder. 2000. Identifying and Filtering Near-
Duplicate Documents. In Proceedings of the 11th An-
nual Symposium on Combinatorial Pattern Matching,
pages 1?10, London, UK. Springer-Verlag.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4), pages 47?54.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2001. Fact or fiction: Content classification for dig-
ital libraries. In DELOS Workshop: Personalisation
and Recommender Systems in Digital Libraries.
William H Fletcher. 2004. Making the Web More Useful
as a Source for Linguistic Corpora. Corpus Linguistics
in North America, (January 2003):191?205.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2005.
Building Minority Language Corpora by Learning to
Generate Web Search Queries. Knowl. Inf. Syst.,
7(1):56?83.
Taher H. Haveliwala. 2002. Topic-sensitive PageRank.
Proceedings of the eleventh international conference
on World Wide Web - WWW ?02, page 517.
Thad Hughes and Daniel Ramage. 2007. Lexical Se-
mantic Relatedness with Random Graph Walks. In
Proceedings of EMNLP, 2007, pages 581?589.
Kyo Kageura and Bin Umino. 1996. Methods of au-
tomatic term recognition: A review. Terminology,
3(2):259?289.
Rada Mihalcea and Paul Tarau. 2004. TextRank bringing
order into text. In Proceedings of EMNLP, pages 404?
411. Barcelona: ACL.
Zaiqing Nie, Yuanzhi Zhang, J.R. Wen, and W.Y. Ma.
2005. Object-level ranking: Bringing order to web
objects. In Proceedings of the 14th international con-
ference on World Wide Web, pages 567?574. ACM.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank Citation Ranking:
Bringing Order to the Web. Technical report, Stanford
InfoLab.
M. Richardson and P. Domingos. 2002. The intelligent
surfer: Probabilistic combination of link and content
information in pagerank. Advances in Neural Infor-
mation Processing Systems, 2:1441?1448.
Serge Sharoff. 2006. Creating general-purpose corpora
using automated search engine queries. M. Baroni, S.
Bernardini (eds.) WaCky! Working papers on the Web
as Corpus, Bologna, 2006, pages 63?98.
Richard C. Wang and William W. Cohen. 2007.
Language-independent set expansion of named enti-
ties using the web. Proceedings of IEEE International
Conference on Data Mining (ICDM 2007), Omaha,
NE, USA. 2007.
41
