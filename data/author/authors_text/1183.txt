TANGO: Bilingual Collocational Concordancer 
Jia-Yan Jian  
Department of Computer 
Science 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
g914339@oz.nthu.edu.tw 
Yu-Chia Chang  
Inst. of Information 
System and Applictaion 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
u881222@alumni.nthu.e
du.tw 
Jason S. Chang 
Department of Computer 
Science 
National Tsing Hua 
University 
101, Kuangfu Road, 
Hsinchu, Taiwan 
jschang@cs.nthu.edu.tw 
 
Abstract 
In this paper, we describe TANGO as a 
collocational concordancer for looking up 
collocations. The system was designed to 
answer user?s query of bilingual collocational 
usage for nouns, verbs and adjectives. We first 
obtained collocations from the large 
monolingual British National Corpus (BNC). 
Subsequently, we identified collocation 
instances and translation counterparts in the 
bilingual corpus such as Sinorama Parallel 
Corpus (SPC) by exploiting the word-
alignment technique. The main goal of the 
concordancer is to provide the user with a 
reference tools for correct collocation use so 
as to assist second language learners to acquire 
the most eminent characteristic of native-like 
writing. 
1 Introduction 
Collocations are a phenomenon of word 
combination occurring together relatively often. 
Collocations also reflect the speaker?s fluency of a 
language, and serve as a hallmark of near native-
like language capability. 
Collocation extraction is critical to a range of 
studies and applications, including natural 
language generation, computer assisted language 
learning, machine translation, lexicography, word 
sense disambiguation, cross language information 
retrieval, and so on.  
Hanks and Church (1990) proposed using point-
wise mutual information to identify collocations in 
lexicography; however, the method may result in 
unacceptable collocations for low-count pairs. The 
best methods for extracting collocations usually 
take into consideration both linguistic and 
statistical constraints. Smadja (1993) also detailed 
techniques for collocation extraction and 
developed a program called XTRACT, which is 
capable of computing flexible collocations based 
on elaborated statistical calculation. Moreover, log 
likelihood ratios are regarded as a more effective 
method to identify collocations especially when the 
occurrence count is very low (Dunning, 1993).    
Smadja?s XTRACT is the pioneering work on 
extracting collocation types. XTRACT employed 
three different statistical measures related to how 
associated a pair to be collocation type. It is 
complicated to set different thresholds for each 
statistical measure. We decided to research and 
develop a new and simple method to extract 
monolingual collocations. 
We also provide a web-based user interface 
capable of searching those collocations and its 
usage. The concordancer supports language 
learners to acquire the usage of collocation. In the 
following section, we give a brief overview of the 
TANGO concordancer. 
2 TANGO 
TANGO is a concordancer capable of answering 
users? queries on collocation use. Currently, 
TANGO supports two text collections: a 
monolingual corpus (BNC) and a bilingual corpus 
(SPC). The system consists of four main parts: 
2.1 Chunk and Clause Information 
Integrated 
For CoNLL-2000 shared task, chunking is 
considered as a process that divides a sentence into 
syntactically correlated parts of words. With the 
benefits of CoNLL training data, we built a 
chunker that turn sentences into smaller syntactic 
structure of non-recursive basic phrases to 
facilitate precise collocation extraction. It becomes 
easier to identify the argument-predicate 
relationship by looking at adjacent chunks. By 
doing so, we save time as opposed to n-gram 
statistics or full parsing. Take a text in CoNLL-
2000 for example: 
The words correlated with the same chunk tag 
can be further grouped together (see Table 1). For 
instance, with chunk information, we can extract 
 Confidence/B-NP in/B-PP the/B-NP 
pound/I-NP is/B-VP widely/I-VP ex-
pected/I-VP to/I-VP take/I-VP an-
other/B-NP sharp/I-NP dive/I-NP if/B-
SBAR trade/B-NP figures/I-NP for/B-PP 
September/B-NP  
 
 
(Note: Every chunk type is associated with two 
different chunk tags: B-CHUNK for the first word 
of the chunk and I-CHUNK for the other words in 
the same chunk) 
 
the target VN collocation ?take dive? from the 
example by considering the last word of two 
adjacent VP and NP chunks. We build a robust and 
efficient chunking model from training data of the 
CoNLL shared task, with up to 93.7% precision 
and recall. 
Sentence chunking Features
Confidence NP 
in PP 
the pound NP 
is expected to take VP 
another sharp dive NP 
if SBAR 
trade figures NP 
for PP 
September NP 
Table 1: Chunked Sentence 
In some cases, only considering the chunk 
information is not enough. For example, the 
sentence ??the attitude he had towards the 
country is positive?? may cause problem. With 
the chunk information, the system extracts out the 
type ?have towards the country? as a VPN 
collocation, yet that obviously cuts across two 
clauses and is not a valid collocation. To avoid that 
kind of errors, we further take the clause 
information into account. 
With the training and test data from CoNLL-
2001, we built an efficient HMM model to identify 
clause relation between words. The language 
model provides sufficient information to avoid 
extracting wrong collocations. Examples show as 
follows (additional clause tags will be attached): 
 
(1) ?.the attitude (S* he has *S) toward the country 
(2) (S* I think (S* that the people are most 
concerned with the question of (S* when 
conditions may become ripe. *S)S)S) 
As a result, we can avoid combining a verb with 
an irrelevant noun as its collocate as ?have toward 
country? in (1) or ?think ? people? in (2). When 
the sentences in the corpus are annotated with the 
chunk and clause information, we can 
consequently extract collocations more precisely. 
2.2 Collocation Type Extraction 
A large set of collocation candidates can be 
obtained from BNC, via the process of integrating 
chunk and clause information. We here consider 
three prevalent Verb-Noun collocation structures 
in corpus: VP+NP, VP+PP+NP, and VP+NP+PP. 
Exploiting Logarithmic Likelihood Ratio (LLR) 
statistics, we can calculate the strength of 
association between two collocates. The 
collocational type with threshold higher than 7.88 
(confidence level 99.5%) will be kept as one entry 
in our collocation type list. 
2.3 Collocation Instance Identification 
We subsequently identify collocation instances 
in the bilingual corpus (SPC) with the collocation 
types extracted from BNC in the previous step. 
Making use of the sequence of chunk types, we 
again single out the adjacent structures of VN, 
VPN, and VNP. With the help of chunk and clause 
information, we thus find the valid instances where 
the expected collocation types are located, so as to 
build a collocational concordance. Moreover, the 
quantity and quality of BNC also facilitate the 
collocation identification in another smaller 
bilingual corpus with better statistic measure. 
English sentence Chinese sentence 
If in this time no one 
shows concern for them, 
and directs them to 
correct thinking, and 
teaches them how to 
express and release 
emotions, this could very 
easily leave them with a 
terrible personality 
complex they can never 
resolve. 
???????
???????
???????
???????
???????
???????
???????
?? 
Occasionally some 
kungfu movies may 
appeal to foreign 
audiences, but these too 
are exceptions to the 
rule. 
??????
???????
???????
?????? 
Table 2: Examples of collocational translation 
memory 
 
Type Collocation types in BNC  
VN 631,638 
VPN 15,394 
VNP 14,008 
Table 3: The result of collocation types extracted 
from BNC and collocation instances identified in 
SPC 
2.4 Extracting Collocational Translation 
Equivalents in Bilingual Corpus 
When accurate instances are obtained from 
bilingual corpus, we continue to integrate the 
statistical word-alignment techniques (Melamed, 
1997) and dictionaries to find the translation 
candidates for each of the two collocates. We first 
locate the translation of the noun. Subsequently, 
we locate the verb nearest to the noun translation 
to find the translation for the verb. We can think of 
collocation with corresponding translations as a 
kind of translation memory (shows in Table 2).The 
implementation result of BNC and SPC shows in 
the Table 3, 4, and 5. 
3 Collocation Concordance 
With the collocation types and instances 
extracted from the corpus, we built an online 
collocational concordancer called TANGO for 
looking up translation memory. A user can type in 
any English query and select the intended part of 
speech of query and collocate. For example in 
Figure 1, after query for the verb collocates of the 
noun ?influence? is submitted, the results are 
displayed on the return page. The user can then 
browse through different collocates types and also 
click to get to see all the instances of a certain 
collocation type. 
Noun VN types 
Language 320 
Influence 319 
Threat 222 
Doubt 199 
Crime 183 
Phone 137 
Cigarette 121 
Throat 86 
Living 79 
Suicide 47 
Table 4: Examples of collocation types including 
a given noun  in BNC 
 
 
VN type Example 
Exert 
influence 
That means they would 
already be exerting their 
influence by the time the 
microwave background was 
born. 
Exercise 
influence 
The Davies brothers, Adrian 
(who scored 14 points) and 
Graham (four), exercised an 
important creative influence 
on Cambridge fortunes while 
their flankers Holmes and 
Pool-Jones were full of fire 
and tenacity in the loose. 
Wield 
influence 
Fortunately, George V had 
worked well with his father 
and knew the nature of the 
current political trends, but he 
did not wield the same 
influence internationally as his 
esteemed father. 
Table 5: Examples of collocation instances 
extracted from SPC 
Moreover, using the technique of bilingual 
collocation alignment and sentence alignment, the 
system will display the target collocation with 
highlight to show translation equivalents in con-
text. Translators or learners, through this web-
based interface, can easily acquire the usage of 
each collocation with relevant instances. This 
collocational concordancer is a very useful tool for 
self-inductive learning tailored to intermedi-ate or 
advanced English learners. 
Users can obtain the result of the VN or AN 
collocations related to their query. TANGO shows 
the collocation types and instances with 
collocations and translation counterparts high-
lighted. 
The evaluation (shows in Table 6) indicates an 
average precision of 89.3 % with regard to 
satisfactory. 
4 Conclusion and Future Work 
In this paper, we describe an algorithm that 
employs linguistic and statistical analyses to 
extract instance of VN collocations from a very 
large corpus; we also identify the corresponding 
translations in a parallel corpus. The algorithm is 
applicable to other types of collocations without 
being limited by collocation?s span. The main 
difference between our algorithm and previous 
work lies in that we extract valid instances instead 
of types, based on linguistic information of chunks 
and clauses. Moreover, in our research we observe 
Type The number of 
selected 
sentences 
Translation 
Memory 
Translation 
Memory (*) 
Precision of 
Translation 
Memory 
Precision of 
Translation 
Memory (*) 
VN 100 73 90 73 90 
VPN 100 66 89 66 89 
VNP 100 78 89 78 89 
Table 6: Experiment result of collocational translation memory from Sinorama parallel Corpus 
 
 
Figure 1: The caption of the table 
other types related to VN such as VPN (ie. verb + 
preposition + noun) and VNP (ie. verb + noun + 
preposition), which will also be crucial for 
machine translation and computer assisted 
language learning. In the future, we will apply our 
method to more types of collocations, to pave the 
way for more comprehensive applications. 
Acknowledgements 
This work is carried out under the project 
?CANDLE? funded by National Science Council 
in Taiwan (NSC92-2524-S007-002). Further 
information about CANDLE is available at 
http://candle.cs.nthu.edu.tw/. 
References  
Dunning, T (1993) Accurate methods for the statistics 
of surprise and coincidence, Computational 
Linguistics 19:1, 61-75. 
Hanks, P. and Church, K. W. Word association norms, 
mutual information, and lexicography. 
Computational Linguistics, 1990, 16(1), pp. 22-29. 
Melamed, I. Dan. "A Word-to-Word Model of 
Translational Equivalence". In Procs. of the ACL97. 
pp 490-497. Madrid Spain, 1997.  
Smadja, F. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1):143-177. 
 
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 41?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Computational Analysis of Move Structures in Academic Abstracts 
Jien-Chen Wu1   Yu-Chia Chang1   Hsien-Chin Liou2   Jason S. Chang1 
CS1 and FLL2, National Tsing Hua Univ. 
{d928322,d948353}@oz.nthu.edu.tw, hcliu@mx.nthu.edu.tw, 
jason.jschang@gmail.com 
Abstract 
This paper introduces a method for 
computational analysis of move 
structures in abstracts of research articles. 
In our approach, sentences in a given 
abstract are analyzed and labeled with a 
specific move in light of various 
rhetorical functions. The method involves 
automatically gathering a large number 
of abstracts from the Web and building a 
language model of abstract moves. We 
also present a prototype concordancer, 
CARE, which exploits the move-tagged 
abstracts for digital learning. This system 
provides a promising approach to Web-
based computer-assisted academic 
writing. 
1 Introduction 
In recent years, with the rapid development of 
globalization, English for Academic Purposes 
has drawn researchers' attention and become the 
mainstream of English for Specific Purposes, 
particularly in the field of English of Academic 
Writing (EAW). EAW deals mainly with genres, 
including research articles (RAs), reviews, 
experimental reports, and other types of 
academic writing. RAs play the most important 
role of offering researchers the access to actively 
participating in the academic and discourse 
community and sharing academic research 
information with one another. 
Abstracts are constantly regarded as the first 
part of RAs and few scholarly RAs go without an 
abstract. ?A well-prepared abstract enables 
readers to identify the basic content of a 
document quickly and accurately.? (American 
National Standards Institute, 1979) Therefore, 
RAs' abstracts are equally important to writers 
and readers. 
Recent research on abstract requires manually 
analysis, which is time-consuming and labor-
intensive. Moreover, with the rapid development 
of science and technology, learners are 
increasingly engaged in self-paced learning in a 
digital environment. Our study, therefore, 
attempts to investigate ways of automatically 
analyzing the move structure of English RAs? 
abstracts and develops an online learning system, 
CARE (Concordancer for Academic wRiting in 
English). It is expected that the automatic 
analytical tool for move structures will facilitate 
non-native speakers (NNS) or novice writers to 
be aware of appropriate move structures and 
internalize relevant knowledge to improve their 
writing. 
2 Macrostructure of Information in 
RAs 
Swales (1990) presented a simple and succinct 
picture of the organizational pattern for a RA?
the IMRD structure (Introduction, Methods, 
Results, and Discussion). Additionally Swales 
(1981, 1990) introduced the theory of genre 
analysis of a RA and a four-move scheme, which 
was later refined as the "Create a Research 
Space" (CARS) model for analyzing a RA?s 
introduction section.  
Even though Swales seemed to have 
overlooked the abstract section, in which he did 
not propose any move analysis, he himself 
plainly realized ?abstracts continue to remain a 
neglected field among discourse analysts? 
(Swales, 1990, p. 181). Salager-Meyer (1992) 
also stated, ?Abstracts play such a pivotal role in 
any professional reading? (p. 94). Seemingly 
researchers have perceived this view, so research 
has been expanded to concentrate on the abstract 
in recent years. 
Anthony (2003) further pointed out, ?research 
has shown that the study of rhetorical 
organization or structure of texts is particularly 
useful in the technical reading and writing 
classroom? (p. 185). Therefore, he utilized 
computational means to create a system, Mover, 
which could offer move analysis to assist 
abstract writing and reading. 
3 CARE 
Our system focuses on automatically 
computational analysis of move structures (i.e. 
41
Background, Purpose, Method, Result, and 
Conclusion) in RA abstracts. In particular, we 
investigate the feasibility of using a few 
manually labeled data as seeds to train a Markov 
model and to automatically acquire move-
collocation relationships based on a large number 
of unlabeled data. These relationships are then 
used to analyze the rhetorical structure of 
abstracts. It is important that only a small 
number of manually labeled data are required 
while much of move tagging knowledge is 
learned from unlabeled data. We attempt to 
identify which rhetorical move is correspondent 
to a sentence in a given abstract by using features 
(e.g. collocations in the sentence). Our learning 
process is shown as follows: 
 
(1)Automatically collect abstracts from the Web for 
     training 
(2)Manually label each sentence in a small set of given  
     abstracts 
(3)Automatically extract collocations from all abstracts 
(4)Manually label one move for each distinct collocation 
(5)Automatically expand collocations indicative of each 
    move 
(6)Develop a hidden Markov model for move tagging 
Figure 1: Processes used to learn collocation 
classifiers 
3.1 Collecting Training Data 
In the first four processes, we collected data 
through a search engine to build the abstract 
corpus A. Three specialists in computer science 
tagged a small set of the qualified abstracts based 
on our coding scheme of moves. Meanwhile, we 
extracted the collocations (Jian et al, 2004) from 
the abstract corpus, and labeled these extracted 
collocations with the same coding scheme.  
3.2 Automatically Expanding Collocations 
for Moves 
To balance the distribution in the move-tagged 
collocation (MTC), we expand the collocation for 
certain moves in this stage. We use the one-
move-per-collocation constraint to bootstrap, 
which mainly hinges on the feature redundancy 
of the given data, a situation where there is often 
evidence to indicate that a given should be 
annotated with a certain move. That is, given one 
collocation ci is tagged with move mi, all 
sentences S containing collocation ci will be 
tagged with mi as well; meanwhile, the other 
collocations in S are thus all tagged with mi. For 
example: 
 
Step 1. The collocation ?paper address? 
extracted from corpus A is labeled with the ?P? 
move. Then we use it to label other untagged 
sentences US (e.g. Examples (1) through (2)) 
containing ?paper address? as ?P? in A. As a 
result, these US become tagged sentences TS 
with ?P? move. 
 
  (1)This paper addresses the state explosion problem in  
       automata based ltl model checking. //P// 
  (2)This paper addresses the problem of fitting mixture  
       densities to multivariate binned and truncated data. //P// 
 
Step 2. We then look for other features (e.g. the 
collocation, ?address problem?) that occur in TS 
of A to discover new evidences of a ?P? move 
(e.g. Examples (3) through (4)). 
 
  (3)This paper addresses the state explosion problem in  
       automata based ltl model checking. 
  (4)This paper addresses the problem of fitting mixture   
       densities to multivariate binned and truncated data. 
 
Step 3. Subsequently, the feature ?address 
problem? can be further exploited to tag 
sentences which realize the ?P? move but do not 
contain the collocation ?paper address?, thus 
gradually expanding the scope of the annotations 
to A. For example, in the second iteration, 
Example (5) and (6) can be automatically tagged 
as indicating the ?P? move. 
 
   (5)In this paper we address the problem of query  
       answering using views for non-recursive data log    
       queries embedded in a Description Logics  
       knowledge base. //P// 
  (6)We address the problem of learning robust  
       plans for robot navigation by observing  
       particular robot behaviors. //P// 
 
From these examples ((5) and (6)), we can 
extend to another feature ?we address?, which 
can be tagged as ?P? move as well. The 
bootstrapping processes can be repeated until no 
new feature with high enough frequency is found 
(a sample of collocation expanded list is shown 
in Table1).  
 
Type Collocation Move Count of 
Collocation 
with mj 
Total of 
Collocation 
Occurrences
NV we present P 3,441 3,668 
NV we show R 1,985 2,069 
NV we propose P 1,722 1,787 
NV we describe P 1,505 1,583 
? ? ? ? ? 
Table 1: The sample of the expanded collocation 
list 
42
3.3 Building a HMM for Move Tagging 
The move sequence probability P(ti+1? ti) is 
given as the following description: 
We are given a corpus of unlabeled abstracts A 
= {A1,?, AN}. We are also given a small labeled 
subset S = {L1,?, Lk} of A, where each abstract 
Li consists of a sequence of sentence and move 
{t1, t2,?, tk}. The moves ti take out of a value 
from a set of possible move M = {m1,m2,?,mn}. 
Then 11
( | )( | )
( )
i i
i i
i
N t tP t t
N t
+
+
? ?= ? ?? ?
 
According to the bi-gram move sequence 
score (shown in Table 2), we can see move 
sequences follow a certain schematic pattern. For 
instance, the ?B? move is usually directly 
followed by the ?P? move or ?B? move, but not 
by the ?M? move. Also rarely will a ?P? move 
occur before a ?B? move. Furthermore, an 
abstract seldom have a move sequence wherein 
?P? move directly followed by the ?R? move, 
which tends to be a bad move structure. In sum, 
the move progression generally follows the 
sequence of "B-P-M-R-C". 
 
Table 2: The score of bi-gram move sequence 
(Note that ?$? denotes the beginning or the 
ending of a given abstract.) 
 
Finally, we synchronize move sequence and 
one-move-per-collocation probabilities to train a 
language model to automatically learn the 
relationship between those extracted linguistic 
features based on a large number of unlabeled 
data. Meanwhile, we set some parameters of the 
proposed model, such as, the threshold of the 
number of collocation occurring in a given 
abstract, the weight of move sequence and 
collocation and smoothing. Based on these 
parameters, we implement the Hidden Markov 
Model (HMM). The algorithm is described as the 
following: 
1 1 1 1 1( ,...., ) ( ) ( | ) ( | ) ( | )n i i i ip s s p t p s t p t t p s t?= ?  
The moves ti take out of a value from a set of 
possible moves M={m1, m2, ?., mk} (The 
following parameters ?1 and ?2 will be 
determined based on some heuristics). 
( | )i i ip S t m=  
= ?1 if Si contains a collocation in MTCj 
 i j=  
= ?2 if Si contains a collocation in MTCj 
 but i j?  
= 
1
k
 if Si does not contain a collocation MTCj 
The optimal move sequence t* is 
1 2
1 2 1
, ,...,
( *, *,..., *) ( ,..., | ,..., )arg max
n
n n i n
t t t
t t t p s s t t=  
In summary, at the beginning of training time, 
we use a few human move-tagged sentences as 
seed data. Then, collocation-to-move and move-
to-move probabilities are employed to build the 
HMM. This probabilistic model derived at the 
training stage will be applied at run time. 
4 Evaluation 
In terms of the training data, we retrieved 
abstracts from the search engine, Citeseer; a 
corpus of 20,306 abstracts (95,960 sentences) 
was generated. Also 106 abstracts composed of 
709 sentences were manually move-tagged by 
four informants. Meanwhile, we extracted 72,708 
collocation types and manually tagged 317 
collocations with moves.  
At run time, 115 abstracts containing 684 
sentences were prepared to be the training data. 
We then used our proposed HMM to perform 
some experimentation with the different values 
of parameters: the frequency of collocation types, 
the number of sentences with collocation in each 
abstract, move sequence score and collocation 
score.  
4.1 Performance of CARE 
We investigated how well the HMM model 
performed the task of automatic move tagging 
under different values of parameters. The 
parameters involved included the weight of 
transitional probability function, the number of 
sentences in an abstract, the minimal number of 
instance for the applicable collocations. Figure 2 
indicates the best precision of 80.54% when 627 
sentences were qualified with the set of various 
Move ti Move ti+1 - log P (ti+1|ti) 
$ B 0.7802 
$ P 0.6131 
B B 0.9029 
B M 3.6109 
B P 0.5664 
C $ 0.0000 
M $ 4.4998 
M C 1.9349 
M M 0.7386 
M R 1.0033 
P M 0.4055 
P P 1.1431 
P R 4.2341 
R $ 0.9410 
R C 0.8232 
R R 1.7677 
43
parameters, including 0.7 as the weight of 
transitional probability function and a frequency 
threshold of 18 for a collocation to be applicable, 
and the minimally two sentences containing an 
applicable collocation. Although it is important 
to have many collocations, it is crucial that we 
set an appropriate frequency threshold of 
collocation so as not to include unreliable 
collocation and lower the precision rate. 
 
Figure2: The results of tagging performance with 
different setting of weight and threshold for 
applicable collocations (Note that C_T denotes 
the frequency threshold of collocation) 
5 System Interface 
The goal of the CARE System is to allow a 
learner to look for instances of sentences labeled 
with moves. For this purpose, the system is 
developed with three text boxes for learners to 
enter queries in English (as shown in Figure3.): 
? Single word query (i.e. directly input one 
word to query)  
? Multi-word query (i.e. enter the result 
show to find citations that contain the 
three words, ?the?, ?paper? and ?show? 
and all the derivatives) 
? Corpus selection (i.e. learners can focus on 
a corpus in a specific domain)  
Once a query is submitted, CARE displays the 
results in returned Web pages. Each result 
consists of a sentence with its move annotation. 
The words matching the query are highlighted. 
Figure 3: The sample of searching result with the 
phrase ?the result show? 
6 Conclusion 
In this paper, we have presented a method for 
computational analysis of move structures in 
RAs' abstracts and addressed its pedagogical 
applications. The method involves learning the 
inter-move relationships, and some labeling rules 
we proposed. We used a large number of 
abstracts automatically acquired from the Web 
for training, and exploited the HMM to tag 
sentences with the move of a given abstract. 
Evaluation shows that the proposed method 
outperforms previous work with higher precision. 
Using the processed result, we built a prototype 
concordance, CARE, enriched with words, 
phrases and moves. It is expected that NNS can 
benefit from such a system in learning how to 
write an abstract for a research article. 
References 
Anthony, L. and Lashkia, G. V. 2003. Mover: A 
machine learning tool to assist in the reading and 
writing of technical papers. IEEE Trans. Prof. 
Communication, 46:185-193.  
American National Standards Institute. 1979. 
American national standard for writing abstracts. 
ANSI Z39, 14-1979. New York: Author.  
Jian, J. Y., Chang, Y. C., and Chang, J. S. 2004. 
TANGO: Bilingual Collocational Concordancer, 
Post & demo in ACL 2004, Barcelona. 
Salager-Meyer, F. S. 1992. A text-type and move 
analysis study of verb tense and modality 
distribution in medical English abstracts. English 
for Specific Purposes, 11:93-113.  
Swales, J.M. 1981. Aspects of article introductions. 
Birmingham, UK: The University of Aston, 
Language Studies Unit.   
Swales, J.M. 1990. Genre analysis: English in 
Academic and Research Settings. Cambridge 
University Press.  
44
Proceedings of the ACL 2010 Conference Short Papers, pages 115?119,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Collocation Suggestion in Academic Writing 
 Jian-Cheng Wu1 Yu-Chia Chang1,* Teruko Mitamura2 Jason S. Chang1 1 National Tsing Hua University Hsinchu, Taiwan {wujc86, richtrf, jason.jschang} @gmail.com 
2 Carnegie Mellon University  Pittsburgh, United States            teruko@cs.cmu.edu 
 Abstract In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speak-ers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica-tion. The system generates and ranks suggestions to assist learners? collocation usages in their academic writing with sat-isfactory results. * 1 Introduction The notion of collocation has been widely dis-cussed in the field of language teaching for dec-ades. It has been shown that collocation, a suc-cessive common usage of words in a chain, is important in helping language learners achieve native-like fluency. In the field of English for Academic Purpose, more and more researchers are also recognizing this important feature in academic writing. It is often argued that colloca-tion can influence the effectiveness of a piece of writing and the lack of such knowledge might cause cumulative loss of precision (Howarth, 1998). Many researchers have discussed the function of collocations in the highly conventionalized and specialized writing used within academia. Research also identified noticeable increases in the quantity and quality of collocational usage by                                                            * Corresponding author: Yu-chia Chang (Email address: richtrf@gmail.com) 
native speakers (Howarth, 1998). Granger (1998) reported that learners underuse native-like collo-cations and overuse atypical word combinations. This disparity in collocation usage between na-tive and non-native speakers is clear and should receive more attention from the language tech-nology community. To tackle such word usage problems, tradi-tional language technology often employs a da-tabase of the learners' common errors that are manually tagged by teachers or specialists (e.g. Shei and Pain, 2000; Liu, 2002). Such system then identifies errors via string or pattern match-ing and offer only pre-stored suggestions. Com-piling the database is time-consuming and not easily maintainable, and the usefulness is limited by the manual collection of pre-stored sugges-tions. Therefore, it is beneficial if a system can mainly use untagged data from a corpus contain-ing correct language usages rather than the error-tagged data from a learner corpus. A large corpus of correct language usages is more readily avail-able and useful than a small labeled corpus of incorrect language usages. For this suggestion task, the large corpus not only provides us with a rich set of common col-locations but also provides the context within which these collocations appear. Intuitively, we can take account of such context of collocation to generate more suitable suggestions. Contextual information in this sense often entails more lin-guistic clues to provide suggestions within sen-tences or paragraph. However, the contextual information is messy and complex and thus has long been overlooked or ignored. To date, most fashionable suggestion methods still rely upon the linguistic components within collocations as well as the linguistic relationship between mis-used words and their correct counterparts (Chang et al, 2008; Liu, 2009).  In contrast to other research, we employ con-textual information to automate suggestions for verb-noun lexical collocation. Verb-noun collo-cations are recognized as presenting the most 
115
challenge to students (Howarth, 1996; Liu, 2002). More specifically, in this preliminary study we start by focusing on the word choice of verbs in collocations which are considered as the most difficult ones for learners to master (Liu, 2002; Chang, 2008). The experiment confirms that our collocation writing assistant proves the feasibility of using machine learning methods to automatically prompt learners with collocation suggestions in academic writing.  2 Collocation Checking and Suggestion This study aims to develop a web service, Collo-cation Inspector (shown in Figure 1) that accepts sentences as input and generates the related can-didates for learners. In this paper, we focus on automatically pro-viding academic collocation suggestions when users are writing up their abstracts. After an ab-stract is submitted, the system extracts linguistic features from the user?s text for machine learning model. By using a corpus of published academic texts, we hope to match contextual linguistic clues from users? text to help elicit the most rele-vant suggestions. We now formally state the problem that we are addressing: Problem Statement: Given a sentence S writ-ten by a learner and a reference corpus RC, our goal is to output a set of most probable sugges-tion candidates c1, c2, ... , cm. For this, we train a classifier MC to map the context (represented as feature set f1, f2, ..., fn) of each sentence in RC to the collocations. At run-time, we predict these collocations for S as suggestions. 2.1 Academic Collocation Checker Train-ing Procedures Sentence Parsing and Collocation Extraction: We start by collecting a large number of ab-stracts from the Web to develop a reference cor-pus for collocation suggestion. And we continue to identify collocations in each sentence for the subsequent processing. Collocation extraction is an essential step in preprocessing data. We only expect to extract the collocation which comprises components having a syntactic relationship with one another. How-ever, this extraction task can be complicated. Take the following scholarly sentence from the reference corpus as an example (example (1)): 
(1) We introduce a novel method for learning to find documents on the web. 
 Figure 1. The interface for the collocation suggestion  
nsubj (introduce-2, We-1) det (method-5, a-3) amod (method-5, novel-4) dobj (introduce-2, method-5) prepc_for (introduce-2, learning-7) aux (find-9, to-8) ? ? Figure 2. Dependency parsing of Example (1)  Traditionally, through part-of-speech tagging, we can obtain a tagged sentence as follows (ex-ample (2)). We can observe that the desired col-location ?introduce method?, conforming to ?VERB+NOUN? relationship, exists within the sentence. However, the distance between these two words is often flexible, not necessarily rigid. Heuristically writing patterns to extract such verb and noun might not be effective. The patterns between them can be tremendously varied. In addition, some verbs and nouns are adjacent, but they might be intervened by clause and thus have no syntactic relation with one another (e.g. ?pro-pose model? in example (3)). 
(2) We/PRP  introduce/VB  a/DT  novel/JJ  method/NN  for/IN  learning/VBG  to/TO  find/VB  documents/NNS  on/IN  the/DT  web/NN  ./.  
(3) We proposed that the web-based model would be more ef-fective than corpus-based one. A natural language parser can facilitate the ex-traction of the target type of collocations. Such parser is a program that works out the grammati-cal structure of sentences, for instance, by identi-fying which group of words go together or which 
116
word is the subject or object of a verb. In our study, we take advantage of a dependency parser, Stanford Parser, which extracts typed dependen-cies for certain grammatical relations (shown in Figure 2). Within the parsed sentence of example (1), we can notice that the extracted dependency ?dobj (introduce-2, method-4)? meets the crite-rion.  Using a Classifier for the Suggestion task: A classifier is a function generally to take a set of attributes as an input and to provide a tagged class as an output. The basic way to build a clas-sifier is to derive a regression formula from a set of tagged examples. And this trained classifier can thus make predication and assign a tag to any input data. The suggestion task in this study will be seen as a classification problem. We treat the colloca-tion extracted from each sentence as the class tag (see examples in Table 1). Hopefully, the system can learn the rules between tagged classes (i.e. collocations) and example sentences (i.e. schol-arly sentences) and can predict which collocation is the most appropriate one given attributes ex-tracted from the sentences. Another advantage of using a classifier to automate suggestion is to provide alternatives with regard to the similar attributes shared by sentences. In Table 1, we can observe that these collocations exhibit a similar discourse function and can thus become interchangeable in these sentences. Therefore, based on the outputs along with the probability from the classifier, we can provide more than one adequate suggestions.  Feature Selection for Machine Learning: In the final stage of training, we build a statistical machine-learning model. For our task, we can use a supervised method to automatically learn the relationship between collocations and exam-ple sentences. We choose Maximum Entropy (ME) as our train-ing algorithm to build a collocation suggestion classifier. One advantage of an ME classifier is that in addition to assigning a classification it can provide the probability of each assignment. The ME framework estimates probabilities based on the principle of making as few assumptions as possible. Such constraints are derived from the training data, expressing relationships between features and outcomes.  Moreover, an effective feature selection can increase the precision of machine learning. In our study, we employ the contextual features which  
Table 1. Example sentences and class tags (colloca-tions) Example Sentence  Class tag   We introduce a novel method for learning to find documents on the web.  introduce  We presented a method of improving Japa-nese dependency parsing by using large-scale statistical information.  present  In this paper, we will describe a method of identifying the syntactic role of antece-dents, which consists of two phases  describe  In this paper, we suggest a method that automatically constructs an NE tagged cor-pus from the web to be used for learning of NER systems.  suggest   consist of two elements, the head and the ngram of context words:  Head: Each collocation comprises two parts, collocate and head. For example, in a given verb-noun collocation, the verb is the collocate as well as the target for which we provide suggestions; the noun serves as the head of collocation and convey the essential meaning of the collocation. We use the head as a feature to condition the classifier to generate candidates relevant to a given head.  Ngram: We use the context words around the target collocation by considering the correspond-ing unigrams and bigrams words within the sen-tence. Moreover, to ensure the relevance, those context words, before and after the punctuation marks enclosing the collocation in question, will be excluded. We use the parsed sentence from previous step (example (2)) to show the extracted context features1 (example (4)): 
(4) CN=method UniV_L=we UniV_R=a UniV_R=novel UniN_L=a UniN_L=novel UniN_R=for UniN_R=learn BiV_R=a_novel BiN_L=a_novel BiN_R=for_learn BiV_I=we_a BiN_I=novel_for  
                                                           1 CN refers to the head within collocation. Uni and Bi indi-cate the unigram and bigram context words of window size two respectively. V and N differentiate the contexts related to verb or noun. The ending alphabets L, R, I show the posi-tion of the words in context, L = left, R = right, and I = in between. 
117
2.2 Automatic Collocation Suggestion at Run-time After the ME classifier is automatically trained, the model is used to find out the best collocation suggestion. Figure 3 shows the algorithm of pro-ducing suggestions for a given sentence. The input is a learner?s sentence in an abstract, along with an ME model trained from the reference corpus.  In Step (1) of the algorithm, we parse the sen-tence for data preprocessing. Based on the parser output, we extract the collocation from a given sentence as well as generate features sets in Step (2) and (3). After that in Step (4), with the trained machine-learning model, we obtain a set of likely collocates with probability as predicted by the ME model. In Step (5), SuggestionFilter singles out the valid collocation and returns the best collocation suggestion as output in Step (6). For example, if a learner inputs the sentence like Example (5), the features and output candidates are shown in Table 2. 
(5) There are many investiga-tions about wireless network communication, especially it is important to add Internet transfer calculation speeds.  3 Experiment From an online research database, CiteSeer, we have collected a corpus of 20,306 unique ab-stracts, which contained 95,650 sentences. To train a Maximum Entropy classifier, 46,255 col-locations are extracted and 790 verbal collocates are identified as tagged classes for collocation suggestions. We tested the classifier on scholarly sentences in place of authentic student writings which were not available at the time of this pilot study. We extracted 364 collocations among 600 randomly selected sentences as the held out test data not overlapping with the training set. To automate the evaluation, we blank out the verb collocates within these sentences and treat these verbs directly as the only correct suggestions in question, although two or more suggestions may be interchangeable or at least appropriate. In this sense, our evaluation is an underestimate of the performance of the proposed method.    While evaluating the quality of the suggestions provided by our system, we used the mean recip-rocal rank (MRR) of the first relevant sugges-tions returned so as to assess whether the sugges-tion list contains an answer and how far up the answer is in the list as a quality metric of the sys-  
Procedure CollocationSuggestion(sent, MEmodel)   (1)   parsedSen = Parsing(sent) (2)   extractedColl = CollocationExtraction(parsedSent) (3)   features = AssignFeature(ParsedSent)   (4)   probCollection = MEprob(features, MEmodel)    (5)   candidate = SuggestionFilter(probCollection) (6)   Return candidate  Figure 3. Collocation Suggestion at Run-time  Table 2. An example from learner?s sentence Extracted Collocation Features Ranked Candidates 
add speed 
CN=speed UniV_L=important UniV_L=to UniV_R=internet UniV_R=transfer UniN_L=transfer UniN_L=calculation BiV_L=important_to BiV_R=internet_transfer BiN_L=transfer_calcula-tion BiV_I=to_intenet 
improve increase determine maintain ? ? 
 Table 3. MRR for different feature sets Feature Sets Included In Classifier MRR  Features of HEAD 0.407 Features of CONTEXT 0.469 Features of HEAD+CONTEXT 0.518  tem output. Table 3 shows that the best MRR of our prototype system is 0.518. The results indi-cate that on average users could easily find an-swers (exactly reproduction of the blanked out collocates) in the first two to three ranking of suggestions. It is very likely that we get a much higher MMR value if we would go through the lists and evaluate each suggestion by hand. Moreover, in Table 3, we can further notice that contextual features are quite informative in com-parison with the baseline feature set containing merely the feature of HEAD. Also the integrated feature set of HEAD and CONTEXT together achieves a more satisfactory suggestion result. 4 Conclusion Many avenues exist for future research that are important for improving the proposed method. For example, we need to carry out the experi-ment on authentic learners? texts. We will con-duct a user study to investigate whether our sys-tem would improve a learner?s writing in a real setting. Additionally, adding classifier features based on the translation of misused words in learners? text could be beneficial  (Chang et al, 
118
2008). The translation can help to resolve preva-lent collocation misuses influenced by a learner's native language. Yet another direction of this research is to investigate if our methodology is applicable to other types of collocations, such as AN and PN in addition to VN dealt with in this paper. In summary, we have presented an unsuper-vised method for suggesting collocations based on a corpus of abstracts collected from the Web. The method involves selecting features from the reference corpus of the scholarly texts. Then a classifier is automatically trained to determine the most probable collocation candidates with regard to the given context. The preliminary re-sults show that it is beneficial to use classifiers for identifying and ranking collocation sugges-tions based on the context features.  Reference Y. Chang, J. Chang, H. Chen, and H. Liou. 2008. An automatic collocation writing assistant for Taiwan-ese EFL learners: A case of corpus-based NLP technology. Computer Assisted Language Learn-ing, 21(3), pages 283-299. S. Granger. 1998. Prefabricated patterns in advanced EFL writing: collocations and formulae. In Cowie, A. (ed.) Phraseology: theory, analysis and applica-tions. Oxford University Press, Oxford, pages 145-160. P. Howarth. 1996. Phraseology in English Academic Writing. T?bingen: Max Niemeyer Verlag. P. Howarth. 1998. The phraseology of learner?s aca-demic writing. In Cowie, A. (ed.) Phraseology: theory, analysis and applications. Oxford Univer-sity Press, Oxford, pages 161-186. D. Hawking and N. Craswell. 2002. Overview of the TREC-2001 Web track. In Proceedings of the 10th Text Retrieval Conference (TREC 2001), pages 25-31. L. E. Liu. 2002. A corpus-based lexical semantic in-vestigation of verb-noun miscollocations in Taiwan learners? English. Unpublished master?s thesis, Tamkang University, Taipei, January. A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated suggestions for miscollocations. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 47-50. C. C. Shei and H. Pain. 2000. An ESL writer?s collo-cational aid. Computer Assisted Language Learn-ing, 13, pages 167-182. 
119
