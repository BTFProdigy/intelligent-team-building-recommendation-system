Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1815?1827,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploring Demographic Language Variations to Improve Multilingual
Sentiment Analysis in Social Media
Svitlana Volkova
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
Different demographics, e.g., gender or age,
can demonstrate substantial variation in their
language use, particularly in informal contexts
such as social media. In this paper we focus on
learning gender differences in the use of sub-
jective language in English, Spanish, and Rus-
sian Twitter data, and explore cross-cultural
differences in emoticon and hashtag use for
male and female users. We show that gen-
der differences in subjective language can ef-
fectively be used to improve sentiment anal-
ysis, and in particular, polarity classification
for Spanish and Russian. Our results show
statistically significant relative F-measure im-
provement over the gender-independent base-
line 1.5% and 1% for Russian, 2% and 0.5%
for Spanish, and 2.5% and 5% for English for
polarity and subjectivity classification.
1 Introduction
Sociolinguistics and dialectology have been study-
ing the relationships between language and speech at
the phonological, lexical and morphosyntactic lev-
els and social identity for decades (Picard, 1997;
Gefen and Ridings, 2005; Holmes and Meyerhoff,
2004; Macaulay, 2006; Tagliamonte, 2006). Re-
cent studies have focused on exploring demographic
language variations in personal email communica-
tion, blog posts, and public discussions (Boneva et
al., 2001; Mohammad and Yang, 2011; Eisenstein
et al, 2010; O?Connor et al, 2010; Bamman et al,
2012). However, one area that remains largely unex-
plored is the effect of demographic language varia-
tion on subjective language use, and whether these
differences may be exploited for automatic senti-
ment analysis. With the growing commercial im-
portance of applications such as personalized rec-
ommender systems and targeted advertising (Fan
and Chang, 2009), detecting helpful product review
(Ott et al, 2011), tracking sentiment in real time
(Resnik, 2013), and large-scale, low-cost, passive
polling (O?Connor et al, 2010), we believe that sen-
timent analysis guided by user demographics is a
very important direction for research.
In this paper, we focus on gender demographics
and language in social media to investigate differ-
ences in the language used to express opinions in
Twitter for three languages: English, Spanish, and
Russian. We focus on Twitter data because of its vol-
ume, dynamic nature, and diverse population world-
wide.1 We find that some words are more or less
likely to be positive or negative in context depend-
ing on the the gender of the author. For example, the
word weakness is more likely to be used in a pos-
itive way by women (Chocolate is my weakness!)
but in a negative way by men (Clearly they know
our weakness. Argggg). The Russian word ???????
(achieve) is used in a positive way by male users and
in a negative way by female users.
Our goals of this work are to (1) explore the gen-
der bias in the use of subjective language in so-
cial media, and (2) incorporate this bias into models
to improve sentiment analysis for English, Spanish,
and Russian. Specifically, in this paper we:
? investigate multilingual lexical variations in the
use of subjective language, and cross-cultural
1As of May 2013, Twitter has 500m users (140m of them
in the US) from more than 100 countries.
1815
emoticon and hashtag usage on a large scale in
Twitter data;2
? show that gender bias in the use of subjec-
tive language can be used to improve sentiment
analysis for multiple languages in Twitter.
? demonstrate that simple, binary features repre-
senting author gender are insufficient; rather, it
is the combination of lexical features, together
with set-count features representing gender-
dependent sentiment terms that is needed for
statistically significant improvements.
To the best of our knowledge, this work is the first
to show that incorporating gender leads to signifi-
cant improvements for sentiment analysis, particu-
larly subjectivity and polarity classification, for mul-
tiple languages in social media.
2 Related Work
Numerous studies since the early 1970?s have inves-
tigated gender-language differences in interaction,
theme, and grammar among other topics (Schiffman,
2002; Sunderland et al, 2002). More recent research
has studied gender differences in telephone speech
(Cieri et al, 2004; Godfrey et al, 1992) and emails
(Styler, 2011). Mohammad and Yang (2011) ana-
lyzed gender differences in the expression of senti-
ment in love letters, hate mail, and suicide notes, and
emotional word usage across genders in email.
There has also been a considerable amount of
work in subjectivity and sentiment analysis over
the past decade, including, more recently, in mi-
croblogs (Barbosa and Feng, 2010; Bermingham
and Smeaton, 2010; Pak and Paroubek, 2010; Bifet
and Frank, 2010; Davidov et al, 2010; Li et
al., 2010; Kouloumpis et al, 2011; Jiang et al,
2011; Agarwal et al, 2011; Wang et al, 2011;
Calais Guerra et al, 2011; Tan et al, 2011; Chen
et al, 2012; Li et al, 2012). In spite of the surge of
research in both sentiment and social media, only a
limited amount of work focusing on gender identi-
fication has looked at differences in subjective lan-
guage across genders within social media. Thel-
wall (2010) found that men and women use emoti-
cons to differing degrees on MySpace, e.g., female
2Gender-dependent and independent lexical resources of
subjective terms in Twitter for Russian, Spanish and English can
be found here: http://www.cs.jhu.edu/~svitlana/
users express positive emoticons more than male
users. Other researchers included subjective patterns
as features for gender classification of Twitter users
(Rao et al, 2010). They found that the majority of
emotion-bearing features, e.g., emoticons, repeated
letters, exasperation, are used more by female than
male users, which is consistent with results reported
in other recent work (Garera and Yarowsky, 2009;
Burger et al, 2011; Goswami et al, 2009; Argamon
et al, 2007). Other related work is that of Otter-
bacher (2010), who studied stylistic differences be-
tween male and female reviewers writing product
reviews, and Mukherjee and Liu (2010), who ap-
plied positive, negative and emotional connotation
features for gender classification in microblogs.
Although previous work has investigated gen-
der differences in the use of subjective language,
and features of sentiment have been used in gender
identification, to the best of our knowledge no one
has yet investigated whether gender differences in
the use of subjective language can be exploited to
improve sentiment classification in English or any
other language. In this paper we seek to answer this
question for the domain of social media.
3 Data
For the experiments in this paper, we use three sets
of data for each language: a large pool of data (800K
tweets) labeled for gender but unlabeled for senti-
ment, plus 2K development data and 2K test data
labeled for both sentiment and gender. We use the
unlabeled data to bootstrap Twitter-specific lexicons
and investigate gender differences in the use of sub-
jective language. We use the development data for
parameter tuning while bootstrapping, and the test
data for sentiment classification.
For English, we download tweets from the corpus
created by Burger et al (2011). This dataset con-
tains 2,958,103 tweets from 184K users, excluding
retweets. Retweets are omitted because our focus is
on the sentiment of the person tweeting; in retweets,
the words originate from a different user. All users
in this corpus have gender labels, which Burger et
al. automatically extracted from self-reported gen-
der on Facebook or MySpace profiles linked to by
the Twitter users. English tweets are identified using
a compression-based language identification (LID)
1816
tool (Bergsma et al, 2012). According to LID,
there are 1,881,620 (63.6%) English tweets from
which we select a random, gender-balanced sample
of 0.8M tweets. Burger?s corpus does not include
Russian and Spanish data on the same scale as En-
glish. Therefore, for Russian and Spanish we con-
struct a new Twitter corpus by downloading tweets
from followers of region-specific news and media
Twitter feeds. We use LID to identify Russian and
Spanish tweets, and remove retweets as before. In
this data, gender is labeled automatically based on
user first and last name morphology with a precision
above 0.98 for all languages.
Sentiment labels for tweets in the development
and test sets are obtained using Amazon Mechanical
Turk. For each tweet we collect annotations from
five workers and use majority vote to determine the
final label for the tweet. Snow et al (2008) show
that for a similar task, labeling emotion and valence,
on average four non-expert labelers are needed to
achieve an expert level of annotation. Below are the
example Russian tweets labeled for sentiment:
? Pos: ??? ?? ??????? ?????? ???? ? ??-
????? ????? ???????? ???... (It is a great
pleasure to go to bed after a long day at work...)
? Neg: ????????? ???????? ???????? ??-
???? ??? ??????! (Dear Mr. Prokhorov just
buy the elections!)
? Both: ????????? ???? ?? ??????? ?????!
?? ???? ?????????? ????????? ??? ????
????? :) (It was crowded at the local market!
But I got presents for my family:-))
? Neutral: ???? ????? ?????? ????? (Kiev is
a very old city).
Table 1 gives the distribution of tweets over senti-
ment and gender labels for the development and test
sets for English (EDEV, ETEST), Spanish (SDEV,
STEST), and Russian (RDEV, RTEST).
Data Pos Neg Both Neut ? ?
EDEV 617 357 202 824 1,176 824
ETEST 596 347 195 862 1,194 806
SDEV 358 354 86 1,202 768 1,232
STEST 317 387 93 1203 700 1,300
RDEV 452 463 156 929 1,016 984
RTEST 488 380 149 983 910 1,090
Table 1: Gender and sentiment label distribution in the
development and test sets for all languages.
4 Subjective Language and Gender
To study the intersection of subjective language and
gender in social media, ideally we would have a
large corpus labeled for both. Although our large
corpus is labeled for gender, it is not labeled for sen-
timent. Only the 4K tweets for each language that
compose the development and test sets are labeled
for both gender and sentiment. Obtaining sentiment
labels for all tweets would be both impractical and
expensive. Instead we use large multilingual senti-
ment lexicons developed specifically for Twitter as
described below. Using these lexicons we can begin
to explore the relationship between subjective lan-
guage and gender in the large pool of data labeled
for gender but unlabeled for sentiment. We also
look at the relationship between gender and the use
of different hashtags and emoticons. These can be
strong indicators of sentiment in social media, and in
fact are sometimes used to create noisy training data
for sentiment analysis in Twitter (Pak and Paroubek,
2010; Kouloumpis et al, 2011).
4.1 Bootstrapping Subjectivity Lexicons
Recent work by Banea et.al (2012) classifies meth-
ods for bootstrapping subjectivity lexicons into two
types: corpus-based and dictionary-based. Corpus-
based methods extract subjectivity lexicons from
unlabeled data using different similarity metrics
to measure the relatedness between words, e.g.,
Pointwise Mutual Information (PMI). Corpus-based
methods have been used to bootstrap lexicons
for ENGLISH (Turney, 2002) and other languages,
including ROMANIAN (Banea et al, 2008) and
JAPANESE (Kaji and Kitsuregawa, 2007).
Dictionary-based methods rely on relations be-
tween words in existing lexical resources. For exam-
ple, Rao and Ravichandran (2009) construct HINDI
and FRENCH sentiment lexicons using relations in
WordNet (Miller, 1995), Rosas et. al. (2012) boot-
strap a SPANISH lexicon using SentiWordNet (Bac-
cianella et al, 2010) and OpinionFinder,3 Clematide
and Klenner (2010), Chetviorkin et al (2012) and
Abdul-Mageed et. al. (2011) automatically expand
and evaluate GERMAN, RUSSIAN and ARABIC sub-
jective lexicons.
3www.cs.pitt.edu/mpqa/opinionfinder
1817
We use the corpus-based, language-independent
approach proposed by Volkova et al (2013) to boot-
strap Twitter-specific subjectivity lexicons. To start,
the new lexicon is seeded with terms from the initial
lexicon LI . On each iteration, tweets in the unla-
beled data are labeled using the current lexicon. If a
tweet contains one or more terms from the lexicon it
is marked subjective, otherwise neutral. Tweet po-
larity is determined in a similar way, but takes into
account negation. For every term not in the lexi-
con with a frequency threshold, the probability of
that word appearing in a subjective sentence is cal-
culated. The top k terms with a subjective probabil-
ity are then added to the lexicon. Bootstrapping con-
tinues until there are no more new terms meeting the
criteria to add to the lexicon. The parameters are op-
timized using a grid search on the development data
using F-measure for subjectivity classification. In
Table 2 we report size and term polarity from the ini-
tial LI and the bootstrapped LB lexicons. Although
more sophisticated bootstrapping methods exist, this
approach has been shown to be effective for atomi-
cally learning subjectivity lexicons in multiple lan-
guages on a large scale without any external, rich,
lexical resources, e.g., WordNet, or advanced NLP
tools, e.g., syntactic parsers (Wiebe, 2000) or infor-
mation extraction tools (Riloff and Wiebe, 2003).
For English, seed terms for bootstrapping are
the strongly subjective terms in the MPQA lexicon
(Wilson et al, 2005). For Spanish and Russian, the
seed terms are obtained by translating the English
seed terms using a bi-lingual dictionary, collecting
subjectivity judgments from MTurk on the transla-
tions, filtering out translations that are not strongly
subjective, and expanding the resulting word lists
with plurals and inflectional forms.
To verify that bootstrapping does provide a bet-
ter resource than existing dictionary-expanded lexi-
cons, we compare our Twitter-specific lexicons LB
English Spanish Russian
LEI L
E
B L
S
I L
S
B L
R
I L
R
B
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The initial LI and the bootstrapped LB (high-
lighted) lexicon term count (LI ? LB) with polarity
across languages (thousands).
to the corresponding initial lexicons LI and the ex-
isting state-of-the-art subjective lexicons including:
? 8K strongly subjective English terms from Sen-
tiWordNet ?E (Baccianella et al, 2010);
? 1.5K full strength terms from the Spanish sen-
timent lexicon ?S (Perez-Rosas et al, 2012);
? 5K terms from the Russian sentiment lexicon
?R (Chetviorkin and Loukachevitch, 2012).
For that we apply rule-based subjectivity classi-
fication on the test data.4 This subjectivity classi-
fier predicts that a tweet is subjective if it contains
at least one, or at least two subjective terms from
the lexicon. To make a fair comparison, we auto-
matically expand ?E with plurals and inflectional
forms, ?S with the inflectional forms for verbs, and
?R with the inflectional forms for adverbs, adjec-
tives and verbs. We report precision, recall and F-
measure results in Table 3 and show that our boot-
strapped lexicons outperform the corresponding ini-
tial lexicons and the external resources.
Subj ? 1 Subj ? 2
P R F P R F
?E 0.67 0.49 0.57 0.76 0.16 0.27
LEI 0.69 0.73 0.71 0.79 0.34 0.48
LEB 0.64 0.91 0.75 0.7 0.74 0.72
?S 0.52 0.39 0.45 0.62 0.07 0.13
LSI 0.50 0.73 0.59 0.59 0.36 0.45
LSB 0.44 0.91 0.59 0.51 0.71 0.59
?R 0.61 0.49 0.55 0.74 0.17 0.29
LRI 0.72 0.34 0.46 0.83 0.07 0.13
LRB 0.64 0.58 0.61 0.74 0.23 0.35
Table 3: Precision, recall and F-measure results for sub-
jectivity classification using the external ?, initial LI and
bootstrapped LB lexicons for all languages.
4.2 Lexical Evaluation
With our Twitter-specific sentiment lexicons, we
can now investigate how the subjective use of these
terms differs depending on gender for our three lan-
guages. Figure 1 illustrates what we expect to find.
{F} and {M} are the sets of subjective terms used
by females and males, respectively. We expect that
some terms will be used by males, but never by fe-
males, and vice-versa. The vast majority, however,
will be used by both genders. Within this set of
shared terms, many words will show little difference
4A similar rule-based approach using terms from the
MPQA lexicon is suggested by (Riloff and Wiebe, 2003).
1818
Figure 1: Gender-dependent vs. independent subjectivity
terms (+ and - indicates term polarity).
Figure 2: The distribution of gender-dependent GDep
and gender-independent GInd sentiment terms.
in their subjective use when considering gender, but
there will be some words for which gender will have
an influence. Of particular interest for our work are
words in which the polarity of a term as it is used in
context is gender-influenced, the extreme case being
terms that flip their polarity depending on the gender
of the user. Polarity may be different because the
concept represented by the term tends to be viewed
in a different light depending on gender. There are
also words like weakness in which a more positive or
more negative word sense tends to be used by men
or women. In Figure 2 we show the distribution of
gender-specific and gender-independent terms from
the LB lexicons for all languages.
To identify gender-influenced terms in our lexi-
cons, we start by randomly sampling 400K male and
400K female tweets for each language from the data.
Next, for both genders we calculate the probability
of term ti appearing in a tweet with another subjec-
tive term (Eq.1), and the probability of it appearing
with a positive or negative term (Eq.2-3) from LB .
pti(subj?g) =
c(ti, P, g) + c(ti,N, g)
c(ti, g)
, (1)
where g ? F,M and P and N are positive and nega-
tive sets of terms from the initial lexicon LI .
pti(+?g) =
c(ti, P, g)
c(ti, P, g) + c(ti,N, g)
(2)
pti(??g) =
c(ti,N, g)
c(ti, P, g) + c(ti,N, g)
(3)
We introduce a novel metric ?p+ti to measure po-
larity change across genders. For every subjective
term ti we want to maximize the difference5:
?p+ti = ?pti(+?F ) ? pti(+?M)?
s.t.
RRRRRRRRRRRR
1 ?
tfsubjti (F )
tfsubjti (M)
RRRRRRRRRRRR
? ?, tfsubjti (M) ? 0, (4)
where p(+?F ) and p(+?M) are probabilities that
term ti is positive for females and males respec-
tively; tfsubjti (F ) and tf
subj
ti (M) are correspond-
ing term frequencies (if tfsubjti (F ) > tf
subj
ti (M) the
fraction is flipped); ? is a threshold that controls
the level of term frequency similarity6. The terms
in which polarity is most strongly gender-influenced
are those with ?? 0 and ?p+ti ? 1.
Table 4 shows a sample of the most strongly
gender-influenced terms from the initial LI and the
bootstrapped LB lexicons for all languages. A plus
(+) means that the term tends to be used positively
by women and minus (?) means that the term tends
to be used positively by men. For instance, in En-
glish we found that perfecting is used with negative
polarity by male users but with positive polarity by
female users; the term dogfighting has negative po-
larity for women but positive polarity for men.
4.3 Hashtags
People may also express positive or negative senti-
ment in their tweets using hashtags. From our bal-
anced samples of 800K tweets for each language,
we extracted 611, 879, and 71 unique hashtags for
English, Spanish, and Russian, respectively. As we
did for terms in the previous section, we evaluated
the subjective use of the hashtags. Some of these are
clearly expressing sentiment (#horror), while others
seem to be topics that people are frequently opinion-
ated about (#baseball, #latingrammy, #spartak).
5One can also maximize ?p?ti = ?pti(??F ) ? pti(??M)?.
6? = 0 means term frequencies are identical for both gen-
ders; ?? 1 indicates increasing gender divergence.
1819
English Initial Terms LEI ?p
+ ? English Bootstrapped Terms LEB ?p
+ ?
perfecting + 0.7 0.2 pleaseeeeee + 0.7 0.0
weakened + 0.1 0.0 adorably + 0.6 0.4
saddened ? 0.1 0.0 creatively ? 0.6 0.5
misbehaving ? 0.4 0.0 dogfighting ? 0.7 0.5
glorifying ? 0.7 0.5 overdressed ? 1.0 0.3
Spanish Initial Terms LSI Spanish Bootstrapped Terms L
S
B
fiasco (fiasco) + 0.7 0.3 cafe?na (caffeine) + 0.7 0.5
triunfar (succeed) + 0.7 0.0 claro (clear) + 0.7 0.3
inconsciente (unconscious) ? 0.6 0.2 cancio (dog) ? 0.3 0.3
horroriza (horrifies) ? 0.7 0.3 llevara (take) ? 0.8 0.3
groseramente (rudely) ? 0.7 0.3 recomendarlo (recommend) ? 1.0 0.0
Russian Initial Terms LRI Russian Bootstrapped Terms L
R
B
?????????? (magical) + 0.7 0.3 ???????? (dream!) + 0.7 0.3
???????????? (sensational) + 0.7 0.3 ???????? (dancing) + 0.7 0.3
????????? (adorable) ? 0.7 0.0 ?????? (complicated) ? 1.0 0.0
????????? (temptation) ? 0.7 0.3 ??????????? (young) ? 1.0 0.0
??????????? (deserve) ? 1.0 0.0 ??????? (achieve) ? 1.0 0.0
Table 4: Sample of subjective terms sorted by ?p+ to show lexical differences and polarity change across genders
(module is not applied as defined in Eq.1 to demonstrate the polarity change direction).
English ?p+ ? Spanish ?p+ ? Russian ?p+ ?
#parenting + 0.7 0.0 #rafaelnarro (politician) + 1.0 0.0 #????? (advise) + 1.0 0.0
#vegas ? 0.2 0.8 #amores (loves) + 0.2 1.0 #ukrlaw + 1.0 1.0
#horror ? 0.6 0.7 #britneyspears + 0.1 0.3 #spartak (soccer team) ? 0.7 0.9
#baseball ? 0.6 0.9 #latingrammy ? 0.5 0.1 #??? (dreams) ? 1.0 0.0
#wolframalpha ? 0.7 1.0 #metallica (music band) ? 0.5 0.8 #iphones ? 1.0 1.0
Table 5: Hashtag examples with opposite polarity across genders for English, Spanish, and Russian.
Table 5 gives the hashtags, correlated with sub-
jective language, that are most strongly gender-
influenced. Analogously to ?p+ values in Table 4, a
plus (+) means the hashtag is more likely to be used
positively by women, and a minus (?) means the
hashtag is more likely to be used positively by men.
For example, in English we found that male users
tend to express positive sentiment in tweets men-
tioning #baseball, while women tend to be nega-
tive about this hashtag. The opposite is true for the
hashtag #parenting.
4.4 Emoticons
We investigate how emoticons are used differently
by men and women in social media following the
work by (Bamman et al, 2012). For that we rely on
the lists of emoticons from Wikipedia7 and present
the cross-cultural and gender emoticon differences
in Figure 3. The frequency of each emoticon is given
7List of emoticons from Wikipedia http://en.
wikipedia.org/wiki/List_of_emoticons
on the right of each language chart, with probability
of use by a male user in that language given on the
x-axis. The top 8 emoticons are the same across lan-
guages and sorted by English frequency.
We found that emoticons in English data are used
more overall by female users, which is consistent
with previous findings in Schnoebelen?s work.8 In
addition, we found that some emoticons like :-)
(smile face) and :-o (surprised) are used equally by
both genders, at least in Twitter. When comparing
English emoticon usage to other languages, there are
some similarities, but also some clear differences. In
Spanish data, several emoticons are more likely to be
used by male than by female users, e.g., :-o (sur-
prised) and :-& (tongue-tied), and the difference in
probability of use by males and females is greater
for the emoticons, as compared to the same emoti-
cons for English. Interestingly, in Russian Twitter
8Language and emotion (talks, essays and reading notes)
www.stanford.edu/~tylers/emotions.shtml
1820
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 34.4K
 8.7K 
 4.1K 
 2.7K 
 0.9K 
 0.7K 
 0.4K 
 0.1K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
()  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 19.1K
 9.5K 
 1.5K 
 0.1K 
 0.3K 
 0.3K 
 0.1K 
 1.5K 
 0.1K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
:-o  
p(Male|Emoticon)
0.0 0.2 0.4 0.6 0.8 1.0
 41.5K
 4.5K 
 4.6K 
 0.4K 
 0.4K 
 0.1K 
 0.1K 
 0.4K 
 0.4K 
 0.1K 
:)  
:(  
:-)  
:-&  
:-(  
:[  
:-/  
8)  
%)  
()  
Figure 3: Probability of gender and emoticons for English, Spanish and Russian (from left to right).
data emoticons tend to be used more or equally by
male users rather than female users.
5 Experiments
The previous section showed that there are gender
differences in the use of subjective language, hash-
tags, and emoticons in Twitter. We aim leverage
these differences to improve subjectivity and po-
larity classification for the informal, creative and
dynamically changing multilingual Twitter data.9
For that we conduct experiments using gender-
independent GInd and gender-dependent GDep
features and compare the results to evaluate the in-
fluence of gender on sentiment classification.
We experiment with two classification ap-
proaches: (I) rule-based classifier which uses only
subjective terms from the lexicons designed to verify
if the gender differences in subjective language cre-
ate enough of a signal to influence sentiment classifi-
cation; (II) state-of-the-art supervised models which
rely on lexical features as well as lexicon set-count
features.10,11 Moreover, to show that the gender-
9For polarity classification we distinguish between positive
and negative instances, which is the approach typically reported
in the literature for recognizing polarity (Velikovich et al, 2010;
Yessenalina and Cardie, 2011; Taboada et al, 2011)
10A set-count feature is a count of the number of instances
from a set of terms that appears in a tweet.
11We also experimented with repeated punctuation (!!, ??)
and letters (nooo, reealy), which are often used in sentiment
classification in social media. However, we found these features
sentiment signal can be learned by more than one
classifier we apply a variety of classifiers imple-
mented in Weka (Hall et al, 2009). For that we do
10-fold cross validation over English, Spanish, and
Russian test data (ETEST, STEST and RTEST) la-
beled with subjectivity (pos, neg, both vs. neut) and
polarity (pos vs. neg) as described in Section 3.
5.1 Models
For the rule-basedGIndRBsubj classifier, tweets are la-
beled as subjective or neutral as follows:
GIndRBsubj = {
1 if w? ? f? ? 0.5,
0 otherwise
(5)
where w? ? f? stands for weighted set features, e.g.,
terms from LI only, emoticons E, or different part-
of-speech tags (POS) from LB weighted using w =
p(subj) = p(subj?M) + p(subj?F ) subjectivity
score as shown in Eq.1. We experiment with the
POS tags to show the contribution of each POS to
sentiment classification.
Similarly, for the rule-based GIndRBpol classifier,
tweets are labeled as positive or negative:
GIndRBpol = {
1 if w?+ ? f?+ ? w?? ? f??,
0 otherwise
(6)
where f?+, f?? are feature sets that include only posi-
tive and negative features fromLI orLB;w+ andw?
to be noisy and adding them decreased performance.
1821
0.6 0.7 0.8 0.9
0.62
0.64
0.66
0.68
0.70
Recall
Preci
sion
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
L_I
(a) Rule-based subjectivity
0.65 0.70 0.75 0.80 0.85 0.90
0.65
0.70
0.75
0.80
0.85
Recall
Preci
sion
L_I
+E
+A
+R
+V
+N
L_I
+E
+A
+R
+V
+N
(b) Rule-based polarity
BL R N B AB RF J48 SVM
C lassifiers
F-me
asure
0.55
0.60
0.65
0.70
0.75
0.80
0.85
GIn d Subj
AN D
GDepSubj
AN D
GIndPol
AN D
GDepPol
AN D
(c) SL subjectivity and polarity
Figure 4: Rule-based (RB) and Supervised Learning (SL) sentiment classification results for English. LI - the initial
lexicon, E - emoticons, A,R,V,N are adjectives, adverbs, verbs, nouns from LB .
are positive and negative polarity scores estimated
using Eq.2 - 3 such as: w+ = p(+?M) + p(+?F ) and
w? = p(??M) + p(??F ).
The gender-dependent rule-based classifiers are
defined in a similar way. Specifically, f? is replaced
by f?M and f?F in Eq.5 and f??, f?+ are replaced
by f?M?, f?F? and f?M+, f?F+ respectively in Eq.6.
We learn subjectivity s? and polarity p? score vectors
using Eq.1-3. The difference between GInd and
GDep models is that GInd scores w?, w?+ and w??
are not conditioned on gender.
For gender-independent classification using su-
pervised models, we build feature vectors using lex-
ical features V represented as term frequencies, to-
gether with set-count features from the lexicons:
f?GIndsubj = [LI , LB,E, V ];
f?GIndpol = [L
+
I , L
+
B,E
+, L?I , L
?
B,E
?, V ].
Finally, for gender-dependent supervised models,
we try different feature combinations. (A) We ex-
tract set-count features for gender-dependent subjec-
tive terms from LI , LB, and E jointly:
f?GDep?Jsubj = [L
M
I , L
M
B ,E
M , LFI , L
F
B,E
F , V ];
f?Dep?Jpol = [L
M+
I , L
M+
B ,E
M+, LF+I , L
F+
B ,E
F+
LM?I , L
M?
B ,E
M?, LF?I , L
F?
B ,E
F?, V ].
(B) We extract disjoint (prefixed) gender-specific
features (in addition to lexical features V ) by rely-
ing only on female set-count features when classify-
ing female tweets; and only male set-count features
for male tweets. We refer to the joint features as
GInd?J andGDep?J , and to the disjoint features
GInd ?D and GDep ?D.
5.2 Results
Figures 4a and 4b show performance improvements
for subjectivity and polarity classification under the
rule-based approach when taking into account gen-
der. The left figure shows precision-recall curves
for subjective vs. neutral classification, and the mid-
dle figure shows precision-recall curves for positive
vs. negative classification. We measure performance
starting with features from LI , and then incremen-
tally add emoticon features E and features from LB
one part of speech at a time to show the contribution
of each part of speech for sentiment classification.12
This experiment shows that there is a clear improve-
ment for the models parameterized with gender, at
least for the simple, rule-based model.
For the supervised models we experiment with
a variety of learners for English to show that gen-
der differences in subjective language improve sen-
timent classification for many learning algorithms.
We present the results in Figure 4c. For subjectiv-
ity classification, Support Vector Machines (SVM),
Naive Bayes (NB) and Bayesian Logistic Regres-
sion (BLR) achieve the best results, with improve-
ments in F-measure ranging from 0.5 - 5%. The po-
larity classifiers overall achieve much higher scores,
with improvements for GDep features ranging from
1-2%. BLR with Gaussian prior is the top scorer
12POS from the Twitter POSTagger (Gimpel et al, 2011).
1822
P R F A Arand P R F A Arand
English subj vs. neutral p(subj)=0.57 English pos vs. neg p(pos)=0.63
GIndLR 0.62 0.58 0.60 0.66 ? 0.78 0.83 0.80 0.71 ?
GDep ? J 0.64 0.62 0.63 0.68 0.66 0.80 0.83 0.82 0.73 0.70
?R,% +3.23 +6.90 +5.00 +3.03 3.03? +2.56 0.00 +2.50 +2.82 4.29?
GIndSVM 0.66 0.70 0.68 0.72 ? 0.79 0.86 0.82 0.77 ?
GDep ?D 0.66 0.71 0.68 0.72 0.70 0.80 0.87 0.83 0.78 0.76
?R,% ?0.45 +0.71 0.00 ?0.14 2.85? +0.38 +0.23 +0.24 +0.41 2.63?
Spanish subj vs. neutral p(subj)=0.40 Spanish pos vs. neg p(pos)=0.45
GIndLL 0.67 0.71 0.68 0.61 ? 0.71 0.63 0.67 0.71 ?
GDep ? J 0.67 0.72 0.69 0.62 0.61 0.72 0.65 0.68 0.71 0.68
?R,% 0.00 +1.40 +0.58 +0.73 1.64? +2.53 +3.17 +1.49 0.00 4.41?
GIndSVM 0.68 0.79 0.73 0.65 ? 0.66 0.65 0.65 0.69 ?
GDep ?D 0.68 0.79 0.73 0.66 0.65 0.68 0.67 0.67 0.71 0.68
?R,% +0.35 +0.21 +0.26 +0.54 1.54? +2.43 +2.44 +2.51 +2.08 4.41?
Russian subj vs. neutral p(subj)=0.51 Russian pos vs. neg p(pos)=0.58
GIndLR 0.66 0.68 0.67 0.67 ? 0.66 0.72 0.69 0.62 ?
GDep ? J 0.66 0.69 0.68 0.67 0.66 0.68 0.73 0.70 0.64 0.63
?R,% 0.00 +1.47 +0.75 0.00 1.51? +3.03 +1.39 +1.45 +3.23 1.58?
GIndSVM 0.67 0.75 0.71 0.70 ? 0.64 0.73 0.68 0.62 ?
GDep ?D 0.67 0.76 0.71 0.70 0.69 0.65 0.74 0.69 0.63 0.62
?R,% ?0.30 +1.46 +0.56 +0.14 1.44? +0.93 +1.92 +1.46 +1.49 1.61?
Table 6: Sentiment classification results obtained using gender-dependent and gender-independent joint and disjoint
features for Logistic Regression (LR) and SVM models.
for polarity classification with an F-measure of 82%.
We test our results for statistical significance us-
ing McNemar?s Chi-squared test (p-value < 0.01) as
suggested by Dietterich (1998). Only three classi-
fiers, J48, AdaBoostM1 (AB) and Random Forest
(RF) do not always show significant improvements
for GDep features over GInd features. However,
for the majority of classifiers, GDep models outper-
formGIndmodels for both tasks, demonstrating the
robustness of GDep features for sentiment analysis.
In Table 6 we report results for subjectivity and
polarity classification using the best performing
classifiers (as shown in Figure 4c) :
- Logistic Regression (LR) (Genkin et al, 2007)
for GInd ? J and GDep ? J models.
- SVM model with radial-based kernel for
GInd ? D and GDep ? D models. We use
LibSVM implementation (EL-Manzalawy and
Honavar, 2005).
Each ?R(%) row shows the relative percent im-
provements in terms of precision P , recall R, F-
measure F and accuracy A for GDep compared to
GInd models. Our results show that differences in
subjective language across genders can be exploited
to improve sentiment analysis, not only for English
but for multiple languages. For Spanish and Russian
results are lower for subjectivity classification, we
suspect, because lexical features V are already in-
flected for gender and set-count features are down-
weighted by the classifier. For polarity classifica-
tion, on the other hand, gender-dependent features
provide consistent, significant improvements (1.5-
2.5%) across all languages.
As a reality check, Table 6 also reports accuracies
(in Arand columns) for experiments that use random
permutations of male and female subjective terms,
which are then encoded as gender-dependent set-
count features as before. We found that all gender-
dependent models, GDep ? J and GDep ?D, out-
performed their random equivalents for both subjec-
tivity and polarity classification (as reflected by rel-
ative accuracy decrease ? forArand compared toA).
These results further confirm the existence of gen-
der bias in subjective language for any of our three
languages and its importance for sentiment analysis.
Finally, we check whether encoding gender as
a binary feature would be sufficient to improve
sentiment classification. For that we encode fea-
1823
English Spanish Russian
P R P R P R
(a) 0.73 0.93 0.68 0.63 0.66 0.74
(b) 0.72 0.94 0.69 0.64 0.66 0.74
(c) 0.78 0.83 0.71 0.63 0.66 0.72
(d) 0.69 0.93 0.71 0.62 0.65 0.76
(e) 0.80 0.83 0.72 0.65 0.68 0.73
Table 7: Precision and recall results for polarity classifi-
cation: encoding gender as a binary feature vs. gender-
dependent features GDep ? J .
tures such as: (a) unigram term frequencies V , (b)
term frequencies and gender binary V +GBin, (c)
gender-independent GInd, (d) gender-independent
and gender binary GBin + GInd, and (e) gender-
dependent GDep ? J . We train logistic-regression
model for polarity classification and report precision
and recall results in Table 7. We observe that includ-
ing gender as a binary feature does not yield signif-
icant improvements compared to GDep ? J for all
three languages.
6 Conclusions
We presented a qualitative and empirical study that
analyses substantial and interesting differences in
subjective language between male and female users
in Twitter, including hashtag and emoticon usage
across cultures. We showed that incorporating au-
thor gender as a model component can significantly
improve subjectivity and polarity classification for
English (2.5% and 5%), Spanish (1.5% and 1%) and
Russian (1.5% and 1%). In future work we plan to
develop new models for joint modeling of personal-
ized sentiment, user demographics e.g., age and user
preferences e.g., political favorites in social media.
Acknowledgments
The authors thank the anonymous reviewers for
helpful comments and suggestions.
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and sentiment
analysis of modern standard Arabic. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, pages 587?591.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media (LSM?11), pages 30?38.
Shlomo Argamon, Moshe Koppel, James W. Pen-
nebaker, and Jonathan Schler. 2007. Min-
ing the blogosphere: Age, gender and the va-
rieties of self-expression. First Monday, 12(9).
http://www.firstmonday.org/ojs/index.php/fm/article/
view/2003/1878.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 2200?2204.
David Bamman, Jacob Eisenstein, and Tyler Schnoebe-
len. 2012. Gender in Twitter: styles, stances, and so-
cial networks. Computing Research Repository.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2008.
A bootstrapping method for building subjectivity lex-
icons for languages with scarce resources. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), pages
2764?2767.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING?10),
pages 36?44.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media (LSM?12), pages 65?74.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 1833?1836.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in Twitter streaming data. In Proceed-
ings of the 13th International Conference on Discovery
Science (DS?10), pages 1?15.
Bonka Boneva, Robert Kraut, and David Frohlich. 2001.
Using email for personal relationships: The differ-
ence gender makes. American Behavioral Scientist,
45(3):530?549.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on Twit-
ter. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1301?1309.
1824
Pedro Henrique Calais Guerra, Adriano Veloso, Wagner
Meira Jr, and Virg?lio Almeida. 2011. From bias to
opinion: a transfer-learning approach to real-time sen-
timent analysis. In Proceedings of the17th Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD?11), pages 150?158.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun
Wang, and Amit P. Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity
from Twitter. In Proceedings of the Sixth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM?12), pages 50?57.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for prod-
uct meta-domain. In Proceedings of the 25rd In-
ternational Conference on Computational Linguistics
(COLING?12), pages 593?610.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The Fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation (LREC?04), pages 69?71.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?10), pages 7?13.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING?10), pages 241?249.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895?1923.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for ge-
ographic lexical variation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?10), pages 1277?1287.
Yasser EL-Manzalawy and Vasant Honavar, 2005.
WLSVM: Integrating LibSVM into Weka Environment.
http://www.cs.iastate.edu/ yasser/wlsvm.
Teng-Kai Fan and Chia-Hui Chang. 2009. Sentiment-
oriented contextual advertising. Advances in Informa-
tion Retrieval, 5478:202?215.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710?718.
David Gefen and Catherine M. Ridings. 2005. If you
spoke as she does, sir, instead of the way you do: a so-
ciolinguistics perspective of gender differences in vir-
tual communities. SIGMIS Database, 36(2):78?92.
Alexander Genkin, David D. Lewis, and David Madigan.
2007. Large-scale Bayesian logistic regression for text
categorization. Technometrics, 49:291?304.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42?47.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: telephone speech corpus
for research and development. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP?92), pages 517?520.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers age and gen-
der. In Proceedings of AAAI Conference on Weblogs
and Social Media, pages 214?217.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploratory Newsletter, 11(1):10?18.
Janet Holmes and Miriam Meyerhoff. 2004. The Hand-
book of Language and Gender. Blackwell Publishing.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 151?160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building
lexicon for sentiment analysis from massive collection
of HTML documents. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?07), pages 1075?1083.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM?11), pages 538?541.
Guangxia Li, Steven Hoi, Kuiyu Chang, and Ramesh
Jain. 2010. Micro-blogging sentiment detection
by collaborative online learning. In Proceedings of
IEEE 10th International Conference on Data Mining
(ICDM?10), pages 893?898.
Hao Li, Yu Chen, Heng Ji, Smaranda Muresan, and
Dequan Zheng. 2012. Combining social cognitive
theories with linguistic features for multi-genre senti-
ment analysis. In Proceedings of the 26th Pacific Asia
1825
Conference on Language,Information and Computa-
tion (PACLIC?12), pages 27?136.
Ronald Macaulay. 2006. Pure grammaticalization: The
development of a teenage intensifier. Language Varia-
tion and Change, 18(03):267?283.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2012.
Multilingual subjectivity and sentiment analysis. In
Proceedings of the Association for Computational Lin-
guistics (ACL?12).
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Saif Mohammad and Tony Yang. 2011. Tracking senti-
ment in mail: How genders differ on emotional axes.
In Proceedings of the 2nd Workshop on Computa-
tional Approaches to Subjectivity and Sentiment Anal-
ysis (WASSA?11), pages 70?79.
Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?10), pages 207?217.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. A mixture model of de-
mographic lexical variation. In Proceedings of NIPS
Workshop on Machine Learning in Computational So-
cial Science, pages 1?7.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309?319.
Jahna Otterbacher. 2010. Inferring gender of movie re-
viewers: exploiting writing style, content and meta-
data. In Proceedings of the 19th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?10), pages 369?378.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
pages 1320?1326.
Veronica Perez-Rosas, Carmen Banea, and Rada Mihal-
cea. 2012. Learning sentiment lexicons in Spanish.
In Proceedings of the 8th International Conference
on Language Resources and Evaluation (LREC?12),
pages 3077?3081.
Rosalind W. Picard. 1997. Affective computing. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?09),
pages 675?682.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Contents
(SMUC?10), pages 37?44.
Philip Resnik. 2013. Getting real(-time) with live
polling. http://vimeo.com/68210812.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP?03), pages 105?
112.
Harold Schiffman. 2002. Bibliography of gender and
language. http://ccat.sas.upenn.edu/ haroldfs/popcult/
bibliogs/gender/genbib.htm.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?08), pages 254?263.
Will Styler. 2011. The EnronSent Corpus.
Technical report, University of Colorado
at Boulder Institute of Cognitive Science.
http://verbs.colorado.edu/enronsent/.
Jane Sunderland, Ren-Feng Duann, and Paul
Bake. 2002. Gender and genre bibliography.
www.ling.lancs.ac.uk/pubs/clsl/clsl122.pdf.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-based meth-
ods for sentiment analysis. Computational Linguis-
tics, 37(2):267?307.
Sali A. Tagliamonte. 2006. Analysing Sociolinguistic
Variation. Cambridge University Press, 1st. Edition.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings of
the 17th International Conference on Knowledge Dis-
covery and Data Mining (KDD?11), pages 1397?1405.
Mike Thelwall, David Wilkinson, and Sukhvinder Uppal.
2010. Data mining emotion in social network com-
munication: Gender differences in MySpace. Journal
of the American Society for Information Science and
Technology, 61(1):190?199.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability
of web-derived polarity lexicons. In Proceedings of
1826
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?10), pages 777?785.
Svitlana Volkova, Theresa Wilson, and David Yarowsky.
2013. Exploring sentiment in social media: Boot-
strapping subjectivity clues from multilingual Twitter
streams. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL?13), pages 505?510.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and
Ming Zhang. 2011. Topic sentiment analysis in Twit-
ter: A graph-based hashtag sentiment classification ap-
proach. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Manage-
ment (CIKM?11), pages 1031?1040.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence (AAAI?00, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP?05), pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP?11), pages
172?182.
1827
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 306?314,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
CLex: A Lexicon for Exploring Color, Concept and Emotion
Associations in Language
Svitlana Volkova
Johns Hopkins University
3400 North Charles
Baltimore, MD 21218, USA
svitlana@jhu.edu
William B. Dolan
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
billdol@microsoft.com
Theresa Wilson
HLTCOE
810 Wyman Park Drive
Baltimore, MD 21211, USA
taw@jhu.edu
Abstract
Existing concept-color-emotion lexicons
limit themselves to small sets of basic emo-
tions and colors, which cannot capture the
rich pallet of color terms that humans use
in communication. In this paper we begin
to address this problem by building a novel,
color-emotion-concept association lexicon
via crowdsourcing. This lexicon, which we
call CLEX, has over 2,300 color terms, over
3,000 affect terms and almost 2,000 con-
cepts. We investigate the relation between
color and concept, and color and emotion,
reinforcing results from previous studies, as
well as discovering new associations. We
also investigate cross-cultural differences in
color-emotion associations between US and
India-based annotators.
1 Introduction
People typically use color terms to describe the
visual characteristics of objects, and certain col-
ors often have strong associations with particu-
lar objects, e.g., blue - sky, white - snow. How-
ever, people also take advantage of color terms to
strengthen their messages and convey emotions in
natural interactions (Jacobson and Bender, 1996;
Hardin and Maffi, 1997). Colors are both indica-
tive of and have an effect on our feelings and emo-
tions. Some colors are associated with positive
emotions, e.g., joy, trust and admiration and some
with negative emotions, e.g., aggressiveness, fear,
boredom and sadness (Ortony et al 1988).
Given the importance of color and visual de-
scriptions in conveying emotion, obtaining a
deeper understanding of the associations between
colors, concepts and emotions may be helpful for
many tasks in language understanding and gener-
ation. A detailed set of color-concept-emotion as-
sociations (e.g., brown - darkness - boredom; red -
blood - anger) could be quite useful for sentiment
analysis, for example, in helping to understand
what emotion a newspaper article, a fairy tale, or
a tweet is trying to evoke (Alm et al 2005; Mo-
hammad, 2011b; Kouloumpis et al 2011). Color-
concept-emotion associations may also be useful
for textual entailment, and for machine translation
as a source of paraphrasing.
Color-concept-emotion associations also have
the potential to enhance human-computer inter-
actions in many real- and virtual-world domains,
e.g., online shopping, and avatar construction in
gaming environments. Such knowledge may al-
low for clearer and hopefully more natural de-
scriptions by users, for example searching for
a sky-blue shirt rather than blue or light blue
shirt. Our long term goal is to use color-emotion-
concept associations to enrich dialog systems
with information that will help them generate
more appropriate responses to users? different
emotional states.
This work introduces a new lexicon of color-
concept-emotion associations, created through
crowdsourcing. We call this lexicon CLEX1. It
is comparable in size to only two known lexi-
cons: WORDNET-AFFECT (Strapparava and Val-
itutti, 2004) and EMOLEX (Mohammad and Tur-
ney, 2010). In contrast to the development of
these lexicons, we do not restrict our annotators
to a particular set of emotions. This allows us to
1Available for download at:
http://research.microsoft.com/en-us/
downloads/
Questions about the data and the access process may be
sent to svitlana@jhu.edu
306
collect more linguistically rich color-concept an-
notations associated with mood, cognitive state,
behavior and attitude. We also do not have any
restrictions on color naming, which helps us to
discover a rich lexicon of color terms and collo-
cations that represent various hues, darkness, sat-
uration and other natural language collocations.
We also perform a comprehensive analysis of
the data by investigating several questions includ-
ing: What affect terms are evoked by a certain
color, e.g., positive vs. negative? What con-
cepts are frequently associated with a particular
color? What is the distribution of part-of-speech
tags over concepts and affect terms in the data col-
lected without any preselected set of affect terms
and concepts? What affect terms are strongly as-
sociated with a certain concept or a category of
concepts and is there any correlation with a se-
mantic orientation of a concept?
Finally, we share our experience collecting the
data using crowdsourcing, describe advantages
and disadvantages as well as the strategies we
used to ensure high quality annotations.
2 Related Work
Interestingly, some color-concept associations
vary by culture and are influenced by the tra-
ditions and beliefs of a society. As shown in
(Sable and Akcay, 2010) green represents danger
in Malaysia, envy in Belgium, love and happiness
in Japan; red is associated with luck in China and
Denmark, but with bad luck in Nigeria and Ger-
many and reflects ambition and desire in India.
Some expressions involving colors share the
same meaning across many languages. For in-
stance, white heat or red heat (the state of high
physical and mental tension), blue-blood (an aris-
tocrat, royalty), white-collar or blue collar (of-
fice clerks). However, there are some expres-
sions where color associations differ across lan-
guages, e.g., British or Italian black eye becomes
blue in Germany, purple in Spain and black-butter
in France; your French, Italian and English neigh-
bors are green with envy while Germans are yel-
low with envy (Bortoli and Maroto, 2001).
There has been little academic work on con-
structing color-concept and color-emotion lexi-
cons. The work most closely related to ours
collects concept-color (Mohammad, 2011c) and
concept-emotion (EMOLEX) associations, both
relying on crowdsourcing. His project involved
collecting color and emotion annotations for
10,170 word-sense pairs from Macquarie The-
saurus2. They analyzed their annotations, looking
for associations with the 11 basic color terms from
Berlin and Key (1988). The set of emotion labels
used in their annotations was restricted to the set
of 8 basic emotions proposed by Plutchik (1980).
Their annotators were restricted to the US, and
produced 4.45 annotations per word-sense pair on
average.
There is also a commercial project from Cym-
bolism3 to collect concept-color associations. It
has 561,261 annotations for a restricted set of 256
concepts, mainly nouns, adjectives and adverbs.
Other work on collecting emotional aspect
of concepts includes WordNet-Affect (WNA)
(Strapparava and Valitutti, 2004), the General En-
quirer (GI) (Stone et al 1966), Affective Forms
of English Words (Bradley and Lang, 1999) and
Elliott?s Affective Reasoner (Elliott, 1992).
The WNA lexicon is a set of affect terms from
WordNet (Miller, 1995). It contains emotions,
cognitive states, personality traits, behavior, at-
titude and feelings, e.g., joy, doubt, competitive,
cry, indifference, pain. Total of 289 affect terms
were manually extracted, but later the lexicon was
extended using WordNet semantic relationships.
WNA covers 1903 affect terms - 539 nouns, 517
adjectives, 238 verbs and 15 adverbs.
The General Enquirer covers 11,788 concepts
labeled with 182 category labels including cer-
tain affect categories (e.g., pleasure, arousal, feel-
ing, pain) in addition to positive/negative seman-
tic orientation for concepts4.
Affective Forms of English Words is a work
which describes a manually collected set of nor-
mative emotional ratings for 1K English words
that are rated in terms of emotional arousal (rang-
ing from calm to excited), affective valence (rang-
ing from pleasant to unpleasant) and dominance
(ranging from in control to dominated).
Elliott?s Affective Reasoner is a collection of
programs that is able to reason about human emo-
tions. The system covers a set of 26 emotion cat-
egories from Ortony et al1988).
Kaya (2004) and Strapparava and Ozbal (2010)
both have worked on inferring emotions associ-
ated with colors using semantic similarity. Their
2http://www.macquarieonline.com.au
3http://www.cymbolism.com/
4http://www.wjh.harvard.edu/?inquirer/
307
research found that Americans perceive red as ex-
citement, yellow as cheer, purple as dignity and
associate blue with comfort and security. Other
research includes that geared toward discovering
culture-specific color-concept associations (Gage,
1993) and color preference, for example, in chil-
dren vs. adults (Ou et al 2011).
3 Data Collection
In order to collect color-concept and color-
emotion associations, we use Amazon Mechani-
cal Turk5. It is a fast and relatively inexpensive
way to get a large amount of data from many cul-
tures all over the world.
3.1 MTurk and Data Quality
Amazon Mechanical Turk is a crowdsourcing
platform that has been extensively used for ob-
taining low-cost human annotations for various
linguistic tasks over the last few years (Callison-
Burch, 2009). The quality of the data obtained
from non-expert annotators, also referred to as
workers or turkers, was investigated by Snow et
al (2008). Their empirical results show that the
quality of non-expert annotations is comparable
to the quality of expert annotations on a variety of
natural language tasks, but the cost of the annota-
tion is much lower.
There are various quality control strategies that
can be used to ensure annotation quality. For in-
stance, one can restrict a ?crowd? by creating a
pilot task that allows only workers who passed
the task to proceed with annotations (Chen and
Dolan, 2011). In addition, new quality control
mechanisms have been recently introduced e.g.,
Masters. They are groups of workers who are
trusted for their consistent high quality annota-
tions, but to employ them costs more.
Our task required direct natural language in-
put from workers and did not include any mul-
tiple choice questions (which tend to attract more
cheating). Thus, we limited our quality control ef-
forts to (1) checking for empty input fields and (2)
blocking copy/paste functionality on a form. We
did not ask workers to complete any qualification
tasks because it is impossible to have gold stan-
dard answers for color-emotion and color-concept
associations. In addition, we limited our crowd to
5http://www.mturk.com
a set of trusted workers who had been consistently
working on similar tasks for us.
3.2 Task Design
Our task was designed to collect a linguistically
rich set of color terms, emotions, and concepts
that were associated with a large set of colors,
specifically the 152 RGB values corresponding to
facial features of cartoon human avatars. In to-
tal we had 36 colors for hair/eyebrows, 18 for
eyes, 27 for lips, 26 for eye shadows, 27 for fa-
cial mask and 18 for skin. These data is necessary
to achieve our long-term goal which is to model
natural human-computer interactions in a virtual
world domain such as the avatar editor.
We designed two MTurk tasks. For Task 1, we
showed a swatch for one RGB value and asked
50 workers to name the color, describe emotions
this color evokes and define a set of concepts as-
sociated with that color. For Task 2, we showed a
particular facial feature and a swatch in a particu-
lar color, and asked 50 workers to name the color
and describe the concepts and emotions associ-
ated with that color. Figure 1 shows what would
be presented to worker for Task 2.
Q1. How would you name this color?
Q2. What emotion does this color evoke?
Q3. What concepts do you associate with it?
Figure 1: Example of MTurk Task 2. Task 1 is the
same except that only a swatch is given.
The design that we suggested has a minor lim-
itation in that a color swatch may display differ-
ently on different monitors. However, we hope to
overcome this issue by collecting 50 annotations
per RGB value. The example color
e
? emotion
c
?
concept associations produced by different anno-
tators ai are shown below:
? [R=222, G=207, B=186] (a1) light golden
yellow
e
? purity, happiness
c
? butter cookie,
vanilla; (a2) gold
e
? cheerful, happy
c
? sun,
corn; (a3) golden
e
? sexy
c
? beach, jewelery.
? [R=218, G=97, B=212] (a4) pinkish pur-
ple
e
? peace, tranquility, stressless
c
? justin
308
bieber?s headphones, someday perfume; (a5)
pink
e
? happiness
c
? rose, bougainvillea.
In addition, we collected data about workers?
gender, age, native language, number of years of
experience with English, and color preferences.
This data is useful for investigating variance in an-
notations for color-emotion-concept associations
among workers from different cultural and lin-
guistic backgrounds.
4 Data Analysis
We collected 15,200 annotations evenly divided
between the two tasks over 12 days. In total, 915
workers (41% male, 51% female and 8% who did
not specify), mainly from India and United States,
completed our tasks as shown in Table 1. 18%
workers produced 20 or more annotations. They
spent 78 seconds on average per annotation with
an average salary rate $2.3 per hour ($0.05 per
completed task).
Country Annotations
India 7844
United States 5824
Canada 187
United Kingdom 172
Colombia 100
Table 1: Demographic information about annota-
tors: top 5 countries represented in our dataset.
In total, we collected 2,315 unique color terms,
3,397 unique affect terms, and 1,957 unique con-
cepts for the given 152 RGB values. In the
sections below we discuss our findings on color
naming, color-emotion and color-concept associ-
ations. We also give a comparison of annotated
affect terms and concepts from CLEX and other
existing lexicons.
4.1 Color Terms
Berlin and Kay (1988) state that as languages
evolve they acquire new color terms in a strict
chronological order. When a language has only
two colors they are white (light, warm) and black
(dark, cold). English is considered to have 11 ba-
sic colors: white, black, red, green, yellow, blue,
brown, pink, purple, orange and gray, which is
known as the B&K order.
In addition, colors can be distinguished along at
most three independent dimensions of hue (olive,
orange), darkness (dark, light, medium), satura-
tion (grayish, vivid), and brightness (deep, pale)
(Mojsilovic, 2002). Interestingly, we observe
these dimensions in CLEX by looking for B&K
color terms and their frequent collocations. We
present the top 10 color collocations for the B&K
colors in Table 2. As can be seen, color terms
truly are distinguished by darkness, saturation and
brightness terms e.g., light, dark, greenish, deep.
In addition, we find that color terms are also as-
sociated with color-specific collocations, e.g., sky
blue, chocolate brown, pea green, salmon pink,
carrot orange. These collocations were produced
by annotators to describe the color of particular
RGB values. We investigate these color-concept
associations in more details in Section 4.3.
In total, the CLEX has 2,315 unique color
Color Co-occurrences
?
white off, antique, half, dark, black, bone,
milky, pale, pure, silver
0.62
black light, blackish brown, brownish,
brown, jet, dark, green, off, ash,
blackish grey
0.43
red dark, light, dish brown, brick, or-
ange, brown, indian, dish, crimson,
bright
0.59
green dark, light, olive, yellow, lime, for-
est, sea, dark olive, pea, dirty
0.54
yellow light, dark, green, pale, golden,
brown, mustard, orange, deep,
bright
0.63
blue light, sky, dark, royal, navy, baby,
grey, purple, cornflower, violet
0.55
brown dark, light, chocolate, saddle, red-
dish, coffee, pale, deep, red,
medium
0.67
pink dark, light, hot, pale, salmon, baby,
deep, rose, coral, bright
0.55
purple light, dark, deep, blue, bright,
medium, pink, pinkish, bluish,
pretty
0.69
orange light, burnt, red, dark, yellow,
brown, brownish, pale, bright, car-
rot
0.68
gray dark, light, blue, brown, charcoal,
leaden, greenish, grayish blue, pale,
grayish brown
0.62
Table 2: Top 10 color term collocations for the
11 B&K colors; co-occurrences are sorted by fre-
quency from left to right in a decreasing order;
?10
1 p(? | color) is a total estimated probability
of the top 10 co-occurrences.
309
Agreement Color Term
% of overall Exact match 0.492
agreement Substring match 0.461
Free-marginal Exact match 0.458
Kappa Substring match 0.424
Table 3: Inter-annotator agreement on assigning
names to RGB values: 100 annotators, 152 RGB
values and 16 color categories including 11 B&K
colors, 4 additional colors and none of the above.
names for the set of 152 RGB values. The
inter-annotator agreement rate on color naming is
shown in Table 3. We report free-marginal Kappa
(Randolph, 2005) because we did not force an-
notators to assign certain number of RGB values
to a certain number of color terms. Additionally,
we report inter-annotator agreement for an exact
string match e.g., purple, green and a substring
match e.g., pale yellow = yellow = golden yellow.
4.2 Color-Emotion Associations
In total, the CLEX lexicon has 3,397 unique af-
fect terms representing feelings (calm, pleasure),
emotions (joy, love, anxiety), attitudes (indiffer-
ence, caution), and mood (anger, amusement).
The affect terms in CLEX include the 8 basic emo-
tions from (Plutchik, 1980): joy, sadness, anger,
fear, disgust, surprise, trust and anticipation6
CLEX is a very rich lexicon because we did not
restrict our annotators to any specific set of affect
terms. A wide range of parts-of-speech are rep-
resented, as shown in the first column in Table 4.
For instance, the term love is represented by other
semantically related terms such as: lovely, loved,
loveliness, loveless, love-able and the term joy is
represented as enjoy, enjoyable, enjoyment, joy-
ful, joyfulness, overjoyed.
POS Affect Terms, % Concepts, %
Nouns 79 52
Adjectives 12 29
Adverbs 3 5
Verbs 6 12
Table 4: Main syntactic categories for affect terms
and concepts in CLEX.
The manually constructed portion of
WORDNET-AFFECT includes 101 positive
and 188 negative affect terms (Strapparava and
6The set of 8 Plutchik?s emotions is a superset of emotions
from (Ekman, 1992).
Valitutti, 2004). Of this set, 41% appeared at
least once in CLEX. We also looked specifically
at the set of terms labeled as emotions in the
WORDNET-AFFECT hierarchy. Of these, 12 are
positive emotions and 10 are negative emotions.
We found that 9 out of 12 positive emotion
terms (except self-pride, levity and fearlessness)
and 9 out of 10 negative emotion terms (except in-
gratitude) also appear in CLEX as shown in Table
5. Thus, we can conclude that annotators do not
associate any colors with self-pride, levity, fear-
lessness and ingratitude. In addition, some emo-
tions were associated more frequently with colors
than others. For instance, positive emotions like
calmness, joy, love are more frequent in CLEX
than expectation and ingratitude; negative emo-
tions like sadness, fear are more frequent than
shame, humility and daze.
Positive Freq. Negative Freq.
calmness 1045 sadness 356
joy 527 fear 250
love 482 anxiety 55
hope 147 despair 19
affection 86 compassion 10
enthusiasm 33 dislike 8
liking 5 shame 5
expectation 3 humility 3
gratitude 3 daze 1
Table 5: WORDNET-AFFECT positive and neg-
ative emotion terms from CLEX. Emotions are
sorted by frequency in decreasing order from the
total 27,802 annotations.
Next, we analyze the color-emotion associ-
ations in CLEX in more detail and compare
them with the only other publicly-available color-
emotion lexicon, EMOLEX. Recall that EMOLEX
(Mohammad, 2011a) has 11 B&K colors associ-
ated with 8 basic positive and negative emotions
from (Plutchik, 1980). Affect terms in CLEX are
not labeled as conveying positive or negative emo-
tions. Instead, we use the overlapping 289 affect
terms between WORDNET-AFFECT and CLEX
and propagate labels from WORDNET-AFFECT to
the corresponding affect terms in CLEX. As a re-
sult we discover positive and negative affect term
associations with the 11 B&K colors. Table 6
shows the percentage of positive and negative af-
fect term associations with colors for both CLEX
and EMOLEX.
310
Positive Negative
CLEX EL CLEX EL
white 2.5 20.1 0.3 2.9
black 0.6 3.9 9.3 28.3
red 1.7 8.0 8.2 21.6
green 3.3 15.5 2.7 4.7
yellow 3.0 10.8 0.7 6.9
blue 5.9 12.0 1.6 4.1
brown 6.5 4.8 7.6 9.4
pink 5.6 7.8 1.1 1.2
purple 3.1 5.7 1.8 2.5
orange 1.6 5.4 1.7 3.8
gray 1.0 5.7 3.6 14.1
Table 6: The percentage of affect terms associated
with B&K colors in CLEX and EMOLEX (similar
color-emotion associations are shown in bold).
The percentage of color-emotion associations
in CLEX and EMOLEX differs because the set of
affect terms in CLEX consists of 289 positive and
negative affect terms compared to 8 affect terms
in EMOLEX. Nevertheless, we observe the same
pattern as (Mohammad, 2011a) for negative emo-
tions. They are associated with black, red and
gray colors, except yellow becomes a color of
positive emotions in CLEX. Moreover, we found
the associations with the color brown to be am-
biguous as it was associated with both positive
and negative emotions. In addition, we did not ob-
serve strong associations between white and pos-
itive emotions. This may be because white is the
color of grief in India. The rest of the positive
emotions follow the EMOLEX pattern and are as-
sociated with green, pink, blue and purple colors.
Next, we perform a detailed comparison be-
tween CLEX and EMOLEX color-emotion asso-
ciations for the 11 B&K colors and the 8 basic
emotions from (Plutchik, 1980) in Table 7. Recall
that annotations in EMOLEX are done by workers
from the USA only. Thus, we report two num-
bers for CLEX - annotations from workers from
the USA (CA) and all annotations (C). We take
EMOLEX results from (Mohammad, 2011c). We
observe a strong correlation between CLEX and
EMOLEX affect lexicons for some color-emotion
associations. For instance, anger has a strong as-
sociation with red and brown, anticipation with
green, fear with black, joy with pink, sadness
with black, brown and gray, surprise with yel-
low and orange, and finally, trust is associated
with blue and brown. Nonetheless, we also found
a disagreement in color-emotion associations be-
tween CLEX and EMOLEX. For instance antic-
ipation is associated with orange in CLEX com-
pared to white, red or yellow in EMOLEX. We also
found quite a few inconsistent associations with
the disgust emotion. This inconsistency may be
explained by several reasons: (a) EMOLEX asso-
ciates emotions with colors through concepts, but
CLEX has color-emotion associations obtained
directly from annotators; (b) CLEX has 3,397
affect terms compared to 8 basic emotions in
EMOLEX. Therefore, it may be introducing some
ambiguous color-emotion associations.
Finally, we investigate cross-cultural differ-
ences in color-emotion associations between the
two most representative groups of our annotators:
US-based and India-based. We consider the 8
Plutchik?s emotions and allow associations with
all possible color terms (rather than only 11 B&K
colors). We show top 5 colors associated with
emotions for two groups of annotators in Figure 2.
For example, we found that US-based annotators
associate pink with joy, dark brown with trust vs.
India-based annotators who associate yellow with
joy and blue with trust.
4.3 Color-Concept Associations
In total, workers annotated the 152 RGB values
with 37,693 concepts which is on average 2.47
concepts compared to 1.82 affect term per anno-
tation. CLEX contains 1,957 unique concepts in-
cluding 1,667 nouns, 23 verbs, 28 adjectives, and
12 adverbs. We investigate an overlap of con-
cepts by part-of-speech tag between CLEX and
other lexicons including EMOLEX (EL), Affec-
tive Norms of English Words (AN), General In-
quirer (GI). The results are shown in Table 8.
Finally, we generate concept clusters associ-
ated with yellow, white and brown colors in Fig-
ure 3. From the clusters, we observe the most
frequent k concepts associated with these colors
have a correlation with either positive or negative
emotion. For example, white is frequently associ-
ated with snow, milk, cloud and all of these con-
cepts evolve positive emotions. This observation
helps resolve the ambiguity in color-emotion as-
sociations we found in Table 7.
5 Conclusions
We have described a large-scale crowdsourcing
effort aimed at constructing a rich color-emotion-
311
white black red green yellow blue brown pink purple orange grey
anger
C - 3.6 43.4 0.3 0.3 0.3 3.3 0.6 0.3 1.5 2.1
CA - 3.8 40.6 0.8 - - 4.5 - 0.8 2.3 0.8
EA 2.1 30.7 32.4 5.0 5.0 2.4 6.6 0.5 2.3 2.5 9.9
sadness
C 0.3 24.0 0.3 0.6 0.3 4.2 11.4 0.3 2.2 0.3 10.3
CA - 22.2 - 0.6 - 5.3 9.4 - 4.1 - 12.3
EA 3.0 36.0 18.6 3.4 5.4 5.8 7.1 0.5 1.4 2.1 16.1
fear
C 0.8 43.0 8.9 2.0 1.2 0.4 6.1 0.4 0.8 0.4 2.0
CA - 29.5 10.5 3.2 1.1 - 3.2 - 1.1 1.1 4.2
EA 4.5 31.8 25.0 3.5 6.9 3.0 6.1 1.3 2.3 3.3 11.8
disgust
C - 2.3 1.1 11.2 1.1 1.1 24.7 1.1 3.4 1.1 -
CA - - - 14.8 1.8 - 33.3 - 1.8 - -
EA 2.0 33.7 24.9 4.8 5.5 1.9 9.7 1.1 1.8 3.5 10.5
joy
C 1.0 0.2 0.2 3.4 5.7 4.2 4.2 9.1 4.4 4.0 0.6
CA 0.9 - 0.3 3.3 4.5 4.8 2.7 10.6 4.2 3.9 0.6
EA 21.8 2.2 7.4 14.1 13.4 11.3 3.1 11.1 6.3 5.8 2.8
trust
C - - 1.2 3.5 1.2 17.4 8.1 1.2 1.2 5.8 1.2
CA - - 3.0 6.1 3.0 3.0 9.1 - - 3.0 3.0
EA 22.0 6.3 8.4 14.2 8.3 14.4 5.9 5.5 4.9 3.8 5.8
surprise
C - - - 3.3 6.7 6.7 3.3 3.3 6.7 13.3 3.3
CA - - - - 5.6 5.6 - 5.6 11.1 11.1 -
EA 11.0 13.4 21.0 8.3 13.5 5.2 3.4 5.2 4.1 5.6 8.8
anticipation
C - - - 5.3 5.3 - 5.3 5.3 - 15.8 5.3
CA - - - - - - - 10.0 - 10.0 10.0
EA 16.2 7.5 11.5 16.2 10.7 9.5 5.7 5.9 3.1 4.9 8.4
Table 7: The percentage of the 8 basic emotions associated with 11 B&K colors in CLEX vs. EMOLEX,
e.g., sadness is associated with black by 36% of annotators in EMOLEX(EA), 22.1% in CLEX(CA) by
US-based annotators only and 24% in CLEX(C) by all annotators; we report zero associations by ?-?.
(a) Joy - US: 331, I: 154 (b) Trust - US: 33, I: 47 (c) Surprise - US: 18, I: 12 (d) Anticipation - US: 10, I: 9
(e) Anger - US: 133, I: 160 (f) Sadness - US: 171, I: 142 (g) Fear - US: 95, I: 105 (h) Disgust - US: 54, I: 16
Figure 2: Apparent cross-cultural differences in color-emotion associations between US- and India-
based annotators. 10.6% of US workers associated joy with pink, while 7.1% India-based workers
associated joy with yellow (based on 331 joy associations from the US and from 154 India).
312
(a) Yellow (b) Brown (c) White
Figure 3: Concept clusters of color-concept associations for ambiguous colors: yellow, white, brown.
concept association lexicon, CLEX. This lexicon
links concepts, color terms and emotions to spe-
cific RGB values. This lexicon may help to dis-
ambiguate objects when modeling conversational
interactions in many domains. We have examined
the association between color terms and positive
or negative emotions.
Our work also investigated cross-cultural dif-
ferences in color-emotion associations between
India- and US-based annotators. We identified
frequent color-concept associations, which sug-
gests that concepts associated with a particular
color may express the same sentiment as the color.
Our future work includes applying statistical
inference for discovering a hidden structure of
concept-emotion associations. Moreover, auto-
matically identifying the strength of association
between a particular concept and emotions is an-
other task which is more difficult than just iden-
tifying the polarity of the word. We are also in-
terested in using a similar approach to investigate
CLEX?AN CLEX?EL CLEX?GI
Noun 287 Noun 574 Noun 708
Verb 4 Verb 13 Verb 17
Adj 28 Adj 53 Adj 66
Adv 1 Adv 2 Adv 3
320 642 794
AN\CLEX EL\CLEX GI\CLEX
712 7,445 11,101
CLEX\AN CLEX\EL CLEX\GI
1,637 1,315 1,163
Table 8: An overlap of concepts by part-of-
speech tag between CLEX and existing lexicons.
CLEX?GI stands for the intersection of sets,
CLEX\GI denotes the difference of sets.
the way that colors are associated with concepts
and emotions in languages other than English.
Acknowledgments
We are grateful to everyone in the NLP group
at Microsoft Research for helpful discussion and
feedback especially Chris Brocket, Piali Choud-
hury, and Hassan Sajjad. We thank Natalia Rud
from Tyumen State University, Center of Linguis-
tic Education for helpful comments and sugges-
tions.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine
learning for text-based emotion prediction. In
Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 579?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Brent Berlin and Paul Kay. 1988. Basic Color Terms:
their Universality and Evolution. Berkley: Univer-
sity of California Press.
M. Bortoli and J. Maroto. 2001. Translating colors in
web site localisation. In In The Proceedings of Eu-
ropean Languages and the Implementation of Com-
munication and Information Technologies (Elicit).
M. Bradley and P. Lang. 1999. Affective forms for
english words (anew): Instruction manual and af-
fective ranking.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s me-
chanical turk. In EMNLP ?09: Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 286?295, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
313
David L. Chen and William B. Dolan. 2011. Building
a persistent workforce on mechanical turk for mul-
tilingual data collection. In Proceedings of The 3rd
Human Computation Workshop (HCOMP 2011),
August.
Paul Ekman. 1992. An argument for basic emotions.
Cognition & Emotion, 6(3):169?200.
Clark Davidson Elliott. 1992. The affective reasoner:
a process model of emotions in a multi-agent sys-
tem. Ph.D. thesis, Evanston, IL, USA. UMI Order
No. GAX92-29901.
J. Gage. 1993. Color and culture: Practice and mean-
ing from antiquity to abstraction, univ. of calif.
C. Hardin and L. Maffi. 1997. Color Categories in
Thought and Language.
N. Jacobson and W. Bender. 1996. Color as a deter-
mined communication. IBM Syst. J., 35:526?538,
September.
N. Kaya. 2004. Relationship between color and emo-
tion: a study of college students. College Student
Journal.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. ICWSM.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?
41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011a. Colourful language: Mea-
suring word-colour associations. In Proceedings
of the 2nd Workshop on Cognitive Modeling and
Computational Linguistics, pages 97?106, Port-
land, Oregon, USA, June. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011b. From once upon a time
to happily ever after: Tracking emotions in novels
and fairy tales. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 105?114, Portland, OR, USA, June. Associa-
tion for Computational Linguistics.
Saif M. Mohammad. 2011c. Even the abstract have
colour: consensus in word-colour associations. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies: short papers - Volume 2,
HLT ?11, pages 368?373, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Aleksandra Mojsilovic. 2002. A method for color
naming and description of color composition in im-
ages. In Proc. IEEE Int. Conf. Image Processing,
pages 789?792.
Andrew Ortony, Gerald L. Clore, and Allan Collins.
1988. The Cognitive Structure of Emotions. Cam-
bridge University Press, July.
Li-Chen Ou, M. Ronnier Luo, Pei-Li Sun, Neng-
Chung Hu, and Hung-Shing Chen. 2011. Age ef-
fects on colour emotion, preference, and harmony.
Color Research and Application.
R. Plutchik, 1980. A general psychoevolutionary the-
ory of emotion, pages 3?33. Academic press, New
York.
Justus J. Randolph. 2005. Author note: Free-marginal
multirater kappa: An alternative to fleiss fixed-
marginal multirater kappa.
P. Sable and O. Akcay. 2010. Color: Cross cultural
marketing perspectives as to what governs our re-
sponse to it. In In The Proceedings of ASSBS, vol-
ume 17.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Carlo Strapparava and Gozde Ozbal. 2010. The color
of emotions in text. COLING, pages 28?32.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In In: Proceed-
ings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC 2004), Lis-
bon, pages 1083?1086.
314
Proceedings of NAACL-HLT 2013, pages 416?425,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning to Relate Literal and Sentimental Descriptions of Visual Properties
Mark Yatskar
Computer Science & Engineering
University of Washington
Seattle, WA
my89@cs.washington.edu
Svitlana Volkova
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Asli Celikyilmaz
Conversational Understanding Sciences
Microsoft
Mountain View, CA
asli@ieee.org
Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
billdol@microsoft.edu
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Language can describe our visual world at
many levels, including not only what is lit-
erally there but also the sentiment that it in-
vokes. In this paper, we study visual language,
both literal and sentimental, that describes the
overall appearance and style of virtual char-
acters. Sentimental properties, including la-
bels such as ?youthful? or ?country western,?
must be inferred from descriptions of the more
literal properties, such as facial features and
clothing selection. We present a new dataset,
collected to describe Xbox avatars, as well as
models for learning the relationships between
these avatars and their literal and sentimen-
tal descriptions. In a series of experiments,
we demonstrate that such learned models can
be used for a range of tasks, including pre-
dicting sentimental words and using them to
rank and build avatars. Together, these re-
sults demonstrate that sentimental language
provides a concise (though noisy) means of
specifying low-level visual properties.
1 Introduction
Language can describe varied aspects of our visual
world, including not only what is literally there but
also the social, cultural, and emotional sentiment it
invokes. Recently, there has been a growing effort
to study literal language that describes directly ob-
servable properties, such as object color, shape, or
This is a light tan young man
with short and trim haircut. He
has straight eyebrows and large
brown eyes. He has a neat and
trim appearance.
State of mind: angry, upset,
determined. Likes: country
western, rodeo. Occupation:
cowboy, wrangler, horse trainer.
Overall: youthful, cowboy.
Figure 1: (A) Literal avatar descriptions and (B) sen-
timental descriptions of four avatar properties, in-
cluding possible occupations and interests.
category (Farhadi et al, 2009; Mitchell et al, 2010;
Matuszek et al, 2012). Here, we add a focus on
sentimental visual language, which compactly de-
scribes more subjective properties such as if a person
looks determined, if a resume looks professional, or
if a restaurant looks romantic. Such models enable
many new applications, such as text editors that au-
tomatically select properties including font, color, or
text alignment to best match high level descriptions
such as ?professional? or ?artistic.?
416
In this paper, we study visual language, both lit-
eral and sentimental, that describes the overall ap-
pearance and style of virtual characters, like those in
Figure 1. We use literal language as feature norms, a
tool used for studying semantic information in cog-
nitive science (Mcrae et al, 2005). Literal words,
such ?black? or ?hat,? are annotated for objects to in-
dicate how people perceive visual properties. Such
feature norms provide our gold-standard visual de-
tectors, and allow us to focus on learning to model
sentimental language, such as ?youthful? or ?goth.?
We introduce a new corpus of descriptions of
Xbox avatars created by actual gamers. Each avatar
is specified by 19 attributes, including clothing and
body type, allowing for more than 1020 possibil-
ities. Using Amazon Mechanical Turk,1 we col-
lected literal and sentimental descriptions of com-
plete avatars and many of their component parts,
such as the cowboy hat in Figure 1(B). In all, there
are over 100K descriptions. To demonstrate poten-
tial for learning, we also report an A/B test which
shows that native speakers can use sentimental de-
scriptions to distinguish the labeled avatars from
random distractors. This new data will enable study
of the relationships between the co-occurring literal
and sentimental text in a rich visual setting.2
We describe models for three tasks: (i) classify-
ing when words match avatars, (ii) ranking avatars
given a description, and (iii) constructing avatars to
match a description. Each model includes literal part
descriptions as feature norms, enabling us to learn
which literal and sentinel word pairs best predict
complete avatars.
Experiments demonstrate the potential for jointly
modeling literal and sentimental visual descriptions
on our new dataset. The approach outperforms sev-
eral baselines and learns varied relationships be-
tween the sentimental and literal descriptions. For
example, in one experiment ?nerdy student? is pre-
dictive of an avatar with features indicating its shirt
is ?plaid? and glasses are ?large? and faces that are
not ?bearded.? We also show that individual sen-
timental words can be predicted but that multiple
avatars can match a single sentimental description.
Finally, we use our model to build complete avatars
1www.mturk.com
2Data available at http://homes.cs.washington.
edu/?my89/avatar.
and show that we can accurately predict the senti-
mental terms annotators ascribe to them.
2 Related Work
To the best of our knowledge, our focus on learn-
ing to understand visual sentiment descriptions is
novel. However, visual sentiment has been stud-
ied from other perspectives. Jrgensen (1998) pro-
vides examples which show that visual descriptions
communicate social status and story information in
addition to literal object and properties. Tousch et
al. (2012) draw the distinction between ?of-ness?
(objective and concrete) and ?about-ness? (subjec-
tive and abstract) in image retrieval, and observe
that many image queries are abstract (for example,
images about freedom). Finally, in descriptions of
people undergoing emotional distress, Fussell and
Moss (1998) show that literal descriptions co-occur
frequently with sentimental ones.
There has been significant work on more lit-
eral aspects of grounded language understand-
ing, both visual and non-visual. The Words-
Eye project (Coyne and Sproat, 2001) generates
3D scenes from literal paragraph-length descrip-
tions. Generating literal textual descriptions of vi-
sual scenes has also been studied, including both
captions (Kulkarni et al, 2011; Yang et al, 2011;
Feng and Lapata, 2010) and descriptions (Farhadi
et al, 2010). Furthermore, Chen and Dolan (2011)
collected literal descriptions of videos with the
goal of learning paraphrases while Zitnick and
Parikh (2013) describe a corpus of descriptions for
clip art that supports the discovery of semantic ele-
ments of visual scenes.
There has also been significant recent work on au-
tomatically recovering visual attributes, both abso-
lute (Farhadi et al, 2009) and relative (Kovashka et
al., 2012), a challenge that we avoid having to solve
with our use of feature norms (Mcrae et al, 2005).
Grounded language understanding has also re-
ceived significant attention, where the goal is to
learn to understand situated non-visual language
use. For example, there has been work on learning
to execute instructions (Branavan et al, 2009; Chen
and Mooney, 2011; Artzi and Zettlemoyer, 2013),
provide sports commentary (Chen et al, 2010), un-
derstand high level strategy guides to improve game
417
Figure 2: The number of assets per category and ex-
ample images from the hair, shirt and hat categories.
play (Branavan et al, 2011; Eisenstein et al, 2009),
and understand referring expression (Matuszek et
al., 2012).
Finally, our work is similar in spirit to sentiment
analysis (Pang et al, 2002), emotion detection from
images and speech (Zeng et al, 2009), and metaphor
understanding (Shutova, 2010a; Shutova, 2010b).
However, we focus on more general visual context.
3 Data Collection
We gathered a large number of natural language de-
scriptions from Mechanical Turk (MTurk). They in-
clude: (1) literal descriptions of specific facial fea-
tures, clothing or accessories and (2) high level sub-
jective descriptions of human-generated avatars.3
Literal Descriptions We showed annotators a sin-
gle image of clothing, a facial feature or an acces-
sory and asked them to produce short descriptions.
Figure 2 shows the distribution over object types.
We restricted descriptions to be between 3 and 15
words. In all, we collected 33.2K descriptions and
had on average 7 words per descriptions. The ex-
ample annotations with highlighted overlapping pat-
terns are in Table 1.
Sentimental Descriptions We also collected 1913
gamer-created avatars from the web. The avatars
were filtered to contain only items from the set of
665 for which we gathered literal descriptions. The
gender distribution is 95% male.
3(2) also has phrases describing emotional reactions. We
also collected (3) multilingual literal, (4) relative literal and (5)
comprehensive full-body descriptions. We do not use this data,
but it will be included in the public release.
LITERAL DESCRIPTIONS
full-sleeved executive blue shirt
blue , long-sleeved button-up shirt
mens blue button dress shirt with dark blue stripes
multi-blue striped long-sleeve button-up dress
shirt with cuffs and breast pocket
Table 1: Literal descriptions of shirt in Figure 2.
To gather high level sentimental descriptions, an-
notators were presented with an image of an avatar
and asked to list phrases in response to the follow
different aspects:
- State of mind of the avatar.
- Things the avatar might care about.
- What the avatar might do for a living.
- Overall appearance of the avatar.
6144 unique vocabulary items occurred in these
descriptions, but only 1179 occurred more than 10
times. Figure 1 (B) shows an avatar and its corre-
sponding sentimental descriptions.
Quality Control All annotations in our dataset are
produced by non-expert annotators. We relied on
manual spot checks to limit poor annotations. Over
time, we developed a trusted crowd of annotators
who produced only high quality annotations during
the earliest stage of data collection.
4 Feasibility
Our hypothesis is that sentimental language does not
uniquely identify an avatar, but instead summarizes
or otherwise describes its overall look. In general,
there is a trade off between concise and precise de-
scriptions. For example, given a single word you
might be able to generally describe the overall look
of an avatar, but a long, detailed, literal description
would be required to completely specify their ap-
pearance.
To demonstrate that the sentimental descriptions
we collected are precise enough to be predictive
of appearance, we conducted an experiment that
prompts people to judge when avatars match de-
scriptions. We created an A/B test where we show
English speakers two avatars and one sentimental
description. They were asked to select which avatar
is better matched by the description and how dif-
ficult they felt, on a scale from 1 to 4, it was to
judge. For 100 randomly selected descriptions, we
418
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2  
1
 1.5
 2
 2.5
 3
 3.5
data 
diffic
ulty l
ess t
han X
Kapp
a vs 
Cum
ulativ
e Dif
ficult
y
game
r is m
ajorit
y lab
el kapp
a
portio
n of d
ata
Figure 3: Judged task difficulty versus agreement,
gamer avatar preference, and percentage of data cov-
ered. The difficulty axis is cumulative.
asked 5 raters to compare the gamer avatars to ran-
domly generated ones (where each asset is selected
independently according to a uniform distribution).
Figure 3 shows a plot of Kappa and the percent of
the time a majority of the raters selected the gamer
avatar. The easiest 20% of the data pairs had the
strongest agreement, with kappa=.92, and two thirds
of the data has kappa = .70. While agreement falls
off to .52 for the full data set, the gamer avatar re-
mains the majority judgment 81% of the time.
The fact that random avatars are sometimes pre-
ferred indicates that it can be difficult to judge sen-
timental descriptions. Consider the avatars in Fig-
ure 4. Neither conforms to a clear sentimental de-
scription based on the questions we asked. The
right one is described with conflicting words and
the words describing the left one are very general
(like ?dumb?). This corresponds to our intuition that
while many avatars can be succinctly summarized
with our questions, some would be more easily de-
scribed using literal language.
5 Tasks and Evaluation
We formulate three tasks to study the feasibility of
learning the relationship between sentimental and
literal descriptions. In this section, we first define
the space of possible avatars, followed by the tasks.
Avatars Figure 5 summarizes the notation we will
develop to describe the data. An avatar is defined by
a 19 dimensional vector ~a where each position is an
State of mind:
playful, happy;
Likes: sex
Occupation: hobo
Overall: dumb
State of mind: content, humble, satisfied,
peaceful, relaxed, calm. Likes: fashion,
friends, money, cars, music, education.
Occupation: teacher, singer, actor,
performer, dancer, computer engineer.
Overall: nerdy, cool, smart, comfy,
easygoing, reserved
Figure 4: Avatars rated as difficult.
index into a list of possible items~i. Each dimension
represents a position on the avatar, for example, hat
or nose. Each possible item is called an asset and
is associated with a set of positions it can fill. Most
assets take up exactly one position, while there are
a few cases where assets take multiple positions.4
An avatar ~a is valid if all of its mandatory positions
are filled, and no two assets conflict on a position.
Mandatory positions include hair, eyes, ears, eye-
brows, nose, mouth, chin, shirt, pants, and shoes.
All other positions are optional. We refer to this set
of valid ~a as A. Practically speaking, if an avatar is
not valid, it cannot be reliably rendered graphically.
Each item i is associated with the literal descrip-
tions ~di ? D where D is the set of literal descrip-
tions. Furthermore, every avatar~a is associated a list
of sentimental query words ~q, describing subjective
aspects of an avatar.5
Sentimental Word Prediction We first study in-
dividual words. The word prediction task is to de-
cide whether a given avatar can be described with a
4For example, long sleeve shirts cover up watches, so they
take up both shirt and wristwear positions. Costumes tend to
span many more positions, for example there a suit that takes
up shirt, pants, wristwear and shoes positions.
5We do not distinguish which prompt (e.g., ?state of mind?
or ?occupation?) a word in ~q came from, although the vocabu-
laries are relatively disjoint.
419
Figure 5: Avatars, queries, items, literal descriptions.
particular sentimental word q?. We evaluate perfor-
mance with F-score.
Avatar Ranking We also consider an avatar re-
trieval task, where the goal is to rank the set of
avatars in our data, ?j=1...n ~aj , according to which
one best matches a sentimental description, ~qi. As
an automated evaluation, we report the average per-
centile position assigned to the true ~ai for each ex-
ample. However, in general, many different avatars
can match each ~qi, an interesting phenomena we will
further study with human evaluation.
Avatar Generation Finally, we consider the prob-
lem of generating novel, previously unseen avatars,
by selecting a set of items that best embody some
sentimental description. As with ranking, we aim to
construct the avatar ~ai that matches each sentimen-
tal description ~qi. We evaluate by considering the
item overlap between ~ai and the output avatar ~a?,
discounting for empty positions:6
f =
?| ~a?|
j=1 I( ~a
?
j = ~aij)
max(numparts( ~a?), numparts(~ai))
, (1)
where numparts returns the number of non-empty
avatar positions. The score is a conservative measure
because some items are significantly more visually
salient than others. For instance, shirts and pants oc-
cupy a large portion of the physical realization of the
avatar, while rings are small and virtually unnotice-
able. We additionally perform a human evaluation
in Section 8 to better understand these challenges.
6Optional items are infrequently used. Therefore not pre-
dicting them at all offers a strong baseline. Yet doing this
demonstrates nothing about an algorithm?s ability to predict
items which contribute to the sentimental qualities of an avatar.
6 Methods
We present two different models: one that considers
words in isolation and another that jointly models
the query words. This section defines the models
and how we learn them.
6.1 Independent Sentimental Word Model
The independent word model (S-Independent) as-
sumes that each word independently describes the
avatar. We construct a separate linear model for each
word in the vocabulary.
To train these model, we transform the data to
form a binary classification problem for each word,
where the positive data includes all avatars the word
was seen with, (q, ~ai, 1) for all i and q ? ~qi, and the
rest are negative, (q, ~ai, 0) for all i and q /? ~qi.
We use the following features:
? an indicator feature for the cross product of a
sentiment query word q, a literal description
word w ? D, and the avatar position index j
(for example, q = ?angry? with w = ?pointy?
and j = eyebrows):
I(q ? ~qi, w ? ~daij , j)
? a bias feature for keeping a position empty:
I(q ? ~qi, aij = empty, j)
These features will allow the model to capture
correlations between our feature norms which pro-
vide descriptions of visual attributes, like black, and
sentimental words, like gothic.
420
S-Independent is used for both word prediction
and ranking. For prediction, we train a linear model
using averaged binary perceptron. For ranking, we
try to rank all positive instances above negative in-
stances. We use an averaged structured perceptron
to train the ranker (Collins, 2002). To rank with re-
spect to an entire query ~qi, we sum the scores of each
word q ? ~qi.
6.2 Joint Sentimental Model
The second approach (S-Joint) jointly models the
query words to learn the relationships between lit-
eral and sentimental words with score s:
s(~a|~q,D) =
|~a|?
i=1
|~q|?
j=1
?T f(~ai, ~qj , ~dai)
Where every word in the query has a separate factor
and every position is treated independently subject
to the constraint that ~a is valid. The feature function
f uses the same features as the word independent
model above.
This model is used for ranking and generation.
For ranking, we try to rank the avatar ai for query
qi above all other avatars in the candidate set. For
generation, we try to score ai above all other valid
avatars given the query qi. In both cases, we train
with averaged structured perceptron (Collins, 2002)
on the original data, containing query, avatar pairs
(~qi, ~ai).
7 Experimental Setup
Random Baseline For the ranking and avatar gen-
eration tasks, we report random baselines. For rank-
ing, we randomly order the avatars. In the genera-
tion case, we select an item randomly for every posi-
tion. This baseline does not generate optional assets
because they are rare in the real data.
Sentimental-Literal Overlap (SL-Overlap) We
also report a baseline that measures the overlap be-
tween words in the sentiment query ~qi and words in
the literal asset descriptions D. In generation, for
each position in the avatar, ~ai, SL-Overlap selects
the item whose literal description has the most words
in common with ~qi. If no item had overlap with the
query, we backoff to a random choice. In the case of
ranking, it orders avatars by the sum over every po-
sition of the number of words in common between
Word F-Score Precision Recall N
happi 0.84 0.89 0.78 149
student 0.78 0.82 0.74 129
friend 0.76 0.84 0.70 153
music 0.74 0.89 0.63 148
confid 0.74 0.82 0.76 157
sport 0.69 0.62 0.76 76
casual 0.63 0.6 0.67 84
youth 0.6 0.57 0.64 88
waitress 0.59 0.42 1 5
smart 0.57 0.54 0.6 88
fashion 0.54 0.54 0.54 70
monei 0.54 0.52 0.56 76
cool 0.54 0.52 0.56 84
relax 0.53 0.52 0.56 90
game 0.51 0.44 0.62 61
musician 0.51 0.44 0.61 66
parti 0.51 0.43 0.62 58
content 0.5 0.47 0.53 75
friendli 0.49 0.42 0.6 56
smooth 0.49 0.4 0.63 57
Table 2: Top 20 words (stemmed) for classification.
N is the number of occurances in the test set.
the literal description and the query, ~qi. This base-
line tests the degree to which literal and sentimental
descriptions overlap lexically.
Feature Generation For all models that use lexi-
cal features, we limited the number of words. 6144
unique vocabulary items occur in the query set, and
3524 in the literal description set. There are over
400 million entries in the full set of features that in-
clude the cross product of these sets with all possible
avatar positions, as described in Section 6. Since this
would present a challenge for learning, we prune in
two ways. We stem all words with a Porter stemmer.
We also filter out all features which do not occur at
least 10 times in our training set. The final model
has approximately 700k features.
8 Results
We present results for the tasks described in Sec-
tion 5 with the appropriate models from Section 6.
8.1 Word Prediction Results
The goal of our first experiment is to study when
individual sentiment words can be accurately pre-
dicted. We computed sentimental word classifica-
tion accuracy for 1179 word classes with 10 or more
421
Algorithm Percentile Rank
S-joint 77.3
S-independant 73.5
SL-overlap 60.4
Random 48.8
Table 3: Automatic evaluation of ranking. The aver-
age percentile that a test avatar was ranked given its
sentimental description.
mentions. Table 2 shows the top 20 words ordered
by F-score.7 Many common words can be predicted
with relatively high accuracy. Words with strong
individual cues like happy (a smiling mouth), and
confidence (wide eyes) and nerdi (particular glasses)
can be predicted well.
The average F-score among all words was .085.
33.2% of words have an F-score of zero. These zeros
include words like: unusual, bland, sarcastic, trust,
prepared, limber, healthy and poetry. Some of these
words indicate broad classes of avatars (e.g., unusual
avatars) and others indicate subtle modifications to
looks that without other words are not specific (e.g.,
a prepared surfer vs. a prepared business man). Fur-
thermore, evaluation was done assuming that when
a word is not mentioned, it is should be predicted as
negative. This fails to account for the fact that peo-
ple do not mention everything that?s true, but instead
make choices about what to mention based on the
most relevant qualities. Despite these difficulties,
the classification performance shows that we can ac-
curately capture usage patterns for many words.
8.2 Ranking Results
Ranking allows us to test the hypothesis that multi-
ple avatars are valid for a high level description. Fur-
thermore, we consider the differences between S-
Joint and S-Independent, showing that jointly mod-
elings all words improves ranking performance.
Automatic Evaluation The results are shown in
Table 3. Both S-Independent and S-Joint outperform
the SL-overlap baseline. SL-Overlap?s poor perfor-
mance can be attributed to low direct overlap be-
tween sentimental words and literal words. S-Joint
also outperforms the S-Independent.
7Accuracy numbers are inappropriate in this case because
the number of negative instances, in most cases, is far larger
than the number of positive ones.
Inspection of the parameters shows that S-Joint
does better than S-Independent in modeling words
that only relate to a subset of body positions. For
example, in one case we found that for the word
?puzzled? nearly 50% of the weights were on fea-
tures that related to eyebrows and eyes. This type
of specialization was far more pronounced for S-
Joint. The joint nature of the learning allows the fea-
tures for individual words to specialize for specific
positions. In contrast, S-Independent must indepen-
dently predict all parts for every word.
Human Evaluation We report human relevancy
judgments for the top-5 returned results from S-
Joint. On average, 56.2% were marked to be rele-
vant. This shows that S-Joint is performing better
than automatic numbers would indicate, confirming
our intuition that there is a one-to-many relationship
between a sentimental description and avatars. Sen-
timental descriptions, while having significant sig-
nal, are not exact. These results also indicate that
relying on automatic measures of accuracy that as-
sume a single reference avatar underestimates per-
formance. Figure 6 shows the top ranked results
returned by S-Joint for a sentimental description
where the model performs well.
8.3 Generation Results
Finally we evaluate three models for avatar genera-
tion: Random, SL-Overlap and S-Joint using auto-
matic measures and human evaluation.
Automatic Evaluation Table 4 presents results
for automatic evaluation. The Random baseline per-
forms badly, on average assigning items correctly to
less than 1 position in the generated avatar. The SL-
Overlap baseline improves, but still performs quite
poorly. The S-Joint model performs significantly
better, correctly guessing 2-3 items for each output
avatar. However, as we will see in the manual eval-
uation, many of the non-matching parts it produces
are still a good fit for the query.
Human Evaluation As before, there are many
reasonable avatars that could match as well as the
reference avatars. Therefore, we also evaluated gen-
eration with A/B tests, much like in Section 4. An-
notators were asked to judge which of two avatars
better matched a sentimental description. They
422
pensive,confrontational; music,socializing; musician,bar tending,club owner; smart,cool.
Figure 6: A sentimental description paired with the highest ranked avatars found by S-Joint.
Model Overlap
Random 0.041
SL-Overlap 0.049
S-Joint 0.126
Table 4: Automatic generation evaluation results.
The item overlap metric is defined in Section 5.
Kappa Majority Random Sys.
SL-Overlap 0.20 0.25 0.34 0.32
S-Joint 0.52 0.90 0.07 0.81
Gamer 0.52 0.81 0.08 0.77
Table 5: Human evaluation of automatically gener-
ated avatars. Majority represents the percentage of
time the system output is preferred by a majority of
raters. Random and System (Sys.) indicate the per-
centage of time each was preferred.
could rate System A or System B as better, or re-
port that they were equal or that neither matched
the description. We consider two comparisons: SL-
Overlap vs. Random and S-Joint vs Random. Five
annotators performed each condition, rating 100 ex-
amples with randomly ordered avatars.
We report the results for human evaluation includ-
ing kappa, majority judgments, and a distribution
over judgments in Table 5. The SL-Overlap baseline
is indistinguishable from a random avatar. This con-
trasts with the ranking case, where this simple base-
line showed improvement, indicating that generation
is a much harder problem. Furthermore, agreement
is low; people felt the need to make a choice but
were not consistent.
We also see in Table 5 that people prefer the S-
Joint model outputs to random avatars as often as
they prefer gamer to random. While this does not
necessarily imply that S-Joint creates gamer-quality
avatars, it indicates substantial progress by learning
a mapping between literal and sentimental words.
Qualitative Results Table 6 presents the highest
and lowest weighted features for different sentimen-
tal query words. Figure 7 shows four descriptions
that were assigned high quality avatars.
In general, many of the weaker avatars had as-
pects of the descriptions but lacked such distinctive
overall looks. This was especially true when the
descriptions contained seemingly contradictory in-
formation. For example, one avatar was described
as being both nerdy and popular. We generated a
look that had aspects of both of these descriptions,
including a head that contained both conservative el-
ements (like glasses) and less conservative elements
(like crazy hair and earrings). However, the combi-
nation would not be described as nerdy or popular,
because of difficult to predict global interactions be-
tween the co-occurring words and items. This is an
important area for future work.
9 Conclusions
We explored how visual language, both literal and
sentimental, maps to the overall physical appearance
and style of virtual characters. While this paper fo-
cused on avatar design, our approach has implica-
tions for a broad class of natural language-driven
423
Ambition; business,
fashion, success;
salesman; smooth,
professional.
Capable, confident, firm; heavy metal,
extreme sports, motorcycles; engineer,
mechanic, machinist; aggressive,
strong, protective.
Stressed, bored,
discontent; emo music;
works at a record store;
goth, dark, drab.
Happy, content, confident,
home, career, family,
secretary,student,
classy,clean,casual
Figure 7: Avatars automatically generated with the S-Joint model.
Sentiment positive features negative features
happi mouth:thick, mouth:smilei, mouth:make, mouth:open mouth:tight, mouth:emotionless, mouth:brownish, mouth:attract
gothic shoes:brown, shirt:black, pants:hot, shirt:band shirt:half, shirt:tight, pants:sexi, hair:brownish
retro eyebrows:men, eyebrows:large, hair:round, pants:light eyebrows:beauti, pants:side; eyebrows:trim, pants:cut
beach pants:yello, pants:half, nose:narrow, pants:white shirt:brown, shirt:side; shoes:long, pants:jean
Table 6: Most positive and negative features for a word stem. A feature is [position]:[literal word].
dialog scenarios. In many situations, a user may
be perfectly able to formulate a high-level descrip-
tion of their intent (?Make my resume look cleaner?
?Buy me clothes for a summer wedding,? or ?Play
something more danceable?) while having little or
no understanding of the complex parameter space
that the underlying software must manipulate in or-
der to achieve this result.
We demonstrated that these high-level sentimen-
tal specifications can have a strong relationship to
literal aspects of a problem space and showed that
sentimental language is a concise, yet noisy, way
of specifying high level characteristics. Sentimen-
tal language is an unexplored avenue for improving
natural language systems that operate in situated set-
tings. It has the potential to bridge the gap between
lay and expert understandings of a problem domain.
Acknowledgments
This work is partially supported by the DARPA
CSSG (N11AP20020) and the NSF (IIS-1115966).
The authors would like to thank Chris Brockett,
Noelle Sophy, Rico Malvar for helping with collect-
ing and processing the data. We would also like
to thank Tom Kwiatkowski and Nicholas FitzGer-
ald and the anonymous reviewers for their helpful
comments.
References
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping in-
structions to actions. Transactions of the Association
for Computational Linguistics, 1(1):49?62.
SRK Branavan, H. Chen, L.S. Zettlemoyer, and R. Barzi-
lay. 2009. Reinforcement learning for mapping in-
structions to actions. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 82?90.
SRK Branavan, David Silver, and Regina Barzilay. 2011.
Learning to win by reading manuals in a monte-carlo
framework. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 268?
277.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 190?200.
D.L. Chen and R.J. Mooney. 2011. Learning to interpret
natural language navigation instructions from observa-
424
tions. In Proceedings of the 25th AAAI Conference on
Artificial Intelligence (AAAI-2011), pages 859?865.
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a multilingual sportscaster: Using per-
ceptual context to learn language. Journal of Artificial
Intelligence Research, 37:397?435.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing, pages 1?8.
B. Coyne and R. Sproat. 2001. Wordseye: an automatic
text-to-scene conversion system. In Proceedings of the
28th annual conference on Computer graphics and in-
teractive techniques, pages 487?496.
J. Eisenstein, J. Clarke, D. Goldwasser, and D. Roth.
2009. Reading to learn: Constructing features from
semantic abstracts. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 958?967.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their attributes.
In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a story:
generating sentences from images. In Proceedings of
the 11th European conference on Computer Vision,
ECCV?10, pages 15?29.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 831?839.
Susan R Fussell and Mallie M Moss. 1998. Figura-
tive language in emotional communication. Social and
cognitive approaches to interpersonal communication,
page 113.
Corinne Jrgensen. 1998. Attributes of images in describ-
ing tasks. Information Processing & Management,
34(23):161 ? 174.
Adriana Kovashka, Devi Parikh, and Kristen Grauman.
2012. Whittlesearch: Image search with relative at-
tribute feedback. In Computer Vision and Pattern
Recognition (CVPR), pages 2973?2980.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A.C.
Berg, and T.L. Berg. 2011. Baby talk: Understanding
and generating simple image descriptions. In Com-
puter Vision and Pattern Recognition (CVPR), pages
1601?1608.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A Joint
Model of Language and Perception for Grounded At-
tribute Learning. In Proc. of the 2012 International
Conference on Machine Learning.
Ken Mcrae, George S. Cree, Mark S. Seidenberg, and
Chris Mcnorgan. 2005. Semantic feature production
norms for a large set of living and nonliving things.
Behavior Research Methods, 37(4):547?559.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual domain.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference, INLG ?10, pages 95?
104.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 79?86.
Ekaterina Shutova. 2010a. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?10, pages
688?697.
Anne-Marie Tousch, Stphane Herbin, and Jean-Yves Au-
dibert. 2012. Semantic hierarchies for image annota-
tion: A survey. Pattern Recognition, 45(1):333 ? 345.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods in
Natural Language Processing.
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009.
A survey of affect recognition methods: Audio, vi-
sual, and spontaneous expressions. Pattern Analy-
sis and Machine Intelligence, IEEE Transactions on,
31(1):39?58.
C Lawrence Zitnick and Devi Parikh. 2013. Bringing
semantics into focus using visual abstraction. In Com-
puter Vision and Pattern Recognition (To Appear).
425
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1669?1679,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Lightly Supervised Learning of Procedural Dialog Systems
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Pallavi Choudhury, Chris Quirk, Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
pallavic,chrisq,
billdol@microsoft.com
Luke Zettlemoyer
Computer Science and Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Procedural dialog systems can help users
achieve a wide range of goals. However,
such systems are challenging to build,
currently requiring manual engineering of
substantial domain-specific task knowl-
edge and dialog management strategies. In
this paper, we demonstrate that it is pos-
sible to learn procedural dialog systems
given only light supervision, of the type
that can be provided by non-experts. We
consider domains where the required task
knowledge exists in textual form (e.g., in-
structional web pages) and where system
builders have access to statements of user
intent (e.g., search query logs or dialog
interactions). To learn from such tex-
tual resources, we describe a novel ap-
proach that first automatically extracts task
knowledge from instructions, then learns a
dialog manager over this task knowledge
to provide assistance. Evaluation in a Mi-
crosoft Office domain shows that the indi-
vidual components are highly accurate and
can be integrated into a dialog system that
provides effective help to users.
1 Introduction
Procedural dialog systems aim to assist users
with a wide range of goals. For example, they
can guide visitors through a museum (Traum et
al., 2012; Aggarwal et al, 2012), teach students
physics (Steinhauser et al, 2011; Dzikovska et
al., 2011), or enable interaction with a health care
U: ?I want to add page numbers and a title?
S: ?Top or Bottom of the page??
U: ?Top?
S: ?Please select page design from the tem-
plates? (*System shows drop down menu*)
U: *User selects from menu*
S: ?Enter header or footer content?
U: ?C.V.?
S: ?Task completed.?
Figure 1: An example dialog interaction between
a system (S) and user (U) that can be automatically
achieved by learning from instructional web page
and query click logs.
system (Morbini et al, 2012; Rizzo et al, 2011).
However, such systems are challenging to build,
currently requiring expensive, expert engineering
of significant domain-specific task knowledge and
dialog management strategies.
In this paper, we present a new approach for
learning procedural dialog systems from task-
oriented textual resources in combination with
light, non-expert supervision. Specifically, we as-
sume access to task knowledge in textual form
(e.g., instructional web pages) and examples of
user intent statements (e.g., search query logs or
dialog interactions). Such instructional resources
are available in many domains, ranging from
recipes that describe how to cook meals to soft-
ware help web pages that describe how to achieve
goals by interacting with a user interface.1
1ehow.com,wikianswers.com
1669
There are two key challenges: we must (1)
learn to convert the textual knowledge into a us-
able form and (2) learn a dialog manager that pro-
vides robust assistance given such knowledge. For
example, Figure 1 shows the type of task assis-
tance that we are targeting in the Microsoft Office
setting, where the system should learn from web
pages and search query logs. Our central contribu-
tion is to show that such systems can be built with-
out the help of knowledge engineers or domain ex-
perts. We present new approaches for both of our
core problems. First, we introduce a method for
learning to map instructions to tree representations
of the procedures they describe. Nodes in the tree
represent points of interaction with the questions
the system can ask the user, while edges represent
user responses. Next, we present an approach that
uses example user intent statements to simulate di-
alog interactions, and learns how to best map user
utterances to nodes in these induced dialog trees.
When combined, these approaches produce a com-
plete dialog system that can engage in conversa-
tions by automatically moving between the nodes
of a large collection of induced dialog trees.
Experiments in the Windows Office help do-
main demonstrate that it is possible to build an
effective end-to-end dialog system. We evaluate
the dialog tree construction and dialog manage-
ment components in isolation, demonstrating high
accuracy (in the 80-90% range). We also conduct
a small-scale user study which demonstrates that
users can interact productively with the system,
successfully completing over 80% of their tasks.
Even when the system does fail, it often does so in
a graceful way, for example by asking redundant
questions but still reaching the goal within a few
additional turns.
2 Overview of Approach
Our task-oriented dialog system understands user
utterances by mapping them to nodes in dialog
trees generated from instructional text. Figure 2
shows an example of a set of instructions and the
corresponding dialog tree. This section describes
the problems that we must solve to enable such in-
teractions, and outlines our approach for each.
Knowledge Acquisition We extract task knowl-
edge from instructional text (e.g., Figure 2, left)
that describes (1) actions to be performed, such
as clicking a button, and (2) places where input
is needed from the user, for example to enter the
contents of the footer or header they are trying to
create. We aim to convert this text into a form that
will enable a dialog system to automatically assist
with the described task. To this end, we construct
dialog trees (e.g., Figure 2, right) with nodes to
represent entire documents (labeled as topics t),
nodes to represent user goals or intents (g), and
system action nodes (a) that enable execution of
specific commands. Finally, each node has an as-
sociated system action as, which can prompt user
input (e.g., with the question ?Top or bottom of
the page??) and one or more user actions au that
represent possible responses. All nodes connect
to form a tree structure that follows the workflow
described in the document. Section 3 presents a
scalable approach for inducing dialog trees.
Dialog Management To understand user intent
and provide task assistance, we need a dialog man-
agement approach that specifies what the system
should do and say. We adopt a simple approach
that at all times maintains an index into a node in
a dialog tree. Each system utterance is then simply
the action as for that node. However, the key chal-
lenge comes in interpreting user utterances. After
each user statement, we must automatically up-
date our node index. At any point, the user can
state a general goal (e.g., ?I want to add page num-
bers?), refine their goal (e.g., ?in a footer?), or both
(e.g.,?I want to add page numbers in the footer?).
Users can also change their goals in the process of
completing the tasks.
We develop a simple classification approach
that is robust to these different types of user behav-
ior. Specifically, we learn classifiers that, given the
dialog interaction history, predict how to pick the
next tree node from the space of all nodes in the di-
alog trees that define the task knowledge. We iso-
late two specific cases, classifying initial user ut-
terances (Section 4) and classifying all subsequent
utterances (Section 5). This approach allows us to
isolate the difference in language for the two cases,
and bias the second case to prefer tree nodes near
the current one. The resulting approach allows for
significant flexibility in traversing the dialog trees.
Data and Evaluation We collected a large set of
such naturally-occurring web search queries that
resulted in a user click on a URL in the Microsoft
Office help domain.2 We found that queries longer
that 4-5 words often resembled natural language
utterances that could be used for dialog interac-
2http://office.microsoft.com
1670
Figure 2: An example instructional text paired with a section of the corresponding dialog tree.
tions, for example how do you add borders, how
can I add a footer, how to insert continuous page
numbers, and where is the header and footer.
We also collected instructional texts from the
web pages that describe how to solve 76 of the
most pressing user goals, as indicated by query
click log statistics. On average 1,000 user queries
were associated with each goal. To some extent
clickthroughs can be treated as a proxy for user
frustration; popular search targets probably repre-
sent user pain points.
3 Building Dialog Trees from
Instructions
Our first problem is to convert sets of instructions
for user goals to dialog trees, as shown in Figure
2. These goals are broadly grouped into topics
(instructional pages). In addition, we manually
associate each node in a dialog tree with a train-
ing set of 10 queries. For the 76 goals (246 in-
structions) in our data, this annotation effort took
a single annotator a total of 41 hours. Scaling this
approach to the entire Office help domain would
require a focused annotation effort. Crucially,
though, this annotation work can be carried out by
non-specialists, and could even be crowdsourced
(Bernstein et al, 2010).
Problem Definition As input, we are given in-
structional text (p1 . . . pn), comprised of topics
(t1 . . . tn) describing:
(1) high-level user intents (e.g., t1 ? ?add and for-
mat page numbers?)
(2) goals (g1, . . . , gk) that represent more spe-
cific user intents (e.g., g1 ? ?add header or
footer content to a preformatted page number
design?, g2 ? ?place the page number in the
side margin of the page?).
Given instructional text p1 . . . pn and queries
q1 . . . qm per topic ti, our goals are as follows:
Figure 3: Relationships between user queries and
OHP with goals, instructions and dialog trees.
- for every instructional page pi extract a topic
ti and a set of goals g1 . . . gk;
- for every goal gj for a topic ti, extract a set of
instructions i1 . . . il;
- from topics, goals and instructions, construct
dialog trees f1 . . . fn (one dialog tree per
topic). Classify instructions to user interac-
tion types thereby identifying system action
nodes a1s . . . als. Transitions between these
nodes are the user actions a1u . . . alu.
Figure 2 (left) presents an example of a topic
extracted from the help page, and a set of goals
and instructions annotated with user action types.
In the next few sections of the paper, we out-
line an overall system component design demon-
strating how queries and topics are mapped to the
dialog trees in Figure 3. The figure shows many-
to-one relations between queries and topics, one-
to-many relations between topics and goals, goals
and instructions, and one-to-one relations between
topics and dialog trees.
User Action Classification We aim to classify
instructional text (i1 . . . il) for every goal gj in the
decision tree into four categories: binary, selec-
tion, input or none.
Given a single instruction i with category au,
we use a log-linear model to represent the distri-
1671
bution over the space of possible user actions. Un-
der this representation, the user action distribution
is defined as:
p(au|i, ?) =
e???(au,i)?
a?u e
???(au,i) , (1)
where ?(au, i) ? Rn is an n-dimensional fea-
ture representation and ~? is a parameter vector we
aim to learn. Features are indicator functions of
properties of the instructions and a particular class.
For smoothing we use a zero mean, unit variance
Gaussian prior (0, 1) that penalizes ~? for drifting
too far from the mean, along with the following
optimization function:
log p(Au, ?|I) = log p(Au|I, ?)? log p(?) =
=
?
au,i?(Au,I)
p(au|i, ?)?
?
i
(? ? ?i)2
2?2i
+ k
(2)
We use L-BFGS (Nocedal and Wright, 2000) as
an optimizer.
Experimental Setup As described in Section 2,
our dataset consists of 76 goals grouped into 30
topics (average 2-3 goals per topic) for a total of
246 instructions (average 3 instructions per goal).
We manually label all instructions with user ac-
tion au categories. The distribution over cate-
gories is binary=14, input=23, selection=80 and
none=129. The data is skewed towards the cat-
egories none and selection. Many instruction do
not require any user input and can be done auto-
matically, e.g., ?On the Insert tab, in the Header
and Footer group, click Page Number?. The ex-
ample instructions with corresponding user action
labels are shown in Figure 2 (left) . Finally, we di-
vide the 246 instructions into 2 sets: 80% training
and 20% test, 199 and 47 instructions respectively.
Results We apply the user action type classifi-
cation model described in the Eq.1 and Eq.2 to
classify instructions from the test set into 4 cate-
gories. In Table 1 we report classification results
for 2 baselines: a majority class and heuristic-
based approach, and 2 models with different fea-
ture types: ngrams and ngrams + stems. For a
heuristic baseline, we use simple lexical clues to
classify instructions (e.g., X or Y for binary, select
Y for selection and type X, insert Y for input). Ta-
ble 1 summarizes the results of mapping instruc-
tional text to user actions.
Features # Features Accuracy
Baseline 1: Majority ? 0.53
Baseline 2: Heuristic ? 0.64
Ngrams 10,556 0.89
Ngrams + Stems 12,196 0.89
Table 1: Instruction classification results.
Building the Dialog Trees Based on the classi-
fied user action types, we identify system actions
a1s . . . als which correspond to 3 types of user ac-
tions a1s . . . als (excluding none type) for every goal
in a topic ti. This involved associating all words
from an instruction il with a system action als. Fi-
nally, for every topic we automatically construct a
dialog tree as shown in Figure 2 (right). The dia-
log tree includes a topic t1 with goals g1 . . . g4, and
actions (user actions au and system actions as).
Definition 1. A dialog tree encodes a user-system
dialog flow about a topic ti represented as a di-
rected unweighted graph fi = (V,E) where top-
ics, goals and actions are nodes of correspond-
ing types {t1 . . . tn}, {g1 . . . gk}, {a1 . . . al} ? V .
There is a hierarchical dependency between topic,
goal and action nodes. User interactions are
represented by edges ti ? {g1 . . . gk}, a1u =
(gj , a1) . . . alu = (ak?1, ak) ? E.
For example, in the dialog tree in Figure 2 there
is a relation t1 ? g4 between the topic t1 ?add
and format page numbers? and the goal g4 ?in-
clude page of page X of Y with the page number?.
Moreover, in the dialog tree, the topic level node
has one index i ? [1..n], where n is the number
of topics. Every goal node includes information
about its parent (topic) node and has double index
i.j, where j ? [1..k]. Finally, action nodes include
information about their parent (goal) and grand-
parent (topic) nodes and have triple index i.j.z,
where z ? [1..l].
4 Understanding Initial Queries
This section presents a model for classifying ini-
tial user queries to nodes in a dialog tree, which
allows for a variety of different types of queries.
They can be under-specified, including informa-
tion about a topic only (e.g., ?add or delete page
numbers?); partially specified, including informa-
tion about a goal (e.g., ?insert page number?); or
over-specified, including information about an ac-
tion ( e.g., ?page numbering at bottom page?.)
1672
Figure 4: Mapping initial user queries to the nodes
on different depth in a dialog tree.
Problem Definition Given an initial query, the
dialog system initializes to a state s0, searches for
the deepest relevant node given a query, and maps
the query to a node on a topic ti, goal gj or action
ak level in the dialog tree fi, as shown in Figure 4.
More formally, as input, we are given automati-
cally constructed dialog trees f1 . . . fn for instruc-
tional text (help pages) annotated with topic, goal
and action nodes and associated with system ac-
tions as shown in Figure 2 (right). From the query
logs, we associate queries with each node type:
topic qt, goal qg and action qa. This is shown in
Figure 2 and 4. We join these dialog trees repre-
senting different topics into a dialog network by
introducing a global root. Within the network,
we aim to find (1) an initial dialog state s0 that
maximizes the probability of state given a query
p(s0|q, ?); and (2) the deepest relevant node v ? V
on topic ti, goal gj or action ak depth in the tree.
Initial Dialog State Model We aim to predict
the best node in a dialog tree ti, gj , al ? V based
on a user query q. A query-to-node mapping is en-
coded as an initial dialog state s0 represented by a
binary vector over all nodes in the dialog network:
s0 = [t1, g1.1, g1.2, g1.2.1 . . . , tn, gn.1, gn.1.1].
We employ a log-linear model and try to maxi-
mize initial dialog state distribution over the space
of all nodes in a dialog network:
p(s0|q, ?) =
e
?
i ?i?i(s0,q)
?
s?0 e
?
i ?i?i(s?0,q)
, (3)
Optimization follows Eq. 2.
We experimented with a variety of features.
Lexical features included query ngrams (up to 3-
grams) associated with every node in a dialog tree
with removed stopwords and stemming query un-
igrams. We also used network structural features:
Accuracy
Features Topic Goal Action
Random 0.10 0.04 0.04
TFIDF 1Best 0.81 0.21 0.45
Lexical (L) 0.92 0.66 0.63
L + 10TFIDF 0.94 0.66 0.64
L + 10TFIDF + PO 0.94 0.65 0.65
L + 10TFIDF + QO 0.95 0.72 0.69
All above + QHistO 0.96 0.73 0.71
Table 2: Initial dialog state classification results
where L stands for lexical features, 10TFIDF - 10
best tf-idf scores, PO - prompt overlap, QO - query
overlap, and QHistO - query history overlap.
tf-idf scores, query ngram overlap with the topic
and goal descriptions, as well as system action
prompts, and query ngram overlap with a history
including queries from parent nodes.
Experimental Setup For each dialog tree,
nodes corresponding to single instructions were
hand-annotated with a small set of user queries,
as described in Section 3. Approximately 60% of
all action nodes have no associated queries3 For
the 76 goals, the resulting dataset consists of 972
node-query pairs, 80% training and 20% test.
Results The initial dialog state classification
model of finding a single node given an initial
query is described in Eq. 3.
We chose two simple baselines: (1) randomly
select a node in a dialog network and (2) use a tf-
idf 1-best model.4 Stemming, stopword removal
and including top 10 tf-idf results as features led
to a 19% increase in accuracy on an action node
level over baseline (2). Adding the following fea-
tures led to an overall 26% improvement: query
overlap with a system prompt (PO), query overlap
with other node queries (QO), and query overlap
with its parent queries (QHistO) .
We present more detailed results for topic, goal
and action nodes in Table 2. For nodes deeper in
the network, the task of mapping a user query to an
action becomes more challenging. Note, however,
that the action node accuracy numbers actually un-
3There are multiple possible reasons for this: the soft-
ware user interface may already make it clear how to accom-
plish this intent, the user may not understand that the software
makes this fine-grained option available to them, or their ex-
perience with search engines may lead them to state their in-
tent in a more coarse-grained way.
4We use cosine similarity to rank all nodes in a dialog
network and select the node with the highest rank.
1673
derstate the utility of the resulting dialog system.
The reason is that even incorrect node assignments
can lead to useful system performance. As long
as a misclassification results being assigned to a
too-high node within the correct dialog tree, the
user will experience a graceful failure: they may
be forced to answer some redundant questions, but
they will still be able to accomplish the task.
5 Understanding Query Refinements
We also developed a classifier model for mapping
followup queries to the nodes in a dialog network,
while maintaining a dialog state that summarizes
the history of the current interaction.
Problem Definition Similar to the problem def-
inition in Section 4, we are given a network of di-
alog trees f1 . . . fn and a query q?, but in addition
we are given the previous dialog state s, which
contains the previous user utterance q and the last
system action as. We aim to find a new dialog
state s? that pairs a node from the dialog tree with
updated history information, thereby undergoing a
dialog state update.
We learn a linear classifier that models
p(s?|q?, q, as, ?), the dialog state update distribu-
tion, where we constrain the new state s? to contain
the new utterance q? we are interpreting. This dis-
tribution models 3 transition types: append, over-
ride and reset.
Definition 2. An append action defines a dialog
state update when transitioning from a node to its
children at any depth in the same dialog tree e.g.,
ti ? gi.j (from a topic to a goal node), gi.j ?
ai.j.z (from a goal to an action node) etc.
Definition 3. An override action defines a dialog
state update when transitioning from a goal to its
sibling node. It could also be from an action node5
to another in its parent sibling node in the same di-
alog tree e.g., gi.j?1 ? gi.j (from one goal to an-
other goal in the same topic tree), ai.j.z ? ai.?j.z
(from an action node to another action node in a
different goal in the same dialog tree) etc.
Definition 4. A reset action defines a dialog state
update when transitioning from a node in a current
dialog tree to any other node at any depth in a
dialog tree other than the current dialog tree e.g.,
ti ? t?i, (from one topic node to another topic
5A transition from ai.j.z must be to a different goal or an
action node in a different goal but in the same dialog tree.
(a) Updates from topic node ti
(b) Updates from goal node gj
(c) Updates from action node al
Figure 5: Information state updates: append, reset
and override updates based on Definition 2, 3 and
4, respectively, from topic, goal and action nodes.
node) ti ? g?i.j (from a topic node to a goal node
in a different topic subtree), etc.
The append action should be selected when the
user?s intent is to clarify a previous query (e.g.,
?insert page numbers? ? ?page numbers in the
footer?). An override action is appropriate when
the user?s intent is to change a goal within the
same topic (e.g., ?insert page number? ?change
page number?). Finally, a reset action should be
used when the user?s intent is to restart the dialog
(e.g., ?insert page x of y? ? ?set default font?).
We present more examples for append, override
and reset dialog state update actions in Table 3.
1674
Previous Utterance, q User Utterance, q? Transition Update Action, a
inserting page numbers qt1 add a background ti ? t?i 2, reset-T, reset
how to number pages qt2 insert numbers on pages in margin ti ? si.j 1.4, append-G, append
page numbers qt3 set a page number in a footer ti ? ai.j.z 1.2.1, append-A, append
page number a document qt4 insert a comment ti ? g?i.j 21.1, reset-G, reset
page number qt5 add a comment ?redo? ti ? a?i.j.z 21.2.1, reset-A, reset
page x of y qg1 add a border gi.j ? t?i 6, reset-T, resetformat page x of x qg2 enter text and page numbers gi.j ? gi.?j 1.1, override-G, overrideenter page x of y qg3 page x of y in footer gi.j ? ai.j.z 1.3.1, append-A, appendinserting page x of y qg4 setting a default font gi.j ? g?i.j 6.1, reset-G, resetshowing page x of x qg5 set default font and style gi.j ? a?i.j.z 6.4.1, reset-A, resetpage numbers bottom qa1 make a degree symbol ai.j.z ? t?i 13, reset-T, reset
numbering at bottom page qa2 insert page numbers ai.j.z ? gi.?j 1.1, override-G, override
insert footer page numbers qa3 page number design ai.j.z?1 ? ai.j.z 1.2.2, append-A, append
headers page number qa4 comments in document ai.j.z ? g?i.j 21.1, reset-G, reset
page number in a footer qa5 changing initials in a comment ai.j.z ? a?i.j.z 21.2.1, reset-A, reset
Table 3: Example q and q? queries for append, override and reset dialog state updates.
Figure 5 illustrates examples of append, over-
ride and reset dialog state updates. All transitions
presented in Figure 5 are aligned with the example
q and q? queries in Table 3.
Dialog State Update Model We use a log-linear
model to maximize a dialog state distribution over
the space of all nodes in a dialog network:
p(s?|q?, q, as?) =
e
?
i ?i?i(s?,q?,as,q)
?
s?? e
?
i ?i?i(s??,q?,as,q)
, (4)
Optimization is done as described in Section 3.
Experimental Setup Ideally, dialog systems
should be evaluated relative to large volumes of
real user interaction data. Our query log data,
however, does not include dialog turns, and so we
turn to simulated user behavior to test our system.
Our approach, inspired by recent work (Schatz-
mann et al, 2006; Scheffler and Young, 2002;
Georgila et al, 2005), involves simulating dialog
turns as follows. To define a state s we sam-
ple a query q from a set of queries per node v
and get a corresponding system action as for this
node; to define a state s?, we sample a new query
q? from another node v? ? V, v 6= v? which
is sampled using a prior probability biased to-
wards append: p(append)=0.7, p(override)=0.2,
p(reset)=0.1. This prior distribution defines a dia-
log strategy where the user primarily continues the
current goal and rarely resets.
We simulate 1100 previous state and new query
pairs for training and 440 pairs for testing. The
features were lexical, including word ngrams,
stems with no stopwords; we also tested network
structure, such as:
- old q and new q? query overlap (QO);
- q? overlap with a system prompt as (PO);
- q? ngram overlap with all queries from the old
state s (SQO);
- q? ngram overlap with all queries from the
new state s? (S?QO);
- q? ngram overlap with all queries from the
new state parents (S?ParQO).
Results Table 4 reports results for dialog state
updates for topic, goal and action nodes. We also
report performance for two types of dialog updates
such as: append (App.) and override (Over.).
We found that the combination of lexical and
query overlap with the previous and new state
queries yielded the best accuracies: 0.95, 0.84 and
0.83 for topic, goal and action node level, respec-
tively. As in Section 4, the accuracy on the topic
level node was highest. Perhaps surprisingly, the
reset action was perfectly predicted (accuracy is
100% for all feature combinations, not included
in figure). The accuracies for append and override
actions are also high (append 95%, override 90%).
Features Topic Goal Action App. Over.
L 0.92 0.76 0.78 0.90 0.89
L+Q 0.93 0.80 0.80 0.92 0.83
L+P 0.93 0.80 0.79 0.91 0.85
L+Q+P 0.94 0.80 0.80 0.93 0.85
L+SQ 0.94 0.82 0.81 0.93 0.85
L+S?Q 0.93 0.80 0.80 0.91 0.90
L+S?+ParQ 0.94 0.80 0.80 0.91 0.86
L+Q+S?Q 0.94 0.81 0.81 0.91 0.88
L+SQ+S?Q 0.95 0.84 0.83 0.94 0.88
Table 4: Dialog state updates classification ac-
curacies where L stands for lexical features, Q -
query overlap, P - prompt overlap, SQ - previous
state query overlap, S?Q - new state query overlap,
S?ParQ - new state parent query overlap.
1675
6 The Complete Dialog System
Following the overall setup described in Section 2,
we integrate the learned models into a complete
dialog system. To evaluate the quality of the in-
teractions with this system, we performed a small
scale user study, as described here.
Experimental Setup We randomly chose 8
goals, across topics, from the 76 used to train our
models. For each goal, six users were shown a Mi-
crosoft Word document before and after the goal
was executed. Users were allowed to interact with
the system in an unconstrained manner providing a
total of 48 interactions. If at any point they thought
the system response was irrelevant to their intent,
they could either rephrase or start over.
User Study We objectively measured task com-
pletion and the quality of individual system utter-
ances. We also asked the users to rate the system
(on a scale of 1-5, with 5 being best) on various
measures, including whether the system asked rel-
evant questions. Table 5 summarizes the results.
In general, users were able to complete the tasks
and were reasonably satisfied.
Discussion Figure 6 shows four example di-
alogs. User utterances ranged from single key-
words to rich, sentence-like goal descriptions. For
the shorter inputs, the system was generally able
to identify a small cohort of high-probability in-
tents. For longer inputs, especially with explicitly
specified parameter settings, the system was usu-
ally able to elicit whatever additional information
was required to complete the task. The majority of
errors fell into two categories when the system:
I. missed key information in the input and
asked redundant questions ? Figure 6 (C);
II. failed to correctly identify the intent, and
asked an irrelevant question ? Figure 6 (D).
Table 5 shows the frequency of each error type.
Objective Measures Observed Rates
User achieved task 83%
System asked irrelevant questions 19%
System asked redundant questions 18%
User Satisfaction Score Std.Dev.
Overall satisfaction 3.42 0.49
Questions were relevant 4.17 0.41
Ambiguity was managed 4.00 0.89
Table 5: User study results.
U: ?I want a page number and title at the bot-
tom of the page?
S: ?Choose a page number design from the
gallery of designs?
U: *User selects design*
S: ?To insert header or footer content before the
page number, type the content, and then press
TAB to position the content?
U: ?Professional Resume?
S: ?Task Completed?
(A)
U: ?Alphabetize bullets?
S: ?Do you want to sort a list alphabetically?
U: ?Sort a list?
S: ?Select the text in a bulleted or numbered
list?
U: *User selection*
S: ?Ascending or Descending??
U: ?Ascending?
S: ?Task Completed?
(B)
U: ?I want the background color to be yellow?
S: ?Choose a color for the document?
U: ?Yellow ?
S: ?Task completed?
(C)
U: ?I want to change the font face and size?
S: ?Choose the default font?
U: ?No, reduce the size of the text?
S: *System Restarts*
(D)
Figure 6: Four example dialogs from the user
study, including cases that (A and B) complete
successfully, (C) have a redundant question, and
(D) fail to recognize the user intent.
7 Related work
To the best of our knowledge, this paper presents
the first effort to induce full procedural dialog sys-
tems from instructional text and query click logs.
Grounded Language Learning There has been
significant interest in grounded language learn-
ing. Perhaps the most closely related work
learns to understand instructions and automati-
1676
cally complete the tasks they describe (Branavan
et al, 2009; Vogel and Jurafsky, 2010; Kush-
man et al, 2009; Branavan et al, 2010; Artzi and
Zettlemoyer, 2013). However, these approaches
did not model user interaction. There are also
many related approaches for other grounded lan-
guage problems, including understanding game
strategy guides (Branavan et al, 2011), model-
ing users goals in a Windows domain (Horvitz
et al, 1998), learning from conversational inter-
action (Artzi and Zettlemoyer, 2011), learning
to sportscast (Chen and Mooney, 2011), learning
from event streams (Liang et al, 2009), and learn-
ing paraphrases from crowdsourced captions of
video snippets (Chen and Dolan, 2011).
Dialog Generation from Text Similarly to Pi-
wek?s work (2007; 2010; 2011), we study extract-
ing dialog knowledge from documents (mono-
logues or instructions). However, Piwek?s ap-
proach generates static dialogs, for example to
generate animations of virtual characters having a
conversation. There is no model of dialog man-
agement or user interaction, and the approach does
not use any machine learning. In contrast, to the
best of our knowledge, we are the first to demon-
strate it is possible to learn complete, interactive
dialog systems using instructional texts (and non-
expert annotation).
Learning from Web Query Logs Web query
logs have been extensively studied. For example,
they are widely used to represent user intents in
spoken language dialogs (Tu?r et al, 2011; Celiky-
ilmaz et al, 2011; Celikyilmaz and Hakkani-Tur,
2012). Web query logs are also used in many other
NLP tasks, including entity linking (Pantel et al,
2012) and training product and job intent classi-
fiers (Li et al, 2008).
Dialog Modeling and User Simulation Many
existing dialog systems learn dialog strategies
from user interactions (Young, 2010; Rieser and
Lemon, 2008). Moreover, dialog data is often lim-
ited and, therefore, user simulation is commonly
used (Scheffler and Young, 2002; Schatzmann et
al., 2006; Georgila et al, 2005).
Our overall approach is also related to many
other dialog management approaches, including
those that construct dialog graphs from dialog data
via clustering (Lee et al, 2009), learn information
state updates using discriminative classification
models (Hakkani-Tur et al, 2012; Mairesse et al,
2009), optimize dialog strategy using reinforce-
ment learning (RL) (Scheffler and Young, 2002;
Rieser and Lemon, 2008), or combine RL with
information state update rules (Heeman, 2007).
However, our approach is unique in the use of in-
ducing task and domain knowledge with light su-
pervision to assist the user with many goals.
8 Conclusions and Future Work
This paper presented a novel approach for au-
tomatically constructing procedural dialog sys-
tems with light supervision, given only textual re-
sources such as instructional text and search query
click logs. Evaluations demonstrated highly accu-
rate performance, on automatic benchmarks and
through a user study.
Although we showed it is possible to build com-
plete systems, more work will be required to scale
the approach to new domains, scale the complex-
ity of the dialog manager, and explore the range of
possible textual knowledge sources that could be
incorporated. We are particularly interested in sce-
narios that would enable end users to author new
goals by writing procedural instructions in natural
language.
Acknowledgments
The authors would like to thank Jason Williams
and the anonymous reviewers for their helpful
comments and suggestions.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, An-
thanasios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David R. Traum. 2012. The twins
corpus of museum visitor questions. In Proceedings
of LREC.
Yoav Artzi and Luke Zettlemoyer. 2011. Learning
to recover meaning from unannotated conversational
interactions. In NIPS Workshop In Learning Seman-
tics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of ACM Symposium on User
Interface Software and Technology.
1677
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: learn-
ing to map high-level instructions to commands. In
Proceedings of ACL.
S. R. K. Branavan, David Silver, and Regina Barzi-
lay. 2011. Learning to win by reading manuals in
a monte-carlo framework. In Proceedings of ACL.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2012. A
joint model for discovery of aspects in utterances.
In Proceedings of ACL.
Asli Celikyilmaz, Dilek Hakkani-Tu?r, and Gokhan Tu?r.
2011. Mining search query logs for spoken language
understanding. In Proceedings of ICML.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of AAAI.
Myroslava Dzikovska, Amy Isard, Peter Bell, Jo-
hanna D. Moore, Natalie B. Steinhauser, Gwen-
dolyn E. Campbell, Leanne S. Taylor, Simon Caine,
and Charlie Scott. 2011. Adaptive intelligent tuto-
rial dialogue in the beetle ii system. In Proceedings
of AIED.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Proceed-
ings of Eurospeech.
Dilek Hakkani-Tur, Gokhan Tur, Larry Heck, Ashley
Fidler, and Asli Celikyilmaz. 2012. A discrimi-
native classification-based approach to information
state updates for a multi-domain dialog system. In
Proceedings of Interspeech.
Peter Heeman. 2007. Combining Reinforcement
Learning with Information-State Update Rules. In
Proceedings of ACL.
Eric Horvitz, Jack Breese, David Heckerman, David
Hovel, and Koos Rommelse. 1998. The Lumiere
project: Bayesian user modeling for inferring the
goals and needs of software users. In Proceedings
of Uncertainty in Artificial Intelligence.
Nate Kushman, Micah Brodsky, S. R. K. Branavan,
Dina Katabi, Regina Barzilay, and Martin Rinard.
2009. WikiDo. In ACM HotNets.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proceedings of NAACL.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken lan-
guage understanding from unaligned data using dis-
criminative classification models. In Proceedings of
Acoustics, Speech and Signal Processing.
Fabrizio Morbini, Eric Forbell, David DeVault, Kenji
Sagae, David R. Traum, and Albert A. Rizzo. 2012.
A mixed-initiative conversational dialogue system
for healthcare. In Proceedings of SIGDIAL.
Jorge Nocedal and Stephen J. Wright. 2000. Numeri-
cal Optimization. Springer.
Patric Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent.
In Proceedings of ACL.
Paul Piwek and Svetlana Stoyanchev. 2010. Generat-
ing expository dialogue from monologue: Motiva-
tion, corpus and preliminary rules. In Proceedings
of NAACL.
Paul Piwek and Svetlana Stoyanchev. 2011. Data-
oriented monologue-to-dialogue generation. In Pro-
ceedings of ACL, pages 242?247.
Paul Piwek, Hugo Hernault, Helmut Prendinger, and
Mitsuru Ishizuka. 2007. T2d: Generating dialogues
between virtual agents automatically from text. In
Proceedings of Intelligent Virtual Agents.
Verena Rieser and Oliver Lemon. 2008. Learning ef-
fective multimodal dialogue strategies from wizard-
of-oz data: Bootstrapping and evaluation. In Pro-
ceedings of ACL.
A. Rizzo, Kenji Sagae, E. Forbell, J. Kim, B. Lange,
J. Buckwalter, J. Williams, T. Parsons, P. Kenny,
David R. Traum, J. Difede, and B. Rothbaum. 2011.
Simcoach: An intelligent virtual human system for
providing healthcare information and support. In
Proceedings of ITSEC.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2).
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
Human Language Technology Research.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava Dzikovska, and Johanna D. Moore. 2011.
1678
Talk like an electrician: Student dialogue mimick-
ing behavior in an intelligent tutoring system. In
Proceedings of AIED.
David R. Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William R. Swartout. 2012.
Ada and grace: Direct interaction with museum vis-
itors. In Proceedings of Intelligent Virtual Agents.
Go?khan Tu?r, Dilek Z. Hakkani-Tu?r, Dustin Hillard, and
Asli C?elikyilmaz. 2011. Towards unsupervised spo-
ken language understanding: Exploiting query click
logs for slot filling. In Proceedings of Interspeech.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Steve Young. 2010. Cognitive user interfaces. In IEEE
Signal Processing Magazine.
1679
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 505?510,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues
from Multilingual Twitter Streams
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Theresa Wilson
HLTCOE
Johns Hopkins University
Baltimore, MD
taw@jhu.edu
David Yarowsky
CLSP
Johns Hopkins University
Baltimore, MD
yarowsky@cs.jhu.edu
Abstract
We study subjective language in social
media and create Twitter-specific lexi-
cons via bootstrapping sentiment-bearing
terms from multilingual Twitter streams.
Starting with a domain-independent, high-
precision sentiment lexicon and a large
pool of unlabeled data, we bootstrap
Twitter-specific sentiment lexicons, us-
ing a small amount of labeled data to
guide the process. Our experiments on
English, Spanish and Russian show that
the resulting lexicons are effective for
sentiment classification for many under-
explored languages in social media.
1 Introduction
The language that people use to express opinions
and sentiment is extremely diverse. This is true for
well-formed data, such as news and reviews, and
it is particularly true for data from social media.
Communication in social media is informal, ab-
breviations and misspellings abound, and the per-
son communicating is often trying to be funny,
creative, and entertaining. Topics change rapidly,
and people invent new words and phrases.
The dynamic nature of social media together
with the extreme diversity of subjective language
has implications for any system with the goal
of analyzing sentiment in this domain. General,
domain-independent sentiment lexicons have low
coverage. Even models trained specifically on so-
cial media data may degrade somewhat over time
as topics change and new sentiment-bearing terms
crop up. For example, the word ?occupy? would
not have been indicative of sentiment before 2011.
Most of the previous work on sentiment lexicon
construction relies on existing natural language
processing tools, e.g., syntactic parsers (Wiebe,
2000), information extraction (IE) tools (Riloff
and Wiebe, 2003) or rich lexical resources such
as WordNet (Esuli and Sebastiani, 2006). How-
ever, such tools and lexical resources are not avail-
able for many languages spoken in social media.
While English is still the top language in Twitter,
it is no longer the majority. Thus, the applicabil-
ity of these approaches is limited. Any method for
analyzing sentiment in microblogs or other social
media streams must be easily adapted to (1) many
low-resource languages, (2) the dynamic nature of
social media, and (3) working in a streaming mode
with limited or no supervision.
Although bootstrapping has been used for learn-
ing sentiment lexicons in other domains (Turney
and Littman, 2002; Banea et al, 2008), it has not
yet been applied to learning sentiment lexicons for
microblogs. In this paper, we present an approach
for bootstrapping subjectivity clues from Twitter
data, and evaluate our approach on English, Span-
ish and Russian Twitter streams. Our approach:
? handles the informality, creativity and the dy-
namic nature of social media;
? does not rely on language-dependent tools;
? scales to the hundreds of new under-explored
languages and dialects in social media;
? classifies sentiment in a streaming mode.
To bootstrap subjectivity clues from Twitter
streams we rely on three main assumptions:
i. sentiment-bearing terms of similar orienta-
tion tend to co-occur at the tweet level (Tur-
ney and Littman, 2002);
ii. sentiment-bearing terms of opposite orienta-
tion do not co-occur at the tweet level (Ga-
mon and Aue, 2005);
iii. the co-occurrence of domain-specific and
domain-independent subjective terms serves
as a signal of subjectivity.
505
2 Related Work
Mihalcea et.al (2012) classifies methods for boot-
strapping subjectivity lexicons into two types:
corpus-based and dictionary-based.
Dictionary-based methods rely on existing lex-
ical resources to bootstrap sentiment lexicons.
Many researchers have explored using relations in
WordNet (Miller, 1995), e.g., Esuli and Sabastiani
(2006), Andreevskaia and Bergler (2006) for En-
glish, Rao and Ravichandran (2009) for Hindi and
French, and Perez-Rosas et al (2012) for Spanish.
Mohammad et al (2009) use a thesaurus to aid
in the construction of a sentiment lexicon for En-
glish. Other works (Clematide and Klenner, 2010;
Abdul-Mageed et al, 2011) automatically expands
and evaluates German and Arabic lexicons. How-
ever, the lexical resources that dictionary-based
methods need, do not yet exist for the majority of
languages in social media. There is also a mis-
match between the formality of many language re-
sources, such as WordNet, and the extremely in-
formal language of social media.
Corpus-based methods extract subjectivity and
sentiment lexicons from large amounts of unla-
beled data using different similarity metrics to
measure the relatedness between words. Hatzivas-
siloglou and McKeown (1997) were the first to ex-
plore automatically learning the polarity of words
from corpora. Early work by Wiebe (2000) iden-
tifies clusters of subjectivity clues based on their
distributional similarity, using a small amount of
data to bootstrap the process. Turney (2002) and
Velikovich et al (2010) bootstrap sentiment lexi-
cons for English from the web by using Pointwise
Mutual Information (PMI) and graph propaga-
tion approach, respectively. Kaji and Kitsuregawa
(2007) propose a method for building sentiment
lexicon for Japanese from HTML pages. Banea
et al (2008) experiment with Lexical Semantic
Analysis (LSA) (Dumais et al, 1988) to bootstrap
a subjectivity lexicon for Romanian. Kanayama
and Nasukawa (2006) bootstrap subjectivity lexi-
cons for Japanese by generating subjectivity can-
didates based on word co-occurrence patterns.
In contrast to other corpus-based bootstrapping
methods, we evaluate our approach on multiple
languages, specifically English, Spanish, and Rus-
sian. Also, as our approach relies only on the
availability of a bilingual dictionary for translating
an English subjectivity lexicon and crowdsourcing
for help in selecting seeds, it is more scalable and
better able to handle the informality and the dy-
namic nature of social media. It also can be effec-
tively used to bootstrap sentiment lexicons for any
language for which a bilingual dictionary is avail-
able or can be automatically induced from parallel
corpora.
3 Data
For the experiments in this paper, we use three
sets of data for each language: 1M unlabeled
tweets (BOOT) for bootstrapping Twitter-specific
lexicons, 2K labeled tweets for development data
(DEV), and 2K labeled tweets for evaluation
(TEST). DEV is used for parameter tuning while
bootstrapping, and TEST is used to evaluating the
quality of the bootstrapped lexicons.
We take English tweets from the corpus con-
structed by Burger et al (2011) which con-
tains 2.9M tweets (excluding retweets) from 184K
users.1 English tweets are identified automati-
cally using a compression-based language identifi-
cation (LID) tool (Bergsma et al, 2012). Accord-
ing to LID, there are 1.8M (63.6%) English tweets,
which we randomly sample to create BOOT, DEV
and TEST sets for English. Unfortunately, Burger?s
corpus does not include Russian and Spanish data
on the same scale as English. Therefore, for
other languages we construct a new Twitter corpus
by downloading tweets from followers of region-
specific news and media feeds.
Sentiment labels for tweets in DEV and TEST
sets for all languages are obtained using Amazon
Mechanical Turk. For each tweet we collect an-
notations from five workers and use majority vote
to determine the final label for the tweet. Snow
et al (2008) show that for a similar task, labeling
emotion and valence, on average four non-expert
labelers are needed to achieve an expert level of
annotation. Table 1 gives the distribution of tweets
over sentiment labels for the development and test
sets for English (E-DEV, E-TEST), Spanish (S-
DEV, S-TEST), and Russian (R-DEV, R-TEST).
Below are examples of tweets in Russian with En-
glish translations labeled with sentiment:
? Positive: ? ?????? ??????? ???????
? ???? ??????? (Planning for delicious
breakfast and lots of movies);
? Negative: ???? ????????, ? ? ??? ??????
(I want to die and I will do that);
1They provided the tweet IDs, and we used the Twitter
Corpus Tools to download the tweets.
506
Data Positive Neg Both Neutral
E-DEV 617 357 202 824
E-TEST 596 347 195 862
S-DEV 358 354 86 1,202
S-TEST 317 387 93 1203
R-DEV 452 463 156 929
R-TEST 488 380 149 983
Table 1: Sentiment label distribution in develop-
ment DEV and test TEST datasets across languages.
? Both: ??????? ???????? ?????? ???
????? ?? ?? ????. ???? ?????? ????-
?? (I want to write about the movie rougher
but I will not. Although the actors are good);
? Neutral: ?????? ????? ????? ????????
?????? ?????? (Why clever thoughts come
only at night?).
4 Lexicon Bootstrapping
To create a Twitter-specific sentiment lexicon for
a given language, we start with a general-purpose,
high-precision sentiment lexicon2 and bootstrap
from the unlabeled data (BOOT) using the labeled
development data (DEV) to guide the process.
4.1 High-Precision Subjectivity Lexicons
For English we seed the bootstrapping pro-
cess with the strongly subjective terms from the
MPQA lexicon3 (Wilson et al, 2005). These
terms have been previously shown to be high-
precision for recognizing subjective sentences
(Riloff and Wiebe, 2003).
For the other languages, the subjective seed
terms are obtained by translating English seed
terms using a bilingual dictionary, and then col-
lecting judgments about term subjectivity from
Mechanical Turk. Terms that truly are strongly
subjective in translation are used for seed terms
in the new language, with term polarity projected
from the English. Finally, we expand the lexicons
with plurals and inflectional forms for adverbs, ad-
jectives and verbs.
4.2 Bootstrapping Approach
To bootstrap, first the new lexicon LB(0) is seeded
with the strongly subjective terms from the orig-
inal lexicon LI . On each iteration i ? 1, tweets
in the unlabeled data are labeled using the lexicon
2Other works on generating domain-specific sentiment
lexicons e.g., from blog data (Jijkoun et al, 2010) also start
with a general, domain-specific lexicon.
3http://www.cs.pitt.edu/mpqa/
from the previous iteration, LB(i?1). If a tweet
contains one or more terms from LB(i?1) it is con-
sidered subjective, otherwise objective. The polar-
ity of subjective tweets is determined in a similar
way: if the tweet contains ? 1 positive terms, tak-
ing into account the negation, it is considered neg-
ative; if it contains ? 1 negative terms, taking into
account the negation, it is considered positive.4 If
it contains both positive and negative terms, it is
considered to be both. Then, for every term not in
LB(i?1) that has a frequency ? ?freq, the proba-
bility of that term being subjective is calculated as
shown in Algorithm 1 line 10. The top ?k terms
with a subjective probability ? ?pr are then added
to LB(i). The polarity of new terms is determined
based on the probability of the term appearing in
positive or negative tweets as shown in line 18.5
The bootstrapping process terminates when there
are no more new terms meeting the criteria to add.
Algorithm 1 BOOTSTRAP (?, ?pr, ?freq, ?topK )
1: iter = 0, ? = 0.5, LB(~?)? LI(?)
2: while (stop 6= true) do
3: LiterB (~?)? ?,?LiterB (~?)? ?
4: for each new term w ? {V \ LB(~?)} do
5: for each tweet t ? T do
6: if w ? t then
7: UPDATE c(w,LB(~?)), c(w,LposB (~?)), c(w)8: end if
9: end for
10: psubj(w)? c(w,LB(~?))c(w)
11: ppos(w)? c(w,L
pos
B (~?))
c(w,LB(~?))
12: LiterB (~?)? w, psubj(w), ppol(w)13: end for
14: SORT LiterB (~?) by psubj(w)15: while (K ? ?topK) do
16: for each new term w ? LiterB (~?) do
17: if [psubj(w) ? ?pr and cw ? ?freq then
18: if [ppos(w) ? 0.5] then
19: wpol ? positive
20: else
21: wpol ? negative
22: end if
23: ?LiterB (~?)? ?LiterB (~?) + wpol24: end if
25: end for
26: K = K + 1
27: end while
28: if [?LiterB (~?) == 0] then29: stop? true
30: end if
31: LB(~?)? LB(~?) + ?LiterB (~?)32: iter = iter + 1
33: end while
4If there is a negation in the two words before a sentiment
term, we flip its polarity.
5Polarity association probabilities should sum up to 1
ppos(w|LB(~?)) + pneg(w|LB(~?)) = 1.
507
English Spanish Russian
LEI LEB LSI LSB LRI LRB
Pos 2.3 16.8 2.9 7.7 1.4 5.3
Neg 2.8 4.7 5.2 14.6 2.3 5.5
Total 5.1 21.5 8.1 22.3 3.7 10.8
Table 2: The original and the bootstrapped (high-
lighted) lexicon term count (LI ? LB) with polar-
ity across languages (thousands).
The set of parameters ~? is optimized using a grid
search on the development data using F-measure
for subjectivity classification. As a result, for En-
glish ~? = [0.7, 5, 50] meaning that on each itera-
tion the top 50 new terms with a frequency ? 5
and probability ? 0.7 are added to the lexicon.
For Spanish, the set of optimal parameters ~? =
[0.65, 3, 50] and for Russian - ~? = [0.65, 3, 50]. In
Table 2 we report size and term polarity from the
original LI and the bootstrapped LB lexicons.
5 Lexicon Evaluations
We evaluate our bootstrapped sentiment lexicons
English LEB , Spanish LSB and Russian LRB by com-
paring them with existing dictionary-expanded
lexicons that have been previously shown to be ef-
fective for subjectivity and polarity classification
(Esuli and Sebastiani, 2006; Perez-Rosas et al,
2012; Chetviorkin and Loukachevitch, 2012). For
that we perform subjectivity and polarity classifi-
cation using rule-based classifiers6 on the test data
E-TEST, S-TEST and R-TEST.
We consider how the various lexicons perform
for rule-based classifiers for both subjectivity and
polarity. The subjectivity classifier predicts that
a tweet is subjective if it contains a) at least one,
or b) at least two subjective terms from the lexi-
con. For the polarity classifier, we predict a tweet
to be positive (negative) if it contains at least one
positive (negative) term taking into account nega-
tion. If the tweet contains both positive and nega-
tive terms, we take the majority label.
For English we compare our bootstrapped lex-
icon LEB against the original lexicon LEI and
strongly subjective terms from SentiWordNet 3.0
(Esuli and Sebastiani, 2006). To make a fair
comparison, we automatically expand SentiWord-
Net with noun plural forms and verb inflectional
forms. In Figure 1 we report precision, recall
6Similar approach to a rule-based classification using
terms from he MPQA lexicon (Riloff and Wiebe, 2003).
and F-measure results. They show that our boot-
strapped lexicon significantly outperforms Senti-
WordNet for subjectivity classification. For polar-
ity classification we get comparable F-measure but
much higher recall for LEB compared to SWN .
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SWN 0.57 0.27 0.78
LEI 0.71 0.48 0.82
LEB 0.75 0.72 0.78
Figure 1: Precision (x-axis), recall (y-axis) and
F-measure (in the table) for English: LEI = ini-
tial lexicon, LEB = bootstrapped lexicon, SWN =
strongly subjective terms from SentiWordNet.
For Spanish we compare our bootstrapped lex-
icon LSB against the original LSI lexicon, and the
full and medium strength terms from the Span-
ish sentiment lexicon constructed by Perez-Rosas
et el. (2012). We report precision, recall and F-
measure in Figure 2. We observe that our boot-
strapped lexicon yields significantly better perfor-
mance for subjectivity classification compared to
both full and medium strength terms. However,
our bootstrapped lexicon yields lower recall and
similar precision for polarity classification.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
SM 0.44 0.17 0.64
SF 0.47 0.13 0.66
LSI 0.59 0.45 0.58
LSB 0.59 0.59 0.55
Figure 2: Precision (x-axis), recall (y-axis) and F-
measure (in the table) for Spanish: LSI = initial
lexicon, LSB = bootstrapped lexicon, SF = full
strength terms; SM = medium strength terms.
508
For Russian we compare our bootstrapped lex-
icon LRB against the original LRI lexicon, and the
Russian sentiment lexicon constructed by Chetv-
iorkin and Loukachevitchet (2012). The external
lexicon in Russian P was built for the domain
of product reviews and does not include polarity
judgments for subjective terms. As before, we
expand the external lexicon with the inflectional
forms for adverbs, adjectives and verbs. We report
results for Russian in Figure 3. We find that for
subjectivity our bootstrapped lexicon shows better
performance compared to the external lexicon (5k
terms). However, the expanded external lexicon
(17k terms) yields higher recall with a significant
drop in precision. Note that for Russian, we report
polarity classification results for LRB and LRI lexi-
cons only because P does not have polarity labels.
(a) Subj ? 1 (b) Subj ? 2 (c) Polarity
Lexicon Fsubj?1 Fsubj?2 Fpolarity
P 0.55 0.29 ?
PX 0.62 0.47 ?
LRI 0.46 0.13 0.73
LRB 0.61 0.35 0.73
Figure 3: Precision (x-axis), recall (y-axis) and F-
measure for Russian: LRI = initial lexicon, LRB =
bootstrapped lexicon, P = external sentiment lex-
icon, PX = expanded external lexicon.
We next perform error analysis for subjectiv-
ity and polarity classification for all languages and
identify common errors to address them in future.
For subjectivity classification we observe that
applying part-of-speech tagging during the boot-
strapping could improve results for all languages.
We could further improve the quality of the lex-
icon and reduce false negative errors (subjec-
tive tweets classified as neutral) by focusing on
sentiment-bearing terms such as adjective, adverbs
and verbs. However, POS taggers for Twitter are
only available for a limited number of languages
such as English (Gimpel et al, 2011). Other false
negative errors are often caused by misspellings.7
7For morphologically-rich languages, our approach cov-
ers different linguistic forms of terms but not their mis-
spellings. However, it can be fixed by an edit-distance check.
We also find subjective tweets with philosophi-
cal thoughts and opinions misclassified, especially
in Russian, e.g., ?????? ?? ?????? ?? ??????
? ?????????? ???????? ????? ?? ??? ???-
?? ??? ?? ??????? ?? ???????? (Sometimes we
are not ready to fulfill our dreams yet but, at the
same time, we do not want to scare them). Such
tweets are difficult to classify using lexicon-based
approaches and require deeper linguistic analysis.
False positive errors for subjectivity classifica-
tion happen because some terms are weakly sub-
jective and can be used in both subjective and
neutral tweets e.g., the Russian term ??????????
(brag) is often used as subjective, but in a tweet
??????? ?? ????? ?????????? ??????? (never
brag about your future) it is used as neutral. Simi-
larly, the Spanish term buenas (good) is often used
subjectively but it is used as neutral in the follow-
ing tweet ?@Diveke me falto el buenas! jaja que
onda que ha pasado? (I miss the good times we
had, haha that wave has passed!).
For polarity classification, most errors happen
because our approach relies on either positive or
negative polarity scores for a term but not both.8
However, in the real world terms may sometimes
have both usages. Thus, some tweets are misclas-
sified (e.g., ?It is too warm outside?). We can
fix this by summing over weighted probabilities
rather than over term counts. Additional errors
happen because tweets are very short and convey
multiple messages (e.g., ?What do you mean by
unconventional? Sounds exciting!?) Thus, our ap-
proach can be further improved by adding word
sense disambiguation and anaphora resolution.
6 Conclusions
We propose a scalable and language independent
bootstrapping approach for learning subjectivity
clues from Twitter streams. We demonstrate the
effectiveness of the bootstrapping procedure by
comparing the resulting subjectivity lexicons with
state-of the-art sentiment lexicons. We perform
error analysis to address the most common error
types in the future. The results confirm that the
approach can be effectively exploited and further
improved for subjectivity classification for many
under-explored languages in social media.
8During the bootstrapping we calculate probability for a
term to be positive and negative, e.g., p(warm|+) = 0.74
and p(warm|?) = 0.26. But during polarity classification
we rely on the highest probability score and consider it to be
?the polarity? for the term e.g., positive for warm.
509
References
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard arabic. In Pro-
ceedings of ACL/HLT.
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of
EACL.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proceedings of LREC.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of 2nd Workshop on
Language in Social Media.
John D. Burger, John C. Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twittier. In Proceedings of EMNLP.
Ilia Chetviorkin and Natalia V. Loukachevitch. 2012.
Extraction of Russian sentiment lexicon for product
meta-domain. In Proceedings of COLING.
Simon Clematide and Manfred Klenner. 2010. Eval-
uation and extension of a polarity lexicon for Ger-
man. In Proceedings of the 1st Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis.
Susan T. Dumais, George W. Furnas, Thomas K. Lan-
dauer, Scott Deerwester, and Richard Harshman.
1988. Using latent semantic analysis to improve
access to textual information. In Proceedings of
SIGCHI.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting
low association with known sentiment terms. In
Proceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twittier: annotation, features, and experiments.
In Proceedings of ACL.
Vasileios Hatzivassiloglou and Kathy McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL.
Valentin Jijkoun, Maarten de Rijke, and Wouter
Weerkamp. 2010. Generating focused topic-
specific sentiment lexicons. In Proceedings of ACL.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive
collection of html documents. In Proceedings of
EMNLP.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of
EMNLP.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2012. Multilingual subjectivity and sentiment anal-
ysis. In Proceedings of ACL.
George A. Miller. 1995. Wordnet: a lexical database
for English. Communications of the ACM, 38(11).
Saif Mohammad, Cody Dunne, and Bonnie Dorr.
2009. Generating high-coverage semantic orienta-
tion lexicons from overtly marked words and a the-
saurus. In Proceedings of EMNLP.
Veronica Perez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in Span-
ish. In Proceedings of LREC.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceed-
ings of EACL.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. Computing Research
Repository.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The viabil-
ity of web-derived polarity lexicons. In Proceedings
of NAACL.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of EMNLP.
510
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186?196,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Inferring User Political Preferences from Streaming Communications
Svitlana Volkova,
1
Glen Coppersmith
2
and Benjamin Van Durme
1,2
1
Center for Language and Speech Processing,
2
Human Language Technology Center of Excellence,
Johns Hopkins University, Baltimore, MD 21218
svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.edu
Abstract
Existing models for social media per-
sonal analytics assume access to thou-
sands of messages per user, even though
most users author content only sporadi-
cally over time. Given this sparsity, we:
(i) leverage content from the local neigh-
borhood of a user; (ii) evaluate batch mod-
els as a function of size and the amount
of messages in various types of neighbor-
hoods; and (iii) estimate the amount of
time and tweets required for a dynamic
model to predict user preferences. We
show that even when limited or no self-
authored data is available, language from
friend, retweet and user mention commu-
nications provide sufficient evidence for
prediction. When updating models over
time based on Twitter, we find that polit-
ical preference can be often be predicted
using roughly 100 tweets, depending on
the context of user selection, where this
could mean hours, or weeks, based on the
author?s tweeting frequency.
1 Introduction
Inferring latent user attributes such as gender, age,
and political preferences (Rao et al, 2011; Za-
mal et al, 2012; Cohen and Ruths, 2013) auto-
matically from personal communications and so-
cial media including emails, blog posts or public
discussions has become increasingly popular with
the web getting more social and volume of data
available. Resources like Twitter
1
or Facebook
2
become extremely valuable for studying the un-
derlying properties of such informal communica-
tions because of its volume, dynamic nature, and
diverse population (Lunden, 2012; Smith, 2013).
1
http://www.demographicspro.com/
2
http://www.wolframalpha.com/facebook/
The existing batch models for predicting latent
user attributes rely on thousands of tweets per
author (Rao et al, 2010; Conover et al, 2011;
Pennacchiotti and Popescu, 2011a; Burger et al,
2011; Zamal et al, 2012; Nguyen et al, 2013).
However, most Twitter users are less prolific than
those examined in these works, and thus do not
produce the thousands of tweets required to obtain
their levels of accuracy e.g., the median number of
tweets produced by a random Twitter user per day
is 10. Moreover, recent changes to Twitter API
querying rates further restrict the speed of access
to this resource, effectively reducing the amount of
data that can be collected in a given time period.
In this paper we analyze and go beyond static
models formulating personal analytics in social
media as a streaming task. We first evaluate batch
models that are cognizant of low-resource predic-
tion setting described above, maximizing the effi-
ciency of content in calculating personal analytics.
To the best of our knowledge, this is the first work
that makes explicit the tradeoff between accuracy
and cost (manifest as calls to the Twitter API),
and optimizes to a different tradeoff than state-of-
the-art approaches, seeking maximal performance
when limited data is available. In addition, we
propose streaming models for personal analytics
that dynamically update user labels based on their
stream of communications which has been ad-
dressed previously by Van Durme (2012b). Such
models better capture the real-time nature of evi-
dence being used in latent author attribute predic-
tions tasks. Our main contributions include:
- develop low-resource and real-time dynamic
approaches for personal analytics using as an
example the prediction of political preference
of Twitter users;
- examine the relative utility of six different
notions of ?similarity? between users in an
implicit Twitter social network for personal
analytics;
186
- experiments are performed across multiple
datasets supporting the prediction of politi-
cal preference in Twitter, to highlight the sig-
nificant differences in performance that arise
from the underlying collection and annota-
tion strategies.
2 Identifying Twitter Social Graph
Twitter users interact with one another and en-
gage in direct communication in different ways
e.g., using retweets, user mentions e.g., @youtube
or hashtags e.g., #tcot, in addition to having ex-
plicit connections among themselves such as fol-
lowing, friending. To investigate all types of social
relationships between Twitter users and construct
Twitter social graphs we collect lists of followers
and friends, and extract user mentions, hashtags,
replies and retweets from communications.
3
2.1 Social Graph Definition
Lets define an attributed, undirected graph G =
(V,E), where V is a set of vertices and E is a set
of edges. Each vertex v
i
represents someone in
a communication graph i.e., communicant: here
a Twitter user. Each vertex is attributed with a
feature vector
~
f(v
i
) which encodes communica-
tions e.g., tweets available for a given user. Each
vertex is associated with a latent attribute a(v
i
),
in our case it is binary a(v
i
) ? {D,R}, where
D stands for Democratic and R for Republican
users. Each edge e
ij
? E represents a connec-
tion between v
i
and v
j
, e
ij
= (v
i
, v
j
) and defines
different social circles between Twitter users e.g.,
follower (f), friend (b), user mention (m), hash-
tag (h), reply (y) and retweet (w). Thus, E ?
V
(2)
?{f, b, h,m,w, y}. We denote a set of edges
of a given type as ?
r
(E) for r ? {f, b, h,m,w, y}.
We denote a set of vertices adjacent to v
i
by so-
cial circle type r as N
r
(v
i
) which is equivalent to
{v
j
| e
ij
? ?
r
(E)}. Following Filippova (2012)
we refer to N
r
(v
i
) as v
i
?s social circle, otherwise
known as a neighborhood. In most cases, we only
work with a sample of a social circle, denoted by
N
?
r
(v
i
) where |N
?
r
(v
i
)| = k is its size for v
i
.
Figure 1 presents an example of a social graph
derived from Twitter. Notably, users from differ-
ent social circles can be shared across the users of
the same or different classes e.g., a user v
j
can be
3
The code and detailed explanation on how we col-
lected all six types of user neighbors and their com-
munications using Twitter API can be found here:
http://www.cs.jhu.edu/ svitlana/
Figure 1: An example of a social graph with follower, friend,
@mention, reply, retweet and hashtag social circles for each
user of interest e.g., blue: Democratic, red: Republican.
in both follower circle v
j
? N
f
(v
i
), v
i
? D and
retweet circle v
j
? N
w
(v
k
), v
k
? R.
2.2 Candidate-Centric Graph
We construct candidate-centric graph G
cand
by
looking into following relationships between the
users and Democratic or Republican candidates
during the 2012 US Presidential election. In the
Fall of 2012, leading up to the elections, we ran-
domly sampled n = 516 Democratic and m =
515 Republican users. We labeled users as Demo-
cratic if they exclusively follow both Democratic
candidates
4
? BarackObama and JoeBiden but
do not follow both Republican candidates ? Mit-
tRomney and RepPaulRyan and vice versa. We
collectively refer to D and R as our ?users of in-
terest? for which we aim to predict political prefer-
ence. For each such user we collect recent tweets
and randomly sample their immediate k = 10
neighbors from follower, friend, user mention, re-
ply, retweet and hashtag social circles.
2.3 Geo-Centric Graph
We construct a geo-centric graph G
geo
by col-
lecting n = 135 Democratic and m = 135 Re-
publican users from the Maryland, Virginia and
Delaware region of the US with self-reported po-
litical preference in their biographies. Similar to
the candidate-centric graph, for each user we col-
lect recent tweets and randomly sample user social
circles in the Fall of 2012. We collect this data to
get a sample of politically less active users com-
pared to the users from candidate-centric graph.
2.4 ZLR Graph
We also consider a G
ZLR
graph constructed from
a dataset previously used for political affiliation
4
As of Oct 12, 2012, the number of followers for Obama,
Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.
187
classification (Zamal et al, 2012). This dataset
consists of 200 Republican and 200 Democratic
users associated with 925 tweets on average per
user.
5
Each user has on average 6155 friends with
642 tweets per friend. Sharing restrictions and rate
limits on Twitter data collection only allowed us to
recreate a semblance of ZLR data
6
? 193 Demo-
cratic and 178 Republican users with 1K tweets
per user, and 20 neighbors of four types including
follower, friends, user mention and retweet with
200 tweets per neighbor for each user of interest.
3 Batch Models
Baseline User Model As input we are given a set
of vertices representing users of interest v
i
? V
along with feature vectors
~
f(v
i
) derived from con-
tent authored by the user of interest. Each user
is associated with a non-zero number of publicly
posted tweets. Our goal is assign to a category
each user of interest v
i
based on
~
f(v
i
). Here we
focus on a binary assignment into the categories
Democratic D or Republican R. The log-linear
model
7
for such binary classification is:
?
v
i
=
{
D (1 + exp[?
~
? ?
~
f(v
i
)])
?1
? 0.5,
R otherwise.
(1)
where features are normalized word ngram counts
extracted from v
i
?s tweets
~
f
t
(v
i
) : D?t(v
i
)? R.
The proposed baseline model follows the same
trends as the existing state-of-the-art approaches
for user attribute classification in social media as
described in Section 8. Next we propose to ex-
tend the baseline model by taking advantage of
language in user social circles as describe below.
Neighbor Model As input we are given user-local
neighborhood N
r
(v
i
), where r is a neighborhood
type. Besides the neighborhood?s type r, each is
characterized by:
? the number of communications per neighbor
~
f
t
(N
r
), t = {5, 10, 15, 25, 50, 100, 200};
5
The original dataset was collected in 2012 and has
been recently released at http://icwsm.cs.mcgill.ca/. Politi-
cal labels are extracted from http://www.wefollow.com as de-
scribed by Pennacchiotti and Popescu (2011b).
6
This inability to perfectly replicate prior work based on
Twitter is a recognized problem throughout the community of
computational social science, arising from the data policies of
Twitter itself, it is not specific to this work.
7
We use log-linear models over reasonable alternatives
such as perceptron or SVM, following the practice of a wide
range of previous work in related areas (Smith, 2004; Liu et
al., 2005; Poon et al, 2009) including text classification in so-
cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).
? the order of the social circle ? the num-
ber of neighbors per user of interest |N
r
| =
deg(v
i
), n = {1, 2, 5, 10}.
Our goal is to classify users of interest using
evidence (e.g., communications) from their local
neighborhood
?
n
~
f
t
[N
r
(v
i
)] ?
~
f(N
r
) as Demo-
cratic or Republican. The corresponding log-
linear model is defined as:
?
N
r
=
{
D (1 + exp[?
~
? ?
~
f(N
r
)])
?1
? 0.5,
R otherwise.
(2)
To check whether our static models are cog-
nizant of low-resource prediction settings we com-
pare the performance of the user model from Eq.1
and the neighborhood model from Eq.2. Follow-
ing the streaming nature of social media, we see
the scarce available resource as the number of re-
quests allowed per day to the Twitter API. Here
we abstract this to a model assumption where we
receive one tweet t
k
at a time and aim to maximize
classification performance with as few tweets per
user as possible:
8
? for the baseline user model:
minimize
k
?
k
t
k
(v
i
),
(3)
? for the neighborhood model:
minimize
k
?
n
?
k
t
k
[N
r
(v
i
)].
(4)
4 Streaming Models
We rely on straightforward Bayesian rule update
to our batch models in order to simulate a real-
time streaming prediction scenario as a first step
beyond the existing models as shown in Figure 2.
The model makes predictions of a latent user at-
tribute e.g., Republican under a model assumption
of sequentially arriving, independent and identi-
cally distributed observations T = (t
1
, . . . , t
k
)
9
.
The model dynamically updates posterior proba-
bility estimates p(a(v
i
) = R|t
k
) for a given user
8
The separate issue is that many authors simply don?t
tweet very often. For instance, 85.3% of all Twitter
users post less than one update per day as reported at
http://www.sysomos.com/insidetwitter/. Thus, their commu-
nications are scare even if we could get al of them without
rate limiting from Twitter API.
9
Given the dynamic character of online discourse it will
clearly be of interest in the future to consider models that go
beyond the iid assumption.
188
p(R|t
1
)
0.6
v
i
v
i
v
i
p(R|t
1
, t
2
)
0.7
p(R|t
1
, . . . t
k
)
0.9
N
r
N
r
N
r
Time, ??2?1 ?k
Figure 2: Stream-based classification of an attribute a(v
i
) ?
{R,D} given a stream of communications t
1
, t
2
, . . . , t
k
au-
thored by a user v
i
or user immediate neighbors from N
r
social circles at time ?
1
, ?
2
, . . . , ?
k
.
v
i
as an additional evidence t
k
is acquired, as de-
fined in a general form below for any latent at-
tribute a(v
i
) ? A given the tweets T of user v
i
:
p(a(v
i
) = x ? A | T ) =
p(T | a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
p(T | a(v
i
) = y) ? p(a(v
i
) = y)
=
?
k
p(t
k
| a(v
i
) = x) ? p(a(v
i
) = x)
?
y?A
?
k
p(t
k
| a(v
i
) = y) ? p(a(v
i
) = y),
(5)
where y is the number of all possible attribute val-
ues, and k is the number of tweets per user.
For example, to predict user political prefer-
ence, we start with a prior P (R) = 0.5, and se-
quentially update the posterior p(R | T ) by accu-
mulating evidence from the likelihood p(t
k
|R):
p(R | T ) =
?
k
p(t
k
|R) ? p(R)
?
k
P (t
k
|R) ? p(R) +
?
k
P (t
k
|D) ? p(D).
(6)
Our goal is to maximize posterior probability
estimates given a stream of communications for
each user in the data over (a) time ? and (b) the
number of tweets T . For that, for each user we
take tweets that arrive continuously over time and
apply two different streaming models:
? User Model with Dynamic Updates: re-
lies exclusively on user tweets t
(v
i
)
1
, . . . , t
(v
i
)
k
following the order they arrive over time ? ,
where for each user v
i
we dynamically up-
date the posterior p(R | t
(v
i
)
1
, . . . , t
(v
i
)
k
).
? User-Neighbor Model with Dynamic Up-
dates: relies on both neighbor N
r
commu-
nications including friend, follower, retweet,
user mention and user tweets t
(v
i
)
1
, . . . , t
(N
r
)
k
following the order they arrive over time ? ;
here we dynamically update the posterior
probability p(R | t
(v
i
)
1
, . . . , t
(N
r
)
k
).
5 Experimental Setup
We design a set of experiments to analyze static
and dynamic models for political affiliation classi-
fication defined in Sections 3 and 4.
5.1 Batch Classification Experiments
We first answer whether communications from
user-local neighborhoods can help predict politi-
cal preference for the user. To explore the con-
tribution of different neighborhood types we learn
static user and neighbor models on G
cand
, G
geo
and G
ZLR
graphs. We also examine the ability of
our static models to predict user political prefer-
ences in low-resource setting e.g., 5 tweets.
The existing models follow a standard setup
when either user or neighbor tweets are available
during train and test. For a static neighbor model
we go beyond that, and train our the model on all
data available per user, but only apply part of the
data at the test time, pushing the boundaries of
how little is truly required for classification. For
example, we only use follower tweets for G
test
,
but we use tweets from all types of neighbors for
G
train
. Such setup will simulate different real-
world prediction scenarios which have not been
previously explored, to our knowledge e.g., when
a user has a private profile or has not tweeted yet,
and only user neighbor tweets are available.
We experiment with our static neighbor model
defined in Eq.2 with the aim to:
1. evaluate neighborhood size influence, we
change the number of neighbors and try n =
[1, 2, 5, 10] neighbor(s) per user;
2. estimate neighbor content influence, we alter-
nate the amount of content per neighbor and
try t = [5, 10, 15, 25, 50, 100, 200] tweets.
We perform 10-fold cross validation
10
and run
100 random restarts for every n and t parame-
ter combination. We compare our static neigh-
bor and user models using the cost functions
from Eq.3 and Eq.4. For all experiments we use
LibLinear (Fan et al, 2008), integrated in the
Jerboa toolkit (Van Durme, 2012a). Both mod-
els defined in Eq.1 and Eq.2 are learned using
normalized count-based word ngram features ex-
tracted from either user or neighbor tweets.
11
10
For each fold we split the data into 3 parts: 70% train,
10% development and 20% test.
11
For brevity we omit reporting results for bigram and tri-
gram features, since unigrams showed superior performance.
189
5.2 Streaming Classification Experiments
We evaluate our models with dynamic Bayesian
updates on a continuous stream of communica-
tions over time as shown in Figure 2. Unlike static
model experiments, we are not modeling the in-
fluence of the number of neighbors or the amount
of content per neighbor. Here, we order user and
neighbor communication streams by real world
time of posting and measure changes in posterior
probabilities over time. The main purpose of these
experiments is to quantitatively evaluate (1) the
number of tweets and (2) the amount of real world
time it takes to observe enough evidence on Twit-
ter to make reliable predictions.
We experiment with log-linear models defined
in Eq. 1 and 2 and continuously estimate the poste-
rior probabilities P (R | T ) as defined in Eq.6. We
average the posterior probability results over the
users in G
cand
, G
geo
and G
ZLR
graphs. We train
streaming models on an attribute balanced subset
of tweets for each user v
i
excluding v
i
?s tweets (or
v
i
?s neighbor tweets for a joint model). This setup
is similar to leave-one-out classification. The clas-
sifier is learned using binary word ngram features
extracted from user or user-neighbor communi-
cations. We prefer binary to normalized count-
based features to overcome sparsity issues caused
by making predictions on each tweet individually.
6 Static Classification Results
6.1 Modeling User Content Influence
We investigate classification decision probabilities
for our static user model ?
v
i
by making predic-
tions on a random set of 5 vs. 100 tweets per user.
To our knowledge only limited work on personal
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
1.0
User
Clas
sific
ation
 dec
ision
 (pro
babi
lity)
misclassified
misclassifiedcorrect
correct
Figure 3: Classification probabilities for ?
v
i
estimated over
100 users in G
cand
tested on 5 (blue) vs. 100 (green) tweets
per user where Republican = 1, Democratic = 0, filled mark-
ers = correctly classified, not filled = misclassified users.
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l
l l
l l l l
l l l
l l
l l l l
l
l l l l l
l l
l l l
l l
(a) G
geo
: 2 neighbors
l l l l
l l l
5 10 20 50 100 2000
.500
.550
.600
.650
.70
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l l l l l ll l l l
l l
l l
l l l l
l
l l l
l l l l
l l l l
l l l
(b) G
geo
: 10 neighbors
l
l l
l l
l l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
10 50 100 200 400
FriendFollowerHashtagUsermention
RetweetReplyUser
l
l
l l
l l l l ll
l l
l l
l l
l l
l l
l l
l l
l l
l l l
(c) G
cand
: 2 neighbors
l
l l
l l l
l
5 10 20 50 100 2000.
500.5
50.60
0.650
.700.7
5
log(Tweets Per Neighbor)
Accurac
y
50 100 250 500 1000 2000
FriendFollowerHashtagUsermention
RetweetReplyUser
l l
l l l
l l l
l l l l
l
l l l
l
l l
l l l
l l
l l
l l l
(d) G
cand
: 10 neighbors
Figure 4: Modeling the influence of the number of tweets per
neighbor t=[5, .., 200] for G
cand
and G
geo
graphs.
analytics (Burger et al, 2011; Van Durme, 2012b)
have performed this straight-forward comparison.
For that purpose, we take a random partition con-
taining 100 users ofG
cand
graph and perform four
independent classification experiments ? two runs
using 5 and two runs using 100 tweets per user.
Figure 3 demonstrates that more tweets during
prediction time lead to higher accuracy by show-
ing that more users with 100 tweets are correctly
classified e.g., filled green markers in the right up-
per quadrant are true Republicans and in the left
lower quadrant are true Democrats. Moreover, a
lot of users with 100 tweets are close to 0.5 deci-
sion probability which suggests that the classifier
is just uncertain rather then being completely off,
e.g., misclassified Republican users with 5 tweets
(not filled blue markers in the right lower quad-
rant) are close to 0. These results follow natu-
rally from the underlying feature representation:
having more tweets per user leads to a lower vari-
ance estimate of a target multinomial distribution.
The more robustly this distribution is estimated
(based on having more tweets) the more confident
we should be in the classifier output.
6.2 Modeling Neighbor Content Influence
Here we discuss the results for our static neighbor-
hood model. We study the influence of the neigh-
borhood type r and size in terms of the number of
neighbors n and tweets t per neighbor.
190
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
(a) G
cand
: 5 tweets
l
l
l
l
1 2 5 100.50
0.550
.600.
650.7
00.75
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
(b) G
cand
: 200 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
5 10 25 50
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l l
l
l
l
l
l
l l
l
l
l
(c) G
geo
: 5 tweets
l
l
l l
1 2 5 100.50
0.55
0.60
0.65
0.70
log(Number of Neighbors)
Accurac
y
200 400 1000 2000
FriendFollowerHashtag UsermentionRetweetReply
l l
l l
l l
l
l
l
l
l
l
l
l
l
l
l l
l l
(d) G
geo
: 200 tweets
Figure 5: Modeling the influence of the number of neighbors
per user n=[1, .., 10] for G
cand
and G
geo
graphs.
In Figure 4 we present accuracy results for
G
cand
and G
geo
graphs. Following Eq.3 and 4, we
spent an equal amount of resources to obtain 100
user tweets and 10 tweets from 10 neighbors. We
annotate these ?points of equal number of commu-
nications? with a line on top marked with a corre-
sponding number of user tweets.
We show that three of six social circles ? friend,
retweet and user-mention yield better accuracy
compared to the user model for all graphs when
t ? 250. Thus, for effectively classifying a given
user v
i
it is better to take 200 tweets each from 10
neighbors rather than 2,000 tweets from the user.
The best accuracy for G
cand
is 0.75 for friend,
follower, retweet and user-mention neighborhoods
which is 0.03 higher than the user baseline; for
G
geo
is 0.67 for user-mention and 0.64 for retweet
circles compared to 0.57 for the user model; for
G
ZLR
is 0.863 for retweet and 0.849 for friend
circles which is 0.11 higher that the user baseline.
Finally, similarly to the results for the user model
given in Figure 3, increasing the number of tweets
per neighbor from 5 to 200 leads to a significant
gain in performance for all neighborhood types.
6.3 Modeling Neighborhood Size
In Figure 5 we present accuracy results to show
neighborhood size influence on classification per-
formance for G
geo
and G
cand
graphs. Our re-
sults demonstrate that even small changes to the
neighborhood size n lead to better performance
which does not support the claims by Zamal et al
(2012). We demonstrate that increasing the size
of the neighborhood leads to better performance
across six neighborhood types. Friend, user men-
tion and retweet neighborhoods yield the highest
accuracy for all graphs. We observe that when the
number of neighbors is n = 1, the difference in
accuracy across all neighborhood types is less sig-
nificant but for n ? 2 it becomes more significant.
7 Streaming Classification Results
7.1 Modeling Dynamic Posterior Updates
from a User Stream
Figures 6a and 6b demonstrate dynamic user
model prediction results averaged over users from
G
cand
and G
ZLR
graphs. Each figure outlines
changes in sequential average probability esti-
mates p
?
(R | T ) for each individual self-authored
tweet t
k
as defined in Eq. 6. The average proba-
bility estimates p
?
(R | T ) are reported for every 5
tweets in a stream T = (t
1
, . . . t
k
) as
?
n
P (R|t
k
)
n
,
where n is the total number of users with the same
attribute R or D. We represent p
?
(R | T ) as a
box and whisker plot with the median, lower and
upper quantiles to show the variance; the length of
whiskers indicate lower and upper extreme values.
We find similar behavior across all three graphs.
In particular, the posterior estimates converge
faster when predicting Democratic than Republi-
can users but it has been trained on an equal num-
ber of tweets per class. We observe that average
posterior estimates P
?
(R | T ) converge faster to 0
0.5
0.6
0.7
0.8
0.9
1.0
0 20 40 60
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 20 40 60
Tweet Stream (T)
p(R
epu
blic
an|T
)
(a) User G
cand
0.5
0.6
0.7
0.8
0.9
1.0
0 50 100 150
p(R
epu
blic
an|T
)
0.0
0.1
0.2
0.3
0.4
0.5
0 50 100 150
Tweet Stream (T)
p(R
epu
blic
an|T
)
(b) User G
ZLR
Figure 6: Streaming classification results from user commu-
nications for G
cand
and G
ZLR
graphs averaged over every 5
tweets (red - Republican, blue - Democratic).
191
300
400
500
0 5 10 15
Time in Weeks
Us
ers
(a) User G
cand
0
100
200
0 20 40 60 80
Time in Weeks
Us
ers
(b) User G
ZLR
300
400
500
0 1 2 3 4 5
Time in Weeks
Us
ers
(c) User-Neigh G
cand
0
100
200
0 10 20 30 40
Time in Weeks
Us
ers
(d) User-Neigh G
ZLR
Figure 7: Time needed for (a) - (b) dynamic user model and
(c) - (d) joint user-neighbor model to infer political prefer-
ences of Democratic (blue) and Republican (red) users at
75% (dotted line) and 95% (solid line) accuracy levels.
(Democratic) than to 1 (Republican) in Figures 6a
and 6b. It suggests that language of Democrats is
more expressive of their political preference than
language of Republicans. For example, frequent
politically influenced terms used widely by Demo-
cratic users include faith4liberty, constitutionally,
pass, vote2012, terroristic.
The variance for average posterior estimates
decreases when the number of tweets increases
for all three datasets. Moreover, we detect that
P
?
(R|T ) estimates for users in G
cand
converge 2-
3 times faster in terms of number of tweets than
for users in G
ZLR
. The lowest convergence is de-
tected for G
geo
where after t
k
= 250 tweets the
average posterior estimate P
?
(R | t
k
) = 0.904 ?
0.044 and P
?
(D | t
k
) = 0.861 ? 0.008. It means
that users inG
cand
are more politically vocal com-
pared to users in G
ZLR
and G
geo
. As a result,
less active users in G
geo
just need more than 250
tweets to converge to a true 0 or 1 class. These re-
sults are coherent with the outcomes for our static
models shown in Figures 4 and 5. These findings
further confirm that differences in performance are
caused by various biases present in the data due to
distinct sampling and annotation approaches.
Figure 7a and 7b illustrate the amount of time
required for the user model to infer political pref-
erences estimated for 1,031 users inG
cand
and 371
users inG
ZLR
. The amount of time needed can be
evaluated for different accuracy levels e.g., 0.75
and 0.95. Thus, with 75% accuracy we classify:
? 100 (?20%) Republican users in 3.6 hours
and Democratic users in 2.2 hours for G
cand
;
? 100 (?56%) R users in 20 weeks and 100
(?52%) D users in 8.9 weeks for G
ZLR
which is 800 times longer that for G
cand
;
? 100 (?75%) R users in 12 weeks and 80
(?60%) D users in 19 weeks for G
geo
.
Such extreme divergences in the amount of time
required for classification across all graphs should
be of strong interest to researchers concerned with
latent attribute prediction tasks because Twitter
users produce messages with extremely different
frequencies. In our case, users in G
ZLR
tweet ap-
proximately 800 times less frequently than users
in G
cand
.
7.2 Modeling Dynamic Posterior Updates
from a Joint User-Neighbor Stream
We estimate dynamic posterior updates from a
joint stream of user and neighbor communications
in G
geo
, G
cand
and G
ZLR
graphs. To make a fair
comparison with a streaming user model, we start
with the same user tweet t
0
(v
i
). Then instead of
waiting for the next user tweet we rely on any
neighbor tweets that appear until the user produces
the next tweet t
1
(v
i
). We rely on communications
from four types of neighbors such as friends, fol-
lowers, retweets and user mentions.
The convergence rate for the average posterior
probability estimates P
?
(R|T ) depending on the
number of tweets is similar to the user model re-
sults presented in Figure 6. However, for G
geo
the variance for P
?
(R|T ) is higher for Democratic
users; for G
ZLR
P
?
(R|T ) ? 1 for Republicans
in less than 110 tweets which is ?t = 40 tweets
faster than the user model; for G
cand
the conver-
gence for both P
?
(R|T ) ? 1 and P
?
(D|T ) ? 0
is not significantly different than the user model.
Figures 7c and 7d show the amount of time re-
quired for a joint user-neighbor model to infer po-
litical preferences estimated for users inG
cand
and
G
ZLR
. We find that with 75% accuracy we can
classify 100 users for:
? G
cand
: Republican users in 23 minutes and
Democratic users in 10 minutes;
? G
ZLR
: R users in 3.2 weeks and D users in
1.1 weeks which is 7 times faster on average
across attributes than for the user model;
? G
geo
: R users in 1.2 weeks and D users in
3.5 weeks which is on average 6 times faster
across attributes than for the user model.
Similar or better P
?
(R|T ) convergence in terms
of the number of tweets and, especially, in the
amount of time needed for user and user-neighbor
192
models further confirms that neighborhood con-
tent is useful for political preference prediction.
Moreover, communications from a joint stream al-
low to make an inference up to 7 times faster.
8 Related Work
Supervised Batch Approaches The vast major-
ity of work on predicting latent user attributes in
social media apply supervised static SVM mod-
els for discrete categorical e.g., gender and re-
gression models for continuous attributes e.g., age
with lexical bag-of-word features for classifying
user gender (Garera and Yarowsky, 2009; Rao et
al., 2010; Burger et al, 2011; Van Durme, 2012b),
age (Rao et al, 2010; Nguyen et al, 2011; Nguyen
et al, 2013) or political orientation. We present an
overview of the existing models for political pref-
erence prediction in Table 1.
Bergsma et al (2012) following up on Rao?s
work (2010) on adding socio-linguistic features
to improve gender, ethnicity and political prefer-
ence prediction show that incorporating stylistic
and syntactic information to the bag-of-word fea-
tures improves gender classification.
Other methods characterize Twitter users by ap-
plying limited amounts of network structure in-
formation in addition to lexical features. Con-
Approach Users Tweets Features Accur.
Rao et al
(2010)
1K 2M
ngrams
socio-ling
stacked
0.824
0.634
0.809
Pennacchiotti
and Popescu
(2011a)
10.3K ?
ling-all
soc-all
full
0.770
0.863
0.889
Conover et
al. (2011)
1,000 1M
full-text
hashtags
clusters
0.792
0.908
0.949
Zamal et al
(2012)
400
400K
3.85M
4.25M
UserOnly
Nbr
User-Nbr
11
0.890
0.920
0.932
Cohen and
Ruths
(2013)
397
1.8K
262
196
397K
1.8M
262K
196K
features
from (Za-
mal et al,
2012)
0.910
0.840
0.680
0.870
This paper
(batch clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
206K
2M
54K
540K
371K
1.5M
user ngrams
neighbor
user ngrams
neighbor
user ngrams
neighbor
0.720
0.750
0.570
0.670
0.886
0.920
This paper
(dynamic
Bayesian
update clas-
sification)
G
cand
1,031
G
geo
270
G
ZLR
371
103K
130K
54K
67K
74K
185K
user stream
user-neigh.
user stream
user-neigh.
user stream
user-neigh.
0.995
0.999
0.843
0.882
0.892
0.999
Table 1: Overview of the existing approaches for political
preference classification in Twitter.
nover et al (2011) rely on identifying strong parti-
san clusters of Democratic and Republican users
in a Twitter network based on retweet and user
mention degree of connectivity, and then combine
this clustering information with the follower and
friend neighborhood size features. Pennacchiotti
et al (2011a; 2011b) focus on user behavior, net-
work structure and linguistic features. Similar to
our work, they assume that users from a partic-
ular class tend to reply and retweet messages of
the users from the same class. We extend this as-
sumption and study other relationship types e.g.,
friends, user mentions etc. Recent work by Wong
et al (2013) investigates tweeting and retweet-
ing behavior for political learning during 2012 US
Presidential election. The most similar work to
ours is by Zamal et al (2012), where the authors
apply features from the tweets authored by a user?s
friend to infer attributes of that user. In this paper,
we study different types of user social circles in
addition to a friend network.
Additionally, using social media for mining po-
litical opinions (O?Connor et al, 2010a; May-
nard and Funk, 2012) or understanding socio-
political trends and voting outcomes (Tumasjan
et al, 2010; Gayo-Avello, 2012; Lampos et al,
2013) is becoming a common practice. For in-
stance, Lampos et al (2013) propose a bilinear
user-centric model for predicting voting intentions
in the UK and Australia from social media data.
Other works explore political blogs to predict what
content will get the most comments (Yano et al,
2013) or analyze communications from Capitol
Hill
12
to predict campaign contributors based on
this content (Yano and Smith, 2013).
Unsupervised Batch Approaches Bergsma et
al. (2013) show that large-scale clustering of user
names improves gender, ethnicity and location
classification on Twitter. O?Connor et al (2010b)
following the work by Eisenstein (2010) propose
a Bayesian generative model to discover demo-
graphic language variations in Twitter. Rao et
al. (2011) suggest a hierarchical Bayesian model
which takes advantage of user name morphology
for predicting user gender and ethnicity. Golbeck
et al (2010) incorporate Twitter data in a spatial
model of political ideology.
Streaming Approaches Van Durme (2012b)
proposed streaming models to predict user gen-
der in Twitter. Other works suggested to process
12
http://www.tweetcongress.org
193
text streams for a variety of NLP tasks e.g., real-
time opinion mining and sentiment analysis in so-
cial media (Pang and Lee, 2008), named entity
disambiguation (Sarmento et al, 2009), statistical
machine translation (Levenberg et al, 2011), first
story detection (Petrovi?c et al, 2010), and unsu-
pervised dependency parsing (Goyal and Daum?e,
2011). Massive Online Analysis (MOA) toolkit
developed by Bifet et al (2010) is an alternative to
the Jerboa package used in this work developed
by Van Durme (2012a). MOA has been effec-
tively used to detect sentiment changes in Twitter
streams (Bifet et al, 2011).
9 Conclusions and Future Work
In this paper, we extensively examined state-of-
the-art static approaches and proposed novel mod-
els with dynamic Bayesian updates for streaming
personal analytics on Twitter. Because our stream-
ing models rely on communications from Twitter
users and content from various notions of user-
local neighborhood they can be effectively applied
to real-time dynamic data streams. Our results
support several key findings listed below.
Neighborhood content is useful for personal
analytics. Content extracted from various notions
of a user-local neighborhood can be as effective
or more effective for political preference classifi-
cation than user self-authored content. This may
be an effect of ?sparseness? of relevant user data,
in that users talk about politics very sporadically
compared to a random sample of their neighbors.
Substantial signal for political preference
prediction is distributed in the neighborhood.
Querying for more neighbors per user is more ben-
eficial than querying for extra content from the
existing neighbors e.g., 5 tweets from 10 neigh-
bors leads to higher accuracy than 25 tweets from
2 neighbors or 50 tweets from 1 neighbor. This
may be also the effect of data heterogeneity in
social media compared to e.g., political debate
text (Thomas et al, 2006). These findings demon-
strate that a substantial signal is distributed over
the neighborhood content.
Neighborhoods constructed from friend,
user mention and retweet relationships are
most effective. Friend, user mention and retweet
neighborhoods show the best accuracy for predict-
ing political preferences of Twitter users. We think
that friend relationships are more effective than
e.g., follower relationships because it is very likely
that users share common interests and preferences
with their friends, e.g. Facebook friends can even
be used to predict a user?s credit score.
13
User
mentions and retweets are two primary ways of in-
teraction on Twitter. They both allow to share in-
formation e.g., political news, events with others
and to be involved in direct communication e.g.,
live political discussions, political groups.
Streaming models are more effective than
batch models for personal analytics. The predic-
tions made using dynamic models with Bayesian
updates over user and joint user-neighbor commu-
nication streams demonstrate higher performance
with lower resources spent compared to the batch
models. Depending on user political involvement,
expressiveness and activeness, the perfect predic-
tion (approaching 100% accuracy) can be made
using only 100 - 500 tweets per user.
Generalization of the classifiers for political
preference prediction. This work raises a very
important but under-explored problem of the gen-
eralization of classifiers for personal analytics in
social media, also recently discussed by Cohen
and Ruth (2013). For instance, the existing models
developed for political preference prediction are
all trained on Twitter data but report significantly
different results even for the same baseline mod-
els trained using bag-of-word lexical features as
shown in Table 1. In this work we experiment with
three different datasets. Our results for both static
and dynamic models show that the accuracy in-
deed depends on the way the data was constructed.
Therefore, publicly available datasets need to be
released for a meaningful comparison of the ap-
proaches for personal analytics in social media.
In future work, we plan to incorporate itera-
tive model updates from newly classified com-
munications similar to online perceptron-style up-
dates. In addition, we aim to experiment with
neighborhood-specific classifiers applied towards
the tweets from neighborhood-specific streams
e.g., friend classifier used for friend tweets,
retweet classifier applied to retweet tweets etc.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their helpful comments.
13
http://money.cnn.com/2013/08/26/technology/social/
facebook-credit-score/
194
References
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 327?337.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1010?1019.
Albert Bifet, Geoff Holmes, Bernhard Pfahringer,
Philipp Kranen, Hardy Kremer, Timm Jansen, and
Thomas Seidl. 2010. MOA: Massive online analy-
sis, a framework for stream classification and clus-
tering. Journal of Machine Learning Research,
11:44?50.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, 17:5?11.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1301?1309.
Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It?s Not Easy! In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM), pages 91?99.
Michael D. Conover, Bruno Gonc?alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the political alignment
of Twitter users. In Proceedings of Social Comput-
ing, pages 192?199.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1277?1287.
Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-
ang Rui Wang, and Chih Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871?1874.
Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1478?1488.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 710?718.
Daniel Gayo-Avello. 2012. No, you cannot predict
elections with Twitter. Internet Computing, IEEE,
16(6):91?94.
Jennifer Golbeck, Justin M. Grimes, and Anthony
Rogers. 2010. Twitter use by the u.s. congress.
Journal of the American Society for Information Sci-
ence and Technology, 61(8):1612?1621.
Amit Goyal and Hal Daum?e, III. 2011. Approxi-
mate scalable bounded space sketch for large data
NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 250?261.
Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
993?1003.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statis-
tical machine translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT), pages 177?186.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 459?466.
Ingrid Lunden. 2012. Analyst: Twitter
passed 500M users in june 2012, 140m of
them in US; Jakarta ?biggest tweeting? city.
http://techcrunch.com/2012/07/30/analyst-twitter-
passed-500m-users-in-june-2012-140m-of-them-in-
us-jakarta-biggest-tweeting-city/.
Diana Maynard and Adam Funk. 2012. Automatic de-
tection of political opinions in tweets. In Proceed-
ings of the 8th International Conference on The Se-
mantic Web (ESWC), pages 88?99.
Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and
Mung Chiang. 2013. Quantifying political leaning
from tweets and retweets. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Dong Nguyen, Noah A. Smith, and Carolyn P. Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 115?123.
195
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. ?How old do you think I am??
A study of language and age in Twitter. In Proceed-
ings of the AAAI Conference on Weblogs and Social
Media (ICWSM), pages 439?448.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 122?129.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010b. A mixture model of de-
mographic lexical variation. In Proceedings of the
NIPS Workshop on Machine Learning and Social
Computing.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations of Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
user classification in twitter. In Proceedings of the
17th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 430?438.
Marco Pennacchiotti and Ana Maria Popescu. 2011b.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
pages 281?288.
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to Twitter. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL).
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 209?217.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC), pages 37?44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hier-
archical Bayesian models for latent attribute detec-
tion in social media. In Proceedings of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM).
Lu??s Sarmento, Alexander Kehlenbeck, Eug?enio
Oliveira, and Lyle Ungar. 2009. An approach to
web-scale named-entity disambiguation. In Pro-
ceedings of the 6th International Conference on Ma-
chine Learning and Data Mining in Pattern Recog-
nition (MLDM), pages 689?703.
Noah A. Smith. 2004. Log-linear models.
Craig Smith. 2013. May 2013 by the
numbers: 16 amazing Twitter stats.
http://expandedramblings.com/index.php/march-
2013-by-the-numbers-a-few-amazing-twitter-stats/.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 327?335.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting elections with Twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, pages
178?185.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical re-
port, Human Language Technology Center of Excel-
lence.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 48?58.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 61?72.
Tao Yano and Noah A. Smith. 2013. What?s worthy
of comment? content and comment volume in po-
litical blogs. In International AAAI Conference on
Weblogs and Social Media (ICWSM).
Tao Yano, Dani Yogatama, and Noah A. Smith. 2013.
A penny for your tweets: Campaign contributions
and capitol hill microblogs. In Proceedings of the
International AAAI Conference on Weblogs and So-
cial Media (ICWSM).
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media, pages 387?390.
196
