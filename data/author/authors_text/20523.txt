Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 511?522,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Comparison of Selectional Preference Models for Automatic Verb
Classification
Will Roberts and Markus Egg
Institut f?r Anglistik und Amerikanistik, Humboldt University
10099 Berlin, Germany
{will.roberts,markus.egg}@anglistik.hu-berlin.de
Abstract
We present a comparison of different selec-
tional preference models and evaluate them
on an automatic verb classification task in
German. We find that all the models we
compare are effective for verb clustering;
the best-performing model uses syntactic
information to induce nouns classes from
unlabelled data in an unsupervised man-
ner. A very simple model based on lexical
preferences is also found to perform well.
1 Introduction
Selectional preferences (Katz and Fodor, 1963;
Wilks, 1975; Resnik, 1993) are the tendency for
a word to semantically select or constrain which
other words may appear in a direct syntactic re-
lation with it. Selectional preferences (SPs) have
been a perennial knowledge source for NLP tasks
such as word sense disambiguation (Resnik, 1997;
Stevenson and Wilks, 2001; McCarthy and Car-
roll, 2003) and semantic role labelling (Erk, 2007);
and recognising selectional violations is thought
to play a role in identifying and interpreting meta-
phor (Wilks, 1978; Shutova et al., 2013). We focus
on the SPs of verbs, since determining which argu-
ments are typical of a given verb sheds light on the
semantics of that verb.
In this study, we present the first empirical com-
parison of different SP models from the perspective
of automatic verb classification (Schulte im Walde,
2009; Sun, 2012), the task of grouping verbs to-
gether based on shared syntactic and semantic prop-
erties.
We cluster German verbs using features captur-
ing their valency or subcategorisation, following
prior work (Schulte im Walde, 2000; Esteve Ferrer,
2004; Schulte im Walde, 2006; Sun et al., 2008;
Korhonen et al., 2008; Li and Brew, 2008), and
investigate the effect of adding information about
verb argument preferences. SPs are represented
by features capturing lexical information about the
heads of arguments to the verbs; we restrict our
focus here to nouns.
We operationalise a selectional preference model
as a function which maps such an argument head
to a concept label. We submit that the primary
characteristic of such a model is its granularity. In
our baseline condition, all nouns are mapped to the
same label; this effectively captures no information
about a verb?s SPs (i.e., we cluster verbs using sub-
categorisation information only). On the other ex-
treme, each noun is its own concept label; we term
this condition lexical preferences (LP). Between
the baseline and LP lie a spectrum of models, in
which multiple concepts are distinguished, and
each concept label can represent multiple nouns.
Our main hypothesis is that verb clustering will
work best using a model of such intermediate gran-
ularity. This follows the intuition that verbs would
seem to select for classes of nouns; for instance,
we suppose that essen ?eat? would tend to prefer as
a direct object a noun from the abstract concept Es-
sen (?food?). We assume that these concepts can be
expressed independently of particular predicates;
that is, there exist selectional preference models
that will work for all verbs (and all grammatical
relations). Further benefits of grouping nouns into
classes include combating data sparsity, as well
as deriving models which can generalise to nouns
unseen in training data.
Another parameter of a selectional preference
model is the methodology used to induce the con-
ceptual classes; put another way, the success of
an SP model hinges on how it represents concepts.
In this paper, we investigate the choice of noun
categorisation method through an empirical com-
parison of selectional preference models previously
used in the literature.
We set out to investigate the following questions:
1. What classes of nouns are effective descriptors
511
of selectional preference concepts? For ex-
ample, do they correspond to features such as
ANIMATE?
2. What is the appropriate granularity of selec-
tional preference concepts?
3. Which methods of classifying nouns into con-
cepts are most effective at capturing selec-
tional preferences for verb clustering?
This paper is structured as follows: In Section 2,
we introduce our baseline method of clustering
verbs using subcategorisation information and de-
scribe evaluation; Section 3 lists the models of se-
lectional preferences that we compare in this work;
Section 4 presents results and discussion; Section 5
summarises related work; and Section 6 concludes
with directions for future research.
2 Automatic verb classification
Verb classifications such as VerbNet (Kipper-
Schuler, 2005) allow generalisations about the syn-
tax and semantics of verbs and have proven useful
for a range of NLP tasks; however, creation of these
resources is expensive and time-consuming. Auto-
matic verb classification seeks to learn verb classes
automatically from corpus data in a cheaper and
faster way. This endeavour is possible due to the
link between a verb?s semantics and its syntactic be-
haviour (Levin, 1993). Recent research has found
that even automatically-acquired classifications can
be useful for NLP applications (Shutova et al., 2010;
Guo et al., 2011). In this section, we introduce the
verb classification method used by our baseline
model, which clusters verbs based on subcategor-
isation information. Following this, Section 2.2 ex-
plains the gold standard verb clustering and cluster
purity metric which we use for evaluation.
2.1 Baseline model
In this work, we take subcategorisation to mean
the requirement of a verb for particular types of
argument or concomitant. For example, the English
verb put subcategorises for subject, direct object,
and a prepositional phrase (PP) like on the shelf :
(1) [
NP
Al] put [
NP
the book] [
PP
on the shelf].
A subcategorisation frame (SCF) describes a
combination of arguments required by a specific
verb; a description of the set of SCFs which a verb
may take is called its subcategorisation preference.
We acquire descriptions of verbal SCF preferences
on the basis of unannotated corpus data.
Our experiments use the SdeWaC corpus (Faa?
and Eckart, 2013), containing 880 million words
in 45 million sentences; this is a subset of deWaC
(Baroni et al., 2009), a corpus of 10
9
words extrac-
ted from Web search results. SdeWaC is filtered
to include only those sentences which are max-
imally parsable
1
. We parsed SdeWaC with the
mate-tools dependency parser (Bohnet et al.,
2013)
2
, which performs joint POS and morpholo-
gical tagging, as well as lemmatisation. Our sub-
categorisation analyses are delivered by the rule-
based SCF tagger described by Roberts et al. (2014),
which operates using the dependency parses and as-
signs each finite verb an SCF type. The SCF tags are
taken from the SCF inventory proposed by Schulte
im Walde (2002), which indicates combinations
of nominal and verbal complement types, such as
nap:f?r.Acc (transitive verb, with a PP headed
by f?r ?for?). Examples of complements are n for
nominative subject, and a for accusative direct ob-
ject; in SCFs which include PPs (p), the SCF tag
specifies the head of the PP and the case of the pre-
positional argument (Acc in our example indicates
the accusative case of the prepositional argument).
The SCF tagger undoes passivisation and analyses
verbs embedded in modal and tense constructions.
We record 673 SCF types in SdeWaC.
From SdeWaC, we extracted the first 3,000,000
verb instances assigned an SCF tag by the SCF tag-
ger, where the verb lemma is one of the 168 listed
in our gold standard clustering (this requires ap-
proximately 270 million words of parsed text, or
25% of SdeWaC). We refer to this as our test set.
In this set, each verb is seen on average 17,857
times; the most common is geben (?give?, 328,952
instances), and the least is grinsen (?grin?, 50).
We represent verbs as vectors, where each di-
mension represents a different SCF type. Vector
entries are initialised with SCF code counts over
the test set, and each vector is then normalised to
sum to 1, so that a vector represents a discrete prob-
ability distribution over the SCF inventory. We use
the Jensen-Shannon divergence as a dissimilarity
measure between pairs of verb vectors. The Jensen-
Shannon divergence (Lin, 1991) is an information-
theoretic, symmetric measure (Equation (2)) re-
1
The filtering used a rule-based dependency parser to es-
timate a per-token parse error rate for each sentence, and
removed those sentences with very high error rates.
2
https://code.google.com/p/mate-tools/
512
lated to the Kullback-Leibler divergence (Equa-
tion (3)).
JS(p, q) = D(p||
p+ q
2
) +D(q||
p+ q
2
) (2)
D(p||q) =
?
i
p
i
log
p
i
q
i
(3)
With this dissimilarity measure, we use hier-
archical clustering with Ward?s criterion (Ward,
Jr, 1963) to partition the verbs into K disjoint sets
(i.e., hard clustering), where we match K to the
number of classes in our gold standard (described
below).
2.2 Evaluation paradigm
We evaluate the automatically induced verb cluster-
ings against a manually-constructed gold standard,
published by Schulte im Walde (2006, page 162ff.).
This Levin-style classification groups 168 high-
and low-frequency verbs into 43 semantic classes;
examples include Aspect (e.g., anfangen ?begin?),
Propositional Attitude (e.g., denken ?think?), and
Weather (e.g., regnen ?rain?). Some of the classes
are further sub-classified; for the purposes of our
evaluation, we ignore the hierarchical structure of
the classification and consider each class or sub-
class to be a separate entity. In this way, we obtain
classes of fairly comparable size and sufficient se-
mantic consistency.
3
We evaluate a given verb clustering against
the gold standard using the pairwise F -score
(Hatzivassiloglou and McKeown, 1993). To calcu-
late this statistic, we construct a contingency table
over the
(
n
2
)
pairs of verbs, the idea being that the
gold standard provides binary judgements about
whether two verbs should be clustered together or
not. If a clustering agrees with the gold standard as
to whether a pair of verbs belong together or not,
this is a ?correct? answer. Using the contingency
table, the standard information retrieval measures
of precision (P ) and recall (R) can be computed;
the F -score is then the harmonic mean of these:
F = 2PR/(P +R). The random baseline is 2.08
(calculated as the average score of 50 random parti-
tions), and the optimal score is 95.81, calculated by
evaluating the gold standard against itself. As the
gold standard includes polysemous verbs, which
3
In contrast, a top-level class like ?Transfer of Possession
(Obtaining)?, not only covers 25% of the gold standard, it also
comprises the semantically very diverse subclasses ?Transfer
of Possession (Giving)?, ?Manner of Motion?, and ?Emotion?.
belong to more than one cluster, the optimal score is
calculated by randomly picking one of their senses;
the average is then taken over 50 such trials.
The pairwise F -score is known to be somewhat
nonlinear (Schulte im Walde, 2006), penalising
early clustering ?mistakes? more than later ones,
but it has the advantage that we can easily determ-
ine statistical significance using the contingency
table and McNemar?s test.
We use only one clustering algorithm and one
purity metric, because our prior work shows that
the most important choices for verb clustering are
the distance measure used, and how verbs are rep-
resented. These factors set, we expect similar per-
formance trends from different algorithms, with
predictable variation (e.g., spectral tends to outper-
form hierarchical clustering, which in turn outper-
forms k-means). Combining Ward?s criterion and
F -score is a trade-off at this point; the criterion is
deterministic, giving reproducible results without
computational complexity, but disallows estimates
of density over our evaluation metric and is greedy
(see discussion in Section 4.3).
3 Selectional preference models
In this section, we introduce the various SP models
that we compare in this paper. In all cases, we
hold the verb clustering procedure described in the
previous section unchanged, with the exception
that SCF tags for verbs are parameterised for
selectional preferences. As an example, a verb
instance observed in a simple transitive frame with
a nominal subject and accusative object would
receive the SCF tag na. Assuming that a given SP
model places the subject noun in the SP concept
animate and the object noun in the concept
concrete, the parameterised SCF tag would be
na
*
subj-{animate}
*
obj-{concrete}.
This process captures argument co-occurrence
information about verb instances, and has the effect
of multiplying the SCF inventory size, making the
verb vectors described in Section 2.1 both longer
and sparser.
We evaluate various types of SP models: the
simple lexical preferences model; three models
which perform automatic unsupervised induction
of noun concepts from unlabelled data; and one
which uses a manually-built lexical resource. As
far as we are aware, two of these, the word space
and LDA models, have never been applied to verb
classification before.
513
N Coverage of test set
100 12.08%
200 17.18%
500 26.11%
1,000 32.70%
5,000 45.31%
10,000 49.09%
50,000 55.69%
100,000 57.67%
Table 1: Fraction of verb instances in the test set
parameterised by LP as a function of the number of
nouns N included in the LP model.
3.1 Lexical preferences
The LP model is the simplest in our study after the
baseline condition; it simply maps a noun to its own
lemma. We include as a parameter of the LP model
a maximum number of nouns N to admit as LP
tags. In this way, the LP model parameterises SCFs
using only the N most frequent nouns in SdeWaC;
nouns beyond rank N are treated as if they were
unseen. Table 1 indicates what fraction of the 3
million verb instances receive SCF tags specifying
one or more LPs as a function of this parameter.
Note that the coverage approaches an asymptote
of around 60%. This is due to the fact that noun
arguments are not observed for every verb instance;
many verbs? arguments are pronominal or verbal
and are not treated by our SP models. Setting N
allows a simple way of tuning the LP model: With
increasing N , the LP model should capture more
data about verb instances, but after a point this
benefit should be cancelled out by the increasing
sparsity in the verb vectors.
3.2 Sun and Korhonen model
The SP model described in this section (SUN) was
first used by Sun and Korhonen (2009) to de-
liver state-of-the-art verb classification perform-
ance for English; more recently, the technique was
applied to successfully identify metaphor in free
text (Shutova et al., 2010; Shutova et al., 2013).
It uses co-occurrence counts that describe which
nouns are found with which verbs in which gram-
matical relations; this information is used to sort the
nouns into classes in a procedure almost identical
to our verb clustering method described in Sec-
tion 2.1.
We extract all verb instances in SdeWaC which
are analysed by the SCF tagger, and count all
(verb, grammatical relation, nominal argument
head) triples, where the grammatical relation is
subject, direct (accusative) object, indirect (dative)
object, or prepositional object
4
, and is listed in the
verb instance?s SCF tag; we undo passivisation, re-
move instances of auxiliary and modal verbs, and
filter out those triples seen less than 10 times in the
corpus.
These observations cover 60,870 noun types and
33,748,390 tokens, co-occurring with 6,705 verb
types (11,426 verb-grammatical-relation types); an
example is (sprechen, obj, Wort) (?speak? with dir-
ect object ?word?, occurring 1,585 times)
5
. We rep-
resent each noun by a vector whose 11,426 dimen-
sions are the different verb-grammatical-relation
pairs; coordinates in the vector indicate the ob-
served corpus counts. The vectors are then norm-
alised to sum to 1, such that each represents some
particular noun?s discrete probability distribution
over the set of verb-grammatical-relation pairs. The
distance between two noun vectors is defined to
be the Jensen-Shannon divergence between their
probability distributions, and we partition the set
of nouns into M groups using hierarchical Ward?s
clustering.
The SP model then maps a noun to an arbitrary
label indicating which of the M disjoint sets that
noun is to be found in (i.e., all nouns in the first
noun class map to the concept label concept1);
we employ the parameter M to model SP concept
granularity. As with the LP model, we use the
parameter N to indicate how many nouns are in-
cluded in the SUN model; we search the parameter
values N = {300, 500, 1000, 5000, 10000} and
N
M
= {5, 10, 15, 20, 30, 50}.
3.3 Word space model
Word space models (WSMs, (Sahlgren, 2006;
Turney and Pantel, 2010)) use word co-occurrence
counts to represent the distributional semantics of a
word. This strategy makes possible a clustering of
nouns that does not depend on verbal dependencies
in the first place.
4
We have also experimented with adding features for each
noun showing nominal modification features (e.g., (schwarz,
nmod, Haar), ?hair? modified by ?black?), but these seem to
hurt performance.
5
Triples representing prepositional object relations are dis-
tinguished by preposition (e.g., the triple (geben, prep-in,
Auftrag), ?give? with PP headed by ?in? with argument head
?contract?, an idiomatic expression meaning ?to commission?
something).
514
Dagan et al. (1999) address the problem of data
sparseness for the automatic determination of word
co-occurrence probabilities, which includes selec-
tional preferences. They introduce the idea of es-
timating the probability of hitherto unseen word
combinations using available information on words
that are closest w.r.t. distributional word similar-
ity. Following this idea, Erk (2007) and Pad? et al.
(2007) describe a memory-based SP model, using a
WSM similarity measure to generalise the model to
unseen data.
We build a WSM of German nouns and use it to
partition nouns into disjoint sets, which we then
employ as with the SUN model. We compute word
co-occurrence counts across the whole SdeWaC
corpus, using as features the 50,000 most common
words in SdeWaC, skipping the first 50 most com-
mon words (i.e., we use words 50 through 50,050),
with sentences as windows. We lemmatise the cor-
pus and remove all punctuation; no other normalisa-
tion is performed. Co-occurrence counts between
a word w
i
and a feature c
j
are weighted using the
t-test scheme:
ttest(w
i
, c
j
) =
p(w
i
, c
j
)? p(w
i
)p(c
j
)
?
p(w
i
)p(c
j
)
We use a recent technique called context selec-
tion (Polajnar and Clark, 2014) to improve the word
space model, whereby only the C most highly
weighted features are kept for each word vector.
We set C by optimising the correlation between the
word space model?s cosine similarity and a data
set of human semantic relatedness judgements for
65 word pairs (Gurevych and Niederlich, 2005); at
C = 380, we obtain Spearman ? = 0.813 and Pear-
son r = 0.707 (human inter-annotator agreement
for this data set is given as r = 0.810).
After this, we build a similarity matrix between
all pairs of nouns using the cosine similarity, and
then partition the set of N nouns into M disjoint
classes using spectral clustering with the MNCut
algorithm (Meil
?
a and Shi, 2001). As with the SUN
model, this SP model assigns labels to nouns indic-
ating which noun class they belong to. We search
the same parameter space for N and M as for the
SUN model.
3.4 GermaNet
Statistical models of SPs have often used WordNet
as a convenient and well-motivated inventory of
concepts (e.g., Resnik (1997), Li and Abe (1998),
Clark and Weir (2002)). Typically, such models
make use of probabilistic treatments to determine
an appropriate concept granularity separately for
each predicate; we opt here for a simple model that
allows more direct control over concept granularity.
We take the set of concepts relevant to describing
selectional preferences to be a target set of synsets
in GermaNet (Hamp and Feldweg, 1997), and rep-
resent the target set as the set of synsets which are
at some depth d or less in the GermaNet noun hier-
archy: {s | depth(s) ? d} where depth(s) counts
the number of hypernym links separating s from
the root of the hierarchy. We model concept gran-
ularity by varying d = 1 . . . 6; at d = 1, the target
set is of size 5, and at d = 6, it is of size 17,125.
Nouns are attributed to concepts as follows: Given
a noun belonging to a synset s, either s is in the
target set, or we take s?s lowest hypernym in the
target set. For polysemous nouns, each synset list-
ing a sense of the noun votes for a member of the
target set; the noun observation is then spread over
the target set using the votes as weights.
This procedure makes our GermaNet SP model a
soft clustering over nouns (i.e., a noun can belong
to more than one SP concept); a consequence of
this is that a single verb occurrence in the corpus
can contribute fractional counts to multiple SCF
types.
3.5 LDA
Latent Dirichlet allocation (Blei et al., 2003) is a
generative model that discovers similarities in data
using latent variables; it is frequently used for topic
modelling. LDA models of SPs have been proposed
by ? S?aghdha (2010) and Ritter et al. (2010);
previous to this, Rooth et al. (1999) also described
a latent variable model of SPs.
We implement the LDA model of selectional pref-
erences described by ? S?aghdha (2010). Gener-
atively, the model produces nominal arguments to
verbs as follows: For a given (verb, grammatical re-
lation) pair (v, r), (1) Sample a noun class z from a
from a multinomial distribution ?
v,r
with a Dirich-
let prior parameterised by ?; (2) Sample a noun n
from a multinomial distribution ?
z
with a Dirichlet
prior parameterised by ?. Like ? S?aghdha, we use
an asymmetric Dirichlet prior for ?
v,r
(i.e., ? can
differ for each noun class) and a symmetric prior
for ?
z
(? is the same for each ?
z
). We estimate
the LDA model using the MALLET software (Mc-
Callum, 2002) using the same (verb, grammatical
515
relation, argument head) co-occurrence statistics
used for the SUN model. We train for 1,000 it-
erations using the software?s default parameters,
allowing the LDA hyperparameters ? and ? to be
re-estimated every 10 iterations. We build mod-
els with 50 or 100 topics as a proxy to concept
granularity; models include number of nouns N of
{500, 1000, 5000, 10000, 50000, 100000}.
As with the GermaNet-based model, the LDA
model creates a soft clustering of nouns; the abil-
ity of a noun to have degrees of membership in
multiple concepts might be a good way to model
polysemy. We also experiment with a hard cluster-
ing version of the LDA model; to do this, we assign
each noun n its most likely class label z using the
model?s estimate for P (z|n).
4 Results
We experimented with applying the SP models to
different combinations of grammatical relations
(e.g., only subject, only object, subject+object,
etc.), but generally obtained better results by para-
meterising SCF tags for all grammatical relations.
Table 2 summarises the evaluation scores and para-
meter settings for the best-performing SP models,
applied to verb arguments in all four grammatical
relations (subject, direct, indirect and prepositional
object)
6
. The table also indicates the number of
SCF types constructed by each SP model (i.e., the
number of dimensions of the vectors representing
verbs).
All the SP models we compare help with auto-
matic verb clustering. Using McNemar?s test on
the contingency tables underlying the F -scores, all
models score better than the baseline at at least the
p < 0.01 level. LDA-hard is better than the Ger-
maNet, LDA-soft, WSM and LP models at at least
the p < 0.05 level; SUN is better (p ? 0.05) than
all models except LDA-hard. All other performance
differences are not statistically significant
7
.
We can also demonstrate the effectiveness of the
SP models with a regression analysis on the models?
coverage of the test set. By varying the number of
nouns N included in the SP models which use this
parameter (LP, SUN, WSM, LDA), or by paramet-
erising SCF tags with SP information only for par-
6
Due to space constraints, we do not present here a de-
tailed per-model study of performance as a function of para-
meter settings; we feel a summary to be adequate, since the
relative performances of the models reflect trends across a
range of parameter settings.
7
Using a significance criterion of p < 0.05.
ticular combinations of grammatical relations, dif-
ferent numbers of the verb instances in the test data
will end up with SP information in their SCF tags
(this is the ?coverage? statistic in Table 1); with
the exception of the GermaNet model, all of the SP
models we examine here show positive correlation
between the number of verb instances tagged for
SP information and verb clustering performance.
This effect is independent of parameter settings,
indicating the performance benefit conferred by the
SP models is robust.
4.1 Comparison of SP models
The GermaNet model is the least successful in our
study. It achieves its best performance with a depth
of 5; after this, verb clustering performance drops
off again. Verb clustering using the GermaNet
SP model is only slightly better than the baseline
condition.
Against our expectations, the hard clustering
LDA models perform better than the soft cluster-
ing ones, achieving the second highest score in our
evaluation; also, in contrast to the other SP mod-
els studied in this paper, LDA performs best with
fewer, coarser-grained topics. We observe that the
soft clustering models produce verb vectors more
than an order of magnitude longer than the hard
clustering models, and suggest that simple soft clus-
tering may be causing problems with data sparsity
that interfere with verb clustering. We have also
observed that the topics found by LDA do not rep-
resent polysemy as we had hoped. While some
of the topics discovered by the LDA models can
be easily assigned labels (e.g., body parts, people,
quantities, emotions, places, buildings, tools, etc.),
others are less cohesive. We found that frequent
words (e.g., time, person) are generated with high
probability by multiple topics in ways that do not
appear to reflect multiple word senses, and that the
100-topic models exhibit this property to a greater
extent. For instance, Zeit ?time? is highly predict-
ive of three topics in the 50-topic models, of which
only the highest-weighted topic groups time ex-
pressions together; in the 100-topic models, Zeit
is found in six topics. Again, of these six, only
the topic with the highest ? consists of time expres-
sions. In the 50-topic models, we find 11 topics that
we cannot assign a coherent label; in the 100-topic
models, there are 38 of these mismatched topics.
In our work to date, we have not found that LDA
models with greater numbers of topics find more
516
SP model Parameters Granularity F -score Number of SCF types
SUN 10,000 nouns 1,000 noun classes 39.76 248,665
LDA (hard) 10,000 nouns 50 topics 39.10 78,409
LP 5,000 nouns 38.02 388,691
WSM 10,000 nouns 500 noun classes 36.95 149,797
LDA (soft) 10,000 nouns 50 topics 35.91 1,524,338
GermaNet depth = 5 8,196 synsets 34.41 851,265
Baseline 33.47 673
Table 2: Evaluation of the best SP models.
10
0
10
1
10
2
10
3
10
4
10
5
N
31
32
33
34
35
36
37
38
P
a
i
r
F
0.0
0.1
0.2
0.3
0.4
0.5
0.6
C
o
v
e
r
a
g
e
Figure 1: Verb clustering performance (black) and
test set coverage (grey) of the LP model as a func-
tion of the number of nouns N included in the
model.
specific concepts; it is possible that this problem
might be alleviated by careful filtering of the (verb,
grammatical relation, noun) triples, but we leave
this question to future research.
The LP model is very effective, which is surpris-
ing given its simplicity. As expected, with increas-
ing N , we do observe sparsity effects which hurt
verb clustering performance (see Figure 1).
Our best performing model is SUN. Our best res-
ult is obtained with 10,000 nouns (the maximum
value of N that we tried) in 1,000 classes, giving
relatively fine-grained classes (on average 10 nouns
per class). Table 3 shows some example noun
classes learned by the SUN model. These include:
groups with synonyms or near synonyms, often in-
cluding alternate spellings of the same word (such
as in the truck grouping); and groups of closely-
related co-hyponyms, such as the body part group-
ing and the clothing grouping. In the latter, bill,
joint responsibility, complicity and inscription are
also included as things which can be borne, this
is due to the fact that the SUN noun clustering is
based on triples of verbs, grammatical relations,
and nouns.
LKW (truck), Lkw (truck), Lastwagen (truck),
Castor (container for highly radioactive mater-
ial), Laster (truck), Krankenwagen (ambulance),
Transporter (van), Traktor (tractor)
Hand (hand), Kopf (head), Fu? (foot), Haar
(hair), Bein (leg), Arm (arm), Zahn (tooth), Fell
(fur)
Leiche (corpse), Leichnam (body), Sch?del
(skull), Skelett (skeleton), Wrack (wreck), Mu-
mie (mummy), Tr?mmer (debris)
Sauna (sauna), Badezimmer (bathroom),
Schwimmbad (swimming pool), Nachbildung
(replica), Kamin (fireplace), Aufenthaltsraum
(common room), Mensa (cafeteria)
Rechnung (bill), Kopftuch (headscarf), Uniform
(uniform), Anzug (suit), Helm (helmet), Gewand
(garment), Handschuh (glove), Mitverantwor-
tung (joint responsibility), Bart (beard), R?s-
tung (armour), Mitschuld (complicity), Socke
(sock), Jeans (jeans), Sonnenbrille (sunglasses),
Aufschrift (inscription), Pullover (sweater),
Weste (vest), Handschellen (handcuffs), H?rner
(horns), Kennzeichen (marking), Tracht (tradi-
tional costume), Korsett (corset), Schuhwerk
(footwear), Kopfbedeckung (headgear), Pelz
(fur), Maulkorb (muzzle)
Missionar (missionary), Weihnachtsmann
(Santa Claus), Selbstmordattent?ter (sui-
cide bomber), Bote (messenger), Nikolaus
(Nicholas), Killer (killer), Bomber (bomber),
Osterhase (Easter bunny)
Table 3: Example noun clusters in the SUN SP
model.
517
Furthermore, there are thematically related
groups (corpse, body, etc., and sauna, bathroom,
etc.). All months are placed together in one 12-
word group.
Some classes can be easily subdivided into sep-
arate groups, and sometimes the source for this can
be guessed: For example, sports (football, golf, ten-
nis) are lumped together with musical instruments
(guitar, piano, violin) and film roles (starring role,
supporting role), these all being things that can
be played. Many groups of personal roles (such
as various kinds of government ministers) are dis-
tinguished, as are diseases and medications; other
groupings contain proper names or geographical
locations, sometimes of surprising specificity (e.g.,
authors, Biblical names, philosophers, NGOs, East-
ern European countries, foreign currencies, Ger-
man male first names, newspapers, television chan-
nels). The last group in Table 3 shows a grouping
which appears to combine two of these semantic-
ally narrow categories, in which Santa Claus and
the Easter bunny are united with killers and suicide
bombers.
4.2 Noun classes as SP concepts
The WSM SP model is not as successful as SUN, but,
due to the methodological similarity between these
two (SP concepts modelled as hard partitions of
nouns), it affords us an opportunity to investigate
the question of what properties might make for an
effective noun partition.
The WSM model partitions nouns based on
paradigmatic information (which sentence con-
texts a noun appears in), rather than SUN?s use
of syntagmatic information (which grammatical
contexts a noun appears in). Therefore, it is per-
haps not surprising that the noun classes derived
by the WSM are organised thematically, and the
synonym/co-hyponym structure observed in the
SUN noun classes is in many cases absent (e.g.,
{Pferd (horse), Reiter (rider), Stall (stable), Sattel
(saddle), Stute (mare)}; these classes can easily
conflate semantic roles (e.g., Agent for rider and
Location for stable), which is presumably unhelp-
ful for representing selectional preferences.
The distribution of noun classes also differs
between SUN and WSM. The largest noun class
in the WSM model contains 1,076 high-frequency
nouns which are semantically unrelated (day, ques-
tion, case, part, reason, kind, form, week, person,
month, . . . ). We suppose that these nouns are them-
10
5
10
6
10
7
10
8
Number of verb instances
15
20
25
30
35
40
45
P
a
i
r
F
Baseline
LP
WSM
SUN
LDA-hard
Figure 2: Verb clustering performance of SP mod-
els as a function of number of verb instances.
atically ?neutral? and are classed together by virtue
of their usage in a wide variety of sentences. This
one noun class by itself subsumes 13.6% of all
noun tokens in SdeWaC. WSM also includes 56
singleton noun classes; the variance in noun class
size is 2800. For comparison, in SUN, the largest
noun class has 73 words, and the smallest, 2 (there
are 12 of these two-word classes); noun class size
variance is 37. The 73-word class in SUN does in-
deed appear to be a grab bag (including gas, taboo,
pioneer, mustard, spy, mafia, and skinhead), but
these are uncommon words and account for only
0.1% of noun tokens in SdeWaC. The next two
most common classes (with some 40 nouns each)
are lists of names (politicians? surnames, and male
first names). The noun class in the SUN model con-
taining the largest number of high-frequency nouns
(28 nouns: human, child, woman, man, people, Mr.,
mother, father, . . . ) only covers 3.6% of noun us-
ages in SdeWaC and is both semantically cohesive
and intuitively useful as a SP concept.
These issues raise the question of why the WSM
model is effective at all for verb classification.
We think that the larger less-related noun classes
neither help nor hurt verb clustering, and we find
that some of the thematic classes represent abstrac-
tions that should be useful for describing SPs. Ex-
amples include lists of body parts, countries (separ-
ate classes for Europe, Africa, Asia, etc.), diseases,
human names, articles of clothing, and the group
{fruit, apple, banana, pear, strawberry}.
4.3 Effects of test set size
We were curious if the success of the LP model
might be due to the size of the test set preventing
518
sparsity from becoming a problem. To pursue this
question, we take the four best performing SP mod-
els and run the verb clustering evaluation with the
number of verb instances in the test set varying
between 10,000 and the full SdeWaC corpus (11
million). The results are displayed in Figure 2. This
graph indicates that below 3? 10
5
verb instances,
sparsity seems to become a problem for all mod-
els on this task, and the baseline delivers the best
performance. Above this threshold, it seems that
sparsity is not a major issue: LP performs fairly con-
sistently, and is competitive with the SUN model.
We attribute this to our use of the Jensen-Shannon
divergence as a verb dissimilarity measure, which
seems relatively robust to data sparsity. The LDA-
hard model with its fewer topics seems to do quite
well with fewer data; as the test set size increases, it
drops off in the rankings. At the maximum number
of verb instances, the best-performing models are
SUN, WSM and the lexical preferences. The figure
also shows that our evaluation metric is not smooth
(note, e.g., the fluctuations in the baseline score).
We believe that this reflects a degree of instability
in the Ward?s hierarchical clustering algorithm; this
clustering method is greedy, and clustering errors
can be expected to propagate, which might explain
the jaggedness of the plot.
4.4 Conclusions
To conclude, we summarise the results of our ana-
lysis, using the questions formulated in the Intro-
duction as guidelines.
First, we wanted to compare the efficiency of
different classes of nouns as descriptors of selec-
tional preference concepts. Our findings suggest
that noun classes are most effective when they are
semantically highly consistent, representing groups
of strongly related nouns. It seems reasonable that
SP concepts representing collections of synonyms
would be useful for generalising observations, and
should represent arguments better than simple LP.
A classification of proper names (e.g., as human,
corporation, country, medication) is also useful.
This implies that we can expect features such as
ANIMATE to be shared by all members of a noun
cluster.
Second, we were interested in the appropriate
granularity of selectional preference concepts. In
our evaluation, we have observed a tendency for
smaller, more specific noun classes to be superior;
this holds because data sparsity is not a problem
in our experiment. Beyond this finding, we would
have liked to present a direct juxtaposition of differ-
ent models on ?granularity? but this is difficult: We
have not yet identified a strong abstraction of gran-
ularity from the proxies we use (e.g., GermaNet
depth, or SUN?s N/M ).
Finally, which methods of classifying nouns into
concepts are most effective at capturing selectional
preferences for verb clustering? In our experiments,
the SUN and LDA-hard models proved to be more
effective than lexical preferences, supporting our
primary hypothesis that some level of SP concept
granularity above the lexical level is desirable for
verb clustering. On the other hand, the LP model is
only slightly worse than SUN and LDA-hard, mak-
ing it attractive because it is so simple. As we have
shown, the potential data sparsity issues with LP
can be alleviated by judiciously choosing the value
of the N parameter that controls the number of
nouns included in the model. In addition, compar-
ing the SUN and WSM models, and observing the
performance of the LDA-hard method, we conclude
that inducing noun classes using syntagmatic in-
formation is more effective than using paradigmatic
relations.
5 Related work
In this study, we have looked at the utility of selec-
tional preferences for automatic verb classification.
Some previous research has followed this line of
inquiry, though prior studies have not compared
alternative methods of modelling SPs. Schulte im
Walde (2006) presented a detailed examination of
parameters for k-means-based verb clustering in
German, using the same gold standard that we em-
ploy here. She reports on the effects of adding SP
information to a SCF-based verb clustering using
15 high-level GermaNet synsets as SP concepts; SP
information for some combinations of grammatical
relations improves clustering performance slightly,
but neither are the effects consistent, nor is the
improvement delivered by the SP model over the
SCF-based baseline statistically significant. Schulte
im Walde et al. (2008) used expectation maximisa-
tion to induce latent verb clusters from the British
National Corpus while simultaneously building a
tree cut model of SPs on the WordNet hierarchy
using a minimum description length method; their
evaluation focuses on the induced soft verb clusters,
reporting the model?s estimated perplexity of (verb,
grammatical relation, argument head) triples. The
519
SPs are described qualitatively by presenting two
example cases. Sun and Korhonen (2009) study
the effect of adding selectional preferences to a
subcategorisation-based verb clustering in Eng-
lish using the SUN model (see Section 3.2). They
demonstrate that adding SPs to the SCF preference
data leads to the best results on their two clustering
evaluations; overall, their best results come from
using SP information only for the subject gram-
matical relation. They employ coarse SP concepts
(20 or 30 noun clusters) which capture general se-
mantic categories (Human, Building, Idea, etc.).
Selectional preferences are usually evaluated
either from a word sense disambiguation stand-
point using pseudo-words (Chambers and Juraf-
sky, 2010), or in terms of how acceptable an ar-
gument is with a verb, via regression against hu-
man plausibility judgements. Several studies have
compared SP methodologies from the latter per-
spective. These include Brockmann and Lapata
(2003), who compared three GermaNet-based mod-
els of SP, showing that different models were most
effective for describing different grammatical re-
lations; ? S?aghdha (2010), who compared dif-
ferent LDA-based models of SP, showing these to
be effective for a variety of grammatical relations;
and ? S?aghdha and Korhonen (2012), who show
that WordNet tree cut models, LDA, and a hybrid
LDA-WordNet model are effective for describing
verb-object relations.
6 Future work
Our GermaNet model delivered disappointing per-
formance in this study; we would be interested in
seeing whether a more sophisticated implementa-
tion such as the tree cut model of Li and Abe (1998)
would be more competitive. We also would like to
explore alternative noun clustering methods such
as CBC (Pantel and Lin, 2002) and Brown clusters
(Brown et al., 1992), which were not covered in
this work; these would fit easily into our SP eval-
uation paradigm. More challenging would be a
verb classification-based evaluation of the SP mod-
els of (Rooth et al., 1999) and (Schulte im Walde
et al., 2008), which use expectation maximisation
to simultaneously cluster verbs into verb classes
and nominal arguments into noun classes; these ap-
proaches are not compatible with the evaluation
framework we have used here. Finally, the SP
model of Bergsma et al. (2008) has also achieved
impressive results on a number of tasks, but has not
been investigated for use in verb classification.
Our verb clustering evaluation in this work
has matched K, the number of clusters found by
Ward?s method, to the number of classes in the
gold standard. Since the number of clusters has an
influence on the quality of the ensuing semantic
classification (Schulte im Walde, 2006, page 180f.),
we will also be running our experiments with dif-
ferent settings of K to explore whether this also
influences the overall results of our evaluation.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed Web-
crawled corpora. Language Resources and Evalu-
ation, 43(3):209?226.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative learning of selectional preference
from unlabeled text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 59?68.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Rich?rd Farkas, Filip Ginter, and Jan Haji?c. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. Transactions of the Association
for Computational Linguistics, 1:415?428.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional pref-
erence acquisition. In Proceedings of the Tenth Con-
ference on European Chapter of the Association for
Computational Linguistics, pages 27?34.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
Nathanael Chambers and Daniel Jurafsky. 2010. Im-
proving the use of pseudo-words for evaluating se-
lectional preferences. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 445?453.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Ido Dagan, Lillian Lee, and Fernando C.N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1?3):43?
69.
520
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 216?223.
Eva Esteve Ferrer. 2004. Towards a semantic classi-
fication of Spanish verbs based on subcategorisation
information. In Proceedings of the Student Research
Workshop at the Annual Meeting of the Association
for Computational Linguistics, pages 37?42.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC - A
corpus of parsable sentences from the Web. In Lan-
guage processing and knowledge in the Web, pages
61?68. Springer, Berlin, Heidelberg.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumentat-
ive zoning of scientific documents. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 273?283.
Iryna Gurevych and Hendrik Niederlich. 2005. Com-
puting semantic relatedness in German with revised
information content metrics. In Proceedings of "On-
toLex 2005 - Ontologies and Lexical Resources"
IJCNLP?05 Workshop, pages 28?33.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet:
A lexical-semantic net for German. In Proceedings
of ACL Workshop on Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications, pages 9?15.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of ad-
jectival scales: Clustering adjectives according to
meaning. In Proceedings of the 31st Annual Meet-
ing on Association for Computational Linguistics,
pages 172?182.
Jerrold J. Katz and Jerry A. Fodor. 1963. The structure
of a semantic theory. Language, 39:170?210.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Anna Korhonen, Yuval Krymolowski, and Nigel Col-
lier. 2008. The choice of features for classifica-
tion of verbs in biomedical texts. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 449?456.
Beth Levin. 1993. English verb classes and altern-
ations: A preliminary investigation. University of
Chicago Press, Chicago.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217?244.
Jianguo Li and Chris Brew. 2008. Which are the best
features for automatic verb classification. In Pro-
ceedings of ACL-08: HLT, pages 434?442.
Jianhua Lin. 1991. Divergence measures based on the
Shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(1):145?151.
Andrew McCallum. 2002. MALLET: A machine
learning for language toolkit.
Diana McCarthy and John Carroll. 2003. Disambig-
uating nouns, verbs, and adjectives using automat-
ically acquired selectional preferences. Computa-
tional Linguistics, 29(4):639?654.
Marina Meil?a and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings of the
International Conference on Artificial Intelligence
and Statistics (AISTATS).
Diarmuid ? S?aghdha and Anna Korhonen. 2012.
Modelling selectional preferences in a lexical hier-
archy. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics, pages 170?
179.
Diarmuid ? S?aghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 435?444.
Sebastian Pad?, Ulrike Pad?, and Katrin Erk. 2007.
Flexible, corpus-based modelling of human plausib-
ility judgements. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 400?409.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the Eighth
ACM SIGKDD International Conference on Know-
ledge Discovery and Data Mining, pages 613?619.
Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230?
238.
Philip Resnik. 1993. Selection and information: A
class-based approach to lexical relationships. Ph.D.
thesis, University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A lat-
ent Dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 424?434.
Will Roberts, Markus Egg, and Valia Kordoni. 2014.
Subcategorisation acquisition from raw text for a
free word-order language. In Proceedings of the
521
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 298?
307.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantic-
ally annotated lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 104?111.
Magnus Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible, and Helmut Schmid. 2008. Combining
EM training and the MDL principle for an automatic
verb classification incorporating selectional prefer-
ences. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics,
pages 496?504.
Sabine Schulte im Walde. 2000. Clustering verbs se-
mantically according to their alternation behaviour.
In Proceedings of the 18th Conference on Computa-
tional Linguistics, pages 747?753.
Sabine Schulte im Walde. 2002. A subcategorisation
lexicon for German verbs induced from a lexicalised
PCFG. In Proceedings of the 3rd Conference on
Language Resources and Evaluation (LREC), pages
1351?1357.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Sabine Schulte im Walde. 2009. The induction of
verb frames and verb classes from corpora. In
Anke L?deling and Merja Kyt?, editors, Corpus
linguistics: An international handbook, volume 2,
chapter 44, pages 952?971. Mouton de Gruyter, Ber-
lin.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna
Korhonen. 2013. Statistical metaphor processing.
Computational Linguistics, 39(2):301?353.
Mark Stevenson and Yorick Wilks. 2001. The interac-
tion of knowledge sources in word sense disambigu-
ation. Computational Linguistics, 27(3):321?349.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 638?647.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In Proceedings of the Ninth International Confer-
ence on Intelligent Text Processing and Computa-
tional Linguistics, pages 16?27.
Lin Sun. 2012. Automatic induction of verb classes
using clustering. Ph.D. thesis, University of Cam-
bridge, Cambridge.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Joe H. Ward, Jr. 1963. Hierarchical grouping to optim-
ize an objective function. Journal of the American
Statistical Association, 58(301):236?244.
Yorick Wilks. 1975. An intelligent analyzer and un-
derstander of English. Communications of the ACM,
18(5):264?274.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
522
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 298?307,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Subcategorisation Acquisition from Raw Text for a Free Word-Order
Language
Will Roberts and Markus Egg and Valia Kordoni
Institute f?ur Anglistik und Amerikanistik, Humboldt University
10099 Berlin, Germany
{will.roberts,markus.egg,evangelia.kordoni}@anglistik.hu-berlin.de
Abstract
We describe a state-of-the-art automatic
system that can acquire subcategorisation
frames from raw text for a free word-order
language. We use it to construct a subcate-
gorisation lexicon of German verbs from a
large Web page corpus. With an automatic
verb classification paradigm we evaluate
our subcategorisation lexicon against a pre-
vious classification of German verbs; the
lexicon produced by our system performs
better than the best previous results.
1 Introduction
We introduce a state-of-the-art system for the ac-
quisition of subcategorisation frames (SCFs) from
large corpora, which can deal with languages with
very free word order. The concrete language we
treat is German; its word order variability is illus-
trated in (1)?(4), all of which express the sentence
The man gave the old dog a chop:
(1) Dem alten Hund gab der Mann ein Schnitzel.
(2) Ein Schnitzel gab dem alten Hund der Mann.
(3) Ein Schnitzel gab der Mann dem alten Hund.
(4) Der Mann gab dem alten Hund ein Schnitzel.
On the basis of raw text, the system can be
used to build extensive SCF lexicons for German
verbs. Subcategorisation means that lexical items
require specific obligatory concomitants or argu-
ments; we focus on verb subcategorisation. E.g.,
the verb geben ?give? requires three arguments, the
nominative subject der Mann ?the man?, the dative
indirect object dem alten Hund ?the old dog?, and
the accusative direct object ein Schnitzel ?a chop?.
Other syntactic items may be subcategorised for,
too, e.g. both stellen and its English translation
put subcategorise for subject, direct object, and a
prepositional phrase (PP) like on the shelf :
(5) [
NP
Al] put [
NP
the book] [
PP
on the shelf].
Subcategorisation frames describe a combina-
tion of arguments required by a specific verb. The
set of SCFs for a verb is called its subcategori-
sation preference. Our system follows much pre-
vious work by counting PPs that accompany the
verb among its complements, even though they are
not obligatory (so-called ?adjuncts?), because PP
adjuncts are excellent clues to a verb?s semantics
(Sun et al., 2008). However, nominal and clausal
adjuncts do not count as verbal complements.
SCF information can benefit all applications
that need information on predicate-argument struc-
ture, e.g., parsing, verb clustering, semantic role la-
belling, or machine translation. Automatic acquisi-
tion of SCF information with minimal supervision
is also crucial to construct useful resources quickly.
The main innovation of the presented new sys-
tem is to address two challenges simultaneously,
viz., SCF acquisition from raw text and the focus
on languages with a very free word order. With
this system, we create an SCF lexicon for German
verbs and evaluate this lexicon against a previously
published manual verb classification, showing bet-
ter performance than has been reported until now.
After an overview of previous work on SCF ac-
quisition in Section 2, Section 3 describes our sub-
categorisation acquisition system, and Section 4
the SCF lexicon that we build using it. In Sec-
tions 5 and 6 we evaluate the SCF lexicon on a verb
classification task and discuss our results; Section 7
then concludes with directions for future work.
298
2 Previous work
To date, research on SCF acquisition from corpora
has mostly targeted English. Brent and Berwick
(1991) detect five SCFs by looking for attested
contexts where argument slots are filled by closed-
class lexical items (pronouns or proper names).
Briscoe and Carroll (1997) detect 163 SCFs with
a system that builds an SCF lexicon whose en-
tries include the relative frequency of SCF classes.
Potential SCF patterns are extracted from a cor-
pus parsed with a dependency-based parser, and
then filtered by hypothesis testing on binomial fre-
quency data. Korhonen (2002) refines Briscoe and
Carroll (1997)?s system using back-off estimates
on the WordNet semantic class of the verb?s pre-
dominant sense, assuming that semantically similar
verbs have similar SCFs, following Levin (1993).
Some current statistical methods for Semantic Role
Labelling build models that also capture subcat-
egorisation information, e.g., Grenager and Man-
ning (2006). Schulte im Walde (2009) offers a re-
cent survey of the SCF acquisition literature.
SCF acquisition is also an important step in the
automatic semantic role labelling (Grenager and
Manning, 2006; Lang and Lapata, 2010; Titov and
Klementiev, 2012). Semantic roles of a verb de-
scribe the kind of involvement of entities in the
event introduced by the verb, e.g., as agent (active,
often not affected by the event) or patient (passive,
often affected). On the basis of these SCFs, se-
mantic roles can be assigned due to the interdepen-
dence between semantic roles and their syntactic
realisations, called Argument Linking (Levin, 1993;
Levin and Rappaport Hovav, 2005).
Acquiring SCFs for languages with a very fixed
word order like English needs only a simple syn-
tactic analysis, which mainly relies on the prede-
termined sequencing of arguments in the sentence,
e.g., Grenager and Manning (2006). When word
order is freer, the analysis gets more complicated,
and must include a full syntactic parse.
What is more, German is a counterexample to
Manning?s (1993) expectation that freedom of
word order should be matched by an increase in
case and/or agreement marking. This is due to a
very high degree of syncretism (identity of word
forms) in German paradigms for nouns, adjectives,
and determiners. E.g., the noun Auto ?car? has only
two forms, Auto for nominative, dative, and ac-
cusative singular, and Autos for genitive singular
and all four plural forms. This is in contrast to some
other free word order languages for which SCF
acquisition has been studied, like Modern Greek
(Maragoudakis et al., 2000) and Czech (Sarkar and
Zeman, 2000). A one-many relation between word
forms and case is also one of the problems for SCF
acquisition in Urdu (Ghulam, 2011).
For German, initial studies used semi-automatic
techniques and manual evaluation (Eckle-Kohler,
1999; Wauschkuhn, 1999). The first automatic sub-
categorisation acquisition system for German is de-
scribed by Schulte im Walde (2002a), who defined
an SCF inventory and manually wrote a grammar
to analyse verb constructions according to these
frames. A lexicalised PCFG parser using this gram-
mar was trained on 18.7 million words of German
newspaper text; the trained parser model contained
explicit subcategorisation frequencies, which could
then be extracted to construct a subcategorisation
lexicon for 14,229 German verbs. This work was
evaluated against a German dictionary, the Duden
Stilw?orterbuch (Schulte im Walde, 2002b).
Schulte im Walde and Brew (2002) used the sub-
categorisation lexicon created by the system to au-
tomatically induce a set of semantic verb classes
with an unsupervised clustering algorithm. This
clustering was evaluated against a small manually
created semantic verb classification. Schulte im
Walde (2006) continues this work using a larger
manual verb classification. The SCFs used in this
study are defined at three levels of granularity. The
first level (38 different SCFs) lists only the comple-
ments in the frame; the second one adds head and
case information for PP complements (183 SCFs).
The third level examined the effect of adding selec-
tional preferences, but results were inconclusive.
A recent paper (Scheible et al., 2013) describes a
system similar to ours, built on a statistical depen-
dency parser, and using some of the same kinds
of rules as we describe in Section 3.1; this system
is evaluated in a task-based way (e.g., to improve
the performance of a SMT system) and cannot be
directly compared to our system in this paper.
3 The SCF acquisition system
This section describes the first contribution of this
paper, a state-of-the-art subcategorisation acquisi-
tion system for German. Its core component is a
rule-based SCF tagger which operates on phrase
structure analyses, as delivered by a statistical
parser. Given a parse of a sentence, the tagger as-
signs each finite verb in the sentence an SCF type.
299
We use the SCF inventory of Schulte im Walde
(2002a), which includes complements like n for
nominative subject, a for accusative direct object,
d for dative indirect object, r for reflexive pronoun,
and x for expletive es (?it?) subject. Clausal com-
plements can be infinite (i); finite ones can have
the verb in second position (S-2) or include the
complementiser dass ?that? (S-dass). Comple-
ments can be combined as in na (transitive verb);
for PPs in SCFs, the head is specified, e.g., p:f?ur
for PP complements headed by f?ur ?for?
1
.
Due to the free word order, simple phrase struc-
ture like that used for analysis of English is not
enough to specify the syntax of German sentences.
Therefore we use the annotation scheme in the
manually constructed German treebanks NEGRA
and TIGER (Skut et al., 1997; Brants et al., 2002),
which decorate parse trees with edge labels specify-
ing the syntactic roles of constituents. We automat-
ically annotate the parse trees from our statistical
parser using a simple machine learning model.
In the next section, we illustrate the operation of
the SCF tagger with reference to examples; then in
Section 3.2 we describe our edge labeller.
3.1 The SCF tagger
The SCF tagger begins by collecting complements
co-occurring with a verb instance using the phrase
structure of the sentence. In our system, we obtain
phrase structure information for unannotated text
using the Berkeley Parser (Petrov et al., 2006), a
statistical unlexicalised parser trained on TIGER.
Fig. 1 illustrates the phrase structure analysis and
edge labels in the TIGER corpus for (6):
(6) Das hielte ich f?ur moralisch au?erordentlich
fragw?urdig.
?I?d consider that morally extremely
questionable?.
Its finite verb hielte (from halten ?hold?) has
three complements, the subject ich ?I?, edge-
labelled with SB, the direct object das ?that?, la-
belled with OA, and a PP headed by f?ur ?for? (MO
stands for ?modifier?). After collecting comple-
ments, the SCF tagger uses this edge label infor-
mation to determine the complements? syntactic
roles, and assigns the verb the corresponding SCF;
in the case of halten above, the SCF is nap:f?ur.
1
We digress from Schulte im Walde?s original SCF inven-
tory in that we do not indicate case information in PPs.
The rule-based SCF tagger handles auxiliary and
modal verb constructions, passive alternations, sep-
arable verb prefixes, and raising and control con-
structions. E.g., the subject sie ?they? of anfangen
?begin? in (7) doubles as the subject of its infinite
clausal complement; hence, it shows up in the SCF
of the complement?s head geben ?give?, too:
(7) Sie fingen an, mir Stromschl?age zu geben.
?They started to give me electric shocks.?
The tagger also handles involved cases with
many complements, including PPs and clauses as
in (8). As the SCF inventory allows at most three
complements in an SCF, such cases call for pri-
oritising of verbal complements (e.g., subjects, ob-
jects, and clausal complements are preferred over
PP complements). Consequently, the main verb
empfehlen ?recommend? in (8), which has a subject,
a dative object, a PP, and an infinitival clausal com-
plement, is assigned the SCF ndi. Another chal-
lenging task which relies on edge label information
is filtering out clausal adjuncts (relative clauses and
parentheticals) so as not to include them in SCFs.
(8) [
PP
Am Freitag] empfahl [
NP:Nom
der
Aufsichtsrat] [
NP:Dat
den Aktion?aren], [
S
das
Angebot abzulehnen].
?On Friday the board of directors advised
shareholders to turn down the offer.?
The 17 rules of the SCF tagger are simple; most
of them categorise the complements of a specific
verb instance; e.g., if a nominal complement to the
verb is edge-labelled as a nominative subject, add n
to the verb?s SCF, unless the verb is in the passive,
in which case add a to the SCF.
Our system was optimised by progressively re-
fining the SCF tagger?s rules through manual error
analysis on sentences from TIGER. The result is
an automatic SCF tagger that is resilient to varia-
tions in sentence structure and is firmly based on
linguistically motivated knowledge. As a test case
for its linguistic soundness, we chose the perfect
parses in the TIGER treebank and found that the
tagger is very accurate in capturing subcategorisa-
tion information inherent in these data.
3.2 The edge labeller
To obtain edge label information for the parses de-
livered by the Berkeley Parser, we built a novel
machine learning classifier to annotate parse trees
300
SPDS
Das
VVFIN
hielte
PPER
ich
PP
APPR
f?ur
AP
moralisch au?erordentlich fragw?urdig
$.
.
OA
HD
SB
MO
AC
NK
Figure 1: Edge labels in the TIGER corpus.
with TIGER edge label information. This edge la-
beller is a maximum entropy (multiclass logistic
regression) model built using the Stanford Classi-
fier package
2
. We include features such as:
? The part of speech of the complement;
? The first word of the complement;
? The lexical head of the complement;
? N-grams on the end of the lexical head of the
complement;
? The kind of article of a complement;
? The presence or absence of specific article
forms in other complements to the same verb;
? Position of the complement with respect to a
reflexive pronoun in the sentence;
? The lemmatised form of the verb governing
the complement (i.e., the verb on which the
complement depends syntactically);
? The clause type of the governing verb; and,
? Active or passive voice of the governing verb.
We do no tuning and use the software?s default
hyperparameters (L2 regularisation with ? = 3).
This classifier was trained from edge label data
extracted from the NEGRA and TIGER corpora;
our training set contained 300,000 samples (ap-
proximately 25% from NEGRA and 75% from
TIGER). On a held-out test set of 10% (contain-
ing 34,000 samples), the classifier achieves a final
F-score of 95.5% on the edge labelling task.
The edge labeller makes the simplifying assump-
tion that verbal complements can be labelled inde-
pendently. Consequently, it tends to annotate multi-
ple complements as subject for each verb. This has
to do with the numerical dominance of subjects,
which make up about 40% of all verb complements,
more than three times the number of the next most
common complement type (direct object).
Therefore we first collect all possible labels with
associated probabilities that the edge labeller as-
2
http://nlp.stanford.edu/software/
classifier.shtml
signs to each complement of a verb. We then
choose the set of labels with the highest probability
that includes at most one subject and at most one
accusative direct object for the verb, assuming that
the joint probability of a set of labels is the product
of the individual label probabilities.
We use our edge labeller in this work for mor-
phological disambiguation of nominals and for
identifying clausal adjuncts, but the edge labeller
is a standalone reusable component, which might
be equally well be used to mark up parse trees for,
e.g., a semantic role labelling system.
4 The subcategorisation lexicon
With the system described in Sec. 3, we build a Ger-
man subcategorisation lexicon that collects counts
of ?lemma,SCF? on deWaC (Baroni et al., 2009),
a corpus of text extracted from Web search re-
sults, with 10
9
words automatically POS-tagged
and lemmatised by the TreeTagger (Schmid, 1994).
A subset of this corpus, SdeWaC (Faa? and Eckart,
2013), has been preprocessed to include only sen-
tences which are maximally parsable; this smaller
corpus includes 880 million words in 45 million
sentences. We parsed 3 million sentences (80 mil-
lion words) of SdeWaC; after filtering out those
verb lemmas seen only five times or fewer in the
corpus, we are left with statistics on 8 million verb
instances, representing 9,825 verb lemmas.
As a concrete example for the resulting SCF lexi-
con, consider the entry for sprechen ?talk? in Fig. 2,
which occurs 16,254 times in our SCF lexicon.
Sprechen refers to a conversation with speaker,
hearer, topic, message, and code: Speakers are ex-
pressed by nominative NPs, hearers, by mit-, bei-
or zu-PPs, topics, by von- and ?uber-PPs. The code
is expressed in in-PPs, and the message, by ac-
cusative NPs (einige Worte sprechen ?to say a few
words?), main-clause complements or subordinate
dass (?that?) sentences. Other uses of the verb are
301
np:von (2715), n (2696), na (1380), np:mit
(1247), np:in (1132), nS-2 (1064), np:?uber
(853), np:f?ur (695), nS-dass (491), np:zu
(307), nap:in (280), nap:von (275), ni (261),
np:bei (212), np:gegen (192), np:an (186),
naS-2 (172), np:aus (168), np:auf (112),
nap:?uber (112)
Figure 2: SCF lexicon for sprechen
figurative , e.g., sprechen gegen ?be a counterar-
gument to?. As the distinction between arguments
and adjuncts is gradual in our system, some adjunct
patterns appear in the lexicon, too, but only with
low frequency, e.g., np:auf, in which the auf -PP
expresses the setting of the conversation, as in auf
der Tagung sprechen ?speak at the convention?.
For reference, we also constructed an SCF lexi-
con from the NEGRA and TIGER corpora, which
together comprise about 1.2 million words. This
SCF lexicon contains statistics on 133,897 verb
instances (5,316 verb lemmas). While the manual
annotations in NEGRA and TIGER mean that this
SCF lexicon has virtually no noise, the small size
of the corpora results in problems with data spar-
sity and negatively impacts the utility of this re-
source (see discussion in Section 6.2).
5 Automatic verb classification
The remainder of the paper sets out to establish the
relevance of our SCF acquisition system by com-
parison to previous work. As stated in Sec. 2, the
only prior automatic German SCF acquisition sys-
tem is that of Schulte im Walde (2002a), which was
evaluated directly against an electronic version of
a large dictionary; as this is not an open access
resource, we cannot perform a similar evaluation.
We opt therefore to use a task-based evaluation
to compare our system directly with Schulte im
Walde?s, and leave manual evaluation for future
work. We refer back to the experiment set up by
Schulte im Walde (2006) to automatically induce
classifications of German verbs by clustering them
on the basis of their SCF preferences as listed in
her SCF lexicon. By casting this experiment as a
fixed task, we can compare our system directly to
hers. The link between subcategorisation and verb
semantics is linguistically sound, due to the inter-
dependence between verb meanings and the num-
ber and kinds of their syntactic arguments (Levin,
1993; Levin and Rappaport Hovav, 2005). E.g.,
only transitive verbs that denote a change of state
like cut and break enter in the middle construction
(The bread cuts easily.), with the patient or theme
argument appearing as the syntactic subject. Thus,
verbs whose SCF preferences show such an alter-
nation can be predicted to denote a change of state.
We adopt the automatic verb classification
paradigm to evaluate our system, replicating
Schulte im Walde?s (2006) experiment to the best
of our ability. We argue that by evaluating our
SdeWaC SCF lexicon described in the previous
section, we simultaneously evaluate our subcate-
gorisation acquisition system; this technique also
allows us to demonstrate the semantic relevance of
our SCF lexicon. Section 5.1 introduces the man-
ual verb classification we use as a gold standard
and Section 5.2 describes our unsupervised clus-
tering technique. Our evaluation of the clustering
against the gold standard then follows in Section 6.
5.1 Manual verb classifications
The semantic verb classification proposed by
Schulte im Walde (2006, page 162ff.), hereafter
SiW2006, comprises 168 high- and low-frequency
verbs grouped into 43 semantic classes, with be-
tween 2 and 7 verbs per class. Examples of these
classes are Aspect (e.g., anfangen ?begin?), Propo-
sitional Attitude (e.g., denken ?think?), Transfer of
Possession (Obtaining) (e.g., bekommen ?get?), and
Weather (e.g., regnen ?rain?). Some of the classes
are subclassified
3
, e.g., Manner of Motion, with
the subclasses Locomotion (klettern ?climb?), Ro-
tation (rotieren ?rotate?), Rush (eilen ?hurry?), Ve-
hicle (fliegen ?fly?), and Flotation (gleiten ?glide?).
These classes are related to Levin classes in that
some are roughly equivalent to a Levin class (e.g.,
Aspect and Levin?s Begin class), others are sub-
groups of Levin classes, e.g., Position is a sub-
group of Levin?s Dangle class; finally, some classes
lump together Levin classes, e.g., Transfer of Pos-
session (Obtaining) combines Levin?s Get and Ob-
tain classes. This shows that these classes could be
integrated into a large-scale classification of Ger-
man verbs in the style of Levin (1993).
5.2 Clustering
From the counts of ?lemma,SCF? in the SCF lexi-
con, we can estimate the conditional probability
that a particular verb v appears with an SCF f :
3
For the purpose of our evaluation, we disregard class-
subclass relations and consider subclasses as separate entities.
302
P (scf = f |lemma = v). We smooth these con-
ditional probability distributions by backing off to
the prior probability P (scf) (Katz, 1987).
With these smoothed conditional probabilities,
we cluster verbs with k-means clustering (Forgy,
1965), a hard clustering technique, which partitions
a set of objects into k clusters. The algorithm is ini-
tialised with a starting set of k cluster centroids; it
then proceeds iteratively, first assigning each ob-
ject to the cluster whose centroid is closest under
some distance measure, and then calculating new
centroids to represent the centres of the updated
clusters. The algorithm terminates when the assign-
ment of objects to clusters no longer changes.
D(p?q) =
?
i
p
i
log
p
i
q
i
(9)
irad(p, q) = D(p?
p+ q
2
) +D(q?
p+ q
2
) (10)
skew(p, q) = D(p??q + (1? ?)p) (11)
In our experiments, verbs are represented by
their conditional probability distributions over
SCFs. As distance measures, we use two variants
of the Kullback-Leibler divergence (9), a measure
of the dissimilarity of two probability distributions.
The KL divergence from p to q is undefined if at
some point q but not p is zero, so we use measures
based on KL without this problem, viz., the in-
formation radius (aka Jensen-Shannon divergence,
a symmetric metric, (10)), as well as skew diver-
gence (an asymmetric dissimilarity measure which
smoothes q by interpolating it to a small degree
with p, (11)), where we set the interpolation param-
eter to be ? = 0.9, to make our results comparable
to Schulte im Walde?s (2006)
4
.
As mentioned, the k-means algorithm is ini-
tialised with a set of cluster centroids; in this study,
we initialise the centroids by random partitions
(each of the n objects is randomly assigned to one
of k clusters, and the centroids are then computed
as the means of these random partitions). Because
the random initial centroids influence the final clus-
tering, we repeat the clustering a number of times.
We also initialise the k-means cluster centroids
using agglomerative hierarchical clustering, a de-
terministic iterative bottom-up process. Hierarchi-
cal clustering initially assigns verbs to singleton
clusters; the two clusters which are ?nearest? to
4
Schulte im Walde (2006) takes ? = 0.9 although Lee
(1999) recommends ? = 0.99 or higher values in her original
description of skew divergence.
each other are then joined together, and this pro-
cess is repeated until the desired number of clusters
is obtained. Hierarchical clustering is performed
to group the verbs into k clusters; the centroids
of these clusters are then used to initialise the k-
means algorithm. While there exist several variants
of hierarchical clustering, we use Ward?s method
(Ward, Jr, 1963) for merging clusters, which at-
tempts to minimise the variance inside clusters;
Ward?s criterion was previously found to be the
most effective hierarchical clustering technique for
verb classification (Schulte im Walde, 2006).
6 Evaluation
This section presents the results of evaluating the
unsupervised verb clustering based on our SCF lex-
ica against the gold standard described in Sec. 5.1.
6.1 Results
We use two cluster purity measures, defined in
Fig. 3; we intentionally target our numerical eval-
uations to be directly comparable with previous
results in the literature. As k-means is a hard clus-
tering algorithm, we consider a clustering C to be
an equivalence relation that partitions n verbs into
k disjoint subsets C = {C
1
, . . . , C
k
}.
The first of these purity measures, adjusted Rand
index (Rand
a
in Eq. (12)) judges clustering simi-
larity using the notion of the overlap between a
cluster C
i
in a given clustering C and a cluster G
j
in a gold standard clustering G, this value being
denoted by CG
ij
= |C
i
? G
j
|; values of Rand
a
range between 0 for chance and 1 for perfect cor-
relation. The other metric, the pairwise F -score
(PairF, Eq. (13)), operates by constructing a con-
tingency table on the
(
n
2
)
pairs of verbs, the idea
being that the gold standard provides binary judge-
ments about whether two verbs should be clustered
together or not. If a clustering agrees with the gold
standard in clustering a pair of verbs together or
separately, this is a ?correct? answer; by extension,
information retrieval measures such as precision
(P ) and recall (R) can be computed.
Table 1 shows the performance of our SCF lex-
ica, evaluated against the SiW2006 gold standard.
The random baseline is given by PairF = 2.08 and
Rand
a
= ?0.004 (calculated as the average of 50
random partitions). The optimal baseline is PairF
= 95.81 and Rand
a
= 0.909, calculated by evalu-
ating the gold standard against itself. As the gold
standard includes polysemous verbs, which belong
303
Rand
a
(C,G) =
?
i,j
(
CG
ij
2
)
?
[
?
i
(
|C
i
|
2
)
?
j
(
|G
j
|
2
)
]
/
(
n
2
)
1
2
[
?
i
(
|C
i
|
2
)
+
?
j
(
|G
j
|
2
)
]
?
[
?
i
(
|C
i
|
2
)
?
j
(
|G
j
|
2
)
]
/
(
n
2
)
(12)
PairF(C,G) =
2P (C,G)R(C,G)
P (C,G) +R(C,G)
(13)
Figure 3: Evaluation metrics used to compare clusterings to gold standards.
Data Set Eval Distance Manual Random Best Random Mean Ward
Schulte im Walde PairF IRad 40.23 1.34? 16.15 13.37 17.86? 17.49
Skew 47.28 2.41? 18.01 14.07 15.86? 15.23
Rand
a
IRad 0.358 0.001? 0.118 0.093 0.145? 0.142
Skew 0.429 ?0.002? 0.142 0.102 0.158? 0.158
NEGRA/TIGER PairF IRad 30.77 2.06? 14.67 12.39 16.13? 15.52
Skew 40.19 3.47? 12.95 11.48 14.05? 14.31
Rand
a
IRad 0.281 0.000? 0.122 0.094 0.134? 0.129
Skew 0.382 ?0.015? 0.102 0.089 0.112? 0.114
SdeWaC PairF IRad 42.66 1.62? 20.36 18.26 26.94? 27.50
Skew 50.38 2.99? 20.75 17.80 24.60? 24.94
Rand
a
IRad 0.387 ?0.006? 0.167 0.146 0.232? 0.238
Skew 0.465 0.008? 0.170 0.143 0.208? 0.211
Table 1: Evaluation of the NEGRA/TIGER and SdeWaC SCF lexica using the SiW2006 gold standard.
to more than one cluster, the optimal baseline is
calculated by randomly picking one of their senses;
the average is then taken over 50 such trials.
We cluster using k = 43, matching the number
of clusters in the gold standard. Of the 168 verbs in
SiW2006, 159 are attested in NEGRA and TIGER
(17,285 instances), and 167 are found in SdeWaC
(1,047,042 instances)
5
.
We report the results using k-means clustering
initialised under a variety of conditions. ?Manual?
shows the quality of the clustering achieved when
initialising k-means with the gold standard classes.
We also initialise clustering 10 times using ran-
dom partitions. For the best clustering
6
in these
10, ?Random Best? shows the evaluation of both
the starting random partition and the final cluster-
ing found by k-means; ?Random Mean? shows the
average cluster purity of the 10 final clusterings.
?Ward? shows the evaluation of the clustering ini-
tialised with centroids found by hierarchical clus-
5
Verbs missing from the clustering reduce the maximum
achievable cluster purity score.
6
Specifically, we take the clustering result with the mini-
mum intra-cluster distance (not the clustering result with the
best performance on the gold standard).
tering of the verbs using Ward?s method. Again,
both the initial partition found by Ward?s method
and the k-means solution based on it are shown.
For comparison, we list the results of Schulte
im Walde (2006, p. 174, Table 7) for the second
level of SCF granularity, with PP head and case
information (see Sec. 2 for Schulte im Walde?s
analysis). While this seems the most appropriate
comparison to draw, since we also collect statis-
tics about PPs, it is ambitious because, as noted
in Section 3, our SCF lexica lack case informa-
tion about PPs.
7
Compared to Schulte im Walde?s
numbers, the NEGRA/TIGER SCF lexicon scores
significantly worse on the PairF evaluation metric
under all conditions, and also on the Rand
a
metric
using the skew divergence measure (Rand
a
/IRad
is not significantly different). The SdeWaC SCF
lexicon scores better on all metrics and conditions;
these results are significant at the p < 0.001 level
8
.
7
PP case information is relevant for prepositions that can
take both locative and directional readings, as in in der Stadt
(dative) ?in town? und in die Stadt (accusative) ?to town?.
8
Statistical significance is calculated by running repeated
k-means clusterings with random partition initialisation and
evaluating the results using the relevant purity metrics. These
repeated clustering scores represent a random variable (a func-
304
6.2 Discussion
Sec. 6.1 compared the SCF lexicon created us-
ing SdeWaC with the lexicon built by Schulte im
Walde (2002a), showing that our lexicon achieves
significantly better results on the verb clustering
task. We interpret this to be indicative of a more
accurate subcategorisation lexicon, and, by exten-
sion, of a more accurate SCF acquisition system.
We attribute this superior performance primar-
ily to our use of a statistical parser as opposed to
a hand-written grammar. This design choice has
several advantages. First, the parser delivers robust
syntactic analyses, which we can expect to be rel-
atively domain-independent. Second, we make no
prior assumptions about the variety of subcategori-
sation phenomena that might appear in text, decou-
pling the identification of SCFs from the ability
to parse natural language. Third, the fact that our
parser and edge labeller are trained on the 800,000
word NEGRA/TIGER corpus means that we bene-
fit from the linguistic expertise that went into build-
ing that treebank. Our use of off-the-shelf tools
(the parser and our simple yet effective machine
learning model describing edge label information)
makes our system considerably simpler and easier
to implement than Schulte im Walde?s. We see our
system as more easily extensible to other languages
for which there is a parser and an initial syntacti-
cally annotated corpus to train the edge labeller on.
The NEGRA/TIGER SCF lexicon performs not
as well on the verb clustering evaluations, as fewer
verbs are attested in NEGRA/TIGER compared to
the SdeWaC SCF lexicon and gold standard clus-
terings. Data sparsity can be a problem in SCF ac-
quisition; all other factors being equal, using more
data to construct an SCF lexicon should make pat-
terns in the language more readily visible and re-
duce the chance of missing a particular lemma-
SCF combination accidentally. A secondary ef-
fect is that models of verb subcategorisation prefer-
ences like the ones used here can be more precisely
estimated as the counts of observed verb instances
increase, particularly for low-frequency verbs.
Error analysis of our SCF lexicon reveals low
counts of expletive subjects. The edge labeller is
supposed to annotate semantically empty subjects
(es, ?it?) as expletive; for clusterings examined in
Sec. 5.1, this would affect weather verbs (e.g., es
tion of the random cluster centroids used to initialise the k-
means clustering). These samples are normally distributed, so
we determine statistical significance using a t-test against the
?Random Mean? results reported by Schulte im Walde (2006).
regnet, ?it?s raining?). However, in our SdeWaC
SCF lexicon, expletive subjects are clearly under-
represented. Our SCF lexicon built on TIGER,
where expletive subjects are systematically la-
belled, has the SCF xa as the most common SCF
for the verb geben (in es gibt ?there is?). In con-
trast, in our SdeWaC SCF lexicon, the most com-
mon SCF is the transitive na, with xa in seventh
place. I.e., the edge labeller does not identify all
expletive subjects, which is due to the fact that ex-
pletive subjects are syntactically indistinguishable
from neuter pronominal subjects, so the edge la-
beller does not have a rich feature set to inform it
about this category. But since, statistically, exple-
tive pronouns make up less than 1% of subjects
in TIGER, the prior probability of labelling a con-
stituent as expletive is very low. Due to these fig-
ures, we do not expect this issue to seriously impact
the quality of our verb classification evaluations.
7 Future work
In this paper we have presented a state-of-
the-art subcategorisation acquisition system for
free-word order languages, and used it to cre-
ate a large subcategorisation frame lexicon for
German verbs. Our SCF lexicon resource is
available at http://amor.cms.hu-berlin.
de/?robertsw/scflex.html. We are per-
forming a manual evaluation of the output of our
system, which we will report soon.
We plan to continue this work first by expanding
our SCF lexicon with case information and selec-
tional preferences, second by using our SCF clas-
sifier and lexicon for verbal Multiword Expression
identification in German, and last by comparing
it to existing verb classifications, either by using
available resources for German like the SALSA
corpus (Burchardt et al., 2006), or by translating
parts of VerbNet into German to create a more
extensive gold standard for verb clustering in the
spirit of Sun et al. (2010) who found that Levin?s
verb classification can be translated to French and
still usefully allow generalisation over verb classes.
Finally, we plan to perform in vivo evaluation
of our SCF lexicon, to determine what benefit
it can deliver for NLP applications such as Se-
mantic Role Labelling and Word Sense Disam-
biguation. Recent research has found that even
automatically-acquired verb classifications can be
useful for NLP applications (Shutova et al., 2010;
Guo et al., 2011).
305
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In TLT, pages 24?41.
Michael R. Brent and Robert C. Berwick. 1991. Auto-
matic acquisition of subcategorization frames from
tagged text. In HLT, pages 342?345. Morgan Kauf-
mann.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. CoRR,
cmp-lg/9702002.
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado, and Manfred Pinkal.
2006. The SALSA corpus: A German corpus re-
source for lexical semantics. In LREC.
Judith Eckle-Kohler. 1999. Linguistic knowledge for
automatic lexicon acquisition from German text cor-
pora. Ph.D. thesis, Universit?at Stuttgart.
Gertrud Faa? and Kerstin Eckart. 2013. SdeWaC - A
corpus of parsable sentences from the Web. In Lan-
guage processing and knowledge in the Web, pages
61?68. Springer, Berlin, Heidelberg.
Edward W. Forgy. 1965. Cluster analysis of multivari-
ate data: Efficiency versus interpretability of classifi-
cations. Biometrics, 21:768?769.
Raza Ghulam. 2011. Subcategorization acquisition
and classes of predication in Urdu. Ph.D. thesis,
Universit?at Konstanz.
Trond Grenager and Christopher D. Manning. 2006.
Unsupervised discovery of a statistical verb lexicon.
In EMNLP, pages 1?8.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In EMNLP,
pages 273?283.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401.
Anna Korhonen. 2002. Subcategorization acquisi-
tion. Technical report, University of Cambridge,
Computer Laboratory.
Joel Lang and Mirella Lapata. 2010. Unsupervised
induction of semantic roles. In HLT, pages 939?947.
Lillian Lee. 1999. Measures of distributional similar-
ity. In ACL, pages 25?32.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment realization. Cambridge University Press, Cam-
bridge.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press, Chicago.
Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In ACL, pages 235?242.
Manolis Maragoudakis, Katia Lida Kermanidis, and
George Kokkinakis. 2000. Learning subcategoriza-
tion frames from corpora: A case study for modern
Greek. In Proceedings of COMLEX 2000, Work-
shop on Computational Lexicography and Multime-
dia Dictionaries, pages 19?22.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL, pages 433?440.
Anoop Sarkar and Daniel Zeman. 2000. Automatic
extraction of subcategorization frames for Czech. In
COLING, pages 691?697.
Silke Scheible, Sabine Schulte im Walde, Marion
Weller, and Max Kisselew. 2013. A compact but lin-
guistically detailed database for German verb subcat-
egorisation relying on dependency parses from Web
corpora: Tool, guidelines and resource. In Web as
Corpus Workshop.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In NeMLaP, vol-
ume 12, pages 44?49.
Sabine Schulte im Walde and Chris Brew. 2002. Induc-
ing German semantic verb classes from purely syn-
tactic subcategorisation information. In ACL, pages
223?230.
Sabine Schulte im Walde. 2002a. A subcategorisation
lexicon for German verbs induced from a lexicalised
PCFG. In LREC, pages 1351?1357.
Sabine Schulte im Walde. 2002b. Evaluating verb sub-
categorisation frames learned by a German statisti-
cal grammar against manual definitions in the Duden
Dictionary. In EURALEX, pages 187?197.
Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?194.
Sabine Schulte im Walde. 2009. The induction of
verb frames and verb classes from corpora. In Anke
L?udeling and Merja Kyt?o, editors, Corpus linguis-
tics: An international handbook, volume 2, chap-
ter 44, pages 952?971. Mouton de Gruyter, Berlin.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In COLING, pages 1002?1010.
306
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In ANLP, pages 88?95.
Lin Sun, Anna Korhonen, and Yuval Krymolowski.
2008. Verb class discovery from rich syntactic data.
In CICLing, pages 16?27, Haifa, Israel.
Lin Sun, Anna Korhonen, Thierry Poibeau, and C?edric
Messiant. 2010. Investigating the cross-linguistic
potential of VerbNet-style classification. In COL-
ING, pages 1056?1064, Beijing, China.
Ivan Titov and Alexandre Klementiev. 2012. A
Bayesian approach to unsupervised semantic role in-
duction. In EACL, pages 12?22.
Joe H. Ward, Jr. 1963. Hierarchical grouping to opti-
mize an objective function. Journal of the American
Statistical Association, 58(301):236?244.
Oliver Wauschkuhn. 1999. Automatische Extrak-
tion von Verbvalenzen aus deutschen Textkorpora.
Shaker Verlag.
307
