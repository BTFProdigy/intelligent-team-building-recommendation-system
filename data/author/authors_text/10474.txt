Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 844?853,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Untangling the Cross-Lingual Link Structure of Wikipedia
Gerard de Melo
Max Planck Institute for Informatics
Saarbru?cken, Germany
demelo@mpi-inf.mpg.de
Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
weikum@mpi-inf.mpg.de
Abstract
Wikipedia articles in different languages
are connected by interwiki links that are
increasingly being recognized as a valu-
able source of cross-lingual information.
Unfortunately, large numbers of links are
imprecise or simply wrong. In this pa-
per, techniques to detect such problems are
identified. We formalize their removal as
an optimization task based on graph re-
pair operations. We then present an al-
gorithm with provable properties that uses
linear programming and a region growing
technique to tackle this challenge. This
allows us to transform Wikipedia into a
much more consistent multilingual regis-
ter of the world?s entities and concepts.
1 Introduction
Motivation. The open community-maintained en-
cyclopedia Wikipedia has not only turned the In-
ternet into a more useful and linguistically di-
verse source of information, but is also increas-
ingly being used in computational applications as
a large-scale source of linguistic and encyclope-
dic knowledge. To allow cross-lingual navigation,
Wikipedia offers cross-lingual interwiki links that
for instance connect the Indonesian article about
Albert Einstein to the corresponding articles in
over 100 other languages. Such links are extraor-
dinarily valuable for cross-lingual applications.
In the ideal case, a set of articles connected di-
rectly or indirectly via such links would all de-
scribe the same entity or concept. Due to concep-
tual drift, different granularities, as well as mis-
takes made by editors, we frequently find con-
cepts as different as economics and manager in the
same connected component. Filtering out inaccu-
rate links enables us to exploit Wikipedia?s multi-
linguality in a much safer manner and allows us to
create a multilingual register of named entities.
Contribution. Our research contributions are:
1) We identify criteria to detect inaccurate connec-
tions in Wikipedia?s cross-lingual link structure.
2) We formalize the task of removing such links
as an optimization problem. 3) We introduce an
algorithm that attempts to repair the cross-lingual
graph in a minimally invasive way. This algorithm
has an approximation guarantee with respect to
optimal solutions. 4) We show how this algorithm
can be used to combine all editions of Wikipedia
into a single large-scale multilingual register of
named entities and concepts.
2 Detecting Inaccurate Links
In this paper, we model the union of cross-lingual
links provided by all editions of Wikipedia as an
undirected graph G = (V,E) with edge weights
w(e) for e ? E. In our experiments, we simply
honour each individual link equally by defining
w(e) = 2 if there are reciprocal links between the
two pages, 1 if there is a single link, and 0 other-
wise. However, our framework is flexible enough
to deal with more advanced weighting schemes,
e.g. one could easily plug in cross-lingual mea-
sures of semantic relatedness between article texts.
It turns out that an astonishing number of con-
nected components in this graph harbour inac-
curate links between articles. For instance, the
Esperanto article ?Germana Imperiestro? is about
German emporers and another Esperanto article
?Germana Imperiestra Regno? is about the Ger-
man Empire, but, as of June 2010, both are linked
to the English and German articles about the Ger-
man Empire. Over time, some inaccurate links
may be fixed, but in this and in large numbers of
other cases, the imprecise connection has persisted
for many years. In order to detect such cases, we
need to have some way of specifying that two ar-
ticles are likely to be distinct.
844
Figure 1: Connected component with inaccurate
links (simplified)
2.1 Distinctness Assertions
Figure 1 shows a connected component that con-
flates the concept of television as a medium with
the concept of TV sets as devices. Among other
things, we would like to state that ?Television? and
?T.V.? are distinct from ?Television set? and ?TV
set?. In general, we may have several sets of enti-
ties Di,1, . . . , Di,li , for which we assume that any
two entities u,v from different sets are pairwise
distinct with some degree of confidence or weight.
In our example, Di,1 = {?Television?,?T.V.?}
would be one set, andDi,2 = {?Television set?,?TV
set?} would be another set, which means that we
are assuming ?Television?, for example, to be dis-
tinct from both ?Television set? and ?TV set?.
Definition 1. (Distinctness Assertions) Given a
set of nodes V , a distinctness assertion is a col-
lection Di = (Di,1, . . . , Di,li) of pairwise dis-
joint (i.e. Di,j ? Di,k = ? for j 6= k) sub-
sets Di,j ? V that expresses that any two nodes
u ? Di,j , v ? Di,k from different subsets (j 6= k)
are asserted to be distinct from each other with
some weight w(Di) ? R.
We found that many components with inaccurate
links can be identified automatically with the fol-
lowing distinctness assertions.
Criterion 1. (Distinctness between articles from
the same Wikipedia edition) For each language-
specific edition of Wikipedia, a separate asser-
tion (Di,1, Di,2, . . . ) can be made, where each
Di,j contains an individual article together with
its respective redirection pages. Two articles from
the same Wikipedia very likely describe distinct
concepts unless they are redirects of each other.
For example, ?Georgia (country)? is distinct from
?Georgia (U.S. State)?. Additionally, there are also
redirects that are clearly marked by a category or
template as involving topic drift, e.g. redirects
from songs to albums or artists, from products to
companies, etc. We keep such redirects in a Di,j
distinct from the one of their redirect targets.
Criterion 2. (Distinctness between categories
from the same Wikipedia edition) For each
language-specific edition of Wikipedia, a separate
assertion (Di,1, Di,2, . . . ) is made, where each
Di,j contains a category page together with any
redirects. For instance, ?Category:Writers? is dis-
tinct from ?Category:Writing?.
Criterion 3. (Distinctness for links with anchor
identifiers) The English ?Division by zero?, for in-
stance, links to the German ?Null#Division?. The
latter is only a part of a larger article about the
number zero in general, so we can make a dis-
tinctness assertion to separate ?Division by zero?
from ?Null?. In general, for each interwiki link or
redirection with an anchor identifier, we add an as-
sertion (Di,1, Di,2) where Di,1,Di,2 represent the
respective articles without anchor identifiers.
These three types of distinctness assertions are
instantiated for all articles and categories of all
Wikipedia editions. The assertion weights are tun-
able; the simplest choice is using a uniform weight
for all assertions (note that these weights are dif-
ferent from the edge weights in the graph). We
will revisit this issue in our experiments.
2.2 Enforcing Consistency
Given a graph G representing cross-lingual links
between Wikipedia pages, as well as distinctness
assertions D1, . . . , Dn with weights w(Di), we
may find that nodes that are asserted to be dis-
tinct are in the same connected component. We
can then try to apply repair operations to recon-
cile the graph?s link structure with the distinctness
asssertions and obtain global consistency. There
are two ways to modify the input, and for each
we can also consider the corresponding weights
as a sort of cost that quantifies how much we are
changing the original input:
a) Edge cutting: We may remove an edge e ?
E from the graph, paying cost w(e).
b) Distinctness assertion relaxation: We may
remove a node v ? V from a distinctness as-
sertion Di, paying cost w(Di).
845
Removing edges allows us to split connected com-
ponents into multiple smaller components, thereby
ensuring that two nodes asserted to be distinct are
no longer connected directly or indirectly. In Fig-
ure 1, for instance, we could delete the edge from
the Spanish ?TV set? article to the Japanese ?televi-
sion? article. In constrast, removing nodes from
distinctness assertions means that we decide to
give up our claim of them being distinct, instead
allowing them to share a connected component.
Our reliance on costs is based on the assump-
tion that the link structure or topology of the graph
provides the best indication of which cross-lingual
links to remove. In Figure 1, we have distinct-
ness assertions between nodes in two densely con-
nected clusters that are tied together only by a sin-
gle spurious link. In such cases, edge removals
can easily yield separate connected components.
When, however, the two nodes are strongly con-
nected via many different paths with high weights,
we may instead opt for removing one of the two
nodes from the distinctness assertion.
The aim will be to balance the costs for remov-
ing edges from the graph with the costs for remov-
ing nodes from distinctness assertions to produce
a consistent solution with a minimal total repair
cost. We accommodate our knowledge about dis-
tinctness while staying as close as possible to what
Wikipedia provides as input.
This can be formalized as the Weighted
Distinctness-Based Graph Separation (WDGS)
problem. Let G be an undirected graph with a set
of vertices V and a set of edges E weighted by
w : E ? R. If we use a set C ? V to spec-
ify which edges we want to cut from the original
graph, and sets Ui to specify which nodes we want
to remove from distinctness assertions, we can be-
gin by defining WDGS solutions as follows.
Definition 2. (WDGS Solution). Given a graph
G = (V,E) and n distinctness assertionsD1, . . . ,
Dn, a tuple (C,U1, . . . , Un) is a valid WDGS so-
lution if and only if ?i, j, k 6= j, u ? Di,j \ Ui,
v ? Di,k \ Ui: P(u, v, E \ C) = ?, i.e. the set of
paths from u to v in the graph (V,E \C) is empty.
Definition 3. (WDGS Cost). Let w : E ? R
be a weight function for edges e ? E, and w(Di)
(i = 1 . . . n) be weights for the distinctness as-
sertions. The (total) cost of a WDGS solution
S = (C,U1, . . . , Un) is then defined as
c(S) = c(C,U1, . . . , Un)
=
[
?
e?C
w(e)
]
+
[
n?
i=1
|Ui|w(Di)
]
Definition 4. (WDGS). A WDGS problem instance
P consists of a graph G = (V,E) with edge
weights w(e) and n distinctness assertions D1,
. . . , Dn with weights w(Di). The objective con-
sists in finding a solution (C,U1, . . . , Un) with
minimal cost c(C,U1, . . . , Un).
It turns out that finding optimal solutions effi-
ciently is a hard problem (proofs in Appendix A).
Theorem 1. WDGS is NP-hard and APX-hard. If
the Unique Games Conjecture (Khot, 2002) holds,
then it is NP-hard to approximate WDGS within
any constant factor ? > 0.
3 Approximation Algorithm
Due to the hardness of WDGS, we devise a
polynomial-time approximation algorithm with an
approximation factor of 4 ln(nq + 1) where n is
the number of distinctness assertions and q =
max
i,j
|Di,j |. This means that for all problem in-
stances P , we can guarantee
c(S(P ))
c(S?(P ))
? 4 ln(nq + 1),
where S(P ) is the solution determined by our al-
gorithm, and S?(P ) is an optimal solution. Note
that this approximation guarantee is independent
of how long each Di is, and that it merely repre-
sents an upper bound on the worst case scenario.
In practice, the results tend to be much closer to
the optimum, as will be shown in Section 4.
Our algorithm first solves a linear program (LP)
relaxation of the original problem, which gives
us hints as to which edges should most likely be
cut and which nodes should most likely be re-
moved from distinctness assertions. Note that this
is a continuous LP, not an integer linear program
(ILP); the latter would not be tractable due to the
large number of variables and constraints of the
problem. After solving the linear program, a new
? extended ? graph is constructed and the optimal
LP solution is used to define a distance metric on
it. The final solution is obtained by smartly se-
lecting regions in this extended graph as the in-
dividual output components, employing a region
846
growing technique in the spirit of the seminal work
by Leighton and Rao (1999). Edges that cross the
boundaries of these regions are cut.
Definition 5. Given a WDGS instance, we define a
linear program of the following form:
minimize
?
e?E
dew(e) +
n?
i=1
li?
j=1
?
v?Di,j
ui,vw(Di)
subject to
pi,j,v = ui,v ?i, j<li, v ? Di,j (1)
pi,j,v + ui,v ? 1 ?i, j<li, v ?
S
k>j
Di,k (2)
pi,j,v ? pi,j,u + de ?i, j<li, e=(u,v) ? E (3)
de ? 0 ?e ? E (4)
ui,v ? 0 ?i, v ?
liS
j=1
Di,j (5)
pi,j,v ? 0 ?i, j<li, v?V (6)
The LP uses decision variables de and ui,v, and
auxiliary variables pi,j,v that we refer to as poten-
tial variables. The de variables indicate whether
(in the continuous LP: to what degree) an edge
e should be deleted, and the ui,v variables indi-
cate whether (to what degree) v should be removed
from a distinctness assertion Di. The LP objec-
tive function corresponds to Definition 3, aiming
to minimize the total costs. A potential variable
pi,j,v reflects a sort of potential difference between
an assertionDi,j and a node v. If pi,j,v = 0, then v
is still connected to nodes in Di,j . Constraints (1)
and (2) enforce potential differences between Di,j
and all nodes in Di,k with k > j. For instance,
for distinctness between ?New York City? and ?New
York? (the state), they might require ?New York?
to have a potential of 1, while ?New York City?
has a potential of 0. The potential variables are
tied to the deletion variables de for edges in Con-
straint (3) as well as to the ui,v in Constraints (1)
and (2). This means that the potential difference
pi,j,v + ui,v ? 1 can only be obtained if edges are
deleted on every path between ?New York City? and
?New York?, or if at least one of these two nodes is
removed from the distinctness assertion (by setting
the corresponding ui,v to non-zero values). Con-
straints (4), (5), (6) ensure non-negativity.
Having solved the linear program, the next ma-
jor step is to convert the optimal LP solution into
the final ? discrete ? solution. We cannot rely
on standard rounding methods to turn the optimal
fractional values of the de and ui,v variables into
a valid solution. Often, all solution variables have
small values and rounding will merely produce an
empty (C,U1, . . . , Un) = (?, ?, . . . , ?). Instead,
a more sophisticated technique is necessary. The
optimal solution of the LP can be used to define
an extended graph G? with a distance metric d be-
tween nodes. The algorithm then operates on this
graph, in each iteration selecting regions that be-
come output components and removing them from
the graph. A simple example is shown in Figure 2.
The extended graph contains additional nodes and
edges representing distinctness assertions. Cutting
one of these additional edges corresponds to re-
moving a node from a distinctness assertion.
Definition 6. Given G = (V,E) and distinct-
ness assertions D1, . . . , Dn with weights w(Di),
we define an undirected graph G? = (V ?, E?)
where V ? = V ? {vi,v | i = 1 . . . n, w(Di) >
0, v ?
?
j Di,j}, E
? = {e ? E | w(e) > 0} ?
{(v, vi,v) | v ? Di,j , w(Di) > 0}. We accordingly
extend the definition of w(e) to additionally cover
the new edges by defining w(e) = w(Di) for e =
(v, vi,v). We also extend it for sets S of edges by
defining w(S) =
?
e?S w(e). Finally, we define a
node distance metric
d(u, v) =
?
??????????
??????????
0 u = v
de (u, v) ? E
ui,v u = vi,v
ui,u v = vi,u
min
p?
P(u,v,E?)
?
(u?,v?)
?p
d(u?, v?) otherwise,
where P(u, v, E?) denotes the set of acyclic paths
between two nodes in E?. We further fix
c?f =
?
(u,v)?E?
d(u, v)w(e)
as the weight of the fractional solution of the LP
(c?f is a constant based on the original E?, irre-
spective of later modifications to the graph).
Definition 7. Around a given node v in G?, we
consider regions R(v, r) ? V with radius r. The
cut C(v, r) of a given region is defined as the set
of edges in G? with one endpoint within the region
and one outside the region:
R(v, r) = {v? ? V ? | d(v, v?) ? r}
C(v, r) = {e ? E? | |e ?R(v, r)| = 1}
For sets of nodes S ? V , we define R(S, r) =
?
v?S
R(v, r) and C(S, r) =
?
v?S
C(v, r).
847
Figure 2: Extended graph with two added nodes
v1,u, v1,v representing distinctness between ?Tele-
visio?n? and ?Televisor?, and a region around v1,u
that would cut the link from the Japanese ?Televi-
sion? to ?Televisor?
Definition 8. Given q = max
i,j
|Di,j |, we approxi-
mate the optimal cost of regions as:
c?(v, r) =
?
e=(u,u?)?E?:
e?R(v,r)
d(u, u?)w(e) (1)
+
?
e?C(v,r)
v??e?R(v,r)
(r ? d(v, v?))w(e)
c?(S, r) =
1
nq
c?f +
?
v?S
c?(v, r) (2)
The first summand accounts for the edges en-
tirely within the region, and the second one ac-
counts for the edges in C(v, r) to the extent that
they are within the radius. The definition of c?(S, r)
contains an additional slack component that is re-
quired for the approximation guarantee proof.
Based on these definitions, Algorithm 3.1 uses
the LP solution to construct the extended graph.
It then repeatedly, as long as there is an unsatis-
fied assertion Di, chooses a set S of nodes con-
taining one node from each relevant Di,j . Around
the nodes in S it simultaneously grows |S| regions
with the same radius, a technique previously sug-
gested by Avidor and Langberg (2007). These re-
gions are essentially output components that de-
termine the solution. Repeatedly choosing the
radius that minimizes w(C(S,r))c?(S,r) allows us to ob-
tain the approximation guarantee, because the dis-
tances in this extended graph are based on the so-
lution of the LP. The properties of this algorithm
are given by the following two theorems (proofs in
Appendix A).
Theorem 2. The algorithm yields a valid WDGS
solution (C,U1, . . . , Un).
Theorem 3. The algorithm yields a solution
(C,U1, . . . , Un) with an approximation factor of
4 ln(nq + 1) with respect to the cost of the op-
timal WDGS solution (C?, U?1 , . . . , U
?
n), where n
is the number of distinctness assertions and q =
max
i,j
|Di,j |. This solution can be obtained in poly-
nomial time.
4 Results
4.1 Wikipedia
We downloaded February 2010 XML dumps of
all available editions of Wikipedia, in total 272
editions that amount to 86.5 GB uncompressed.
From these dumps we produced two datasets.
Dataset A captures cross-lingual interwiki links
between pages, in total 77.07 million undirected
edges (146.76 million original links). Dataset
B additionally includes 2.2 million redirect-based
edges. Wikipedia deals with interwiki links to
redirects transparently, however there are many
redirects with titles that do not co-refer, e.g. redi-
rects from members of a band to the band, or from
aspects of a topic to the topic in general. We only
included redirects in the following cases:
? the titles of redirect and redirect target match
after Unicode NFKD normalization, diacrit-
ics removal, case conversion, and removal of
punctuation characters
? the redirect uses certain templates or cate-
gories that indicate co-reference with the tar-
get (alternative names, abbreviations, etc.)
We treated them like reciprocal interwiki links by
assigning them a weight of 2.
4.2 Application of Algorithm
The choice of distinctness assertion weights de-
pends on how lenient we wish to be towards con-
ceptual drift, allowing us to opt for more fine- or
more coarse-grained distinctions. In our experi-
ments, we decided to prefer fine-grained concep-
tual distinctions, and settled on a weight of 100.
We analysed over 20 million connected com-
ponents in each dataset, checking for distinctness
assertions. For the roughly 110,000 connected
components with relevant distinctness assertions,
848
Algorithm 3.1 WDGS Approximation Algorithm
1: procedure SELECT(V,E, V ?, E?, w,D1, . . . , Dn, l1, . . . , ln)
2: solve linear program given by Definition 5 . determine optimal fractional solution
3: construct G? = (V ?, E?) . extended graph (Definition 6)
4: C ? {e ? E | w(e) = 0} . cut zero-weighted edges
5: Ui ?
li?1?
j=1
Di,j ?i : w(Di) = 0 . remove zero-weighted Di
6: while ?i, j, k > j, u ? Di,j , v ? Di,k : P(vi,u, vi,v, E?) 6= ? do . find unsatisfied assertion
7: S ? ? . set of nodes around which regions will be grown
8: for all j in 1 . . . li ? 1 do . arbitrarily choose node from each Di,j
9: if ?v ? Di,j : vi,v ? V ? then S ? S ? vi,v
10: D ? {d(u, v) ? 12 | u ? S, v ? V
?} ? {12} . set of distances
11: choose  such that ?d, d? ? D : 0 <  |d? d?| . infinitesimally small
12: r ? argmin
r=d?: d?D\{0}
w(C(S, r))
c?(S, r)
. choose optimal radius (ties broken arbitrarily)
13: V ? ? V ? \R(S, r) . remove regions from G?
14: E? ? {e ? E? | e ? V ?}
15: C ? C ? (C(S, r) ? E) . update global solution
16: for all i? in 1 . . . n do
17: Ui? ? Ui? ? {v | (vi?,v, v) ? C(S, r)}
18: for all j in 1 . . . li? do Di?,j ? Di?,j ? V ? . prune distinctness assertions
19: return (C,U1, . . . , Un)
we applied our algorithm, relying on the commer-
cial CPLEX tool to solve the linear programs. In
most cases, the LP solving took less than a second,
however the LP sizes grow exponentially with the
number of nodes and hence the time complex-
ity increases similarly. In about 300 cases per
dataset, CPLEX took too long and was automat-
ically killed or the linear program was a priori
deemed too large to complete in a short amount
of time. For these cases, we adopted an alternative
strategy described later on.
Table 1 provides the experimental results for the
two datasets. Dataset B is more connected and
thus has fewer connected components with more
pairs of nodes asserted to be distinct by distinct-
ness assertions. The LP given by Definition 5
provides fractional solutions that constitute lower
bounds on the optimal solution (cf. also Lemma
5 in Appendix A), so the optimal solution can-
not have a cost lower than the fractional LP solu-
tion. Table 1 shows that in practice, our algorithm
achieves near-optimal results.
4.3 Linguistic Adequacy
The near-optimal results of our algorithm apply
with respect to our problem formalization, which
aims at repairing the graph in a minimally inva-
Table 1: Algorithm Results
Dataset A Dataset B
Connected
components
23,356,027 21,161,631
? with distinctness
assertions
112,857 113,714
? algorithm applied
successfully
112,580 113,387
Distinctness
assertions
380,694 379,724
Node pairs con-
sidered distinct
916,554 1,047,299
Lower bound on
optimal cost
1,255,111 1,245,004
Cost of our solution 1,306,747 1,294,196
Factor 1.04 1.04
Edges to be deleted
(undirected)
1,209,798 1,199,181
Nodes to be merged 603 573
sive way. It may happen, however, that the graph?s
topology is misleading, and that in a specific case
deleting many cross-lingual links to separate two
entities is more appropriate than looking for a
conservative way to separate them. This led us
849
to study the linguistic adequacy. Two annotators
evaluated 200 randomly selected separated pairs
from Dataset A consisting of an English and a
German article, with an inter-annotator agreement
(Cohen ?) of 0.656. Examples are given in Table
2. We obtained a precision of 87.97% ? 0.04%
(Wilson score interval) against the consensus an-
notation. Many of the errors are the result of ar-
ticles having many inaccurate outgoing links, in
which case they may be assigned to the wrong
component. In other cases, we noted duplicate ar-
ticles in Wikipedia.
Occasionally, we also observed differences in
scope, where one article would actually describe
two related concepts in a single page. Our algo-
rithm will then either make a somewhat arbitrary
assignment to the component of either the first or
second concept, or the broader generalization of
the two concepts becomes a separate, more gen-
eral connected component.
4.4 Large Problem Instances
When problem instances become too large, the lin-
ear programs can become too unwieldy for lin-
ear optimization software to cope with on current
hardware. In such cases, the graphs tend to be very
sparsely connected, consisting of many smaller,
more densely connected subgraphs. We thus in-
vestigated graph partitioning heuristics to decom-
pose larger graphs into smaller parts that can more
easily be handled with our algorithm. The METIS
algorithms (Karypis and Kumar, 1998) can de-
compose graphs with hundreds of thousands of
nodes almost instantly, but favour equally sized
clusters over lower cut costs. We obtained parti-
tionings with costs orders of magnitude lower us-
ing the heuristic by Dhillon et al (2007).
4.5 Database of Named Entities
The partitioning heuristics allowed us to process
all entries in the complete set of Wikipedia dumps
and produce a clean output set of connected com-
ponents where each Wikipedia article or category
belongs to a connected component consisting of
pages about the same entity or concept. We can re-
gard these connected components as equivalence
classes. This means that we obtain a large-scale
multilingual database of named entities and their
translations. We are also able to more safely trans-
fer information cross-lingually between editions.
For example, when an article a has a category c in
the French Wikipedia, we can suggest the corre-
sponding Indonesian category for the correspond-
ing Indonesian article.
Moreover, we believe that this database will
help extend resources like DBPedia and YAGO
that to date have exclusively used the English
Wikipedia as their repository of entities and
classes. With YAGO?s category heuristics, even
entirely non-English connected components can
be assigned a class in WordNet as long as at least
one of the relevant categories has an English page.
So, the French Wikipedia article on the Dutch
schooner ?JR Tolkien?, despite the lack of a cor-
responding English article, can be assigned to the
WordNet synset for ?ship?. Using YAGO?s plu-
ral heuristic to distinguish classes (Einstein is a
physicist) from topic descriptors (Einstein belongs
to the topic physics), we determined that over 4.8
million connected components can be linked to
WordNet, greatly surpassing the 3.2 million arti-
cles covered by the English Wikipedia alone.
5 Related Work
A number of projects have used Wikipedia as a
database of named entities (Ponzetto and Strube,
2007; Silberer et al, 2008). The most well-
known are probably DBpedia (Auer et al, 2007),
which serves as a hub in the Linked Data Web,
Freebase1, which combines human input and au-
tomatic extractors, and YAGO (Suchanek et al,
2007), which adds an ontological structure on top
of Wikipedia?s entities. Wikipedia has been used
cross-lingually for cross-lingual IR (Nguyen et al,
2009), question answering (Ferra?ndez et al, 2007)
as well as for learning transliterations (Pasternack
and Roth, 2009), among other things.
Mihalcea and Csomai (2007) have studied pre-
dicting new links within a single edition of
Wikipedia. Sorg and Cimiano (2008) considered
the problem of suggesting new cross-lingual links,
which could be used as additional inputs in our
problem. Adar et al (2009) and Bouma et al
(2009) show how cross-lingual links can be used
to propagate information from one Wikipedia?s in-
foboxes to another edition.
Our aggregation consistency algorithm uses
theoretical ideas put forward by researchers study-
ing graph cuts (Leighton and Rao, 1999; Garg et
al., 1996; Avidor and Langberg, 2007). Our prob-
lem setting is related to that of correlation cluster-
ing (Bansal et al, 2004), where a graph consist-
1http://www.freebase.com/
850
Table 2: Examples of separated concepts
English concept German concept
(translated)
Explanation
Coffee percolator French Press different types of brewing devices
Baqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a merger
of Baqa al-Gharbiyye and Jatt
Leucothoe (plant) Leucothea (Orchamos) the second refers to a figure of Greek
mythology
Old Belarusian language Ruthenian language the second is often considered slightly
broader
ing of positively and negatively labelled similar-
ity edges is clustered such that similar items are
grouped together, however our approach is much
more generic than conventional correlation clus-
tering. Charikar et al (2005) studied a variation
of correlation clustering that is similar to WDGS,
but since a negative edge would have to be added
between each relevant pair of entities in a distinct-
ness assertion, the approximation guarantee would
only be O(log(n |V |2)). Minimally invasive re-
pair operations on graphs have also been stud-
ied for graph similarity computation (Zeng et al,
2009), where two graphs are provided as input.
6 Conclusions and Future Work
We have presented an algorithmic framework for
the problem of co-reference that produces consis-
tent partitions by intelligently removing edges or
allowing nodes to remain connected. This algo-
rithm has successfully been applied to Wikipedia?s
cross-lingual graph, where we identified and elim-
inated surprisingly large numbers of inaccurate
connections, leading to a large-scale multilingual
register of names.
In future work, we would like to investigate
how our algorithm behaves in extended settings,
e.g. we can use heuristics to connect isolated,
unconnected articles to likely candidates in other
Wikipedias using weighted edges. This can be
extended to include mappings from multiple lan-
guages to WordNet synsets, with the hope that
the weights and link structure will then allow the
algorithm to make the final disambiguation deci-
sion. Additional scenarios include dealing with
co-reference on the Linked Data Web or mappings
between thesauri. As such resources are increas-
ingly being linked to Wikipedia and DBpedia, we
believe that our techniques will prove useful in
making mappings more consistent.
A Proofs
Proof (Theorem 1). We shall reduce the mini-
mum multicut problem to WDGS. The hardness
claims then follow from Chawla et al (2005).
Given a graph G = (V,E) with a positive cost
c(e) for each e ? E, and a set D = {(si, ti) | i =
1 . . . k} of k demand pairs, our goal is to find
a multicut M with respect to D with minimum
total cost
?
e?M c(e). We convert each demand
pair (si, ti) into a distinctness assertion Di =
({si}, {ti}) with weight w(Di) = 1+
?
e?E c(e).
An optimal WDGS solution (C,U1, . . . , Uk) with
cost c then implies a multicut C with the same
weight, because each w(Di) >
?
e?E c(e), so
all demand pairs will be satisfied. C is a minimal
multicut because any multicut C ? with lower cost
would imply a valid WDGS solution (C ?, ?, . . . , ?)
with a cost lower than the optimal one, which is a
contradiction.
Lemma 4. The linear program given by Defini-
tion 5 enforces that for any i,j,k 6= j,u ? Di,j ,
v ? Di,k, and any path v0, . . . , vt with v0 = u,
vt = v we obtain ui,u+
?t?1
l=0 d(vl,vl+1)+ui,v ? 1.
The integer linear program obtained by aug-
menting Definition 5 with integer constraints
de, ui,v, pi,j,v ? {0, 1} (for all applicable e, i, j,
v) produces optimal solutions (C,U1, . . . , Uk) for
WDGS problems, obtained as C = ({e ? E | de =
1}, Ui = {v | ui,v = 1}.
Proof. Without loss of generality, let us assume
that j < k. The LP constraints give us pi,j,vt ?
pi,j,vt?1 +d(vt?1,vt), . . . , pi,j,v1 ? pi,j,v0 +d(v0,v1),
as well as pi,j,v0 = ui,u and pi,j,vt + ui,v ? 1.
Hence 1 ? pi,j,vt+ui,v ? ui,u+
?t?1
l=0 d(vl,vl+1)+
ui,v.
With added integrality constraints, we obtain ei-
ther u ? Ui, v ? Ui, or at least one edge along any
path from u to v is cut, i.e. P(u, v, E \ C) = ?.
851
This proves that any ILP solution enduces a valid
WDGS solution (Definition 2).
Clearly, the integer program?s objective func-
tion minimizes c(C,U1, . . . , Un) (Definition 3) if
C = ({e ? E | de = 1}, Ui = {v | ui,v = 1}.
To see that the solutions are optimal, it thus suf-
fices to observe that any optimal WDGS solution
(C?, U?1 , . . . , U
?
n) yields a feasible ILP solution
de = IC?(e), ui,v = IU?i (v).
Proof (Theorem 2). ri < 12 holds for any ra-
dius ri chosen by the algorithm, so for any re-
gion R(v0, r) grown around a node v0, and any
two nodes u, v within that region, the triangle in-
equality gives us d(u, v) ? d(u, v0) + d(v0, v) <
1
2 +
1
2 = 1 (maximal distance condition). At
the same time, by Lemma 4 and Definition 6 for
any u ? Di,j , v ? Di,k (j 6= k), we obtain
d(vi,u, vi,v) = d(vi,u, u) + d(u, v) + d(v, vi,v) ?
1. With the maximal distance condition above, this
means that vi,u and vi,v cannot be in the same re-
gion. Hence u, v cannot be in the same region,
unless the edge from vi,u to u is cut (in which case
u will be placed in Ui) or the edge from v to vi,v
is cut (in which case v will be placed in Ui). Since
each region is separated from other regions via C,
we obtain that ?i, j, k 6= j, u, v: u ? Di,j \ Ui,
v ? Di,k \ Ui implies P(u, v, E \ C) = ?, so a
valid solution is obtained.
Lemma 5 (essentially due to Garg et al (1996)).
For any i where ?j, k > j, u ? Di,j , v ? Di,k :
P(vi,u, vi,v, E?) 6= ? and w(Di) > 0, there exists
an r such that w(C(S, r)) ? 2 ln(nq + 1) c?(S, r),
0 ? r < 12 for any set S consisting of vi,v nodes.
Proof. Define w(S, r) =
?
v?S w(C(v, r)). We
will prove that there exists an appropriate r with
w(C(S, r)) ? w(S, r) ? 2 ln(nq+1) c?(S, r). As-
sume, for reductio ad absurdum, that ?r ? [0, 12) :
w(S, r) > 2 ln(nq + 1)c?(S, r). As we expand
the radius r, we note that c?(S, r) ddr = w(S, r)
whereever c? is differentiable with respect to r.
There are only a finite number of points r1,. . . ,rl?1
in (0, 12) where this is not the case (namely, when
?u ? S, v ? V ? : d(u, v) = ri). Also note
that c? increases monotonically for increasing val-
ues of r, and that it is universally greater than
zero (since there is a path between vi,u, vi,v). Set
r0 = 0, rl = 12 and choose  such that 0 <  
min{rj+1 ? rj | j < l}. Our assumption then
implies:
l?
j=1
? rj?
rj?1+
w(S,r)
c?(S,r) dr
>
[
l?
j=1
rj ? rj?1 ? 2
]
2 ln(nq + 1)
l?
j=1
ln c?(S, rj ? )? ln c?(S, rj?1 + )
>
(
1
2 ? 2l
)
2 ln(nq + 1)
ln c?(S, 12 ? )? ln c?(S, 0)
> (1? 4l) ln(nq + 1)
c?(S, 12?)
c?(S,0) > (nq + 1)
1?4l
c?(S, 12 ? ) > (nq + 1)
1?4lc?(S, 0)
For small , the right term can get arbitrarily close
to (nq+1)c?(S, 0) ? c?f + c?(S, 0), which is strictly
larger than c?(S, 12 ? ) no matter how small  be-
comes, so the initial assumption is false.
Proof (Theorem 3). Let Si, ri denote the set
S and radius r chosen in particular iterations,
and ci the corresponding costs incurred: ci =
w(C(Si, r) ? E) + |Ui|w(Di) = w(C(Di, r)).
Note that any ri chosen by the algorithm will in
fact fulfil the criterion described by Lemma 5, be-
cause ri is chosen to minimize the ratio between
the two terms, and the minimizing r ? [0, 12)
must be among the r considered by the algo-
rithm (w(C(Di, r)) only changes at one of those
points, so the minimum is reached by approach-
ing the points from the left). Hence, we obtain
ci ? 2 ln(n+ 1)c?(Si, ri). For our global solution,
note that there is no overlap between the regions
chosen within an iteration, since regions have a
radius strictly smaller than 12 , while vi,u, vi,v for
u ? Di,j , v ? Di,k, j 6= k have a distance of
at least 1. Nor is there any overlap between re-
gions from different iterations, because in each it-
eration the selected regions are removed from G?.
Globally, we therefore obtain c(C,U1, . . . , Un) =?
i ci < 2 ln(nq + 1)
?
i c?(Si, ri) ? 2 ln(nq +
1)2c?f (observe that i ? nq). Since c?f is the ob-
jective score for the fractional LP relaxation solu-
tion of the WDGS ILP (Lemma 4), we obtain c?f ?
c(C?, U?1 , . . . , U
?
n), and thus c(C,U1, . . . , Un) <
4 ln(n+ 1)c(C?, U?1 , . . . , U
?
n).
To obtain a solution in polynomial time, note
that the LP size is polynomial with respect to nq
and may be solved using a polynomial algorithm
(Karmarkar, 1984). The subsequent steps run in
O(nq) iterations, each growing up to |V | regions
using O(|V |2) uniform cost searches.
852
References
Eytan Adar, Michael Skinner, and Daniel S. Weld.
2009. Information arbitrage across multi-lingual
Wikipedia. In Ricardo A. Baeza-Yates, Paolo Boldi,
Berthier A. Ribeiro-Neto, and Berkant Barla Cam-
bazoglu, editors, Proceedings of the 2nd Interna-
tional Conference on Web Search and Web Data
Mining, WSDM 2009, pages 94?103. ACM.
So?ren Auer, Chris Bizer, Jens Lehmann, Georgi Kobi-
larov, Richard Cyganiak, and Zachary Ives. 2007.
DBpedia: a nucleus for a web of open data. In
Aberer et al, editor, The Semantic Web, 6th Interna-
tional Semantic Web Conference, 2nd Asian Seman-
tic Web Conference, ISWC 2007 + ASWC 2007, Bu-
san, Korea, November 11?15, 2007, Lecture Notes
in Computer Science 4825. Springer.
Adi Avidor and Michael Langberg. 2007. The multi-
multiway cut problem. Theoretical Computer Sci-
ence, 377(1-3):35?42.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89?113.
Gosse Bouma, Sergio Duarte, and Zahurul Islam.
2009. Cross-lingual alignment and completion of
Wikipedia templates. In CLIAWS3 ?09: Proceed-
ings of the Third International Workshop on Cross
Lingual Information Access, pages 21?29, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. 2005. Clustering with qualitative informa-
tion. Journal of Computer and System Sciences,
71(3):360?383.
Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yu-
val Rabani, and D. Sivakumar. 2005. On the hard-
ness of approximating multicut and sparsest-cut. In
In Proceedings of the 20th Annual IEEE Conference
on Computational Complexity, pages 144?153.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors.
a multilevel approach. IEEE Trans. Pattern Anal.
Mach. Intell., 29(11):1944?1957.
Sergio Ferra?ndez, Antonio Toral, O?scar Ferra?ndez, An-
tonio Ferra?ndez, and Rafael Mun?oz. 2007. Ap-
plying Wikipedia?s multilingual knowledge to cross-
lingual question answering. In NLDB, pages 352?
363.
Naveen Garg, Vijay V. Vazirani, and Mihalis Yan-
nakakis. 1996. Approximate max-flow min-
(multi)cut theorems and their applications. SIAM
Journal on Computing (SICOMP), 25:698?707.
Narendra Karmarkar. 1984. A new polynomial-time
algorithm for linear programming. In STOC ?84:
Proceedings of the 16th Annual ACM Symposium on
Theory of Computing, pages 302?311, New York,
NY, USA. ACM.
George Karypis and Vipin Kumar. 1998. A fast and
high quality multilevel scheme for partitioning irreg-
ular graphs. SIAM Journal on Scientific Computing,
20(1):359?392.
Subhash Khot. 2002. On the power of unique 2-prover
1-round games. In STOC ?02: Proceedings of the
34th Annual ACM Symposium on Theory of Com-
puting, pages 767?775, New York, NY, USA. ACM.
Tom Leighton and Satish Rao. 1999. Multicommodity
max-flow min-cut theorems and their use in design-
ing approximation algorithms. Journal of the ACM,
46(6):787?832.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
Proceedings of the 16th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2007),
pages 233?242, New York, NY, USA. ACM.
D. Nguyen, A. Overwijk, C. Hauff, R.B. Trieschnigg,
D. Hiemstra, and F.M.G. Jong de. 2009. Wiki-
Translate: query translation for cross-lingual infor-
mation retrieval using only Wikipedia. In Carol
Peters, Thomas Deselaers, Nicola Ferro, and Julio
Gonzalo, editors, Evaluating Systems for Multilin-
gual and Multimodal Information Access, Lecture
Notes in Computer Science 5706, pages 58?65.
Jeff Pasternack and Dan Roth. 2009. Learning bet-
ter transliterations. In CIKM ?09: Proceeding of the
18th ACM Conference on Information and Knowl-
edge Management, pages 177?186, New York, NY,
USA. ACM.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
AAAI 2007: Proceedings of the 22nd Conference
on Artificial Intelligence, pages 1440?1445. AAAI
Press.
Carina Silberer, Wolodja Wentland, Johannes Knopp,
and Matthias Hartung. 2008. Building a multilin-
gual lexical resource for named entity disambigua-
tion, translation and transliteration. In European,
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Philipp Sorg and Philipp Cimiano. 2008. Enrich-
ing the crosslingual link structure of Wikipedia - a
classification-based approach. In Proceedings of the
AAAI 2008 Workshop on Wikipedia and Artifical In-
telligence.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In Proceedings of the 16th International
World Wide Web conference, WWW, New York, NY,
USA. ACM Press.
Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang,
Jianhua Feng, and Lizhu Zhou. 2009. Comparing
stars: On approximating graph edit distance. Pro-
ceedings of the VLDB Endowment, 2(1):25?36.
853
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
UWN: A Large Multilingual Lexical Knowledge Base
Gerard de Melo
ICSI Berkeley
demelo@icsi.berkeley.edu
Gerhard Weikum
Max Planck Institute for Informatics
weikum@mpi-inf.mpg.de
Abstract
We present UWN, a large multilingual lexi-
cal knowledge base that describes the mean-
ings and relationships of words in over 200
languages. This paper explains how link pre-
diction, information integration and taxonomy
induction methods have been used to build
UWN based on WordNet and extend it with
millions of named entities from Wikipedia.
We additionally introduce extensions to cover
lexical relationships, frame-semantic knowl-
edge, and language data. An online interface
provides human access to the data, while a
software API enables applications to look up
over 16 million words and names.
1 Introduction
Semantic knowledge about words and named enti-
ties is a fundamental building block both in vari-
ous forms of language technology as well as in end-
user applications. Examples of the latter include
word processor thesauri, online dictionaries, ques-
tion answering, and mobile services. Finding se-
mantically related words is vital for query expan-
sion in information retrieval (Gong et al, 2005),
database schema matching (Madhavan et al, 2001),
sentiment analysis (Godbole et al, 2007), and ontol-
ogy mapping (Jean-Mary and Kabuka, 2008). Fur-
ther uses of lexical knowledge include data cleaning
(Kedad and M?tais, 2002), visual object recognition
(Marsza?ek and Schmid, 2007), and biomedical data
analysis (Rubin and others, 2006).
Many of these applications have used English-
language resources like WordNet (Fellbaum, 1998).
However, a more multilingual resource equipped
with an easy-to-use API would not only enable us to
perform all of the aforementioned tasks in additional
languages, but also to explore cross-lingual applica-
tions like cross-lingual IR (Etzioni et al, 2007) and
machine translation (Chatterjee et al, 2005).
This paper describes a new API that makes lexical
knowledge about millions of items in over 200 lan-
guages available to applications, and a correspond-
ing online user interface for users to explore the data.
We first describe link prediction techniques used to
create the multilingual core of the knowledge base
with word sense information (Section 2). We then
outline techniques used to incorporate named enti-
ties and specialized concepts (Section 3) and other
types of knowledge (Section 4). Finally, we describe
how the information is made accessible via a user in-
terface (Section 5) and a software API (Section 6).
2 The UWN Core
UWN (de Melo and Weikum, 2009) is based on
WordNet (Fellbaum, 1998), the most popular lexi-
cal knowledge base for the English language. Word-
Net enumerates the senses of a word, providing a
short description text (gloss) and synonyms for each
meaning. Additionally, it describes relationships be-
tween senses, e.g. via the hyponymy/hypernymy re-
lation that holds when one term like ?publication? is
a generalization of another term like ?journal?.
This model can be generalized by allowing words
in multiple languages to be associated with a mean-
ing (without, of course, demanding every meaning
be lexicalized in every language). In order to ac-
complish this at a large scale, we automatically link
151
terms in different languages to the meanings already
defined in WordNet. This transforms WordNet into
a multilingual lexical knowledge base that covers
not only English terms but hundreds of thousands
of terms from many different languages.
Unfortunately, a straightforward translation runs
into major difficulties because of homonyms and
synonyms. For example, a word like ?bat? has 10
senses in the English WordNet, but a German trans-
lation like ?Fledermaus? (the animal) only applies to
a small subset of those senses (cf. Figure 1). This
challenge can be approached by disambiguating us-
ing machine learning techniques.
Figure 1: Word sense ambiguity
Knowledge Extraction An initial input knowl-
edge base graph G0 is constructed by ex-
tracting information from existing wordnets,
translation dictionaries including Wiktionary
(http://www.wiktionary.org), multilingual thesauri
and ontologies, and parallel corpora. Additional
heuristics are applied to increase the density of the
graph and merge near-duplicate statements.
Link Prediction A sequence of knowledge graphs
Gi are iteratively derived by assessing paths from
a new term x to an existing WordNet sense z via
some English translation y covered by WordNet. For
instance, the German ?Fledermaus? has ?bat? as a
translation and hence initially is tentatively linked to
all senses of ?bat? with a confidence of 0. In each
iteration, the confidence values are then updated to
reflect how likely it seems that those links are cor-
rect. The confidences are predicted using RBF-
kernel SVM models that are learnt from a training
set of labelled links between non-English words and
senses. The feature space is constructed using a se-
ries of graph-based statistical scores that represent
properties of the previous graph Gi?1 and addition-
ally make use of measures of semantic relatedness
and corpus frequencies. The most salient features
xi(x, z) are of the form:
?
y??(x,Gi?1)
?(x, y) sim?x(y, z) (1)
?
y??(x,Gi?1)
?(x, y) sim?x(y, z)
sim?x(y, z) + dissimx(y, z)
(2)
The formulae consider the out-neighbourhood y ?
?(x,Gi?1) of x, i.e. its translations, and then ob-
serve how strongly each y is tied to z. The function
sim? computes the maximal similarity between any
sense of y and the current sense z. The dissim func-
tion computes the sum of dissimilarities between
senses of y and z, essentially quantifying how many
alternatives there are to z. Additional weighting
functions ?, ? are used to bias scores towards senses
that have an acceptable part-of-speech and senses
that are more frequent in the SemCor corpus.
Relying on multiple iterations allows us to draw
on multilingual evidence for greater precision and
recall. For instance, after linking the German ?Fled-
ermaus? to the animal sense of ?bat?, we may be able
to infer the same for the Turkish translation ?yarasa?.
Results We have successfully applied these tech-
niques to automatically create UWN, a large-scale
multilingual wordnet. Evaluating random samples
of term-sense links, we find (with Wilson-score in-
tervals at ? = 0.05) that for French the preci-
sion is 89.2% ? 3.4% (311 samples), for German
85.9% ? 3.8% (321 samples), and for Mandarin
Chinese 90.5% ? 3.3% (300 samples). The over-
all number of new term-sense links is 1,595,763, for
822,212 terms in over 200 languages. These figures
can be grown further if the input is extended by tap-
ping on additional sources of translations.
3 MENTA: Named Entities and
Specialized Concepts
The UWN Core is extended by incorporating large
amounts of named entities and language- and
domain-specific concepts from Wikipedia (de Melo
and Weikum, 2010a). In the process, we also obtain
152
human-readable glosses in many languages, links to
images, and other valuable information. These ad-
ditions are not simply added as a separate knowl-
edge base, but fully connected and integrated with
the core. In particular, we create a mapping between
Wikipedia and WordNet in order to merge equiva-
lent entries and we use taxonomy construction meth-
ods in order to attach all new named entities to their
most likely classes, e.g. ?Haight-Ashbury? is linked
to a WordNet sense of the word ?neighborhood?.
Information Integration Supervised link predic-
tion, similar to the method presented in Section 2, is
used in order to attach Wikipedia articles to semanti-
cally equivalent WordNet entries, while also exploit-
ing gloss similarity as an additional feature. Addi-
tionally, we connect articles from different multilin-
gual Wikipedia editions via their cross-lingual inter-
wiki links, as well as categories with equivalent ar-
ticles and article redirects with redirect targets.
We then consider connected components of di-
rectly or transitively linked items. In the ideal case,
such a connected component consists of a number
of items all describing the same concept or entity, in-
cluding articles from different versions of Wikipedia
and perhaps also categories or WordNet senses.
Unfortunately, in many cases one obtains con-
nected components that are unlikely to be correct,
because multiple articles from the same Wikipedia
edition or multiple incompatible WordNet senses are
included in the same component. This can be due
to incorrect links produced by the supervised link
prediction, but often even the original links from
Wikipedia are not consistent.
In order to obtain more consistent connected com-
ponents, we use combinatorial optimization meth-
ods to delete certain links. In particular, for each
connected component to be analysed, an Integer
Linear Program formalizes the objective of mini-
mizing the costs for deleted edges and the costs for
ignoring soft constraints. The basic aim is that of
deleting as few edges as possible while simultane-
ously ensuring that the graph becomes as consistent
as possible. In some cases, there is overwhelming
evidence indicating that two slightly different arti-
cles should be grouped together, while in other cases
there might be little evidence for the correctness of
an edge and so it can easily be deleted with low cost.
While obtaining an exact solution is NP-hard and
APX-hard, we can solve the corresponding Linear
Program using a fast LP solver like CPLEX and sub-
sequently apply region growing techniques to obtain
a solution with a logarithmic approximation guaran-
tee (de Melo and Weikum, 2010b).
The clean connected components resulting from
this process can then be merged to form aggregate
entities. For instance, given WordNet?s standard
sense for ?fog?, water vapor, we can check which
other items are in the connected component and
transfer all information to the WordNet entry. By
extracting snippets of text from the beginning of
Wikipedia articles, we can add new gloss descrip-
tions for fog in Arabic, Asturian, Bengali, and many
other languages. We can also attach pictures show-
ing fog to the WordNet word sense.
Taxonomy Induction The above process con-
nects articles to their counterparts in WordNet. In
the next step, we ensure that articles without any di-
rect counterpart are linked to WordNet as well, by
means of taxonomic hypernymy/instance links (de
Melo and Weikum, 2010a).
We generate individual hypotheses about likely
parents of entities. For instance, articles are con-
nected to their Wikipedia categories (if these are not
assessed to be mere topic descriptors) and categories
are linked to parent categories, etc. In order to link
categories to possible parent hypernyms in Word-
Net, we adapt the approach proposed for YAGO
(Suchanek et al, 2007) of determining the headword
of the category name and disambiguating it.
Since we are dealing with a multilingual scenario
that draws on articles from different multilingual
Wikipedia editions that all need to be connected to
WordNet, we apply an algorithm that jointly looks
at an entity and all of its parent candidates (not just
from an individual article, but all articles in the same
connected component) as well as superordinate par-
ent candidates (parents of parents, etc.), as depicted
in Figure 2. We then construct a Markov chain based
on this graph of parents that also incorporates the
possibility of random jumps from any parent back
to the current entity under consideration. The sta-
tionary probability of this Markov chain, which can
be obtained using random walk methods, provides
us a ranking of the most likely parents.
153
Figure 2: Noisy initial edges (left) and cleaned, integrated output (right), shown in a simplified form
Figure 3: UWN with named entities
Results Overall, we obtain a knowledge base with
5.4 million concepts or entities and 16.7 million
words or names associated with them from over
200 languages. Over 2 million named entities come
only from non-English Wikipedia editions, but their
taxonomic links to WordNet still have an accuracy
around 90%. An example excerpt is shown in Fig-
ure 3, with named entities connected to higher-level
classes in UWN, all with multilingual labels.
4 Other Extensions
Word Relationships Another plugin provides
word relationships and properties mined from Wik-
tionary. These include derivational and etymologi-
cal word relationships (e.g. that ?grotesque? comes
from the Italian ?grotta?: grotto, artificial cave), al-
ternative spellings (e.g. ?encyclop?dia? for ?en-
cyclopedia?), common misspellings (e.g. ?minis-
cule? for ?minuscule?), pronunciation information
(e.g. how to pronounce ?nuclear?), and so on.
Frame-Semantic Knowledge Frame semantics is
a cognitively motivated theory that describes words
in terms of the cognitive frames or scenarios that
they evoke and the corresponding participants in-
volved in them. For a given frame, FrameNet
provides definitions, involved participants, associ-
ated words, and relationships. For instance, the
Commerce_goods-transfer frame normally
involves a seller and a buyer, among other things,
and different words like ?buy? and ?sell? can be cho-
sen to describe the same event.
Such detailed knowledge about scenarios is
largely complementary in nature to the sense re-
lationships that WordNet provides. For instance,
WordNet emphasizes the opposite meaning of the
words ?happy? and ?unhappy?, while frame seman-
tics instead emphasizes the cognitive relatedness of
words like ?happy?, ?unhappy?, ?astonished?, and
?amusement?, and explains that typical participants
include an experiencer who experiences the emo-
tions and external stimuli that evoke them. There
have been individual systems that made use of both
forms of knowledge (Shi and Mihalcea, 2005; Cop-
pola and others, 2009), but due to their very different
nature, there is currently no simple way to accom-
plish this feat. Our system addresses this by seam-
lessly integrating frame semantic knowledge into the
system. We draw on FrameNet (Baker et al, 1998),
the most well-known computational instantiation of
frame semantics. While the FrameNet project is
generally well-known, its use in practical applica-
154
tions has been limited due to the lack of easy-to-use
APIs and because FrameNet alne does not cover as
many words as WordNet. Our API simultaneously
provides access to both sources.
Language information For a given language, this
extension provides information such as relevant
writing systems, geographical regions, identifica-
tion codes, and names in many different languages.
These are all integrated into WordNet?s hypernym
hierarchy, i.e. from language families like the Sinitic
languages one may move down to macrolanguages
like Chinese, and then to more specific forms like
Mandarin Chinese, dialect groups like Ji-Lu Man-
darin, or even dialects of particular cities.
The information is obtained from ISO standards,
the Unicode CLDR as well as Wikipedia and then
integrated with WordNet using the information in-
tegration strategies described above (de Melo and
Weikum, 2008). Additionally, information about
writing systems is taken from the Unicode CLDR
and information about individual characters is ob-
tained from the Unicode, Unihan, and Hanzi Data
databases. For instance, the Chinese character ???
is connected to its radical component ??? and to its
pronunciation component ???.
5 Integrated Query Interface and Wiki
We have developed an online interface that provides
access to our data to interested researchers (yago-
knowledge.org/uwn/ ), as shown in Figure 4.
Interactive online interfaces offer new ways of in-
teracting with lexical knowledge that are not possi-
ble with traditional print dictionaries. For example,
a user wishing to find a Spanish word for the concept
of persuading someone not to believe something
might look up the word ?persuasion? and then navi-
gate to its antonym ?dissuasion? to find the Spanish
translation. A non-native speaker of English looking
up the word ?tercel? might find it helpful to see pic-
tures available for the related terms ?hawk? or ?fal-
con? ? a Google Image search for ?tercel? merely de-
livers images of Toyota Tercel cars.
While there have been other multilingual inter-
faces to WordNet-style lexical knowledge in the past
(Pianta et al, 2002; Atserias and others, 2004), these
provide less than 10 languages as of 2012. The most
similar resource is BabelNet (Navigli and Ponzetto,
2010), which contains multilingual synsets but does
not connect named entities from Wikipedia to them
in a multilingual taxonomy.
Figure 4: Part of Online Interface
6 Integrated API
Our goal is to make the knowledge that we have de-
rived available for use in applications. To this end,
we have developed a fully downloadable API that
can easily be used in several different programming
languages. While there are many existing APIs for
WordNet and other lexical resources (e.g. (Judea et
al., 2011; Gurevych and others, 2012)), these don?t
provide a comparable degree of integrated multilin-
gual and taxonomic information.
Interface The API can be used by initializing an
accessor object and possibly specifying the list of
plugins to be loaded. Depending on the particular
application, one may choose only Princeton Word-
Net and the UWN Core, or one may want to in-
clude named entities from Wikipedia and frame-
semantic knowledge derived from FrameNet, for in-
stance. The accessor provides a simple graph-based
lookup API as well as some convenience methods
for common types of queries.
An additional higher-level API module imple-
ments several measures of semantic relatedness. It
also provides a simple word sense disambiguation
method that, given a tokenized text with part-of-
155
speech and lemma annotations, selects likely word
senses by choosing the senses (with matching part-
of-speech) that are most similar to words in the con-
text. Note that these modules go beyond existing
APIs because they operate on words in many differ-
ent languages and semantic similarity can even be
assessed across languages.
Data Structures Under the hood, each plugin re-
lies on a disk-based associative array to store the
knowledge base as a labelled multi-graph. The out-
going labelled edges of an entity are saved on disk in
a serialized form, including relation names and rela-
tion weights. An index structure allows determining
the position of such records on disk.
Internally, this index structure is implemented as
a linearly-probed hash table that is also stored ex-
ternally. Note that such a structure is very efficient
in this scenario, because the index is used as a read-
only data store by the API. Once an index has been
created, write operations are no longer performed,
so B+ trees and similar disk-based balanced tree in-
dices commonly used in relational database manage-
ment systems are not needed. The advantage is that
this enables faster lookups, because retrieval opera-
tions normally require only two disk reads per plu-
gin, one to access a block in the index table, and
another to access a block of actual data.
7 Conclusion
UWN is an important new multilingual lexical re-
source that is now freely available to the community.
It has been constructed using sophisticated knowl-
edge extraction, link prediction, information integra-
tion, and taxonomy induction methods. Apart from
an online querying and browsing interface, we have
also implemented an API that facilitates the use of
the knowledge base in applications.
References
Jordi Atserias et al 2004. The MEANING multilingual
central repository. In Proc. GWC 2004.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc.
COLING-ACL 1998.
Niladri Chatterjee, Shailly Goyal, and Anjali Naithani.
2005. Resolving pattern ambiguity for English to
Hindi machine translation using WordNet. In Proc.
Workshop Translation Techn. at RANLP 2005.
Bonaventura Coppola et al 2009. Frame detection over
the Semantic Web. In Proc. ESWC.
Gerard de Melo and Gerhard Weikum. 2008. Language
as a foundation of the Semantic Web. In Proc. ISWC.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proc. CIKM 2009.
Gerard de Melo and Gerhard Weikum. 2010a. MENTA:
Inducing multilingual taxonomies from Wikipedia. In
Proc. CIKM 2010.
Gerard de Melo and Gerhard Weikum. 2010b. Untan-
gling the cross-lingual link structure of Wikipedia. In
Proc. ACL 2010.
Oren Etzioni, Kobi Reiter, Stephen Soderland, and Mar-
cus Sammer. 2007. Lexical translation with applica-
tion to image search on the Web. In Proc. MT Summit.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for news
and blogs. In Proc. ICWSM.
Zhiguo Gong, Chan Wa Cheang, and Leong Hou U.
2005. Web query expansion by WordNet. In Proc.
DEXA 2005.
Iryna Gurevych et al 2012. Uby: A large-scale uni-
fied lexical-semantic resource based on LMF. In Proc.
EACL 2012.
Yves R. Jean-Mary and Mansur R. Kabuka. 2008. AS-
MOV: Results for OAEI 2008. In Proc. OM 2008.
Alex Judea, Vivi Nastase, and Michael Strube. 2011.
WikiNetTk ? A tool kit for embedding world knowl-
edge in NLP applications. In Proc. IJCNLP 2011.
Zoubida Kedad and Elisabeth M?tais. 2002. Ontology-
based data cleaning. In Proc. NLDB 2002.
Jayant Madhavan, P. Bernstein, and E. Rahm. 2001.
Generic schema matching with Cupid. In Proc. VLDB.
Marcin Marsza?ek and C. Schmid. 2007. Semantic hier-
archies for visual object recognition. In Proc. CVPR.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proc. ACL 2010.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing an aligned
multilingual database. In Proc. GWC.
Daniel L. Rubin et al 2006. National Center for Biomed-
ical Ontology. OMICS, 10(2):185?98.
Lei Shi and Rada Mihalcea. 2005. Putting the pieces to-
gether: Combining FrameNet, VerbNet, and WordNet
for robust semantic parsing. In Proc. CICLing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proc. WWW 2007.
156
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1041?1051,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Structured Learning for Taxonomy Induction with Belief Propagation
Mohit Bansal
TTI Chicago
mbansal@ttic.edu
David Burkett
Twitter Inc.
dburkett@twitter.com
Gerard de Melo
Tsinghua University
gdm@demelo.org
Dan Klein
UC Berkeley
klein@cs.berkeley.edu
Abstract
We present a structured learning approach
to inducing hypernym taxonomies using a
probabilistic graphical model formulation.
Our model incorporates heterogeneous re-
lational evidence about both hypernymy
and siblinghood, captured by semantic
features based on patterns and statistics
from Web n-grams and Wikipedia ab-
stracts. For efficient inference over tax-
onomy structures, we use loopy belief
propagation along with a directed span-
ning tree algorithm for the core hyper-
nymy factor. To train the system, we ex-
tract sub-structures of WordNet and dis-
criminatively learn to reproduce them, us-
ing adaptive subgradient stochastic opti-
mization. On the task of reproducing
sub-hierarchies of WordNet, our approach
achieves a 51% error reduction over a
chance baseline, including a 15% error re-
duction due to the non-hypernym-factored
sibling features. On a comparison setup,
we find up to 29% relative error reduction
over previous work on ancestor F1.
1 Introduction
Many tasks in natural language understanding,
such as question answering, information extrac-
tion, and textual entailment, benefit from lexical
semantic information in the form of types and hy-
pernyms. A recent example is IBM?s Jeopardy!
system Watson (Ferrucci et al, 2010), which used
type information to restrict the set of answer can-
didates. Information of this sort is present in term
taxonomies (e.g., Figure 1), ontologies, and the-
sauri. However, currently available taxonomies
such as WordNet are incomplete in coverage (Pen-
nacchiotti and Pantel, 2006; Hovy et al, 2009),
unavailable in many domains and languages, and
vertebrate
mammal
placental
cow rodent
squirrel rat
metatherian
marsupial
kangaroo
reptile
diapsid
snake crocodilian
anapsid
chelonian
turtle
1
Figure 1: An excerpt of WordNet?s vertebrates taxonomy.
time-intensive to create or extend manually. There
has thus been considerable interest in building lex-
ical taxonomies automatically.
In this work, we focus on the task of taking col-
lections of terms as input and predicting a com-
plete taxonomy structure over them as output. Our
model takes a loglinear form and is represented
using a factor graph that includes both 1st-order
scoring factors on directed hypernymy edges (a
parent and child in the taxonomy) and 2nd-order
scoring factors on sibling edge pairs (pairs of hy-
pernym edges with a shared parent), as well as in-
corporating a global (directed spanning tree) struc-
tural constraint. Inference for both learning and
decoding uses structured loopy belief propagation
(BP), incorporating standard spanning tree algo-
rithms (Chu and Liu, 1965; Edmonds, 1967; Tutte,
1984). The belief propagation approach allows us
to efficiently and effectively incorporate hetero-
geneous relational evidence via hypernymy and
siblinghood (e.g., coordination) cues, which we
capture by semantic features based on simple sur-
face patterns and statistics from Web n-grams and
Wikipedia abstracts. We train our model to max-
imize the likelihood of existing example ontolo-
gies using stochastic optimization, automatically
learning the most useful relational patterns for full
taxonomy induction.
As an example of the relational patterns that our
1041
system learns, suppose we are interested in build-
ing a taxonomy for types of mammals (see Fig-
ure 1). Frequent attestation of hypernymy patterns
like rat is a rodent in large corpora is a strong sig-
nal of the link rodent ? rat. Moreover, sibling
or coordination cues like either rats or squirrels
suggest that rat is a sibling of squirrel and adds
evidence for the links rodent ? rat and rodent
? squirrel. Our supervised model captures ex-
actly these types of intuitions by automatically dis-
covering such heterogeneous relational patterns as
features (and learning their weights) on edges and
on sibling edge pairs, respectively.
There have been several previous studies on
taxonomy induction. e.g., the incremental tax-
onomy induction system of Snow et al (2006),
the longest path approach of Kozareva and Hovy
(2010), and the maximum spanning tree (MST)
approach of Navigli et al (2011) (see Section 4 for
a more detailed overview). The main contribution
of this work is that we present the first discrimina-
tively trained, structured probabilistic model over
the full space of taxonomy trees, using a struc-
tured inference procedure through both the learn-
ing and decoding phases. Our model is also the
first to directly learn relational patterns as part of
the process of training an end-to-end taxonomic
induction system, rather than using patterns that
were hand-selected or learned via pairwise clas-
sifiers on manually annotated co-occurrence pat-
terns. Finally, it is the first end-to-end (i.e., non-
incremental) system to include sibling (e.g., coor-
dination) patterns at all.
We test our approach in two ways. First, on
the task of recreating fragments of WordNet, we
achieve a 51% error reduction on ancestor-based
F1 over a chance baseline, including a 15% error
reduction due to the non-hypernym-factored sib-
ling features. Second, we also compare to the re-
sults of Kozareva and Hovy (2010) by predicting
the large animal subtree of WordNet. Here, we
get up to 29% relative error reduction on ancestor-
based F1. We note that our approach falls at a
different point in the space of performance trade-
offs from past work ? by producing complete,
highly articulated trees, we naturally see a more
even balance between precision and recall, while
past work generally focused on precision.
1
To
1
While different applications will value precision and
recall differently, and past work was often intentionally
precision-focused, it is certainly the case that an ideal solu-
tion would maximize both.
avoid presumption of a single optimal tradeoff, we
also present results for precision-based decoding,
where we trade off recall for precision.
2 Structured Taxonomy Induction
Given an input term set x = {x
1
, x
2
, . . . , x
n
},
we wish to compute the conditional distribution
over taxonomy trees y. This distribution P (y|x)
is represented using the graphical model formu-
lation shown in Figure 2. A taxonomy tree y is
composed of a set of indicator random variables
y
ij
(circles in Figure 2), where y
ij
= ON means
that x
i
is the parent of x
j
in the taxonomy tree
(i.e. there exists a directed edge from x
i
to x
j
).
One such variable exists for each pair (i, j) with
0 ? i ? n, 1 ? j ? n, and i 6= j.
2
In a factor graph formulation, a set of factors
(squares and rectangles in Figure 2) determines the
probability of each possible variable assignment.
Each factor F has an associated scoring function
?
F
, with the probability of a total assignment de-
termined by the product of all these scores:
P (y|x) ?
?
F
?
F
(y) (1)
2.1 Factor Types
In the models we present here, there are three
types of factors: EDGE factors that score individ-
ual edges in the taxonomy tree, SIBLING factors
that score pairs of edges with a shared parent, and
a global TREE factor that imposes the structural
constraint that y form a legal taxonomy tree.
EDGE Factors. For each edge variable y
ij
in
the model, there is a corresponding factor E
ij
(small blue squares in Figure 2) that depends only
on y
ij
. We score each edge by extracting a set
of features f(x
i
, x
j
) and weighting them by the
(learned) weight vector w. So, the factor scoring
function is:
?
E
ij
(y
ij
) =
{
exp(w ? f(x
i
, x
j
)) y
ij
= ON
exp(0) = 1 y
ij
= OFF
SIBLING Factors. Our second model also in-
cludes factors that permit 2nd-order features look-
ing at terms that are siblings in the taxonomy tree.
For each triple (i, j, k) with i 6= j, i 6= k, and
j < k,
3
we have a factor S
ijk
(green rectangles in
2
We assume a special dummy root symbol x
0
.
3
The ordering of the siblings x
j
and x
k
doesn?t mat-
ter here, so having separate factors for (i, j, k) and (i, k, j)
would be redundant.
1042
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
T
(a) Edge Features Only
y01 y02 y0n
y1ny12
y21 y2n
yn1 yn2
E02E01 E0n
E1nE12
E21 E2n
En1 En2
S12n
S21n
Sn12
T
(b) Full Model
Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.
Figure 2b) that depends on y
ij
and y
ik
, and thus
can be used to encode features that should be ac-
tive whenever x
j
and x
k
share the same parent, x
i
.
The scoring function is similar to the one above:
?
S
ijk
(y
ij
, y
ik
) =
{
exp(w ? f(x
i
, x
j
, x
k
)) y
ij
= y
ik
= ON
1 otherwise
TREE Factor. Of course, not all variable as-
signments y form legal taxonomy trees (i.e., di-
rected spanning trees). For example, the assign-
ment ?i, j, y
ij
= ON might get a high score, but
would not be a valid output of the model. Thus,
we need to impose a structural constraint to ensure
that such illegal variable assignments are assigned
0 probability by the model. We encode this in our
factor graph setting using a single global factor T
(shown as a large red square in Figure 2) with the
following scoring function:
?
T
(y) =
{
1 y forms a legal taxonomy tree
0 otherwise
Model. For a given global assignment y, let
f(y) =
?
i,j
y
ij
=ON
f(x
i
, x
j
) +
?
i,j,k
y
ij
=y
ik
=ON
f(x
i
, x
j
, x
k
)
Note that by substituting our model?s factor scor-
ing functions into Equation 1, we get:
P (y|x) ?
{
exp(w ? f(y)) y is a tree
0 otherwise
Thus, our model has the form of a standard loglin-
ear model with feature function f .
2.2 Inference via Belief Propagation
With the model defined, there are two main in-
ference tasks we wish to accomplish: computing
expected feature counts and selecting a particular
taxonomy tree for a given set of input terms (de-
coding). As an initial step to each of these pro-
cedures, we wish to compute the marginal prob-
abilities of particular edges (and pairs of edges)
being on. In a factor graph, the natural infer-
ence procedure for computing marginals is belief
propagation. Note that finding taxonomy trees is
a structurally identical problem to directed span-
ning trees (and thereby non-projective dependency
parsing), for which belief propagation has previ-
ously been worked out in depth (Smith and Eisner,
2008). Therefore, we will only briefly sketch the
procedure here.
Belief propagation is a general-purpose infer-
ence method that computes marginals via directed
messages passed from variables to adjacent fac-
tors (and vice versa) in the factor graph. These
messages take the form of (possibly unnormal-
ized) distributions over values of the variable. The
two types of messages (variable to factor or fac-
tor to variable) have mutually recursive defini-
tions. The message from a factor F to an adjacent
variable V involves a sum over all possible val-
ues of every other variable that F touches. While
the EDGE and SIBLING factors are simple enough
to compute this sum by brute force, performing
the sum na??vely for computing messages from the
TREE factor would take exponential time. How-
1043
ever, due to the structure of that particular factor,
all of its outgoing messages can be computed si-
multaneously in O(n
3
) time via an efficient adap-
tation of Kirchhoff?s Matrix Tree Theorem (MTT)
(Tutte, 1984) which computes partition functions
and marginals for directed spanning trees.
Once message passing is completed, marginal
beliefs are computed by merely multiplying to-
gether all the messages received by a particular
variable or factor.
2.2.1 Loopy Belief Propagation
Looking closely at Figure 2a, one can observe
that the factor graph for the first version of our
model, containing only EDGE and TREE factors,
is acyclic. In this special case, belief propagation
is exact: after one round of message passing, the
beliefs computed (as discussed in Section 2.2) will
be the true marginal probabilities under the cur-
rent model. However, in the full model, shown
in Figure 2b, the SIBLING factors introduce cy-
cles into the factor graph, and now the messages
being passed around often depend on each other
and so they will change as they are recomputed.
The process of iteratively recomputing messages
based on earlier messages is known as loopy belief
propagation. This procedure only finds approx-
imate marginal beliefs, and is not actually guar-
anteed to converge, but in practice can be quite
effective for finding workable marginals in mod-
els for which exact inference is intractable, as is
the case here. All else equal, the more rounds
of message passing that are performed, the closer
the computed marginal beliefs will be to the true
marginals, though in practice, there are usually di-
minishing returns after the first few iterations. In
our experiments, we used a fairly conservative up-
per bound of 20 iterations, but in most cases, the
messages converged much earlier than that.
2.3 Training
We used gradient-based maximum likelihood
training to learn the model parameters w. Since
our model has a loglinear form, the derivative
of w with respect to the likelihood objective is
computed by just taking the gold feature vec-
tor and subtracting the vector of expected feature
counts. For computing expected counts, we run
belief propagation until completion and then, for
each factor in the model, we simply read off the
marginal probability of that factor being active (as
computed in Section 2.2), and accumulate a par-
tial count for each feature that is fired by that fac-
tor. This method of computing the gradient can be
incorporated into any gradient-based optimizer in
order to learn the weights w. In our experiments
we used AdaGrad (Duchi et al, 2011), an adaptive
subgradient variant of standard stochastic gradient
ascent for online learning.
2.4 Decoding
Finally, once the model parameters have been
learned, we want to use the model to find taxon-
omy trees for particular sets of input terms. Note
that if we limit our scores to be edge-factored,
then finding the highest scoring taxonomy tree
becomes an instance of the MST problem (also
known as the maximum arborescence problem
for the directed case), which can be solved effi-
ciently in O(n
2
) quadratic time (Tarjan, 1977) us-
ing the greedy, recursive Chu-Liu-Edmonds algo-
rithm (Chu and Liu, 1965; Edmonds, 1967).
4
Since the MST problem can be solved effi-
ciently, the main challenge becomes finding a way
to ensure that our scores are edge-factored. In the
first version of our model, we could simply set the
score of each edge to be w?f(x
i
, x
j
), and the MST
recovered in this way would indeed be the high-
est scoring tree: arg maxyP (y|x). However, this
straightforward approach doesn?t apply to the full
model which also uses sibling features. Hence, at
decoding time, we instead start out by once more
using belief propagation to find marginal beliefs,
and then set the score of each edge to be its belief
odds ratio:
b
Y
ij
(ON)
b
Y
ij
(OFF)
.
5
3 Features
While spanning trees are familiar from non-
projective dependency parsing, features based on
the linear order of the words or on lexical identi-
4
See Georgiadis (2003) for a detailed algorithmic proof,
and McDonald et al (2005) for an illustrative example. Also,
we constrain the Chu-Liu-Edmonds MST algorithm to out-
put only single-root MSTs, where the (dummy) root has ex-
actly one child (Koo et al, 2007), because multi-root span-
ning ?forests? are not applicable to our task.
Also, note that we currently assume one node per term. We
are following the task description from previous work where
the goal is to create a taxonomy for a specific domain (e.g.,
animals). Within a specific domain, terms typically just have
a single sense. However, our algorithms could certainly be
adapted to the case of multiple term senses (by treating the
different senses as unique nodes in the tree) in future work.
5
The MST that is found using these edge scores is actually
the minimum Bayes risk tree (Goodman, 1996) for an edge
accuracy loss function (Smith and Eisner, 2008).
1044
ties or syntactic word classes, which are primary
drivers for dependency parsing, are mostly unin-
formative for taxonomy induction. Instead, induc-
ing taxonomies requires world knowledge to cap-
ture the semantic relations between various unseen
terms. For this, we use semantic cues to hyper-
nymy and siblinghood via features on simple sur-
face patterns and statistics in large text corpora.
We fire features on both the edge and the sibling
factors. We first describe all the edge features
in detail (Section 3.1 and Section 3.2), and then
briefly describe the sibling features (Section 3.3),
which are quite similar to the edge ones.
For each edge factor E
ij
, which represents the
potential parent-child term pair (x
i
, x
j
), we add
the surface and semantic features discussed below.
Note that since edges are directed, we have sepa-
rate features for the factors E
ij
versus E
ji
.
3.1 Surface Features
Capitalization: Checks which of x
i
and x
j
are
capitalized, with one feature for each value of the
tuple (isCap(x
i
), isCap(x
j
)). The intuition is that
leaves of a taxonomy are often proper names and
hence capitalized, e.g., (bison, American bison).
Therefore, the feature for (true, false) (i.e., parent
capitalized but not the child) gets a substantially
negative weight.
Ends with: Checks if x
j
ends with x
i
, or not. This
captures pairs such as (fish, bony fish) in our data.
Contains: Checks if x
j
contains x
i
, or not. This
captures pairs such as (bird, bird of prey).
Suffix match: Checks whether the k-length suf-
fixes of x
i
and x
j
match, or not, for k =
1, 2, . . . , 7.
LCS: We compute the longest common substring
of x
i
and x
j
, and create indicator features for
rounded-off and binned values of |LCS|/((|x
i
|+
|x
j
|)/2).
Length difference: We compute the signed length
difference between x
j
and x
i
, and create indica-
tor features for rounded-off and binned values of
(|x
j
| ? |x
i
|)/((|x
i
| + |x
j
|)/2). Yang and Callan
(2009) use a similar feature.
3.2 Semantic Features
3.2.1 Web n-gram Features
Patterns and counts: Hypernymy for a term pair
(P=x
i
, C=x
j
) is often signaled by the presence
of surface patterns like C is a P, P such as C
in large text corpora, an observation going back
to Hearst (1992). For each potential parent-child
edge (P=x
i
, C=x
j
), we mine the top k strings
(based on count) in which both x
i
and x
j
occur
(we use k=200). We collect patterns in both direc-
tions, which allows us to judge the correct direc-
tion of an edge (e.g., C is a P is a positive signal
for hypernymy whereas P is a C is a negative sig-
nal).
6
Next, for each pattern in this top-k list, we
compute its normalized pattern count c, and fire
an indicator feature on the tuple (pattern, t), for
all thresholds t (in a fixed set) s.t. c ? t. Our
supervised model then automatically learns which
patterns are good indicators of hypernymy.
Pattern order: We add features on the order (di-
rection) in which the pair (x
i
, x
j
) found a pattern
(in its top-k list) ? indicator features for boolean
values of the four cases: P . . . C, C . . . P , neither
direction, and both directions. Ritter et al (2009)
used the ?both? case of this feature.
Individual counts: We also compute the indi-
vidual Web-scale term counts c
x
i
and c
x
j
, and
add a comparison feature (c
x
i
>c
x
j
), plus features
on values of the signed count difference (|c
x
i
| ?
|c
x
j
|)/((|c
x
i
| + |c
x
j
|)/2), after rounding off, and
binning at multiple granularities. The intuition is
that this feature could learn whether the relative
popularity of the terms signals their hypernymy di-
rection.
3.2.2 Wikipedia Abstract Features
The Web n-grams corpus has broad coverage but
is limited to up to 5-grams, so it may not contain
pattern-based evidence for various longer multi-
word terms and pairs. Therefore, we supplement
it with a full-sentence resource, namely Wikipedia
abstracts, which are concise descriptions (hence
useful to signal hypernymy) of a large variety of
world entities.
Presence and distance: For each potential edge
(x
i
, x
j
), we mine patterns from all abstracts in
which the two terms co-occur in either order, al-
lowing a maximum term distance of 20 (because
beyond that, co-occurrence may not imply a rela-
tion). We add a presence feature based on whether
the process above found at least one pattern for
that term pair, or not. We also fire features on
the value of the minimum distance d
min
at which
6
We also allow patterns with surrounding words, e.g., the
C is a P and C , P of.
1045
the two terms were found in some abstract (plus
thresholded versions).
Patterns: For each term pair, we take the top-k
?
patterns (based on count) of length up to l from
its full list of patterns, and add an indicator feature
on each pattern string (without the counts). We use
k
?
=5, l=10. Similar to the Web n-grams case, we
also fire Wikipedia-based pattern order features.
3.3 Sibling Features
We also incorporate similar features on sibling
factors. For each sibling factor S
ijk
which rep-
resents the potential parent-children term triple
(x
i
, x
j
, x
k
), we consider the potential sibling term
pair (x
j
, x
k
). Siblinghood for this pair would be
indicated by the presence of surface patterns such
as either C
1
or C
2
, C
1
is similar to C
2
in large cor-
pora. Hence, we fire Web n-gram pattern features
and Wikipedia presence, distance, and pattern fea-
tures, similar to those described above, on each
potential sibling term pair.
7
The main difference
here from the edge factors is that the sibling fac-
tors are symmetric (in the sense that S
ijk
is redun-
dant to S
ikj
) and hence the patterns are undirected.
Therefore, for each term pair, we first symmetrize
the collected Web n-grams and Wikipedia patterns
by accumulating the counts of symmetric patterns
like rats or squirrels and squirrels or rats.
8
4 Related Work
In our work, we assume a known term set and
do not address the problem of extracting related
terms from text. However, a great deal of past
work has considered automating this process, typ-
ically taking one of two major approaches. The
clustering-based approach (Lin, 1998; Lin and
Pantel, 2002; Davidov and Rappoport, 2006; Ya-
mada et al, 2009) discovers relations based on the
assumption that similar concepts appear in sim-
7
One can also add features on the full triple (x
i
, x
j
, x
k
)
but most such features will be sparse.
8
All the patterns and counts for our Web and Wikipedia
edge and sibling features described above are extracted after
stemming the words in the terms, the n-grams, and the ab-
stracts (using the Porter stemmer). Also, we threshold the
features (to prune away the sparse ones) by considering only
those that fire for at least t trees in the training data (t = 4 in
our experiments).
Note that one could also add various complementary types of
useful features presented by previous work, e.g., bootstrap-
ping using syntactic heuristics (Phillips and Riloff, 2002),
dependency patterns (Snow et al, 2006), doubly anchored
patterns (Kozareva et al, 2008; Hovy et al, 2009), and Web
definition classifiers (Navigli et al, 2011).
ilar contexts (Harris, 1954). The pattern-based
approach uses special lexico-syntactic patterns to
extract pairwise relation lists (Phillips and Riloff,
2002; Girju et al, 2003; Pantel and Pennacchiotti,
2006; Suchanek et al, 2007; Ritter et al, 2009;
Hovy et al, 2009; Baroni et al, 2010; Ponzetto
and Strube, 2011) and semantic classes or class-
instance pairs (Riloff and Shepherd, 1997; Katz
and Lin, 2003; Pas?ca, 2004; Etzioni et al, 2005;
Talukdar et al, 2008).
We focus on the second step of taxonomy induc-
tion, namely the structured organization of terms
into a complete and coherent tree-like hierarchy.
9
Early work on this task assumes a starting par-
tial taxonomy and inserts missing terms into it.
Widdows (2003) place unknown words into a re-
gion with the most semantically-similar neigh-
bors. Snow et al (2006) add novel terms by greed-
ily maximizing the conditional probability of a set
of relational evidence given a taxonomy. Yang and
Callan (2009) incrementally cluster terms based
on a pairwise semantic distance. Lao et al (2012)
extend a knowledge base using a random walk
model to learn binary relational inference rules.
However, the task of inducing full taxonomies
without assuming a substantial initial partial tax-
onomy is relatively less well studied. There is
some prior work on the related task of hierarchical
clustering, or grouping together of semantically
related words (Cimiano et al, 2005; Cimiano and
Staab, 2005; Poon and Domingos, 2010; Fountain
and Lapata, 2012). The task we focus on, though,
is the discovery of direct taxonomic relationships
(e.g., hypernymy) between words.
We know of two closely-related previous sys-
tems, Kozareva and Hovy (2010) and Navigli et
al. (2011), that build full taxonomies from scratch.
Both of these systems use a process that starts
by finding basic level terms (leaves of the fi-
nal taxonomy tree, typically) and then using re-
lational patterns (hand-selected ones in the case of
Kozareva and Hovy (2010), and ones learned sep-
arately by a pairwise classifier on manually anno-
tated co-occurrence patterns for Navigli and Ve-
lardi (2010), Navigli et al (2011)) to find interme-
diate terms and all the attested hypernymy links
between them.
10
To prune down the resulting tax-
9
Determining the set of input terms is orthogonal to our
work, and our method can be used in conjunction with vari-
ous term extraction approaches described above.
10
Unlike our system, which assumes a complete set of
terms and only attempts to induce the taxonomic structure,
1046
onomy graph, Kozareva and Hovy (2010) use a
procedure that iteratively retains the longest paths
between root and leaf terms, removing conflicting
graph edges as they go. The end result is acyclic,
though not necessarily a tree; Navigli et al (2011)
instead use the longest path intuition to weight
edges in the graph and then find the highest weight
taxonomic tree using a standard MST algorithm.
Our work differs from the two systems above
in that ours is the first discriminatively trained,
structured probabilistic model over the full space
of taxonomy trees that uses structured inference
via spanning tree algorithms (MST and MTT)
through both the learning and decoding phases.
Our model also automatically learns relational pat-
terns as a part of the taxonomic training phase, in-
stead of relying on hand-picked rules or pairwise
classifiers on manually annotated co-occurrence
patterns, and it is the first end-to-end (i.e., non-
incremental) system to include heterogeneous re-
lational information via sibling (e.g., coordina-
tion) patterns.
5 Experiments
5.1 Data and Experimental Regime
We considered two distinct experimental setups,
one that illustrates the general performance of
our model by reproducing various medium-sized
WordNet domains, and another that facilitates
comparison to previous work by reproducing the
much larger animal subtree provided by Kozareva
and Hovy (2010).
General setup: In order to test the accuracy
of structured prediction on medium-sized full-
domain taxonomies, we extracted from WordNet
3.0 all bottomed-out full subtrees which had a
tree-height of 3 (i.e., 4 nodes from root to leaf),
and contained (10, 50] terms.
11
This gives us
761 non-overlapping trees, which we partition into
both these systems include term discovery in the taxonomy
building process.
11
Subtrees that had a smaller or larger tree height were dis-
carded in order to avoid overlap between the training and test
divisions. This makes it a much stricter setting than other
tasks such as parsing, which usually has repeated sentences,
clauses and phrases between training and test sets.
To project WordNet synsets to terms, we used the first (most
frequent) term in each synset. A few WordNet synsets have
multiple parents so we only keep the first of each such pair of
overlapping trees. We also discard a few trees with duplicate
terms because this is mostly due to the projection of different
synsets to the same term, and theoretically makes the tree a
graph.
70/15/15% (533/114/114 trees) train/dev/test sets.
Comparison setup: We also compare our method
(as closely as possible) with related previous work
by testing on the much larger animal subtree made
available by Kozareva and Hovy (2010), who cre-
ated this dataset by selecting a set of ?harvested?
terms and retrieving all the WordNet hypernyms
between each input term and the root (i.e., an-
imal), resulting in ?700 terms and ?4,300 is-a
ancestor-child links.
12
Our training set for this an-
imal test case was generated from WordNet us-
ing the following process: First, we strictly re-
move the full animal subtree from WordNet in or-
der to avoid any possible overlap with the test data.
Next, we create random 25-sized trees by picking
random nodes as singleton trees, and repeatedly
adding child edges from WordNet to the tree. This
process gives us a total of ?1600 training trees.
13
Feature sources: The n-gram semantic features
are extracted from the Google n-grams corpus
(Brants and Franz, 2006), a large collection of
English n-grams (for n = 1 to 5) and their fre-
quencies computed from almost 1 trillion tokens
(95 billion sentences) of Web text. The Wikipedia
abstracts are obtained via the publicly available
dump, which contains almost ?4.1 million ar-
ticles.
14
Preprocessing includes standard XML
parsing and tokenization. Efficient collection of
feature statistics is important because these must
be extracted for millions of query pairs (for each
potential edge and sibling pair in each term set).
For this, we use a hash-trie on term pairs (sim-
ilar to that of Bansal and Klein (2011)), and scan
once through the n-gram (or abstract) set, skipping
many n-grams (or abstracts) based on fast checks
of missing unigrams, exceeding length, suffix mis-
matches, etc.
5.2 Evaluation Metric
Ancestor F1: Measures the precision, recall, and
F
1
= 2PR/(P +R) of correctly predicted ances-
12
This is somewhat different from our general setup where
we work with any given set of terms; they start with a large
set of leaves which have substantial Web-based relational
information based on their selected, hand-picked patterns.
Their data is available at http://www.isi.edu/
?
kozareva/
downloads.html.
13
We tried this training regimen as different from that of
the general setup (which contains only bottomed-out sub-
trees), so as to match the animal test tree, which is of depth
12 and has intermediate nodes from higher up in WordNet.
14
We used the 20130102 dump.
1047
System P R F1
Edges-Only Model
Baseline 5.9 8.3 6.9
Surface Features 17.5 41.3 24.6
Semantic Features 37.0 49.1 42.2
Surface+Semantic 41.1 54.4 46.8
Edges + Siblings Model
Surface+Semantic 53.1 56.6 54.8
Surface+Semantic (Test) 48.0 55.2 51.4
Table 1: Main results on our general setup. On the devel-
opment set, we present incremental results on the edges-only
model where we start with the chance baseline, then use sur-
face features only, semantic features only, and both. Finally,
we add sibling factors and features to get results for the full,
edges+siblings model with all features, and also report the
final test result for this setting.
tors, i.e., pairwise is-a relations:
P =
|isa
gold
? isa
predicted
|
|isa
predicted
|
, R =
|isa
gold
? isa
predicted
|
|isa
gold
|
5.3 Results
Table 1 shows our main results for ancestor-based
evaluation on the general setup. We present a de-
velopment set ablation study where we start with
the edges-only model (Figure 2a) and its random
tree baseline (which chooses any arbitrary span-
ning tree for the term set). Next, we show results
on the edges-only model with surface features
(Section 3.1), semantic features (Section 3.2), and
both. We see that both surface and semantic fea-
tures make substantial contributions, and they also
stack. Finally, we add the sibling factors and fea-
tures (Figure 2b, Section 3.3), which further im-
proves the results significantly (8% absolute and
15% relative error reduction over the edges-only
results on the ancestor F1 metric). The last row
shows the final test set results for the full model
with all features.
Table 2 shows our results for comparison to
the larger animal dataset of Kozareva and Hovy
(2010).
15
In the table, ?Kozareva2010? refers
to Kozareva and Hovy (2010) and ?Navigli2011?
refers to Navigli et al (2011).
16
For appropri-
15
These results are for the 1st order model due to the scale
of the animal taxonomy (?700 terms). For scaling the 2nd
order sibling model, one can use approximations, e.g., prun-
ing the set of sibling factors based on 1st order link marginals,
or a hierarchical coarse-to-fine approach based on taxonomy
induction on subtrees, or a greedy approach of adding a few
sibling factors at a time. This is future work.
16
The Kozareva and Hovy (2010) ancestor results are ob-
tained by using the output files provided on their webpage.
System P R F1
Previous Work
Kozareva2010 98.6 36.2 52.9
Navigli2011
??
97.0
??
43.7
??
60.3
??
This Paper
Fixed Prediction 84.2 55.1 66.6
Free Prediction 79.3 49.0 60.6
Table 2: Comparison results on the animal dataset of
Kozareva and Hovy (2010). Here, ?Kozareva2010? refers to
Kozareva and Hovy (2010) and ?Navigli2011? refers to Nav-
igli et al (2011). For appropriate comparison to each previ-
ous work, we show our results both for the ?Fixed Prediction?
setup, which assumes the true root and leaves, and for the
?Free Prediction? setup, which doesn?t assume any prior in-
formation. The ?? results of Navigli et al (2011) represent a
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details.
ate comparison to each previous work, we show
results for two different setups. The first setup
?Fixed Prediction? assumes that the model knows
the true root and leaves of the taxonomy to provide
for a somewhat fairer comparison to Kozareva and
Hovy (2010). We get substantial improvements
on ancestor-based recall and F1 (a 29% relative
error reduction). The second setup ?Free Predic-
tion? assumes no prior knowledge and predicts the
full tree (similar to the general setup case). On
this setup, we do compare as closely as possible
to Navigli et al (2011) and see a small gain in F1,
but regardless, we should note that their results are
incomparable (denoted by ?? in Table 2) because
they have a different ground-truth data condition:
their definition and hypernym extraction phase in-
volves using the Google define keyword, which
often returns WordNet glosses itself.
We note that previous work achieves higher an-
cestor precision, while our approach achieves a
more even balance between precision and recall.
Of course, precision and recall should both ide-
ally be high, even if some applications weigh one
over the other. This is why our tuning optimized
for F1, which represents a neutral combination
for comparison, but other F
?
metrics could also
be optimized. In this direction, we also tried an
experiment on precision-based decoding (for the
?Free Prediction? scenario), where we discard any
edges with score (i.e., the belief odds ratio de-
scribed in Section 2.4) less than a certain thresh-
old. This allowed us to achieve high values of pre-
cision (e.g., 90.8%) at still high enough F1 values
(e.g., 61.7%).
1048
Hypernymy features
C and other P > P > C
C , P of C is a P
C , a P P , including C
C or other P P ( C
C : a P C , american P
C - like P C , the P
Siblinghood features
C
1
and C
2
C
1
, C
2
(
C
1
or C
2
of C
1
and / or C
2
, C
1
, C
2
and either C
1
or C
2
the C
1
/ C
2
<s> C
1
and C
2
</s>
Table 3: Examples of high-weighted hypernymy and sibling-
hood features learned during development.
butterfly
copper
American copper
hairstreak
Strymon melinus
admiral
white admiral
1
Figure 3: Excerpt from the predicted butterfly tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
6 Analysis
Table 3 shows some of the hypernymy and sibling-
hood features given highest weight by our model
(in general-setup development experiments). The
training process not only rediscovers most of the
standard Hearst-style hypernymy patterns (e.g., C
and other P, C is a P), but also finds various
novel, intuitive patterns. For example, the pattern
C, american P is prominent because it captures
pairs like Lemmon, american actor and Bryon,
american politician, etc. Another pattern > P >
C captures webpage navigation breadcrumb trails
(representing category hierarchies). Similarly, the
algorithm also discovers useful siblinghood fea-
tures, e.g., either C
1
or C
2
, C
1
and / or C
2
, etc.
Finally, we look at some specific output errors
to give as concrete a sense as possible of some sys-
tem confusions, though of course any hand-chosen
examples must be taken as illustrative. In Figure
3, we attach white admiral to admiral, whereas
the gold standard makes these two terms siblings.
In reality, however, white admirals are indeed a
species of admirals, so WordNet?s ground truth
turns out to be incomplete. Another such example
is that we place logistic assessment in the evalu-
bottle
flask
vacuum flask thermos Erlenmeyer flask
wine bottle jeroboam
1
Figure 4: Excerpt from the predicted bottle tree. The terms
attached erroneously according to WordNet are marked in red
and italicized.
ation subtree of judgment, but WordNet makes it
a direct child of judgment. However, other dictio-
naries do consider logistic assessments to be eval-
uations. Hence, this illustrates that there may be
more than one right answer, and that the low re-
sults on this task should only be interpreted as
such. In Figure 4, our algorithm did not recog-
nize that thermos is a hyponym of vacuum flask,
and that jeroboam is a kind of wine bottle. Here,
our Web n-grams dataset (which only contains fre-
quent n-grams) and Wikipedia abstracts do not
suffice and we would need to add richer Web data
for such world knowledge to be reflected in the
features.
7 Conclusion
Our approach to taxonomy induction allows het-
erogeneous information sources to be combined
and balanced in an error-driven way. Direct indi-
cators of hypernymy, such as Hearst-style context
patterns, are the core feature for the model and are
discovered automatically via discriminative train-
ing. However, other indicators, such as coordina-
tion cues, can indicate that two words might be
siblings, independently of what their shared par-
ent might be. Adding second-order factors to our
model allows these two kinds of evidence to be
weighed and balanced in a discriminative, struc-
tured probabilistic framework. Empirically, we
see substantial gains (in ancestor F1) from sibling
features, and also over comparable previous work.
We also present results on the precision and recall
trade-offs inherent in this task.
Acknowledgments
We would like to thank the anonymous review-
ers for their insightful comments. This work
was supported by BBN under DARPA contract
HR0011-12-C-0014, 973 Program China Grants
2011CBA00300, 2011CBA00301, and NSFC
Grants 61033001, 61361136003.
1049
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Thorsten Brants and Alex Franz. 2006. The Google
Web 1T 5-gram corpus version 1.1. LDC2006T13.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the
shortest arborescence of a directed graph. Science
Sinica, 14(1396-1400):270.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided agglom-
erative clustering algorithm. In Proceedings of the
ICML 2005 Workshop on Learning and Extending
Lexical Ontologies with Machine Learning Meth-
ods.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. Journal of Arti-
ficial Intelligence Research, 24(1):305?339.
Dmitry Davidov and Ari Rappoport. 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
In Proceedings of COLING-ACL.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71:233?240.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the Web:
An experimental study. Artificial Intelligence,
165(1):91?134.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the DeepQA
project. AI magazine, 31(3):59?79.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy
induction using hierarchical random graphs. In Pro-
ceedings of NAACL.
Leonidas Georgiadis. 2003. Arborescence optimiza-
tion problems solvable by edmonds algorithm. The-
oretical Computer Science, 301(1):427?437.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of NAACL.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of EMNLP.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In Proceedings of the Workshop on NLP for Ques-
tion Answering (EACL 2003).
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of
EMNLP-CoNLL.
Zornitsa Kozareva and Eduard Hovy. 2010. A
semi-supervised method to learn and construct tax-
onomies using the Web. In Proceedings of EMNLP.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceedings of COLING.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexical
taxonomies from scratch. In Proceedings of IJCAI.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL.
1050
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of CIKM.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In Proceedings of
COLING-ACL.
William Phillips and Ellen Riloff. 2002. Exploiting
strong syntactic heuristics and co-training to learn
semantic lexicons. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Michael Strube. 2011.
Taxonomy induction based on a collaboratively
built knowledge repository. Artificial Intelligence,
175(9):1737?1756.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of ACL.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-
based approach for building semantic lexicons. In
Proceedings of EMNLP.
Alan Ritter, Stephen Soderland, and Oren Etzioni.
2009. What is this, anyway: Automatic hypernym
discovery. In Proceedings of AAAI Spring Sympo-
sium on Learning by Reading and Learning to Read.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of WWW.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pas?ca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of EMNLP.
Robert E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25?35.
William T. Tutte. 1984. Graph theory. Addison-
Wesley.
Dominic Widdows. 2003. Unsupervised methods
for developing taxonomies by combining syntactic
and statistical information. In Proceedings of HLT-
NAACL.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym dis-
covery based on distributional similarity and hierar-
chical structures. In Proceedings of EMNLP.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of ACL-IJCNLP.
1051
Transactions of the Association for Computational Linguistics, 1 (2013) 279?290. Action Editor: Lillian Lee.
Submitted 11/2012; Revised 1/2013; Published 7/2013. c?2013 Association for Computational Linguistics.
Good, Great, Excellent:
Global Inference of Semantic Intensities
Gerard de Melo
ICSI, Berkeley
demelo@icsi.berkeley.edu
Mohit Bansal
CS Division, UC Berkeley
mbansal@cs.berkeley.edu
Abstract
Adjectives like good, great, and excellent are
similar in meaning, but differ in intensity. In-
tensity order information is very useful for
language learners as well as in several NLP
tasks, but is missing in most lexical resources
(dictionaries, WordNet, and thesauri). In this
paper, we present a primarily unsupervised
approach that uses semantics from Web-scale
data (e.g., phrases like good but not excel-
lent) to rank words by assigning them posi-
tions on a continuous scale. We rely on Mixed
Integer Linear Programming to jointly deter-
mine the ranks, such that individual decisions
benefit from global information. When rank-
ing English adjectives, our global algorithm
achieves substantial improvements over pre-
vious work on both pairwise and rank corre-
lation metrics (specifically, 70% pairwise ac-
curacy as compared to only 56% by previous
work). Moreover, our approach can incorpo-
rate external synonymy information (increas-
ing its pairwise accuracy to 78%) and extends
easily to new languages. We also make our
code and data freely available.1
1 Introduction
Current lexical resources such as dictionaries and
thesauri do not provide information about the in-
tensity order of words. For example, both WordNet
(Miller, 1995) and Roget?s 21st Century Thesaurus
(thesaurus.com) present acceptable, great, and su-
perb as synonyms of the adjective good. However,
a native speaker knows that these words represent
varying intensity and can in fact generally be ranked
by intensity as acceptable< good< great< superb.
Similarly, warm < hot < scorching are identified as
synonyms in these resources. Ranking information,
1http://demelo.org/gdm/intensity/
however, is crucial because it allows us to differen-
tiate e.g. between various intensities of an emotion,
and is hence very useful for humans when learning a
language or judging product reviews, as well as for
automatic text understanding and generation tasks
such as sentiment and subjectivity analysis, recog-
nizing textual entailment, question answering, sum-
marization, and coreference and discourse analysis.
In this work, we attempt to automatically rank
sets of related words by intensity, focusing in par-
ticular on adjectives. This is made possible by the
vast amounts of world knowledge that are now avail-
able. We use lexico-semantic information extracted
from a Web-scale corpus in conjunction with an al-
gorithm based on a Mixed Integer Linear Program
(MILP). Linguistic analyses have identified phrases
such as good but not great or hot and almost scorch-
ing in a text corpus as sources of evidence about the
relative intensities of words. However, pure infor-
mation extraction approaches often fail to provide
enough coverage for real-world downstream appli-
cations (Tandon and de Melo, 2010), unless some
form of advanced inference is used (Snow et al,
2006; Suchanek et al, 2009).
In our work, we address this sparsity problem by
relying on Web-scale data and using an MILP model
that extends the pairwise scores to a more com-
plete joint ranking of words on a continuous scale,
while maintaining global constraints such as transi-
tivity and giving more weight to the order of word
pairs with higher corpus evidence scores. Instead
of considering intensity ranking as a pairwise deci-
sion process, we thus exploit the fact that individual
decisions may benefit from global information, e.g.
about how two words relate to some third word.
Previous work (Sheinman and Tokunaga, 2009;
Schulam and Fellbaum, 2010; Sheinman et al,
2012) has also used lexico-semantic patterns to or-
279
der adjectives. They mainly evaluate their algorithm
on a set of pairwise decisions, but also present a par-
titioning approach that attempts to form scales by
placing each adjective to the left or right of pivot
words. Unfortunately, this approach often fails be-
cause many pairs lack order-based evidence even on
the Web, as explained in more detail in Section 3.
In contrast, our MILP jointly uses information
from all relevant word pairs and captures com-
plex interactions and inferences to produce inten-
sity scales. We can thus obtain an order between
two adjectives even when there is no explicit evi-
dence in the corpus (using evidence for related pairs
and transitive inference). Our global MILP is flex-
ible and can also incorporate additional synonymy
information if available (which helps the MILP find
an even better ranking solution). Our approach also
extends easily to new languages. We describe two
approaches for this multilingual extension: pattern
projection and cross-lingual MILPs.
We evaluate our predicted intensity rankings us-
ing both pairwise classification accuracy and rank-
ing correlation coefficients, achieving strong results,
significantly better than the previous approach by
Sheinman & Tokunaga (32% relative error reduc-
tion) and quite close to human-level performance.
2 Method
In this section, we describe each step of our ap-
proach to ordering adjectives on a single, relative
scale. Our method can also be applied to other word
classes and to languages other than English.
2.1 Web-based Scoring Model
2.1.1 Intensity Scales
Near-synonyms may differ in intensity, e.g. joy
vs. euphoria, or drizzle vs. rain. This is particu-
larly true of adjectives, which can represent different
degrees of a given quality or attribute such as size
or age. Many adjectives are gradable and thus al-
low for grading adverbial modifiers to express such
intensity degrees, e.g., a house can be very big or
extremely big. Often, however, completely differ-
ent adjectives refer to varying degrees on the same
scale, e.g., huge, gigantic, gargantuan. Even adjec-
tives like enormous (or superb, impossible) that are
considered non-gradable from a syntactic perspec-
tive can be placed on a such a scale.
Weak-Strong Patterns Strong-Weak Patterns
? (,) but not ? not ? (,) just ?
? (,) if not ? not ? (,) but just ?
? (,) although not ? not ? (,) still ?
? (,) though not ? not ? (,) but still ?
? (,) (and/or) even ? not ? (,) although still ?
? (,) (and/or) almost ? not ? (,) though still ?
not only ? but ? ? (,) or very ?
not just ? but ?
Table 1: Ranking patterns used in this work. Among the
patterns represented by the regular expressions above, we
use only those that capture less than or equal to five words
(to fit in the Google n-grams, see Section 2.1.2). Articles
(a, an, the) are allowed to appear before the wildcards
wherever possible.
2.1.2 Intensity Patterns
Linguistic studies have found lexical patterns like
?? but not ?? (e.g. good but not great) to reveal order
information between a pair of adjectives (Sheinman
and Tokunaga, 2009). We assume that we have two
sets of lexical patterns that allow us to infer the most
likely ordering between two words when encoun-
tered in a corpus. A first pattern set, Pws, contains
patterns that reflect a weak-strong order between a
pair of word (the first word is weaker than the sec-
ond), and a second pattern set, Psw, captures the
strong-weak order. See Table 1 for the adjective pat-
terns that we used in this work (and see Section 4.1
for implementation details regarding our pattern col-
lection). Many of these patterns also apply to other
parts of speech (e.g. ?drizzle but not rain?, ?running
or even sprinting?), with significant discrimination
on the Web in the right direction.
2.1.3 Pairwise Scores
Given an input set of words to be placed on a
scale, we first collect evidence of their intensity or-
der by using the above-mentioned intensity patterns
and a large, Web-scale text corpus.
Previous work on information extraction from
limited-sized raw text corpora revealed that cover-
age is often limited (Hearst, 1992; Hatzivassiloglou
and McKeown, 1993). Some studies (Chklovski
and Pantel, 2004; Sheinman and Tokunaga, 2009)
used hit counts from an online search engine, but
this is unstable and irreproducible (Kilgarriff, 2007).
To avoid these issues, we use the largest available
280
(good, great) (great, good) (small, minute)
good , but not great? 24492.0 not great , just good? 248.0 small , almost minute? 97.0
good , if not great? 1912.0 great or very good? 89.0 small , even minute? 41.0
good , though not great? 504.0 not great but still good? 47.0
good , or even great? 338.0
not just good but great? 181.0
good , almost great? 156.0
Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs.
static corpus of counts, the Google n-grams corpus
(Brants and Franz, 2006), which contains English
n-grams (n = 1 to 5) and their observed frequency
counts, generated from nearly 1 trillion word tokens
and 95 billion sentences.
We consider each pair of words (a1, a2) in the in-
put set in turn. For each pattern p in the two pattern
sets (weak-strong Pws and strong-weak Psw), we in-
sert the word pair into the pattern as p(a1, a2) to get
a phrasal query like ?big but not huge?. This is done
by replacing the two wildcards in the pattern by the
two words in order. Finally, we scan the Web n-
grams corpus in a batch approach similar to Bansal
and Klein (2011) and collect frequencies of all our
phrase queries. Table 2 depicts some examples of
useful intensity-based phrase queries and their fre-
quencies in the Web-scale corpus. We also collect
frequencies for the input word unigrams and the pat-
terns for normalization purposes. Given a word pair
(a1, a2) and a corpus count function cnt, we define
W1 =
1
P1
?
p1?Pws
cnt(p1(a1, a2))
S1 =
1
P2
?
p2?Psw
cnt(p2(a1, a2))
W2 =
1
P1
?
p1?Pws
cnt(p1(a2, a1))
S2 =
1
P2
?
p2?Psw
cnt(p2(a2, a1)) (1)
with
P1 =
?
p1?Pws
cnt(p1)
P2 =
?
p2?Psw
cnt(p2), (2)
such that the final overall weak-strong score is
score(a1, a2) =
(W1 ? S1)? (W2 ? S2)
cnt(a1) ? cnt(a2)
. (3)
Here W1 and S1 represent Web evidence of a1
and a2 being in the weak-strong and strong-weak
relation, respectively. W2 and S2 fit the reverse
pair (a2, a1) in the patterns and hence represent
the strong-weak and weak-strong relations, respec-
tively, in the opposite direction. Hence, overall,
(W1 ? S1) ? (W2 ? S2) represents the total weak-
strong score of the pair (a1, a2), i.e. the score of a1
being on the left of a2 on a relative intensity scale,
such that score(a1, a2) = ?score(a2, a1). The raw
frequencies in the score are divided by counts of the
patterns and by individual word unigram counts to
obtain a pointwise mutual information (PMI) style
normalization and hence avoid any bias in the score
due to high-frequency patterns or word unigrams.2
2.2 Global Ordering with an MILP
2.2.1 Objective and Constraints
Given pairwise scores, we now aim at producing a
global ranking of the input words that is much more
informative than the original pairwise scores. Joint
inference from multiple word pairs allows us to ben-
efit from global information: Due to the sparsity of
the pattern evidence, determining how two adjec-
tives relate to each other can sometimes e.g. only
be inferred by observing how each of them relate to
some third adjective.
We assume that we are given N input words A =
a1, . . . , aN that we wish to place on a linear scale,
say [0, 1]. Thus each word ai is to be assigned a
position xi ? [0, 1] based on the pairwise weak-
strong weights score(ai, aj). A positive value for
2In preliminary experiments on a development set, we also
evaluated other intuitive forms of normalization.
281
Figure 1: The input weak-strong data may contain one
or more cycles, e.g. due to noisy patterns, so the final
ranking will have to choose which input scores to honor
and which to remove.
score(ai, aj) means that ai is supposedly weaker
than aj and hence we would like to obtain xi < xj .
A negative value for score(ai, aj) means that ai is
assumed to be stronger than aj , so we would want
to obtain xi > xj . Therefore, intuitively, our goal
corresponds to maximizing the objective
?
i,j
sgn(xj ? xi) ? score(ai, aj) (4)
Note that it is important to use the signum func-
tion sgn() here, because we only care about the rel-
ative order of xi and xj . Maximizing?ij(xj?xi) ?
score(ai, aj) would lead to all words being placed
at the edges of the scale, because the highest scores
would dominate over all other ones. We do include
the score magnitudes in the objective, because they
help resolve contradictions in the pairwise scores
(e.g., see Figure 1). This is discussed in more de-
tail in Section 2.2.2.
In order to maximize this non-differentiable ob-
jective, we use Mixed Integer Linear Programming
(MILP), a variant of linear programming in which
some but not all of the variables are constrained to
be integers. Using an MILP formalization, we can
find a globally optimal solution in the joint deci-
sion space, and unlike previous work, we jointly ex-
ploit global information rather than just individual
local (pairwise) scores. To encode the objective in a
MILP, we need to introduce additional variables dij ,
wij , sij to capture the effect of the signum function,
as explained below.
We additionally also enable our MILP to make
use of any external equivalence (synonymy) infor-
mation E ? {1, . . . , N} ? {1, . . . , N} that may be
available. In this context, two words are considered
synonymous if they are close enough in meaning to
be placed on (almost) the same position in the inten-
sity scale. If (i, j) ? E, we can safely assume that
ai, aj have near-equivalent intensity, so we should
encourage xi, xj to remain close to each other. The
MILP is defined as follows:
maximize?
(i,j) 6?E
(wij ? sij) ? score(ai, aj)
?
?
(i,j)?E
(wij + sij) C
subject to
dij = xj ? xi ?i, j ? {1, . . . , N}
dij ? wijC ? 0 ?i, j ? {1, . . . , N}
dij + (1? wij)C > 0 ?i, j ? {1, . . . , N}
dij + sijC ? 0 ?i, j ? {1, . . . , N}
dij ? (1? sij)C < 0 ?i, j ? {1, . . . , N}
xi ? [0, 1] ?i ? {1, . . . , N}
wij ? {0, 1} ?i, j ? {1, . . . , N}
sij ? {0, 1} ?i, j ? {1, . . . , N}
The difference variables dij simply capture differ-
ences between xi, xj . C is any very large constant
greater than ?i,j |score(ai, aj)|; the exact value is
irrelevant. The indicator variables wij and sij are
jointly used to determine the value of the signum
function sgn(dij) = sgn(xj ? xi). Variables wij
become 1 if and only if dij > 0 and hence serve
as indicator variables for weak-strong relationships
in the output. Variables sij become 1 if and only if
dij < 0 and hence serve as indicator variables for
a strong-weak relationship in the output. The ob-
jective encourages wij = 1 for score(ai, aj) > 0
and sij = 1 for score(ai, aj) < 0.3 When equiva-
lence (synonymy) information is available, then for
(i, j) ? E both sij = 0 andwij = 0 are encouraged.
2.2.2 Discussion
Our MILP uses intensity evidence of all input
pairs together and assimilates all the scores via
global transitivity constraints to determine the posi-
tions of the input words on a continuous real-valued
scale. Hence, our approach addresses drawbacks
3In order to avoid numeric instability issues due to very
small score(ai, aj) values after frequency normalization, in
practice we have found it necessary to rescale them by a fac-
tor of 1 over the smallest |score(ai, aj)| > 0.
282
Figure 2: Equivalence Information: Knowing that am, a2
are synonyms gives the MILP an indication of where to
place an on the scale with respect to a1, a2, a3
of local or divide-and-conquer approaches, where
adjectives are scored with respect to selected pivot
words, and hence many adjectives that lack pairwise
evidence with the pivots are not properly classified,
although they may have order evidence with some
third adjective that could help establish the ranking.
Optional synonymy information can further help, as
shown in Figure 2.
Moreover, our MILP also gives higher weight
to pairs with higher scores, which is useful when
breaking global constraint cycles as in the simple
example in Figure 1. If we need to break a con-
straint violating triangle or cycle, we would have to
make arbitrary choices if we were ranking based on
sgn(score(a, b)) alone. Instead, we can choose a
better ranking based on the magnitude of the pair-
wise scores. A stronger score between an adjective
pair doesn?t necessarily mean that they should be
further apart in the ranking. It means that these two
words are attested together on the Web with respect
to the intensity patterns more than with other candi-
date words. Therefore, we try to respect the order of
such word pairs more in the final ranking when we
are breaking constraint-violating cycles.
3 Related Work
Hatzivassiloglou and McKeown (1993) presented
the first step towards automatic identification of ad-
jective scales, thoroughly discussing the background
of adjective semantics and a means of discovering
clusters of adjectives that belong on the same scale,
thus providing one way of creating the input for our
ranking algorithm.
Inkpen and Hirst (2006) study near-synonyms and
nuances of meaning differentiation (such as stylistic,
attitudinal, etc.). They attempt to automatically ac-
quire a knowledge base of near-synonym differences
via an unsupervised decision-list algorithm. How-
ever, their method depends on a special dictionary
of synonym differences to learn the extraction pat-
terns, while we use only a raw Web-scale corpus.
Mohammad et al (2013) proposed a method of
identifying whether two adjectives are antonymous.
This problem is related but distinct, because the de-
gree of antonymy does not necessarily determine
their position on an intensity scale. Antonyms (e.g.,
little, big) are not necessarily on the extreme ends of
scales.
Sheinman and Tokunaga (2009) and Sheinman et
al. (2012) present the most closely related previous
work on adjective intensities. They collect lexico-
semantic patterns via bootstrapping from seed adjec-
tive pairs to obtain pairwise intensities, albeit using
search engine ?hits?, which are unstable and prob-
lematic (Kilgarriff, 2007). While their approach
is primarily evaluated in terms of a local pairwise
classification task, they also suggest the possibil-
ity of ordering adjectives on a scale using a pivot-
based partitioning approach. Although intuitive in
theory, the extracted pairwise scores are frequently
too sparse for this to work. Thus, many adjec-
tives have no score with a particular headword. In
our experiments, we reimplemented this approach
and show that our MILP method improves over it
by allowing individual pairwise decisions to benefit
more from global information. Schulam and Fell-
baum (2010) apply the approach of Sheinman and
Tokunaga (2009) to German adjectives. Our method
extends easily to various foreign languages as de-
scribed in Section 5.
Another related task is the extraction of lexico-
syntactic and lexico-semantic intensity-order pat-
terns from large text corpora (Hearst, 1992;
Chklovski and Pantel, 2004; Tandon and de Melo,
2010). Sheinman and Tokunaga (2009) follows
Davidov and Rappoport (2008) to automatically
bootstrap adjective scaling patterns using seed ad-
jectives and Web hits. These methods thus can be
used to provide the input patterns for our algorithm.
VerbOcean by Chklovski and Pantel (2004) ex-
tracts various fine-grained semantic relations (in-
cluding the stronger-than relation) between pairs of
verbs, using lexico-syntactic patterns over the Web.
283
Our approach of jointly ranking a set of words using
pairwise evidence is also applicable to the VerbO-
cean pairs, and should help address similar sparsity
issues of local pairwise decisions. Such scales will
again be quite useful for language learners and lan-
guage understanding tools.
de Marneffe et al (2010) infer yes-or-no answers
to questions with responses involving scalar adjec-
tives in a dialogue corpus. They correlate adjectives
with ratings in a movie review corpus to find that
good appears in lower-rated reviews than excellent.
Finally, there has been a lot of work on measuring
the general sentiment polarity of words (Hatzivas-
siloglou and McKeown, 1997; Hatzivassiloglou and
Wiebe, 2000; Turney and Littman, 2003; Liu and
Seneff, 2009; Taboada et al, 2011; Yessenalina and
Cardie, 2011; Pang and Lee, 2008). Our work in-
stead aims at producing a large, unrestricted number
of individual intensity scales for different qualities
and hence can help in fine-grained sentiment analy-
sis with respect to very particular content aspects.
4 Experiments
4.1 Data
Input Clusters In order to obtain input clusters for
evaluation, we started out with the satellite cluster or
?dumbbell? structure of adjectives in WordNet 3.0,
which consists of two direct antonyms as the poles
and a number of other satellite adjectives that are se-
mantically similar to each of the poles (Gross and
Miller, 1990). For each antonymy pair, we deter-
mined an extended dumbbell set by looking up syn-
onyms and words in related (satellite adjective and
?see-also?) synonym sets. We cut such an extended
dumbbell into two antonymous halves and treated
each of these halves as a potential input adjective
cluster.
Most of these WordNet clusters are noisy for the
purpose of our task, i.e. they contain adjectives that
appear unrelatable on a single scale due to polysemy
and semantic drift, e.g. violent with respect to super-
natural and affected. Motivated by Sheinman and
Tokunaga (2009), we split such hard-to-relate ad-
jectives into smaller scale-specific subgroups using
the corpus evidence4. For this, we consider an undi-
4Note that we do not use the WordNet dataset of Sheinman
and Tokunaga (2009) for evaluation, as it does not provide full
438 
115 60 35 19 12 14 5 4 3 0 100 
200 300 
400 500 
2 3 4 5 6 7 8 9 10-14 15-17 
# of c
hains
 
Length of chain 
Figure 3: The histogram of cluster sizes after partitioning.
41 
27 
12 3 3 2 0 
10 
20 
30 
40 
50 
3 4 5 6 7 8 
# of 
chai
ns 
Length of chain 
Figure 4: The histogram of cluster sizes in the test set.
rected edge between each pair of adjectives that has
a non-zero intensity score (based on the Web-scale
scoring procedure described in Section 2.1.3). The
resulting graph is then partitioned into connected
components such that any adjectives in a subgraph
are at least indirectly connected via some path and
thus much more likely to belong to the same inten-
sity scale. While this does break up partitions when-
ever there is no corpus evidence connecting them,
ordering the adjectives within each such partition re-
mains a challenging task. This is because the Web
evidence will still not necessarily directly relate all
adjectives (in a partition) to each other. Addition-
ally, the Web evidence may still indicate the wrong
direction. Figure 3 shows the size distribution of the
resulting partitions.
Patterns To construct our intensity pattern set, we
started with a couple of common rankable adjective
seed pairs such as (good, great) and (hot, boiling)
and used the Web-scale n-grams corpus (Brants and
Franz, 2006) to collect the few most frequent pat-
terns between and around these seed-pairs (in both
directions). Among these, we manually chose a
scales. Instead, their annotators only made pairwise compar-
isons with select words, using a 5-way classification scheme
(neutral, mild, very mild, intense, very intense).
284
small set of intuitive patterns that are linguistically
useful for ordering adjectives, several of which had
not been discovered in previous work. These are
shown in Table 1. Note that we only collected pat-
terns that were not ambiguous in the two orders, for
example the pattern ?? , not ?? is ambiguous be-
cause it can be used as both ?good, not great? and
?great, not good?. Alternatively, one can easily also
use fully-automatic bootstrapping techniques based
on seed word pairs (Hearst, 1992; Chklovski and
Pantel, 2004; Yang and Su, 2007; Turney, 2008;
Davidov and Rappoport, 2008). However, our semi-
automatic approach is a simple and fast process that
extracts a small set of high-quality and very gen-
eral adjective-scaling patterns. This process can
quickly be repeated from scratch in any other lan-
guage. Moreover, as described in Section 5.1, the
English patterns can also be projected automatically
to patterns in other languages.
Development and Test Sets Section 2.1 describes
the method for collecting the intensity scores for ad-
jective pairs, using Web-scale n-grams (Brants and
Franz, 2006). We relied on a small development
set to test the MILP structure and the pairwise score
setup. For this, we manually chose 5 representative
adjective clusters from the full set of clusters.
The final test set, distinct from this development
set, consists of 569 word pairs in 88 clusters, each
annotated by two native speakers of English. Both
the gold test data (and our code) are freely avail-
able.5 To arrive at this data, we randomly drew 30
clusters each for cluster sizes 3, 4, and 5+ from the
histogram of partitioned adjective clusters in Fig-
ure 3. While labeling a cluster, annotators could ex-
clude words that they deemed unsuitable to fit on
a single shared intensity scale with the rest of the
cluster. Fortunately, the partitioning described ear-
lier had already separated most such cases into dis-
tinct clusters. The annotators ordered the remaining
words on a scale. Words that seemed indistinguish-
able in strength could share positions in their anno-
tation.
As our goal is to compare scale formation algo-
rithms, we did not include trivial clusters of size 2.
On such trivial clusters, the Web evidence alone de-
termines the output and hence all algorithms, includ-
5http://demelo.org/gdm/intensity/
ing the baseline, obtain the same pairwise accuracy
(defined below) of 93.3% on a separate set of 30 ran-
dom clusters of size 2.
Figure 4 shows the distribution of cluster sizes in
our main gold set. The inter-annotator agreement in
terms of Cohen?s ? (Cohen, 1960) on the pairwise
classification task with 3 labels (weaker, stronger,
or equal/unknown) was 0.64. In terms of pairwise
accuracy, the agreement was 78.0%.
4.2 Metrics
In order to thoroughly evaluate the performance of
our adjective ordering procedure, we rely on both
pairwise and ranking-correlation evaluation metrics.
Consider a set of input words A = {a1, a2, . . . , an}
and two rankings for this set ? a gold-standard rank-
ing rG(A) and a predicted ranking rP (A).
4.2.1 Pairwise Accuracy
For a pair of words ai, aj , we may consider the
classification task of choosing one of three labels (<,
>, =?) for the case of ai being weaker, stronger, and
equal (or unknown) in intensity, respectively, com-
pared to a2:
L(a1, a2) =
?
?
?
< if r(ai) < r(aj)
> if r(ai) > r(aj)
=? if r(ai) = r(aj)
For each pair (a1, a2), we compute gold-standard
labelsLG(a1, a2) and predicted labelsLP (a1, a2) as
above, and then the pairwise accuracy PW (A) for
a particular ordering on A is simply the fraction of
pairs that are correctly classified, i.e. for which the
predicted label is same as the gold-standard label:
PW (A) =
?
i<j
1{LG(ai, aj) = LP (ai, aj)}
?
i<j
1
4.2.2 Ranking Correlation Coefficients
Our second type of evaluation assesses the
rank correlation between two ranking permutations
(gold-standard and predicted). Many studies use
Kendall?s tau (Kendall, 1938), which measures the
total number of pairwise inversions, while others
prefer Spearman?s rho (Spearman, 1904), which
measures the L1 distance between ranks.
285
Kendall?s tau correlation coefficient We use the
?b version of Kendall?s correlation metric, as it in-
corporates a correction for ties (Kruskal, 1958; Dou
et al, 2008):
?b =
P ?Q?
(P +Q+X0) ? (P +Q+ Y0)
where P is the number of concordant pairs, Q is
the number of discordant pairs, X0 is the number
of pairs tied in the first ranking, Y0 is the number of
pairs tied in the second ranking. Given the two rank-
ings of an adjective set A, the gold-standard ranking
rG(A) and the predicted ranking rP (A), two words
ai, aj are:
? concordant iff both rankings have the same strict
order of the two elements, i.e., rG(ai) > rG(aj)
and rP (ai) > rP (aj), or rG(ai) < rG(aj) and
rP (ai) < rP (aj).
? discordant iff the two rankings have an inverted
strict order of the two elements, i.e., rG(ai) >
rG(aj) and rP (ai) < rP (aj), or rG(ai) <
rG(aj) and rP (ai) > rP (aj).
? tied iff rG(ai) = rG(aj) or rP (ai) = rP (aj).
Spearman?s rho correlation coefficient For two
n-sized ranked lists {xi} and {yi}, the Spearman
correlation coefficient is defined as the Pearson cor-
relation coefficient between the ranks of variables:
? =
?
i
(xi ? x?) ? (yi ? y?)
??
i
(xi ? x?)2 ?
?
i
(yi ? y?)2
Here, x? and y? denote the means of the values in the
respective lists. We use the standard procedure for
handling ties correctly. Tied values are assigned the
average of all ranks of items sharing the same value
in the ranked list sorted in ascending order of the
values.
Handling Inversions While annotating, we some-
times observed that the ordering itself was very clear
but the annotators disagreed about which end of a
particular scale was to count as the strong one, e.g.
when transitioning from soft to hard or from alpha
to beta. We thus also report average absolute values
of both correlation coefficients, as these properly ac-
count for anticorrelations. Our test set only contains
clusters of size 3 or larger, so there is no need to
account for inversions in clusters of size 2.
4.3 Results
In Table 3, we use the evaluation metrics mentioned
above to compare several different approaches.
Web Baseline The first baseline simply reflects
the original pairwise Web-based intensity scores.
We classify (with one of 3 labels) a given pair of
adjectives using the Web-based intensity scores (as
described in Section 2.1.3) as follows:
Lbaseline(a1, a2) =
?
?
?
< if score(ai, aj) > 0
> if score(ai, aj) < 0
=? if score(ai, aj) = 0
Since score(ai, aj) represents the weak-strong
score of the two adjectives, a more positive value
means a higher likelihood of ai being weaker (<, on
the left) in intensity than aj .
In Table 3, we observe that the (micro-averaged)
pairwise accuracy, as defined earlier, for the origi-
nal Web baseline is 48.2%, while the ranking mea-
sures are undefined because the individual pairs do
not lead to a coherent scale.
Divide-and-Conquer The divide-and-conquer
baseline recursively splits a set of words into three
subgroups, placed to the left (weaker), on the same
position (no evidence), or to the right (stronger) of a
given randomly chosen pivot word.
While this approach shows only a minor improve-
ment in terms of the pairwise accuracy (50.6%), its
main benefit is that one obtains well-defined inten-
sity scales rather than just a collection of pairwise
scores.
Sheinman and Tokunaga The approach by
Sheinman and Tokunaga (2009) involves a simi-
lar divide-and-conquer based partitioning in the first
phase, except that their method makes use of syn-
onymy information from WordNet and uses all syn-
onyms in WordNet?s synset for the headword as
neutral pivot elements (if the headword is not in
WordNet, then the word with the maximal unigram
frequency is chosen). In the second phase, their
method performs pairwise comparisons within the
more intense and less intense subgroups. We reim-
plement their approach here, using the Google N-
Grams dataset instead of online Web search engine
hits. We observe a small improvement over the Web
baseline in terms of pairwise accuracy. Note that the
286
Method Pairwise Accuracy Avg. ? Avg. |? | Avg. ? Avg. |?|
Web Baseline 48.2% N/A N/A N/A N/A
Divide-and-Conquer 50.6% 0.45 0.53 0.52 0.62
Sheinman and Tokunaga (2009) 55.5% N/A N/A N/A N/A
MILP 69.6% 0.57 0.65 0.64 0.73
MILP with synonymy 78.2% 0.57 0.66 0.67 0.80
Inter-Annotator Agreement 78.0% 0.67 0.76 0.75 0.86
Table 3: Main test results
Predicted Class
Weaker Tie Stronger
True Class
Weaker 117 127 15
Tie 5 42 15
Stronger 11 122 115
Table 4: Confusion matrix (Web baseline)
rank correlation measure scores are undefined for
their approach. This is because in some cases their
method placed all words on the same position in the
scale, which these measures cannot handle even in
their tie-corrected versions. Overall, the Sheinman
and Tokunaga approach does not aggregate informa-
tion sufficiently well at the global level and often
fails to make use of transitive inference.
MILP Our MILP exploits the same pairwise
scores to induce significantly more accurate pair-
wise labels with 69.6% accuracy, a 41% relative
error reduction over the Web baseline, 38% over
Divide-and-Conquer, and 32% over Sheinman and
Tokunaga (2009). We further see that our MILP
method is able to exploit external synonymy (equiv-
alence) information (using synonyms marked by the
annotators). The accuracy of the pairwise scores as
well as the quality of the overall ranking increase
even further to 78.2%, approaching the human inter-
annotator agreement. In terms of average correlation
coefficients, we observe similar improvement trends
from the MILP, but of different magnitudes, because
these averages give small clusters the same weight
as larger ones.
4.4 Analysis
Confusion Matrices For a given approach, we
can study the confusion matrix obtained by cross-
tabulating the gold classification with the predicted
Predicted Class
Weaker Tie Stronger
True Class
Weaker 177 29 53
Tie 9 24 29
Stronger 15 38 195
Table 5: Confusion matrix (MILP)
classification of every unique pair of adjectives in
the ground truth data. Table 4 shows the confusion
matrix for the Web baseline. We observe that due to
the sparsity of pairwise intensity order evidence, the
baseline method predicts too many ties.
Table 5 provides the confusion matrix for the
MILP (without external equivalence information)
for comparison. Although the middle column still
shows that the MILP predicts more ties than humans
annotators, we find that a clear majority of all unique
pairs are now correctly placed along the diagonal.
This confirms that our MILP successfully infers new
ordering decisions, although it uses the same input
(corpus evidence) as the baseline. The remaining
ties are mostly just the result of pairs for which there
simply is no evidence at all in the input Web counts.
Note that this problem could for instance be circum-
vented by relying on a crowdsourcing approach: A
few dispersed tie-breakers are enough to allow our
MILP to correct many other predictions.
Predicted Examples Finally, in Table 6, we pro-
vide a selection of real results obtained by our algo-
rithm. For instance, it correctly inferred that terri-
fying is more intense than creepy or scary, although
the Web pattern counts did not provide any explicit
information about these words pairs. In some cases,
however, the Web evidence did not suffice to draw
the right conclusions, or it was misleading due to is-
sues like polysemy (as for the word funny).
287
Accuracy Prediction Gold Standard
Good
hard
< painful
< hopeless
hard
< painful
< hopeless
full
< stuffed
< (overflowing,
overloaded)
full
< stuffed
< overflowing
< overloaded
unusual
< uncommon
< rare
< exceptional
< extraordinary
uncommon
< unusual
< rare
< extraordinary
< exceptional
Average creepy
< scary
< sinister
< frightening
< terrifying
creepy
< (scary, frightening)
< terrifying
< sinister
Bad (awake, conscious)< alive
< aware
alive
< awake
< (aware, conscious)
strange
< (unusual, weird)
< (funny, eerie)
(strange, funny)
< unusual
< weird
< eerie
Table 6: Some examples (of bad, average and good accu-
racy) of our MILP predictions (without synonymy infor-
mation) and the corresponding gold-standard annotation.
While we show results on gold-standard chains
here for evaluation purposes, in practice one can also
recombine two [0, 1] chains for a pair of antonymic
clusters to form a single scale from [?1, 1] that visu-
alizes the full spectrum of available adjectives along
a dimension, from adjacent all the way to removed,
or from black to glaring.
5 Extension to Multilingual Ordering
Our method for globally ordering words on a scale
can easily be applied to languages other than En-
glish. The entire process is language-independent
as long as the required resources are available and a
small number of patterns are chosen. For morpho-
logically rich languages, the information extraction
step of course may require additional morphologi-
cal analysis tools for stemming and aggregating fre-
quencies across different forms.
Alternatively, a cross-lingual projection approach
is possible at multiple levels, utilizing information
from the English data and ranking. As the first step,
the set of words in the target language that we wish
to rank can be projected from the English word set if
necessary ? e.g., as shown in de Melo and Weikum
(2009). Next, we outline two projection methods for
the ordering step. The first method is based on pro-
jection of the English intensity-ordering patterns to
the new language, and then using the same MILP
as described in Section 2.2. In the second method,
we also change the MILP and add cross-lingual con-
straints to better inform the target language?s ad-
jective ranking. A detailed empirical evaluation of
these approaches remains future work.
5.1 Cross-Lingual Pattern Projection
Instead of creating new patterns, in many cases
we obtain quite adequate intensity patterns by us-
ing cross-lingual projection. We simply take sev-
eral adjective pairs, instantiate the English patterns
with them, and obtain new patterns using a machine
translation system. Filling the wildcards in a pat-
tern, say ?? but not ??, with good/excellent results in
?good but not excellent?. This phrase is then trans-
lated into the target language using the translation
system, say into German ?gut aber nicht ausgezeich-
net?. Finally, put back the wildcards in the place of
the translations of the adjective words, here gut and
ausgezeichnet, to get the corresponding German pat-
tern ?? aber nicht ??. Table 7 shows various German
intensity patterns that we obtain by projecting from
the English patterns as described. The process is re-
peated with multiple adjective pairs in case different
variants are returned, e.g. due to morphology. Most
of these translations deliver useful results.
Now that we have the target language adjectives
and the ranking patterns, we can compute the pair-
wise intensity scores using large-scale data in that
language. We can use the Google n-grams cor-
pora for 10 European languages (Brants and Franz,
2009), and also for Chinese (LDC2010T02) and
Japanese (LDC2009T08). For other languages, one
can use available large raw-text corpora or Web
crawling tools.
5.2 Crosslingual MILP
To improve the rankings for lesser-resourced lan-
guages, we can further use a joint MILP approach
for the new language we want to transfer this pro-
cess to. Additional constraints between the English
288
Weak-Strong Patterns Strong-Weak Patterns
English German English German
? but not ? ? aber nicht ? not ? just ? nicht ? gerade ?
? if not ? ? wenn nicht ? not ? but just ? nicht ? aber nur ?
? and almost ? ? und fast ? not ? though still ? nicht ? aber immer noch ?
not just ? but ? nicht nur ? sondern ? ? or very ? ? oder sehr ?
Table 7: Examples of German intensity patterns projected (translated) directly from the English patterns.
words and their corresponding target language trans-
lations, in combination with the English ranking in-
formation, allow the algorithm to obtain better rank-
ings for the target words whenever the non-English
target language corpus does not provide sufficient
intensity order evidence.
In this case, the input set A contains words
in multiple languages. The Web intensity scores
score(ai, aj) should be set to zero when comparing
words across languages. We instead link them using
a translation table T ? {1, . . . , N} ? {1, . . . , N}
from a translation dictionary or phrase table. Here,
(i, j) ? T signifies that ai is a translation of aj . We
do not require a bijective relationship between them
(i.e., translations needn?t be unique). The objective
function is augmented by adding the new term
?
(i,j)?T
(w?ij + s?ij)CT (5)
for a constant CT > 0 that determines how much
weight we assign to translations as opposed to the
corpus count scores. The MILP is extended by
adding the following extra constraints.
dij ? w?ijCT < ?dmax ?i, j ? {1, . . . , N}
dij + (1? w?ij)CT ? ?dmax ?i, j ? {1, . . . , N}
dij + s?ijCT > dmax ?i, j ? {1, . . . , N}
dij ? (1? s?ij)CT ? dmax ?i, j ? {1, . . . , N}
w?ij ? {0, 1} ?i, j ? T
s?ij ? {0, 1} ?i, j ? T
The variables di,j , as before, encode distances be-
tween positions of words on the scale, but now also
include cross-lingual pairs of words in different lan-
guages. The new constraints encourage translational
equivalents to remain close to each other, preferably
within a desired (but not strictly enforced) maximum
distance dmax. The new variables w?ij , s?ij are sim-
ilar to wij , sij in the standard MILP. However, the
w?ij become 1 if and only if dij ? ?dmax and the s?ij
become 1 if and only if dij ? dmax. If both w?ij and
s?ij are 1, then the two words have a small distance
?dmax ? dij ? dmax. The augmented objective
function explicitly encourages this for translational
equivalents. Overall, this approach thus allows evi-
dence from a language with more Web evidence to
improve the process of adjective ordering in lesser-
resourced languages.
6 Conclusion
In this work, we have presented an approach to the
challenging and little-studied task of ranking words
in terms of their intensity on a continuous scale. We
address the issue of sparsity of the intensity order ev-
idence in two ways. First, pairwise intensity scores
are computed using linguistically intuitive patterns
in a very large, Web-scale corpus. Next, a Mixed
Integer Linear Program (MILP) expands on this fur-
ther by inferring new relative relationships. Instead
of making ordering decisions about word pairs in-
dependently, our MILP considers the joint decision
space and factors in e.g. how two adjectives relate
to some third adjective, thus enforcing global con-
straints such as transitivity.
Our approach is general enough to allow addi-
tional evidence such as synonymy in the MILP,
and can straightforwardly be applied to other word
classes (such as verbs), and to other languages
(monolingually as well as cross-lingually). The
overall results across multiple metrics are substan-
tially better than previous approaches, and fairly
close to human agreement on this challenging task.
Acknowledgments
We would like to thank the editor and the anony-
mous reviewers for their helpful feedback.
289
References
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL 2011.
Thorsten Brants and Alex Franz. 2006. The Google Web
1T 5-gram corpus version 1.1. LDC2006T13.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 European languages, version 1. LDC2009T25.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37?46.
Dmitry Davidov and Ari Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
sat analogy questions. In Proceedings of ACL 2008.
Marie-Catherine de Marneffe, Christopher D. Manning,
and Christopher Potts. 2010. Was it good? it was
provocative. learning the meaning of scalar adjectives.
In Proceedings of ACL 2010.
Gerard de Melo and Gerhard Weikum. 2009. Towards
a universal wordnet by learning from combined evi-
dence. In Proceedings of CIKM 2009.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-Rong
Wen. 2008. Are click-through data adequate for learn-
ing web search rankings? In Proc. of CIKM 2008.
Derek Gross and Katherine J. Miller. 1990. Adjectives
in WordNet. International Journal of Lexicography,
3(4):265?277.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1993. Towards the automatic identification of adjecti-
val scales: Clustering adjectives according to meaning.
In Proceedings of ACL 1993.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL 1997.
Vasileios Hatzivassiloglou and Janyce M. Wiebe. 2000.
Effects of adjective orientation and gradability on sen-
tence subjectivity. In Proceedings of COLING 2000.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING
1992.
Diana Inkpen and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym dif-
ferences. Computational Linguistics, 32(2):223?262.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30(1/2):81?93.
Adam Kilgarriff. 2007. Googleology is bad science.
Computational Linguistics, 33(1).
William H. Kruskal. 1958. Ordinal measures of associa-
tion. Journal of the American Statistical Association,
53(284):814?861.
Jingjing Liu and Stephanie Seneff. 2009. Review senti-
ment scoring via a parse-and-paraphrase paradigm. In
Proceedings of EMNLP 2009.
George A. Miller. 1995. WordNet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Said M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Peter F. Schulam and Christiane Fellbaum. 2010. Au-
tomatically determining the semantic gradation of ger-
man adjectives. In Proceedings of KONVENS 2010.
Vera Sheinman and Takenobu Tokunaga. 2009. AdjS-
cales: Visualizing differences between adjectives for
language learners. IEICE Transactions on Information
and Systems, 92(8):1542?1550.
Vera Sheinman, Takenobu Tokunaga, I. Julien, P. Schu-
lam, and C. Fellbaum. 2012. Refining WordNet adjec-
tive dumbbells using intensity relations. In Proceed-
ings of Global WordNet Conference 2012.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006.
Charles Spearman. 1904. The proof and measurement of
association between two things. The American journal
of psychology, 15(1):72?101.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. SOFIE: a self-organizing framework
for information extraction. In Proceedings of WWW
2009.
Maite Taboada, Julian Brooke, Milan Tofiloskiy, and
Kimberly Vollz. 2011. Lexicon-based methods for
sentiment analysis. Computational Linguistics.
Niket Tandon and Gerard de Melo. 2010. Information
extraction from web-scale n-gram data. In Proceed-
ings of the SIGIR 2010 Web N-gram Workshop.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Trans. Inf. Syst.,
21(4):315?346, October.
Peter D. Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of COLING 2008.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL
2007.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of EMNLP 2011.
290
