Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 25?32,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Aggregating Machine Learning and Rule Based Heuristics for Named 
Entity Recognition 
 Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and  
Dipti Misra Sharma 
Language Technologies Research Centre, 
International Institute of Information Technology, 
Hyderabad, India. 
karthikg@students.iiit.ac.in, surana.h@gmail.com,  
ashwini_vaidya@research.iiit.ac.in, praneethms@students.iiit.ac.in, 
dipti@iiit.ac.in 
 
 
 
Abstract 
This paper, submitted as an entry for the 
NERSSEAL-2008 shared task, describes a 
system build for Named Entity Recognition 
for South and South East Asian Languages.  
Our paper combines machine learning 
techniques with language specific heuris-
tics to model the problem of NER for In-
dian languages. The system has been tested 
on five languages: Telugu, Hindi, Bengali, 
Urdu and Oriya. It uses CRF (Conditional 
Random Fields) based machine learning, 
followed by post processing which in-
volves using some heuristics or rules. The 
system is specifically tuned for Hindi and 
Telugu, we also report the results for the 
other four languages. 
1 Introduction 
Named Entity Recognition (NER) is a task that 
seeks to locate and classify entities (?atomic ele-
ments?) in a text into predefined categories such as 
the names of persons, organizations, locations, ex-
pressions of times, quantities, etc. It can be viewed 
as a two stage process: 
  
1. Identification of entity boundaries 
2. Classification into the correct category 
 
For example, if ?Mahatma Gandhi? is a named 
entity in the corpus, it is necessary to identify the 
beginning and the end of this entity in the sentence. 
Following this step, the entity must be classified 
into the predefined category, which is NEP 
(Named Entity Person) in this case. 
This task is the precursor for many natural lan-
guage processing applications. It has been used in 
Question Answering (Toral et al 2005) as well as 
Machine Translation (Babych et al 2004). 
The NERSSEAL contest has used 12 categories 
of named entities to define a tagset. The data has 
been manually tagged for training and testing pur-
poses for the contestants. 
The task of building a named entity recognizer 
for South and South East Asian languages presents 
several problems related to their linguistic charac-
teristics. We will first discuss some of these lin-
guistic issues, followed by a description of the 
method used. Further, we show some of the heuris-
tics used for post-processing and finally an analy-
sis of the results obtained.  
2 Previous Work  
The linguistic methods generally use rules 
manually written by linguists. There are several 
rule based NER systems, containing mainly lexi-
calized grammar, gazetteer lists, and list of trigger 
words, which are capable of providing upto 92% f-
measure accuracy for English (McDonald, 1996; 
Wakao et al, 1996).  
Linguistic approach uses hand-crafted rules 
which need skilled linguistics. The chief disadvan-
tage of these rule-based techniques is that they re-
quire huge experience and grammatical knowledge 
of the particular language or domain and these sys-
tems are not transferable to other languages or do-
mains. However, given the closer nature of many 
Indian languages, the cost of adaptation of a re-
25
source from one language to another could be quite 
less (Singh and Surana, 2007). 
Various machine learning techniques have also 
been successfully used for the NER task. Generally 
hidden markov model (Bikel et al,1997), maxi-
mum entropy (Borthwick, 1999), conditional ran-
dom field (Li and Mccallum, 2004) are more popu-
lar machine learning techniques used for the pur-
pose of NER. 
Hybrid systems have been generally more effec-
tive at the task of NER. Given lesser data and more 
complex NE classes which were present in 
NERSSEAL shared task, hybrid systems make 
more sense. Srihari et al (2000) combines MaxEnt, 
hidden markov model (HMM) and handcrafted 
rules to build an NER system. 
Though not much work has been done for other 
South Asian languages, some previous work fo-
cuses on NER for Hindi. It has been previously 
attempted by Cucerzan and Yarowsky in their lan-
guage independent NER work which used morpho-
logical and contextual evidences (Cucerzan and 
Yarowsky, 1999). They ran their experiment with 
5 different languages. Among these the accuracy 
for Hindi was the worst. For Hindi the system 
achieved 42% f-value with a recall of 28% and 
about 85% precision. A result which highlights 
lack of good training data, and other various issues 
involved with linguistic handling of Indian lan-
guages. 
Later approaches have resulted in better results 
for Hindi. Hindi NER system developed by Wei Li 
and Andrew Mccallum (2004) using conditional 
random fields (CRFs) with feature induction have 
achieved f-value of 71%. (Kumar and Bhat-
tacharyya, 2006) used maximum entropy markov 
model to achieve f-value of upto 80%. 
3 Some Linguistic Issues 
3.1 Agglutinative Nature 
Some of the SSEA languages have agglutinative 
properties.  For example, a Dravidian language like 
Telugu has a number of postpositions attached to a 
stem to form a single word. An example is: 
 
guruvAraMwo = guruvAraM + wo  
up to Wednesday = Wednesday + up to 
 
Most of the NERs are suffixed with several dif-
ferent postpositions, which increase the number of 
distinct words in the corpus.  This in turn affects 
the machine learning process. 
3.2 No Capitalization 
All the five languages have scripts without graphi-
cal cues like capitalization, which could act as an 
important indicator for NER.  For a language like 
English, the NER system can exploit this feature to 
its advantage. 
3.3 Ambiguity 
One of the properties of the named entities in these 
languages is the high overlap between common 
names and proper names. For instance Kamal (in 
Hindi) can mean ?lotus?, which is not a named en-
tity, but it can also be a person?s name, in which 
case, it is a named entity. 
Among the named entities themselves, there is 
ambiguity between a location name Bangalore ek 
badzA shaher heI (Bangalore is a big city) or a per-
son?s surname ?M. Bangalore shikshak heI? (M. 
Bangalore is a teacher). 
3.4 Low POS Tagging Accuracy for Nouns 
For English, the available tools like POS (Part of 
Speech) tagger can be used to provide features for 
machine learning. This is not very helpful for 
SSEA languages because the accuracy for noun 
and proper noun tags is quite low (PVS and G., 
2006) Hence, features based on POS tags cannot 
be used for NER for these languages. 
To illustrate this difficulty, we conducted the 
following experiment. A POS tagger (described in 
PVS & G.,2006) was run on the Hindi test data.  
The data had 544 tokens with NEL, NEP, NEO 
tags.  The POS tagger should have given the NNP 
(proper noun) tag for all those named entities. 
However the tagger was able to tag only 80 tokens 
accurately. This meant that only 14.7% of the 
named entities were correctly recognized. 
3.5 Spelling Variation 
One other important language related issue is the 
variation in the spellings of proper names. For in-
stance the same name Shri Ram Dixit can be writ-
ten as Sri. Ram Dixit, Shree Ram Dixit, Sh. R. Dixit 
and so on. This increases the number of tokens to 
be learnt by the machine and would perhaps also 
require a higher level task like co-reference resolu-
tion. 
 
26
2.6 Pattern of suffixes We have converted this format into the BIO 
format as described in Ramshaw et. al. For exam-
ple, the above format will now be shown as: 
 
Named entities of Location (NEL) or Person 
(NEP) will share certain common suffixes, which 
can be exploited by the learning algorthm. For in-
stance, in Hindi, -pur (Rampur, Manipur) or -giri 
(Devgiri) are suffixes that will appear in the named 
entities for Location. Similarly, there are suffixes 
like -swamy (Ramaswamy, Krishnaswamy) or -
deva (Vasudeva, Mahadeva) which can be com-
monly found in named entities for person. These 
suffixes are cues for some of the named entities in 
the SSEA languages. 
 
Rabindranath  B-NEP 
Tagore   I-NEP 
ne   O 
kahaa   O 
 
The training data set contains (approximately) 
400,000 Hindi, 50,000 Telugu, 35,000 Urdu, 
93,000 Oriya and 120,000 Bengali words respec-
tively.  
A NER system can be rule-based, statistical or 
hybrid. A rule-based NER system uses hand-
written rules to tag a corpus with named entities. A 
statistical NER system learns the probabilities of 
named entities using training data, whereas hybrid 
systems use both. 
5 Conditional Random Fields 
Conditional Random Fields (CRFs) are undirected 
graphical models used to calculate the conditional 
probability of values on designated output nodes 
given values assigned to other designated input 
nodes. Developing rule-based taggers for NER can be cumbersome as it is a language specific process. 
Statistical taggers require large amount of anno-
tated data (the more the merrier) to train.  Our sys-
tem is a hybrid NER tagger which first uses Condi-
tional Random Fields (CRF) as a machine learning 
technique followed by some rule based post-
processing. 
In the special case in which the output nodes of 
the graphical model are linked by edges in a linear 
chain, CRFs make a first-order Markov independ-
ence assumption, and thus can be understood as 
conditionally-trained finite state machines (FSMs). 
Let o = (o,,o
We treat the named entity recognition problem 
as a sequential token-based tagging problem. 
According to Lafferty et. al. CRF outperforms 
other Machine Learning algorithms viz., Hidden 
Markov Models (HMM), Maximum Entropy 
Markov Model (MEMM) for  sequence labeling 
tasks.  
4 Training data 
The training data given by the organizers was in 
SSF format1. For example in SSF format, the 
named entity ?Rabindranath Tagore? will be shown 
in the following way: 
0 (( SSF 
1  ((  NP  <ne=NEP> 
1.1  Rabindranath 
1.2 Tagore 
)) 
2 ne 
3 kahaa 
 )) 
 
                                                          
1 http://shiva.iiit.ac.in/SPSAL2007/ssf-analysis-representation.pdf
2,o3 ,o4 ,... oT  ) be some observed in-
put data sequence, such as a sequence of words in 
text in a document,(the values on n input nodes of 
the graphical model). Let S be a set of FSM states, 
each of which is associated with a label, l ? ?. 
Let s = (s ,s ,s  ,s  ,... s1 2 3 4 T ) be some sequence of 
states, (the values on T output nodes). By the 
Hammersley-Clifford theorem, CRFs define the 
conditional probability of a state sequence given an 
input sequence to be: 
 
where Zo is a normalization factor over all state 
sequences is an arbitrary feature function over its 
arguments, and ?k is a learned weight for each fea-
ture function. A feature function may, for example, 
be defined to have value 0 or 1. Higher ? weights 
make their corresponding FSM transitions more 
likely. CRFs define the conditional probability of a 
label sequence based on the total probability over 
the state sequences, 
 
 
27
 
where l(s) is the sequence of labels correspond-
ing to the labels of the states in sequence s. 
Note that the normalization factor, Zo, (also 
known in statistical physics as the partition func-
tion) is the sum of the scores of all possible states. 
 
And that the number of state sequences is expo-
nential in the input sequence length, T. In arbitrar-
ily-structured CRFs, calculating the partition func-
tion in closed form is intractable, and approxima-
tion methods such as Gibbs sampling or loopy be-
lief propagation must be used. In linear-chain 
structured CRFs (in use here for sequence model-
ing), the partition function can be calculated effi-
ciently by dynamic programming. 
6 CRF Based Machine Learning 
We used the CRF model to perform the initial tag-
ging followed by post-processing. 
6.1 Statistical Tagging 
In the first phase, we have used language inde-
pendent features to build the model using CRF. 
Orthographic features (like capitalization, decimals), 
affixes (suffixes and prefixes), context (previous 
words and following words), gazetteer features, POS 
and morphological features etc. are generally used for 
NER. In English and some other languages, capitali-
zation features play an important role as NEs are 
 generally capitalized for these languages. Unfortu-
nately as explained above this feature is not applica-
ble for the Indian languages. 
Precision Recall F-Measure  
Pm Pn Pl Rm Rn Rl Fm Fn Fl  
Bengali 53.34 49.28 58.27 26.77 25.88 31.19 35.65 33.94 40.63 
Hindi 59.53 63.84 64.84 41.21 41.74 40.77 48.71 50.47 50.06 
Oriya 39.16 40.38 63.70 23.39 19.24 28.15 29.29 26.06 39.04 
Telugu 10.31 71.96 65.45 68.00 30.85 29.78 08.19 43.19 40.94 
Urdu 43.63 44.76 48.96 36.69 34.56 39.07 39.86 39.01 43.46 
Table 1: Evaluation of the NER System for Five Languages 
The exact set of features used are described be-
low. 
6.2 Window of the Words 
Words preceding or following the target word may 
be useful for determining its category. Following a 
few trials we found that a suitable window size is 
five. 
6.3 Suffixes 
Statistical suffixes of length 1 to 4 have been con-
sidered. These can capture information for named 
entities having the NEL tag like Hyderabad, 
Secunderabad, Ahmedabad etc., all of which end 
in -bad. We have collected lists of such suffixes for 
NEP (Named Entity Person) and NEL (Named En-
tity Location) for Hindi. In the machine learning 
model, this resource can be used as a binary fea-
ture. A sample of these lists is as follows: 
 
Type of NE Example suffixes 
(Hindi) 
NE- Location -desa, -vana, -nagara,  
-garh, -rashtra, -giri  
NE ? Person -raja, -natha, -lal, -bhai,-
pathi, -krishnan 
 Table 2: Suffixes for Hindi NER 
28
7 Heuristics Based Post Processing 6.4 Prefixes 
Statistical prefixes of length 1 to 4 have been con-
sidered. These can take care of the problems asso-
ciated with a large number of distinct tokens. As 
mentioned earlier, agglutinative languages can 
have a number of postpositions. The use of pre-
fixes will increase the probability of   Hyderabad 
and Hyderabadlo (Telugu for ?in Hyderabad?) be-
ing treated as the same token. 
Complex named entities like fifty five kilograms 
contain a Named Entity Number within a Named 
Entity Measure. We observed that these were not 
identified accurately enough in the machine learn-
ing based system. Hence, instead of applying ma-
chine learning to handle nested entities we make 
use of rule-based post processing.  
7.1 Second Best Tag 
Table 3: F-Measure (Lexical) for NE Tags 
 Bengali Hindi Oriya Telugu Urdu 
It was observed that the recall of the CRF model is 
low. In order to improve recall, we have used the 
following rule:  if the best tag given by the CRF 
model is O (not a named entity) and the confidence 
of the second best tag is greater than 0.15, then the 
second best tag is considered as the correct tag. 
NEP 35.22 54.05 52.22 01.93 31.22 
NED NA 42.47 01.97 NA 21.27 
NEO 11.59 45.63 14.50 NA 19.13 
NEA NA 61.53 NA NA NA 
We observed an increase of 7% in recall and 3% 
decrease in precision. This resulted in a 4% in-
crease in the F-measure, which is a significant in-
crease in performance. The decrease in precision is 
expected as we are taking the second tag. 
NEB NA NA NA NA NA 
NETP 42.30 NA NA NA NA 
NETO 33.33 13.77 NA 01.66 NA 
NEL 45.27 62.66 48.72 01.49 57.85 
7.2 Nested Entities NETI 55.85 79.09 40.91 71.35 63.47 
NEN 62.67 80.69 24.94 83.17 13.75 One of the important tasks in the contest was to 
identify nested named entities. For example if we 
consider eka kilo (Hindi: one kilo) as NEM 
(Named Entity Measure), it contains a NEN 
(Named Entity Number) within it. 
NEM 60.51 43.75 19.00 26.66 84.10 
NETE 19.17 31.52 NA 08.91 NA
The CRF model tags eka kilo as NEM and in or-
der to tag eka as NEN we have made use of other 
resources like a gazetteer for the list of numbers. 
We used such lists for four languages. 
6.5 Start of a sentence 
There is a possibility of confusing the NEN 
(Named Entity Number) in a sentence with the 
number that appears in a numbered list. The num-
bered list will always have numbers at the begin-
ning of a sentence and hence a feature that checks 
for this property will resolve the ambiguity with an 
actual NEN. 
7.3 Gazetteers 
For Hindi, we made use of three different kinds of 
gazetteers. These consisted of lists for measures 
(entities like kilogram, millimetre, lakh), numerals 
and quantifiers (one, first, second) and time ex-
pressions (January, minutes, hours) etc. Similar 
lists were used for all the other languages except 
Urdu. These gazetteers were effective in identify-
ing this relatively closed class of named entities 
and showed good results for these languages. 
6.6 Presence of digits 
Usually, the presence of digits indicates that the 
token is a named entity. For example, the tokens 
92, 10.1 will be identified as Named Entity Num-
ber based on the binary feature ?contains digits?. 
6.7 Presence of  four digits 8 Evaluation 
If the token is a four digit number, it is likelier to 
be a NETI (Named Entity Time). For example, 
1857, 2007 etc. are most probably years. 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
 
29
1. Maximal Matches: The largest possible 
named entities are matched with the refer-
ence data. 
The amount of annotated corpus available for 
Hindi was substantially more. This should have 
ideally resulted in better results for Hindi with the 
machine learning approach. But, the results were 
only marginally better than other languages. A ma-
jor reason for this was that a very high percentage 
(44%) of tags in Hindi were NETE. The tagset 
gives examples like ?Horticulture?, ?Conditional 
Random Fields? for the tag NETE. It has also been 
mentioned that even manual annotation is harder 
for NETE as it is domain specific. This affected the 
overall results for Hindi because the performance 
for NETE was low (Table 3). 
2. Nested Matches: The largest possible as 
well as nested named entities are matched. 
3. Lexical Item Matches: The lexical items 
inside largest possible named entities are 
matched. 
9 Results 
The results of evaluation as explained in the previ-
ous section are shown in the Table-1. The F-
measures for nested lexical match are also shown 
individually for each named entity tag separately in 
Table-3 
 Num of 
NE tokens
Num of 
known NE 
% of un-
known NE
Bengali 1185 277 23.37 
10 Unknown Words Hindi 1120 417 37.23 
Table 4 shows the number of unknown words pre-
sent in the test data when compared with the train-
ing data. 
Oriya 1310 563 42.97 
Telugu 1150 145 12.60 
First column shows the number of unique 
Named entity tags present in the test data for each 
language. Second column shows the number of 
unique known named entities present in the test 
data. Third column shows the percentage of unique 
unknown words present in the test data of different 
languages when compared to training data. 
Urdu 631 179 28.36 
Table 4: Unknown Word 
 
Also, the F-measures of NEN, NETI, and NEM 
could have been higher because they are relatively 
closed classes. However, certain NEN can be am-
biguous (Example: eka is a NEN for ?one? in 
Hindi, but in a different context it can be a non-
number. For instance eka-doosra is Hindi for ?each 
other?). 
11 Error Analysis 
We can observe from the results that the maximal 
F-measure for Telugu is very low when compared 
to lexical F-measure and nested F-measure. The 
reason is that the test data of Telugu contains a 
large number of long named entities (around 6 
words), which in turn contain around 4 - 5 nested 
named entities. Our system was able to tag nested 
named entities correctly unlike maximal named 
entity. 
In a language like Telugu, NENs will appear as 
inflected words. For example 2001lo, guru-
vaaramto. 
10     Conclusion and Further Work 
In this paper we have presented the results of using 
a two stage hybrid approach for the task of named 
entity recognition for South and South East Asian 
Languages. We have achieved decent Lexical F-
measures of 40.63, 50.06, 39.04, 40.94, and 43.46 
for Bengali, Hindi, Oriya, Telugu and Urdu respec-
tively without using many language specific re-
sources. 
We can also observe that the maximal F-
measure for Telugu is very low when compared to 
other languages. This is because Telugu test data 
has very few known words. 
Urdu results are comparatively low chiefly be-
cause gazetteers for numbers and measures were 
unavailable.  
We plan to extend our work by applying our 
method to other South Asian languages, and by 
using more language specific constraints and re-
sources. We also plan to incorporate semi-
supervised extraction of rules for NEs (Saha et. al, 
30
2008) and use transliteration techniques to produce 
Indian language gazetteers (Surana and Singh, 
2008). Use of character models for increasing the 
lower recalls (Shishtla et. al, 2008) is also under-
way. We also plan to enrich the Indian dependency 
tree bank (Begum et. al, 2008) by use of our NER 
system. 
 
11 Acknowledgments 
 
   We would like to thank the organizer Mr. Anil 
Kumar Singh deeply for his continuous support 
during the shared task.  
References 
B. Babych, and A. Hartley, Improving Machine transla-
tion Quality with Automatic Named Entity Recognition. 
www.mt-archive.info/EAMT-2003- Babych.pdf 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra 
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for Indian languages. In 
Proceedings of IJCNLP-2008, Hyderabad, India. 
M. Bikel Daniel, Miller Scott, Schwartz Richard and 
Weischedel Ralph. 1997. Nymble: A High Perfor 
mance Learning Name-finder. In Proceedings of the 
Fifth Conference on Applied Natural Language 
Processing. 
S. Cucerzan, and D. Yarowsky, 1999. Language inde-
pendent named entity recognition combining mor-
phological and contextual evidence. Proceedings of 
the Joint SIGDAT Conference on EMNLP and VLC. 
N. Kumar and Pushpak Bhattacharyya. 2006. Named 
Entity Recognition in Hindi using MEMM. In Tech-
nical Report, IIT Bombay, India. 
John Lafferty, Andrew McCallum and Fernando          
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segmenting and Labeling Sequence 
Data. Proc.   18th International Conf. on Machine 
Learning. 
D. McDonald 1996. Internal and external evidence in 
the identification and semantic categorization of 
proper names. In B. Boguraev and J. Pustejovsky, 
editors, Corpus Processing for Lexical Acquisition. 
Avinesh PVS and Karthik G. Part-Of-Speech Tagging 
and Chunking Using Conditional Random Fields and 
Transformation Based Learning. Proceedings of the 
SPSAL workshop during IJCAI?07. 
Lance Ramshaw and Mitch Marcus. Text Chunking 
Using Transformation-Based Learning. Proceedings 
of the Third Workshop on Very Large Corpora. 
S.K. Saha , S. Chatterji , S. Dandapat , S. Sarkar  and P. 
Mitra 2008. A Hybrid Approach for Named Entity 
Recognition in Indian Languages. In Proceedings of 
IJCNLP Workshop on NER for South and South East 
Asian Languages. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
P. Shishtla, P. Pingali , V. Varma  2008. A Character n-
gram Based Approach for Improved Recall in Indian 
Language NER. In Proceedings of IJCNLP Work-
shop on NER for South and South East Asian Lan-
guages. 
Cucerzan Silviu and Yarowsky David. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. In Proceed-
ings of the Joint SIGDAT Conference on EMNLP and 
VLC. 
A. K. Singh and H. Surana  Can Corpus Based Meas-
ures be Used for Comparative Study of Languages? 
In Proceedings of Ninth Meeting of the ACL Special 
Interest Group in Computational Morphology and 
Phonology. ACL. 2007. 
R. Srihari, C. Niu and W. Li  2000. A Hybrid Approach 
for Named Entity and Sub-Type Tagging. In Pro-
ceedings of the sixth conference on Applied natural 
language processing. 
H. Surana and A. K. Singh 2008. A More Discerning 
and Adaptable Multilingual Transliteration Mecha-
nism for Indian Languages. In Proceedings of the 
Third International Joint Conference on Natural 
Language Processing. 
Charles Sutton, An Introduction to Conditional Random 
Fields for Relational Learning. 
T. Wakao , R. Gaizauskas  and Y. Wilks 1996. Evalua-
tion of an algorithm for the recognition and classifi-
cation of proper names. In Proceedings of COLING. 
 
Li Wei and McCallum Andrew. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. In 
ACM Transactions on Computational Logic. 
CRF++:.Yet another Toolkit. 
http://crfpp.sourceforge.net/ 
 
 
31
 32
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 117?122,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Part-Of-Speech Tagging for Gujarati Using Conditional Random 
Fields 
Chirag Patel and Karthik Gali 
Language Technologies Research Centre 
International Institute of Information Technology 
Hyderabad, India 
chirag_p,karthikg@students.iiit.ac.in 
 
 
Abstract 
This paper describes a machine learning 
algorithm for Gujarati Part of Speech Tag-
ging. The machine learning part is per-
formed using a CRF model. The features 
given to CRF are properly chosen keeping 
the linguistic aspect of Gujarati in mind. As 
Gujarati is currently a less privileged lan-
guage in the sense of being resource poor, 
manually tagged data is only around 600 
sentences. The tagset contains 26 different 
tags which is the standard Indian Language 
(IL) tagset. Both tagged (600 sentences) 
and untagged (5000 sentences) are used for 
learning. The algorithm has achieved an 
accuracy of 92% for Gujarati texts where 
the training corpus is of 10,000 words and 
the test corpus is of 5,000 words. 
1 Introduction 
Parts of Speech tagging is the process of tagging 
the words of a running text with their categories 
that best suits the definition of the word as well as 
the context of the sentence in which it is used. This 
process is often the first step for many NLP appli-
cations. Work in this field is usually either statisti-
cal or machine learning based, or rule based. Some 
of the models that use the first approach are Hid-
den Markov Models (HMMs), Conditional Ran-
dom Fields (CRFs), Maximum Entropy Markov 
Models (MEMMs), etc. 
The other method is the rule based approach 
where by we formulate rules based on the study of 
the linguistic aspect of the language. These rules 
are directly applied on the test corpus. The statisti-
cal learning based tools attack the problem mostly 
as a classification problem. They are not language 
specific and hence they fail when semantic knowl-
edge is needed while tagging a word with more 
than one sense. Even for unknown words, i.e., 
those words which have not appeared in the train-
ing corpus, these tools go by the probabilities but 
are not guaranteed to give the correct tag as they 
lack the semantic knowledge of the language. 
Also, they need a large annotated corpus. But the 
bright side of these tools is they can tag any word 
(known or unknown) with a high accuracy based 
on the probabilities of similar tags occurring in a 
particular context and some features provided for 
learning from the training data. 
On the other hand, purely rule based systems fail 
when the word is unknown or does not satisfy any 
of the rules. These systems just crash if the word is 
unknown. They cannot predict the plausible or 
likely tag. Hence an exhaustive set of rules are 
needed to achieve a high accuracy using this ap-
proach. 
There is another class of tools which are the hy-
brid ones. These may perform better than plain 
statistical or rule based approaches. The hybrid 
tools first use the probabilistic features of the sta-
tistical tools and then apply the language specific 
rules on the results as post processing. The best 
approach which seems intuitive is to generalize the 
language specific rules and convert them into fea-
tures. Then incorporate these features into the sta-
tistical tools. The problem here is the lack of con-
trol and flexibility on the statistical tools. So the 
perfect selection of features is what actually mat-
ters with respect to the accuracy. The more lan-
117
guage specific features that can be designed the 
higher accuracy can be achieved. 
2 Previous Work 
Different approaches have been used for part-of-
speech tagging previously. Some have focused on 
rule based linguistically motivated part-of-speech 
tagging such as by Brill (Brill, 1992 and Brill, 
1994). On the machine learning side, most of the 
previous work uses two main machine learning 
approaches for sequence labeling. The first ap-
proach relies on k-order generative probabilistic 
models of paired input sequences, for instance 
HMM (Frieda and McCallum, 2000) or multilevel 
Markov Models (Bikel et al 1999). 
CRFs bring together the best of generative and 
classification models. Like classification models, 
they can accommodate many statistically corre-
lated features of the input, and they are trained dis-
criminatively. And like generative models they can 
also tradeoff decisions at different sequence posi-
tions to obtain a globally optimal labeling. Condi-
tional Random Fields were first used for the task of 
shallow parsing by Lafferty et al (Lafferty et al, 
2000), where CRFs were applied for NP chunking 
for English on WSJ corpus and reported a per-
formance of 94.38%. For Hindi, CRFs were first 
applied to shallow parsing by Ravindran et al 
(Ravindran et. al., 2006) and Himanshu et al (Hi-
manshu et. al., 2006) for POS tagging and chunk-
ing, where they reported a performance of 89.69% 
and 90.89% respectively. Lafferty also showed that 
CRFs beat related classification models as well as 
HMMs on synthetic data and on POS-tagging task. 
Several POS taggers using supervised learning, 
both over word instances and tagging rules, report 
precision greater than 96% for English. For Hindi 
and other South Asian languages, the tagged cor-
pora is limited and together with higher morpho-
logical complexity of these languages it poses a 
difficulty in achieving results as good as those 
achieved for English in the past. 
3 Conditional Random Fields 
Charles Sutton et al (Sutton et al, 2005) formu-
lated CRFs as follows. Let G be a factor graph 
over Y. Then p(y|x) is a conditional random field if 
for any fixed x, the distribution p(y|x) factorizes 
according to G. Thus, every conditional distribu-
tion p(y|x) is a CRF for some, perhaps trivial, fac-
tor graph. If F = {A} is the set of factors in G, and 
each factor takes the exponential family form, then 
the conditional distribution can be written as 
 
X here is a random variable over data sequences 
to be labeled, and Y is a random variable over cor-
responding label sequences. All components Yi of 
Y are assumed to range over a finite label alphabet 
Y. For example, X might range over natural lan-
guage sentences and Y range over part-of-speech 
tagging of those sentences, with Y the set of possi-
ble part-of-speech tags. The random variables X 
and Y are jointly distributed, but in a discrimina-
tive framework we construct a conditional model 
p(Y|X) from paired observation and label se-
quences, and do not explicitly model the marginal 
p(X). 
CRFs define conditional probability distribu-
tions P(Y|X) of label sequences given input se-
quences. Lafferty et al defines the probability of a 
particular label sequence Y given observation se-
quence X to be a normalized product of potential 
functions each of the form: 
 
exp(??jtj(Yi-1,Yi,X,i)+??ksk (Yi,X,i)) 
 
where tj(Yi-1,Yi,X,i) is a transition feature func-
tion of the entire observation sequence and the la-
bels at positions i and i-1 in the label sequence; sk 
(Yi,X,i) is a state feature function of the label at 
position I and the observation sequence; and ?j and 
?k are parameters to be estimated from training 
data. 
 
Fj(Y,X)= ? fj (Yi-1,Yi,X,i) 
 
where each fj (Yi-1,Yi,X,i) is either a state func-
tion s(Yi-1,Yi,X,i) or a transition function t(Yi-
1,Yi,X,i). This allows the probability of a label 
sequence Y given an observation sequence X to be 
written as: 
P(Y|X, ?) = (1/Z(X)) exp(??j Fj(Y,X)) 
 
where Z(X) is a normalization factor. 
4 IL Tagset 
The currently used tagset for this project and which 
is a standard for Indian Languages is the IL (Indian 
118
Languages) tagset. The tagset consists of 26 tags. 
These have been specially designed for Indian 
Languages. The tagset contains the minimum tags 
necessary at the Parts of Speech tagging level. It 
copes with the phenomena of fineness versus 
coarseness. The tags are broadly categorized into 5 
main groups, with the nouns consisting of the gen-
eral nouns, space or time related nouns or proper 
nouns, and the verbs consisting of the main and the 
auxiliary verbs. Another category is of the noun 
and verb modifiers like adjectives, quantifiers and 
adverbs. Finally, there are numbers, cardinals etc. 
5 Approach 
Approach presented in this paper is a machine 
learning model. It uses supervised as well as unsu-
pervised techniques. It uses a CRF to statistically 
tag the test corpus. The CRF is trained using fea-
tures over a tagged and untagged data. A CRF 
when provided with good features gives accuracy 
much better than other models. The intuition here 
is that if we convert the linguistic rules specific to 
Gujarati in to features provided to CRF, then we 
make use of advantages of both statistical and rule 
based approach. But due to lack of control and 
flexibility not all features can be incorporated in 
the CRF. So after the CRF is done we do the error 
analysis. From the errors we formulate rules, 
which are general and language specific, and then 
convert them to new features and apply them back 
to CRF. This increases the accuracy. 
Gujarati when viewed linguistically is a free 
word order language. It is partially agglutinative, 
in the sense maximum 4 suffixes can attach to the 
main root. Words in Gujarati can have more than 
one sense where the tags are different in different 
senses. For e.g. ?paNa? can be a particle meaning ? 
?also?, and also can be a connective meaning ? 
?but?. ?pUrI? can be a noun meaning ? ?an eat-
able?, can be an adjective meaning ? ?finished?, 
and can also be a verb meaning ? ?to fill?. 
Also, in Gujarati, postpositions can be or can not 
be attached to the head word. For e.g. One may 
write ?rAme? or ?rAma e? literally meaning 
?rAma (ergative)?. 
Most of all, this language can drop words from 
the sentences. For example: 
 
Sent:     baXA  loko     GaramAM   gayA. 
Literal:    all    people   house + in   went. 
Tags:   QF  NN    NN          VM 
 
Here, we can drop the noun (NN) ?loko? and in 
which case the quantifier (QF) ?baXA? now be-
comes the noun (NN). 
Features used in CRF are suffixes, prefixes, 
numbers etc. For e.g. Words having suffix ?ne?, 
like ?grAhakone? are tagged as NN. CRF learns 
from the tags given to words with same suffixes in 
the training data. This suffix window is 4. This 
way the vibhakti information is explored. Similarly 
if words like ?KAine? and ?KAwo? come in the 
training corpus the CRF learns the preffix and tags 
other words with that prefix. This way the stem 
information is explored. Also if the token is a 
number then it must be QC, and if it has a number 
in it then it must be a NNP. 
6 Experiments 
Initially we just ran a rule based tagging code on 
the test data. This code used both machine learning 
and rule based features for tagging. It gave an ac-
curacy of 86.43%. The error analysis revealed that, 
as the training corpus being less, the unknown 
words are many and also well distributed over the 
tags. Hence the heuristics were not effective. 
Then we ran a CRF tool on the test data. We 
found it giving an accuracy of 89.90%. Then dur-
ing the error analysis we observed that the features 
were not up to the mark. Then we selected particu-
lar features which were generalization of rule 
based, used in the previous code, and more specific 
to Gujarati. This increased the accuracy to 91.74%. 
Then after adding more heuristics the accuracy was 
in fact reducing. Heuristics like converting all 
NNPs to NNs, removing some tags as options 
while tagging the unknown words like 
CC,QW,PRP etc. as these in a language are very 
limited and are expected that they must have came 
once in the training corpus. We also tried tagging 
the word on the basis of possible tags between the 
two surrounding words. But that too reduced the 
accuracy. Also heuristics like previous and current 
word vibhakti combination failed. 
 
Training data Test data Results (%) 
11185 5895 91.74 
Table-1. POS Tagging Results and Data Size 
 
119
7 Error Analysis 
Here the above table confirms that the errors 
have occurred across all the tags. This is mainly 
due to lack of training data. The numbers of un-
known words in the corpus were around 40%. The 
CRF while using the features and the probabilities 
to tag a particular unknown word made mistakes 
due to the flexible nature of the language. For e.g. 
the maximum errors occurred because of tagging 
an adjective by a noun. An example: 
 
motA`QFC BAganA`QF viSeRa`NN SEk-
SaNika`JJ jaruriyAwo`NN GarAvawA`VM 
bAlYako`NN sAmAnya`JJ skUlamAM`NN 
jaSe`VM .`SYM  
 
 
Actual Tag Assigned Tag Counts 
JJ NN 58 
NNP NN 35 
NN JJ 26 
NN VM 22 
NNC NN 21 
PSP NN 19 
VM VAUX 19 
NNPC NN 18 
NNC JJ 17 
NST NN 14 
VM NN 13 
Table-2. Errors Made by the Tagger. 
 
In the above example the word ?viSeRa`NN? is 
wrongly tagged. This being an adjective is tagged 
as NN, firstly because it is an unknown word. Also 
in this language adjectives may or may not occur 
before the nouns. Hence the probability of this un-
known word to be a NN or a JJ is equal or will de-
pend on the number of instances of both in the 
training corpus. Further more there is more prob-
ability of it being tagged as a noun as the next 
word is an adjective. There are very less instances 
where two adjectives come together in the training 
corpus. Again the chances of it being a noun in-
crease as the QF mostly precede nouns instead of 
adjectives. Here we also have a QF before the un-
known word. The same reason also is responsible 
for the third class of errors ? NN being wrongly 
tagged as JJ. These errors can only be corrected if 
the word is some how known. Again the next class 
of errors is the Named Entity Recognition problem 
which is an open problem in itself. 
8 Conclusion 
We have trained a CRF on Gujarati which gives an 
accuracy of around 92%. From the experiments we 
observed that if the language specific rules can be 
formulated in to features for CRF then the accu-
racy can be reached to very high extents. The CRF 
learns from both tagged that is 600 sentences and 
also untagged data, which is 5,000 sentences.  
From the errors we conclude that as the training 
data increases, the less number of unknown words 
will be encountered in the test corpus, which will 
increase the accuracy. We can also use some ma-
chine readable resources like dictionaries, morphs 
etc. when ever they are built.  
9 Intuition 
We noticed that on a less amount of training data 
also we have a good accuracy. The reason we felt 
intuitive was Gujarati uses the best part of the vib-
hakti feature linguistically. It, being more aggluti-
native than Hindi has more word forms, hence 
more word coverage, and being some less aggluti-
native than Telugu, has less ambiguity and also is 
practical to hard code the vibhaktis, uses the best 
part of advantages of the vibhakti feature in POS 
tagging. Based only on the hard coded vibhakti 
information we could tag around 1500 unknown 
words out of 5000.  
10 Future work 
We are looking forward to manually tag more 
training data in the future. We will also be trying to 
build language resources for Gujarati that will help 
in the Tagger. By increasing the amount of training 
data we expect an appreciable increase in the accu-
racy. 
References 
Himanshu Agarwal and Anirudh Mani. 2006. Part of 
Speech Tagging and Chunk-ing with Conditional 
Random Fields. In the Proceedings of NWAI work-
shop. 
Pranjal Awasthi, Delip Rao, Balaraman Ravindran. 
2006. Part Of Speech Tagging and Chunking with 
HMM and CRF. Proceedings of the NLPAI contest 
workshop during  NWAI ?06, SIGAI Mumbai. 
Karthik Kumar G, Sudheer K, Avinesh PVS. 2006. 
Comparative study of various Machine Learning 
methods For Telugu Part of Speech tagging. Pro-
120
ceedings of the NLPAI contest workshop during 
NWAI ?06, SIGAI Mumbai. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: Probabil-
istic Models for Segment-ing and Labeling Sequence 
Data. In proceedings of ICML?01. 
Avinesh PVS and Karthik G. 2007. Part-Of-Speech 
Tagging and Chunking Using Conditional Random 
Fields and Transformation Based Learning. Proceed-
ings of the SPSAL workshop during IJCAI?07. 
Fei Sha and Fernando Pereira. 2003. Shallow Parsing 
with Conditional Random Fields. In the Proceedings 
of HLT-NAACL. 
Charles Sutton. 2007. An Introduction to Conditional 
Random Fields for Relational Learning. In proceed-
ings of ICML?07. 
CRF++: Yet Another Toolkit. 
http://chasen.org/~taku/software/CRF++ 
 
121
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
122
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 105?110,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Experiments in Telugu NER: A Conditional Random Field Approach
Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma
{praneethms,karthikg}@students.iiit.ac.in,{pvvpr,vv}@iiit.ac.in
Language Technologies Research Centre
International Institute of Information Technology
Hyderabad, India
Abstract
Named Entity Recognition(NER) is the task
of identifying and classifying tokens in a
text document into predefined set of classes.
In this paper we show our experiments
with various feature combinations for Tel-
ugu NER. We also observed that the prefix
and suffix information helps a lot in find-
ing the class of the token. We also show
the effect of the training data on the perfor-
mance of the system. The best performing
model gave an F?=1 measure of 44.91. The
language independent features gave an F?=1
measure of 44.89 which is close to F?=1
measure obtained even by including the lan-
guage dependent features.
1 Introduction
The objective of NER is to identify and classify all
tokens in a text document into predefined classes
such as person, organization, location, miscella-
neous. The Named Entity information in a document
is used in many of the language processing tasks.
NER was created as a subtask in Message Under-
standing Conference (MUC) (Chinchor, 1997). This
reflects the importance of NER in the area of Infor-
mation Extraction (IE). NER has many applications
in the areas of Natural Language Processing, Infor-
mation Extraction, Information Retrieval and speech
processing. NER is also used in question answer-
ing systems (Toral et al, 2005; Molla et al, 2006),
and machine translation systems (Babych and Hart-
ley, 2003). It is also a subtask in organizing and re-
trieving biomedical information (Tsai, 2006).
The process of NER consists of two steps
? identification of boundaries of proper nouns.
? classification of these identified proper nouns.
The Named Entities(NEs) should be correctly iden-
tified for their boundaries and later correctly classi-
fied into their class. Recognizing NEs in an English
document can be done easily with a good amount
of accuracy(using the capitalization feature). Indian
Languages are very much different from the English
like languages.
Some challenges in named entity recognition that
are found across various languages are: Many
named entities(NEs) occur rarely in the corpus i.e
they belong to the open class of nouns. Ambiguity
of NEs. Ex Washington can be a person?s name or a
place name. There are many ways of mentioning the
same Named Entity(NE). In case of person names,
Ex: Abdul Kalam, A.P.J.Kalam, Kalam refer to the
same person. And, in case of place names Waran-
gal, WGL both refer to the same location. Named
Entities mostly have initial capital letters. This dis-
criminating feature of NEs can be used to solve the
problem to some extent in English.
Indian Languages have some additional chal-
lenges: We discuss the challenges that are specific
to Telugu. Absence of capitalization. Ex: The con-
densed form of the person name S.R.Shastry is writ-
ten as S.R.S in English and is represented as srs in
Telugu. Agglutinative property of the Indian Lan-
guages makes the identification more difficult. Ag-
glutinative languages such as Turkish or Finnish,
Telugu etc. differ from languages like English in
105
the way lexical forms are generated. Words are
formed by productive affixations of derivational and
inflectional suffixes to roots or stems. For example:
warangal, warangal ki, warangalki, warangallo,
warangal ni etc .. all refer to the place Waran-
gal. where lo, ki, ni are all postpostion markers
in Telugu. All the postpositions get added to the
stem hyderabad. There are many ways of represent-
ing acronyms. The letters in acronyms could be the
English alphabet or the native alphabet. Ex: B.J.P
and BaJaPa both are acronyms of Bharatiya Janata
Party. Telugu has a relatively free word order when
compared with English. The morpohology of Tel-
ugu is very complex. The Named Entity Recogni-
tion algorithm must be able handle most of these
above variations which otherwise are not found in
languages like English. There are not rich and robust
tools for the Indian Languages. For Telugu, though
a Part Of Speech(POS) Tagger for Telugu, is avail-
able, the accuracy is less when compared to English
and Hindi.
2 Problem Statement
NER as sequence labelling task
Named entity recognition (NER) can be modelled
as a sequence labelling task (Lafferty et al, 2001).
Given an input sequence of words W n1 = w1w2w3
...wn, the NER task is to construct a label sequence
Ln1 = l1l2l3 ...ln , where label li either belongs to
the set of predefined classes for named entities or
is none(representing words which are not proper
nouns). The general label sequence ln1 has the high-
est probability of occuring for the word sequence
W n1 among all possible label sequences, that is
L?n1 = argmax {Pr (L
n
1 |W
n
1 ) }
3 Conditional Random Fields
Conditional Random Fields (CRFs) (Wallach, 2004)
are undirected graphical models used to calculate the
conditional probability of values on designated out-
put nodes given values assigned to other designated
input nodes. In the special case in which the output
nodes of the graphical model are linked by edges in a
linear chain, CRFs make a first-order Markov inde-
pendence assumption, and thus can be understood as
conditionally-trained finite state machines(FSMs).
Let o = ? O1,O2,...OT ? be some observed input
data sequence, such as a sequence of words in text
in a document,(the values on n input nodes of the
graphical model). Let S be a set of FSM states, each
of which is associated with a label, l ?L .
Let s = ? s1,s2,... sT ,? be some sequence of states,(the
values on T output nodes). By the Hammersley-
Clifford theorem CRFs define the conditional prob-
ability of a state sequence given an input sequence
to be
P(s|o) =
1
Zo
? exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
where Zo is a normalization factor over all state
sequences, is an arbitrary feature function over its ar-
guments, and ?k is a learned weight for each feature
function. A feature function may, for example, be
defined to have value 0 or 1. Higher ? weights make
their corresponding FSM transitions more likely.
CRFs define the conditional probability of a la-
bel sequence based on total probability over the state
sequences, P(l|o) = ?s:l(s)=l P(s|o) where l(s) is the
sequence of labels corresponding to the labels of the
states in sequence s. Note that the normalization fac-
tor, Zo, (also known in statistical physics as the parti-
tion function) is the sum of the scores of all possible
state sequences,
Zo = ?
s?ST
?exp(
T
?
t=1
?
k
?k fk (st?1,st ,o, t))
and that the number of state sequences is expo-
nential in the input sequence length,T. In arbitrarily-
structure CRFs, calculating the partition function in
closed form is intractable, and approximation meth-
ods such as Gibbs sampling, or loopy belief propa-
gation must be used.
4 Features
There are many types of features used in general
NER systems. Many systems use binary features
i.e. the word-internal features, which indicate the
presence or absence of particular property in the
word. (Mikheev, 1997; Wacholder et al, 1997;
Bikel et al, 1997). Following are examples of
binary features commonly used. All-Caps (IBM),
Internal capitalization (eBay), initial capital (Abdul
Kalam), uncapitalized word (can), 2-digit number
106
(83, 28), 4-digit number (1273, 1984), all digits (8,
31, 1228) etc. The features that correspond to the
capitalization are not applicable to Telugu. We have
not used any binary features in our experiments.
Gazetteers are used to check if a part of the
named entity is present in the gazetteers. We don?t
have proper gazetteers for Telugu.
Lexical features like a sliding window
[w?2,w?1,wo,w1,w2] are used to create a lexi-
cal history view. Prefix and suffix tries were also
used previously(Cucerzan and Yarowsky,1999).
Linguistics features like Part Of Speech, Chunk,
etc are also used.
4.1 Our Features
We donot have a highly accurate Part Of
Speech(POS) tagger. In order to obtain some
POS and chunk information, we ran a POS Tagger
and chunker for telugu (PVS and G, 2007) on the
data. And from that, we used the following features
in our experiments.
Language Independent Features
current token: w0
previous 3 tokens: w?3,w?2,w?1
next 3 tokens: w1,w2,w3
compound feature:w0 w1
compound feature:w?1 w0
prefixes (len=1,2,3,4) of w0: pre0
suffixes (len=1,2,3,4) of w0: su f0
Language Dependent Features
POS of current word: POS0
Chunk of current word: Chunk0
Each feature is capable of providing some infor-
mation about the NE.
The word window helps in using the context in-
formation while guessing the tag of the token. The
prefix and suffix feature to some extent help in cap-
turing the variations that may occur due to aggluti-
nation.
The POS tag feature gives a hint whether the word
is a proper noun. When this is a proper noun it has
a chance of being a NE. The chunk feature helps in
finding the boundary of the NE.
In Indian Languages suffixes and other inflections
get attached to the words increasing the length of the
word and reducing the number of occurences of that
word in the entire corpus. The character n-grams can
capture these variations.
5 Experimental Setup
5.1 Corpus
We conducted the experiments on the developement
data released as a part of NER for South and South-
East Asian Languages (NERSSEAL) Competetion.
The corpus in total consisted of 64026 tokens out
of which 10894 were Named Entities(NEs). We di-
vided the corpus into training and testing sets. The
training set consisted of 46068 tokens out of which
8485 were NEs. The testing set consisted of 17951
tokens out of which 2407 were NEs. The tagset as
mentioned in the release, was based on AUKBC?s
ENAMEX,TIMEX and NAMEX, has the follow-
ing tags: NEP (Person), NED (Designation), NEO
(Organization), NEA (Abbreviation), NEB (Brand),
NETP (Title-Person), NETO (Title-Object), NEL
(Location), NETI (Time), NEN (Number), NEM
(Measure) & NETE (Terms).
5.2 Tagging Scheme
The corpus is tagged using the IOB tagging scheme
(Ramshaw and Marcus, 1995). In this scheme each
line contains a word at the beginning followed by
its tag. The tag encodes the type of named entity
and whether the word is in the beginning or inside
the NE. Empty lines represent sentence(document)
boundaries. An example is given in table 1.
Words tagged with O are outside of named en-
tities and the I-XXX tag is used for words inside a
named entity of type XXX. Whenever two entities
of type XXX are immediately next to each other,
the first word of the second entity will be tagged B-
XXX in order to show that it starts another entity.
This tagging scheme is the IOB scheme originally
put forward by Ramshaw and Marcus (1995).
5.3 Experiments
To evaluate the performance of our Named Entity
Recognizer, we used three standard metrics namely
precision, recall and f-measure. Precision measures
the number of correct Named Entities(NEs) in the
107
Token Named Entity Tag
Swami B-NEP
Vivekananda I-NEP
was O
born O
on O
January B-NETI
, I-NETI
12 I-NETI
in O
Calcutta B-NEL
. O
Table 1: IOB tagging scheme.
machine tagged file over the total number of NEs in
the machine tagged file and the recall measures the
number of correct NEs in the machine tagged file
over the total number of NEs in the golden standard
file while F-measure is the weighted harmonic mean
of precision and recall:
F =
(
? 2+1
)
RP
? 2R+P
with
? = 1
where P is Precision, R is Recall and F is F-measure.
W?n+n: A word window :w?n, w?n+1, .., w?1, w0,
w1, .., wn?1, wn.
POSn: POS nth token.
Chn: Chunk of nth token.
pren: Prefix information of nth token. (prefix
length=1,2,3,4)
su fn: Suffix information of nth token. (suffix
length=1,2,3,4)
The more the features, the better is the perfor-
mance. The inclusion of the word window, prefix
and suffix features have increased the F?=1 mea-
sure significantly. Whenever the suffix feature is
included, the performance of the system increased.
This shows that the system is able to caputure those
agglutinative language variations. We also have ex-
perimented changing the training data size. While
varying the training data size, we have tested the
performance on the same amount of testing data of
17951 tokens.
6 Conclusion & Future Work
The inclusion of prefix and suffix feature helps in
improving the F?=1 measure (also recall) of the sys-
tem. As the size of the training data is increased,
the F?=1 measure is increased. Even without the
language specific information the system is able to
perform well. The suffix feature helped improve the
recall. This is due to the fact that the POS tagger
also uses the same features in predicting the POS
tags. Prefix, suffix and word are three non-linguistic
features that resulted in good performance. We plan
to experiment with the character n-gram approach
(Klein et al, 2003) and include gazetteer informa-
tion.
References
Bogdan Babych and Anthony Hartley. 2003. Improv-
ing machine translation quality with automatic named
entity recognition. In Proceedings of Seventh Inter-
national EAMT Workshop on MT and other language
technology tools, Budapest, Hungary.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the fifth conference on Applied natural language pro-
cessing, pages 194?201, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Nancy Chinchor. 1997. Muc-7 named entity task defini-
tion. Technical Report Version 3.5, Science Applica-
tions International Corporation, Fairfax, Virginia.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003, pages 180?183, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Andrei Mikheev. 1997. Automatic rule induction
for unknown-word guessing. Comput. Linguist.,
23(3):405?423.
108
Features Precision Recall F?=1
Ch0 51.41% 9.19% 15.59
POS0 46.32% 9.52% 15.80
POS0.Ch0 46.63% 9.69% 16.05
W?3+3.Ch0 59.08% 19.50% 29.32
W?3+3.POS0 58.43% 19.61% 29.36
Ch0.pren 53.97% 24.76% 33.95
POS0.pren 53.94% 24.93% 34.10
POS0.Ch0.pren 53.94% 25.32% 34.46
POS0.su fn 47.51% 29.36% 36.29
POS0.Ch0.su fn 48.02% 29.24% 36.35
Ch0.su fn 48.55% 29.13% 36.41
W?3+3.POS0.pren 62.98% 27.45% 38.24
W?3+3.POS0.Ch0.pren 62.95% 27.51% 38.28
W?3+3.Ch0.pren 62.88% 27.62% 38.38
W?3+3.POS0.su fn 60.09% 30.53% 40.49
W?3+3.POS0.Ch0.su fn 59.93% 30.59% 40.50
W?3+3.Ch0.su fn 61.18% 30.81% 40.98
POS0.Ch0.pren.su fn 57.83% 34.57% 43.27
POS0.pren.su fn 57.41% 34.73% 43.28
Ch0.pren.su fn 57.80% 34.68% 43.35
W?3+3.Ch0.pren.su fn 64.12% 34.34% 44.73
W?3+3.POS0.pren.su fn 64.56% 34.29% 44.79
W?3+3.POS0.Ch0.pren.su fn 64.07% 34.57% 44.91
Table 2: Average Precision,Recall and F?=1 measure for different language dependent feature combinations.
Features Precision Recall F?=1
w 57.05% 20.62% 30.29
pre 53.65% 23.87% 33.04
suf 47.75% 29.19% 36.23
w.pre 63.08% 27.56% 38.36
w.suf 60.93% 30.76% 40.88
pre.suf 57.94% 34.96% 43.61
w.pre.suf 64.80% 34.34% 44.89
Table 3: Average Precision,Recall and F?=1 measure for different language independent feature combina-
tions.
Diego Molla, Menno van Zaanen, and Daniel Smith.
2006. Named entity recognition for question answer-
ing. In Proceedings of Australasian Language Tech-
nology Workshop 2006, Sydney, Australia.
Avinesh PVS and Karthik G. 2007. Part-of-speech tag-
ging and chunking using conditional random fields and
transformation based learning. In In Proceedings of
SPSAL-2007 Workshop.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Antonio Toral, Elisa Noguera, Fernando Llopis, and
Rafael Mun?oz. 2005. Improving question answering
using named entity recognition. In Proceedings of the
10th NLDB congress, Lecture notes in Computer Sci-
ence, Alicante, Spain. Springer-Verlag.
109
Number of Words Precision Recall F?=1
2500 51.37% 9.47% 15.99
5000 64.74% 11.93% 20.15
7500 61.32% 13.50% 22.13
10000 66.88% 23.31% 34.57
12500 63.42% 27.39% 38.26
15000 63.55% 31.26% 41.91
17500 60.58% 30.64% 40.70
20000 58.32% 30.03% 39.64
22500 57.72% 29.75% 39.26
25000 59.33% 29.92% 39.78
27500 60.91% 30.03% 40.23
30000 62.77% 30.42% 40.98
32500 62.66% 30.64% 41.16
35000 62.08% 30.81% 41.18
37500 61.02% 30.87% 41.00
40000 61.60% 31.09% 41.33
42500 62.12% 32.44% 42.62
45000 62.70% 32.77% 43.05
47500 63.20% 32.72% 43.12
50000 64.29% 34.29% 44.72
Table 4: The effect of training data size on the performance of the NER.
Richard Tzong-Han Tsai. 2006. A hybrid approach to
biomedical named entity recognition and semantic role
labeling. In Proceedings of the 2006 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 243?246, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Nina Wacholder, Yael Ravin, and Misook Choi. 1997.
Disambiguation of proper names in text. In Proceed-
ings of the fifth conference on Applied natural lan-
guage processing, pages 202?208, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
HannaM.Wallach. 2004. Conditional random fields: An
introduction. Technical Report MS-CIS-04-21, Uni-
versity of Pennsylvania, Department of Computer and
Information Science, University of Pennsylvania.
110
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Realisation from Bag of Words with dependency constraints
Karthik Gali, Sriram Venkatapathy
Language Technologies Research Centre,
IIIT-Hyderabad, Hyderabad, India
{karthikg@students,sriram@research}.iiit.ac.in
Abstract
In this paper, we present five models for sentence
realisation from a bag-of-words containing mini-
mal syntactic information. It has a large variety
of applications ranging from Machine Translation
to Dialogue systems. Our models employ simple
and efficient techniques based on n-gram Language
modeling.
We evaluated the models by comparing the syn-
thesized sentences with reference sentences using
the standard BLEU metric(Papineni et al, 2001).
We obtained higher results (BLEU score of 0.8156)
when compared to the state-of-art results. In fu-
ture, we plan to incorporate our sentence realiser in
Machine Translation and observe its effect on the
translation accuracies.
1 Introduction
In applications such as Machine Translation (MT)
and Dialogue Systems, sentence realisation is a ma-
jor step. Sentence realisation involves generating a
well-formed sentence from a bag of lexical items.
These lexical items may be syntactically related to
each other. The level of syntactic information at-
tached to the lexical items might vary with applica-
tion. In order to appeal to the wide range of applica-
tions that use sentence realisation, our experiments
assume only basic syntactic information, such as un-
labeled dependency relationships between the lexi-
cal items.
In this paper, we present different models for sen-
tence realisation. These models consider a bag of
words with unlabelled dependency relations as input
and apply simple n-gram language modeling tech-
niques to get a well-formed sentence.
We now present the role of a sentence realiser
in the task of MT. In transfer-based approaches for
MT1 (Lavie et al, 2003), the source sentence is
first analyzed by a parser (a phrase-structure or a
dependency-based parser). Then the source lexical
items are transferred to the target language using a
bi-lingual dictionary. The target language sentence
is finally realised by applying transfer-rules that map
the grammar of both the languages. Generally, these
transfer rules make use of rich analysis on the source
side such as dependency labels etc. The accuracy of
having such rich analysis (dependency labeling ) is
low and hence, might affect the performance of the
sentence realiser. Also, the approach of manually
constructing transfer rules is costly, especially for
divergent language pairs such as English and Hindi
or English and Japanese. Our models can be used
in this scenario, providing a robust alternative to the
transfer rules.
A sentence realiser can also be used in the frame-
work of a two-step statistical machine translation.
In the two-step framework, the semantic transfer
and sentence realisation are decoupled into indepen-
dent modules. This provides an opporunity to de-
velop simple and efficient modules for each of the
steps. The model for Global Lexical Selection and
Sentence Re-construction (Bangalore et al, 2007)
is one such approach. In this approach, discrimi-
native techniques are used to first transfer semantic
information of the source sentence by looking at the
source sentence globally, this obtaining a accurate
bag-of-words in the target language. The words in
the bag might be attached with mild syntactic infor-
mation (ie., the words they modify) (Venkatapathy
and Bangalore, 2007). We propose models that take
1http://www.isi.edu/natural-language/mteval/html/412.html
19
this information as input and produce the target sen-
tence. We can also use our sentence realiser as an
ordering module in other approaches such as (Quirk
et al, 2005), where the goal is to order an unordered
bag (of treelets in this case) with dependency links.
In Natural Language Generation applications
such as Dialogue systems etc, the set of concepts
and the dependencies between the concepts is ob-
tained first which is known as text planning. These
concepts are then realized into words resulting in a
bag of words with syntactic relations (Bangalore and
Rambow, 2000). This is known as sentence plan-
ning. In the end, the surface string can be obtained
by our models.
In this paper, we do not test our models with any
of the applications mentioned above. However, we
plan to test our models with these applications, es-
pecially on the two-stage statistical MT approach
using the bag-of-words obtained by Global Lexi-
cal Selection (Bangalore et al, 2007),(Venkatapathy
and Bangalore, 2007). Here, we test our models in-
dependent of any application, by beginning with a
given bag-of-words (with dependency links).
The structure of the paper is as follows. We give
an overview of the related work in section 2. In sec-
tion 3, we talk about the effect of dependency con-
straints and gives details of the experimental setup in
section 4. In section 5, we describe about the exper-
iments that have been conducted. In section 6, our
experimental results are presented. In section 7, we
talk about the possible future work and we conclude
with section 8.
2 Related Work
There have been approaches for sentence realisation
such as FUF/SURGE (Elhadad, 1991), OpenCCG
(White, 2004) and XLE (Crouch et al, 2007)
that apply hand-crafted grammars based on partic-
ular linguistic theories. These approaches expect
rich syntactic information as input in order to re-
alise the sentence. There are other approaches in
which the generation grammars are extracted semi-
automatically (Belz, 2007) or automatically (such as
HPSG (Nakanishi and Miyao, 2005), LFG (Cahill
and van Genabith, 2006; Hogan et al, 2007) and
CCG (White et al, 2007)). The limitation of these
approaches is that these cannot be incorporated into
a wide range of applications as they rely on rich
syntactic information for generation. On the con-
trary, we use simple n-gram models to realise (or lin-
earize) a bag-of-words where the only information
available is the presence of various links between the
words.
Our work is similar to a recently published work
by Guo (Guo et al, 2008). They use n-gram models
to realise sentences from the f-structures of HPSG
(equivalent to labeled dependency structure). Their
models rely heavily on the dependency relation la-
bels (also called grammatical roles) available in
HPSG. However, the dependency role information
(of any dependency formalism) is either not read-
ily available in a variety of applications in NLP. We
propose to explore the realisation of a sentence us-
ing minimal syntactic information. Apart from de-
pendency links, we also make use of part-of-speech
tags which are easily available and hence, our sen-
tence realiser can be plugged much easily into var-
ious applications. Guo (Guo et al, 2008) conduct
their experiments by considering gold data as input.
Apart from using gold data as input, we also con-
duct experiments by assuming noisy input data to
test the robustness of our models. The search al-
gorithm used by both Guo and us is locally greedy
i.e., we compute the best string at every node. Guo
uses the Viterbi algorithm to get best string whereas
we consider and score all permutations to obtain the
best string.
There has been burgeoning interest in the prob-
abilistic models for sentence realisation, especially
for realisation ranking in a two stage sentence real-
isation architecture where in the first stage a set of
sentence realisations are generated and then a real-
isation ranker will choose the best of them (Banga-
lore and Rambow, 2000).
One major observation in our experiments was
that the POS tags held immensely in the task of sen-
tence realisation.
3 Effect of Dependency Constraints
There is a major advantage in using dependency
constraints for sentence realisation. The search
space reduces drastically when the constraints are
applied. These constraints state that the realised sen-
tences should be projective with respect to the de-
20
pendency structure (unordered) of the input bag-of-
words ie.., any word and its children in the depen-
dency tree should project as a contiguous unit in the
realised sentence. This is a safe assumption to make
as the non-projectivity in English is only used to
account for Long-Distance Dependencies and such
cases are low in number (Guo et al, 2008).
is going
Ram to school
Figure 1: Bag of words with dependency constraints
and head marked
We now present an example to show how the de-
pendency constraints reduce the search space. For
example, consider an unordered dependency tree in
Figure 1, which has five words. If we don?t use the
constraints provided by the dependency tree then the
search space is 5! (120). But, if we use the con-
straints provided by the dependency tree then the
search space is 2! + 4! = 28. There is a huge reduc-
tion in the search space if we use the constraints pro-
vided by the dependency tree. Further, it has been
shown in (Chang and Toutanova, 2007) that apply-
ing the constraints also aids for the synthesis of bet-
ter constructed sentences.
4 Experimental Set-up
For the experiments, we use the WSJ portion of the
Penn tree bank (Marcus et al, 1993), using the stan-
dard train/development/test splits, viz 39,832 sen-
tences from 2-21 sections, 2416 sentences from sec-
tion 23 for testing and 1,700 sentences from sec-
tion 22 for development. The input to our sen-
tence realiser are bag of words with dependency
constraints which are automatically extracted from
the Penn treebank using head percolation rules used
in (Magerman, 1995), which do not contain any or-
der information. We also use the provided part-of-
speech tags in some experiments.
In a typical application, the input to the sentence
realiser is noisy. To test the robustness of our models
in such scenarios, we also conduct experiments with
noisy input data. We parse the test data with an un-
labelled projective dependency parser (Nivre et al,
2006) and drop the order information to obtain the
input to our sentence realiser. However we still use
the correct bag of words. We propose to test this as-
pect in future by plugging our sentence realiser in
Machine Translation.
Table 1 shows the number of nodes having a par-
ticular number of children in the test data.
Children countNodes Children countNodes
0 30219 5 1017
1 13649 6 685
2 5887 7 269
3 3207 8 106
4 1526 > 8 119
Table 1: The number of nodes having a particular
number of children in the test data
From Table 1, we can see that more than 96% of
the internal nodes of the trees contain five or less
children. It means that for almost all the nodes, the
reordering complexity is minimal. This makes this
approach very feasible if the order of a sub-tree is
computed after the order of the sub-trees of its chil-
dren is fixed. Hence, the approaches that we present
in the next section use bottom-up traversal of the
tree. During the traversal, the appropriate order of
every sub-tree is fixed.
5 Experiments
The task here is to realise a well formed sentence
from a bag of words with dependency constraints
(unordered dependency tree) for which we propose
five models using n-gram based Language modeling
techinque. We train the language models of order 3
using Good-Turning smoothing on the training data
of Penn Treebank.
5.1 Model 1 : Sentential Language Model
We traverse the tree in bottom up manner and find
the best phrase at each subtree. The best phrase cor-
responding to the subtree is assigned to the root node
of the sub-tree during the traversal.
Let the node n have N children represented as ci
(1 < i < N ). During the bottom up traversal, the
21
children ci are assigned best phrases before process-
ing node n. Let the best phrases corresponding to the
children be p(ci). The best phrase corresponding to
the node n is computed by exploring the permuta-
tions of n and the best phrases p(ci) corresponding
to the children ci. The total number of permutations
that are explored are (N+1)!. A sentential language
model is applied on each of the candidate phrases to
select the best phrase.
p(n) = bestPhrase ( perm (n, ? i p(ci)) o LM )
(1)
In Sentential Language Model, we used a LM that
is trained on complete sentences of the training cor-
pus to score the permutations.
5.2 Model 2 : Subtree-type based Language
Models(STLM)
The major problem with model 1 is that we are us-
ing a common sentential language model (trained on
complete sentences) to score phrases corresponding
to various sub-tree types. In this model, we build
different LMs for phrases corresponding to different
subtree-types.
To build STLMs, the training data is parsed first.
Each subtree in the parse structure is represented
by the part-of-speech tag of its head. Different lan-
guage models are created for each of the POS tags.
We have 44 different language models each corre-
sponding to a particular POS tag. For example, a
IN language model contains phrases like in hour, of
chaos, after crash, in futures, etc and VBD language
model contains phrases like were criticized, never
resumed while training.
So, in this model we realise a sentence from a
unordered dependency tree by traversing the depen-
dency tree in bottom-up manner as we did in model
1; but while scoring the permuted phrases we use
different language models for subtrees headed by
words of various pos tags.
p(n) = bestPhrase ( perm (n, ? i p(ci)) o LMPOS(n) )
(2)
Here, LMPOS(n) represents the language model
associated with the part-of-speech of the node n.
5.3 Model 3 : Head-word STLM
In the models presented earlier, a node and its chil-
dren are ordered using the best phrases of the chil-
dren. For example, the best phrase assigned to the
node ?was? is computed by taking of the permutation
of ?was? and its children ?The equity market?, ?illiq-
uid? and ?.? and then applying the language model.
In model 3, instead of considering best phrases while
ordering, the heads of the the children ci are consid-
ered. For example, the best phrase assigned to the
node ?was? is computed by first permuting the nodes
?was?, ?market?, ?illiquid? and ?.? and then apply-
ing the language models trained on the treelets (head
and children) and not on entire sub-trees.
The major advantage of using this model is that
order at a node is independent of the best phrases of
its descendants and also any mistakes in computa-
tion of best phrases of descendants doesn?t effect the
choice of reordering decision at a particular node.
5.4 Model 4 : POS based STLM
We now experiment by using Part-Of-Speech (POS)
tags of words for ordering the nodes. In the previ-
ous approaches, the language models were trained
on the words which were then used to compute the
best strings associated with various nodes. Here,
we order the node and its children using a language
model trained on POS tag sequences. The motiva-
tion behind buliding such kind of Language models
is that it deals with unseen words effectively. Hence,
in this model, the best phrase corresponding to the
node ?was? is obtained by permuting the POS tags
of the words ?was?, ?market?, ?illiquid and ?.? which
are ?VBZ?, ?NN?, ?NN? and ?.? respectively. As the
best POS tag sequence might correspond to several
orderings of the treelet, a word based STLM is ap-
plied to choose the correct ordering.
The major advantages of this model is that it is
more general and it deals with unseen words effec-
tively. Also, it is much faster than earlier models as
this model is a POS tag based model.
5.5 Model 5: Head-marked POS based STLM
In POS based STLM, the head of a particular node
isn?t marked while applying the language model.
Hence, all the nodes of the treelet are treated equally
while applying the LM. For example, in Figure 2, the
structures of treelets is not taken into account while
applying the head-POS based language model. Both
are treated in the same manner while applying TLM.
In this model, we experiment by marking the head
22
information for the POS of the head word which
treats the treelets in Figure 2 in a different manner to
obtain the best phrase. As the best POS tag sequence
might correspond to several orderings of the treelet,
we test various word-based approaches to choose the
best ordering among the many possibilities. The best
approach was the one where head-word of the treelet
had the POS tag attached to it.
VB
VBP NN
VBP
VB NN
Figure 2: Two different treelets which would have
same best POS tag sequence
6 Results and Discussion
To evaluate our models, we compare the system gen-
erated sentences with reference sentences and get
the BLEU score. As mentioned in section 4, We
evaluate our models on two different types of in-
put. In the first input type, we have bag of words
with dependency constraints extracted from tree-
bank and in the second input type, the dependency
constraints among the bag of words are extracted
from the parser which are noisy. Table 2 shows the
results of model 1-5.
Model Treebank(gold) Parser(noisy)
Model 1 0.5472 0.5514
Model 2 0.6886 0.6870
Model 3 0.7284 0.7227
Model 4 0.7890 0.7783
Model 5 0.8156 0.8027
Table 2: The results of Model 1-5
We can observe that in model 1, BLEU score of
the parser input is high when compared to Treebank
input. This might be because, the parser input is pro-
jective (as we used projective parsing) whereas the
treebank input might contain some non-projective
cases. In general, for all the models, the results with
noisy dependency links are comparable to the cases
where gold dependency links are used which is en-
couraging.
We have taken the Table-3 from (Guo et al,
2008), which shows the BLEU scores of different
Paper BLEU score
Langkilde(2002) 0.757
Nakanishi(2005) 0.705
Cahill(2006) 0.6651
Hogan(2007) 0.6882
White(2007) 0.5768
Guo(2008) 0.7440
Our Model 0.8156
Table 3: Comparsion of results for English WSJ sec-
tion 23
systems on section 23 of PTB. Its really difficult to
compare sentence realisers as the information con-
tained in the input vaires greatly between systems.
But, we can clearly see that the our system performs
better than all the systems. The main observations
from the results are, (1) Searching the entire space of
O(n!) helps, (2) Treelet LM capture characteristics
of phrases headed by various POS tags, in contrast to
sentential LM which is a general LM, (3) POS tags
can play an important role in ordering nodes of a de-
pendency structure, (4) The head models performed
better than the models that used all the nodes of the
sub-tree, and (5) Marking the head of a treelet pro-
vides vital clues to the language model for reorder-
ing.
7 Future Experiments
Although the results of the proposed models are
much higher when compared to other methods, the
major constraint with our models is the computa-
tional complexity, which is O(n!). However, our ap-
proach is still tractable because of the low values of
n. We plan to reduce the search space complexity by
using Viterbi search (Guo et al, 2008), and examine
the drop in results because of that.
The models proposed in paper, consider only the
locally best phrases (local to the sub-tree) at every
step. In order to retain the globally best possibilities
at every step, we plan to use beam search, where we
retain K-best best phrases for every sub-tree.
Also, the goal is to test the approach for
morphologically-rich languages such as Hindi.
Also, it would require us to expand our features set.
We also plan to test the factored models.
The most important experiment that we plan to
23
perform is to test our system in the context of MT,
where the input is more real and noisy.
To train more robust language models, we plan to
use the much larger data on a web scale.
8 Conclusion
In this paper, we had experimented with five ngram
based models for sentence realisation from bag of
words with dependency constraints. We have evalu-
ated our models on two different types of input(gold
and noisy). From the results, we can conclude that
the model ?Marked Head-POS based LM? works
best in both cases.
Acknowledgments
The authors of this work were supported by ILMT
grant 11(10)/2006-HCC(TDIL) and EILMT grant
11(9)/2006HCC(TDIL). We would also like to thank
the four reviewers for their valuable reviews.
References
S. Bangalore and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. Proceedings
of the 18th conference on Computational linguistics.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statisti-
cal Machine Translation through Global Lexical Selec-
tion and Sentence Reconstruction. In Annual Meeting
- ACL, volume 45.
A. Belz. 2007. Probabilistic Generation of Weather
Forecast Texts. In Proceedings of NAACL HLT.
A. Cahill and J. van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LINGUIS-
TICS, volume 44.
P.C. Chang and K. Toutanova. 2007. A Discriminative
Syntactic Word Order Model for Machine Translation.
Proceedings of the 45th Annual Meeting of the ACL.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2007. XLE documen-
tation. Available on-line.
M. Elhadad. 1991. FUF: The universal unifier user man-
ual version 5.0. Department of Computer Science,
Columbia University. New York.
Y. Guo, J. van Genabith, and H. Wang. 2008.
Dependency-Based N-Gram Models for General Pur-
pose Sentence Realisation. Proceedings of the 22nd
conference on Computational linguistics.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Gen-
abith. 2007. Exploiting Multi-Word Units in History-
Based Probabilistic Generation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,
A.F. Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen.
2003. Experiments with a Hindi-to-English transfer-
based MT system under a miserly data scenario. ACM-
TALIP, 2(2).
D.M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd annual meeting
on ACL. ACL Morristown, NJ, USA.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics, 19(2).
H. Nakanishi and Y. Miyao. 2005. Probabilistic models
for disambiguation of an HPSG-based chart generator.
In Proceedings of the International Workshop on Pars-
ing Technology.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proceedings of the
Tenth CoNLL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Proceedings of the 40th Annual Meeting
on ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal SMT. Proceedings of the 43rd Annual Meet-
ing of ACL.
S. Venkatapathy and S. Bangalore. 2007. Three mod-
els for discriminative machine translation using Global
Lexical Selection and Sentence Reconstruction. In
Proceedings of SSST, NAACLHLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages
152?159.
M. White, R. Rajkumar, and S. Martin. 2007. Towards
Broad Coverage Surface Realization with CCG. In
Proceedings of the Workshop on Using Corpora for
NLG: Language Generation and Machine Translation
(UCNLG+ MT).
M. White. 2004. Reining in CCG Chart Realization.
LECTURE NOTES IN COMPUTER SCIENCE.
24
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 162?165,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Simple Parser for Indian Languages in a Dependency Framework 
 
Akshar Bharati, Mridul Gupta, Vineet Yadav, Karthik Gali and Dipti Misra Sharma 
Language Technologies Research Center, 
IIIT-Hyderabad, India 
{mridulgupta,vineetyadav}@students.iiit.ac.in, 
karthikg@research.iiit.ac.in,dipti@iiit.ac.in 
 
 
Abstract 
This paper is an attempt to show that an inter-
mediary level of analysis is an effective way 
for carrying out various NLP tasks for linguis-
tically similar languages. We describe a 
process for developing a simple parser for 
doing such tasks. This parser uses a grammar 
driven approach to annotate dependency rela-
tions (both inter and intra chunk) at an inter-
mediary level. Ease in identifying a particular 
dependency relation dictates the degree of 
analysis reached by the parser. To establish ef-
ficiency of the simple parser we show the im-
provement in its results over previous gram-
mar driven dependency parsing approaches for 
Indian languages like Hindi. We also propose 
the possibility of usefulness of the simple 
parser for Indian languages that are similar in 
nature. 
1 Introduction and Related Work 
Broad coverage parsing is a challenging task. For 
languages such as the Indian languages, it be-
comes all the more difficult as these languages 
are morphologically richer and the word order 
for these languages is relatively variable and less 
bound. Although dependency grammar driven 
parsing is much better suited for such type of 
languages (Hudson, 1984; Mel?Cuk, 1988), ro-
bust broad coverage parsing (Bharati et. al, 2008) 
still involves extensive analysis. Achieving good 
results in parsing for these languages may re-
quire large amount of linguistic resources such as 
annotated corpora, verb frames, lexicon etc. On 
the other hand, pure shallow parsing techniques 
(PVS and Gali, 2007) are not enough for provid-
ing sufficient information for applications such 
as machine translation, query answering etc.  
It is here that the notion of a simple parser is 
born where the idea is to parse a sentence at a 
coarser level. One could go to a finer level of 
parse depending on the ease with which such a 
parse can be generated. The simple parser that 
we describe here is a grammar oriented model 
that makes use of linguistic features to identify 
relations. We have modeled the simple parser on 
the Paninian grammatical model (Begum et al, 
2008; Bharati et al, 1995) which provides a de-
pendency grammar framework. Paninian depen-
dency grammar works well for analyzing Indian 
languages (Bharati et al, 1993).  We have fol-
lowed karaka1 based approach for parsing. 
An effort has been previously made in gram-
mar driven parsing for Hindi by us (Gupta et al, 
2008) where the focus was not to mark relations 
in a broad coverage sense but to mark certain 
easily identifiable relations using a rule base. In 
this paper, we show improvements in results over 
our previous work by including some additional 
linguistic features which help in identifying rela-
tions better. Our previous work focused only on 
inter-chunk annotation. In this paper, however, 
we have worked on both inter as well as intra 
chunk annotation. We later show their effective-
ness and results at different levels of dependency 
annotation. We also propose how useful the sim-
ple parser is for Indian languages which are simi-
lar in nature. 
2 Paninian Dependency Annotation 
Scheme at Various Levels 
Paninian dependency scheme is based on a mod-
ifier-modified relationship (Bharati et al, 1995). 
The modified chunk (or group) is classified on 
the basis of its part of speech category. A hie-
rarchy of dependency relations is thus estab-
lished on the basis of this category. For example, 
all those relations whose parent (modified group) 
is a verb are classified under the verb modifier 
(vmod) category. Subsequent levels further clas-
sify these relations (or labels). Depth of a level in 
the hierarchy reflects the fineness of the depen-
dency relations/labels. There are five labels at the 
                                               
1 The elements modifying the verb participate in the action 
specified by the verb. These participant relations with the 
verb are called karakas. 
162
coarsest level namely, vmod, nmod (noun mod-
ifier), jjmod (adjective modifier), advmod (ad-
verbial modifier) and ccof (conjunct of). 
Athough, ccof is not strictly a dependency rela-
tion (Begum et al, 2008). Figure 1 shows the 
hierarchy of relations used in the scheme. 
 
 
Figure 1: Hierarchy of Dependency Labels. 
 
The next level comprises of varg (verb argu-
ment), vad (verb adjunct) and vmod_1 2  labels 
under vmod. Under the nmod label, nmod_adj 
(adjective), r6 (genitive) are classified. At the 
most fine grained level, varg and vad further 
branch out into labels like k1, k2, k3, k5, k7 and 
rh, rt, rd, k1s etc. The relations under varg are the 
six karakas that are the most essential partici-
pants in an action. All the other dependency la-
bels3 are non-karakas (for a more detailed expla-
nation see Begum et al (2008) and Bharati et al 
(1995)). 
Languages often have constructions that are 
ambiguous, owing to similar feature and context 
distribution. Thus, in such cases, it is appropriate 
to under-specify the relations (labels) or group 
some of them together. Also, some labels have 
very less frequency of occurrence in the corpus 
and it is thus appropriate to leave them out for 
marking by the simple parser. One can later, on 
the availability of more information, try to identi-
fy and mark such instances with appropriate la-
bels. 
The dependency tagset described in this sec-
tion is used to mark inter-chunk relations. For 
marking relations between words within a chunk 
(intra-chunk), a similar tagset has been devel-
oped. 
                                               
2 vmod_1: A dependency relation in the vmod category, that 
exists between a non-finite verb and its parent verb. It has 
been under-specified for simplicity. 
3A comprehensive list of the dependency tagset can be 
found at http://ltrc.iiit.ac.in/MachineTrans/research/tb/dep-
tagset.pdf 
3 Procedure 
Our approach is corpus based where rules have 
been crafted after studying the corpus. We used 
the Hyderabad Dependency Treebank (HyDT) 
for development and testing our rules. The tree-
bank consists of about 2100 sentences in Hindi, 
of which 1800 were part of the development set 
and 300 were used as test data. Each sentence is 
POS tagged and chunked (Bharati et al, 2006) in 
SSF format (Bharati et al, 2005). 
3.1 Approach 
The simple parser we propose here is a language 
independent engine that takes a rule file specific 
for a particular language (Gupta et. al, 2008). 
Indian languages are similar in various respects 
(Emeneau 1956; 1980). Hence, rules made for 
one language can be efficiently transferred for 
other similar languages. However, there can be 
cases where rules for one language may not work 
for another. These cases can be handled by add-
ing some new rules for that particular language. 
The relative closeness among such languages, 
determines the efficiency of transference of rules 
from one language to another. We have taken 
Hindi and Punjabi, as example languages to sup-
port our proposal. 1(a) below is in Hindi, 
 
1(a). raama  ko      mithaaii acchii    nahii 
     ?Ram - dat?      ?sweets?          ?good?     ?not? 
       lagatii. 
      ?appear? 
 
?Ram does not like sweets.? 
 
Its corresponding Punjabi sentence, 
1(b).  raama   nuu  mitthaai   changii        nii                   
        ?Ram - dat?   ?sweets?  ?good?       ?not? 
     lagadii. 
     ?appear? 
 
?Ram does not like sweets.? 
 
Now, the rules for identifying k14 and k2 in 
Hindi are similar to that of Punjabi. For instance, 
in both the cases, the noun chunk possessing a 
nominative case marker (chunks take the proper-
ties of their heads) and the TAM (tense, aspect 
and modality of the main verb) should agree in 
                                               
4
 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 for example, behaves similar to subject 
and agent. Likewise, k2 behaves like object/theme (Begum 
et al, 2008) 
163
GNP for the noun to be a k2. It is easy to see 
how rules made for identifying certain relations 
in Hindi can be transferred to identify the same 
relations in Punjabi and similarly for other lan-
guages. However, not all rules can be transferred 
from one language to another. 
3.2 Intra-chunk Relations 
We also mark intra-chunk dependency relations. 
The procedure of marking intra-chunk labels is 
also rule based. Rules have been crafted using a 
common POS5 tagset for Indian languages (Bha-
rati et al, 2006). Rules can be applied to other 
languages. However, some rules may not work. 
In those cases we need to add some rules specific 
to the language. The rule format is a five-tuple 
containing the following fields, 
1. Modified word 
2. Modified constraints 
3. Modifier word 
4. Modifier constraints 
5. Dependency relation 
Rules for marking intra-chunk relations have 
been marked studying the POS tagged and 
chunked corpus. Commonly occurring linguistic 
patterns between two or more nodes are drawn 
out in the form of statistics and their figures are 
collected. Such patterns are then converted into 
robust rules. 
4 Experiments and Results 
We conducted experiments using the simple 
parser to establish its efficacy in identifying a 
particular set of relations explained in section 2. 
Experiments were conducted on gold standard 
test data derived from HyDT. The experiments 
were carried out on Hindi. 
4.1 Marking Relations at Various Levels 
We marked dependency labels at various levels 
described above using the proposed simple pars-
er. The results are shown below We report two 
measures for evaluation, labeled (L) and labeled 
attachment (LA). Table 1 shows results for mark-
ing relations at the top most level (cf. Figure 1).  
It should be noted that we have not marked re-
lations like jjmod and advmod because the fre-
quency of their occurrence in the treebank is 
quite low. The focus is only on those relations 
whose frequency of occurrence is above a bare 
minimum (>15). The frequency of labels like 
jjmod and advmod is not above that threshold 
                                               
5 POS: Part of Speech 
value (Relations like k1 and k2 occur more than 
1500 times in the treebank). 
 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
vmod 93.7% 83.0% 76.1% 67.4% 
nmod 83.6% 79.1% 77.5% 73.3% 
ccof 92.9% 82.9% 53.5% 50.4% 
Total 91.8% 82.3% 72.9% 65.4% 
Table 1. Figures for relations at the highest level. 
 
Table 2 below depicts the figures obtained for 
the next level. 
Relation 
Precision 
L LA 
 
Recall 
L     LA 
 
varg 77.7% 69.3% 77.9% 69.4% 
vad 75.2% 66.6% 30.3% 26.9% 
vmod_1 89.6% 75.8% 46.0% 38.9% 
r6 83.2% 78.5% 90.2% 85.2% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
Total 79.1% 71.2% 64.6% 58.2% 
Table 2. Figures for level 2. 
 
In section 1, improvement in marking certain 
relations over our previous attempt (Gupta et. al, 
2008) was mentioned. We provide a comparison 
of the results for the simple parser as opposed to 
the previous results. Figures shown in table 3 
have been reproduced for comparing them 
against the results of the simple parser shown in 
this paper. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 66.0% 57.7% 65.1% 57.6% 
k2 31.3% 28.3% 27.8% 25.1% 
k7(p/t) 80.8% 77.2% 61.0% 58.4% 
r6 82.1% 78.7% 89.6% 85.8% 
nmod__adj 23.2% 21.9% 27.4% 25.8% 
Table 3. Figures reproduced from our previous 
work. 
 
Table 4 shows results of the simple parser. 
Note the improvement in precision values for all 
the relations.  
 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
k1 72.6% 68.0% 67.9% 63.5% 
k2 61.6% 54.1% 29.9% 26.2% 
k7(p/t) 84.6% 77.9% 73.5% 68.7% 
r6 83.2% 78.6% 90.2% 85.5% 
nmod__adj 77.8% 77.8% 10.9% 10.9% 
pof 89.4% 87.7% 25.7% 25.2% 
Table 4. Figures for simple parser. 
164
4.2 Intra-chunk Experiments 
We also carried out some experiments to deter-
mine the efficiency of the simple parser with re-
spect to annotating intra-chunk relations for Hin-
di. Results shown below were obtained after test-
ing the simple parser using gold standard test 
data of about 200 sentences. Table 5 shows fig-
ures for labeled accuracy as well as labeled at-
tachment accuracy. 
 
Relation 
Precision 
L LA 
 
Recall 
L LA 
 
nmod 100% 89.3% 70.0% 62.5% 
nmod__adj 100% 92.7% 85.2% 79.0% 
nmod__dem 100% 100% 100% 100% 
nmod__qf 97.0% 92.4% 80.0% 76.2% 
pof 84.5% 82.1% 94.5% 92.0% 
ccof 91.8% 80.0% 70.9% 62.0% 
jjmod__intf 100% 100% 100% 100% 
Total 96.2% 90.4% 82.6% 77.7% 
Table 5. Figures for intra-chunk annotation. 
5 Conclusion 
We introduced the notion of a simple parser for 
Indian languages which follows a grammar dri-
ven methodology. We compared its performance 
against previous similar attempts and reported its 
efficiency. We showed how by using simple yet 
robust rules one can achieve high performance in 
the identification of various levels of dependency 
relations. 
The immediate tasks for the near future would 
be to identify relative clauses in order to reduce 
labeled attachment errors and hence to come up 
with rules for better identification of clauses. We 
also intend to thoroughly test our rules for Indian 
languages that are similar in nature and hence 
evaluate the efficiency of the simple parser. 
Acknowledgements  
We sincerely thank Samar Husain, for his impor-
tant role in providing us with valuable linguistic 
inputs and ideas. The treebank (Hyderabad de-
pendency treebank, version 0.05) used, was pre-
pared at LTRC, IIIT-Hyderabad. 
References 
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti 
Misra Sharma, Lakshmi Bai, and Rajeev Sangal. 
2008. Dependency annotation scheme for Indian 
languages. In Proceedings of IJCNLP-2008. 
Akshar Bharati, Vineet Chaitanya and Rajeev Sangal. 
1995. Natural Language Processing: A Pani-
nian Perspective, Prentice-Hall of India, New 
Delhi, pp. 65-106. 
Akshar Bharati, Samar Husain, Dipti Misra Sharma, 
and Rajeev Sangal. 2008. A Two-Stage Constraint 
Based Dependency Parser for Free Word Order 
Languages. In Proc. of the COLIPS Interna-
tional Conference on Asian Language 
Processing 2008 (IALP). Chiang Mai, Thailand. 
2008. 
Akshar Bharati and Rajeev Sangal. 1993. Parsing Free 
Word Order Languages in the Paninian Frame-
work, ACL93: Proc. of Annual Meeting of As-
sociation for Computational Linguistics. 
Akshar Bharati, Rajeev Sangal and Dipti M. Sharma. 
2005. ShaktiAnalyser: SSF Representation.  
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma 
and Lakshmi Bai. 2006. AnnCorra: Annotating 
Corpora Guidelines for POS and Chunk Annota-
tion for Indian Languages. Technical Report 
(TR-LTRC-31), Language Technologies Re-
search Centre IIIT, Hyderabad 
http://ltrc.iiit.ac.in/MachineTrans/publications
/technicalReports/tr031/posguidelines.pdf 
Murray B. Emeneau. 1956. India as a linguistic area. 
Linguistics, 32:3-16. 
Murray B. Emeneau. 1980. Language and linguis-
tic area. Essays by Murray B. Emeneau. Se-
lected and introduced by Anwar S. Dil. Stan-
ford University Press. 
Mridul Gupta, Vineet Yadav, Samar Husain and Dipti 
M. Sharma. 2008. A Rule Based Approach for Au-
tomatic Annotation of a Hindi Treebank. In Proc. 
Of the 6th International Conference on Natu-
ral Language Processing (ICON-08), CDAC 
Pune, India. 
R. Hudson. 1984. Word Grammar, Basil Blackwell, 
Oxford, OX4 1JF, England. 
I. Mel'cuk . 1988. Dependency Syntax: Theory and 
Practice, State University, Press of New York. 
Avinesh PVS and Karthik Gali. 2007. Part-of-speech 
tagging and chunking using conditional random 
fields and transformation based learning. In Proc. 
Of IJCAI-07 Workshop on ?Shallow Parsing 
in South Asian Languages?, 2007. 
165
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 124?127,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Modeling Machine Transliteration as a Phrase Based Statistical Machine
Translation Problem
Taraka Rama, Karthik Gali
Language Technologies Research Centre,
IIIT, Hyderabad, India.
{taraka,karthikg}@students.iiit.ac.in
Abstract
In this paper we use the popular phrase-
based SMT techniques for the task of
machine transliteration, for English-Hindi
language pair. Minimum error rate train-
ing has been used to learn the model
weights. We have achieved an accuracy of
46.3% on the test set. Our results show
these techniques can be successfully used
for the task of machine transliteration.
1 Introduction
Transliteration can be defined as the task of tran-
scribing the words from a source script to a tar-
get script (Surana and Singh, 2008). Translitera-
tion systems find wide applications in Cross Lin-
gual Information Retrieval Systems (CLIR) and
Machine Translation (MT) systems. The systems
also find use in sentence aligners and word align-
ers (Aswani and Gaizauskas, 2005). Transcribing
the words from one language to another language
without the use of a bilingual lexicon is a chal-
lenging task as the output word produced in tar-
get language should be such that it is acceptable
to the readers of the target language. The dif-
ficulty arises due to the huge number of Out Of
Vocabulary (OOV) words which are continuously
added into the language. These OOV words in-
clude named entities, technical words, borrowed
words and loan words.
In this paper we present a technique for translit-
erating named entities from English to Hindi us-
ing a small set of training and development data.
The paper is organised as follows. A survey of the
previous work is presented in the next subsection.
Section 2 describes the problem modeling which
we have adopted from (Rama et al, 2009) which
they use for L2P task. Section 3 describes how
the parameters are tuned for optimal performance.
A brief description of the data sets is provided in
Section 4. Section 5 has the results which we have
obtained for the test data. Finally we conclude
with a summary of the methods and a analysis of
the errors.
1.1 Previous Work
Surana and Singh (2008) propose a transliteration
system in which they use two different ways of
transliterating the named entities based on their
origin. A word is classified into two classes either
Indian or foreign using character based n-grams.
They report their results on Telugu and Hindi
data sets. Sherif and Kondrak (2007) propose a
hybrid approach in which they use the Viterbi-
based monotone search algorithm for searching
the possible candidate transliterations. Using the
approach given in (Ristad et al, 1998) the sub-
string translations are learnt. They integrate the
word-based unigram model based on (Knight and
Graehl, 1998; Al-Onaizan and Knight, 2002) with
the above model for improving the quality of
transliterations.
Malik (2006) tries to solve a special case of
transliteration for Punjabi in which they con-
vert from Shahmukhi (Arabic script) to Guru-
mukhi using a set of transliteration rules. Abdul
Jaleel (2003) show that, in the domain of informa-
tion retrieval, the cross language retrieval perfor-
mance was reduced by 50% when the name enti-
ties were not transliterated.
2 Problem Modeling
Assume that given a word, represented as a se-
quence of letters of the source language s = sJ1 =
s1...sj ...sJ , needs to be transcribed as a sequence
of letters in the target language, represented as t
= tI1 = t1...ti...tI . The problem of finding the best
target language letter sequence among the translit-
erated candidates can be represented as:
124
tbest = argmaxt
{Pr (t | s)} (1)
We model the transliteration problem based on
the noisy channel model. Reformulating the above
equation using Bayes Rule:
tbest = argmaxt
p (s | t) p (s) (2)
This formulation allows for a target language
letters? n-gram model p (t) and a transcription
model p (s | t). Given a sequence of letters s, the
argmax function is a search function to output the
best target letter sequence.
From the above equation, the best target se-
quence is obtained based on the product of the
probabilities of transcription model and the prob-
abilities of a language model and their respective
weights. The method for obtaining the transcrip-
tion probabilities is described briefly in the next
section. Determining the best weights is necessary
for obtaining the right target language sequence.
The estimation of the models? weights can be done
in the following manner.
The posterior probability Pr (t | s) can also be
directly modeled using a log-linear model. In
this model, we have a set of M feature func-
tions hm(t, s),m = 1...M . For each feature
function there exists a weight or model parameter
?m,m = 1...M . Thus the posterior probability
becomes:
Pr (t | s) = p?M1 (t | s) (3)
=
exp
[
?Mm=1?mhm(t, s)
]
?
t?I1
exp
[
?Mm=1?mhm(t?I1, s)
] (4)
with the denominator, a normalization factor that
can be ignored in the maximization process.
The above modeling entails finding the suit-
able model parameters or weights which reflect the
properties of our task. We adopt the criterion fol-
lowed in (Och, 2003) for optimising the parame-
ters of the model. The details of the solution and
proof for the convergence are given in Och (2003).
The models? weights, used for the transliteration
task, are obtained from this training.
All the above tools are available as a part of pub-
licly available MOSES (Koehn et al, 2007) tool
kit. Hence we used the tool kit for our experi-
ments.
3 Tuning the parameters
The source language to target language letters
are aligned using GIZA++ (Och and Ney, 2003).
Every letter is treated as a single word for the
GIZA++ input. The alignments are then used to
learn the phrase transliteration probabilities which
are estimated using the scoring function given
in (Koehn et al, 2003).
The parameters which have a major influence
on the performance of a phrase-based SMT model
are the alignment heuristics, the maximum phrase
length (MPR) and the order of the language
model (Koehn et al, 2003). In the context of
transliteration, phrase means a sequence of let-
ters(of source and target language) mapped to each
other with some probability (i.e., the hypothesis)
and stored in a phrase table. The maximum phrase
length corresponds to the maximum number of let-
ters that a hypothesis can contain. Higher phrase
length corresponds a larger phrase table during de-
coding.
We have conducted experiments to see which
combination gives the best output. We initially
trained the model with various parameters on the
training data and tested for various values of the
above parameters. We varied the maximum phrase
length from 2 to 7. The language model was
trained using SRILM toolkit (Stolcke, 2002). We
varied the order of language model from 2 to 8.
We also traversed the alignment heuristics spec-
trum, from the parsimonious intersect at one end
of the spectrum through grow, grow-diag, grow-
diag-final, grow-diag-final-and and srctotrg to the
most lenient union at the other end.
We observed that the best results were obtained
when the language model was trained on 7-gram
and the alignment heuristic was grow-diag-final.
No significant improvement was observed in the
results when the value of MPR was greater than 7.
We have done post-processing and taken care such
that the alignments are always monotonic and no
letter was left unlinked.
4 Data Sets
We have used the data sets provided by organis-
ers of the NEWS 2009 Machine Transliteration
Shared Task (Kumaran and Kellner, 2007). Prior
to the release of the test data only the training data
and development data was available. The training
data and development data consisted of a parallel
corpus having entries in both English and Hindi.
125
The training data and development data had 9975
entries and 974 entries respectively. We used the
training data given as a part of the shared task
for generating the phrase table and the language
model. For tuning the parameters mentioned in the
previous section, we used the development data.
From the training and development data we
have observed that the words can be roughly di-
vided into following categories, Persian, European
(primarily English), Indian, Arabic words, based
on their origin. The test data consisted of 1000 en-
tries. We proceeded to experiment with the test set
once the set was released.
5 Experiments and Results
The parameters described in Section 3 were the
initial settings of the system. The system was
tuned on the development set, as described in
Section 2, for obtaining the appropriate model
weights. The system tuned on the development
data was used to test it against the test data set.
We have obtained the following model weights.
The other features available in the translation sys-
tem such as word penalty, phrase penalty donot
account in the transliteration task and hence were
not included.
language model = 0.099
translation model = 0.122
Prior to the release of the test data, we tested the
system without tuning on development data. The
default model weights were used to test our sys-
tem on the development data. In the next step the
model weights were obtained by tuning the sys-
tem. Although the system allows for a distortion
model, allowing for phrase movements, we did not
use the distortion model as distortion is meaning-
less in the domain of transliteration. The following
measures such as Word Accuracy (ACC), Mean F-
Score, Mean Reciprocal Rank (MRR), MAPref ,
MAP10, MAPsys were used to evaluate our sys-
tem performance. A detailed description of each
measure is available in (Li et al, 2009).
Measure Result
ACC 0.463
Mean F-Score 0.876
MRR 0.573
MAPref 0.454
MAP10 0.201
MAPsys 0.201
Table 1: Evaluation of Various Measures on Test
Data
6 Conclusion
In this paper we show that we can use the pop-
ular phrase based SMT systems successfully for
the task of transliteration. The publicly available
tool GIZA++ was used to align the letters. Then
the phrases were extracted and counted and stored
in phrase tables. The weights were estimated us-
ing minimum error rate training as described ear-
lier using development data. Then beam-search
based decoder was used to transliterate the English
words into Hindi. After the release of the refer-
ence corpora we examined the error results and
observed that majority of the errors resulted in the
case of the foreign origin words. We provide some
examples of the foreign origin words which were
transliterated erroneously.
Figure 1: Error Transliterations of Some Foreign
Origin Words
References
N. AbdulJaleel and L.S. Larkey. 2003. Statistical
transliteration for english-arabic cross language in-
formation retrieval.
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of
the ACL-02 workshop on Computational approaches
to semitic languages, pages 1?13. Association for
Computational Linguistics Morristown, NJ, USA.
N. Aswani and R. Gaizauskas. 2005. A hybrid ap-
proach to align sentences and words in English-
Hindi parallel corpora. Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, page 57.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(4):599?612.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the NAACL:HLT-Volume 1, pages 48?
54. ACL Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL, volume 45, page 2.
126
A. Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proceedings
of the 30th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 721?722. ACM New York, NY,
USA.
H. Li, A. Kumaran, M. Zhang, and V. Pervouch-
ine. 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009). ACL, Singapore, 2009.
M.G.A. Malik. 2006. Punjabi machine transliteration.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 1137?1144. Association for Compu-
tational Linguistics Morristown, NJ, USA.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting on ACL-Volume 1, pages 160?167.
ACL, Morristown, NJ, USA.
T. Rama, A.K. Singh, and S. Kolachina. 2009. Model-
ing letter to phoneme conversion as a phrase based
statistical machine translation problem with mini-
mum error rate training. In The NAACL Student Re-
search Workshop, Boulder, Colorado.
ES Ristad, PN Yianilos, M.T. Inc, and NJ Princeton.
1998. Learning string-edit distance. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(5):522?532.
T. Sherif and G. Kondrak. 2007. Substring-
based transliteration. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LIN-
GUISTICS, volume 45, page 944.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit.
H. Surana and A.K. Singh. 2008. A more discern-
ing and adaptable multilingual transliteration mech-
anism for indian languages. In Proceedings of
the Third International Joint Conference on Natural
Language Processing.
127
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66?74,
COLING 2010, Beijing, August 2010.
A Discriminative Approach for Dependency Based
Statistical Machine Translation
Sriram Venkatapathy
LTRC, IIIT-Hyderabad
sriram@research.iiit.ac.in sangal@mail.iiit.ac.in
Rajeev Sangal
LTRC, IIIT-Hyderabad
Aravind Joshi
University of Pennsylvania
joshi@seas.upenn.edu karthik.gali@gmail.com
Karthik Gali1
Talentica
Abstract
In this paper, we propose a dependency
based statistical system that uses discrim-
inative techniques to train its parameters.
We conducted experiments on an English-
Hindi parallel corpora. The use of syntax
(dependency tree) allows us to address the
large word-reorderings between English
and Hindi. And, discriminative training
allows us to use rich feature sets, includ-
ing linguistic features that are useful in the
machine translation task. We present re-
sults of the experimental implementation
of the system in this paper.
1 Introduction
Syntax based approaches for Machine Translation
(MT) have gained popularity in recent times be-
cause of their ability to handle long distance re-
orderings (Wu, 1997; Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005), especially for
divergent language pairs such as English-Hindi
(or English-Urdu). Languages such as Hindi are
also known for their rich morphology and long
distance agreement of features of syntactically re-
lated units. The morphological richness can be
handled by employing techniques that factor the
lexical items into morphological factors. This
strategy is also useful in the context of English-
Hindi MT (Bharati et al, 1997; Bharati et al,
1This work was done at LTRC, IIIT-Hyderabad, when he
was a masters student, till July 2008
2002; Ananthakrishnan et al, 2008; Ramanathan
et al, 2009) where there is very limited paral-
lel corpora available, and breaking words into
smaller units helps in reducing sparsity. In or-
der to handle phenomenon such as long-distance
word agreement to achieve accurate generation of
target language words, the inter-dependence be-
tween the factors of syntactically related words
need to be modelled effectively.
Some of the limitations with the syntax based
approaches such as (Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005) are, (1) They
do not offer flexibility for adding linguistically
motivated features, and (2) It is not possible to
use morphological factors in the syntax based ap-
proaches. In a recent work (Shen et al, 2009), lin-
guistic and contextual information was effectively
used in the framework of a hierarchical machine
translation system. In their work, four linguistic
and contextual features are used for accurate se-
lection of translation rules. In our approach in
contrast, linguistically motivated features can be
defined that directly effect the prediction of var-
ious elements in the target during the translation
process. This features use syntactic labels and col-
location statistics in order to allow effective train-
ing of the model.
Some of the other approaches related to our
model are the Direct Translation Model 2 (DTM2)
(Ittycheriah and Roukos, 2007), End-to-End Dis-
criminative Approach to MT (Liang et al, 2006)
and Factored Translation Models (Koehn and
Hoang, 2007). In DTM2, a discriminative trans-
66
lation model is defined in the setting of a phrase
based translation system. In their approach, the
features are optimized globally. In contrast to
their approach, we define a discriminative model
for translation in the setting of a syntax based ma-
chine translation system. This allows us to use
both the power of a syntax based approach, as
well as, the power of a large feature space during
translation. In our approach, the weights are op-
timized in order to achieve an accurate prediction
of the individual target nodes, and their relative
positions.
We propose an approach for syntax based sta-
tistical machine translation which models the fol-
lowing aspects of language divergence effectively.
? Word-order variation including long-
distance reordering which is prevalent
between language pairs such as English-
Hindi and English-Japanese.
? Generation of word-forms in the target lan-
guage by predicting the word and its factors.
During prediction, the inter-dependence of
factors of the target word form with the fac-
tors of syntactically related words is consid-
ered.
To accomplish this goal, we visualize the prob-
lem of MT as transformation from a morpho-
logically analyzed source syntactic structure to a
target syntactic structure1 (See Figure 1). The
transformation is factorized into a series of mini-
transformations, which we address as features of
the transformation. The features denote the vari-
ous linguistic modifications in the source structure
to obtain the target syntactic structure. Some of
the examples of features are lexical translation of
a particular source node, the ordering at a particu-
lar source node etc. These features can be entirely
local to a particular node in the syntactic structure
or can span across syntactically related entities.
More about the features (or mini-transformations)
is explained in section 3. The transformation of
a source syntactic structure is scored by taking a
weighted sum of its features 2. Let ? represent
1Note that target structure contains only the target fac-
tors. An accurate and deterministic morphological generator
combines these factors to produce the target word form.
2The features can be either binary-values or real-valued
the transformation of source syntactic structure s,
the score of transformation is computed as repre-
sented in Equation 1.
score(? |s) =
?
i
wi ? fi(?, s) (1)
In Equation 1, f ?is are the various features of
transformation and w?is are the weights of the fea-
tures. The strength of our approach lies in the flex-
ibility it offers in incorporating linguistic features
that are useful in the task of machine translation.
These features are also known as prediction fea-
tures as they map from source language informa-
tion to information in the target language that is
being predicted.
During decoding a source sentence, the goal
is to choose a transformation that has the high-
est score. The source syntactic structure is tra-
versed in a bottom-up fashion and the target syn-
tactic structure is simultaneously built. We used
a bottom-up traversal while decoding because it
builds a contiguous sequence of nodes for the sub-
trees during traversal enabling the application of a
wide variety of language models.
In the training phase, the task is to learn the
weights of features. We use an online large-
margin training algorithm, MIRA (Crammer et
al., 2005), for learning the weights. The weights
are locally updated at every source node during
the bottom-up traversal of the source structure.
For training the translation model, automatically
obtained word-aligned parallel corpus is used. We
used GIZA++ (Och and Ney, 2003) along with the
growing heuristics to word-align the training cor-
pus.
The basic factors of the word used in our exper-
iments are root, part-of-speech, gender, number
and person. In Hindi, common nouns and verbs
have gender information whereas, English doesn?t
contain that information. Apart from the basic
factors, we also consider the role information pro-
vided by labelled dependency parsers. For com-
puting the dependency tree on the source side, We
used stanford parser (Klein and Manning, 2003)
in the experiments presented in this chapter3.
3Stanford parser gives both the phrase-structure tree as
well as dependency relations for a sentence.
67
root=mila,   tense=PAST
gnp=m3sg
root=se
gnp=x3sgroot=raamgnp=m1sg
root=shyaam
gnp=m1sg
root=pay,   tense=PAST
gnp=x3sg,  role=X 
paid/VBD
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
visit/NN
root=visit,  gnp=x3sg
role=obj role=vmodroot=to,     gnp=x3sg
to/TO
root=Shyam,  gnp=x1sg
role=pmod
Shyam/NNP
root=a,    gnp=x3sg
role=nmod
a/DT
Figure 1: Transformation from source structure to target language
The function words such as prepositions and
auxiliary verbs largely express the grammatical
roles/functions of the content words in the sen-
tence. In fact, in many agglutinative languages,
these words are commonly attached to the con-
tent word to form one word form. In this pa-
per, we also conduct experiments where we begin
by grouping the function words with their corre-
sponding function words. These groups of words
are called local-word groups. In these cases, the
function words are considered as factors of the
content words. Section 2 explains more about the
local word groups in English and Hindi.
2 Local Word Groups
Local word groups (LWGs) (Bharati et al, 1998;
Vaidya et al, 2009) consist of a content word and
its associated function words. Local word group-
ing reduces a sentence to a sequence of content
words with the case-markers and tense-markers
acting as their factors. For example, consider
an English sentence ?People of these island have
adopted Hindi as a means of communication?.
?have adopted? is a LWG with root ?adopt? and
tense markers being ?have ed?. Another example
for the LWG will be ?of communication? where
?communication? is the root, and ?of? is the case-
marker. It is to be noted that Local word grouping
is different from chunking, where more than one
content word can be part of a chunk. We obtain lo-
cal word groups in English by processing the out-
put of the stanford parser. In Hindi, the function
words always appear immediately after the con-
tent word4, and it requires simple pattern
matching to obtain the LWGs. The rules ap-
plied are, (1) VM (RB|VAUX)+, and (2) N.* IN.
3 Features
There are three types of transformation features
explored by us, (1) Local Features, (2) Syntactic
Features and, (3) Contextual Features. In this sec-
tion, we describe each of these categories of fea-
tures representing different aspects of transforma-
tion with examples.
3.1 Local Features
The local features capture aspects of local trans-
formation of an atomic treelet in the source
structure to an atomic treelet in the target lan-
guage. Atomic treelet is a semantically non-
decomposible group of one or more nodes in the
syntactic structure. It usually contains only one
node, except for the case of multi-word expres-
sions (MWEs). Figure 2 presents the examples of
local transformation.
Some of the local features used by us in our ex-
periments are (1) dice coefficient, (2) dice coeffi-
cient of roots, (3) dice coefficient of null transla-
tions, (4) treelet translation probability, (5) gnp-
gnp pair, (5) preposition-postposition pair, (6)
tense-tense pair, (7) part-of-speech fertility etc.
Dice coefficients and treelet translation probabil-
ities are measures that express the statistical co-
occurrence of the atomic treelets.
4case-markers are called postpositions
68
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
root=pay,   tense=PASTgnp=x3sg,  role=X 
paid/VBD
visit/NN
root=visit,  gnp=x3sg
role=obj
root=mila,   tense=PASTgnp=m3sgroot=raamgnp=m1sg
Figure 2: Local transformations
3.2 Syntactic Features
The syntactic features are used to model the differ-
ence in the word orders of the two languages. At
every node of the source syntactic structure, these
features define the changes in the relative order
of children during the process of transformation.
They heavily use source information such as part-
of-speech tags and syntactic roles of the source
nodes. One of the features used is reorderPostags.
This feature captures the change in relative po-
sitions of children with respect to their parents
during the tree transformation. An example fea-
ture for the transformation given in Figure 1 is
shown in Figure 3.
  IN   NNP
   
   VB    
  NNP
     VB
 TO
Figure 3: Syntactic feature - reorder postags
The feature reorderPostags is in the form of a
complete transfer rule. To handle cases, where the
left-hand side of ?reorderPostags? does not match
the syntactic structure of the source tree, the sim-
pler feature functions are used to qualify various
reorderings. Instead of using POS tags, feature
functions can be defined that use syntactic roles.
Apart from the above feature functions, we can
also have features that compute the score of a par-
ticular order of children using syntactic language
models (Gali and Venkatapathy, 2009; Guo et al,
2008). Different features can be defined that use
different levels of information pertaining to the
atomic treelet and its children.
3.3 Contextual Features
Contextual features model the inter-dependence
of factors of nodes connected by dependency arcs.
These features are used to enable access to global
information for prediction of target nodes (words
and its factors).
One of the features diceCoeffParent, relates the
parent of a source node to the corresponding target
node (see figure 4.
x1
x2
x3 x4
y
dice
Figure 4: Use of Contextual (parent) information
of x2 for generation of y
The use of this feature is expected to address of
the limitations of using ?atomic treelets? as the ba-
sic units in contrast to phrase based systems which
consider arbitrary sequences of words as units to
encode the local contextual information. In my
case, We relate the target treelet with the contex-
tual information of the source treelet using feature
functions rather than using larger units. Similar
features are used to connect the context of a source
node to the target node.
Various feature functions are defined to han-
dle interaction between the factors of syntacti-
cally related treelets. The gender-number-person
agreement is a factor that is dependent of gender-
number-person factors of the syntactically related
treelets in Hindi. The rules being learnt here
are simple. However, more complex interac-
tions can also be handled though features such as
prep Tense where, the case-marker in the target is
linked to the tense of parent verb.
4 Decoding
The goal is to compute the most probable target
sentence given a source sentence. First, the source
sentence is analyzed using a morphological ana-
lyzer5, local word grouper (see section 2) and a
dependency parser. Given the source structure,
the task of the decoding algorithm is to choose the
transformation that has the maximum score.
5http://www.cis.upenn.edu/?xtag/
69
The dependency tree of the source language
sentence is traversed in a bottom-up fashion for
building the target language structure. At every
source node during the traversal, the local trans-
formation is first computed. Then, the relative or-
der of its children is then computed using the syn-
tactic features. This results in a target structure
associated with the subtree rooted at the particular
node. The target structure associated with the root
node of the source structure is the result of the best
transformation of the entire source structure.
Hence, the task of computing the best transfor-
mation of the entire source structure is factorized
into the tasks of computing the best transforma-
tions of the source treelets. The equation for com-
puting the score of a transformation, Equation 1,
can be modified as Equation 2 given below.
score(? |s) =
?
r
|r| ?
?
i
wi ? fi(?r, r) (2)
where, ?j is the local transformation of the
source treelet r. The best transformation ?? of
source sentence s is,
?? = argmax? score(? |s) (3)
5 Training Algorithm
The goal of the training algorithm is to learn the
feature weights from the word aligned corpus. For
word-alignment, we used the IBM Model 5 imple-
mented in GIZA++ along with the growing heuris-
tics (Koehn et al, 2003). The gold atomic treelets
in the source and their transformation is obtained
by mapping the source node to the target using the
word-alignment information. This information is
stored in the form of transformation tables that is
used for the prediction of target atomic treelets,
prepositions and other factors. The transformation
tables are pruned in order to limit the search and
eliminate redundant information. For each source
element, only the top few entries are retained in
the table. This limit ranges from 3 to 20.
We used an online-large margin algorithm,
MIRA (McDonald and Pereira, 2006; Crammer
et al, 2005), for updating the weights. During
parameter optimization, it is sometimes impossi-
ble to achieve the gold transformation for a node
because the pruned transformation tables may not
lead to the target gold prediction for the source
node. In such cases where the gold transforma-
tion is unreachable, the weights are not updated
at all for the source node as it might cause erro-
neous weight updates. We conducted our exper-
iments by considering both the cases, (1) Identi-
fying source nodes with unreachable transforma-
tions, and (2) Updating weights for all the source
nodes (till a maximum iteration limit). The num-
ber of iterations on the entire corpus can also be
fixed. Typically, two iterations have been found to
be sufficient to train the model.
The dependency tree is traversed in a bottom-up
fashion and the weights are updated at each source
node.
6 Experiments and Results
The important aspects of the translation model
proposed in this paper have been implemented.
Some of the components that handle word in-
sertions and non-projective transformations have
not yet been implemented in the decoder, and
should be considered beyond the scope of this
paper. The focus of this work has been to
build a working syntax based statistical machine
translation system, which can act as a plat-
form for further experiments on similar lines.
The system would be available for download
at http://shakti.iiit.ac.in/?sriram/vaanee.html. To
evaluate this experimental system, a restricted
set of experiments are conducted. The experi-
ments are conducted on the English-Hindi lan-
guage pair using a corpus in tourism domain con-
taining 11300 sentence pairs6.
6.1 Training
6.1.1 Configuration
For training, we used DIT-TOURISM-ALIGN-
TRAIN dataset which is the word-aligned dataset
of 11300 sentence pairs. The word-alignment is
done using GIZA++ (Och and Ney, 2003) toolkit
and then growing heuristics are applied. For
our experiments, we use two growing heuristics,
GROW-DIAG-FINAL-AND and GROW-DIAG-
FINAL as they cover most number of words in
both the sides of the parallel corpora.
6DIT-TOURISM corpus
70
Number of Training Sentences 500
Iterations on Corpus 1-2
Parameter optimization algorithm MIRA
Beam Size 1-20
Maximum update attempts at source node 1-4
Unreachable updates False
Size of transformation tables 3
Table 1: Training Configuration
The training of the model can be performed un-
der different configurations. The configurations
that we used for the training experiments are given
in Table 6.1.1.
6.2 Results
For the complete training, the number of sen-
tences that should be used for the best perfor-
mance of the decoder should be the complete set.
In the paper, we have conducted experiments by
considering 500 training sentences to observe the
best training configuration.
At a source node, the weight vector is itera-
tively updated till the system predicts the gold
transformation. We conducted experiments by fix-
ing the maximum number of update attempts. A
source node, where the gold transformation is not
achieved even after the maximum updates limit,
the update at this source node is termed a update
failure. The source nodes, where the gold trans-
formation is achieved even without making any
updates is known as the correct prediction.
At some of the source nodes, it is not possible
to arrive at the gold target transformation because
of limited size of the training corpus. At such
nodes, we have avoided doing any weight update.
As the desired transformation is unachievable, any
attempt to update the weight vector would cause
noisy weight updates.
We observe various parameters to check the ef-
fectiveness of the training configuration. One of
the parameters (which we refer to as ?updateHits?)
computes the number of successful updates (S)
performed at the source nodes in contrast to num-
ber of failed updates (F ). Successful updates re-
sult in the prediction of the transformation that is
same as the reference transformation. A failed up-
date doesn?t result in the achievement of the cor-
rect prediction even after the maximum iteration
limit (see section 6.1.1) is reached. At some of the
source nodes, the reference transformations are
unreachable (U ). The goal is to choose the con-
figuration that has least number of average failed
updates (F ) because it implies that the model has
been learnt effectively.
UpdateHit
K m P S F U
1. 1 4 1680 2692 84 4081
2. 5 4 1595 2786 75 4081
3. 10 4 1608 2799 49 4081
4. 20 4 1610 2799 47 4081
Table 2: Training Statistics - Effect of Beam Size
From Table 2, we can see that the bigger beam
size leads to a better training of the model. The
beam size was varied between 1 and 20, and the
number of update failures (F ) was observed to be
least at K=20.
UpdateHit
K m P S F U
1. 20 1 1574 2724 158 4081
2. 20 2 1598 2767 91 4081
3. 20 4 1610 2799 47 4081
Table 3: Training Statistics - Effect of maximum
update attempts
In Table 3, we can see that an higher limit on
the maximum number of update attempts results
in less number of update attempts as expected. A
much higher value of m is not preferable because
the training updates makes noisy updates in case
of difficult nodes i.e., the nodes where target trans-
formation is reachable in theory, but is unreach-
able given the set of features.
UpdateHit
K i P S F U
1. 1 1 1680 2692 84 4081
2. 1 2 1679 2694 83 4081
Table 4: Training Statistics - Effect of number of
iterations
Now, we examine the effect of number of it-
71
erations on the quality of the model. In table 4,
we can observe that the number of iterations on
the data has no effect on the quality of the model.
This implies, that the model is adequately learnt
after one pass through the data. This is possible
because of the multiple number of update attempts
allowed at every node. Hence, the weights are up-
dated at a node till the model prediction is consis-
tent with the gold transformation.
Based on the above observations, we consider
the configuration 4 in Table 2 for the decoding ex-
periments.
Now, we present some of the top features
weights leant by the best configuration. The
weights convey that important properties of trans-
formation are being learnt well. Table 5 presents
the weights of the features ?diceRoot?, ?dice-
RootChildren? and ?diceRootParent?.
Feature Weight
dice 75.67
diceChildren 540.31
diceParent 595.94
treelet translation probability (ttp) 1 0.77
treelet translation probability (ttp) 2 389.62
Table 5: Weights of dice coefficient based features
We see that the dice coefficient based local and
contextual features have a positive impact on the
selection of correct transformations. A feature
that uses a syntactic language model to compute
the perplexity per word has a negative weight of
-1.115.
Table 6 presents the top-5 entries of contex-
tual features that describe the translation of source
argument ?nsubj? using contextual information
(?tense? of its parent).
Feature Weight
roleTenseVib:nsubj+NULL NULL 44.194196513246
roleTenseVib:nsubj+has VBN ne 14.4541356715382
roleTenseVib:nsubj+VBD ne 10.9241093097953
roleTenseVib:nsubj+VBP meM 6.14149937079584
roleTenseVib:nsubj+VBP NULL 4.76795730621754
Table 6: Top weights of a contextual feature :
preposition+Tense-postposition
Table 7 presents the top-10 ordering relative po-
sition feature where the head word is a verb. In
this feature, the relative position (left or right) of
the head and the child is captured. For example, a
feature ?relPos:amod-NN?, if active, conveys that
an argument with the role ?amod? is at the left of
a head word with POS tag ?NN?.
Feature Weight
relPos:amod-NN 6.70
relPos:NN-appos 1.62
relPos:lrb-NN 1.62
Table 7: Top weights of relPos feature
6.3 Decoding
We computed the translation accuracies using two
metrics, (1) BLEU score (Papineni et al, 2002),
and (2) Lexical Accuracy (or F-Score) on a test
set of 30 sentences. We compared the accuracy
of the experimental system (Vaanee) presented in
this paper, with Moses (state-of-the-art translation
system) and Shakti (rule-based translation system
7) under similar conditions (with using a develop-
ment set to tune the models). The rule-based sys-
tem considered is a general domain system tuned
to the tourism domain. The best BLEU score for
Moses on the test set is 0.118, and the best lexi-
cal accuracy is 0.512. The best BLEU score for
Shakti is 0.054, and the best lexical accuracy is
0.369.
In comparison, the best BLEU score of Vaanee
is 0.067, while the best lexical accuracy is 0.445.
As observed, the decoding results of the experi-
mental system mentioned here are not yet compa-
rable to the state-of-art. The main reasons for the
low translation accuracies are,
1. Poor Quality of the dataset
The dataset currently available for English-
Hindi language pair is noisy. This is an
extremely large limiting factor for a model
which uses rich linguistic information within
the statistical framework.
2. Low Parser accuracy
7http://shakti.iiit.ac.in/
72
The parser accuracy on the English-Hindi
dataset is low, the reasons being, (1) Noise,
(2) Length of sentences, and (3) Wide scope
of the tourism domain.
3. Word insertions not implemented yet
4. Non-projectivity not yet handled
5. BLEU is not an appropriate metric
BLEU is not an appropriate metric (Anan-
thakrishnan et al, ) for measuring the trans-
lation accuracy into Indian languages.
6. Model is context free as far as targets words
are concerned. Selection depends on chil-
dren but not parents and siblings
This point concerns the decoding algorithm.
The current algorithm is greedy while chos-
ing the best translation at every source node.
It first explores the K-best local transforma-
tions at a source node. It then makes a greedy
selection of the predicted subtree based on
it?s overall score after considering the predic-
tions at the child nodes, and the relative posi-
tion of the local transformation with respect
the predictions at the child nodes.
The problem in this approach is that, an er-
ror once made at a lower level of the tree
is propogated to the top, causing more mis-
takes. A computationally reasonable solution
to this problem is to maintain a K-best list
of predicted subtrees corresponding to every
source node. This allows rectification of a
mistake made at any stage.
The system, however, performs better than the
rule based system. As observed earlier, the right
type of information is being learnt by the model,
and the approach looks promising. The limitations
expressed here shall be addressed in the future.
7 Conclusion
In this work, we presented a syntax based de-
pendency model to effectively handle problems in
translation from English to Indian languages such
as, (1) Large word order variation, and (2) Ac-
curate generation of word-forms in the target lan-
guage by predicted the word and its factors. The
model that we have proposed, has the flexibility of
adding rich linguistic features.
An experimental version of the system has been
implemented, which is available for download at
http://shakti.iiit.ac.in/?sriram/vaanee.html. This
can facilitate as a platform for future research in
syntax based statistical machine translation from
English to Indian languages. We also plan to per-
form experiments using this system between Eu-
ropean languages in future.
The performance of the implemented transla-
tion system, is not yet comparable to the state-
of-art results primarily for two reasons, (1) Poor
quality of available data, because of which our
model which uses rich linguistic information
doesn?t perform as expected, and (2) Components
for word insertion and non-projectivity handling
are yet to be implemented in this version of the
system.
References
Ananthakrishnan, R, B Pushpak, M Sasikumar, and
Ritesh Shah. Some issues in automatic evaluation
of english-hindi mt: more blues for bleu. ICON-
2007.
Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhat-
tacharyya, and M. Sasikumar. 2008. Simple syntac-
tic and morphological processing can help english-
hindi statistical machine translation. In Proceedings
of IJCNLP-2008. IJCNLP.
Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni,
and Rajeev Sangal. 1997. Anusaaraka: Machine
translation in stages. A Quarterly in Artificial Intel-
ligence, NCST, Bombay (renamed as CDAC, Mum-
bai).
Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya,
and Rajeev Sangal. 1998. Paninian grammar
framework applied to english. South Asian Lan-
guage Review, (3).
Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, and
Amba P Kulkarni. 2002. Machine translation activ-
ities in india: A survey. In Proceedings of workshop
on survey on Research and Development of Machine
Translation in Asian Countries.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
73
Crammer, K., R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for structured
classification. Technical report, University of Penn-
sylvania.
Gali, Karthik and Sriram Venkatapathy. 2009. Sen-
tence realisation from bag of words with depen-
dency constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Student Research Workshop and Doctoral Consor-
tium, pages 19?24, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for gen-
eral purpose sentence realisation. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 297?304,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Ittycheriah, Abraham and Salim Roukos. 2007. Di-
rect translation model 2. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 57?64, Rochester, New York, April. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Koehn, Philipp and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-
NAACL 2003), Edmonton, Canada, May.
Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In International Conference
on Computational Linguistics and Association for
Computational Linguistics (COLING/ACL).
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
Och, F.J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, Todd Ward, and W.J.
Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association of Compu-
tational Linguistics, pages 313?318, Philadelphia,
PA, July.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Ramanathan, Ananthakrishnan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: Addressing the crux
of the fluency problem in english-hindi smt. In Pro-
ceedings of ACL-IJCNLP 2009. ACL-IJCNLP.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Vaidya, Ashwini, Samar Husain, Prashanth Reddy, and
Dipti M Sharma. 2009. A karaka based annotation
scheme for english. In Proceedings of CICLing ,
2009.
Wu, Dekai. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2002. A decoder
for syntax-based statistical mt. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 303?310, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
74
