Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 288?297,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Assessing the relative reading level of sentence pairs for text simplification
Sowmya Vajjala and Detmar Meurers
LEAD Graduate School, Seminar f?ur Sprachwissenschaft
Universit?at T?ubingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
While the automatic analysis of the read-
ability of texts has a long history, the use
of readability assessment for text simplifi-
cation has received only little attention so
far. In this paper, we explore readability
models for identifying differences in the
reading levels of simplified and unsimpli-
fied versions of sentences.
Our experiments show that a relative rank-
ing is preferable to an absolute binary one
and that the accuracy of identifying rel-
ative simplification depends on the ini-
tial reading level of the unsimplified ver-
sion. The approach is particularly success-
ful in classifying the relative reading level
of harder sentences.
In terms of practical relevance, the ap-
proach promises to be useful for identi-
fying particularly relevant targets for sim-
plification and to evaluate simplifications
given specific readability constraints.
1 Introduction
Text simplification essentially is the process of
rewriting a given text to make it easier to process
for a given audience. The target audience can ei-
ther be human users trying to understand a text or
machine applications, such as a parser analyzing
text. Text simplification has been used in a vari-
ety of application scenarios, from providing sim-
plified newspaper texts for aphasic readers (Can-
ning and Tait, 1999) to supporting the extraction of
protein-protein interactions in the biomedical do-
main (Jonnalagadda and Gonzalez, 2009).
A related field of research is automatic readabil-
ity assessment, which can be useful for evaluating
text simplification. It can also be relevant for in-
termediate simplification steps, such as the identi-
fication of target sentences for simplification. Yet,
so far there has only been little research connect-
ing the two subfields, possibly because readability
research typically analyzes documents, whereas
simplification approaches generally targeted lex-
ical and syntactic aspects at the sentence level. In
this paper, we attempt to bridge this gap between
readability and simplification by studying read-
ability at a sentence level and exploring how well
can a readability model identify the differences be-
tween unsimplified and simplified sentences.
Our main research questions in this paper are:
1. Can the readability features that worked at the
document level successfully be used at the sen-
tence level? 2. How accurately can we identify the
differences in the sentential reading level before
and after simplification? To pursue these ques-
tions, we started with constructing a document-
level readability model. We then applied it to nor-
mal and simplified versions of sentences drawn
from Wikipedia and Simple Wikipedia.
As context of our work, we first discuss rel-
evant related research. Section 2 then describes
the corpora and the features we used to construct
our readability model. Section 3 discusses the
performance of our readability model in compari-
son with other existing systems. Sections 4 and 5
present our experiments with sentence level read-
ability analysis and the results. In Section 6 we
present our conclusions and plans for future work.
1.1 Related Work
Research into automatic text simplification essen-
tially started with the idea of splitting long sen-
tences into multiple shorter sentences to improve
parsing efficiency (Chandrasekar et al., 1996;
Chandrasekar and Srinivas, 1996). This was
followed by rule-based approaches targeting hu-
man and machine uses (Carroll et al., 1999; Sid-
dharthan, 2002, 2004).
With the availability of a sentence-aligned cor-
pus based on Wikipedia and SimpleWikipedia
288
texts, data-driven approaches, partly inspired by
statistical machine translation, appeared (Specia,
2010; Zhu et al., 2010; Bach et al., 2011; Coster
and Kauchak, 2011; Woodsend and Lapata, 2011).
While simplification methods have evolved, un-
derstanding which parts of a text need to be sim-
plified and methods for evaluating the simplified
text so far received only little attention. The use
of readability assessment for simplification has
mostly been restricted to using traditional read-
ability formulae for evaluating or generating sim-
plified text (Zhu et al., 2010; Wubben et al.,
2012; Klerke and S?gaard, 2013; Stymne et al.,
2013). Some recent work briefly addresses issues
such as classifying sentences by their reading level
(Napoles and Dredze, 2010) and identifying sen-
tential transformations needed for text simplifica-
tion using text complexity features (Medero and
Ostendorf, 2011). Some simplification approaches
for non-English languages (Aluisio et al., 2010;
Gasperin et al., 2009;
?
Stajner et al., 2013) also
touch on the use of readability assessment.
In the present paper, we focus on the neglected
connection between readability analysis and sim-
plification. We show through a cross-corpus eval-
uation that a document level, regression-based
readability model successfully identifies the dif-
ferences between simplified vs. unsimplified sen-
tences. This approach can be useful in various
stages of simplification ranging from identifying
simplification targets to the evaluation of simplifi-
cation outcomes.
2 Corpora and Features
2.1 Corpora
We built and tested our document and sentence
level readability models using three publicly avail-
able text corpora with reading level annotations.
WeeBit Corpus: The WeeBit corpus (Vajjala
and Meurers, 2012) consists of 3,125 articles be-
longing to five reading levels, with 625 articles
per reading level. The texts compiled from the
WeeklyReader and BBC Bitesize target English
language learners from 7 to 16 years of age. We
used this corpus to build our primary readability
model by mapping the five reading levels in the
corpus to a scale of 1?5 and considered readabil-
ity assessment as a regression problem.
Common Core Standards Corpus: This cor-
pus consists of 168 English texts available from
the Appendix B of the Common Core Standards
reading initiative of the U.S. education system
(CCSSO, 2010). They are annotated by experts
with grade bands that cover the grades 1 to 12.
These texts serve as exemplars for the level of
reading ability at a given grade level. This corpus
was introduced as an evaluation corpus for read-
ability models in the recent past (Sheehan et al.,
2010; Nelson et al., 2012; Flor et al., 2013), so we
used it to compare our model with other systems.
Wiki-SimpleWiki Sentence Aligned Corpus:
This corpus was created by Zhu et al. (2010) and
consists of ?100k aligned sentence pairs drawn
from Wikipedia and Simple English Wikipedia.
We removed all pairs of identical sentences, i.e.,
where the Wiki and the SimpleWiki versions are
the same. We used this corpus to study reading
level assessment at the sentence level.
2.2 Features
We started with the feature set described in Vajjala
and Meurers (2012) and added new features fo-
cusing on the morphological and psycholinguistic
properties of words. The features can be broadly
classified into four groups.
Lexical richness and POS features: We
adapted the lexical features from Vajjala and
Meurers (2012). This includes measures of lexical
richness from Second Language Acquisition
(SLA) research and measures of lexical variation
(noun, verb, adjective, adverb and modifier vari-
ation). In addition, this feature set also includes
part-of-speech densities (e.g., the average # of
nouns per sentence). The information needed to
calculate these features was extracted using the
Stanford Tagger (Toutanova et al., 2003). None
of the lexical richness and POS features we used
refer to specific words or lemmas.
Syntactic Complexity features: Parse tree
based features and some syntactic complexity
measures derived from SLA research proved
useful for readability classification in the past, so
we made use of all the syntactic features from
Vajjala and Meurers (2012): mean lengths of
various production units (sentence, clause, t-unit),
measures of coordination and subordination
(e.g., # of coordinate clauses per clause), the
presence of particular syntactic structures (e.g.,
VPs per t-unit), the number of phrases of various
categories (e.g., NP, VP, PP), the average lengths
289
of phrases, the parse tree height, and the number
of constituents per subtree. None of the syntactic
features refer to specific words or lemmas. We
used the BerkeleyParser (Petrov and Klein, 2007)
for generating the parse trees and the Tregex tool
(Levy and Andrew, 2006) to count the occurrences
of the syntactic patterns.
While the first two feature sets are based on our
previous work, as far as we know the next two are
used in readability assessment for the first time.
Features from the Celex Lexical Database:
The Celex Lexical Database (Baayen et al., 1995)
is a database consisting of information about mor-
phological, syntactic, orthographic and phonolog-
ical properties of words along with word frequen-
cies in various corpora. Celex for English contains
this information for more than 50,000 lemmas. An
overview of the fields in the Celex database is pro-
vided online
1
and the Celex user manual
2
.
We used the morphological and syntactic prop-
erties of lemmas as features. We excluded word
frequency statistics and properties which consisted
of word strings. In all, we used 35 morphologi-
cal and 49 syntactic properties that were expressed
using either character or numeric codes in this
database as features for our task.
The morphological properties in Celex include
information about the derivational, inflectional
and compositional features of the words, their
morphological origins and complexity. The syn-
tactic properties of the words in Celex describe
the attributes of a word depending on its parts of
speech. For the morphological and syntactic prop-
erties from this database, we used the proportion
of occurrences per text as features. For example,
the ratio of transitive verbs, complex morphologi-
cal words, and vocative nouns to number of words.
Lemmas from the text that do not have entries in
the Celex database were ignored.
Word frequency statistics from Celex have been
used before to analyze text difficulty in the past
(Crossley et al., 2007). However, to our knowl-
edge, this is the first time morphological and syn-
tactic information from the Celex database is used
for readability assessment.
Psycholinguistic features: The MRC Psy-
cholinguistic Database (Wilson, 1988) is a freely
available, machine readable dictionary annotated
1
http://celex.mpi.nl/help/elemmas.html
2
http://catalog.ldc.upenn.edu/docs/LDC96L14
with 26 linguistic and psychological attributes of
about 1.5 million words.
3
We used the measures
of word familiarity, concreteness, imageability,
meaningfulness, and age of acquisition from
this database as our features, by encoding their
average values per text.
Kuperman et al. (2012) compiled a freely avail-
able database that includes Age of Acquisition
(AoA) ratings for over 50,000 English words.
4
This database was created through crowd sourcing
and was compared with several other AoA norms,
which are also included in the database. For each
of the five AoA norms, we computed the average
AoA of words per text.
Turning to the final resource used, we included
the average number of senses per word as calcu-
lated using the MIT Java WordNet Interface as a
feature.
5
We excluded auxiliary verbs for this cal-
culation as they tend to have multiple senses that
do not necessarily contribute to reading difficulty.
Combining the four feature groups, we encode
151 features for each text.
3 Document-Level Readability Model
In our first experiment, we tested the document-
level readability model based on the 151 features
using the WeeBit corpus. Under a regression per-
spective on readability, we evaluated the approach
using Pearson Correlation and Root Mean Square
Error (RMSE) in a 10-fold cross-validation set-
ting. We used the SMO Regression implementa-
tion from WEKA (Hall et al., 2009) and achieved a
Pearson correlation of 0.92 and an RMSE of 0.53.
The document-level performance of our 151
feature model is virtually identical to that of the re-
gression model we presented in Vajjala and Meur-
ers (2013). But compared to our previous work,
the Celex and psycholinguistic features we in-
cluded here provide more lexical information that
is meaningful to compute even for the sentence-
level analysis we turn to in the next section.
To be able to compare our document-level
results with other contemporary readability ap-
proaches, we need a common test corpus. Nel-
son et al. (2012) compared several state of the art
readability assessment systems using five test sets
and showed that the systems that went beyond tra-
ditional formulae and wordlists performed better
3
http://www.psych.rl.ac.uk
4
http://crr.ugent.be/archives/806
5
http://projects.csail.mit.edu/jwi
290
on these real-life test sets. We tested our model
on one of the publicly accessible test corpora from
this study, the Common Core Standards Corpus.
Flor et al. (2013) used the same test set to study
a measure of lexical tightness, providing a further
performance reference.
Table 1 compares the performance of our model
to that reported for several commercial (indicated
in italics) and research systems on this test set.
Nelson et al. (2012) used Spearman?s Rank Cor-
relation and Flor et al. (2013) used Pearson Corre-
lation as evaluation metrics. To facilitate compar-
ison, for our approach we provide both measures.
System Spearman Pearson
Our System 0.69 0.61
Nelson et al. (2012):
REAP
6
0.54 ?
ATOS
7
0.59 ?
DRP
8
0.53 ?
Lexile
9
0.50 ?
Reading Maturity
10
0.69 ?
SourceRater
11
0.75 ?
Flor et al. (2013):
Lexical Tightness ? -0.44
Flesch-Kincaid ? 0.49
Text length ? 0.36
Table 1: Performance on CommonCore data
As the table shows, our model is the best non-
commercial system and overall second (tied with
the Reading Maturity system) to SourceRater as
the best performing commercial system on this
test set. These results on an independent test set
confirm the validity of our document-level read-
ability model. With this baseline, we turned to a
sentence-level readability analysis.
4 Sentence-Level Binary Classification
For each of the pairs in the Wiki-SimpleWiki Sen-
tence Aligned Corpus introduced above, we la-
beled the sentence from Wikipedia as hard and
that from Simple English Wikipedia as simple.
The corpus thus consisted of single sentences,
each labeled either simple or hard. On this basis,
we constructed a binary classification model.
6
http://reap.cs.cmu.edu
7
http://renlearn.com/atos
8
http://questarai.com/Products/DRPProgram
9
http://lexile.com
10
http://readingmaturity.com
11
http://naeptba.ets.org/SourceRater3
Our document-level readability model does not
include discourse features, so all 151 features can
also be computed for individual sentences. We
built a binary sentence-level classification model
using WEKA?s Sequential Minimal Optimization
(SMO) for training an SVM in WEKA on the
Wiki-SimpleWiki sentence aligned corpus. The
choice of algorithm was primarily motivated by
the fact that it was shown to be efficient in previ-
ous work on readability classification (Feng, 2010;
Hancke et al., 2012; Falkenjack et al., 2013).
The accuracy of the resulting classifier deter-
mining whether a given sentence is simple or
hard was disappointing, reaching only 66% accu-
racy in a 10-fold cross-validation setting. Exper-
iments with different classification algorithms did
not yield any more promising results. To study
how the classification performance is impacted by
the size of the training data, we experimented with
different sizes, using SMO as the classification al-
gorithm. Figure 1 shows the classification accu-
racy with different training set sizes.
 65
 65.5
 66
 66.5
 67
 67.5
 68
 68.5
 0  10  20  30  40  50  60  70  80  90  100
clas
sific
atio
n ac
cura
cy (
in %
)
% of training data used
Relation between Binary Sentence Classification Accuracy and Training Data size
 
Figure 1: Training size vs. classification accuracy
The graph shows that beyond 10% of the training
data, more training data did not result in signifi-
cant differences in classification accuracy. Even
at 10%, the training set contains around 10k in-
stances per category, so the variability of any of
the patterns distinguished by our features is suffi-
ciently represented.
We also explored whether feature selection
could be useful. A subset of features chosen by re-
moving correlated features using the CfsSubsetE-
val method in WEKA did not improve the results,
yielding an accuracy of 65.8%. A simple base-
line based on the sentence length as single feature
results in an accuracy of 60.5%, underscoring the
291
limited value of the rich feature set in this binary
classification setup.
For the sake of a direct comparison with the
document-level model, we also explored modeling
the task as a regression on a 1?2 scale. In compar-
ison to the document-level model, which as dis-
cussed in section 3 had a correlation of 0.92, the
sentence-level model achieves only a correlation
of 0.4. A direct comparison is also possible when
we train the document-level model as a five-class
classifier with SMO. This model achieved a clas-
sification accuracy of ?90% on the documents,
compared to the 66% accuracy of the sentence-
level model classifying sentences. So under each
of these perspectives, the sentence-level models on
the sentence task are much less successful than the
document-level models on the document task.
But does this indicate that it is not possible to
accurately identify the reading level distinctions
between simplified and unsimplified versions at
the sentence level? Is there not enough informa-
tion available when considering a single sentence?
We hypothesized that the drop in the classi-
fication accuracy instead results from the rela-
tive nature of simplification. For each pair of
the Wiki-SimpleWiki sentence aligned corpus we
used, the Wiki sentence was harder than the Sim-
pleWikipedia sentence. But this does not neces-
sarily mean that each of the Wikipedia sentences
is harder than each of the SimpleWikipedia sen-
tences. The low accuracy of the binary classi-
fier may thus simply result from the inappropriate
assumption of an absolute, binary classification
viewing each of the sentences originating from
SimpleWikipedia as simple and each from the reg-
ular Wiki as hard.
The confusion matrices of the binary classifi-
cation suggests some support for this hypothesis,
as more simple sentences were classified as hard
compared to the other way around. This can result
when a simple sentence is simpler than its hard
version, but could actually be simplified further ?
and as such may still be harder than another un-
simplified sentence. The hypothesis thus amounts
to saying that the two-class classification model
mistakenly turned the relative difference between
the sentence pairs into a global classification of in-
dividual sentences, independent of the pairs they
occur in.
How can we verify this hypothesis? The sen-
tence corpus only provides the relative ranking of
the pairs, but we can try to identify more fine-
grained readability levels for sentences by apply-
ing the five class readability model for documents
that was introduced in section 3.
5 Relative Reading Levels of Sentences
We applied the document-level readability model
to the individual sentences from the Wiki-
SimpleWiki corpus to study which reading levels
are identified by our model. As we are using a re-
gression model, the values sometimes go beyond
the training corpus? scale of 1?5. For ease of com-
parison, we rounded off the reading levels to the
five level scale, i.e., 1 means 1 or below, and 5
means 5 or above. Figure 2 shows the distribution
of Wikipedia and SimpleWikipedia sentences ac-
cording to the predictions of our document-level
readability model trained on the WeeBit corpus.
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 1  1.5  2  2.5  3  3.5  4  4.5  5
Per
cen
tage
 of t
he t
otal
 sen
tenc
es a
t tha
t lev
el
Reading level
Distribution of reading levels of Normal and Simplified Sentences
WikiSimple Wiki
Figure 2: Reading level distribution of the
Wikipedia and SimpleWikipedia sentences
The model determines that a high percentage of
the SimpleWiki sentences belong to lower reading
levels, with over 45% at the lowest reading level;
yet there also are some SimpleWikipedia sen-
tences which are aligned even to the highest read-
ability level. In contrast, the regular Wikipedia
sentences are evenly distributed across all reading
levels.
The distributions identified by the model sup-
port our hypothesis that some Wiki sentences are
simpler than some SimpleWikipedia sentences.
Note that this is fully compatible with the fact that
for each pair of (SimpleWiki,Wiki) sentences in-
cluded in the corpus, the former is higher in read-
ing level than the latter; e.g., just consider two sen-
tence pairs with the levels (1, 2) and (3, 5).
292
5.1 On the discriminating power of the model
Zooming in on the relative reading levels of the
paired unsimplified and simplified sentences, we
wanted to determine for how many sentence pairs
the sentence reading levels determined by our
model are compatible with the pair?s ranking. In
other words, we calculated the percentage of pairs
(S,N) in which the reading level of a simplified
sentence (S) is identified as less than, equal to, or
greater than the unsimplified (normal) version of
the sentence (N ), i.e., S<N , S=N , and S>N .
Where simplification split a sentence into multiple
sentences, we computed S as the average reading
level of the split sentences.
Given the regression model setup, we can con-
sider how big the difference between two reading
levels determined by the model should be in or-
der for us to interpret it as a categorical difference
in reading level. Let us call this discriminating
reading-level difference the d-level. For example,
with d = 0.3, a sentence pair determined to be
at levels (3.4, 3.2) would be considered a case of
S=N , whereas (3.4, 3.7) would be an instance of
S <N . The d-value can be understood as a mea-
sure of how fine-grained the model is in identify-
ing reading-level differences between sentences.
If we consider the percentage of samples identi-
fied as S <=N as an accuracy measure, Figure 3
shows the accuracy for different d-values.
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison of Normal and Simplified Sentences
S<=N
Figure 3: Accurately identified S<=N
We can observe that the percentage of instances
that the model correctly identifies as S <= N
steadily increases from 70% to 90% as d increases.
While the value of d in theory can be anything,
values beyond 1 are uninteresting in the context of
this study. At d = 1, most of the sentence pairs
already belong to S=N , so increasing this further
would defeat the purpose of identifying reading-
level differences. The higher the d-value, the more
of the simplified and unsimplified pairs are lumped
together as indistinguishable.
Spelling out the different cases from Figure 3,
the number of pairs identified correctly, equated,
and misclassified as a function of the d-value is
shown in Figure 4.
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison of Normal and Simplified Sentences
S<NS=NS>N
Figure 4: Correctly (S < N ), equated (S = N ),
and incorrectly (S>N ) identified sentence pairs
At d = 0.4, around 50% of the pairs are cor-
rectly classified, 20% are misclassified, and 30%
equated. At d=0.7, the rate of pairs for which no
distinction can be determined already rises above
50%. For d-values between 0.3 and 0.6, the per-
centage of correctly identified pairs exceeds the
percentage of equated pairs, which in turn exceeds
the percentage of misclassified pairs.
5.2 Influence of reading-level on accuracy
We saw in Figure 2 that the Wikipedia sentences
are uniformly distributed across the reading lev-
els, and for each of these sentences, a human sim-
plified version is included in the corpus. Even
sentences identified by our readability model as
belonging to the lower reading levels thus were
further simplified. This leads us to investigate
whether the reading level of the unsimplified sen-
tence influences the ability of our model to cor-
rectly identify the simplification relationship.
To investigate this, we separately analyzed pairs
where the unsimplified sentences had a higher
reading level and those where it had a lower read-
ing level, taking the middle of the scale (2.5) as the
293
cut-off point. Figure 5 shows the accuracies ob-
tained when distinguishing unsimplified sentences
of two readability levels.
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 hav
ing 
S<=
N
d-value
S<=N vs d, when N >=2.5 and N<2.5
N>=2.5N<2.5
Figure 5: Accuracy (S<=N) for different N types
For the pairs where the reading level of the unsim-
plified version is high, the accuracy of the read-
ability model is high (80?95%). In the other case,
the accuracy drops to 65?75% (for 0.3<= d <=
0.6). Presumably the complex sentences for which
the model performs best offer more syntactic and
lexical material informing the features used.
When we split the graph into the three cases
again (S < N , S = N , S > N ), the pairs with a
high-level unsimplified sentence in Figure 6 fol-
low the overall picture of Figure 4.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison at Higher Values of N
S<NS=NS>N
Figure 6: Results for N>=2.5
On the other hand, the results in Figure 7 for the
pairs with an unsimplified sentence at a low read-
ability level establish that the model essentially is
incapable to identify readability differences.
 10
 20
 30
 40
 50
 60
 70
 0  0.2  0.4  0.6  0.8  1
Per
cen
tage
 of t
he t
otal
 sam
ples
 
d-value
Comparison at Lower Values of N
S<NS=NS>N
Figure 7: Results for N<2.5
The correctly identified S<N and the incorrectly
identified S >N cases mostly overlap, indicating
chance-level performance. Increasing the d-level
only increases the number of equated pairs, with-
out much impact on the number of correctly dis-
tinguished pairs.
In real-world terms, this means that it is diffi-
cult to identify simplifications of an already sim-
ple sentence. While some of this difficulty may
stem from the fact that simple sentences are likely
to be shorter and thus offer less linguistic material
on which an analysis can be based, it also points
to a need for more research on features that can
reliably distinguish lower levels of readability.
Summing up, the experiments discussed in this
section show that a document-level readability
model trained on the WeeBit corpus can provide
insightful perspectives on the nature of simplifica-
tion at the sentence level. The results emphasize
the relative nature of readability and the need for
more features capable of identifying characteris-
tics distinguishing sentences at lower levels.
6 Conclusions
We started with constructing a document-level
readability model and compared its performance
with other readability systems on a standard test
set. Having established the state-of-the-art perfor-
mance of our document-level model, we moved on
to investigate the use of the features and the model
at the sentence level.
In the sentence-level research, we first used the
same feature set to construct a two-class readabil-
ity model on the sentences from the Wikipedia-
SimpleWikipedia sentence aligned corpus. The
294
model only achieved a classification accuracy of
66%. Exploring the causes for this low perfor-
mance, we studied the sentences in the aligned
pairs through the lens of our document-level read-
ability model, the regression model based on the
five level data of the WeeBit corpus. Our ex-
periment identifies most of the Simple Wikipedia
sentences as belonging to the lower levels, with
some sentences also showing up at higher lev-
els. The sentences from the normal Wikipedia,
on the other hand, display a uniform distribution
across all reading levels. A simplified sentence
(S) can thus be at a lower reading level than its
paired unsimplified sentence (N) while also being
at a higher reading level than another unsimplified
sentence. Given this distribution of reading lev-
els, the low performance of the binary classifier
is expected. Instead of an absolute, binary differ-
ence in reading levels that counts each Wikipedia
sentence from the corpus as hard and each Simple
Wikipedia sentence as simple, a relative ranking
of reading levels seems to better suit the data.
Inspecting the relative difference in the read-
ing levels of the aligned unsimplified-simplified
sentence pairs, we characterized the accuracy of
predicting the relative reading level ranking in a
pair correctly depending on the reading-level dif-
ference d required to required to identify a cate-
gorical difference. While the experiments were
performed to verify the hypothesis that simpli-
fication is relative, they also confirm that the
document-level readability model trained on the
WeeBit corpus generalized well to Wikipedia-
SimpleWikipedia as a different, sentence-level
corpus.
The analysis revealed that the accuracy depends
on the initial reading level of the unsimplified
sentence. The model performs very well when
the reading level of the unsimplified sentence is
higher, but the features seem limited in their abil-
ity to pick up on the differences between sentences
at the lowest levels. In future work, we thus in-
tend to add more features identifying differences
between lower levels of readability.
Taking the focus on the relative ranking of
the readability of sentences one step further, we
are currently studying if modeling the readability
problem as preference learning or ordinal regres-
sion will improve the accuracy in predicting the
relation between simplified and unsimplified sen-
tence versions.
Overall, the paper contributes to the state of the
art by providing a methodology to quantitatively
evaluate the degree of simplification performed
by an automatic system. The results can also be
potentially useful in providing assistive feedback
for human writers preparing simplified texts given
specific target user constraints. We plan to explore
the idea of generating simplified text with read-
ability constraints as suggested in Stymne et al.
(2013) for Machine Translation.
Acknowledgements
We thank the anonymous reviewers for their de-
tailed comments. Our research was funded by
the LEAD Graduate School (GSC 1028, http:
//purl.org/lead), a project of the Excellence
Initiative of the German federal and state gov-
ernments, and the European Commission?s 7th
Framework Program under grant agreement num-
ber 238405 (CLARA).
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9.
R. H. Baayen, R. Piepenbrock, and L. Gulikers.
1995. The CELEX lexical databases. CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Nguyen Bach, Qin Gao, Stephan Vogel, and Alex
Waibel. 2011. Tris: A statistical sentence simplifier
with log-linear models and margin-based discrimi-
native training. In Proceedings of 5th International
Joint Conference on Natural Language Processing,
pages 474?482. Asian Federation of Natural Lan-
guage Processing.
Yvonne Canning and John Tait. 1999. Syntactic sim-
plification of newspaper text for aphasic readers. In
Proceedings of SIGIR-99 Workshop on Customised
Information Delivery, pages 6?11.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of the 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 269?270.
CCSSO. 2010. Common core state standards for en-
glish language arts & literacy in history/social stud-
ies, science, and technical subjects. appendix B: Text
exemplars and sample performance tasks. Technical
report, National Governors Association Center for
295
Best Practices, Council of Chief State School Of-
ficers. http://www.corestandards.org/
assets/Appendix_B.pdf.
R. Chandrasekar and B. Srinivas. 1996. Automatic in-
duction of rules for text simplification. Technical
Report IRCS Report 96?30, Upenn, NSF Science
and Technology Center for Research in Cognitive
Science.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the 16th International Con-
ference on Computational Linguistics (COLING),
pages 1041?1044.
William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: A new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 665?669, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Scott A. Crossley, David F. Dufty, Philip M. McCarthy,
and Danielle S. McNamara. 2007. Toward a new
readability: A mixed model approach. In Danielle S.
McNamara and Greg Trafton, editors, Proceedings
of the 29th annual conference of the Cognitive Sci-
ence Society. Cognitive Science Society.
Johan Falkenjack, Katarina Heimann M?uhlenbock, and
Arne J?onsson. 2013. Features indicating readability
in swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA).
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility.
Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and
Sandra M. Aluisio. 2009. Learning when to sim-
plify sentences for natural text simplification. In
Encontro Nacional de Intelig?encia Artificial (ENIA-
2009).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10?
18.
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 1063?
1080, Mumbay, India.
Siddhartha Jonnalagadda and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of The 3rd
International Symposium on Languages in Biology
and Medicine, Jeju Island, South Korea, November
8-10, 2009.
Sigrid Klerke and Anders S?gaard. 2013. Simple,
readable sub-sentences. In Proceedings of the ACL
Student Research Workshop.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Julie Medero and Marie Ostendorf. 2011. Identifying
targets for syntactic simplification. In ISCA Interna-
tional Workshop on Speech and Language Technol-
ogy in Education (SLaTE 2011).
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&W ?10, pages 42?50, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difficulty: Testing their predic-
tive value for grade levels and student performance.
Technical report, The Council of Chief State School
Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text
complexity classifications that are aligned with tar-
geted text complexity standards. Technical Report
RR-10-28, ETS, December.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In In Proceedings of the Lan-
guage Engineering Conference 2002 (LEC 2002).
Advaith Siddharthan. 2004. Syntactic simplification
and text cohesion. Technical Report UCAM-CL-
TR-597, University of Cambridge Computer Labo-
ratory.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language (PROPOR?10).
296
Sara Stymne, J?org Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013).
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL, pages
252?259, Edmonton, Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 163?-173.
Sowmya Vajjala and Detmar Meurers. 2013. On the
applicability of readability models to web texts. In
Proceedings of the Second Workshop on Predicting
and Improving Text Readability for Target Reader
Populations.
M.D. Wilson. 1988. The MRC psycholinguistic
database: Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20(1):6?11.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of ACL
2012.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China.
Sanja
?
Stajner, Biljana Drndarevic, and Horaccio Sag-
gion. 2013. Corpus-based sentence deletion and
split decisions for spanish text simplification. In CI-
CLing 2013: The 14th International Conference on
Intelligent Text Processing and Computational Lin-
guistics.
297
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 163?173,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
On Improving the Accuracy of Readability Classification
using Insights from Second Language Acquisition
Sowmya Vajjala
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
sowmya@sfs.uni-tuebingen.de
Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
dm@sfs.uni-tuebingen.de
Abstract
We investigate the problem of readability as-
sessment using a range of lexical and syntac-
tic features and study their impact on predict-
ing the grade level of texts. As empirical ba-
sis, we combined two web-based text sources,
Weekly Reader and BBC Bitesize, targeting
different age groups, to cover a broad range
of school grades. On the conceptual side, we
explore the use of lexical and syntactic mea-
sures originally designed to measure language
development in the production of second lan-
guage learners. We show that the develop-
mental measures from Second Language Ac-
quisition (SLA) research when combined with
traditional readability features such as word
length and sentence length provide a good
indication of text readability across different
grades. The resulting classifiers significantly
outperform the previous approaches on read-
ability classification, reaching a classification
accuracy of 93.3%.
1 Introduction
Reading plays an important role in the development
of first and second language skills, and it is one of
the most important means of obtaining information
about any subject, in and outside of school. How-
ever, teachers often find it difficult to obtain texts
appropriate to the reading level of their students, on
a given topic. In many cases, they end up modifying
or creating texts, which takes significant time and ef-
fort. In addition to such a traditional school setting,
finding texts at the appropriate reading level is also
important in a wide range of real-life contexts in-
volving people with intellectual disabilities, dyslex-
ics, immigrant populations, and second or foreign
language learners.
Readability-based text classification, when used
as a ranking parameter in a search engine, can help
in retrieving texts that suit a particular target reading
level for a given query topic. In the context of lan-
guage learning, a language aware search engine (Ott
and Meurers, 2010) that includes readability classi-
fication can facilitate the selection of texts from the
web that are appropriate for the students in terms of
form and content. This is one of the main motiva-
tions underlying our research.
Readability assessment has a long history
(DuBay, 2006). Traditionally, only a limited set of
surface features such as word length and sentence
length were considered to derive a formula for read-
ability. More recently, advances in computational
linguistics made it possible to automatically extract
a wider range of language features from text. This
facilitated building machine learning models that es-
timate the reading level of a text. On the other hand,
there has also been an on-going stream of research
on reading and text complexity in other areas such as
Second Language Acquisition (SLA) research and
psycholinguistics.
In SLA research, a range of measures have been
proposed to study the development of complexity
in the language produced by learners. These mea-
sures are used to evaluate the oral or written pro-
duction abilities of language learners. The aim of
readability classification, on the other hand, is to re-
trieve texts to be comprehended by readers at a par-
163
ticular level. Since we want to classify and retrieve
texts for learners of different age groups, we hypoth-
esized that these SLA-based complexity measures of
learner production, when used as features for read-
ability classification will improve the performance
of the classifiers. In this paper, we show that this
approach indeed results in a significant performance
improvement compared to previous research.
We used the WeeklyReader website1 as one of
the text used in previous research. We combined it
with texts crawled from the BBC-Bitesize website2,
which provides texts for a different age group. The
combined corpus, WeeBit, covers a comparatively
larger range of ages than covered before.
To summarize, the contributions of this paper are:
? We adapt measures from second language ac-
quisition research to readability classification
and show that the overall classification accu-
racies of an approach including these features
significantly outperforms previous approaches.
? We extend the most widely used WeeklyReader
corpus by combining it with another corpus that
is graded for a different age-group, thereby cre-
ating a larger and more diverse corpus as basis
for future research.
The paper is organized as follows: Section 2 de-
scribes related work on reading level classification
to put our work in context. Section 3 introduces the
corpora we used. Section 4 describes the features
we considered in detail. Section 5 presents the ap-
proach and discusses the results. Section 6 provides
a summary and points to future work.
2 Related Work
The traditional readability formulae made use of a
limited number of surface features, such as the aver-
age sentence length and the average word length in
characters or syllables (Kincaid et al, 1975; Cole-
man and Liau, 1975). Some works also made use
of lists of ?difficult? words, typically based on fre-
quency counts, to estimate readability of texts (Dale
and Chall, 1948; Chall and Dale, 1995; Stenner,
1http://www.weeklyreader.com
2http://www.bbc.co.uk/bitesize
1996). Dubay (2006) provides a broad survey of tra-
ditional approaches to readability assessment. Al-
though the features considered appear shallow in
terms of linguistic modeling, they have been popular
for many years and are widely used.
More recently, the developments in computational
linguistics made it possible to consider various lex-
ical and syntactic features to automatically model
readability. In some of the early works on statis-
tical readability assessment, Si and Callan (2001)
and Collins-Thompson and Callan (2004) reported
the impact of using unigram language models to es-
timate the grade level of a given text. The models
were built on a United States text book corpus.
Heilman et al (2007; 2008b; 2008a) extended
this approach and worked towards retrieving rele-
vant reading materials for language learners in the
REAP3 project. They extended the above mentioned
approach to include a set of manually and later au-
tomatically extracted grammatical features.
Schwarm and Ostendorf (2005) and Petersen and
Ostendorf (2009) report on classification experi-
ments with WeeklyReader data, considering statisti-
cal language models, traditional formulae, as well as
certain basic parse tree features in building an SVM-
based statistical model. Feng et al (2010) and Feng
(2010) went beyond lexical and syntactic features
and studied the impact of several discourse-based
features, comparing their performance on the Week-
lyReader corpus.
While the vast majority of approaches have tar-
geted English texts, some work on other languages
such as German, Portuguese, French and Italian (vor
der Bru?ck et al, 2008; Aluisio et al, 2010; Fran-
cois and Watrin, 2011; Dell?Orletta et al, 2011) is
starting to emerge. Parse-tree-based features have
also been used to measure the complexity of spoken
Swedish (Roll et al, 2007).
The process of text comprehension and the effect
of factors such as the coherence of texts have also
been intensively studied (e.g., Crossley et al, 2007a;
2007b; Graesser et al, 2004) and measures to ana-
lyze the text under this perspective have been imple-
mented in the CohMetrix project.4
The DARPA Machine Reading program created
3http://reap.cs.cmu.edu
4http://cohmetrix.memphis.edu
164
a corpus of general text readability containing var-
ious forms of human and machine generated texts
(Strassel et al, 2010).5 The aim of this program is to
transform natural language texts into a format suit-
able for automatic processing by machines and to
filter out poorly written documents based on the text
quality. Kate et al (2010) used this data set to build
a coarse grained model of text readability.
While in this paper we focus on comparing com-
putational linguistic approaches to readability as-
sessment and improving the state of the art on a tra-
ditional and available data set, Nelson et al (2012)
compared several research and commercially avail-
able text difficulty assessment systems in support of
the Common Core Standards? goal of providing stu-
dents with texts at the appropriate level of difficulty
throughout their schooling.6
Independent of the research on readability, the
complexity of the texts produced by language learn-
ers has been extensively investigated in Second
Language Acquisition (SLA) research (Housen and
Kuiken, 2009). Recent approaches have automated
and compared a number of such complexity mea-
sures for learner language, specifically in English as
Second Language learner narratives (Lu, 2010; Lu,
2011b). So far, there is hardly any work on using
such insights in computational linguistics, though,
with the notable exception of Chen and Zechner
(2011) using SLA features to evaluate spontaneous
non-native speech. Given that graded corpora are
also intended to be used by incremental age groups,
we started to investigate whether the insights from
SLA research can fruitfully be applied to readability
classification.
3 Corpora
We used a combined corpus of WeeklyReader and
BBC-Bitesize to develop a statistical model that
classifies texts into five grade levels, based on the
age groups.
WeeklyReader7 is an educational newspaper, with
articles targeted at four grade levels (Level 2, Level
3, Level 4, and Senior), corresponding to children
5The corpus is apparently intended to be available for public
use, but does not yet seem to be so; we so far were unsuccessful
in obtaining more information from the authors.
6http://www.corestandards.org
7http://www.weeklyreader.com
between ages 7?8, 8?9, 9?10, and 9?12 years. The
articles cover a wide range of non-fiction topics,
from science to current affairs, written according to
the grade level of the readers. The exact criterion
of graded writing is not published by the magazine.
We obtained permission to use the graded magazine
articles and downloaded the archives in 11/2011.8
Though we used the same WeeklyReader text
base as the previous works, the corpus is not identi-
cal since we downloaded our version more recently.
Thus the archive contained more articles per level
and some preprocessing may differ. The Week-
lyReader magazine issues in addition to the actual
articles include teacher guides, student quizzes, im-
ages and brain teaser games, which we did not in-
clude in the corpus. The distribution of articles after
this preprocessing is shown in Table 1.
Grade Age Number of Avg. Number of
Level in Years Articles Sentences/Article
Level 2 7?8 629 23.41
Level 3 8?9 801 23.28
Level 4 9?10 814 28.12
Senior 10-12 1325 31.21
Table 1: The Weekly Reader corpus
BBC-Bitesize9 is a website with articles classi-
fied into four grade levels (KS1, KS2, KS3 and
GCSE), corresponding to children between ages 5?
7, 8?11, 11?14 and 14?16 years. The Bitesize cor-
pus is freely available on the web, and we crawled it
in 2009. Most of the articles at KS1 consisted of im-
ages and flash files and other audio-visual material,
with little text. Hence, we did not include KS1 in
our corpus. We also excluded pages that contained
only images, audio, or video files without text.
To cover a broad range of non-overlapping age
groups, we used Level 2, Level 3 and Level 4 from
WeeklyReader and KS3 and GCSE from Bitesize
data respectively and built a combined corpus cover-
ing learners aged 7 to 16 years. Note that while KS2
covers the age group of 8?11 years, Levels 2, 3, and
8A license to use the texts on the website for research can be
obtained for a small fee from support@weeklyreader.com. To
support comparable research, we will share the exact corpus we
used with other researchers who have obtained a license to use
the WeeklyReader materials.
9http://www.bbc.co.uk/bitesize
165
4 together cover ages 7?10 years. Similarly, the Se-
nior Level overlaps with Level 4 and KS3. Hence,
we excluded KS2 and Senior from the combined
corpus. We will refer to the combined five-level cor-
pus we created in this way as WeeBit. The distribu-
tion of articles in the combined WeeBit corpus after
preprocessing and removing the overlapping grade
levels, is shown in Table 2.
Grade Age Number of Avg. Number of
Level in Years Articles Sentences/Article
Level 2 7?8 629 23.41
Level 3 8?9 801 23.28
Level 4 9?10 814 28.12
KS3 11?14 644 22.71
GCSE 14?16 3500 27.85
Table 2: The WeeBit corpus
To avoid a classification bias towards a class with
more training examples during, for each level in the
WeeBit corpus, 500 documents were taken as train-
ing set and 125 documents were taken as test set.
In total, we trained on a set of 2500 documents and
used a test set of 625 documents, spanning across
five grade levels.
4 Features
To build our classification models, we combined
features used in previous research with other parse
tree features as well as lexical richness and syntactic
complexity features from SLA research. We group
the features into three broad categories: lexical, syn-
tactic and traditional features.
4.1 Lexical Features
Word n-grams have been frequently used as lexical
features in the previous research (Collins-Thompson
and Callan, 2004; Schwarm and Ostendorf, 2005).10
POS n-grams as well as POS-tag ratio features have
also been used in some of the later works (Feng et
al., 2010; Petersen and Ostendorf, 2009).
In the SLA context, independent of the readability
research, Lu (2011a) studied the relationship of lexi-
cal richness to the quality of English as Second Lan-
guage (ESL) learners? oral narratives and analyzed
10In the readability literature, n-grams are traditionally dis-
cussed as lexical features. N-grams beyond unigrams naturally
also encode aspects of syntax.
the distribution of three dimensions of lexical rich-
ness (lexical density, sophistication and variation) in
them using various metrics proposed in the language
acquisition literature. Those measures were used to
analyze a large scale corpus of Chinese learners of
English. We adapted some of the metrics from this
research as our lexical features:
Type-Token Ratio (TTR) is the ratio of number
of word types (T) to total number word tokens in
a text (N). It has been widely used as a measure
of lexical diversity or lexical variation in language
acquisition studies. However, since it is depen-
dent on the text size, various alternative transfor-
mations of TTR came into existence. We consid-
ered Root TTR (T/
?
N ), Corrected TTR (T/
?
2N ),
Bilogarithmic TTR (Log T/Log N) and Uber Index
(Log2T/Log(N/T )).
Another recent TTR variant we considered, which
is not a part of Lu (2011a), is the Measure of Textual
Lexical Diversity (MTLD; McCarthy and Jarvis,
2010). It is a TTR-based approach that is not af-
fected by text length. It is evaluated sequentially, as
the mean length of string sequences that maintain a
default Type-Token Ratio value. That is, the TTR
is calculated at each word. When the default TTR
value is reached, the MTLD count increases by one
and TTR evaluations are again reset. McCarthy and
Jarvis (2010) considered the default TTR as 0.72 and
we continued with the same default.
Considering nouns, adjectives, non-modal and
non-auxiliary verbs and adverbs as lexical items,
Lu (2011a) studied various syntactic category based
word ratio measures. Lexical variation is defined
as the ratio of the number of lexical types to lexi-
cal tokens. Other variants of lexical variation stud-
ied in Lu (2011a) included noun, adjective, modi-
fier, adverb and verb variations, which represent the
proportion of the words of the respective categories
compared to all lexical words in the document. Al-
ternative measures of verb variation, namely Verb
Variation-1 (Tverb/Nverb), Squared Verb Variation-
1 (T 2verb/Nverb) and Corrected Verb Variation-1
(Tverb/
?
2Nverb) are also studied in the literature.
We considered all these measures of lexical varia-
tion as a part of our lexical features. We have also
included Lexical Density, which is the ratio of the
number of lexical items in relation to the total num-
ber of words in a text.
166
In addition to these measures from the SLA lit-
erature, in our lexical features we included the aver-
age number of syllables per word (NumSyll) and the
average number of characters per word (NumChar),
which are used as word-level indicators of text com-
plexity in various traditional formulae (Kincaid et
al., 1975; Coleman and Liau, 1975).
Finally, we included the proportion of words in
the text which are found on the Academic Word List
as another lexical feature. It refers to the word list
created by Coxhead (2000), which contains a list of
most frequent words found in the academic texts.11
The list does not include the most frequent words in
the English language as such. The words in this list
are specific to academic contexts. It was intended to
be used both by teachers and students as a measure
of vocabulary acquisition. We use it as an additional
lexical feature in our work ? and it turned out to be
one of the most predictive features.
All the lexical features we considered in this work
are listed in Table 3. The SLA based lexical features
are referred to as SLALEX in the table. Of these,
Lexical Features from SLA research (SLALEX)
? Lexical Density (LD)
? Type-Token Ratio (TTR)
? Corrected TTR (CTTR)
? Root TTR (RTTR)
? Bilogarithmic TTR (LogTTR)
? Uber Index (Uber)
? Lexical Word Variation (LV)
? Verb Variation-1 (VV1)
? Squared VV1 (SVV1)
? Corrected VV1 (CVV1)
? Verb Variation 2 (VV2)
? Noun Variation (NV)
? Adjective Variation (AdjV)
? Adverb Variation (AdvV)
? Modifier Variation (ModV)
? Mean Textual Lexical Density (MTLD)
Other Lexical Features
? Proportion of words in AWL (AWL)
? Avg. Num. Characters per word (NumChar)
? Avg. Num. Syllables per word (NumSyll)
Table 3: Lexical Features (LEXFEATURES)
11http://en.wikipedia.org/wiki/Academic_Word_List
six features CTTR, RTTR, SVV1, CVV1, AdvV, ModV
were shown by Lu (2011b) to correlate best with the
learner data. We will refer to them as BESTLEX-
SLA, highlighted in italics in the table.
4.2 Syntactic Features
Schwarm and Ostendorf (2005) implemented four
parse tree features (average parse tree height, aver-
age number of SBARs, NPs per sentence and VPs
per sentence) in their work. Feng (2010) considered
more syntactic features, adding the average lengths
of phrases (NP, VP and PP) per sentence in words
and characters, and the total number of respective
phrases in the document. In our work, we started
with reconsidering the above mentioned syntactic
features.
In addition, we included measures of syntactic
complexity from the SLA literature. Lu (2010) se-
lected 14 measures from a large set of measures used
to monitor the syntactic development in language
learners. He then used these measures in the analysis
of syntactic complexity in second language writing
and showed that some of them correlate well with
the syntactic development of adult Chinese learners
of English. They are grouped into five broad cate-
gories:
The first set consists of three measures of syn-
tactic complexity based on the length of a unit at
the sentential, clausal and T-unit level respectively.
The definitions for sentence, clause and T-unit were
adapted from the SLA literature. While a sentence
is considered to be a group of words delimited with
punctuation mark, a clause is any structure with a
subject and a finite verb. Finally, a T-unit is char-
acterized as one main clause plus any subordinate
clause or non-clausal structure that is attached to or
embedded in it.
The second type of measure targets sentence com-
plexity. Clauses per sentence is considered as a sen-
tence complexity measure.
The third set of measures reflect the amount of
subordination in the sentence. They include clauses
per T-unit, complex T-units per T-unit, dependent
clauses per clause and dependent clauses per T-unit.
A complex T-unit is considered as any T-unit that
contains a dependent clause.
The fourth type of measures measured the amount
of co-ordination in a sentence. They consist of co-
167
ordinate phrases per clause and co-ordinate phases
per T-unit. Any adjective, verb, adverb or noun
phrase that dominates a co-ordinating conjunction is
considered a co-ordinate phrase.
The fifth type of measures represented the rela-
tionship between specific syntactic structures and
larger production units. They include complex nom-
inals per clause, complex nominals per T-unit and
verb phrases per T-unit. Complex nominals are com-
prised of a) nouns plus adjective, possessive, prepo-
sitional phrase, relative clause, participle or appos-
itive, b) nominal clauses, c) gerunds and infinitives
in subject positions.
We implemented these 14 syntactic measures as
features in building our classification models, in ad-
dition to existing features. Eight of these features
(MLC, MLT, CP/C, CP/T, CN/C, CN/T, MLS, VP/T)
were argued to correlate best with language develop-
ment. We refer to this subset of eight as BESTSYN-
SLA, shown in italics in Table 4. We will see in sec-
tion 5 that a set including those features also holds
good predictive power for classifying graded texts.
We also included the number of dependent
clauses, complex T-units, and co-ordinate phrases
per sentence as additional syntactic features. Table 4
summarizes the syntactic features used in this paper.
4.3 ?Traditional? Features
The average number of characters per word (Num-
Char), the average number of syllables per word
(NumSyll), and the average sentence length in words
(MLS) have been used to derive formulae for read-
ability in the past. We refer to them as Traditional
Features below. We included MLS in the syntactic
features and NumChar, and NumSyll in the Lexi-
cal features. We also included two popular readabil-
ity formulae, Flesch-Kincaid score (Kincaid et al,
1975) and Coleman-Liau readability formula (Cole-
man and Liau, 1975), as additional features. The
latter will be referred as Coleman below, and both
formulas together as Traditional Formulae.
5 Experiments and Evaluation
We used the Berkeley Parser (Petrov and Klein,
2007) with the standard model they provide for
building syntactic parse trees and defined the pat-
terns for extracting various syntactic features from
Syntactic features from SLA research (SLASYN)
? Mean length of clause (MLC)
? Mean length of a sentence (MLS)
? Mean length of T-unit (MLT)
? Num. of Clauses per Sentence (C/S)
? Num. of T-Units per sentence (T/S)
? Num. of Clauses per T-unit (C/T)
? Num. of Complex-T-Units per T-unit (CT/T)
? Dependent Clause to Clause Ratio (DC/C)
? Dependent Clause to T-unit Ratio (DC/T)
? Co-ordinate Phrases per Clause (CP/C)
? Co-ordinate Phrases per T-unit (CP/T)
? Complex Nominals per Clause (CN/C)
? Complex Nominals per T-unit (CN/T)
? Verb phrases per T-unit (VP/T)
Other Syntactic features
? Num. NPs per sentence (NumNP)
? Num. VPs per sentence (NumVP)
? Num. PPs per sentence (NumPP))
? Avg. length of a NP (NPSize)
? Avg. length of a VP (VPSize)
? Avg. length of a PP (PPSize)
? Num. Dependent Clauses per sentence (NumDC)
? Num. Complex-T units per sentence (NumCT)
? Num. Co-ordinate Phrases per sentence (CoOrd)
? Num. SBARs per sentence (NumSBAR)
? Avg. Parse Tree Height (TreeHeight)
Table 4: Syntactic features (SYNFEATURES)
the trees using the Tregex pattern matcher (Levy and
Andrew, 2006). More details about the patterns from
the SLA literature and their definitions can be found
in Lu (2010). We used the OpenNLP12 tagger to
get POS tag information and calculate Lexical Rich-
ness features. We used the WEKA (Hall et al, 2009)
toolkit for our classification experiments. We ex-
plored different classification algorithms such as De-
cision Trees, Support Vector Machines, and Logis-
tic Regression. The Multi-Layer Perceptron (MLP)-
classifier performed best with various combinations
of features, so we focus on reporting the results for
that algorithm.
12http://opennlp.apache.org
168
Feature set # Features Classifier Performance
Accuracy RMSE
Traditional Formulae 2 38.8% 0.36
Traditional Features 3 70.3% 0.25
Trad. Features + Trad. formulae 5 72.3% 0.32
SLALEX 16 68.1% 0.29
SLASYN 14 71.2% 0.28
SLALEX + SLASYN 30 82.3% 0.23
BEST10SYN 10 69.9% 0.28
All Syntactic Features 25 75.3% 0.27
BEST10LEX 10 82.4% 0.22
All Lexical Features 19 86.7% 0.20
BEST10ALL 10 89.7% 0.18
All features 46 93.3% 0.15
Table 5: Classification results for WeeBit Corpus
5.1 Evaluation Metrics
We report our results in terms of classification accu-
racy and root mean square error.
Classification accuracy refers to the percentage of
instances in the test set that are classified correctly.
The correct classifications include both true posi-
tives and true negatives. However, accuracy does
not reflect how close the prediction is to the actual
value. A difference between expected and predicted
values of one grade level is treated the same way as
the difference of, e.g., four grade levels.
Root mean square error (RMSE) is a measure
which gives a better picture of this difference.
RMSE is the square root of empirical mean of the
squared prediction errors. It is frequently used as
a measure to estimate the deviation of an observed
value from the expected value. In readability assess-
ment, it can be understood as the average difference
between the predicted grade level and the expected
grade level.
5.2 Feature Combinations
Complementing our experiments comparing the dif-
ferent lexical and syntactic features and their com-
bination, we also used WEKA?s information-gain-
based feature selection algorithm, and selected the
Top-10 best features using the ranker method.
When all features were considered, the top 10
most predictive features were found to be: (Num-
Char, NumSyll, MLS, AWL, ModVar, CoOrd, Cole-
man, DC/C, CN/C,and AdvVar), which are referred
to as BEST10ALL in the table.
Considering the 25 syntactic features alone, the
10 most predictive features were: (MLS, CoOrd,
DC/C, CN/C, CP/C, NumPP, VPSize, C/T, CN/T and
NumVP), referred to as BEST10SYN in the table.
The 10 most predictive features amongst all the
lexical features were: (NumChar, NumSyll, AWL,
ModV, AdvV, AdjV, LV, VV1, NV and SVV1). They
are referred to as BEST10LEX in the table.
Although the traditionally used features (Num-
Char, NumSyll, MLS) seem to be the most predictive,
it can be seen from the other top ranked features,
that there is significant overlap between the best fea-
tures identified by WEKA and the features which
Lu (2010; 2011b) identified as correlating best with
language development (shown in italics in Table 3
and Table 4), which supports our hypothesis that the
SLA-based measures are useful features for read-
ability classification of non-learner text too.
5.3 Results
Table 5 shows the results of our classification ex-
periments using WEKA?s Multi-Layer Perceptron
algorithm with different combinations of features.
Combining all features results in the best accuracy
of 93.3%, which is a large improvement over the
current state of the art in readability classification
reported on the WeeklyReader corpus (74.01% by
Feng et al, 2010). It should, however, be kept
169
# Features Highest reported accuracy
Previous work (on WeeklyReader)
(Feng et al, 2010) 122 74.01%
(Petersen and Ostendorf, 2009) 25 63.18%
Syntactic features only (Petersen and Ostendorf, 2009) 4 50.91%
Our Results (on WeeklyReader alone)
Syntactic features from (Petersen and Ostendorf, 2009) 4 50.68%
All our Syntactic Features 25 64.3%
All our Lexical Features 19 84.1%
All our Features 46 91.3%
Our Results (on WeeBit)
All our Syntactic Features 25 75.3%
All our Lexical Features 19 86.7%
All our Features 46 93.3%
Table 6: Overall Results and Comparison with Previous Work
in mind that the improvement is achieved on the
WeeBit corpus which is an extension of the Week-
lyReader corpus previously used. Interestingly, the
result of 89.7% for BEST10ALL, the top 10 features
chosen by the WEKA ranker, are quite close to our
best result, with a very small number of features.
Lexical features seem to perform better than syn-
tactic features when considered separately. How-
ever, this better performance of lexical features was
mainly due to the addition of the traditionally used
features NumChar and NumSyll. So it is no won-
der that these shallow features have been used in
the traditional readability formulae for such a long
time; but the predictive power of the traditional for-
mulae as features by themselves is poor (38.8%), in
line with the conclusions drawn in previous research
(Schwarm and Ostendorf, 2005; Feng et al, 2010)
about the Flesch-Kincaid and Dale-Chall formulae.
Interestingly, Coleman, which was not considered
in those previous approaches, was ranked among
the Top-10 most predictive features by the WEKA
ranker. So it holds a good predictive power when
used as one of the features for the classifier.
We also studied the impact of SLA based fea-
tures alone on readability classification. The perfor-
mance of the SLA based lexical features (SLALEX)
and syntactic features (SLASYN) when considered
separately are still in a comparable range with the
previously reported results on readability classifi-
cation (68.1% and 71.2% respectively). However,
combining both of them resulted in an accuracy of
82.3%, which is a considerable improvement over
previously reported results. It again adds weight to
the initial hypothesis that SLA based features can be
useful for readability classification.
5.4 Comparison with previous work
Table 6 provides an overall comparison of the accu-
racies obtained for the key features sets in our work
with the best results reported in the literature for the
WeeklyReader corpus. However, since our classi-
fication experiments were carried out with a newly
compiled corpus extending the WeeklyReader data,
such a direct comparison is not particularly mean-
ingful by itself. To address this issue, we explored
two avenues.
Firstly, we ran additional experiments, training
and testing on the WeeklyReader data only, includ-
ing the four levels used in previous work on that cor-
pus. A summary of the results can be seen in Table
6. Our approach with 46 features results in 91.3%
accuracy on the WeeklyReader corpus, compared to
74.01% as the best previous WeeklyReader result,
reported by Feng et al (2010) for their much larger
feature set (122 features).
In order to verify the impact of our choice of fea-
tures, we also did a replication of the parsed syntac-
tic feature measures reported by (Schwarm and Os-
tendorf, 2005) on the WeeklyReader corpus and ob-
tained essentially the same accuracy as the one pub-
170
lished (50.7% vs. 50.91%), supporting the compa-
rability of the WeeklyReader data used. The signif-
icant performance increase we reported thus seems
to be due to the new features we integrated from the
SLA literature.
Secondly, we were interested in the impact of the
training size on the results. We therefore investi-
gated how good our best approach (using all fea-
tures) is on a training corpus that is comparable to
the WeeklyReader corpus used in previous work in
terms of the number of documents per class. When
we took 1400 WeeklyReader documents distributed
into four classes as described in Feng et al (2010),
we obtained an accuracy of 84.2%, compared to the
74.01% they reported as best result. Using 2500
documents distributed into four classes as in Pe-
tersen and Ostendorf (2009) we obtained 88.4%,
compared to their best result of 63.18%. Given that
the original corpora used are not available, these
WeeklyReader corpora with the same source, num-
ber of documents, and size of classes are as close
as we can get to a direct comparison. In the future,
the availability of the WeeBit corpus will support a
more direct comparison of approaches.
In sum, the above experiments seem to indicate
that the set of features and classifier used in our ap-
proach play an important role in the resulting signif-
icant increase in accuracy.
6 Conclusion and Discussion
We created a new corpus, WeeBit, by combining
texts from two graded web sources WeeklyReader
and BBC Bitesize. The resulting text corpus is
larger and covers more grade levels, spanning the
age group between 7 and 16 years. We hope that
the availability of this graded corpus will be useful
as an empirical basis for future studies in automatic
readability assessment.13
We studied the impact of various lexical and syn-
tactic features and explored their performance in
combination with features encoding syntactic com-
plexity and lexical richness that were inspired by
Second Language Acquisition research. Our experi-
ments show that not only the full set of features, but
13As mentioned above, we will make the WeeBit corpus
available to all researchers who have obtained the inexpensive
research license from WeeklyReader.
also specific manually or automatically selected sub-
sets of features provide results significantly improv-
ing on the previously published state of the art in
automatic readability assessment. There also seems
to be a clear correlation between the good predictors
according to SLA research on language learning and
those that performed well in text classification.
Although the exact criteria based on which the
individual corpora (WeeklyReader, BBC-Bitesize)
were created is not known, it is possible that they
were created with the well-known, traditional read-
ability formulae in mind. It would be surprising if
the two corpora, compiled in the US and Britain by
different companies, were created with the same set
of measures in mind, so the WeeBit corpus should
be less affected. Still, it is possible that the rea-
son the traditional features NumChar, NumSyll and
MLS held such a strong predictive power is that
these measures were considered when the texts were
written. But removing these traditional features only
strengthens the role of the other features and thereby
the main point of the paper arguing for the usefuless
of SLA developmental measures for readability clas-
sification.
As a part of our future work, we intend to revisit
and study the impact of further classes of features
employed in psycholinguistics and cognitive sci-
ence research, such as those studied in Coh-Metrix
(Graesser et al, 2004) or in the context of retrieving
texts for specific groups of readers (Feng, 2010).
In terms of our overall application goal, we are
currently studying the ability of the classification
models we built to generalize to web data. We then
plan to add the classification model to a language
aware search engine (Ott and Meurers, 2010). Such
a search engine may then also be able to integrate
user feedback on the readability levels of webpages,
to build a dynamic, online model of readability.
7 Acknowledgements
We thank the anonymous reviewers and workshop
organizers for their feedback on the paper. The re-
search leading to these results has received funding
from the European Commission?s 7th Framework
Program under grant agreement number 238405
(CLARA).14
14http://clara.uib.no
171
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL HLT
2010 Fifth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 1?9, Los
Angeles, California, June.
Jeanne S. Chall and Edgar Dale. 1995. Readability
Revisted: The New Dale-Chall Readability Formula.
Brookline Books.
Maio Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for automated
scoring of spontaneous non-native speech. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 722?731,
Portland, Oregon, June.
Meri Coleman and T.L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Journal
of Applied Psychology, 60:283?284.
Kevyn Collins-Thompson and Jamie Callan. 2004. A
language modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL 2004, Boston,
USA.
Averil Coxhead. 2000. A new academic word list.
Teachers of English to Speakers of Other Languages,
34(2):213?238.
Scott A. Crossley, David F. Dufty, Philip M. McCarthy,
and Danielle S. McNamara. 2007a. Toward a new
readability: A mixed model approach. In Danielle S.
McNamara and Greg Trafton, editors, Proceedings of
the 29th annual conference of the Cognitive Science
Society. Cognitive Science Society.
Scott A. Crossley, Max M. Louwerse, Philip M. Mc-
Carthy, and Danielle S. McNamara. 2007b. A lin-
guistic analysis of simplified and authentic texts. The
Modern Language Journal, 91(1):15?30.
Edgar Dale and Jeanne S. Chall. 1948. A formula for
predicting readability. Educational research bulletin;
organ of the College of Education, 27(1):11?28.
Felice Dell?Orletta, Simonetta Montemagni, and Giulia
Venturi. 2011. Read-it: Assessing readability of ital-
ian texts with a view to text simplification. In Proceed-
ings of the 2nd Workshop on Speech and Language
Processing for Assistive Technologies, pages 73?83.
William H. DuBay. 2006. The Classic Readability Stud-
ies. Impact Information, Costa Mesa, California.
Lijun Feng, Martin Jansche, Matt Huenerfauth, and
Noe?mie Elhadad. 2010. A comparison of features for
automatic readability assessment. In In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Beijing, China.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Thomas Francois and Patrick Watrin. 2011. On the con-
tribution of mwe-based features to a readability for-
mula for french as a foreign language. In Proceedings
of Recent Advances in Natural Language Processing,
pages 441?447.
Arthur C. Graesser, Danielle S. McNamara, Max M.
Louweerse, and Zhiqiang Cai. 2004. Coh-metrix:
Analysis of text on cohesion and language. Behav-
ior Research Methods, Instruments and Computers,
36:193?202.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
The SIGKDD Explorations, 11(1).
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combining
lexical and grammatical features to improve readabil-
ity measures for first and second language texts. In
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (HLT-NAACL?07),
pages 460?467, Rochester, New York.
Michael Heilman, Kevyn Collins-Thompson, and Max-
ine Eskenazi. 2008a. An analysis of statistical mod-
els and features for reading difficulty prediction. In
Proceedings of the 3rd Workshop on Innovative Use of
NLP for Building Educational Applications, Colum-
bus, Ohio.
Michael Heilman, Le Zhao, Juan Pino, and Maxine Eske-
nazi. 2008b. Retrieval of reading materials for vocab-
ulary and reading practice. In Proceedings of the Third
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications (BEA-3) at ACL?08, pages 80?
88, Columbus, Ohio.
Alex Housen and Folkert Kuiken. 2009. Complexity,
accuracy, and fluency in second language acquisition.
Applied Linguistics, 30(4):461?473.
Rohit J. Kate, Xiaoqiang Luo, Siddharth Patwardhan,
Martin Franz, Radu Florian, Raymond J. Mooney,
Salim Roukos, and Chris Welty. 2010. Learning to
predict readability using diverse linguistic features. In
23rd International Conference on Computational Lin-
guistics (COLING 2010).
J. P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, and B. S
Chissom. 1975. Derivation of new readability for-
mulas (Automated Readability Index, Fog Count and
Flesch Reading Ease formula) for Navy enlisted per-
sonnel. Research Branch Report 8-75, Naval Techni-
cal Training Command, Millington, TN.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
172
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2011a. A corpus-based evaluation of syn-
tactic complexity measures as indices of college-level
esl writers? language development. TESOL Quarterly,
45(1):36?62, March.
Xiaofei Lu. 2011b. The relationship of lexical richness
to the quality of esl learners? oral narratives. The Mod-
ern Languages Journal. in press.
Philip McCarthy and Scott Jarvis. 2010. Mtld, vocd-
d, and hd-d: A validation study of sophisticated ap-
proaches to lexical diversity assessment. Behavior Re-
search Methods, 42(2):381?392.
J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difficulty: Testing their predictive
value for grade levels and student performance. Tech-
nical report, The Council of Chief State School Offi-
cers.
Niels Ott and Detmar Meurers. 2010. Information re-
trieval for education: Making search engines language
aware. Themes in Science and Technology Education.
Special issue on computer-aided language analysis,
teaching and learning: Approaches, perspectives and
applications, 3(1?2):9?30.
Sarah E. Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter Speech and Language, 23:86?106.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York, April.
Mikael Roll, Johan Frid, and Merie Horne. 2007. Mea-
suring syntactic complexity in spontaneous spoken
swedish. Language and Speech, 50(2).
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 523?530, Ann Ar-
bor, Michigan.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the 10th Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 574?576. ACM.
A. Jackson Stenner. 1996. Measuring reading compre-
hension with the lexile framework. In Fourth North
American Conference on Adolescent/Adult Literacy.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The darpa machine reading program - encouraging lin-
guistic and reasoning research with a series of read-
ing tasks. In Language Resources and Evaluation
(LREC), Malta.
Tim vor der Bru?ck, Sven Hartrumpf, and Hermann Hel-
big. 2008. A readability checker with supervised
learning using deep syntactic and semantic indicators.
Informatica, 32(4):429?-435.
173
Proceedings of the First Workshop on Multilingual Modeling, pages 18?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The Study of Effect of Length in Morphological Segmentation of
Agglutinative Languages
Loganathan Ramasamy and Zdene?k Z?abokrtsky?
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics, Charles University in Prague
{ramasamy, zabokrtsky}@ufal.mff.cuni.cz
Sowmya Vajjala
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
sowmya@sfs.uni-tuebingen.de
Abstract
Morph length is one of the indicative feature
that helps learning the morphology of lan-
guages, in particular agglutinative languages.
In this paper, we introduce a simple unsu-
pervised model for morphological segmenta-
tion and study how the knowledge of morph
length affect the performance of the seg-
mentation task under the Bayesian frame-
work. The model is based on (Goldwater et
al., 2006) unigram word segmentation model
and assumes a simple prior distribution over
morph length. We experiment this model
on two highly related and agglutinative lan-
guages namely Tamil and Telugu, and com-
pare our results with the state of the art Mor-
fessor system. We show that, knowledge of
morph length has a positive impact and pro-
vides competitive results in terms of overall
performance.
1 Introduction
Most of the NLP tasks require one way or an-
other the handling of morphology. The task be-
comes very crucial when the language in ques-
tion is morphologically rich as is the case in many
Indo-European languages. The application of mor-
phology is evident in applications such as Statis-
tical Machine Translation (SMT) (Lee, 2004), de-
pendency parsing, information retrieval and so on.
Apart from the morphological analysis as in the tra-
ditional linguistic sense, morphological segmenta-
tion is also widely used as an easy alternative to
full fledged morphological analysis. In this paper
we mainly focus on the task of morphological seg-
mentation.
The main task in morphological segmentation is
to segment the given token or wordform into set
of morphs or identifying the location of each mor-
pheme boundary within the token. Morphological
segmentation is most suitable for agglutinative lan-
guages (such as Finnish or Turkish) than fusional
languages (such as Semitic languages).
Though both supervised (Koskenniemi, 1983)
and unsupervised methods (Goldsmith, 2001;
Creutz and Lagus, 2005) are extensively studied for
morphological segmentation, unsupervised tech-
niques have the appeal of application to multilin-
gual data with cost effective manner. Within un-
supervised paradigm, various methods have been
explored. Minimum Description Length (MDL)
(Goldsmith, 2001; Creutz and Lagus, 2005) based
approaches are most popular in which the best seg-
mentation corresponds to the compact represen-
tation of morphology and the resulting lexicon.
(Goldwater et al, 2009; Snyder and Barzilay, 2008)
attempted word segmentation and joint segmenta-
tion of related languages using Bayesian approach.
(Demberg, 2007; Dasgupta and Ng, 2007) applied
various probabilistic measures to discover affixes
of wordforms. (Naradowsky and Goldwater, 2009;
Yarowsky and Wicentowski, 2000) explored ways
to model orthographic rules of wordforms.
In this work, we are mainly going to focus on
Bayesian approach. Bayesian approaches provide
natural way of modeling subjective knowledge as
well as separating problem specific aspects from
general aspects. In the case of agglutinative lan-
18
guages, the number of morphemes in a word as well
as morph length play a major role in morpholog-
ical process. The main rationale for this work is
to study linguistic factors (mainly morph length),
so that language specific priors can be applied over
different languages. This will especially be use-
ful when modeling resource poor languages (RPL)
with little or no data, as well as building resources
for RPL from resource rich languages (RRL).
Towards that objective, our main contribution in
this work is, we introduce a simple unsupervised
segmentation model based on Bayesian approach
and we study the effect of morph length prior for
two agglutinative languages.
2 Previous Work
In this section, we briefly survey earlier works that
utilized the morph length information, then we pro-
vide basis for our unsupervised morphological seg-
mentation model and finally we list some prior
works on morphological analysis/segmentation of
Telugu and Tamil.
Snover (2001) used an exponential like distri-
bution for morph length that decreased over word
length, thus favoring shorter morph lengths. Our
work is directly related to (Creutz, 2003) as it
made use of prior distributions on morph length and
frequency of morphs under maximum a posteriori
(MAP) framework. Gamma distribution was used
as a prior distribution for morph length. The main
difference between (Creutz, 2003) and our work is
that, we are going to experiment different morph
lengths under Bayesian framework.
Naradowsky (2011) introduced an exponential
length penalty to prevent the model from under seg-
mentation results. It also emphasized that avoiding
length penalty seriously affected the model. (Poon
et al , 2009) indirectly specified about the morph
length by restricting the number of morphemes per
word.
In this work, we mainly rely on Goldwater (2009;
2006) which conducted an extensive study on the
application of Bayesian approach to word segmen-
tation in child-directed speech utterances. It in-
cluded both unigram and bigram models (based on
Hierarchical Dirichlet Processes) for word segmen-
tation. Gibbs sampling was used to extract sam-
ples (utterances with word boundaries) from pos-
terior distribution. We apply the unigram model
(Goldwater et al, 2009) to morphological segmen-
tation where the word boundaries in speech utter-
ances correspond to morpheme boundaries in word-
forms.
Before we describe unsupervised morphological
segmentation model, we briefly survey the existing
work on Telugu and Tamil morphological segmen-
tation/analysis.
Rao et al (2011) described in detail, the prepara-
tion of a linguistic database for Telugu morpholog-
ical analysis, compiling 2800 morphological cate-
gories and reported a coverage of 95-97%. They
followed a word and paradigm model, which was
considered to be better suited for agglutinative lan-
guages. The issue of out-of-vocabulary words was
handled better in the rule based approach by (Gana-
pathiraju and Levin, 2006). They describe a rule-
based morphological analyzer TelMore for Telugu
nouns and verbs.
Aksharbharathi et al (2004) describes the devel-
opment of a generic morphological analysis shell
that uses dictionaries along with Finite State Trans-
ducers based feature structures, to perform the mor-
phological analysis of a word. The feature struc-
tures were derived from the standard rules of the
grammar in respective languages. This was tested
with Hindi, Telugu, Tamil and Russian.
Kiranmai et al (2010) describe a supervised
morphological analyzer with support vector ma-
chines.
For Tamil, morphological segmentation is rarely
studied. Most of the work is done for morpholog-
ical analysis of wordforms. Most of the analyz-
ers use rule based approaches. Dhanalakshmi et al
(2009) used sequence labeling approach to morpho-
logical analysis of wordforms.
3 Unsupervised Morphological
Segmentation
Consider a wordform (w) of length n composed of
characters from alphabet LA,
w = c1c2c3...cn
The main objective is to identify the character po-
sitions where morpheme boundaries occur. The
19
model we describe here is similar to the cache
model described in (Goldwater et al, 2006) for
word segmentation. We apply the same model to
identify morpheme boundaries. The model makes
decision at every character position in the wordform
for the entire corpus. The hypothesis probability
that no morpheme boundary at position i in word-
form w is calculated as follows,
P (w?i |h) =
nma + ?P0(ma)
Nm + ?
(1)
ma is a substring or a morph in the wordform
w which contains the character position position i.
nma refers to number of times the morph ma oc-
curs in the history of morph counts Nm. In the case
of having a boundary at position i, we will have
two morphs to consider, one morph (ma) to the left
of position i (including i), and another morph (mb)
starting after i. The probability of having a mor-
pheme boundary at position i is calculated in the
same way as Equation 1, but this time with two
morphs,
P (w+i |h) =
nma + ?P0(ma)
Nm + ?
.
nmb + I(ma == mb) + ?P0(mb)
(Nm + 1) + ?
(2)
I(ma == mb) takes the value 1 if both morphs
are same, otherwise the value is 0. Also note that
the additional 1 (due to previous factor) in the de-
nominator of the second part of the equation. In
both the equations, P0 is a base distribution which
can be utilized to put a bias over certain hypothe-
ses. In our case, the base distribution (P0) mainly
assigns probability distribution over morph length.
Additional linguistic factors can also be modeled
this way. ? is a concentration parameter which can
be used to control P0. Overall, the model (in equa-
tion 1 and 2) uses only unigram morph counts.
Every character position (except the last posi-
tion) in a given word is a potential candidate that
can have a morpheme boundary. To determine
whether they really have morpheme boundary or
not, for every character position i inw, we calculate
hypothesis probabilities b+i (i.e. has a morpheme
boundary) and b?i (has no morpheme boundary).
Having calculated the hypothesis probabilities, we
choose the hypothesis by using a weighted coin flip.
In our problem, we have only two hypotheses: (i) a
morpheme boundary and (ii) no morpheme bound-
ary. If the new hypothesis is different from the char-
acter?s previous status, then appropriate data struc-
tures are updated. This procedure is repeated for
many number of iterations.
3.1 Modeling morpheme length
We encode our beliefs about morph length via
base distribution P0. We chose Poisson distribu-
tion for modeling the length of the morphs. Pois-
son distribution utilizing morph length is defined as
P (l, k) = l
ke?l
k! , where l is an expected length of
the morph and when supplied k, it returns the prob-
ability density of a morph having length k. We de-
fine two base distributions based on morph length
prior,
PA0 (m) = p(l, k)
=
lke?l
k!
(3)
PB0 (m) = p(m)p(l, k)
=
nm
| lm |
lke?l
k!
(4)
p(m) is probability of the morph itself. | lm | -
total number of substrings of length equal to the
length of morph m. Morfessor (Creutz and Lagus,
2005) uses Zipfian distribution for frequencies and
gamma length prior for modeling the length of the
morphs. Setting a particular expected morph length
effectively puts a bias towards that particular morph
length (l). We experiment both our base distribu-
tions over different morph lengths.
3.2 Inferencing
Gibbs sampling (Gilks et al, 1996) uses iterative
procedure to repeatedly draw value of a variable
given the current state of all other variables in the
model. In our case, drawing a value is equal to
determining whether there is a boundary at the
character position, thus obtaining individual mor-
phemes. We iteratively segment the given corpus or
list of words into morphological segments. The in-
tuitive idea is that, when we sample enough number
of times i.e. drawing morphological segments of
words given history of segments of all other words,
20
the sampler converges to the posterior distribution
of the morphological segments of the entire corpus.
The Algorithm 1 gives a general outline of how the
Gibbs sampling procedure is applied to morpholog-
ical segmentation.
Algorithm 1: Basic Sampling Procedure
Data: words, model
Result: Segmented words
begin
RandSeg ?? InitializeSegments(words)
Baseline?? Evaluate(RandSeg)
CurrSeg ?? RandSeg
MorphCounts?? GetCounts(CurrSeg)
for i ? iterations do
for j ? size(words) do
for k ? length(words[j]) do
b?k ?? Calculate(P (words[j]
?
k ))
b+k ?? Calculate(P (words[j]
+
k ))
if HasNoBoundaryAt(k) then
add boundary at k with
probability
b+k
b?k +b
+
k
no change at k with probability
b?k
b?k +b
+
k
if HasBoundaryAt(k) then
remove boundary at k with
probability
b?k
b?k +b
+
k
no change at k with probability
b+k
b?k +b
+
k
UpdateCurrSeg(CurrSeg)
AdjustMorphCounts(MorphCounts)
We use temperature (T) settings (not shown in
the algorithm) to make the sampling procedure con-
verge faster. We use 10 values (from 0.1 to 1.0) for
T and raise the probability values of hypotheses to
( 1T ). Also, we make the collection rate very small,
so that only few and substantially different samples
(or morphological segmentation of the entire cor-
pus) are collected.
4 Experimental Setup
The experiments are carried out for the unigram
segmentation model (unsup-uni) as described in
Section 3 and Morfessor system (Creutz and Lagus,
2005). For both Tamil and Telugu, we perform the
following experiments: (i) baseline (ii) unsup-uni
with base distribution PA0 (unsup-uni-p0-len) (iii)
unsup-uni with base distribution PB0 (unsup-uni-
p0-lex-len) and (iv) with Morfessor. For each sys-
tem, we add some knowledge about morph length
(l) and report the accuracy.
The experiments (ii), (iii) and (iv) use additional
dataset known as extra-data. Extra-data is an unan-
notated/unsegmented data which augments the test
data while training the systems. As test data with
gold segmentation is very small, we feel this step is
necessary to make the evaluation credible. The fol-
lowing subsection describes the datasets in detail.
Baseline system corresponds to random segmen-
tation. We evaluate baseline system for morph
lengths 1 to 10. For each morph length (l) experi-
ment, we change the probability of adding a bound-
ary at each character position to be (1l ) except at
l = 1 where the probability is 0.75.
Unsup-uni-p0-len experiment uses base distribu-
tion PA0 (see Section 3.1). We conduct this experi-
ment in 2 steps: (i) running the Gibbs sampler with
the extra-data and (ii) use the parameters (includ-
ing morph counts) from step (i) and run the Gibbs
sampler on test data. We set the expected morph
length (l) in the base distribution PA0 every time we
run the experiment for different morph length. For
the step (i), the Gibbs sampler is run for 10000 iter-
ations with different concentration parameter (?).
We collect samples every 1000 iterations and we
store the last sample as our model along with other
parameters. For step (ii), we use the model from
step (i) and run the Gibbs sampler on test data. We
collect the final sample as our predicted segmen-
tation of the test data and perform evaluation on
the predicted segmentation. In unsup-uni-p0-lex-
len experiment, we use the base distribution PB0
(see Section 3.1). PB0 includes morpheme proba-
bility apart from the length prior. Experiments for
unsup-uni-p0-lex-len is carried out in the same way
as that of unsup-uni-p0-len.
We use gamma distribution length prior for ex-
periments with Morfessor. We train Morfessor on
extra-data for morph lengths 1 to 10. We change
the expected length in the gamma prior for each
morph length experiment. Then we run the Mor-
fessor on test data with same parameters created
during the training.
We use Precision (P), Recall (R) and F-score (F)
21
Lang. Words Chars Morphs Avg. m.(l)
Tamil 1500 12642 3280 3.85
Telugu 998 10303 1733 5.95
Table 1: Gold segmentation: statistics
for evaluating our predicted segmentation with gold
segmentation. Our evaluation is same as (Creutz
and Linde?n, 2004).
4.1 Data
We use EMILLE corpus (Xiao et al , 2004) for
our experiments. The EMILLE corpus contains
monolingual, parallel and annotated data for var-
ious Indian languages. We randomly selected ar-
ticles from monolingual section of Tamil and Tel-
ugu data. The original data were in utf-8 and
we transliterated the data into latin format. The
transliteration step is an important step as it avoids
confusion in specifying morph length (l). As we
already mentioned earlier, we use two sets (extra-
data and test data) of data for each language. For
training of extra-data, we use 30000 unique words
list for each language. For test data, we make words
list from real sentences thus it can contain multi-
ple occurrences of a same wordform. The Table 1
provides the statistics of the test data for which we
have manually performed gold segmentations. At
present, our gold segmentation does not take into
account multiple possible segmentations.
The Figure 1 shows morph counts distribution
of both Tamil and Telugu (derived from gold seg-
ments) according to their morph lengths. Tamil has
more morphs that are shorter in length than Telugu.
5 Results
The Table 2 shows evaluation results for the exper-
imental setup described in the previous section.
For Tamil, most of the morphs have the length 1-
4. The models unsup-uni-p0-len and unsup-uni-p0-
lex-len perform quite well near to that length range.
For the same range (l = 1 to 4), both the models
together perform better than Morfessor in terms of
F-score. The performance of unsup-uni-p0-len and
unsup-uni-p0-lex-len are constantly decreasing and
start to perform worse than Morfessor after length
5. This is somewhat expected that unsup-uni mod-
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 18
Tamil
morph length (l)
mo
rph c
ount
0
200
400
600
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
Telugu
morph length (l)
mo
rph c
ount
0
100
200
300
Figure 1: Morph counts according to morph length (l)
els are quite sensitive to length priors and may per-
form poorly if we assume morph lengths far from
the true range. Whereas, Morfessor has a consis-
tent performance over the entire length range (l = 1
to 10). This implies that, Morfessor is less sensitive
to length priors even if we drastically change the
expected morph length. Unsup-uni-p0-len gave the
best overall performance (F-score - 48.83%) com-
pared to other models in this task.
Telugu?s common morph length ranges from 2-
8. Except at l = 1 & 2, Morfessor beats both
unsup-uni-p0-len and unsup-uni-p0-lex-len in all
other remaining length ranges. Unsup-uni models
perform quite poorly over different length ranges
when comparing with Tamil for the same range. In
this task, Morfessor?s overall performance (F-score
43.63%) is better than unsup-uni models. Mor-
fessor also performs better near the most frequent
morph length range (5-8).
6 Some Observations on (l)
? The results (Table 2) suggest that unsup-uni
model is quite sensitive to morph length pa-
rameter in the prior distributions.
? For Tamil, unsup-uni model performs well
near to the true morph length range. But the
performance deteriorates when the expected
morph length parameter is too different from
22
Language System P/R/F
Morph length (l)
1 2 3 4 5 6 7 8 9 10
Tamil
baseline
P 15.79 15.86 17.04 17.11 15.33 16.33 15.98 14.75 17.63 16.65
R 73.98 50.08 34.92 26.25 19.64 15.50 13.82 11.47 12.31 10.24
F 26.02 24.09 22.91 20.72 17.22 15.91 14.82 12.91 14.50 12.68
unsup-uni-p0-len
P 63.61 62.17 67.99 69.68 69.22 72.77 72.29 68.70 66.73 64.08
R 39.62 40.01 36.49 33.18 28.82 26.47 24.23 22.10 20.65 20.76
F 48.83 48.69 47.49 44.96 40.7 38.82 36.30 33.45 31.54 31.36
unsup-uni-p0-lex-len
P 46.51 59.48 63.79 63.69 56.10 54.58 50.29 48.18 45.99 50.39
R 41.35 41.07 39.34 38.28 36.04 33.69 34.25 34.08 33.02 28.65
F 43.78 48.59 48.67 47.82 43.88 41.66 40.75 39.92 38.44 36.53
Morfessor
P 48.54 48.32 48.61 49.01 50.24 49.07 49.93 49.21 49.42 48.93
R 41.75 40.18 40.07 40.24 40.46 39.84 40.35 39.84 40.40 39.62
F 44.89 43.87 43.93 44.19 44.82 43.98 44.63 44.03 44.64 43.78
Telugu
baseline
P 07.88 08.05 07.91 07.38 07.70 07.54 07.62 08.52 08.96 07.91
R 75.69 51.59 32.97 23.86 20.00 16.00 13.66 13.38 12.97 10.07
F 14.28 13.93 12.76 11.27 11.12 10.25 09.78 10.41 10.60 10.07
unsup-uni-p0-len
P 36.67 37.29 36.2 39.71 41.87 40.58 41.34 39.15 38.10 33.65
R 53.10 51.17 48.14 38.07 29.1 19.31 16.14 11.45 11.03 9.66
F 43.38 43.14 41.33 38.87 34.34 26.17 23.21 17.72 17.11 15.01
unsup-uni-p0-lex-len
P 22.27 26.55 32.46 35.76 28.29 19.31 19.83 18.3 18.17 17.26
R 66.9 58.34 44.41 35.17 35.31 55.17 42.21 49.79 55.45 52.28
F 33.41 36.5 37.51 35.47 31.41 28.6 26.98 26.76 27.37 25.95
Morfessor
P 29.32 29.59 30.48 30.72 30.88 30.85 31.31 30.34 29.88 30.40
R 70.30 69.48 69.48 69.75 70.17 70.30 71.96 70.99 70.58 71.96
F 41.38 41.50 42.38 42.65 42.89 42.88 43.63 42.51 41.99 42.74
Table 2: Results for Tamil and Telugu
the true frequent morph length range.
? However for Telugu, morph length parameter
did not improve the results at the most frequent
morph length range (5-8).
? Concentration parameter (?) too influences
the effect of base distribution as a whole, but
at present, our study does not take into account
?. For small ? values, the base distribution
will not have much effect.
7 Conclusion
In this paper, we mainly studied the effect of knowl-
edge of morph length that could have on the ac-
curacy of morphological segmentation of aggluti-
native languages. Towards that goal, we intro-
duced a simple unsupervised morphological seg-
mentation model based on Bayesian approach that
utilized prior distribution over morph length. The
results showed that the knowledge of length cer-
tainly has a positive impact on the accuracy. Also,
the model provided competitive results in general
and achieved best overall performance (F-score:
48.83%) for Tamil against Morfessor. As a future
work, it would be interesting to see the model and
priors that handle sandhi changes.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Commission?s
7th Framework Program (FP7) under grant agree-
ment n? 238405 (CLARA). We would like to thank
David Marec?ek for useful suggestions about theory
and implementation of the system. We also would
like to thank anonymous reviewers for their useful
comments.
References
Akshar Bharathi, Rajeev Sangal, Dipti M Sharma and
Radhika Mamidi. 2004. Generic Morphological
Analysis Shell. In Proceedings of LREC 2004.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 737?745. 2008.
David Yarowsky and Richard Wicentowski. Minimally
Supervised Morphological Analysis by Multimodal
Alignment. In Proceedings of the 38th Annual Meet-
ing on Association for Computational Linguistics
(ACL), 2000.
Dhanalakshmi V, AnandKumar M, Rekha RU and Ra-
jendran S. 2009. Morphological Analyzer for Ag-
23
glutinative Languages Using Machine Learning Ap-
proaches. In Advances in Recent Technologies in
Communication and Computing, 2009, ARTCom?09,
2009.
Hoifung Poon, Colin Cherry and Kristina Toutanova.
2009. Unsupervised Morphological Segmentation
with Log-Linear Models. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the ACL
(NAACL-HLT), pages 209?217, Boulder, Colorado,
June 2009.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving Morphology Induction by Learning Spelling
Rules. In Proceedings of 21st International Joint
Conference on Artificial Intelligence (IJCAI), 2009.
Jason Naradowsky and Kristina Toutanova. 2011. Un-
supervised Bilingual Morpheme Segmentation and
Alignment with Context-rich Hidden Semi-Markov
Models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 895?904,
June, 2011.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2): pages 153?198, 2001.
Kimmo Koskenniemi. 1983. Two-level morphol-
ogy: A general computational model for word-form
recognition and production. Publication 11, Univer-
sity of Helsinki, Department of General Linguistics,
Helsinki. 1983.
Madhavi Ganapathiraju and Lori Levin. 2006. Tel-
More: Morphological Generator for Telugu Nouns
and Verbs. In Proceedings of the Second Interna-
tional Conference on Digital Libraries. 2006.
Mathias Creutz. 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 280?287, July 2003.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
Morpheme Segmentation and Morphology Induction
from Text Corpora Using Morfessor 1.0. In Publica-
tions in Computer and Information Science, Report
A81, Helsinki University of Technology, 2005.
Mathias Creutz and Krister Linde?n. 2004. Morpheme
Segmentation Gold Standards for Finnish and En-
glish. Publications in Computer and Information Sci-
ence, Report A77, Helsinki University of Technology,
October, 2004.
Matthew G. Snover and Michael R. Brent. 2001. A
Bayesian model for morpheme and paradigm identi-
fication. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics (ACL),
pages 490?498, 2001.
Sai Kiranmai G., K. Mallika, M. Anand Kumar, V.
Dhanalakshmi and K. P. Soman. 2010. Morpho-
logical Analyzer for Telugu using support vector ma-
chines. In Proceedings of ICT 2010.
Sajib Dasgupta and Vincent Ng. 2007. High-
Performance, Language-Independent Morphological
Segmentation. In Proceedings of NAACL HLT 2007,
pages 155?163, 2007.
Sharon Goldwater, Thomas L. Griffiths and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2006.
Sharon Goldwater, Thomas L. Griffiths and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112 (1), pp. 21?54, 2009.
Uma Maheshwar Rao G., Amba Kulkarni P. and Christo-
pher Mala. 2011. A Telugu Morphological Analyzer.
International Telugu Internet Conference Proceed-
ings, Milpitas, California, USA, 28th - 30th Septem-
ber, 2011
Vera Demberg. 2007. A Language-Independent Unsu-
pervised Model for Morphological Segmentation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
920?927, Prague, Czech Republic, June 2007.
Walter R. Gilks, Sylvia Richardson and David Spiegel-
halter. 1996. Markov Chain Monte Carlo in Practice.
Chapman and Hall. 1996.
Xiao Z., McEnery A., Baker P. and Hardie A. 2004.
Developing Asian language corpora: standards and
practice. In Proceedings of the Fourth Workshop on
Asian Language Resources, pp. 1?8, 2004.
Young-Suk Lee. 2004. Morphological Analysis for Sta-
tistical Machine Translation. In Proceedings of the
HLT-NAACL 2004, pp. 57?60, Boston, USA, 2004.
24
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 63?72,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Role of Morpho-Syntactic Features in Estonian Proficiency Classification
Sowmya Vajjala
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
sowmya@sfs.uni-tuebingen.de
Kaidi Lo?o
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
kaidi.loo@student.uni-tuebingen.de
Abstract
We developed an approach to predict the pro-
ficiency level of Estonian language learners
based on the CEFR guidelines. We performed
learner classification by studying morpho-
syntactic variation and lexical richness in texts
produced by learners of Estonian as a sec-
ond language. We show that our features
which exploit the rich morphology of Esto-
nian by focusing on the nominal case and ver-
bal mood are useful predictors for this task.
We also show that re-formulating the classifi-
cation problem as a multi-stage cascaded clas-
sification improves the classification accuracy.
Finally, we also studied the effect of training
data size on classification accuracy and found
that more training data is beneficial in only
some of the cases.
1 Introduction and Motivation
Every year, language learners across the world learn
various languages and take tests that measure their
proficiency level. The Estonian language profi-
ciency examination1 in particular is usually taken
by the immigrant population for citizenship and/or
employment needs in Estonia. Assessing learner
texts to classify them into relevant proficiency lev-
els is usually done by human evaluators and is of-
ten a time consuming process. An approach to au-
tomate this process would complement the human
annotators and reduce the overall effort in evaluat-
ing learner texts for their proficiency. Investigat-
ing features that follow any sort of trend across the
1http://www.ekk.edu.ee/
various proficiency levels among learners is a first
step in building such automatic proficiency classifi-
cation systems. This is the main motivation for our
research.
Several factors might play a role in determining a
learner?s proficiency in a given language. Since we
study the learner corpus of Estonian, a morphologi-
cally complex language with an elaborate declension
and conjugation system, we hypothesized that study-
ing the role of morpho-syntactic features would be a
good starting point to perform proficiency classifi-
cation. We used the Estonian Interlanguage Corpus
(EIC)2, a publicly accessible corpus of written texts
produced by learners of Estonian as a second lan-
guage, for this purpose. All the texts were annotated
with a proficiency level that is based on the Com-
mon European Framework of Reference for Lan-
guages Council of Europe (CEFR). We constructed
various proficiency classification models based on
this corpus by using features motivated primarily by
the morphological complexity of Estonian and found
that true to our hypothesis, they turn out to be good
predictors of the proficiency level.
We also studied the effect of breaking up the
main classification task into sub-tasks and cascad-
ing them. We show that this approach increases the
overall accuracy of proficiency classification. In ad-
dition, we studied the effect of training data size and
found that it does not have a significant impact in
most of the classification tasks we performed. To
summarize, we studied the task of proficiency clas-
sification for Estonian by studying both the aspects
feature engineering and model construction.
2http://evkk.tlu.ee/wwwdata/what_is_evk
63
The rest of this paper is organized as follows: Sec-
tion 2 briefly surveys related work and explains the
context of our research. Section 3 describes our cor-
pus and the experimental setup. Section 4 describes
our feature set. Section 5 describes our experiments
and results. Section 6 concludes the paper with a
discussion on results and directions for future work.
2 Related Work
With the availability of computer based learner cor-
pora, research focusing on studying the criterial fea-
tures that correlate with proficiency levels began to
emerge. A wide body of research exists on studying
the syntactic complexity of texts produced by learn-
ers across different proficiency levels, their lexical
richness and the errors they make (e.g., Lu, 2012;
Vyatkina, 2012; Tono, 2000) . Learner data from
both longitudinal and cross sectional studies was an-
alyzed to understand the linguistic patterns among
learners of different proficiency levels, in Second
Language Acquisition (SLA) research.
Automatic proficiency assessment of learner texts
is another active area of related research, which
plays an important role in language testing. Auto-
mated systems are now being used both for evalua-
tion of language learners and for offering feedback
on their language proficiency (e.g., Williamson,
2009; Burstein et al, 2003 ). Forms of text used for
assessment include mathematical responses, short
answers, essays and spoken responses among oth-
ers (Williamson et al, 2010). Standardized tests like
GRE and GMAT too use such systems to comple-
ment human scorers while evaluating student essays
automatically (Burstein, 2003; Rudner et al, 2005).
Zhang (2008) discusses proficiency classification for
the Examination for the Certificate of Proficiency
in English (ECPE) in detail, by comparing proce-
dures based on four types of measurement models.
The problem of automatic student classification i.e.,
making inferences about a student?s skill level by us-
ing some form of data about them is an active area
of research in Educational data mining (e.g., Des-
marais and Baker, 2012; Baker 2010).
But, automatic approaches for classifying lan-
guage learners into standardized proficiency levels
(e.g., the European CEFR levels3, Common Core
3http://www.coe.int/t/dg4/linguistic/
Standards4) is a relatively new area of interest.
Supnithi et al (2003) used a dataset consisting of
audio transcripts by Japanese learners of English to
build a proficiency classification model with a fea-
ture set that modeled vocabulary, grammatical accu-
racy and fluency. This dataset had 10 levels of pro-
ficiency. Hasan and Khaing (2008) performed profi-
ciency classification with the same dataset using er-
ror rate and fluency features. Dickinson et al (2012)
developed a system for classifying Hebrew learners
into five proficiency levels, using features that focus
on the nature of errors in a corpus of scrambled sen-
tence exercise questions.
Proficiency Classification so far has been predom-
inantly focused on the correlation of error-rate with
proficiency. Although error-rate is a strong indicator
of a learner?s proficiency in a language, consider-
ing other factors like lexical indices or syntactic and
morphological complexity would help in providing
multiple views about the same data. Providing a
non-error driven model, Crossley et al (2011) stud-
ied the impact of various lexical indices in predicting
the learner proficiency level. Using a corpus of 100
writing samples by L2 learners of English classified
in to three levels (beginner, intermediate, advanced),
they built a classification system that analyses lan-
guage proficiency using the Coh-metrix5 lexical in-
dices.
Most of the research about the distinguishing fac-
tors among learners of various proficiency levels has
focused on English. However, issues like morpho-
logical variation, which may not be strong predic-
tors of learner proficiency in English, could be use-
ful in proficiency classification of other languages.
Hence, in this paper, we study the texts produced by
the learners of a morphologically rich and complex
language, Estonian and show that morphology can
be a good predictor for learner proficiency classifi-
cation.
We build our classification models using the Es-
tonian Interlanguage Corpus (EIC), which contains
texts produced by learners of Estonian as a second
language. We modeled our approach based on the
features motivated by the morphological complex-
ity of Estonian. To our knowledge, this is the first
Cadre1_en.asp
4http://www.corestandards.org/
5http://cohmetrix.memphis.edu
64
work that studies the role of morphology based fea-
tures for proficiency classification in general and in
Estonian in particular.
3 Corpus and Experimental Setup
3.1 Corpus
The Estonian Interlanguage Corpus (EIC)6 was cre-
ated by the Talinn University. It is a collection of
written texts produced by learners of Estonian as a
second language. Most of the learners were native
speakers of Russian. The corpus consists mainly of
short essays, answers to questions, translations and
personal letters. The texts are annotated with error
types and incorrect forms. The corpus also provides
information about the learner?s age, gender, educa-
tion and about other languages known to the learner.
Descriptive statistics about the corpus are available
on their website7. The corpus contains around 8000
documents (two million words), most of which are
texts from the Estonian language proficiency exam-
ination. The length of the texts varies in general be-
tween 50 and 1000 words (Eslon, 2007).
Information about the learner?s level of compe-
tence is based on the CEFR guidelines8 and is de-
cided by human annotator judgement. Until late
2008, Estonian language proficiency was tested by
conducting proficiency exams at three levels - the
lowest level A, the medium level B and the highest
level C. Later, the CEFR standards were adapted, di-
viding the development of language proficiency into
six levels (A1, A2, B1, B2, C1, C2). A1 indicates a
basic proficiency and C2 indicates a mastery.
For our current work, we use a sub-corpus con-
sisting of 2000 texts that can be accessibly through
the EIC. These texts are spread across three broad
levels A, B, C instead of the more fine grained six
levels and contain all kinds of texts including short
answers. Although these texts also have an an-
notated version containing information about error-
types and corrections, since our aim in this paper is
to study the effect of morpho-syntactic features, we
considered the raw texts produced by the learners as
6http://evkk.tlu.ee/
7http://evkk.tlu.ee/statistics.html
8http://en.wikipedia.org/wiki/Common_
European_Framework_of_Reference_for_
Languages
they were, without looking at the error annotations.
Table 1 shows a summary of the entire corpus that
was made available.
We prepared a test set consisting of 50 documents
from each category, picked randomly. This test set
was not used to train any of the classifiers we used
in this paper. Further, to avoid a training bias to-
wards any class, we used equal number of instances
from all classes during all our binary and three-class
training processes.
Proficiency Level #Docs Avg. #tokens
A-level 807 182.9
B-level 876 260.3
C-level 307 431.8
Table 1: The EIC Corpus
3.2 Pre-processing
All the texts in our corpus were POS-tagged with the
TreeTagger9 and the tagged output was then used
to extract the required features. The TreeTagger
(Schmid, 1994) is a probabilistic part of speech tag-
ger, which contains parameter files to tag Estonian
data. The tag set was derived from the Tartu Mor-
phologically Disambiguated Corpus tag set10. As
mentioned earlier, we do not use the error annotation
information for these learner texts, in this paper.
4 Features
Our choice of features were primarily motivated by
the nature of the morphology of Estonian.
4.1 The Estonian Language
The Estonian language has about one million native
speakers. It belongs to the Finnic branch of Uralic
languages and is known for it?s complex morphol-
ogy. It is both an agglutinative and a flectional (fu-
sional) language. Some of the prominent features of
Estonian language include:
? 14 productive nominal cases
9http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
10http://www.cl.ut.ee/korpused/
morfkorpus/
65
? no grammatical gender (either of nouns or per-
sonal pronouns) and no articles (either definite
or indefinite)
? the verbal system lacks a morphological future
tense (the present tense is used instead)
? relatively free word order (relations between
words are expressed by case endings)
? extensive compound word formation
? impersonal voice (specific to the Finnic lan-
guages and similar to passive voice. The verb
is conjugated in ?fourth person?, who is never
mentioned)
? Most of the inflected words in Estonian have
two distinctive parts: the stem and the forma-
tive. For example, raamatutele (book, plural,
allative) consists of the stem raamatu and the
formative tele, which in turn consists of plural
marker te and allative case marker le (Erelt et
al., 2007, p. 203).
? Unlike most of other Finnic languages, Esto-
nian also has flective features, i.e., the same
morpheme may have different shapes in differ-
ent word forms. For example, the stem jalg
(?foot?, singular, nominative) may appear as
jala (singular, genitive) or jalga (singular, par-
titive) and plural marker may appear as d, de,
te or i or merged with the stem as in jalad
(plural, nominative), jalgade (plural, genitive)
and jalgu (plural, partitive) (Erelt et al, 2007,
p. 203).
As many of these characteristics are morpholog-
ical in nature, we hypothesized that this morpho-
logical complexity of Estonian may play a role in
the process of language learning and hence may
be a useful predictor for proficiency classification.
Hence, we built our feature set primarily focusing
on the morphological properties of the learner texts.
Apart from these features, we also included other
features based on the Parts of Speech and lexical
variation.
4.2 Morphological Features
In Estonian, as in other Finnic languages, nomi-
nals (nouns, adjectives, numerals and pronouns) and
verbs are inflected for number and case. Estonian
nominals are inflected in 14 different cases. Three of
the nominal cases are grammatical cases, i.e., nom-
inative, genitive and partitive. They fulfill mainly
a syntactic purpose and have a very general gram-
matical meaning. All the other cases are semantic
cases, and they have a more concrete meaning than
grammatical cases, which often can be explained by
means of adverbs or adpositions (Erelt et al, 2007,
p. 241). We considered the proportion of nouns and
adjectives tagged with various cases per document
and included them as our declension features. The
cases we considered in this paper are: nominative,
genitive, partitive, illative, inessive, elative, allative,
adessive, ablative, translative, terminative, essive,
abessive, comitative and short singular illative, i.e.,
aditive case.
The verb in Estonian has finite forms that occur
as predicates and auxiliary components of complex
predicates and non-finite forms. Finite forms are in-
flected for mood, tense, voice, aspect, person and
number. The verb has altogether five moods: the in-
dicative, conditional, imperative, quotative and jus-
sive. It has two simple tenses: the present and the
past, two voices: personal and impersonal, affirma-
tion and negation. Non-finite forms behave differ-
ently. Participles are inflected for voice and tense,
present participles also for case and number, and
supines for voice and case. There is one infinitive
and one gerund, which can be explained as the ines-
sive case form of the infinitve (Erelt, 2003, p. 52). In
this paper, we considered the proportion of verbs be-
longing to various tense, mood, voice, number and
person categories as our features.11.
4.3 POS features
We included the various degrees of comparison of
adjectives and the proportion of words belonging to
various parts of speech among our features. This
group of features also included the proportion of ad-
positions (=prepositions+postpositions) along with
the proportion of prepositions and postpositions sep-
arately. We also included the proportion of co-
ordinating conjunctions and subordinating conjunc-
tions along with that of all conjunctions.
11Examples of various forms of declension and conjugation
can be found in the Estonian morphology guide at: http://
lpcs.math.msu.su/?pentus/etmorf.htm
66
4.4 Lexical Variation features
Lexical variation, also called lexical range indicates
the range of vocabulary displayed in a learner?s lan-
guage use. We implemented the measures of lexical
variation that are used in the English SLA research
to measure the lexical richness of the learners of En-
glish as a second language (Lu, 2012). These in-
cluded the noun variation, verb variation, adjective
variation and verb variation which indicated the ra-
tio of the words with the respective parts of speech
to the total number of lexical words (instead of all
words).
4.5 Text Length Feature
Since text length is one of the most commonly used
measures of learner proficiency and also because of
the variation in average text length across the pro-
ficiency levels (Table1), we included the number of
word tokens per document as a feature.
4.6 Most Predictive Features
Apart from these individual feature groups, we also
performed a feature selection, to identify the most
predictive ones among all our features. We used the
Correlation based Feature Subset (CFS) selection
method in WEKA for this purpose. CFS chooses
a feature subset considering the correlation and the
degree of redundancy between the features. Table 2
consists of a list of the most predictive and non-
redundant features after ranking all the selected fea-
tures based on their Information Gain. This list con-
sisted of five verb morphology based features fol-
lowed by three nominal declension features.
Feature Group
Nominative case NounMorph
Impersonal VerbMorph
Personal VerbMorph
Num. words TextLength
Present tense VerbMorph
2nd person verbs VerbMorph
Prepositions POS
Allative case NounMorph
Imperatives VerbMorph
Translative case NounMorph
Table 2: 10 Most Predictive, Non-redundant Features
It is interesting to note that several characteris-
tics that are prominent in Estonian (cf. Section 4.1)
figured among this list of most predictive features.
Nominative being the top predictor can be explained
due to the difference in (the number of) cases be-
tween Estonian and other languages. For example
(Eslon, 2011) found in her corpus study based on the
same corpus that the learners frequently use nom-
inative case instead of genitive and partitive case.
So, it is to be expected that the usage of the nom-
inative case changes as the proficiency increases.
Impersonal and personal voice are distinctive fea-
tures in Estonian and other Finnic languages, as
they are different from the active and passive voice
that typically exist in other languages (Erelt, 2003).
This may make them difficult to master for language
learners, making them one of the top predictors for
proficiency. Further, Estonian has more postposi-
tions than prepositions. Hence, one could that the
use of prepositions will be replaced by postposi-
tions as the language acquisition progresses (Ehala,
1994).
5 Experiments and Results
We first studied the effect of the individual feature
groups as well as their combination for a three class
classification of Estonian learners into A, B and C
classes. We also studied the impact of a stacking
ensemble on the overall classification accuracy and
found out that it did not result in a significant im-
provement on the test set. Hence, we further investi-
gated the problem as a collection of multi-stage two-
class cascades instead of a single stage three class
classification. For all our classification experiments,
we used the WEKA (Hall et al, 2009) toolkit. We
report the overall classification accuracy as our eval-
uation metric.
5.1 Three Class-Classification
We first considered the learner classification as a sin-
gle step, three class classification problem. Since
50 documents from each category were separated as
a held-out test set (cf. Section 3.1), we built our
three-class models with 250 texts per category as our
training set to ensure that there is a balanced distri-
bution between classes. We trained multiple clas-
sification models considering the individual feature
67
groups and the most predictive feature group. Ta-
ble 3 shows the classification accuracy of various
feature groups, reported using the Sequential Mini-
mal Optimization (SMO) implementation in WEKA
(Platt, 1998).
Features 10-Fold CV Test set
Random baseline 33.33% 33.33%
Noun Morph. 56.64% 52%
Verb Morph 57.55% 58%
POS 52.99% 47.33%
Lex. Variation 43.36% 47.33%
Text Length 33.72% 34%
All Features 62.45% 59.33%
Noun+Verb Morph 61.45% 58%
Top10 features (Table 2) 57.34% 56.58%
Table 3: Estonian Learner Proficiency Classification with
various Feature groups
Although the classification accuracies overall are
not very high, it can be seen from the results that the
morphological variation does play a key role in pro-
ficiency classification of Estonian. While the verbal
morphology features performed best as an individ-
ual feature sub group, the addition of lexical varia-
tion and POS features to the morphological features
added very little to the overall classification accu-
racy.
Text length turned out to be the most predictive
single feature among the top features. It can be seen
from Table 3 that this feature alone resulted in a clas-
sification accuracy of 34%, which is just above the
random baseline (33.33%). But the fact that the C
level in general contained a higher number of es-
says and translations compared to other categories
of text like letters and short answers (than the A and
B levels), thereby resulting in longer texts in gen-
eral, may have resulted text length being the single
most predictive feature. The Top-10 features also
performed on par with the individual morphological
feature subgroups.
5.1.1 Ensemble Model
Since ensemble models are known to obtain a bet-
ter performance than their constituent models, we
compared the performance of a stacking ensemble
against its individual constituent models. We trained
three classification models on the entire feature set,
using the same train-test sets as explained before and
trained an ensemble model with three classifiers. We
used the StackingC implementation of WEKA (See-
wald, 2002) to combine the models, with a linear re-
gression model as our meta classifier. Table 4 shows
the classification accuracies for the individual clas-
sifiers as well as the ensemble on a 10-fold CV of
the training set and on the held out test set. The
ensemble did not result in any significant improve-
ment (<1%) compared to the best model amongst
the three of its individual components (SMO). The
ensemble?s performance on the test set was poor
compared to the best classification model.
Classifier 10-Fold CV Test set
SMO 62.45% 59.33%
Logistic Regression 59.37% 52%
Decision Tree 57.29% 52.33%
Stacked Ensemble 63.28% 57.33%
Table 4: Proficiency Classification With an Ensemble
5.2 Classification Through Two-Class Cascades
Since combining the classifiers as a stacking ensem-
ble did not work, we turned to reformulating our
problem as a cascade of two-class classifiers. Cas-
cade generalization is the process of sequentially
using a set of small classifiers to perform an over-
all classification task. Gama and Brazdil (2000)
showed that a cascade can outperform other ensem-
ble methods like stacking or boosting. Kaynak and
Alpaydin (2000) proposed a method to sequentially
cascade classifiers and showed that this improves the
accuracy without increasing the computational com-
plexity and cost. Although the creation of our clas-
sifier cascades in this paper is not the same as any
of the above mentioned research, their conclusion
that cascading subsets of classifiers to build an over-
all classifier can possibly result in a better accuracy
was the main motivation for this experiment.
The SMO implementation in WEKA also con-
siders multi-class classification as a combination of
pairwise binary classifications. But, in our subse-
quent experiments, we combine our two-class clas-
sifiers as a multi-stage cascade rather than a multi-
expert stacking ensemble. For these experiments,
68
we first built the various binary classifiers that were
later used to construct the cascades. We chose our
combinations both by using a One vs All (OvA) as
well as a One vs One (OvO) strategy. Thus, six bi-
nary classifiers were created, namely:
? (A, B) classifier
? (B, C) classifier
? (C, A) classifier
? (A and Not A) classifier
? (B and Not B) classifier
? (C and Not C) classifier
In all the cases, our training data consisted of
equal number of instances per class. In the cases of
the last three classifiers, the training data for NotA,
NotB and NotC categories consisted of instances
from both the classes that were included in the re-
spective ?Not-? classes. The data from the held-
out test set was not included in any of these binary
classification experiments. The training data size for
each classifier has a different size depending on the
classes involved. In all cases, the number of train-
ing samples per category is equal to the number of
documents belonging to the category with the least
number of documents. Hence, in cases involving
the C-class (ABC, AC, BC, CnotC), we trained the
classifiers with 250 documents per category. In all
the other cases (AB, AnotA, BnotB), we trained the
classifiers with 750 documents per category. Table 5
summarizes the training data size and the classifica-
tion accuracies using 10-fold cross validation. All
the models were trained using the SMO algorithm.
Classifer Training data size Accuracy
A,B 750 per cat 70.8%
B,C 250 per cat 74.59%
A,C 250 per cat 85.93%
A,NotA 750 per cat 74.20%
B,NotB 750 per cat 60.04%
C,NotC 250 per cat 79.69%
Table 5: Binary Classifications of Estonian Learners
This binary classification shows that there is a
clear trend among the features across the proficiency
levels. In the case of a pair-wise classification be-
tween classes, the highest classification accuracy
was achieved for the binary classifier that considered
the A and C classes. Although the classification ac-
curacies of the binary classifiers (A,B) and (B,C) are
considerably higher than the overall three class clas-
sification accuracy (Table 3), they are very low com-
pared to that of the binary classifier (A,C). The con-
fusion between the three classes is the highest when
it involves the middle class, B. This confirmed the
ordinal nature of proficiency classification. In the
second set of binary classifiers, again, the classifier
with a poor performance turned out to be (B,NotB).
To take advantage of the fact that the two-class
classification is much more accurate than the three-
class classification, we studied three class classifica-
tion by building multi-stage classifier cascades us-
ing the above binary classifiers. Based on the output
of the first stage (which is the most accurate classi-
fier), we feed the test instance to one of the remain-
ing classifiers to get the final prediction.
5.2.1 Cascade-1
For the first cascade, we considered the pairwise
binary classifiers that used a One vs One (OvO)
strategy from Table 5. We constructed a classifier
cascade as follows: For each test instance,
? Classify the instance using the classifier (A,C).
? If A, re-classify the instance using the classifier
(A,B).
? if C, re-classify the instance using the classifier
(B,C).
5.2.2 Cascade-2
For the second cascade, we considered the sec-
ond set of binary classifiers from Table 5, which use
a One vs All (OvA) strategy. The cascade is con-
structed as follows: For each test instance,
? Classify the instance using the classifier
(C,NotC).
? If C, classify the instance as C.
? Else, re-classify the instance using the classifier
(A,notA).
69
The choice of these particular combinations of
cascades was motivated by two factors:
? To understand the performance of OvO and
OvA binary classifier cascades independently
? To start with the classifier that has the highest
accuracy as the first stage.
Table 6 compares the performance on the test set
of the cascaded classifiers against the normal 3-class
classifier and a classifier ensemble. Compared to a
normal three-class classifier, the cascaded approach
showed more than 5% improvement in the classifica-
tion accuracy using both the cascades. Compared to
Cascade-1, Cascade-2 performed even better with a
66.66% classification accuracy on the test set. Since
binary classification for certain pairs seemed to be
possible with higher accuracy than the three-class
classification, reformulating three class classifica-
tion as a cascade of binary classifications may result
in a better classification accuracy. This was the ini-
tial motivation for the choice of cascade classifica-
tion. Our results clearly showed that it was a fruitful
experiment.
Classifer Accuracy
Cascade-1 64.66%
Cascade-2 66.66%
3-class,without cascade 59.33%
3-class ensemble 57.33%
Table 6: Comparison of Cascade classification
The cascades need more exploration though.
Also, although the morphological features turned
out to be useful predictors of proficiency classifica-
tion, the classification accuracies are still not very
high. Two possible explanations could be that our
features are good but not sufficient or that the train-
ing data was insufficient.
It is clear from our various classification experi-
ments that the morphological features are good pre-
dictors of proficiency levels. But, surely, there is
much more to language proficiency than morpholog-
ical complexity. So, exploring more features will be
the natural next step to improve the overall classi-
fication accuracy. However, to gain some more in-
sights at this level, we studied the effect of training
data sizes on the various classification tasks we per-
formed.
5.3 Effect of Training Sample Size
We took all the seven different classification mod-
els we used in the earlier experiments and studied
the impact of gradually increasing the training data
size on classification accuracy. For this purpose,we
trained all the classifiers with the complete feature
set using the SMO algorithm. The classifiers studied
include the three class ABC classifier and the binary
classifiers AB, BC, AC, AnotA, BnotB and CnotC.
Table 7 summarizes the effect of splitting the respec-
tive training sets into various train-test splits, on the
classification accuracies.
classifier 50-50 60-40 70-30 80-20
ABC 56.73% 60.05% 61.76% 62.76%
AB 71.07% 71.3% 71.2% 72.04%
BC 71.33% 72.35% 71.73% 74.86%
AC 86.31% 84.95% 84.15% 85.55%
AnotA 75.39% 75.20% 76.65% 75.82%
BnotB 59.05% 57.95% 56.91% 58.08%
CnotC 77.34% 77.56% 77.27% 76.52%
Table 7: Effect of training size on classification accuracy
As the table shows, training data size had an im-
pact only on some of the classification tasks. For
the three class classification, training set size had a
clear effect. Although our corpus had a large num-
ber of texts from A and B compared to C (Table
1), since we used balanced training sets to train all
models, the three-class model had relatively fewer
number of documents per category (250) compared
to, say, the AB classifier (750 per category). Re-
duction of this small training set further by 50% de-
creased the three class classification accuracy from
62.76% (when 80% of the data was used for train-
ing) to 56.73%. So, in this case, training data size
had an effect.
However, an interesting observation is that this
small training sample size (250 documents per cat-
egory) did not have any impact on the classification
performance of the classifier (A,C). This classifier
consistently performed at a higher level compared to
all the other classifiers even when the training data
was only 50% (125 documents per category). Al-
70
though it is possible that the length of the document
played a role here, there was little difference in the
performance (< 1%) even after removing the text
length feature. This indicates a strong differentiation
between the texts of the language learners of levels
A and C, in terms of the features we used.
In case of the other classification tasks, only the
(B,C) classifier showed some effect of the training
data on its overall classification accuracy. While
there might be other reasons that we did not no-
tice yet, it is possible that the inter class overlap
between (A,B) is more compared to the overlap be-
tween (B,C) at least in terms of the features we con-
sidered. Also, the fact that the B-level lies in be-
tween A and C could also have contributed to the
fact that more training data has little effect on clas-
sifiers involving data from all the three classes (An-
otA, BnotB, CnotC).
6 Conclusion and Discussion
In this paper, we discussed the task of classify-
ing learner texts into standardized proficiency lev-
els based on the texts produced by learners of Es-
tonian as a second language. We used the publicly
accessible Estonian Interlanguage Corpus (EIC) and
modeled our classifiers by considering the morpho-
syntactic variation as our primary feature group. We
hypothesized that the morphology may play an im-
portant role in detecting the proficiency levels as Es-
tonian is a morphologically rich and complex lan-
guage.
For building our classifiers, we experimented with
various methods such as three class classifiers, an
ensemble model and multi-stage cascades. Our ex-
periments showed that the multi-stage cascades im-
proved the classification accuracy compared to the
other approaches. Our experiments also showed a
clear trend across the proficiency levels. There was
little classification overlap between the beginner (A)
and the advanced (C) level texts but a strong overlap
of both these levels with the intermediate (B) level.
We can conclude from our experiments that the
morphological features can indeed play an impor-
tant role in the proficiency classification of Estonian.
Although the classification accuracies we achieved
(60-65%) have a long way to go in terms of a real-
world grading application, we believe that this is a
good starting point to explore the role of morphol-
ogy in proficiency classification of Estonian in par-
ticular and other morphologically rich languages in
general.
As a part of our future work, we intend to investi-
gate the role of morphology in Estonian proficiency
classification further. We also want to compare the
proficiency levels across various genres of texts in
the corpus (e.g, essays, personal and official letters,
translations etc.). Another interesting dimension we
want to explore further is the distribution of specific
kinds of morphological phenomena (e.g., case mark-
ers) that exist in Estonian but not in the learner?s na-
tive language, across the different proficiency levels.
It would also be interesting to apply insights from
the theories of second language acquisition research
and study their utility for proficiency classification.
Apart from morphology, we also intend to study the
impact of other features such as lexical sophistica-
tion, error rate, syntactic complexity and discourse
coherence. Finally, on the model construction side,
we plan to investigate and understand the working
of cascaded classifiers better in this context.
Acknowledgments
We thank Dr Pille Eslon from the Talinn University
for sharing the corpus with us. We also thank Serhiy
Bykh, Dr Detmar Meurers and the three anonymous
reviewers for their feedback on the paper. This re-
search is partially funded by the European Commis-
sion?s 7th Framework Program under grant agree-
ment number 238405 (CLARA)12
References
R.S.J.d. Baker. 2010. Mining data for student models. In
Advances in Intelligent Tutoring Systems, pages 323?
338. Springer.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion: Online essay evaluation: An appli-
cation for automated evaluation of student essays. In
Proceedings of the Fifteenth Annual Conference on In-
novative Applications of Artificial Intelligence (IAAI-
03), pages 3?10, Acapulco, Mexico, August.
Jill Burstein, 2003. The e-rater Scoring Engine: Auto-
mated Essay Scoring with Natural Language Process-
ing, chapter 7, pages 107?115. Lawrence Erlbaum As-
sociates, Inc.
12http://clara.uib.no
71
Scott A. Crossley, Tom Salsbury, and Danielle S. Mc-
Namara. 2011. Predicting the proficiency level of
language learners using lexical indices. In Language
Testing.
M.C. Desmarais and R.S.J.d. Baker. 2012. A review of
recent advances in learner and skill modeling in intel-
ligent learning environments. In User Modeling and
User-Adapted Interaction, 22(1-2).
Markus Dickinson, Sandra Ku?bler, and Anthony Meyer.
2012. Predicting learner levels for online exercises
of Hebrew. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 95?104, Montre?al, Canada, June. Association
for Computational Linguistics.
Martin Ehala. 1994. Russian influence and the change in
progress in the Estonian adpositional system. In Lin-
guistica Uralica, 3, pages 177?193.
M. Erelt, T. Erelt, and K. Ross. 2007. Eesti keele
ka?siraamat. Eesti Keele Sihtasutus.
M. Erelt. 2003. Estonian language. Linguistica Uralica.
Estonian Academy Publishers.
Pille Eslon. 2007. O?ppijakeelekorpused ja keeleo?p.
In Tallinna U?likooli keelekorpuste optimaalsus,
to?o?tlemine ja kasutamine, pages 87?120.
Pille Eslon. 2011. Millest ra?a?givad eesti keele
ka?a?ndeadendused? la?hivo?rdlusi. In La?hivertailuja, 21,
pages 45?64.
Joao Gama and Pavel Brazdil. 2000. Cascade general-
ization.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10?
18.
Md Maruf Hasan and Hnin Oo Khaing. 2008. Learner
corpus and its application to automatic level checking
using machine learning algorithms. In Proceedings of
ECTI-CON.
C. Kaynak and E. Alpaydin. 2000. Multistage cascad-
ing of multiple classifiers: One mans noise is another
man?s data. In Proceedings of the 17th International
Conference on Machine Learning (ICML).
Xiaofei Lu. 2012. The relationship of lexical richness to
the quality of esl learners? oral narratives. The Modern
Languages Journal.
John C. Platt. 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector ma-
chines. Technical Report MSR-TR-98-14, Microsoft
Research.
Lawrence Rudner, Veronica Garcia, and Catherine
Welch. 2005. An evaluation of intellimetricTM essay
scoring system using responses to gmat awa prompts.
Technical report, Graduate Management Admission
Council (GMAC).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
A.K. Seewald. 2002. How to make stacking better and
faster while also taking care of an unknown weakness.
In In the proceedings of the Nineteenth International
Conference on Machine Learning, pages 554?561.
Thepchai Supnithi, Kiyotaka Uchimoto, Toyomi Saiga,
Emi Izumi, Sornlertlamvanich Virach, and Hitoshi Isa-
hara. 2003. Automatic proficiency level checking
based on sst corpus. In In Proceedings of RANLP.
Yukio Tono. 2000. A corpus-based analysis of interlan-
guage development: analysing pos tag sequences of
EFL learner corpora. In PALC?99: Practical Applica-
tions in Language Corpora, pages 323?340.
Nina Vyatkina. 2012. The development of second lan-
guage writing complexity in groups and individuals:
A longitudinal learner corpus study. The Modern Lan-
guage Journal. to appear.
David M. Williamson, Randy Elliot Bennett, Stephen
Lazer, Jared Bernstein, Peter W. Foltz, Thomas K.
Landauer, David P. Rubin, Walter D. Way, and Kevin
Sweeney. 2010. Automated scoring for the assess-
ment of common core standards. White Paper.
David M. Williamson. 2009. A framework for im-
plementing automated scoring. In The annual meet-
ing of the American Educational Research Association
(AERA) and the National Council on Measurement in
Education (NCME).
Bo Zhang. 2008. Investigating proficiency classification
for the examination for the certificate of proficiency
in english (ecpe). In Spaan Fellow Working Papers in
Second or Foreign Language Assessment.
72
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 197?206,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Combining Shallow and Linguistically Motivated Features in
Native Language Identification
Serhiy Bykh Sowmya Vajjala Julia Krivanek Detmar Meurers
Seminar fu?r Sprachwissenschaft, Universita?t Tu?bingen
{sbykh, sowmya, krivanek, dm}@sfs.uni-tuebingen.de
Abstract
We explore a range of features and ensembles
for the task of Native Language Identification
as part of the NLI Shared Task (Tetreault et al,
2013). Starting with recurring word-based n-
grams (Bykh and Meurers, 2012), we tested
different linguistic abstractions such as part-
of-speech, dependencies, and syntactic trees
as features for NLI. We also experimented
with features encoding morphological proper-
ties, the nature of the realizations of particu-
lar lemmas, and several measures of complex-
ity developed for proficiency and readabil-
ity classification (Vajjala and Meurers, 2012).
Employing an ensemble classifier incorporat-
ing all of our features we achieved an ac-
curacy of 82.2% (rank 5) in the closed task
and 83.5% (rank 1) in the open-2 task. In
the open-1 task, the word-based recurring n-
grams outperformed the ensemble, yielding
38.5% (rank 2). Overall, across all three tasks,
our best accuracy of 83.5% for the standard
TOEFL11 test set came in second place.
1 Introduction
Native Language Identification (NLI) tackles the
problem of determining the native language of an
author based on a text the author has written in a
second language. With Tomokiyo and Jones (2001),
Jarvis et al (2004), and Koppel et al (2005) as first
publications on NLI, the research focus in computa-
tional linguistics is relatively young. But with over
a dozen new publications in the last two years, it is
gaining significant momentum.
In Bykh and Meurers (2012), we explored a data-
driven approach using recurring n-grams with three
levels of abstraction using parts-of-speech (POS). In
the present work, we continue exploring the contri-
bution and usefulness of more linguistically moti-
vated features in the context of the NLI Shared Task
(Tetreault et al, 2013), where our approach is in-
cluded under the team name ?Tu?bingen?.
2 Corpora used
T11: TOEFL11 (Blanchard et al, 2013) This is the
main corpus of the NLI Shared Task 2013. It con-
sists of essays written by English learners with 11
native language (L1) backgrounds (Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, Turkish), and from three different
proficiency levels (low, medium, high). Each L1 is
represented by a set of 1100 essays (train: 900, dev:
100, test: 100). The labels for the train and dev sets
were given from the start, the labels for the test set
were provided after the results were submitted.
ICLE: International Corpus of Learner English
(Granger et al, 2009) The ICLEv2 corpus consists
of 6085 essays written by English learners of 16 dif-
ferent L1 backgrounds. They are at a similar level of
English proficiency, namely higher intermediate to
advanced and of about the same age. For the cross-
corpus tasks we used the essays for the seven L1s in
the intersection with T11, i.e., Chinese (982 essays),
French (311), German (431), Italian (391), Japanese
(366), Spanish (248), and Turkish (276).
FCE: First Certificate in English Corpus (Yan-
nakoudakis et al, 2011) The FCE dataset consists
of 1238 scripts produced by learners taking the First
Certificate in English exam, assessing English at an
197
upper-intermediate level. For the cross-corpus tasks,
we used the essays by learners of the eight L1s in
the intersection with T11, i.e., Chinese (66 essays),
French (145), German (69), Italian (76), Japanese
(81), Korean (84), Spanish (198), and Turkish (73).
BALC: BUiD (British University in Dubai) Arab
Learner Corpus (Randall and Groom, 2009) The
BALC corpus consists of 1865 English learner texts
written by students with an Arabic L1 background
from the last year of secondary school and the first
year of university. The texts were scored and as-
signed to six proficiency levels. For the cross-corpus
NLI tasks, we used the data from the levels 3?5
amounting to overall 846 texts. We excluded the two
lowest and the highest, sixth level based on pretests
with the full BALC data.
ICNALE: International Corpus Network of
Asian Learners of English (Ishikawa, 2011) The
version of the ICNALE corpus we used consists of
5600 essays written by college students in ten coun-
tries and areas in Asia as well as by English na-
tive speakers. The learner essays are assigned to
four proficiency levels following the CEFR guide-
lines (A2, B1, B2, B2+). For the cross-corpus tasks,
we used the essays written by learners from Korea
(600 essays) and from Pakistan (400).1 Without ac-
cess to a corpus with Hindi as L1, we decided to la-
bel the essays written by Pakistani students as Hindi.
Most of the languages spoken in Pakistan, including
the official language Urdu, belong to the same Indo-
Aryan/-Iranian language family as Hindi. Our main
focus here was on avoiding overlap with Telugu, the
other Indian language in this shared task, which be-
longs to the Dravidian language family.
TU?TEL-NLI: Tu?bingen Telugu NLI Corpus We
collected 200 English texts written by Telugu native
speakers from bilingual (English-Telugu) blogs, lit-
erary articles, news and movie review websites.
NT11: NON-TOEFL11 We combined the ICLE,
FCE, ICNALE, BALC and TU?TEL-NLI sources
discussed above in the NT11 corpus consisting of
overall 5843 essays for 11 L1s, as shown in Table 1.
1We did not include ICNALE data for more L1s to avoid
overrepresentation of already well-represented Asian L1s.
Corpora
L1 ICLE FCE BALC ICNALE TU?TEL #
ARA - - 846 - - 846
CHI 982 66 - - - 1048
FRE 311 145 - - - 456
GER 431 69 - - - 500
HIN - - - 400 - 400
ITA 391 76 - - - 467
JPN 366 81 - - - 447
KOR - 84 - 600 - 684
SPA 248 198 - - - 446
TEL - - - - 200 200
TUR 276 73 - - - 349
# 3005 792 846 1000 200 5843
Table 1: Distribution of essays for the 11 L1s in NT11
3 Features
Recurring word-based n-grams (rc. word ng.)
Following, Bykh and Meurers (2012), we used all
word-based n-grams occurring in at least two texts
of the training set. We focused on recurring uni-
grams and bigrams, which in our previous work and
in T11 testing with the dev set worked best. For the
larger T11 train ? NT11 set, recurring n-grams up
to length five were best, but for uniformity we only
used word-based unigrams and bigrams for all tasks.
As in our previous work, we used a binary feature
representation encoding the presence or absence of
the n-gram in a given essay.
Recurring OCPOS-based n-grams (rc. OCPOS
ng.) All OCPOS n-grams occurring in at least two
texts of the training set were obtained as described
in Bykh and Meurers (2012). OCPOS means that
the open class words (nouns, verbs, adjectives and
cardinal numbers) are replaced by the corresponding
POS tags. For POS tagging we used the OpenNLP
toolkit (http://opennlp.apache.org).
In Bykh and Meurers (2012), recurring OCPOS
n-grams up to length three performed best. How-
ever, for T11 we found that including four- and five-
grams was beneficial. This confirms our assumption
that longer n-grams can be sufficiently common to
be useful (Bykh and Meurers, 2012, p. 433). Thus
we used the recurring OCPOS n-grams up to length
five for the experiments in this paper. We again used
a binary feature representation.
198
Recurring word-based dependencies (rc. word
dep.) Extending the perspective on recurring pieces
of data to other data types, we explored a new fea-
ture: recurring word-based dependencies. A feature
of this type consists of a head and all its immediate
dependents. The dependencies were obtained using
the MATE parser (Bohnet, 2010). The words in each
n-tuple are recorded in lowercase and listed in the or-
der in which they occur in the text; heads thus are not
singled out in this encoding. For example, the sen-
tence John gave Mary an interesting book yields the
following two potential features (john, gave, mary,
book) and (an, interesting, book). As with recur-
ring n-grams we utilized only features occurring in
at least two texts of the training set, and we used a
binary feature representation.
Recurring function-based dependencies (rc.
func. dep.) The recurring function-based depen-
dencies are a variant of the recurring word-based
dependencies described above, where each depen-
dent is represented by its grammatical function. The
above example sentence thus yields the two features
(sbj, gave, obj, obj) and (nmod, nmod, book).
Complexity Given that the proficiency level of a
learner was shown to play a role in NLI (Tetreault
et al, 2012), we implemented all the text com-
plexity features from Vajjala and Meurers (2012),
who used measures of learner language complex-
ity from SLA research for readability classification.
These features consist of lexical richness and syn-
tactic complexity measures from SLA research (Lu,
2010; 2012) as well as other syntactic parse tree
properties and traditionally used readability formu-
lae. The parse trees were built using the Berke-
ley parser (Petrov and Klein, 2007) and the syntac-
tic complexity measures were estimated using the
Tregex package (Levy and Andrew, 2006).
In addition, we included morphological and POS
features from the CELEX Lexical Database (Baayen
et al, 1995). The morphological properties of words
in CELEX include information about the deriva-
tional, inflectional and compositional features of
the words along with information about their mor-
phological origins and complexity. POS properties
of the words in CELEX describe the various at-
tributes of a word depending on its parts of speech.
We included all the non-frequency based and non-
word-string attributes from the English Morphology
Lemma (EML) and English Syntax Lemma (ESL)
files of the CELEX database. We also defined Age
of Acquisition features based on the psycholinguis-
tic database compiled by Kuperman et al (2012). Fi-
nally, we included the ratios of various POS tags to
the total number of words as POS density features,
using the POS tags from the Berkeley parser output.
Suffix features The use of different derivational
and inflectional suffixes may contain information
regarding the L1 ? either through L1 transfer, or
in terms of what suffixes are taught, e.g., for
nominalization. In a very basic approximation of
morphological analysis, we used the porter stem-
mer implementation of MorphAdorner (http://
morphadorner.northwestern.edu). For each
word in a learner text, we removed the stem
it identified from the word, and if a suffix re-
mained, we matched it against the Wiktionary list of
English suffixes (http://en.wiktionary.org/
wiki/Appendix:Suffixes:English). For each
valid suffix thus identified, we defined a binary fea-
ture (suffix, bin.) recording the presence/absence
and a feature counting the number of occurrences
(suffix, cnt.) in a given learner text.
Stem-suffix features We also wondered whether
the subset of morphologically complex unigrams
may be more indicative than considering all uni-
grams as features. As a simple approximation of this
idea, we used the stemmer plus suffix-list approach
mentioned above and used all words for which a suf-
fix was identified as features, both binary (stemsuf-
fix, bin.) and count-based (stemsuffix, cnt.).
Local trees Based on the syntactic trees assigned
by the Berkeley Parser (Petrov and Klein, 2007), we
extracted all local trees, i.e., trees of depth one. For
example, for the sentence I have a tree, the parser
output is: (ROOT (S (NP (PRP I)) (VP (VBP have)
(NP (DT a) (NN tree))) (. .))) for which the local
trees are (S NP VP .), (NP PRP), (NP DT NN), (VP
VBP NP), (ROOT S). Count-based features are used.
Stanford dependencies Tetreault et al (2012) ex-
plored the utility of basic dependencies as features
for NLI. In our approach, we extracted all Stanford
199
dependencies (de Marneffe et al, 2006) using the
trees assigned by the Berkeley Parser. We consid-
ered lemmatized typed dependencies (type dep. lm.)
such as nsubj(work,human) and POS tagged ones
(type dep. POS) such as nsubj(VB,NN) for our fea-
tures. We used count-based features for those typed
dependencies.
Dependency number (dep. num.) We encoded the
number of dependents realized by a verb lemma,
normalized by this lemma?s count. For example, if
the lemma take occurred ten times in a document,
three times with two dependents and seven times
with three dependents, we get the features take:2-
dependents = 3/10 and take:3-dependents = 7/10.
Dependency variability (dep. var.) These fea-
tures count possible dependent-POS combinations
for a verb lemma, normalized by this verb lemma?s
count. If in the example above, the lemma take
occurred three times with two dependents JJ-NN,
two times with three dependents JJ-NN-VB, and five
times with three dependents NN-NN-VB, we ob-
tain take:JJ-NN = 3/10, take:JJ-NN-VB = 2/10, and
take:NN-NN-VB = 5/10.
Dependency POS (dep. POS) These features are
derived from the dep. var. features and encode how
frequent which kind of category was a dependent for
a given verb lemma. Continuing the example above,
take takes dependents of three different categories:
JJ, NN and VB. For each category, we create a fea-
ture, the value of which is the category count divided
by the number of dependents of the given lemma,
normalized by the lemma?s count in the document.
In the example, we obtain take:JJ = (1/2 + 1/3)/10,
take:NN = (1/2 + 1/3 + 2/3)/10, and take:VB = (1/3
+ 1/3)/10.
Lemma realization matrix (lm. realiz.) We spec-
ified a set of features that is calculated for each dis-
tinct lemma and three feature sets generalizing over
all lemmas of the same category:
1. Distinct lemma counts of a specific category
normalized by the total count of this category
in a document. For example, if the lemma can
is found in a document two times as a verb and
five times as a noun, and the document contains
30 verbs and 50 nouns, we obtain the two fea-
tures can:VB = 2/30 and can:NN = 5/50.
2. Type-Lemma ratio: lemmas of same category
normalized by total lemma count
3. Type-Token ratio: tokens of same category nor-
malized by total token count
4. Lemma-Token Ratio: lemmas of same category
normalized by tokens of same category
Proficiency and prompt features Finally, for some
settings in the closed task we also included two nom-
inal features to encode the proficiency (low, medium,
high) and the prompt (P1?P8) features provided as
meta-data along with the T11 corpus.
4 Results
4.1 Evaluation Setup
We developed our approach with a focus on the
closed task, training the models on the T11 train set
and testing them on the T11 dev set. For the
closed task, we report the accuracies on the dev set
for all models (single feature type models and en-
sembles as introduced in sections 4.2 and 4.3),
before presenting the accuracies on the submitted
test set models, which were trained on the T11 train
? dev set. In addition, for the submitted models
we report the accuracies obtained via 10-fold cross-
validation on the T11 train ? dev set using the folds
specification provided by the organizers of the NLI
Shared Task 2013.
The results for the open-1 task are obtained by
training the models on the NT11 set, and the results
for the open-2 task are obtained by training the mod-
els on the T11 train ? dev set ? NT11 set. For the
open-1 and open-2 tasks, we report the basic single
feature type results on the T11 dev set and two sets
of results on the T11 test set: the results for the ac-
tual submitted systems and the results for the com-
plete systems, i.e., including the features used in the
closed task submissions that for the open tasks were
only computed after the submission deadline (given
our focus on the closed task and finite computational
infrastructure). We include the figures for the com-
plete systems to allow a proper comparison of the
performance of our models across the tasks.
Below we provide a description of the various ac-
curacies (%) we report for the different tasks:
200
? Acctest: Accuracy on the T11 test set after
training the model on:
? closed: T11 train ? dev set
? open-1: NT11 set
? open-2: T11 train ? dev set ? NT11 set
? Accdev: Accuracy on the T11 dev set after
training the model on:
? closed: T11 train set
? open-1: NT11 set
? open-2: T11 train set ? NT11 set
? Acc10train?dev: Accuracy on the T11 train ? dev
set obtained via 10-fold cross-validation using
the data split information provided by the orga-
nizers, applicable only for the closed task.
In terms of the tools used for classification, we
employed LIBLINEAR (Fan et al, 2008) using
L2-regularized logistic regression, LIBSVM (Chang
and Lin, 2011) using C-SVC with the RBF kernel
and WEKA SMO (Platt, 1998; Hall et al, 2009) fit-
ting logistic models to SVM outputs (the -M option).
Which classifier was used where is discussed below.
4.2 Single Feature Type Classifier Results
First we evaluated the performance of each fea-
ture separately for the closed task by computing the
Accdev values. These results constituted the basis
for the ensembles discussed in section 4.3. We also
report the corresponding results for the open-1 and
open-2 tasks, which were partly obtained after the
system submission and thus were not used for de-
veloping the approach. As classifier, we generally
used LIBLINEAR, except for complexity and lm.
realiz., where SMO performed consistently better.
The summary of the single feature type performance
is shown in Table 2.
The results reveal some first interesting insights
into the employed feature sets. The figures show
that the recurring word-based n-grams (rc. word ng.)
taken from Bykh and Meurers (2012) are the best
performing single feature type in our set yielding an
Accdev value of 81.3%. This finding is in line with
the previous research on different data sets showing
that lexical information seems to be highly relevant
for the task of NLI (Brooke and Hirst, 2011; Bykh
and Meurers, 2012; Jarvis et al, 2012; Jarvis and
Paquot, 2012; Tetreault et al, 2012). But also the
more abstract linguistic features, such as complexity
Accdev
Feature type closed open-1 open-2
1. rc. word ng. 81.3 42.0 80.3
2. rc. OCPOS ng. 67.6 26.6 64.8
3. rc. word dep. 67.7 30.9 69.4
4. rc. func. dep. 62.4 28.2 61.3
5. complexity 37.6 19.7 36.5
6. stemsuffix, bin. 50.3 21.4 48.8
7. stemsuffix, cnt. 48.2 19.3 47.1
8. suffix, bin. 20.4 9.1 17.5
9. suffix, cnt. 19.0 13.0 17.7
10. type dep. lm. 67.3 25.7 67.5
11. type dep. POS 46.6 27.8 27.6
12. local trees 49.1 26.2 25.7
13. dep. num. 39.7 19.6 41.8
14. dep. var. 41.5 18.6 40.1
15. dep. POS 47.8 21.5 47.4
16. lm. realiz. 70.3 30.3 66.9
Table 2: Single feature type results on T11 dev set
measures, local trees, or dependency variation mea-
sures seem to contribute relevant information, con-
sidering the random baseline of 9% for this task.
Having explored the performance of the single
feature type models, the interesting question was,
whether it is possible to obtain a higher accuracy
than yielded by the recurring word-based n-grams
by combining multiple feature types into a single
model. We thus investigated different combinations,
with a primary focus on the closed task.
4.3 Combining Feature Types
We followed Tetreault et al (2012) in exploring two
options: On the one hand, we combined the differ-
ent feature types directly in a single vector. On the
other hand, we used an ensemble classifier. The en-
semble setup used combines the probability distribu-
tions provided by the individual classifier for each
of the incorporated feature type models. The indi-
vidual classifiers were trained as discussed above,
and ensembles were trained and tested using LIB-
SVM, which in our tests performed better for this
purpose than LIBLINEAR. To obtain the ensemble
training files, we performed 10-fold cross-validation
for each feature model on the T11 train set (for in-
ternal evaluation) and on the T11 train ? dev set (for
201
submission) and took the corresponding probability
estimate distributions. For the ensemble test files,
we took the probability estimate distribution yielded
by each feature model trained on the T11 train set
and tested on the T11 dev set (for internal evalua-
tion), as well as by each feature model trained on
the T11 train ? dev set and tested on the T11 test set
(for submission).
In our tests, the ensemble classifier always outper-
formed the single vector combination, which is in
line with the findings of Tetreault et al (2012). We
thus focused on ensemble classification for combin-
ing the different feature types.
4.4 Closed Task (Main) Results
We submitted the predictions for the systems listed
in Table 3, which we chose in order to test all fea-
ture types together, the best performing single fea-
ture type, everything except for the best single fea-
ture type, and two subsets, with the latter primarily
including more abstract linguistic features.
id system description system type
1 overall system ensemble
2 rc. word ng. single model
3 #1 minus rc. word ng. ensemble
4 well performing subset ensemble
5 ?linguistic subset? ensemble
Table 3: Submitted systems for all three tasks
The results for the submitted systems are shown in
Table 4. Here and in the following result tables, the
system ids in the table headers correspond to the ids
in Table 3, the best result on the test set is shown in
bold, and the symbols have the following meaning:
? x = feature type used
? - = feature type not used
? -* = feature type ready after submission
We report theAcctest,Accdev andAcc10train?dev ac-
curacies introduced in section 4.1. The Accdev re-
sults are consistently better than the Acctest results,
highlighting that relying on a single development
set can be problematic. The cross-validation results
are more closely aligned with the ultimate test set
performance.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. x - x - x
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
proficiency x - x x -
prompt x - x x -
Acctest 82.2 79.6 81.0 81.5 74.7
Accdev 85.4 81.3 83.5 84.9 76.3
Acc10train?dev 82.4 78.9 80.7 81.7 74.1
Table 4: Results for the closed task
Overall, comparing the results for the different
systems shows the following main points (with the
system ids in the discussion shown in parentheses):
? The overall system performed better than any
single feature type alone (cf. Tables 2 and 4).
The ensemble thus is successful in combining
the strengths of the different feature types.
? The rc. word ng. feature type alone (2) per-
formed very well, but the overall system with-
out that feature type (3) still outperformed it.
Thus apparently the different properties ac-
cessed by more elaborate linguistic modelling
contribute some information not provided by
the surface-based n-gram feature.
? A system incorporating a subset of the differ-
ent feature types (4) performed still reasonably
well. Hence, it is conceivable that a subsys-
tem consisting of some selected feature types
would perform equally well (eliminating only
information present in multiple feature types)
or even outperform the overall system (by re-
moving some noise). This point will be inves-
tigated in detail in our future work.
202
? System 5, combining a subset of feature types,
where each one incorporates some degree
of linguistic abstraction (in contrast to pure
surface-based feature types such as word-based
n-grams), performed at a reasonably high level,
supporting the assumption that incorporating
more linguistic knowledge into the system de-
sign has something to contribute.
Putting our results into the context of the NLI
Shared Task 2013, with our best Acctest value of
82.2% for closed as the main task, we ranked fifth
out of 29 participating teams. The best result in
the competition, obtained by the team ?Jarvis?, is
83.6%. According to the significance test results
provided by the shared task organizers, the differ-
ence of 1.4% is not statistically significant (0.124
for pairwise comparison using McNemar?s test).
4.5 Open-1 Task Results
The Accdev values for the single feature type models
for the open-1 task were included in Table 2. The
results for the test set are presented in Table 5. We
report two different Acctest values: the accuracy for
the actual submitted systems (Acctest) and for the
corresponding complete systems (Acctest with ?) as
discussed in section 4.1.
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. x - x x -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS -? - -? - -?
12. local trees -? - -? - -?
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 36.4 38.5 33.2 37.8 21.2
Acctest with ? 37.0 n/a 35.4 n/a 29.9
Table 5: Results for the open-1 task
Conceptually, the open-1 task is a cross-corpus
task, where we used the NT11 data for training and
T11 data for testing. It is more challenging for sev-
eral reasons. First, the models are trained on data
that is likely to be different from the one of the
test set in a number of respects, including possible
differences in genre, task and topic, or proficiency
level. Second, the amount of data we were able to
obtain to train our model is far below what was pro-
vided for the closed task. Thus a drop in accuracy is
to be expected.
Particularly interesting is the fact that our best re-
sult for the open-1 task (38.5%) was obtained using
the rc. word ng. feature type alone. Thus adding
the more abstract features did not improve the accu-
racy. The reason for that may be the smaller train-
ing corpus size, the uneven distribution of the texts
among the different L1s in the NT11 corpus, or the
mentioned potential differences between NT11 and
T11 in genre, task and topic, and learner proficiency.
Also interesting is the fact that the system combining
a subset of feature types outperformed the overall
system. This finding supports the assumption men-
tioned in section 4.4 that the ensemble classifier can
be optimized by informed, selective model combina-
tion instead of combining all available information.
To put our results into the context of the NLI
Shared Task 2013, our best Acctest value of 38.5%
for the open-1 task achieved rank two out of three
participating teams. The best accuracy of 56.5% was
obtained by the team ?Toronto?. While the open-
1 task results in general are much lower than the
closed task results, highlighting an important chal-
lenge for future NLI work, they nevertheless are
meaningful steps forward considering the random
baseline of 9%.
4.6 Open-2 Task Results
For the open-2 task we provide the same information
as for open-1. The Accdev values for the single fea-
ture type models are shown in Table 2, and the two
Acctest values, i.e., the accuracy for the actual sub-
mitted systems (Acctest) and for the complete sys-
tems (Acctest with ?) can be found in Table 6.
For the open-2 task, we put the T11 train ?
dev and NT11 sets together to train our models. The
interesting question behind this task is, whether it is
possible to improve the accuracy of NLI by adding
203
systems
Feature type 1 2 3 4 5
1. rc. word ng. x x - x -
2. rc. OCPOS ng. x - x x -
3. rc. word dep. -? - -? -? -
4. rc. func. dep. x - x x -
5. complexity x - x x x
6. stemsuffix, bin. x - x x x
7. stemsuffix, cnt. x - x - x
8. suffix, bin. x - x x x
9. suffix, cnt. x - x - x
10. type dep. lm. -? - -? - -?
11. type dep. POS x - x - x
12. local trees x - x - x
13. dep. num. x - x x -
14. dep. var. x - x x -
15. dep. POS x - x x -
16. lm. realiz. x - x x -
Acctest 83.5 81.0 79.3 82.5 64.8
Acctest with ? 84.5 n/a 83.3 82.9 79.8
Table 6: Results for the open-2 task
data from corpora other than the one used for test-
ing. This is far from obvious, especially considering
the low results obtained for the open-1 task pointing
to significant differences between the T11 and the
NT11 corpora.
Overall, when using all feature types, our results
for the open-2 task (84.5%) are better than those we
obtained for the closed task (82.2%). So adding data
from a different domain improves the results, which
is encouraging since it indicates that something gen-
eral about the language used is being learned, not
(just) something specific to the T11 corpus. Essen-
tially, the open-2 task also is closest to the real-world
scenario of using whatever resources are available to
obtain the best result possible.
Putting the results into the context of the NLI
Shared Task 2013, our best Acctest value of 83.5%
(84.5%) is the highest accuracy for the open-2 task,
i.e, first rank out of four participating teams.
5 Conclusions
We explored the task of Native Language Identifi-
cation using a range of different feature types in the
context of the NLI Shared Task 2013. We consid-
ered surface features such as recurring word-based
n-grams system as our basis. We then explored
the contribution and usefulness of some more elab-
orate, linguistically motivated feature types for the
given task. Using an ensemble model combining
features based on POS, dependency, parse trees as
well as lemma realization, complexity and suffix in-
formation features, we were able to outperform the
high accuracy achieved by the surface-based recur-
ring n-grams features alone. The exploration of
linguistically-informed features thus is not just of
analytic interest but can also make a quantitative dif-
ference for obtaining state-of-the-art performance.
In terms of future work, we have started exploring
the various feature types in depth to better under-
stand the causalities and correlations behind the re-
sults obtained. We also intend to explore more com-
plex linguistically motivated features further, such
as features based on syntactic alternations as used in
Krivanek (2012). Studying such variation of linguis-
tic properties, instead of recording their presence as
we mostly did in this exploration, also stands to pro-
vide a more directly interpretable perspective on the
feature space identified as effective for NLI.
Acknowledgments
We thank Dr. Shin?ichiro Ishikawa and Dr. Mick
Randall for providing access to the ICNALE corpus
and the BALC corpus respectively. We also thank
the shared task organizers for organizing this inter-
esting competition and sharing the TOEFL11 cor-
pus. Our research is partially funded through the Eu-
ropean Commission?s 7th Framework Program un-
der grant agreement number 238405 (CLARA).
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX lexical database (cd-rom). CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native english. Technical report, Edu-
cational Testing Service.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
24th International Conference on Computational Lin-
guistics (COLING), pages 89?97.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. In
204
Learner Corpus Research 2011 (LCR 2011), Louvain-
la-Neuve.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? in-
vestigating abstraction and domain dependence. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 425?
440, Mumbay, India.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation (LREC-2006), Genoa, Italy,
May 24-26.
R.E. Fan, K.W. Chang, C.J. Hsieh, X.R. Wang, and C.J.
Lin. 2008. Liblinear: A library for large linear classi-
fication. The Journal of Machine Learning Research,
9:1871?1874. Software available at http://www.
csie.ntu.edu.tw/?cjlin/liblinear.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and
Magali Paquot, 2009. International Corpus of Learner
English, Version 2. Presses Universitaires de Louvain,
Louvain-la-Neuve.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update. In
The SIGKDD Explorations, volume 11, pages 10?18.
Shin?ichiro Ishikawa. 2011. A new horizon in learner
corpus studies: The aim of the ICNALE projects. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and language technologies in teaching, learning
and research, pages 3?11. University of Strathclyde
Publishing, Glasgow, UK. http://language.
sakura.ne.jp/icnale/index.html.
Scott Jarvis and Magali Paquot. 2012. Exploring the
role of n-grams in L1-identification. In Scott Jarvis
and Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 71?105. Multi-
lingual Matters.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2004. Investigating L1 lexical transfer
through learners? wordprints. Presented at the 2004
Second Language Research Forum. State College,
Pennsylvania, USA.
Scott Jarvis, Gabriela Castan?eda-Jime?nez, and Rasmus
Nielsen. 2012. Detecting L2 writers? L1s on the
basis of their lexical styles. In Scott Jarvis and
Scott A. Crossley, editors, Approaching Language
Transfer through Text Classification: Explorations in
the Detection-based Approach, pages 34?70. Multilin-
gual Matters.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining (KDD ?05), pages 624?628,
New York.
Julia Krivanek. 2012. Investigating syntactic alternations
as characteristic features of learner language. Master?s
thesis, University of Tu?bingen, April.
Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc
Brysbaert. 2012. Age-of-acquisition ratings for
30,000 english words. Behavior Research Methods,
44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Xiaofei Lu. 2010. Automatic analysis of syntactic
complexity in second language writing. International
Journal of Corpus Linguistics, 15(4):474?496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York, April.
John C. Platt. 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector ma-
chines. Technical Report MSR-TR-98-14, Microsoft
Research.
Mick Randall and Nicholas Groom. 2009. The BUiD
Arab learner corpus: a resource for studying the ac-
quisition of L2 english spelling. In Proceedings of the
Corpus Linguistics Conference (CL), Liverpool, UK.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and found:
Resources and empirical evaluations in native lan-
guage identification. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(COLING), pages 2585?2602, Mumbai, India.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP, At-
lanta, GA, USA, June. Association for Computational
Linguistics.
205
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? naive bayes de-
tection of non-native utterance text. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 239?246.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In Joel
Tetreault, Jill Burstein, and Claudial Leacock, editors,
Proceedings of the 7th Workshop on Innovative Use
of NLP for Building Educational Applications (BEA7)
at NAACL-HLT, pages 163?-173, Montre?al, Canada,
June. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automati-
cally grading ESOL texts. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 180?189, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Corpus available from http://ilexir.co.uk/
applications/clc-fce-dataset.
206
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 59?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
On The Applicability of Readability Models to Web Texts
Sowmya Vajjala Detmar Meurers
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
An increasing range of features is being
used for automatic readability classifica-
tion. The impact of the features typically
is evaluated using reference corpora con-
taining graded reading material. But how
do the readability models and the features
they are based on perform on real-world
web texts? In this paper, we want to take a
step towards understanding this aspect on
the basis of a broad range of lexical and
syntactic features and several web datasets
we collected.
Applying our models to web search re-
sults, we find that the average reading level
of the retrieved web documents is rela-
tively high. At the same time, documents
at a wide range of reading levels are iden-
tified and even among the Top-10 search
results one finds documents at the lower
levels, supporting the potential usefulness
of readability ranking for the web. Finally,
we report on generalization experiments
showing that the features we used gener-
alize well across different web sources.
1 Introduction
The web is a vast source of information on a broad
range of topics. While modern search engines
make use of a range of features for identifying and
ranking search results, the question whether a web
page presents its information in a form that is ac-
cessible to a given reader is only starting to receive
attention. Researching the use of readability as-
sessment as a ranking parameter for web search
can be a relevant step in that direction.
Readability assessment has a long history span-
ning various fields of research from Educational
Psychology to Computer Science. At the same
time, the question which features generalize to dif-
ferent types of documents and whether the read-
ability models are appropriate for real-life appli-
cations has only received little attention.
Against this backdrop, we want to see how well
a state-of-the-art readability assessment approach
using a broad range of features performs when ap-
plied to web data. Based on the approach intro-
duced in Vajjala and Meurers (2012), we thus set
out to explore the following two questions in this
paper:
? Which reading levels can be identified in a
systematic sample of web texts?
? How well do the features used generalize to
different web sources?
The paper is organized as follows: Section 2
surveys related work. Section 3 introduces the cor-
pus and the features we used. Section 4 describes
our readability models. Section 5 discusses our ex-
periments investigating the applicability of these
models to web texts. Section 6 reports on a second
set of experiments conducted to test the generaliz-
ability of the features used. Section 7 concludes
the paper with a discussion of our results.
2 Related Work
2.1 Readability Assessment
The need for assessing the readability of a piece
of text has been explored in the educational re-
search community for over eight decades. DuBay
(2006) provides an overview of early readability
formulae, which were based on relatively shallow
features and wordlists. Some of the formulae are
still being used in practice, as exemplified by the
Flesch-Kincaid Grade Level (Kincaid et al, 1975)
available in Microsoft Word.
More recent computational linguistic ap-
proaches view readability assessment as a
59
classification problem and explore different
types of features. Statistical language modeling
has been a popular approach (Si and Callan,
2001; Collins-Thompson and Callan, 2004),
with the hypothesis that the word usage patterns
across grade levels are distinctive enough. Heil-
man et. al. (2007; 2008) extended this approach
by combining language models with manually
and automatically extracted grammatical features.
The relation of text coherence and cohesion
to readability is well explored in the CohMetrix
project (McNamara et al, 2002). Ma et al (2012a;
2012b) approached readability assessment as a
ranking problem and also compared human versus
automatic feature extraction for the task of label-
ing children?s literature.
The WeeklyReader1, an American educational
newspaper with graded readers has been a pop-
ular source of data for readability classification
research in the recent past. Petersen and Osten-
dorf (2009), Feng et al (2009) and Feng (2010)
used it to build readability models with a range
of lexical, syntactic, language modeling and dis-
course features. In Vajjala and Meurers (2012)
we created a larger corpus, WeeBit, by combining
WeeklyReader with graded reading material from
the BBCBitesize website.2 We adapted measures
of lexical richness and syntactic complexity from
Second Language Acquisition (SLA) research as
features for readability classification and showed
that such measures of proficiency can successfully
be used as features for readability assessment.
2.2 Readability Assessment of Web Texts
Despite the significant body of research on read-
ability assessment, applying it to retrieve relevant
texts from the web has elicited interest only in the
recent past. While Benno?hr (2005) and Newbold
et al (2010) created new readability formulae for
this purpose, Ott and Meurers (2010) and Tan et
al. (2012) used existing readability formulae to fil-
ter search engine results. The READ-X project
(Miltsakaki and Troutt, 2008; Miltsakaki, 2009)
combined standard readability formulae with topic
classification to retrieve relevant texts for users.
The REAP Project3 supports the lexical acqui-
sition of individual learners by retrieving texts that
suit a given learner level. Kidwell et al (2011) also
1http://weeklyreader.com
2http://www.bbc.co.uk/bitesize
3http://reap.cs.cmu.edu
used a word-acquisition model for readability pre-
diction. Collins-Thompson et al (2011) and Kim
et al (2012) employed word distribution based
readability models for personalized search and for
creating entity profiles respectively. Nakatani et
al. (2010) followed a language modeling approach
to rank search results to take user comprehension
into account. Google also has an option to filter
search results based on reading level, apparently
using a language modeling approach.4 Kanungo
and Orr (2009) used search result snippet based
features to predict the readability of short web-
summaries.
All the above approaches primarily restrict
themselves to traditional formulae or statistical
language models encoding the distribution of
words. The effect of lexical and syntactic features
as used in recent research on readability thus re-
mains to be studied in a web context. Furthermore,
the generalizability of the features used to other
data sets also remains to be explored. These are
the primary issues we address in this paper.
3 Corpus and Features
Let us turn to answering our first question: Which
reading levels can be identified in a systematic
sample of web texts? To address this question, we
first need to introduce the features we used, the
graded corpus we used to train the model, and the
nature of the readability model.
Since the goal of this paper is not to present
new features but to explore the application of a
readability approach to the web, we here simply
adopt the feature and corpus setup introduced in
Vajjala and Meurers (2012). The WeeBit corpus
used is a corpus of texts belonging to five reading
levels, corresponding to children of age group 7?
16 years. It consists of 625 documents per reading
level. The articles cover a range of fiction and non-
fiction topics. Each article is labeled as belong-
ing to one of five reading levels: Level 2, Level 3,
Level 4, KS3 and GCSE.
We adapted both the lexical and syntactic fea-
tures of Vajjala and Meurers (2012) to build read-
ability models on the basis of the WeeBit corpus
and then studied their applicability to real-world
documents retrieved from the web as well as the
applicability of those features across different web
sources.
4http://goo.gl/aVy93
60
Lexical features (LEXFEATURES) The lexical
features are motivated by the lexical richness mea-
sures used to estimate the quality of language
learners? oral narratives (Lu, 2012). We included
several type-token ratio variants used in SLA re-
search: generic type token ratio, root TTR, cor-
rected TTR, bilogarithmic TTR and Uber Index.
In addition, there are lexical variation measures
used to estimate the distribution of various parts
of speech in the given text. They include the
noun variation, adjective variation, modifier vari-
ation, adverb variation and verb variation, which
represent the proportion of words of the respec-
tive part of speech categories compared to all lex-
ical words in the document. Alternative measures
for verb variation, namely, Squared Verb Variation
and Corrected Verb Variation are also included.
Apart from these, we also added the traditionally
used measures of average number of characters
per word, average number of syllables per word,
and two readability formulae, the Flesch-Kincaid
score (Kincaid et al, 1975) and the Coleman-Liau
score (Coleman and Liau, 1975). Finally, we in-
cluded the percentage of words from the Aca-
demic Word List5. It is a list created by Coxhead
(2000) which consists of words that are more com-
monly found in academic texts.
Syntactic features (SYNFEATURES) These
features are adapted from the syntactic complexity
measures used to analyze second language writing
(Lu, 2010). They are calculated based on the
parser output of the BerkeleyParser (Petrov and
Klein, 2007), using the Tregex (Levy and Andrew,
2006) pattern matcher. They include: mean
lengths of various production units (sentence,
clause and t-unit); clauses per sentence and t-unit;
t-units per sentence; complex-t units per t-unit
and per sentence; dependent clauses per clause,
t-unit and sentence; co-ordinate phrases per
clause, t-unit and sentence; complex nominals per
clause and t-unit; noun phrases, verb phrases and
preposition phrases per sentence; average length
of NP, VP and PP; verb phrases per t-unit; SBARs
per sentence and average parse tree height.
We refer to the feature subset containing all
the traditionally used features (# char. per word,
# syll. per word and # words per sentence) as
TRADFEATURES in this paper.
5http://simple.wiktionary.org/wiki/
Wiktionary:Academic_word_list
4 The Readability Model
In computational linguistics, readability assess-
ment is generally approached as a classification
problem. To our knowledge, only Heilman et al
(2008) and Ma et al (2012a) experimented with
other kinds of statistical models.
We approach readability assessment as a regres-
sion problem. This produces a model which pro-
vides a continuous estimate of the reading level,
enabling us to see if there are documents that fall
between two levels or above the maximal level
found in the training data. We used the WEKA
implementation of linear regression for this pur-
pose. Since linear regression assumes that the data
falls on an interval scale with evenly spaced read-
ing levels, we used numeric values from 1?5 as
reading levels instead of the original class names
in the WeeBit corpus. Table 1 shows the mapping
from WeeBit classes to numeric values, along with
the age groups per class.
WeeBit class Age (years) Reading level
Level 2 7?8 1
Level 3 8?9 2
Level 4 9?10 3
KS3 11?14 4
GCSE 14?16 5
Table 1: WeeBit Reading Levels for Regression
We report Pearson?s correlation coefficient and
Root Mean Square Error (RMSE) as our evalua-
tion metrics. Correlation coefficient measures the
extent of linear relationship between two random
variables. In readability assessment, a high corre-
lation indicates that the texts at a higher difficulty
level are more likely to receive a higher level pre-
diction from the model and those at lower diffi-
culty level would more likely receive a lower pre-
diction. RMSE can be interpreted as the aver-
age deviation in grade levels between the predicted
and the actual values.
We trained four regression models with the fea-
ture subsets introduced in section 3: LEXFEA-
TURES, SYNFEATURES, TRADFEATURES and
ALLFEATURES. While the criterion used in cre-
ating the graded texts in WeeBit is not known, it
is likely that they were created with the traditional
measures in mind. Indeed, the traditional features
also were among the most predictive features in
Vajjala and Meurers (2012). Hence, apart from
61
training the above mentioned four regression mod-
els, we also trained a fifth model excluding the tra-
ditional features and formulae. This experiment
was performed to verify if the traditional features
are creating a skewed model that relies too heavily
on those well-known and thus easily manipulated
features in making decisions on test data. We refer
to this fifth feature group as NOTRAD.
Table 2 shows the result of our regression ex-
periments using 10-fold cross-validation on the
WeeBit corpus, employing the different feature
subsets and the complete feature set.
Feature Set # Features Corr. RMSE
LEXFEATURES 17 0.84 0.78
SYNFEATURES 25 0.88 0.64
TRADFEATURES 3 0.66 1.06
ALLFEATURES 42 0.92 0.54
NOTRAD 37 0.89 0.63
Table 2: Linear Regression Results for WeeBit
The best correlation of 0.92 was achieved with
the complete feature set. 0.92 is considered a
strong correlation and coupled with an RMSE of
0.54, we can conclude that our regression model
is a good model. In comparison, in Vajjala and
Meurers (2012), where we tackle readability as-
sessment as a classification problem, we obtained
93.3% accuracy on this dataset using all features.
Looking at the feature subsets, there also is a
good correlation between the model predictions
and the actual results in the other cases, except
for the model considering only traditional features.
While traditional features often are among the
most predictive features in readability research,
we also found that a model which does not include
them can perform at a comparable level (0.89).
Comparing these results with previous research
using regression modeling for readability assess-
ment is not particularly meaningful because of the
differences in the corpus and the levels used. For
example, while Heilman et al (2008) used a cor-
pus of 289 texts across 12 reading levels achieving
a correlation of 0.77, we used the WeeBit corpus
containing 3125 texts across 5 reading levels.6
We took the two best models of Table 2,
MODALL using ALLFEATURES and MODNO-
TRAD using the NOTRAD feature set, and set out
to answer our first guiding question, about the
6Direct comparisons on the same data set would be most
indicative, but many datasets, such as the corpus used in Heil-
man et al (2008), are not accessible due to copyright issues.
reading levels which such models can identify in a
systematic sample of web texts.
5 Applying readability models to web texts
To investigate the effect of the two readability
models for real-world web texts, we studied their
performance on two types of web data:
? web documents we crawled from specific
web sites that offer the same type of material
for two groups of readers differing in their
reading skills
? web documents identified by a web search
engine for a sample of web queries selected
from a public query log
5.1 Readability of web data drawn from
characteristic web sites
5.1.1 Web test sets used
Following the approach of Collins-Thompson and
Callan (2005) and Sato et al (2008), who eval-
uated readability models using independent web-
based test sets, we compiled three sets of web doc-
uments that given their origin can be classified into
two classes each:
Wiki ? SimpleWiki: Wikipedia7, along with its
manually simplified version Simple Wikipedia8 is
increasingly used in two-class readability classi-
fication tasks and text simplification approaches
(Napoles and Dredze, 2010; Zhu et al, 2010;
Coster and Kauchak, 2011). We use a collection
of 2000 randomly selected parallel articles from
each of the two websites, which in the following
is referred to as WIKI and SIMPLEWIKI.
Time ? Time for Kids: Time for Kids9 is a divi-
sion of the TIME magazine10, which produces ar-
ticles exclusively for children and is used widely
in classrooms. We took a sample of 2000 docu-
ments each from Time and from Time for Kids for
our experiments and refer them TIME and TFK.
NormalNews ? ChildrensNews: We crawled
websites that contain news articles written for chil-
dren (e.g., http://www.firstnews.co.uk) and
categorized them as CHILDRENSNEWS. We also
crawled freely accessible articles from popular
news websites such as BBC or The Guardian and
7http://en.wikipedia.org
8http://simple.wikipedia.org
9http://www.timeforkids.com
10http://www.time.com
62
categorized them as NORMALNEWS. We took
10K documents from each of these two categories
for our experiments.
These three corpus pairs collected as test cases
differ in several aspects. For example, Sim-
pleWikipedia is not targeting children as such,
whereas Time for Kids and ChildrensNews are.
And SimpleWikipedia ? Wikipedia covers paral-
lel articles in two versions, whereas this is not
the case for the the two Time and the two News
corpora. However, as far as we see these differ-
ences are orthogonal to the issue we are research-
ing here, namely their use as real-life test cases to
study the effect of the classification model learned
on the WeeBit data.
We applied the two regression models which
had performed best on the WeeBit corpus (cf. Ta-
ble 2 in section 4) to these web datasets. The aver-
age reading levels of the different datasets accord-
ing to these two models are reported in Table 3.
Data Set MODALL MODNOTRAD
SIMPLEWIKI 3.86 2.67
TFK 4.15 2.72
CHILDRENSNEWS 4.19 2.39
WIKI 4.21 3.33
TIME 5.04 4.07
NORMALNEWS 5.58 4.42
Table 3: Applying the WeeBit regression model to
the six web datasets
The table shows that both MODALL and MOD-
NOTRAD place the documents from the children
websites (SIMPLEWIKI, TFK and CHILDREN-
SNEWS) at lower reading levels than those from
 0
 10
 20
 30
 40
 50
 60
1 2 3 4 5 higher
% o
f do
cum
ent
s be
long
ing 
to a
 rea
ding
 lev
el
Reading level
Distribution of reading levels across web texts with traditional features
SimpleWikiTFKChildrensNewsWikiTimeNormalNews
Figure 1: Reading levels assigned by MODALL
the regular websites for adults (TIME, WIKI and
NORMALNEWS). However, there is an interesting
difference in the predictions made by the two mod-
els. The MODALL model including the traditional
features consistently assigns a higher reading level
to all the documents, and it also fails to separate
CHILDRENSNEWS (4.19) from WIKI (4.20).
To be able to inspect this in detail, we plot-
ted the class-wise reading level distribution of our
regression models. Figure 1 shows the distribu-
tion of reading levels for these web datasets using
MODALL. As we already knew from the averages,
the model assigns somewhat higher reading levels
to all documents, and the figure confirms that the
texts for children (SIMPLEWIKI, TFK and CHIL-
DRENSNEWS) are only marginally distinguished
from the corresponding websites targeting adult
readers (TIME,WIKI and NORMALNEWS). The
NORMALNEWS dataset alo seems to be placed
in a much higher distribution compared to all the
other test sets, with more than 50% of the docu-
ments getting a prediction of ?higher? (the label
used for documents placed at level 6 or higher).
Figure 2 shows the distribution of reading levels
across the test sets according to MODNOTRAD,
the model without traditional features. The model
provides a broader coverage across all reading lev-
els, with documents from children web sites and
SimpleWikipedia clearly being placed at the lower
end of the spectrum and web pages targeting adults
at the higher end. NORMALNEWS documents are
again placed the highest, but less than 10% fall
outside the range established by WeeBit. TIME
shows the highest diversity, with around 20% for
each reading level above the lowest one.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
1 2 3 4 5 higher
% o
f do
cum
ent
s be
long
ing 
to a
 rea
ding
 lev
el
Reading level
Distribution of reading levels across web texts without traditional features
SimpleWikiTFKChildrensNewsWikiTimeNormalNews
Figure 2: Reading levels using MODNOTRAD
63
The first set of experiments shows that the
readability models which were successful on the
WeeBit reference corpus seem to be able to iden-
tify a corresponding broad range among web doc-
uments that we selected top-down by relying on
prototypical websites targeting ?adult? and ?child?
readers, which are likely to feature more difficult
and easier web documents, respectively. While
we cannot evaluate the difference between the two
models quantitatively, given the lack of an external
gold standard classification of the crawled data,
the MODNOTRAD conceptually seems to do a bet-
ter job at distinguishing the two classes of web-
sites in line with the top-down expectations.
5.2 Readability of search results
Complementing the first set of experiments, estab-
lishing that the readability models are capable of
placing web documents in line with the top-down
classification of the sites they originate from, in
the second set of experiments we want to investi-
gate bottom-up whether for some random topics of
interest, the web offers texts at different readabil-
ity levels. This also is of practical relevance, since
ranking web search results by readability is only
useful if there actually are documents at different
reading levels for a given query.
For this investigation, we took the MOD-
NOTRAD model and used it to estimate the
reading level of web search results. For
web searching, we used the BING search
API (http://datamarket.azure.com/dataset/
bing/search) and computed the reading levels
of the Top-100 search results for a sample of 50
test queries, selected from a publicly accessible
database (Lu and Callan, 2003).
Figure 3 characterizes the data obtained through
the web searches in terms of the percentage of doc-
Figure 3: Documents retrieved per reading level
uments belonging to a given reading level, accord-
ing to the MODNOTRAD model. In the Top-100
search results obtained for each of the 50 queries,
the model identifies documents at all reading lev-
els, with a peak at reading level 4 (corresponding
to KS3 in the original WeeBit dataset).
To determine how much individual queries dif-
fer in terms of the readability of the documents
they retrieve, we also looked at the results for each
query separately. Figure 4 shows the mean read-
ing level of the Top-100 results for each of the 50
search queries. From query to query, the aver-
age readability of the documents retrieved seems
to differ relatively little, with most results falling
into the higher reading levels (4 or above).
Figure 4: Average reading level of search results
Returning to the question whether there are
documents of different reading levels for a given
query, we need to check how much variation exists
around the observed, rather similar averages. Ta-
ble 4 provides the individual reading levels of the
Top-10 search results for a sample of 10 queries
from our experiment, along with the average read-
ing level of the Top-100 results for that query. The
results in Table 4 indicate that indeed there are
documents at a broad range of reading levels even
among the most relevant search results returned by
the BING web search engine.
Looking at the individual query results, we
found that although a lot of news documents
tended towards a higher reading level, it is in-
deed possible to find some texts at lower read-
ing levels even within Top-10 results (indicated in
bold). However, we found that even for queries
that we would expect to result in hits from web-
sites targeting child readers, those sites often did
not make it into the Top-10 results. The same was
true for sites offering ?simple? language, such as
Simple Wikipedia, which was not among the top
64
Result Rank? 1 2 3 4 5 6 7 8 9 10 AvgTop100
Query
local anaesthetic 3.18 4.57 5.35 3.09 4.24 4.6 3.95 4.74 2.72 4.73 3.78
copyright copy law 1.77 4.59 1.43 2.67 4.63 6.2 2.69 1.1 3.87 5.61 4.57
halley comet 1.69 4.47 4.54 4.24 2.37 4.1 4.86 3.56 4.21 3.56 4.04
public offer 4.4 4.35 5.06 5.03 4.36 5.16 4.13 4.67 3.81 1.1 4.39
optic sensor 2.67 3.38 4.5 3.17 2.54 4.19 4.84 1.47 2.2 3.31 3.83
europe union politics 3.61 4.9 6.3 4.02 2.17 4.5 1.47 1.58 4.88 6.33 4.33
presidential poll 4.98 5.38 1.77 6.1 4.76 3.82 1.05 5.11 3.92 4.25 3.95
shakespeare 2.39 2.9 4.2 4.74 4.76 3.89 1.47 2.13 2.6 4.06 3.58
air pollution 1.17 4.93 3.7 2.3 4.36 3.73 3.71 3.49 2.22 2.67 4.21
euclidean geometry 3.88 4.71 4.7 4.3 4.45 4.63 4.04 4.1 3.48 2.58 3.18
Table 4: Reading levels of individual search results
results even when it contained pages directly rel-
evant to the query. To provide access to those
pages, reranking the search results based on read-
ability would thus be of value.
While we do not want to jump to conclusions
based on our sample of 50 queries, the results
of our experiments seem to support the idea that
readability-based re-ranking of web search results
can help users in accessing web documents that
also are at the right level for the given user. Re-
turning to the first overall question that lead us
here, our experiments support the answer that in-
deed there are documents spread across different
reading levels on the web with a tendency towards
higher reading levels.
6 Generalizability of the Feature Set
We can now turn to the second question raised in
the introduction: How well do the features gener-
alize across different classes of web documents?
We saw in section 5.1 that the predictions of the
two models we used varied quite a bit, solely
based on whether the traditional readability fea-
tures were included in the model or not. This con-
firms the need to investigate how generally appli-
cable which types of features are across datasets.
As far as we know, such an experiment vali-
dating the generalizability of features was not yet
performed in this domain. As there are no pub-
licly available graded web datasets to build new
readability models with the same feature set, we
used the datasets we introduced in section 5.1.1 for
creating two-class readability classification mod-
els. Since there are no clear age-group annota-
tions with all these datasets, we decided to use a
broad two-level classification instead of more fine
grained grade levels.
The difference between this experiment and the
previous one lies in the primary question it at-
tempts to answer. Here, the focus is on veri-
fying if the features are capable of building ac-
curate classification models on different training
sets. In the previous experiment, it was on check-
ing if a given classification model (which in that
experiment was trained on the WeeBit corpus) can
successfully discriminate reading levels for docu-
ments from various real-world texts.
We observed in Section 5.1 that with traditional
features, the WeeBit based readability model as-
signed higher reading levels to all the documents
from our web datasets. So, it would perhaps be
a natural step to train these binary classification
models excluding the traditional features. How-
ever, the traditional features may still be useful
(with different weights) for constructing classifi-
cation models with other training data. So, we
trained two sets of models per training set ? one
with ALLFEATURES and another excluding tradi-
tional features (NOTRAD).
We trained binary classification models using
the following training sets:
? TIME ? TFK texts
? WIKI ? SIMPLEWIKI texts
? NORMALNEWS ? KIDSNEWS texts
? TIME+WIKI ? TFK+SIMPLEWIKI texts
We used the Sequential Minimal Optimization
(SMO) algorithm implementation in the WEKA
tool kit to train these classifiers. The choice of
the algorithm here was motivated by the fact that
training is quick and that SMO has successfully
65
been used in previous research on readability as-
sessment (Feng, 2010; Hancke et al, 2012).
Table 5 summarizes the classification accura-
cies obtained with the four models using 10-fold
cross validation for the four web corpora.
Training Set Accuracy-All Accuracy-NoTrad
TIME ? TFK 95.11% 89.52%
WIKI ? SIMPLEWIKI 92.32% 88.81%
NORMALNEWS ? KIDSNEWS 97.93% 92.54%
TIME+WIKI ? TFK+SIMPLEWIKI 93.38% 89.72%
Table 5: Cross-validation accuracies for binary
classification on different web corpora
The results in the table show that the same set
of features consistently result in creating accu-
rate classification models for all four web corpora.
Each of the two-class classification models per-
formed well, despite the fact that the documents
were created by different people and most likely
with different instructions on how to write sim-
ple texts or simplify already existing texts. It was
interesting to note the role of traditional features
in improving the accuracy of these binary classi-
fication models. But, in the previous experiment,
the model with traditional features consistently put
all the documents into higher reading levels. It is
possible that the role of traditional features in the
WeeBit corpus may be skewed as it is likely that it
was prepared with traditional readability measures
in mind. Contrasting the results of these two ex-
periments raises the question of what features hold
more weight in what dataset, which is an interest-
ing issue to explore in the future.
In sum, this experiment provides some clear
evidence for affirmatively answering the second
question about the generalizability of the feature
set we used. The features seem to be sufficiently
general for them to be useful in performing read-
ability assessment of real-world documents.
7 Conclusion and Discussion
In this paper, we set out to investigate the appli-
cability and generalizability of readability models
for real-world web texts. We started with build-
ing readability models using linear regression, on
a 5-level readability corpus with a range of lexi-
cal and syntactic features (section 4). We applied
the two best models thus obtained to several web
datasets we compiled from websites targeting chil-
dren and others designed for adults (section 5.1)
and on the Top-100 results obtained using a stan-
dard web search engine (section 5.2).
We observed that the models identified texts
across a broad range of reading levels in the web
corpora. Our pilot study of the reading levels of
the search results confirmed that readability mod-
els could be useful as re-ranking or filtering pa-
rameters that prioritize relevant results which are
at the right level for a given user. At the same
time, we observed in both these experiments that
the average reading level of general web articles
is relatively high according to our models. Apart
from result ranking, this also calls for the construc-
tion of efficient text simplification systems which
pick up the difficult texts and attempt to simplify
them to a given reading level.
We then proceeded to investigate how well
the features used to build these readability mod-
els generalize across different corpora. For this,
we reused the corpora with articles for children
and adult readers from prototypical websites (sec-
tion 5.1.1) and built four binary classification
models with all of the readability features (sec-
tion 6). Each of the models achieved good clas-
sification accuracies, supporting that the broad
feature set used generalizes well across corpora.
Whether or not to use traditional readability fea-
tures is somewhat difficult to answer since those
formulae are often taken into account when writ-
ing materials, so high classification accuracy on
such corpora may be superficial in that it is not
necessarily indicative of the spectrum of texts
found on the web (section 5.1). This also raises
the more general question which features work
best for which kind of dataset. A systematic ex-
ploration of the effect of the individual features
along with the impact of document topic and genre
on readability would be interesting and relevant to
pursue in the future.
In our future work, we also intend to explore
further features for this task and improve our un-
derstanding of the correlations between the differ-
ent features. Finally, we are considering reformu-
lating readability assessment as ordinal regression
or preference ranking.
Acknowledgements
We would like to thank the anonymous reviewers
for their detailed, useful comments on the paper.
This research was funded by the European Com-
mission?s 7th Framework Program under grant
agreement number 238405 (CLARA).
66
References
Jasmine Benno?hr. 2005. A web-based personalised
textfinder for language learners. Master?s thesis,
School of Informatics, University of Edinburgh.
Meri Coleman and T. L. Liau. 1975. A computer read-
ability formula designed for machine scoring. Jour-
nal of Applied Psychology, 60:283?284.
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting read-
ing difficulty. In Proceedings of HLT/NAACL 2004,
Boston, USA.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical lan-
guage models. Journal of the American Society for
Information Science and Technology, 56(13):1448?
1462.
K. Collins-Thompson, P. N. Bennett, R. W. White,
S. de la Chica, and D. Sontag. 2011. Personaliz-
ing web search results by reading level. In Proceed-
ings of the Twentieth ACM International Conference
on Information and Knowledge Management (CIKM
2011).
William Coster and David Kauchak. 2011. Simple en-
glish wikipedia: A new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 665?669, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Averil Coxhead. 2000. A new academic word list.
Teachers of English to Speakers of Other Languages,
34(2):213?238.
William H. DuBay. 2006. The Classic Readability
Studies. Impact Information, Costa Mesa, Califor-
nia.
Lijun Feng, Nomie Elhadad, and Matt Huenerfauth.
2009. Cognitively motivated features for readability
assessment. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 229?237, Athens, Greece, March. Association
for Computational Linguistics.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING), pages 1063?
1080, Mumbay, India.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combin-
ing lexical and grammatical features to improve
readability measures for first and second language
texts. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL-07), pages 460?467, Rochester, New York.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the 3rd Workshop on Innovative
Use of NLP for Building Educational Applications
at ACL-08, Columbus, Ohio.
Tapas Kanungo and David Orr. 2009. Predicting the
readability of short web summaries. In Proceed-
ings of the Second ACM International Conference
on Web Search and Data Mining, WSDM ?09, pages
202?211, New York, NY, USA. ACM.
P. Kidwell, G. Lebanon, and K. Collins-Thompson.
2011. Statistical estimation of word acquisition with
application to readability prediction. In Journal of
the American Statistical Association. 106(493):21-
30.
Jin Young Kim, Kevyn Collins-Thompson, Paul N.
Bennett, and Susan T. Dumais. 2012. Characteriz-
ing web content, user interests, and search behavior
by reading level and topic. In Proceedings of the
fifth ACM international conference on Web search
and data mining, WSDM ?12, pages 213?222, New
York, NY, USA. ACM.
J. P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, and
B. S Chissom. 1975. Derivation of new readability
formulas (Automated Readability Index, Fog Count
and Flesch Reading Ease formula) for Navy enlisted
personnel. Research Branch Report 8-75, Naval
Technical Training Command, Millington, TN.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Jie Lu and Jamie Callan. 2003. Content-based retrieval
in hybrid peer-to-peer networks. In Proceedings of
the Twelfth International Conference on Information
and Knowledge Management (CIKM?03).
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal, pages 190?208.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012a.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of the
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12,
pages 548?552, Stroudsburg, PA, USA. Association
for Computational Linguistics.
67
Yi Ma, Ritu Singh, Eric Fosler-Lussier, and Robert
Lofthus. 2012b. Comparing human versus au-
tomatic feature extraction for fine-grained elemen-
tary readability assessment. In Proceedings of the
First Workshop on Predicting and Improving Text
Readability for target reader populations, PITR ?12,
pages 58?64, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Danielle S. McNamara, Max M. Louwerse, and
Arthur C. Graesser. 2002. Coh-metrix: Auto-
mated cohesion and coherence scores to predict text
readability and facilitate comprehension. Proposal
of Project funded by the Office of Educational Re-
search and Improvement, Reading Program.
Eleni Miltsakaki and Audrey Troutt. 2008. Real time
web text classification and analysis of reading dif-
ficulty. In Proceedings of the Third Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (BEA-3) at ACL?08, pages 89?97, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
Eleni Miltsakaki. 2009. Matching readers? prefer-
ences and reading skills with appropriate web texts.
In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics: Demonstrations Session, EACL
?09, pages 49?52, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Makoto Nakatani, Adam Jatowt, and Katsumi Tanaka.
2010. Adaptive ranking of search results by consid-
ering user?s comprehension. In Proceedings of the
4th International Conference on Ubiquitous Infor-
mation Management and Communication (ICUIMC
2010), pages 182?192. ACM Press, Suwon, Korea.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&W ?10, pages 42?50, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Neil Newbold, Harry McLaughlin, and Lee Gillam.
2010. Rank by readability: Document weighting for
information retrieval. In Hamish Cunningham, Al-
lan Hanbury, and Stefan Ru?ger, editors, Advances in
Multidisciplinary Retrieval, volume 6107 of Lecture
Notes in Computer Science, pages 20?30. Springer
Berlin / Heidelberg.
Niels Ott and Detmar Meurers. 2010. Information re-
trieval for education: Making search engines lan-
guage aware. Themes in Science and Technology
Education. Special issue on computer-aided lan-
guage analysis, teaching and learning: Approaches,
perspectives and applications, 3(1?2):9?30.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assessment.
Computer Speech and Language, 23:86?106.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Satoshi Sato, Suguru Matsuyoshi, and Yohsuke Kon-
doh. 2008. Automatic assessment of japanese text
readability based on a textbook corpus. In LREC?08.
Luo Si and Jamie Callan. 2001. A statistical model for
scientific readability. In Proceedings of the 10th In-
ternational Conference on Information and Knowl-
edge Management (CIKM), pages 574?576. ACM.
Chenhao Tan, Evgeniy Gabrilovich, and Bo Pang.
2012. To each his own: Personalized content se-
lection based on text comprehensibility. In In Pro-
ceedings of WSDM.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Joel Tetreault, Jill Burstein, and Claudial Leacock,
editors, Proceedings of the 7th Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions (BEA7) at NAACL-HLT, pages 163?173, Mon-
tral, Canada, June. Association for Computational
Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China.
68
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 21?29,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Exploring Measures of ?Readability? for Spoken Language:
Analyzing linguistic features of subtitles
to identify age-specific TV programs
Sowmya Vajjala and Detmar Meurers
LEAD Graduate School, Department of Linguistics
University of T?ubingen
{sowmya,dm}@sfs.uni-tuebingen.de
Abstract
We investigate whether measures of read-
ability can be used to identify age-specific
TV programs. Based on a corpus of BBC
TV subtitles, we employ a range of lin-
guistic readability features motivated by
Second Language Acquisition and Psy-
cholinguistics research.
Our hypothesis that such readability fea-
tures can successfully distinguish between
spoken language targeting different age
groups is fully confirmed. The classifiers
we trained on the basis of these readability
features achieve a classification accuracy
of 95.9%. Investigating several feature
subsets, we show that the authentic mate-
rial targeting specific age groups exhibits
a broad range of linguistics and psycholin-
guistic characteristics that are indicative of
the complexity of the language used.
1 Introduction
Reading, listening, and watching television pro-
grams are all ways to obtain information partly en-
coded in language. Just like books are written for
different target groups, current TV programs target
particular audiences, which differ in their interests
and ability to understand language. For books and
text in general, a wide range of readability mea-
sures have been developed to determine for which
audience the information encoded in the language
used is accessible. Different audiences are com-
monly distinguished in terms of the age or school
level targeted by a given text.
While for TV programs the nature of the inter-
action between the audio-visual presentation and
the language used is a relevant factor, in this pa-
per we want to explore whether the language by
itself is equally characteristic of the particular age
groups targeted by a given TV program. We thus
focused on the language content of the program
as encoded in TV subtitles and explored the role
of text complexity in predicting the intended age
group of the different programs.
The paper is organized as follows. Section 2
introduces the corpus we used, and section 3 the
readability features employed and their motiva-
tion. Section 4 discusses the experimental setup,
the experiments we conducted and their results.
Section 5 puts our research into the context of re-
lated work, before section 6 concludes and pro-
vides pointers to future research directions.
2 Corpus
The BBC started subtitling all the scheduled pro-
grams on its main channels in 2008, implement-
ing UK regulations designed to help the hearing
impaired. Van Heuven et al. (2014) constructed a
corpus of subtitles from the programs run by nine
TV channels of the BBC, collected over a period
of three years, January 2010 to December 2012.
They used this corpus to compile an English word
frequencies database SUBTLEX-UK
1
, as a part of
the British Lexicon Project (Keuleers et al., 2012).
The subtitles of four channels (CBeebies, CBBC,
BBC News and BBC Parliament) were annotated
with the channel names.
While CBeebies targets children aged under 6
years, CBBC telecasts programs for children 6?12
years old. The other two channels (News, Parlia-
ment) are not assigned to a specific age-group, but
it seems safe to assume that they target a broader,
adult audience. In sum, we used the BBC subtitle
corpus with a three-way categorization: CBeebies,
CBBC, Adults.
Table 1 shows the basic statistics for the overall
corpus. For our machine learning experiments, we
use a balanced subcorpus with 3776 instances for
each class. As shown in the table, the programs for
1
http://crr.ugent.be/archives/1423
21
Program Category Age group # texts avg. tokens avg. sentence length
per text (in words)
CBEEBIES < 6 years 4846 1144 4.9
CBBC 6?12 years 4840 2710 6.7
Adults (News + Parliament) > 12 years 3776 4182 12.9
Table 1: BBC Subtitles Corpus Description
the older age-groups tend to be longer (i.e., more
words per text) and have longer sentences. While
text length and sentence length seem to constitute
informative features for predicting the age-group,
we hypothesized that other linguistic properties of
the language used may be at least as informative as
those superficial (and easily manipulated) proper-
ties. Hence, we explored a broad linguistic feature
set encoding various aspects of complexity.
3 Features
The feature set we experimented with consists of
152 lexical and syntactic features that are primar-
ily derived from the research on text complexity
in Second Language Acquisition (SLA) and Psy-
cholinguistics. There are four types of features:
Lexical richness features (LEX): This group
consists of various part-of-speech (POS) tag den-
sities, lexical richness features from SLA research,
and the average number of senses per word.
Concretely, the POS tag features are: the pro-
portion of words belonging to different parts of
speech (nouns, proper nouns, pronouns, determin-
ers, adjectives, verbs, adverbs, conjunctions, in-
terjections, and prepositions) and different verb
forms (VBG, VBD, VBN, VBP in the Penn Tree-
bank tagset; Santorini 1990) per document.
The SLA-based lexical richness features we
used are: type-token ratio and corrected type-
token ratio, lexical density, ratio of nouns, verbs,
adjectives and adverbs to the number of lexical
words in a document, as described in Lu (2012).
The POS information required to extract these
features was obtained using Stanford Tagger
(Toutanova et al., 2003). The average number of
senses for a non-function word was obtained by
using the MIT WordNet API
2
(Finlayson, 2014).
Syntactic complexity features (SYNTAX): This
group of features encodes the syntactic complex-
ity of a text derived from the constituent struc-
ture of the sentences. Some of these features are
2
http://projects.csail.mit.edu/jwi
derived from SLA research (Lu, 2010), specif-
ically: mean lengths of production units (sen-
tence, clause, t-unit), sentence complexity ratio
(# clauses/sentence), subordination in a sentence
(# clauses per t-unit, # complex t-units per t-unit,
# dependent clauses per clause and t-unit), co-
ordination in a sentence (# co-ordinate phrases
per clause and t-unit, # t-units/sentence), and spe-
cific syntactic structures (# complex nominals per
clause and t-unit, # VP per t-unit). Other syntactic
complexity features we made use of are the num-
ber of NPs, VPs, PPs, and SBARs per sentence
and their average length (in terms of # words), the
average parse tree height and the average number
of constituents per sub-tree.
All of these features were extracted using the
Berkeley Parser (Petrov and Klein, 2007) and the
Tregex pattern matcher (Levy and Andrew, 2006).
While the selection of features for these two
classes is based on Vajjala and Meurers (2012), for
the following two sets of features, we explored fur-
ther information available through psycholinguis-
tic resources.
Psycholinguistic features (PSYCH): This group
of features includes an encoding of the average
Age-of-acquisition (AoA) of words according to
different norms as provided by Kuperman et al.
(2012), including their own AoA rating obtained
through crowd sourcing. It also includes mea-
sures of word familiarity, concreteness, imageabil-
ity, meaningfulness and AoA as assigned in the
MRC Psycholinguistic database
3
(Wilson, 1988).
For each feature, the value per text we computed
is the average of the values for all the words in the
text that had an entry in the database.
While these measures were not developed with
readability analysis in mind, we came across one
paper using such features as measures of word
difficulty in an approach to lexical simplification
(Jauhar and Specia, 2012).
3
http://www.psych.rl.ac.uk/
22
Celex features (CELEX): The Celex lexical
database (Baayen et al., 1995) for English con-
sists of annotations for the morphological, syntac-
tic, orthographic and phonological properties for
more than 50k words and lemmas. We included
all the morphological and syntactic properties that
were encoded using character or numeric codes in
our feature set. We did not use frequency informa-
tion from this database.
In all, this feature set consists of 35 morpholog-
ical and 49 syntactic properties per lemma. The
set includes: proportion of morphologically com-
plex words, attributive nouns, predicative adjec-
tives, etc. in the text. A detailed description of
all the properties of the words and lemmas in this
database can be found in the Celex English Lin-
guistic Guide
4
.
For both the PSYCH and CELEX features,
we encode the average value for a given text.
Words which were not included in the respec-
tive databases were ignored for this computation.
On average, around 40% of the words from texts
for covered by CELEX, 75% by Kuperman et al.
(2012) and 77% by the MRC database.
We do not use any features encoding the occur-
rence or frequency of specific words or n-grams in
a document.
4 Experiments and Results
4.1 Experimental Setup
We used the WEKA toolkit (Hall et al., 2009) to
perform our classification experiments and evalu-
ated the classification accuracy using 10-fold cross
validation. As classification algorithm, we used
the Sequential Minimal Optimization (SMO) im-
plementation in WEKA, which marginally outper-
formed (1?1.5%) some other classification algo-
rithms (J48 Decision tree, Logistic Regression and
Random Forest) we tried in initial experiments.
4.2 Classification accuracy with various
feature groups
We discussed in the context of Table 1 that sen-
tence length may be a good surface indicator of
the age-group. So, we first constructed a classifi-
cation model with only one feature. This yielded
a classification accuracy of 71.4%, which we con-
sider as our baseline (instead of a basic random
baseline of 33%).
4
http://catalog.ldc.upenn.edu/docs/
LDC96L14/eug_a4.pdf
We then constructed a model with all the fea-
tures we introduced in section 3. This model
achieves a classification accuracy of 95.9%, which
is a 23.7% improvement over the sentence length
baseline in terms of classification accuracy.
In order to understand what features contribute
the most to classification accuracy, we applied fea-
ture selection on the entire set, using two algo-
rithms available in WEKA, which differ in the way
they select feature subsets:
? InfoGainAttributeEval evaluates the features
individually based on their Information Gain
(IG) with respect to the class.
? CfsSubsetEval (Hall, 1999) chooses a feature
subset considering the correlations between
features in addition to their predictive power.
Both feature selection algorithms use methods
that are independent of the classification algorithm
as such to select the feature subsets.
Information Gain-based feature selection re-
sults in a ranked list of features, which are inde-
pendent of each other. The Top-10 features ac-
cording to this algorithm are listed in Table 2.
Feature Group
avg. AoA (Kuperman et al., 2012) PSYCH
avg. # PPs in a sentence SYNTAX
avg. # instances where the lemma
has stem and affix
CELEX
? avg. parse tree height SYNTAX
? avg. # NPs in a sentence SYNTAX
avg. # instances of affix substitution CELEX
? avg. # prep. in a sentence LEX
avg. # instances where a lemma is
not a count noun
CELEX
avg. # clauses per sentence SYNTAX
? sentence length SYNTAX
Table 2: Ranked list of Top-10 features using IG
As is clear from their description, all Top-10
features encode different linguistic aspects of a
text. While there are more syntactic features fol-
lowed by Celex features in these Top-10 features,
the most predictive feature is a psycholinguistic
feature encoding the average age of acquisition of
words. A classifier using only the Top-10 IG fea-
tures achieves an accuracy of 84.5%.
Applying CfsSubsetEval to these Top-10 fea-
tures set selects the six features not prefixed by a
23
hyphen in the table, indicating that these features
do not correlate with each other (much). A clas-
sifier using only this subset of 6 features achieves
an accuracy of 84.1%.
We also explored the use of CfsSubsetEval fea-
ture selection on the entire feature set instead of
using only the Top 10 features. From the total of
152 features, CfsSubsetEval selected a set of 41
features. Building a classification model with only
these features resulted in a classification accuracy
of 93.9% which is only 2% less than the model
including all the features.
Table 3 shows the specific feature subset se-
lected by the CfsSubsetEval method, including
# preposition phrases
# t-units
# co-ordinate phrases per t-unit
# lexical words in total words
# interjections
# conjunctive phrases
# word senses
# verbs
# verbs, past participle (VBN)
# proper nouns
# plural nouns
avg. corrected type-token ratio
avg. AoA acc. to ratings of Kuperman et al. (2012)
avg. AoA acc. to ratings of Cortese and Khanna (2008)
avg. word imageability rating (MRC)
avg. AoA according to MRC
# morph. complex words (e.g., sandbank)
# morph. conversion (e.g., abandon)
# morph. irrelevant (e.g., meow)
# morph. obscure (e.g., dedicate)
# morph. may include root (e.g., imprimatur)
# foreign words (e.g., eureka)
# words with multiple analyses (e.g., treasurer)
# noun verb affix compounds (e.g., stockholder)
# lemmas with stem and affix (e.g., abundant=abound+ant)
# flectional forms (e.g., bagpipes)
# clipping allomorphy (e.g., phone vs. telephone)
# deriv. allomorphy (e.g., clarify?clarification)
# flectional allomorphy (e.g., verb bear 7? adjective born)
# conversion allomorphy (e.g., halve?half )
# lemmas with affix substitution (e.g., active=action+ive)
# words with reversion (e.g., downpour)
# uncountable nouns
# collective, countable nouns
# collective, uncountable nouns
# post positive nouns.
# verb, expression (e.g., bell the cat)
# adverb, expression (e.g., run amok)
# reflexive pronouns
# wh pronouns
# determinative pronouns
Table 3: CfsSubsetEval feature subset
some examples illustrating the morphological fea-
tures. The method does not provide a ranked list,
so the features here simply appear in the order in
which they are included in the feature vector.
All of these features except for the psycholinguis-
tic features encode the number of occurrences av-
eraged across the text (e.g., average number of
prepositions/sentence in a text) unless explicitly
stated otherwise. The psycholinguistic features
encode the average ratings of words for a given
property (e.g., average AoA of words in a text).
Table 4 summarizes the classification accura-
cies with the different feature subsets seen so far,
with the feature count shown in parentheses.
Feature Subset (#) Accuracy SD
All Features (152) 95.9% 0.37
Cfs on all features (41) 93.9% 0.59
Top-10 IG features (10) 84.5% 0.70
Cfs on IG (6) 84.1% 0.55
Table 4: Accuracy with various feature subsets
We performed statistical significance tests be-
tween the feature subsets using the Paired T-tester
(corrected), provided with WEKA and all the dif-
ferences in accuracy were found to be statistically
significant at p < 0.001. We also provide the Stan-
dard Deviation (SD) of the test set accuracy in the
10 folds of CV per dataset, to make it possible to
compare these experiments with future research on
this dataset in terms of statistical significance.
Table 5 presents the classification accuracies of
individual features from the Top-10 features list
(introduced in Table 2).
Feature Accuracy
AoA Kup Lem 82.4%
# pp 74.0%
# stem & affix 77.7%
avg. parse tree height 73.4%
# np 73.0%
# substitution 74.3%
# prep 72.0%
# uncountable nouns 68.3%
# clauses 72.5%
sentence length 71.4%
Table 5: Accuracies of Top-10 individual features
The table shows that all but one of the features
individually achieves a classification accuracy
above 70%. The first feature (AoA Kup Lem)
24
alone resulted in an accuracy of 82.4%, which is
quite close to the accuracy obtained by all the Top-
10 features together (84.5%).
To obtain a fuller picture of the impact of dif-
ferent feature groups, we also performed ablation
tests removing some groups of features at a time.
Table 6 shows the results of these tests along with
the SD of the 10 fold CV. All the results that are
statistically different at p < 0.001 from the model
with all features (95.9% accuracy, 0.37 SD) are in-
dicated with a *.
Features Acc. SD
All ? AoA Kup Lem 95.9% 0.37
All ? All AoA Features 95.6% 0.58
All ? PSYCH 95.8% 0.31
All ? CELEX 94.7%* 0.51
All ? CELEX?PSYCH 93.6%* 0.66
All ? CELEX?PSYCH?LEX
(= SYNTAX only) 77.5%* 0.99
LEX 93.1%* 0.70
CELEX 90.0%* 0.79
PSYCH 84.5%* 1.12
Table 6: Ablation test accuracies
Interestingly, removing the most predictive in-
dividual feature (AoA Kup Lem) from the feature
set did not change the overall classification accu-
racy at all. Removing all of the AoA features or
all of the psycholinguistic features also resulted in
only a very small drop. The combination of the
linguistic features, covering lexical and syntactic
characteristics as well as the morphological, syn-
tactic, orthographic, and phonological properties
from Celex, thus seem to be equally characteristic
of the texts targeting different age-groups as the
psycholinguistic properties, even though the fea-
tures are quite different in nature.
In terms of separate groups of features, syntac-
tic features alone performed the worst (77.5%) and
lexical richness features the best (93.1%).
To investigate which classes were mixed up by
the classifier, consider Table 7 showing the con-
fusion matrix for the model with all features on a
10-fold CV experiment.
We find that CBeebies is more often con-
fused with the CBBC program for older chil-
dren (156+214) and very rarely with the program
for adults (1+2). The older children programs
(CBBC) are more commonly confused with pro-
grams for adults (36+58) compared to CBeebies
classified as? CBeebies CBBC Adults
CBeebies (0?6) 3619 156 1
CBBC (6?12) 214 3526 36
Adults (12+) 2 58 3716
Table 7: Confusion Matrix
(1+2), which is expected given that the CBBC au-
dience is closer in age to adults than the CBeebies
audience.
Summing up, we can conclude from these ex-
periments that the classification of transcripts into
age groups can be informed by a wide range of lin-
guistics and psycholinguistic features. While for
some practical tasks a few features may be enough
to obtain a classification of sufficient accuracy, the
more general take-home message is that authentic
texts targeting specific age groups exhibit a broad
range of linguistics characteristics that are indica-
tive of the complexity of the language used.
4.3 Effect of text size and training data size
When we first introduced the properties of the cor-
pus in Table 1, it appeared that sentence length
and the overall text length could be important pre-
dictors of the target age-groups. However, the list
of Top-10 features based on information gain was
dominated by more linguistically oriented syntac-
tic and psycholinguistic features.
Sentence length was only the tenth best feature
by information gain and did not figure at all in the
43 features chosen by the CfsSubsetEval method
selecting features that are highly correlated with
the class prediction while having low correlation
between themselves. As mentioned above, sen-
tence length as an individual feature only achieved
a classification accuracy of 71.4%.
The text length is not a part of any feature set we
used, but considering the global corpus properties
we wanted to verify how well it would perform
and thus trained a model with only text length
(#sentences per text) as a feature. This achieved
a classification accuracy of only 56.7%.
The corpus consists of transcripts of whole TV
programs and hence an individual transcript text
typically is longer than the texts commonly used in
readability classification experiments. This raises
the question whether the high classification accu-
racies we obtained are the consequences of the
larger text size.
As a second issue, the training size available for
the 10-fold cross-validation experiments is com-
25
paratively large, given the 3776 text per level
available in the overall corpus. We thus also
wanted to study the impact of the training size on
the classification accuracy achieved.
Pulling these threads together, we compared
the classification accuracy against text length and
training set size to better understand their impact.
For this, we trained models with different text
sizes (by considering the first 25%, 50%, 75% or
100% of the sentences from each text) and with
different training set sizes (from 10% to 100%).
Figure 1 presents the resulting classification ac-
curacy in relation to training set size for the dif-
ferent text sizes. All models were trained with the
full feature set (152 features), using 10-fold cross-
validation as before.
 90
 91
 92
 93
 94
 95
 96
 10  20  30  40  50  60  70  80  90  100
clas
sific
atio
n ac
cura
cy (
in p
erce
nt)
training set size (in percent)
Variation of Classification Accuracy with training set size and text sample size
25% text size50% text size75% text size100% text size
Figure 1: Classification accuracy for different text
sizes and training set sizes
As expected, both the training set size and the
text size affect the classification accuracy. How-
ever, the classification accuracy even for the small-
est text and training set size is always above 90%,
which means that the unusually large text and
training size is not the main factor behind the very
high accuracy rates.
In all four cases of text size, there was a small
effect of training set size on the classification ac-
curacy. But the effect reduced as the text size in-
creased. At 25% text size, for example, the clas-
sification accuracy ranged 90?93% (mean 92.1%,
SD 0.9) as the training set size increased from 10%
to 100%. However, at 100% text size, the range
was only 94.8?96% (mean 95.6%, SD 0.4).
Comparing the results in terms of text size
alone, larger text size resulted in better classifica-
tion accuracy in all cases, irrespective of the train-
ing set size. A longer text will simply provide
more information for the various linguistic fea-
tures, enabling the model to deliver better judg-
ments about the text. However, despite the text
length being reduced to one fourth of its size, the
models built with our feature set always collect
enough information to ensure a classification ac-
curacy of at least 90%.
In the above experiments, we varied the text size
from 10% to 100%. But since these are percent-
ages, texts from CBBC and Adults on average still
are longer than CBEEBIES texts. While this re-
flects the fact that TV transcripts in real life are of
different length, we also wanted to see what hap-
pens when we eliminate such length differences.
We thus trained classification models fixing the
length of all documents to a concrete absolute
length, starting from 100 words (rounded off to the
nearest sentence boundary) increasing the text size
until we achieve the best overall performance. Fig-
ure 2 displays the classification accuracy we ob-
tained for the different (maximum) text sizes, for
all features and feature subsets.
 65
 70
 75
 80
 85
 90
 95
 100
 100  200  300  400  500  600  700  800  900
clas
sific
atio
n ac
cura
cy (
in p
erce
nt)
max. text size (in number of words)
Variation of Classification Accuracy with text sample size in words
All FeaturesPSYCHLEXSYNCELEX
Figure 2: Classification accuracy for different ab-
solute text sizes (in words)
The plot shows that the classification accuracy
already reaches 80% accuracy for short texts, 100
words in length, for the model with all features. It
rises to above 90% for texts which are 300 words
long and reaches the best overall accuracy of al-
most 96% for texts which are 900 words in length.
All the feature subsets too follow the same trend,
with varying degrees of accuracy that is always
lower than the model with all features.
While in this paper, we focus on documents,
the issue whether the data can be reduced further
26
to perform readability at the sentence level is dis-
cussed in Vajjala and Meurers (2014a).
5 Related Work
Analyzing the complexity of written texts and
choosing suitable texts for various target groups
including children is widely studied in computa-
tional linguistics. Some of the popular approaches
include the use of language models and machine
learning approaches (e.g., Collins-Thompson and
Callan, 2005; Feng, 2010). Web-based tools such
as REAP
5
and TextEvaluator
6
are some examples
of real-life applications for selecting English texts
by grade level.
In terms of analyzing spoken language, research
in language assessment has analyzed spoken tran-
scripts in terms of syntactic complexity (Chen and
Zechner, 2011) and other textual characteristics
(Crossley and McNamara, 2013).
In the domain of readability assessment,
the Common Core Standards (http://www.
corestandards.org) guideline texts were
used as a standard test set in the recent past (Nel-
son et al., 2012; Flor et al., 2013). This test set
contains some transcribed speech. However, to
the best of our knowledge, the process of select-
ing suitable TV programs for children as explored
in this paper has not been considered as a case of
readability assessment of spoken language before.
Subtitle corpora have been created and used
in computational linguistics for various pur-
poses. Some of them include video classifica-
tion (Katsiouli et al., 2007), machine translation
(Petukhova et al., 2012), and simplification for
deaf people (Daelemans et al., 2004). But, we are
not aware of any such subtitle research studying
the problem of automatically identifying TV pro-
grams for various age-groups.
This paper thus can be seen as connecting sev-
eral threads of research, from the analysis of text
complexity and readability, via the research on
measuring SLA proficiency that many of the lin-
guistic features we used stem from, to the com-
putational analysis of speech as encoded in subti-
tles. The range of linguistic characteristics which
turn out to be relevant and the very high preci-
sion with which the age-group classification can
be performed, even when restricting the input to
5
http://reap.cs.cmu.edu
6
https://texteval-pilot.ets.org/
TextEvaluator
artificially shortened transcripts, confirm the use-
fulness of connecting these research threads.
6 Conclusions
In this paper, we described a classification ap-
proach identifying TV programs for different
age-groups based on a range of linguistically-
motivated features derived from research on text
readability, proficiency in SLA, and psycholin-
guistic research. Using a collection of subtitle
documents classified into three groups based on
the targeted age-group, we explored different clas-
sification models with our feature set.
The experiments showed that our linguistically
motivated features perform very well, achieving
a classification accuracy of 95.9% (section 4.2).
Apart from the entire feature set, we also exper-
imented with small groups of features by apply-
ing feature selection algorithms. As it turns out,
the single most predictive feature was the age-
of-acquisition feature of Kuperman et al. (2012),
with an accuracy of 82.4%. Yet when this fea-
ture is removed from the overall feature set, the
classification accuracy is not reduced, highlighting
that such age-group classification is informed by a
range of different characteristics, not just a single,
dominating one. Authentic texts targeting specific
age groups exhibit a broad range of linguistics and
psycholinguistic characteristics that are indicative
of the complexity of the language used.
While an information gain-based feature subset
consisting of 10 features resulted in an accuracy of
84.5%, a feature set chosen using the CfsSubsetE-
val method in WEKA gave an accuracy of 93.9%.
Any of the feature groups we tested exceeded the
random baseline (33%) and a baseline using the
popular sentence length feature (71.4%) by a large
margin. Individual feature groups also performed
well at over 90% accurately in most of the cases.
The analysis thus supports multiple, equally valid
perspectives on a given text, each view encoding a
different linguistic aspect.
Apart from the features explored, we also stud-
ied the effect of the training set size and the length
of the text considered for feature extraction on
classification accuracy (Section 4.3). The size of
training set mattered more when the text size was
smaller. Text size, which did not work well as an
individual feature, clearly influences classification
accuracy by providing more information for model
building and testing.
27
In terms of the practical relevance of the re-
sults, one question that needs some attention is
how well the features and trained models gener-
alize across different type of TV programs or lan-
guages. While we have not yet investigated this
for TV subtitles, in experiments investigating the
cross-corpus performance of a model using the
same feature set, we found that the approach per-
forms well for a range of corpora composed of
reading materials for language learners (Vajjala
and Meurers, 2014b). The very high classification
accuracies of the experiments we presented in the
current paper thus seem to support the assumption
that the approach can be useful in practice for au-
tomatically identifying TV programs for viewers
of different age groups.
Regarding the three class distinctions and the
classifier setup we used in this paper, the approach
can also be generalized to other scales and a re-
gression setup (Vajjala and Meurers, 2013).
6.1 Outlook
The current work focused mostly on modeling and
studying different feature groups in terms of their
classification accuracy. Performing error analysis
and looking at the texts where the approach failed
may yield further insights into the problem. Some
aspects of the text that we did not consider in-
clude discourse coherence or topic effects. Study-
ing these two aspects can provide more insights
into the nature of the language used in TV pro-
grams directed at viewers of different ages. A
cross-genre evaluation between written and spo-
ken language complexity across age-groups could
also be insightful.
On the technical side, it would also be useful
to explore the possibility of using a parser tuned
to spoken language, to check if this helps improve
the classification accuracy of syntactic features.
While in this paper we focused on English, a
related readability model also performed well for
German (Hancke et al., 2012) so that we expect
the general approach to be applicable to other lan-
guages, subject to the availability of the relevant
resources and tools.
Acknowledgements
We would like to thank Marc Brysbaert and his
colleagues for making their excellent resources
available to the research community. We also
thank the anonymous reviewers for their useful
feedback. This research was funded by LEAD
Graduate School (GSC 1028, http://purl.org/
lead), a project of the Excellence Initiative of the
German federal and state governments.
References
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database. http:
//catalog.ldc.upenn.edu/LDC96L14.
Maio Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for auto-
mated scoring of spontaneous non-native speech. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
722?731, Portland, Oregon, June.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical lan-
guage models. Journal of the American Society for
Information Science and Technology, 56(13):1448?
1462.
Michael J. Cortese and Maya M. Khanna. 2008. Age
of acquisition ratings for 3,000 monosyllabic words.
Behavior Research Methods, 43:791?794.
Scott Crossley and Danielle McNamara. 2013. Ap-
plications of text analysis tools for spoken re-
sponse grading. Language Learning & Technology,
17:171?192.
Walter Daelemans, Anja Hoethker, and Erik F.
Tjong Kim Sang. 2004. Automatic sentence sim-
plification for subtitling in Dutch and English. In
Fourth International Conference on Language Re-
sources And Evaluation (LREC), pages 1045?1048.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of the 7th Global Wordnet Con-
ference, pages 78?85.
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility (PITR) held at ACL, pages 29?38,
Sofia, Bulgaria. ACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
The SIGKDD Explorations, 11:10?18.
Mark A. Hall. 1999. Correlation-based Feature Selec-
tion for Machine Learning. Ph.D. thesis, The Uni-
versity of Waikato, Hamilton, NewZealand.
28
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING): Technical
Papers, pages 1063?1080, Mumbai, India.
Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex ? lexical simplicity ranking based on
contextual and psycholinguistic features. In In pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (SEM).
Polyxeni Katsiouli, Vassileios Tsetsos, and Stathes
Hadjiefthymiades. 2007. Semantic video classifi-
cation based on subtitles and domain terminologies.
In Proceedings of the 1st International Workshop
on Knowledge Acquisition from Multimedia Content
(KAMC).
Emmanuel Keuleers, Paula Lacey, Kathleen Rastle,
and Marc Brysbaert. 2012. The british lexicon
project: Lexical decision data for 28,730 monosyl-
labic and disyllabic english words. Behavior Re-
search Methods, 44:287?304.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978?990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, pages 2231?2234,
Genoa, Italy. European Language Resources Asso-
ciation (ELRA).
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474?
496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners? oral narratives. The
Modern Languages Journal, pages 190?208.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and
student performance. Technical report, The Coun-
cil of Chief State School Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Volha Petukhova, Rodrigo Agerri, Mark Fishel, Yota
Georgakopoulou, Sergio Penkale, Arantza del Pozo,
Mirjam Sepesy Maucec, Martin Volk, and Andy
Way. 2012. Sumat: Data collection and parallel
corpus compilation for machine translation of sub-
titles. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012), pages 21?28, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Beatrice Santorini. 1990. Part-of-speech tagging
guidelines for the Penn Treebank, 3rd revision, 2nd
printing. Technical report, Department of Computer
Science, University of Pennsylvania.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
ofspeech tagging with a cyclic dependency net-
work. In HLT-NAACL, pages 252?259, Edmonton,
Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications
(BEA) at NAACL-HLT, pages 163?-173, Montr?eal,
Canada. ACL.
Sowmya Vajjala and Detmar Meurers. 2013. On
the applicability of readability models to web texts.
In Proceedings of the Second Workshop on Natural
Language Processing for Improving Textual Acces-
sibility (PITR) held at ACL, pages 59?-68, Sofia,
Bulgaria. ACL.
Sowmya Vajjala and Detmar Meurers. 2014a. Assess-
ing the relative reading level of sentence pairs for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL). ACL.
Sowmya Vajjala and Detmar Meurers. 2014b. Read-
ability assessment for text simplification: From an-
alyzing documents to identifying sentential simplifi-
cations. International Journal of Applied Linguis-
tics, Special Issue on Current Research in Read-
ability and Text Simplification, edited by Thomas
Franc?ois and Delphine Bernhard.
Walter J.B. Van Heuven, Pawel Mandera, Emmanuel
Keuleers, and Marc Brysbaert. 2014. Subtlex-UK:
A new and improved word frequency database for
British English. The Quarterly Journal of Experi-
mental Psychology, pages 1?15.
Michael D. Wilson. 1988. The mrc psycholinguis-
tic database: Machine readable dictionary, version
2. Behavioural Research Methods, Instruments and
Computers, 20(1):6?11.
29
