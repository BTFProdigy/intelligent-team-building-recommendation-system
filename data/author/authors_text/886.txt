Syntactic parser combination for improved dependency analysis
Francis Brunet-Manquat
Laboratoire CLIPS-IMAG
CNRS ? UJF - INPG
385, rue de la  Biblioth?que
BP 53 ? 38041 Grenoble Cedex 9, France
Francis.Brunet-Manquat@imag.fr
Abstract
The goal of this article is to present our work
about a combination of several syntactic
parsers to produce a more robust parser. We
have built a platform which allows us to
compare syntactic parsers for a given language
by splitting their results in elementary pieces,
normalizing them, and comparing them with
reference results. The same platform is used to
combine several parsers to produce a
dependency parser that has larger coverage
and is more robust than its component parsers.
In the future, it should be possible to
?compile? the knowledge extracted from
several analyzers into an autonomous
dependency parser.
1 Introduction
Our laboratory is involved in two international
projects: C-STAR (Blanchon and Boitet 2000),
with its associated European project NESPOLE!,
for speech translation and UNL, Universal
Networking Language (S?rasset and Boitet 2000),
for written translation. These two projects are
characterized by the use of a pivot representation
of the utterance and by the fact that the utterance to
be translated is likely to be ?ill-formed?, i.e. not
conform to a academic language grammar. In a
pivot system, an utterance in a source language is
parsed to yield a pivot representation which
generate into a target language is performed. To
process ill-formed data, we need robust analysis
tools capable of producing a partial analysis.
The goal is to specify, design and develop a
multilingual platform, called DepAn (Dependency
Analysis), which compares parsers for a given
language by splitting their results in elementary
pieces, normalizing them, and comparing them
with reference results, and which combine these
parsers to produce a dependency parser that has
larger coverage and is more robust than its
component parsers.
The platform combines several analyses of the
same utterance, and then computes the best data to
produce the best possible analysis. Our approach is
based on the method called ?vote by majority?, the
more common to the different parsers one data will
be, the stronger its weight will be, and also based
on a training method which adapts each vote
according to the typologies of utterances (domain,
style) and the abilities of the parsers.
The approach used, called c o m b i n a t i o n
approach, has known lots of success in speech
recognition (Fiscus 1997, Schwenck and Gauvain
2000), part of speech tagging (Halteren and al.
1998, Brill and al. 1998, Marquez et Padro 1998),
named entity recognition (Borthwick and al. 1998),
word sense disambiguation (Pedersen, 2000) and
recently in parsing (Henderson and Brill 1999),
Inui and Inui 2000, Monceaux and Robba 2003).
These works prove that combining different
systems provides an improvement in comparison to
the best system.
Our work in syntactic analysis are distinguished
from our predecessors by the combination methods
that we use. Our platform is made up of a
statistical processing, a correspondence processing
and a reconstruction processing. Furthermore, we
base our platform on a dependency representation
that describes the syntactic relations between
words. A study realized within the framework of
the international projects, CSTAR and UNL,
suggests that this representation type is adapted to
a robust and partial parsing.
2 Analysis platform design
The platform must not integrate the parsers, but it
must be able to extract the linguistic data from
their analyses, interpret them, combine them, and
produce a dependency tree (or several) combining
the best extracted data.
2.1 Processing steps
The platform process comprizes two stages: the
standardization of the analysis results and the
construction of the dependency analysis (see
Figure 1).
The standardization is made up of two steps:
? The extraction step permits to recover the
linguistic data of analysis produced by the
linguistic parsers. These parsers are shared out
in three groups according to their analysis
results (Monceaux and Robba 2002): the
parsers based on the chunks which segment
the sentence in syntagms (chunks), the parsers
based on the dependencies which produce
dependencies between words of a sentence,
and the parsers based on the chunks and the
dependencies which segment the sentence in
syntagms and produce dependencies between
syntagms and words.
? The projection step process the extracted data
to produce a set of dependency structures,
called standardized structures. a rate is
associated at each data (pos, syntactic
relations, etc.) according to the parser which
produces it. These rates, called confidence
rates, are pre-calculated during a training step
(see 2.3 Confidence rate). A dependency
structure is described by a matrix
representation offering both handiness and
efficiency (see 2.2 Dependency matrix).
The construction is made up of three steps:
? The correspondence step links the nodes of the
different normalized structures provided by the
previous step. So, we create a structure, called
segmentation network (SN), which represents
the different segmentations of a sentence and
the links between the nodes of the normalized
structures. This network represents the ?pivot
l ink? between these structures (see 3.1
Correspondence of the dependency structures).
? The combination step according to established
links produces a single dependency
representation which contains all the extracted
linguistic data. The resulting data can be such
as inconsistancies i.e. a word can?t be both a
noun and a verb (contradictory part-of-speech).
The confidence rate of these data are  then
recalculated (see 3.2 Combination of the
linguistic data).
? The production step  builds the new
dependency structures according to the
combined data, their new confidence rates, and
some linguistic and structural constraints (see
3.3 Production of the dependency structures).
Figure 1: Functional architecture
2.2 Dependency matrix
Our analysis platform is based on the
dependencies, i.e. it produces dependencies
between the words of a sentence. In our platform, a
dependency structure is described by a matrix
representation. Our representation, called
Dependency Matrix (DM), is made up of a couple
<L,M>:
? L is a list of nodes. A node is made up of a set
of linguistic data (part of speech and
grammatical variable). Each node represents a
word.
? M  is a square matrix which describes the
dependencies between the nodes of L. M(i,j)
contains the set of syntactic dependencies
between i node and j node of L.
Figure 2: Syntatic dependency structure
The DM corresponding to the syntactic
dependency structure above is:
L =
la :: pos=determinant
recherche :: pos=noun
fran?aise :: pos=adjective
perd :: pos=verb
ses :: pos=determinant
moyens :: pos=noun
M =
A matrix representation has two advantages for
the automatic process:
? Handiness: mathematic tools are associated to
matrix: addition, deletion, comparison, etc.
These tools permit a simple processing of the
data contained in the matrix.
? Effectiveness: efficient methods are associated
to matrix: pattern matching methods,
combination methods, etc.
We choose also to use a matrix representation
because the combination of different dependency
structures provides a graph containing all the
possible dependencies.
2.3 Confidence rate training
In (Brunet-Manquat 2003), we present projection
rules to transform the extracted data into a set of
normalized data. Each normalized data D  is
associated to a confidence rate of the data D
according to the parser which produces it.
The rates are calculated according to the parser
evaluations. For each parser 
? 
Ai , we calculate the
recall and the accuracy of each linguistic data D
produced by 
? 
Ai :
? 
Recall
Ai
(D) =
number of corrected data D
number of reference data D
? 
Accuracy
Ai
(D) =
number of corrected data D
number of nominated data D
The confidence rate corresponds to the F-
measure which combines recall and accuracy in a
single measure:
? 
F -measure
Ai
(D) =
(? 2 +1) ?Accuracy
Ai
(D) ?Recall
Ai
(D)? 2 ?Accuracy
Ai
(D) + Recall
Ai
(D)
Where ? is an accuracy coefficient, 1 ? ? ? 0.
Within the framework of our work, we want to
be able to set our platform according to our needs
in analysis: information retrieval, machine
translation, parsing, etc. We introduce the
accuracy coefficient ? (between 0 and 1) into the
F-measure. It permits to customize the platform
and so the final analysis: if ?  is close to 0, the
accuracy will be favoured in the calculation of the
confidence rate. In the following, ? will be equal to
1. It will be interesting to introduce a second
coefficient for the recall (evaluation in progress).
3 Dependency structure construction
At the end of the normalization process, a set of
dependency strutures is associated to each
sentence. The next step consists in combining these
structures to obtain a single dependency
representation which contains all the linguistic data
of these structures. To perform this combination,
we must put in correspondence these dependency
structures.
3.1 Correspondence of the dependency
structures
The structure correspondence consists in
regrouping the nodes representing the same word
into a sentence (the shared minimal data). But it
consists also in representing the word conflicts
produced by the different segmentations of a
sentence because of, for example, compound
words (words high energy or word high-energy),
dictionnary terms (words United Kingdom or word
United_Kingdom), etc.
In order to represent the correspondences, we
create a structure, called segmentation network
(SN), that represents the different segmentations of
a sentence and links the nodes of the normalized
structures. This network represents the ?pivot link?
between these structures.
A SN is a lattice; each node of this lattice
represents a possible segment of a word and serves
to link the nodes of the dependency structures. In
practice, a node N
sn
 of a SN is made up of two
data:
? SNODE: a sequence which represents a
substring of a sentence. For example, the
words of the sentence ?On avait d?nombr?
cent vingt-neuf candidats? have to SNODE:
On[1,2], avait[3-7], d?nombr?[8-15], etc. This
data is based on the proposal of (Boitet and
Zaharin,1988) Structured String-Tree
Correspondences (SSTC).
? L: a set which contains the nodes of the
normalized structures linked to the node N
rs
.
The first step consists in creating an initial SN
for each dependency structure. Each initial node
N
sn
 of an initial SN is created according to a node
N
i
 of the dependency structure S
k
:
SNODE(N
sn
)=SNODE(S
k
.N
i
) & L(N
sn
)={S
k
.N
i
}
Then the nodes of the initial SN are inserted into
the lattice according to their appearance order into
the sentence (according to their SNODE). In the
following, we take two dependency structures S
1
and S
2
, and their initial segmentation networks SN
1
and SN
2
:
Let us do the correspondences between SN
1
 and
SN
2
. First, the initial network SN
1
 is chosen as the
basic SN, called SN
base
. We use two rules to
introduce the nodes of the others SN into the basic
SN:
? Rule 1) Correspondence: If a node N
i
 of SN
k
is equal to a node N
sn
 of the SN
base
 (equal if
SNODE(N
i
)==SNODE(N
sn
)), N
sn
 will be
linked to the node N
i
: L(N
sn
) = L(N
sn
) ? L(N
i
).
? Rule 2) Insertion: If a node N
i
 of SN
k
 is not
equal to a node of SN
base
, this node will be
inserted in SN
base
 according to their SNODE.
The first nodes on[1-2], avait[3-7], d?nombr?[8-
15] of SN
2
 verify the first rule. They correspond to
the nodes on[1-2], avait[3-7], d?nombr?[8-15] of
SN
base
. The fourth node cent vingt-neuf[16-19] of
SN
2
 verifies the second rule, so it is inserted into
SN
base
. The last node of SN
2
 candidats[30-38]
verifies the first rule. We obtain the following
lattice:
The final segmentation network represents the
possible segmentations of the sentence and links
the nodes of structures between them
1
. Now the
correspondences between the nodes of the
structures are established, we can combine these
structures to provide a single dependency
representation, which combine all linguistic data of
these structures.
3.2 Combination of the linguistic data
The correspondences between the different
structures being established, the combination step
of linguistic data can begin. The method used here
is based on the method known as ?majority vote?:
the more common to the different parsers one data
will be, the stronger its weight will be.
At the end of the correspondence phase, a set of
dependency structures and a segmentation network
SN are associated to every sentence. The first stage
consists in creating a dependency structure, called
combined matrix CM (the nodes of the SN will be
used as nodes for this representation) for each
segmentation network. This matrix is filled with
the linguistic data contained in the associated
dependency structures.
For example, we regroup dependencies for the
previous SN
b
:
                                                       
1
 In (Brunet-Manquat 2004), we propose to improve the
correspondence step by adding correspondence rules
allowing processing the compound words or dictionary
terms, for example, by establishing a relation between
the node United_Kingdom and the nodes United and
Kingdom.
Subsequently, we associate a confidence rate p
for each data:
Each confidence rate could be seen as a weighted
vote of a parser A
i
 for a data D. During a training
phase, these rates (votes) will be adapted to the
different abilities of the parser according to the
typologies (domain, style) of the reference
utterances.
Some linguistic data will be equivalent, some
others will be contradictory (for example the
dependency Subject(x, y) is contradictory with the
dependency Object(x, y), the part-of-speech are
mutually contradictory into a same word).
Finally, we group all the linguistic data D
i
 of a
sentence S (D
i
 is the data D provided by the parser
i), and we calculate the new associated confidence
rate, called combined rate, for each data D . A
combined rate of a data D is calculated according
to the confidence rates of all the data D . We
propose two calculations: standardized a n d
corrected calculations.
Standardized calculation: the combined rate of
a data D is equal to the sum of the confidence rates
of the data D
i
 divided by the number n of parsers
that can provide this data D:
? 
Rcombined(D) =
( Rconfidence(Di)
i
?
)
n
Where i = parser producing the data D;
n = number of parsers that can provide the data D.
For example, let us calculate the combined rate
associated to the dependency OBJ(x, y) (the word
y is the object of the word x) provided by the
parsers A1 and A2. The combined rate associated
to OBJ(x, y) is equal to the sum of the two
confidence rates provided by A1 and A2:
confidence(OBJ::A1)=0.5 and confidence
(OBJ::A2)=0.7, divided by the number of parsers
that can provide this type of information (three for
the example), (0.5+0.7+0)/3 = 0.4. If the third
parser provides an other dependency, for example
SUBJ (x, y) (the word y is the subject of the word
x), and if the confidence rate of this data is 0.8, the
merged rate associated to SUBJ(x, y) is equal to
(0+0+0.8)/3 = 0.26.
Corrected calculation: the combined rate of the
data D is equal to the sum of the confidence rates
of data D
i
 minus the sum (multiplied by a
correction coefficient) of the confidence rates of
the data contradictory to D, the whole divided by
the number of parsers that can provide the data D:
? 
Rcombined(D) =
( Rconfidence(Di)
i
?
??? Rconfidence(Dp)
p 
?
)
n
Where i = parser producing the data D;
n = number of parsers that can provide the data D;
p = parser producing a date contradictory to D.
 ?  = an correction coefficient, 1 ? ? ? 0.
For the previous example, the syntactic
dependencies between words x and y are
contradictories: either OBJ(x, y), or SUBJ(x, y).
The combined rate associated to OBJ(x, y) is equal
to ((0.5+0.7) - (0.4*0.8))/3 = 0.29 (with a
correction coefficient at 0.4) and the combined rate
associated to SUBJ(x, y) is equal to (0.8 ?
0.4*(0.5+0.7))/3 = 0.1.
These two calculations favour the linguistic data
provided by the greatest number of parsers. The
corrected calculation permits to treat both the
silence and the contradiction of others parsers.
3.3 Production of the dependency structures
This last step permits to build the new dependency
structures according to the data combined in the
previous step. These structures are produced
according to the combined rate associated to these
data, and linguistic and structural constraints. The
production is based on a constraint satisfaction
method made up of three rules:
Let D  be a part-of-speech or a grammatical
variable of a combined matrix CM:
? For each node N  of CM, D  is kept if its
combined rate is higher than the combined
rates of the contradictory data.
Let D be a syntactic relation of CM:
? Only one syntactic dependency between two
nodes N
i
 et N
k
 is kept: The data with the best
confidence rate on the case CM(i, k) is kept;
? A node N
i
 depends on only one node N
k
: In the
column CM(i), which represents the
dependencies N
k
 ?  N
i
 , only the data with the
best confidence rate is kept.
Concerning the word conflicts resulting from the
different possible segmentations of a sentence, we
choose to keep only the nodes resulting from the
?best? word chunker among our parsers, i.e. the
parser with the word segmentation closer to the
segmentation of the reference corpus. Soon we will
introduce a process that associates a segmentation
rate to each node, which represents the confidence
on the word segmentation according to the parsers,
like the confidence rate on the linguistic data. This
rate will permit us to introduce a segmentation
constraint in our production step.
4 Experimentation and measures
4.1 Parsers and corpus
We experiment our platform on French. We have
three parsers for this evaluation: IFSP (Incremental
Finite-State Parser) (A?t-Mokhtar and Chanod
1997) which builds the syntactic groups (chunks)
of a sentence, and then uses the structure built to
extract the syntactic dependencies between words,
the parser of the GREYC (Vergne 1998) which
combines tagging methods to build not-recursive
chunks and a dependency algorithm to calculate
the dependency structure and XIP (Xerox
Incremental Parser) (Ha?t-mokhtar and al. 2002)
which has different linguistic processings
organized in an incremental way (morphological
tagging, chunk parsing, dependency extraction) to
obtain an dependency analysis.
The corpus used is the corpus of the university
Paris VII (Abeill? and Cl?ment 1999). This corpus
is made up of a million sentences extracted from
?Le Monde?, a French newspaper. The sentences
are chunked and the words tag. A small part of this
corpus was standardized to correspond to a
dependency corpus. For this experimentation, we
use a reference corpus made up of 400 sentences,
arbitrarily selected, made up of long and complex
sentences, 30 words on average per sentence
(minimum 9 words, maximum 73 words). For
example:
? La cessation de paiement constat?e, le tribunal
de commerce nomme un administrateur judiciaire,
qui doit ?valuer les dettes - alors gel?es - et
proposer soit un plan de continuation, soit la
liquidation judiciaire. ?
4.2 Training
The first 200 sentences of the reference corpus are
used in the training step. Our experimentation is
restricted with 10 linguistic data: 6 part-of-speech
(noun, verb, adjective, pronoun, preposition and
determinant) and 4 syntactic dependencies
(subject, object, complement and determinant).
Figure 3 represents the global measures for the
part-of-speech. Figure 4 represents the global
measures for the syntactic dependencies.
The syntactic dependency measures are bad
because the word average per sentence of the
reference corpus is high and also because the
sentences are difficult to analyze. The confidence
rates (F-measure) of each part-of-speech (for
example, F-measure of noun pos: IFSP: 78,4%,
GREYC: 77,8%, XIP: 79,9%) and each syntactic
dependency (for example, F-measure of subject
pos: IFSP: 50,0%, GREYC: 36,4%, XIP: 50,5%)
enable us to produce our combination results on
the 200 remaining sentences (see 3 Dependency
structure construction).
76,9
85,8
81,1
81,8
80,2
81,0
82,8
79,7
81,2
70 75 80 85 90
Recall
Accuracy
F-measure
percentage
XIP
GREYC
IFSP
Figure 3: Part-of-speech measures
61,3
55,0
58,0
52,9
59,1
55,8
66,7
55,0
60,3
50 52 54 56 58 60 62 64 66 68
Recall
Accuracy
F-measure
Percentage
XIP
GREYC
IFSP
Figure 4: Syntactic dependency measures
4.3 Evaluation
We evaluate our platform analysis and the other
parser analysis. The platform DepAn uses the
standardized calculation to combine the linguistic
data (see 3.2 Combination of the linguistic data).
The evaluation of part-of-speech tagging (see
Figure 5) shows that our approach permits a
significant gain of 2.6% in comparison to the best
parser according to the F-measure (DepAn: 86,3%
and GREYC: 84,1%). However, the gain is not
significant for the evaluation of syntactic
dependency (see Figure 6). It is equal to 1.1% in
comparison to the best parser (DepAn: 62,9% and
XIP: 62,2%).
This weak gain is understandable because the
standardized calculation used to combine all the
data D does not consider the data contradictory to
D. To improve the analysis, we propose another
calculation combining the linguistic data, the
corrected calculation (see 3.2 Combination of the
linguistic data). This calculation permits to treat
both silences and contradictions of others parsers.
The evaluation of the platform with the corrected
calculation is currently in progress.
79,8
87,2
83,3
86,0
82,3
84,1
85,4
77,7
81,4
86,5
86,1
86,3
70,0 72,0 74,0 76,0 78,0 80,0 82,0 84,0 86,0 88,0 90,0
Recall
Accuracy
F-measure
Percentage
DePan
XIP
GREYC
IFSP
Figure 5: Part-of-speech measures
61,0
60,7
60,8
55,2
58,0
56,6
69,7
56,2
62,2
70,8
56,6
62,9
50,0 55,0 60,0 65,0 70,0 75,0
Recall
Accuracy
F-measure
Percentage
DePan
XIP
GREYC
IFSP
Figure 6: Syntactic dependency measures
5 Conclusion
The platform DepAn allows us to compare
syntactic parsers for a given language by splitting
their results in elementary pieces, normalizing
them, and comparing them with reference results.
The same platform is used to combine several
parsers to produce a customized dependency
parser, which combines the different abilities of
these parsers and which is adapted to the style or
the domain of the reference utterances.
The evaluations show that our approach, a
combination processing associated with a
statistical processing, improve the analysis in
comparition to the used parsers. The gain is not
significant for the moment but the future
corrections will improve this gain.
Our platform is currently tested on English. We
use the SUSANNE (http://www.grampson.net)
corpus. The SUSANNE Corpus was created, with
the sponsorship of the Economic and Social
Research Council (UK), as part of the process of
developing a comprehensive language-
engineering-oriented taxonomy and annotation
scheme for the logical and surface grammar of
English. The SUSANNE Corpus itself comprises
an approximately 130,000-word subset of the
Brown Corpus of American English.
In the short term, we also hope to combine other
parser types (semantic for example) to the
syntactic parsers to produce multilevel dependency
structures containing several linguistic levels:
semantics, logic, syntactic, etc. In the future, we
hope to learn from the combination of several
parsers. For example, it should by possible to
?compile? the knowledge extracted from these
parsers into an autonomous dependency parser.
6 Acknowledgements
We would like to express our special thanks to all
the creators of the parsers used here for enabling
all of this research by providing their systems to
us.
References
Abeill? A. and L. Cl?ment (1999). A tagged
reference corpus for French, LINC?99
Proceedings, EACL workshop, Bergen.
A?t-Mokhtar S. and Chanod JP. (1997),
Incremental finite-state parsing, in Applied
Natural Language Processing 1997, April 1997,
Washington.
Ait-Mokhtar S., Chanod JP. and Roux C. (2002),
Robustness beyond Shallowness: Incremental
Deep Parsing, in Natural Language Engineering,
8 (2/3), pp 121-144, Cambridge University
Press.
Blanchon H. and Boitet C. (2000). Speech
Translation for French within the C-STAR II
Consortium and Future Perspectives. Proc.
ICSLP 2000. Beijing, China, Oct. 16-20, 2000.
vol 4/4: pp. 412-417.
Boitet Ch. and Zaharin Y. (1988), ?Representation
trees and string-tree correspondences? ,
published in COLING-88, pp 59-64.
Brill E. and Wu J. (1998) Classifier Combinaison
for Improved Lexical Disambiguation. In Proc.
of the 17
th
 COLING, pp. 191-195.
Brothwick A., Sterling J., Agichtein E. and
Grishman R. (1998) Exploiting diverse
knowledge sources via maximum entropy in
named entity recognition. Proceedings of the
sixth workshop on very large corpora, pages
152-160, Montreal.
Brunet-Manquat F. (2004), ?Description et
conception d?une plate-forme robuste combinant
des analyseurs d??nonc??, journal on line ISDM,
vol. 13, f?vrier 2004, 12 pages.
Brunet-Manquat F. (2003), ?Fusionner pour mieux
analyser: quelques id?es et une premi?re
exp?rience?, In Proc. Of RECITAL-2003, Batz-
sur-mer, France, 10-14 juin 2003. Vol 1/2, pp
429-438.
Fiscus J.G. (1997), ?A post-processing system to
yield reduced error word rates: Recognizer
output voting error reduction (ROVER)?,
published in IEEE Workshop on Automatic
Speech Recognizer and Understanding, pp 347-
354.
Halteren H., J. Zavrel and W. Daelemans (1998).
Improving data driven wordclass tagging by
system combination . In Proc. of the 17
th
COLING.
Henderson, J. C. and Bril E. (1999). Exploiting
Diversity in Natural Language Processing:
Combining Parsers. In Proc. of the 1999
SIGDAT Conference on EMNLP and VLC, pp.
187-194.
Illouz G. (1999), ?M?ta-?tiqueteur adaptatif: vers
une utilisation pragmatique des resources
linguistiques?, published in TALN?99.
Inui T. and Inui K. (2000), Committee-based
Decision Making in Probabilistic Partiel
Parsing, In Proc. of COLING-2000.
Marquez and Padro (1998). On the evaluation and
comparaison of taggers?: the effect of noise in
test corpora. Actes COLING/ACL?98, Montreal,
Canada.
Monceaux L. and Isabelle Robba I. (2002), ?Les
analyseurs syntaxiques?: atouts pour une analyse
des questions dans un syst?me de question-
r?ponse?? ?, Actes de TALN?2003, pp.195-204.
Pedersen T. (2000), A Simple Approach to
Building Ensembles of Naive Bayesian
Classifiers for Word Sense Disambigusation In
Proc. of the NAACL, pp. 63-69, 2000.
S?rasset G. and Boitet C. (2000). On UNL as the
future "html of the linguistic content" & the
reuse of existing NLP components in UNL-
related applications with the example of a UNL-
French deconverter, Proc. of COLING-2000,
Saarbr?cken, 31 July ? 3 August 2000
Schwenk H. and Gauvain J.L. (2000), ?Combining
multiple speech recognizers using voting and
language model information?, published in IEEE
International Conference on Speech and
Language Processing (ICSLP), pp. II:915-918.
Vergne J. and Giguet E. (1998), Regards th?orique
sur le ??Tagging??, Actes de TALN?1998, pp 24-
33.
Proceedings of the Workshop on Multilingual Language Resources and Interoperability, pages 25?31,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The LexALP Information System: Term Bank and Corpus for Multi-lingual Legal Terminology Consolidated 
  Verena Lyding, Elena Chiocchetti EURAC research Viale Druso 1, 39100 Bozen/Bolzano - Italy  forename.name@eurac.edu 
Gilles S?rasset, Francis Brunet-Manquat GETA-CLIPS IMAG BP 53, 38041 Grenoble cedex 9 - France forename.name@imag.fr     Abstract Standard techniques used in multilingual terminology management fail to describe legal terminologies as they are bound to different legal systems and terms do not share a common meaning. In the LexALP project, we use a technique defined for general lexical databases to achieve cross language interoperability between lan-guages of the Alpine Convention. In this paper we present the methodology and tools developed for the collection, de-scription and harmonisation of the legal terminology of spatial planning and sus-tainable development in the four lan-guages of the countries of the Alpine Space. 1 Introduction The aim of the LexALP project is to harmo-nise the terminology used by the Alpine Conven-tion, both for internal purposes and for commu-nication among the member states. The Alpine Convention is an international treaty signed by all states of the Alpine territory (France, Monaco, Switzerland Liechtenstein, Austria, Germany, Italy and Slovenia) for the protection of land-scape and sustainable development of this moun-tain area1. The member states speak four differ-ent languages, namely French, German, Italian, and Slovene and have different legal systems and traditions. Hence arises the need for a systematization and uniformation of terminology and clear trans-lation equivalence in all four languages. For this reason, the project intends to provide all                                                 1 cf. also http: www.alpenkonvention.org  
stakeholders and the wider public with an infor-mation system which combines three main com-ponents, a terminology data base, a multilingual corpus and the relative bibliographic data base. In this way the manually revised, elaborated and validated (harmonised) quadrilingual information on the legal terminology (i.e. complete termino-logical entries) will be closely interacting with a facility to dynamically search for additional con-texts in a relevant set of legal texts in all lan-guages and for all main legal systems involved. 2 Multilingual legal information system The information system for the terminology of the Alpine Convention, with a specific focus on spatial planning and sustainable development, will give the possibility to search for relevant terms and their (harmonised or rejected) translations in all 4 official languages of the Alpine Convention in the first module, the term bank. Next to retrieving synonyms and translation equivalents within each legal system, the user will be provided with a representative context and a valid definition of the concept under consideration. Source information will be provided for each text field in the terminological entry. Via a link from the terminological data base to the second module, the corpus facility, the information system will give the possibility to search the corpus for further contexts. Finally, both term bank and corpus will be in-teracting with a third module, the bibliographic database, so as to allow retrieving full informa-tion on text excepts cited in the term bank and to store important meta data on corpus documents. 
25
3 Terminological data 3.1 Data categories and motivations The data categories present in the terminology database allow entering and organising relevant information on the concept under analysis. The term bank interface allows entering of the fol-lowing terminological data categories: denomi-nation/term, definition, context, note, sources (text fields), grammatical information to the term, harmonisation status, processing status, geo-graphical usage, frequency and domain, accord-ing to the appositely elaborated domain classifi-cation structure2 (pull down menus). Again by means of pull-down menus the terminologist will be able to signal to the users which terms are al-ready processed (i.e. checked by legal experts), harmonised or rejected and - most important - to which legal system they belong (the menu geo-graphical usage allows to specify this informa-tion). Furthermore it is possible to specify syno-nyms, short forms, abbreviations etc. in the ter-minological entry and, if necessary, link them to the relative full information already present in the term bank (however, no direct access to these linked data is possible, this must be done via the search interface). Finally, the terminologist is given the possibility of writing general com-ments to the entry. At the very end of one lan-guage entry the terminologist can decide whether to release the data to the public (by clicking on the button ?finish?) or keep it for further fine-tuning (button ?update?). Each term is created in its ?language volume? and described by means of all necessary informa-tion. As soon as one or all equivalents in the other languages are available too, the single en-tries can be linked to each other with the help of an axie (see detailed description below). Searches can be done for all languages or on a user-defined selection of source and target lan-guages. Presently the database allows global searching in all text fields and filtering by source, author, date of creation, as well as by axie name and ids. Results can be displayed in full form, as a short list of terms only or in XML. Some ex-port/import functions are granted. As the term bank serves mainly the scope of diffusing harmonised terminology, the four trans-lation equivalents (validated by a group of ex-perts) are displayed together, whereas rejected synonyms are displayed separately for each search language. In this way the user may well                                                 2 See also 4.1  
look for a non validated synonym and find it in the database but be warned as to which is the preferred term and its harmonised equivalents in the other languages. Figure 1 shows such a situa-tion where the French rejected term ?transport intra-alpin? is linked to the harmonised term ?trafic intra-alpin?.  
 Figure 1: A set of Alpine Convention terms and their relations 3.2 Monolingual data The LexALP term bank consists in 5 volumes for French, German, Italian, Slovene and English (no data is being entered for this fifth language at the moment), which contain the term descrip-tions. The set of data categories is represented in an XML structure that follows a common schema.  <entry id="fra.trafic_intra-alpin.1010743.e"        lang="fra"        legalSystem="AC"        process_status="FINALISED"        status="HARMONISED">   <term>trafic intra-alpin</term>   <grammar>n.m.</grammar>   <domain>Transport</domain>   <usage frequency="common"          geographical-code="INT"          technical="false"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.transport_intra-alpin?"/>   <relatedTerm      isHarmonised="false"     relationToTerm="Synonym"     termref="fra.circulation_intra-?"/>   <definition>     [T]rafic constitu? de trajets ayant leur      point de d?part et/ou d'arriv?e ?      l'int?rieur de l'espace alpin.   </definition>   <source>Prot. Transp., art. 2 </source>   <context url="http://www...">     Des projets routiers ? grand d?bit pour      le trafic intra-alpin peuvent ?tre      r?alis?s, si [...].   </context> </entry> Figure 2: XML form of the term ?trafic intra-alpin? Each entry represents a unique term/meaning. Terms with the same denomination, but belong-
26
ing to different legal systems have, de facto, dif-ferent meanings. Hence, different entries are cre-ated. Terms with different denominations but conveying the same ?meaning? (concept) are also represented using different entries3. In this case, the entries are linked through a synonymy rela-tion. Figure 2 shows the XML structure of the French term ?trafic intra-alpin?, as defined in the Alpine Convention. The term entry is associated to a unique identifier used to establish relations between volume entries. The example term belongs to the Alpine Con-vention legal system4 (code AC). The entry also bears the information on its status (harmonised or rejected) and its processing status (to be proc-essed, provisionally processed or finalised). In addition, a definition (along with its source) and a context may be given. The definition and context should be extracted from a legal text, which must be identified in the source field. 3.3 Achieving language/legal system interoperability As the project deals with several different le-gal terms, standard techniques used in multilin-gual terminology management need to be adapted to the peculiarities of the specialised language of the law. Indeed, terms in different languages are (generally) defined according to different legal systems and these legal systems cannot be changed. Hence, it is not possible to define a common ?meaning? that could be used as a pivot for language interoperability5. In this respect, legal terminology is closer to general lexicography than to standard terminology. In order to achieve language/legal system interoperability we had several options that are used in general lexicography.  Using a set of bilingual dictionaries is not an option here, as we have to deal with at least 16                                                 3 Variants, acronyms, etc. are not considered as dif-ferent denominations. 4 Strictly speaking, the Alpine Convention does not constitute a legal system per se. 5 Consider for instance the difference between the Italian and the Austrian concepts of journalists? pro-fessional confidentiality. Whereas the Redaktionsge-heimnis explicitly underlines that the journalist can refuse to witness in court in order to keep the profes-sional secret, in Italy the segreto giornalistico must obligatorily be lifted on a judge?s request. The two concepts have overlapping meanings in the two states, however, they diverge greatly with respect to the be-haviour in court.  
language/legal system couples (with alpine Con-vention and EU levels, but without taking into account regional levels). Moreover, such a solu-tion will not reflect the multilingual aspect of the Alpine Convention or the Swiss legal system. Finally, building bilingual volumes between the French and Italian legal systems is far beyond the objectives of the LexALP project. Another solution would be to use an ?Eu-rowordnet like? approach (Vossen, 1998) where a specific language/legal system is used as a pivot and elements of the other systems are linked by equivalent or near-equivalent links. As such an approach artificially puts a language in the pivot position, it generally leads to an ?eth-nocentric? view of the other languages. The ad-vantage being that the architecture uses the bilin-gual competence of lexicographers to achieve multilingualism.  In this project, we chose to use ?interlingual acceptions? (a.k.a. axies) as defined in (S?rasset, 1994) to represent such complex contrastive phenomena as generally described in general lexicography work. In this approach, each ?term meaning? is associated to an interlingual accep-tion (or axie). These axies are used to achieve interoperability as a pivot linking terms of differ-ent languages bearing the same meaning. However, as we are dealing with legal terms (bound to different legal systems), it is generally not possible to find terms in different languages that bear the same meaning. In fact such terms can only be found in the Alpine Convention (which is considered as a legal system expressed in all the considered languages). Hence, we use these terms to achieve interoperability between languages. In this aspect, we are close to Eu-rowordnet?s approach as we use a specific legal system as a pivot, but in our case the pivot itself is generally a quadrilingual set of entries.  These harmonised Alpine Convention terms are linked through an interlingual acception. An axie is a place holder for relations. Each interlin-gual acception may be linked to several term en-tries in the languages volumes through termref elements and to other interlingual acceptions through axieref elements, as illustrated in Figure 3. <axie id="axi..1011424.e">  <termref    idref="ita.traffico_intraalpino.1010654.e"    lang="ita"/>  <termref    idref="fra.trafic_intra-alpin.1010743.e"    lang="fra"/>  <termref   idref="deu.inneralpiner_Verkehr.1011065.e"  
27
  lang="deu"/>  <termref    idref="slo.znotrajalpski_promet.1011132.e"    lang="slo"/>  <axieref idref=""/>  <misc></misc> </axie> Figure 3: XML form of the interlingual acception illustrated Figure 1 The termref relation establishes a direct translation relation between these harmonised equivalents. Then, national legal terms are indi-rectly linked to Alpine Convention terms through the axieref relation as illustrated in Figure 4. 
 Figure 4: An example French term, linked to a quadrilingual Alpine Convention Term. 4 Corpus 4.1 Corpus content The corpus comprises around 3000 legal documents of eight legal systems (Germany, It-aly, France, Switzerland, Austria, Slovenia, European law  and international law with the specific framework of the Alpine Convention,) (see table 1).   AT CH DE FR IT SI AC EU INT 612 119 62 613 490 213 38 791 149 Table 1: Corpus documents for each legal system Documents of the supranational level are pro-vided in up to four languages (subject to avail-ability). National legislation is generally added in the national language (monolingual documents) and in case of Switzerland (multilingual docu-ments) in the three official languages of that na-tion (French, German and Italian). The documents are selected by legal experts of the respective legal systems following prede-fined criteria: ? entire documents (no single paragraphs or excerpts etc.); ? strong relevance to the subjects ?spatial planning and sustainable development? as described in art. 9 of the relative Alpine Convention Protocol; 
? primary sources of the law for every sys-tem at national and international/EU level, i.e. normative texts only (laws, codes etc.);  ? latest amendments and versions of all legislation (at time of collection: June ? August 2005); ? terminological relevance. Each document is classified according to the following (bibliographical) categories: full title, short title, abbreviation, legal system, language, legal hierarchy, legal text type, subfield (1, 2 and 3), official date, official number, published in official journal (date, number, page), ? The bib-liographical information of all documents is stored in a database and can at any time be con-sulted by the user. The subfields have been elaborated and se-lected by a team of legal experts, taking into ac-count the classification specificities followed by the Alpine Convention and the need to classify texts from several different legal systems accord-ing to one common structure. For this reason, the legal experts have subdivided the fields spatial planning and sustainable development into 5 main areas, in accordance with the Alpine Con-vention Protocol dealing with these subjects and subsequently adopted an EU-based model for further subdividing the 5 main topics in such a way that all countries involved could classify their selected documents under a maximum of 3 main items, the first of which must be indicated obligatorily. This classification allows an easy selection of all subsets of documents according to subject field. 
 Figure 5: Example of document classification 
28
<header   lang="ita"  creator="X"  created="Fri Feb 17 10:45:15 CET 2006"> <h.title>  Legge_regionale_25974.14_87.txt </h.title>  <bibID>  17658 </bibID> </header> Figure 6: XML-header of corpus documents <text id="17658"> <body id="17658.b"> <div type="intro" id="17658.b.i"> <p id="17658.b.i.p1"> <title id="17658.b.i.p1.ti1"> LEGGE REGIONALE 15/05/1987, N. 014  Disciplina dell' esercizio [?] di fauna selvatica.  </title> </p> </div> <div type="section" id="17658.b.c0.se1"> <p id="17658.b.c0.se1.p1"> <title id="17658.b.c0.se1.p1.ti1"> Art. 1 </title> </p> <p id="17658.b.c0.se1.p2"> <s id="17658.b.c0.se1.p2.s1"> 1. Sull' intero territorio  regionale la caccia selettiva  per qualita', [?] </s> <s id="17658.b.c0.se1.p2.s2"> a) capriolo: dal 15 maggio al  15 gennaio;  </s> <s id="17658.b.c0.se1.p2.s3"> b) cinghiale: dal 15 giugno  al 15 gennaio;  </s> </p> <p id="17658.b.c0.se1.p3"> <s id="17658.b.c0.se1.p3.s1"> 2. E' ammesso l' uso [?]  </s> </p> </div> </body> </text> Figure 7: XML-structure of corpus document 4.2 Structural organization of corpus data Collected in raw text format (one file for each legal text) the documents are first transformed into XML-structured files and in a second step inserted into the database.  The XML-annotation is done in compliance with the Corpus Encoding Standard for XML (XCES) 6 . Slightly simplified, the provided schema7 serves to add structural information to the documents. Each text is segmented into sub-sections like: preamble, chapter, section, para-                                                6 http://www.cs.vassar.edu/XCES/ 7 http://www.cs.vassar.edu/XCES/schema/xcesDoc.xsd 
graph, title and sentence. Furthermore, a link to the classification data (bibliographic data base) is inserted and, in case of multilingual documents, alignment is done at sentence level. The XML-annotated documents hold all the information needed for the insertion into the cor-pus database, such as structural mark-up and bib-liographical information. The full text documents are transformed into sets of database entries, which can be imported into the database. 4.3 Technical organization of corpus data Following the bistro approach as realized for the Corpus Ladin dl?Eurac (CLE) (Streiter et al 2004) the corpus data is stored in a relational database (PostgreSQL). The information present in the XML-annotated documents is distributed among four main tables: document_info, cor-pus_words, corpus_structure, corpus_alignment.   The four tables can be described as follows: document_info: This table holds the meta-information about the documents; each category (like full title, short title, abbreviation, legal sys-tem, language, etc.) is represented by a separate column. For each legal document one entry (one row) with unique identification number is added to the table. These identification numbers are cited in the XML-header of the corpus docu-ments. corpus_words: This table holds the actual text of the collected documents. Instead of stor-ing entire paragraphs as it was done during the creation of CLE, for this corpus a different ap-proach is being tested. Every annotated text is split into an indexed sequence of words, starting with counter one. Once inserted into the database a text is stored as a set of tuples composed of word, position in text and document id (as a ref-erence to the document information).  corpus_structure: This table holds all infor-mation about the internal structure of the docu-ments. Titles, sentences, paragraphs etc. are stored by indicating starting and ending point of the section. For each segment a tuple of segment type, segment id, starting point (indicated by the index of the first word), ending point (indicated by the index of the last word) and document id is added. corpus_alignment: This table defines the alignment of multilingual documents. By provid-ing one column for each language the texts are aligned via the document ids or via the ids of single segments.  
29
The tables are interconnected by explicitly stated references. That means that the columns of one table refer to the values of a certain column of another table. As shown in figure 8 all tables hold a column document_id that refers to the document id of the table document_info. Fur-thermore, the table corpus_structure holds refer-ences to the column position of the table cor-pus_words.  
 Figure 8: Interconnection of tables 5 Searching the corpus Due to the fine-grained classification (see section 4.1) and the structural mark-up (see section 4.2) of all corpus documents, corpus searches can be restricted in the following ways: ? by specifying a subset of corpus docu-ments over which the search should be carried out (e.g. all documents of legal system CH with language French); ? by choosing the type of unit to be dis-played (whole paragraphs <p>, sentences <s>, titles <title>, ?); ? by searching for whole words only (ex-act match) or parts of words (fuzzy match); ? by restricting the number of hits to be displayed at a time. For searches in multilingual documents it will be possible to search for aligned segments, specify-ing search word as well as target translation. For example, the user could search for all alignments of German-Italian sentences that contain the word Umweltschutz translated as tutela ambien-tale (and not with protezione dell?ambiente). Figure 9 shows a simple interface for searching monolingual documents. 
 Figure 9: Example search over monolingual documents 6 Interaction term bank and corpus Term bank and corpus are independent compo-nents which together form the LexALP Informa-tion System. The interaction between corpus and term bank will concern in particular 1) corpus segments used as contexts and definitions in the termino-logical entries, 2) short source references in the term bank (and the associated sets of biblio-graphical information) and 3) legal terms. 6.1 Entering data into term bank When adding citations to a term bank entry, the relative bibliographic information will automati-cally be counterchecked with the contents of the bibliographical database. In case the information about the cited document is already present in the DB, a link to the term bank can be added. Oth-erwise the terminologist is asked to provide all information about the new source to the biblio-graphic database and later create the link. Next to static contexts and definitions present for each terminological entry, each entry will show a button for the dynamic creation of con-texts. Hitting the button will start a context search in the corpus and return all sentences con-taining the term under consideration. 6.2 Searching the corpus When searching the corpus the user will have the opportunity to highlight terms present in the term bank. In the same way standardised or rejected terms can be brought out. Via a link it will then 
30
be possible to directly access the term bank entry for the term found in the corpus. In general each corpus segment is linked to the full set of bibliographic information of the document that the segment is part of. Accessing the source information will lead the user to a de-tailed overview as shown in figure 4. 7 Conclusion In this paper, we have presented the LexALP information system, used to collect, describe and harmonise the terminology used by the Alpine Convention and to link it with national legal ter-minology of the alpine Convention?s member states. Even if we currently give a specific focus on spatial planning and sustainable development, the project is not restricted to these fields and the methodology and tools developed can be adapted to legal terminology of other fields.  In this paper we also proposed a solution to the encoding of multilingual legal terminologies in a context where standard techniques used in multilingual terminology management usually fail. The terminology developed and the corpus used for its development will be accessible on-line for the stakeholders and the wider public through the LexALP information system. 
Acknowledgements The LexALP research project started in Janu-ary 2005 thanks to the funds granted by the IN-TERREG IIIB ?Alpine Space? Programme, a Community Initiative Programme funded by the European Regional Development Fund. References Gilles S?rasset. 1994. Interlingual Lexical Organisa-tion for Multilingual Lexical Databases in NADIA.  In Makoto Nagao, editor, COLING-94, volume 1, pages 278?282, August. Streiter, O., Stuflesser, M. & Ties, I. (2004). CLE, an aligned Tri-lingual Ladin-Italian-German Corpus. Corpus Design and Interface, LREC 2004, Work-shop on "First Steps for Language Documentation of Minority Languages: Computational Linguistic Tools for Morphology, Lexicon and Corpus Com-pilation" Lisbon, May 24, 2004. Vossen, Piek. 1998. Introduction to EuroWordNet. In Nancy Ide, Daniel Greenstein, and Piek Vossen, editors, Special Issue on EuroWordNet, Computers and the Humanities, 32(2-3): 73-89. Wright, Sue Ellen 2001. Data Categories for Termi-nology Management. In Sue Ellen Wright & Gerhard Budin, editors, Handbook of Terminology Management, volume 2, pages 552-569.  
31
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 937?944,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Legal Terminology on the Jibiki Platform:
The LexALP Project
Gilles Se?rasset, Francis Brunet-Manquat
Universite? Joseph Fourier,
Laboratoire CLIPS-IMAG, BP 53
38041 Grenoble Cedex 9 - France,
Gilles.Serasset@imag.fr
Francis.Brunet-Manquat@imag.fr
Elena Chiocchetti
EURAC Research
Viale Druso 1
39100 Bozen/Bolzano - Italy
Elena.Chiocchetti@eurac.edu
Abstract
This paper presents the particular use of
?Jibiki? (Papillon?s web server develop-
ment platform) for the LexALP1 project.
LexALP?s goal is to harmonise the ter-
minology on spatial planning and sustain-
able development used within the Alpine
Convention2, so that the member states
are able to cooperate and communicate
efficiently in the four official languages
(French, German, Italian and Slovene). To
this purpose, LexALP uses the Jibiki plat-
form to build a term bank for the con-
trastive analysis of the specialised termi-
nology used in six different national legal
systems and four different languages. In
this paper we present how a generic plat-
form like Jibiki can cope with a new kind
of dictionary.
1 Introduction
One of the most time-consuming hindrances to
supranational law drafting and convention nego-
tiation is the lack of understanding among nego-
tiators and technical writers. This is not only due
to the fact that different languages are involved,
but mainly to the inherent differences in the legal
systems. Countries that speak the same language
(like France and part of Switzerland) may use the
same word to represent different legal concepts3,
1Legal Language Harmonisation System for Environment
and Spatial Planning within the Multilingual Alps
2http://www.convenzionedellealpi.org
3E.g.: In the German-speaking province of Bolzano Italy
the Landeshauptmann is the president of the provincial coun-
cil, with much more limited competence that the Austrian
Landeshauptmann, who is head of one of the states (Bundes-
land) that are part of the Austrian federation.
as defined in their respective legal traditions. The
same concept may be referred to in different ways
according to the legal system4. Also, terms that
may superficially seem to be translations of each
other can represent different legal notions5.
In order to concretely address these problems,
several institutions representing translators, ter-
minologists, legal experts and computational lin-
guists joined in the LexALP project, co-funded by
EU?s INTERREG IIIb Alpine Space programme.
The objective of the project is to compare the spe-
cialised terminology of six different national legal
systems (Austria, France, Germany, Italy, Switzer-
land and Slovenia) and three supranational sys-
tems (EU law, international law and the particu-
lar framework of the Alpine Convention) in the
four official languages of the Al-pine Convention,
which is an international framework agreement
signed by all countries of the Alpine arc and the
EU. This contrastive analysis serves as a basis for
the work of a group of experts (the Harmonising
Group) who will determine translation equivalents
in French, Italian, German and Slovene (one-to-
one correspondence) in the fields of spatial plan-
ning and sustainable development for use within
the Convention, thus optimising the understanding
between the Alpine states at supranational level.
The tools that are to be developed for these ob-
jectives comprise a corpus bank and a term bank.
The corpus bank is developed by adapting the
bistro system (Streiter et al, 2006; Streiter et al,
2004). The term bank is based on the Jibiki plat-
4See for instance the European Union use of chien drogue
while French legislation calls them chien renifleur.
5For example, in Italy an elezione suppletiva is commonly
held whenever an elected deputy or senator either resigns or
dies. In Germany in such cases the first non-elected candidate
is called to parliament. Ersatzwahlen are a rare phenomenon,
foreseen in some very specific cases.
937
form (Mangeot et al, 2003; Se?rasset, 2004).
This paper details the way the Jibiki platform is
used in order to cope with a new dictionary struc-
ture. The platform provides dictionary access and
edition services without any new and specific de-
velopment.
After a brief overview of the Jibiki platform, we
describe the choices made by the LexALP team for
the structure and organisation of their term bank.
Then, we show how this structure is described us-
ing Jibiki metadata description languages. Finally,
we give some details on the resulting LexALP In-
formation System.
2 Jibiki, The Papillon Dictionary
Development Platform
2.1 Overview
The Jibiki platform has been designed to support
the collaborative development of multilingual dic-
tionaries. This platform is used as the basis of the
Papillon project web site6.
This platform offers several services to its users:
? access to many different dictionaries from a
single easy to use query form,
? advance search for particular dictionary en-
tries through an advanced search form,
? creation and edition of dictionary entries.
What makes the Jibiki platform quite unique is
the fact that it provides these services regardless of
the dictionary structure. In other words it may be
used by any dictionary builder to give access and
collaboratively edit any dictionary, provided that
the resulting dictionary will be freely accessible
online.
2.2 Jibiki Platform Architecture
The Jibiki platform is a framework used to set up
a web server dedicated to the collaborative devel-
opment of multilingual dictionaries. All services
provided by the platform are organised as classi-
cal 3-tier architectures with a presentation layer
(in charge of the interface with users), a business
layer (which provides the services per se) and a
data layer (in charge of the storage of persistent
data).
In order to adapt the Jibiki platform to a new
dictionary, the dictionary manager does not have
6http://www.papillon-dictionary.org/
Papillon Application (java + enhydra
presentation
layer
serveur
HTTP
(apache)
Relational database
(PostgreSQL)
XML-UTF8
HTML
CSS
javascript
+
CGI
WML
xhtml
chtml
business layer data layer
J
D
B
C
Lexie
axie
Dico
Historique
Utilisateur
...
Data
validation
Mailing list
archive
Users/Groups
Contributions
management
Volume
Information
sharing
requests
management
Information
Message
Figure 1: The Jibiki platform general architecture
to write specific java code nor specific dynamic
web pages. The only necessary information used
by the platform consists in:
? a description of the dictionary volumes and
their relations,
? a mapping between the envisaged dictionary
structure and a simple hypothetical dictionary
structure (called CDM)7,
? the definition of the XML structure of each
envisaged dictionary volume by way of XML
schemas,
? the development of a specific edition in-
terface as a standard xhtml form (that can
be adapted from an automatically generated
draft).
3 The LexALP Terminology Structure
3.1 Overview
The objective of the LexALP project is to com-
pare the specialised terminology of six different
national legal systems and three supranational sys-
tems in four different languages, and to harmonise
it, thus optimising communication between the
Alpine states at supranational level. To achieve
this objective, the terminology of the Alpine Con-
vention is described and compared to the equiva-
lent terms used in national legislation. The result-
ing terminology entries feed a specific term bank
that will support the harmonisation work.
As the project deals with legal terms, which re-
fer to concepts that are proper of the considered
national law or international convention, equiva-
lence problems are the norm, given that concepts
are not ?stable? between the different national leg-
islations. Standard terminology techniques for
other fields can not be applied to the field of law,
where the standardisation approach (Felber, 1987;
7This mapping is sufficient for simple dictionary access
938
Felber, 1994) is not applicable. For this, we chose
to use ?acceptions? as they are defined in the Pa-
pillon dictionary (Se?rasset, 1994) to represent the
equivalence links between concepts of the differ-
ent legal systems (Arntz, 1993).
Italian
Slovene
German
French
inneralpiner Verkehr
znotrajalpski promet
transport intra-alpin
circulation intra-alpine
trafic intra-alpin
traffico intraalpino
trasporto intraalpino
Figure 2: An Alpine Convention concept in four
languages
The example given in figure 2 shows a concept
defined in the Alpine Convention. This concept
has the same definition in the four languages of
the Alpine Convention but is expressed by differ-
ent denominations. The Alpine Convention also
uses the terms ?circulation intra-alpine? or ?trans-
port intra-alpin? which are identified as synonyms
by the terminologist.
This illustrates the first goal of the LexALP
project. In different texts, the same concept may
be realised by different terms in the same lan-
guage. This may lead to inefficient communica-
tion. Hence, a single term has to be determined
as part of a harmonised quadruplet of transla-
tion equivalents. The other denominations will be
represented in the term bank as non-harmonised
synonyms in order to direct drafting and translat-
ing within the Alpine Convention towards a more
clear and consistent terminology use for interlin-
gual and supranational communication.
In this example, the lexicographers and jurists
did not identify any existing concept in the differ-
ent national laws that could be considered close
enough to the concept analysed. This is coherent
with the minutes from the French National Assem-
bly which clearly states that the term ?trafic intra-
alpin? (among others) should be clarified by a dec-
laration to be added to the Alpine Convention.
Figure 3 shows an analogous quadrilingual ex-
ample where the Alpine Convention concept may
be related to a legal term defined in the French
laws. In this example the French term is distin-
guished from the Alpine Convention terms, be-
cause these concepts belong to different legal sys-
Italian
Slovene
German
French
principio di precauzione
Vorsorgeprinzip
nacelo preventive
principe de pr?caution
principe de pr?caution
Figure 3: A quadrilingual term extracted from the
Alpine Convention with reference to its equivalent
at French national level
tems (and are not identically defined in them).
Hence, the terminologists created distinct accep-
tions, one for each concept. These acceptions are
related by a translation link.
This illustrates the second goal of the project,
which is to help with the fine comprehension of the
Alpine Convention and with the detailed knowl-
edge necessary to evaluate the implementation and
implementability of the convention in the different
legal systems.
As a by-product of the project, one can see that
there is an indirect relation between concepts from
different national legal systems (by way of their
respective relation to the concepts of the Alpine
Convention). However, establishing these indi-
rect relations is not one of the main objectives of
the LexALP project and would require more direct
contrastive analysis.
3.2 Macro- and Micro- Structures
The LexALP term bank consists in 5 volumes
(for French, German, Italian, Slovene and English)
containing all term descriptions (grammatical in-
formation, definition, contexts etc.). The transla-
tion links are established through a central accep-
tion volume. Figure 2 and 3 show examples of
terms extracted from the Alpine Convention, syn-
onymy links in the French and Italian volumes,
as well as inter-lingual relations by way of accep-
tions.
All language volumes share the same mi-
crostructure. This structure is stored in XML.
Figure 4 shows the xml structure of the French
term ?trafic intra-alpin?, as defined in the Alpine
Convention. The term entry is associated to a
unique identifier used to establish relations be-
tween volume entries. Each term entry belongs
to one (and only one) legal system. The exam-
ple term belongs to the Alpine Convention legal
939
<entry id="fra.trafic_intra-alpin.1010743.e"
lang="fra"
legalSystem="AC"
process_status="FINALISED"
status="HARMONISED">
<term>trafic intra-alpin</term>
<grammar>n.m.</grammar>
<domain>Transport</domain>
<usage frequency="common"
geographical-code="INT"
technical="false"/>
<relatedTerm isHarmonised="false"
relationToTerm="Synonym"
termref="">
transport intra-alpin
</relatedTerm>
<relatedTerm isHarmonised="false"
relationToTerm="Synonym"
termref="">
circulation intra-alpine
</relatedTerm>
<definition>
[T]rafic constitue? de trajets ayant leur
point de de?part et/ou d?arrive?e a` l?inte?-
rieur de l?espace alpin.
</definition>
<source url="">Prot. Transp., art. 2</source>
<context url="http://www...">
Des projets routiers a` grand de?bit pour
le trafic intra-alpin peuvent e?tre re?alise?s,
si [...].
</context>
</entry>
Figure 4: XML form of the term ?trafic intra-
alpin?.
system8 (code AC). The set of known legal sys-
tems includes of course countries belonging to the
Alpine Space (Austria, France, Germany, Italy,
Slovenia and Switzerland9) but also international
treaties or conventions. The entry also bears the
information on its status (harmonised or rejected)
and its process status (to be processed, provision-
ally processed or finalised).
The term itself and its part of speech is also
given, with the general domain to which the term
belongs, along with some usage notes. In these us-
age notes, the attribute geographical-code
allows for discrimination between terms defined
in national (or federal) laws and terms defined in
regional laws as in some of the countries involved
legislative power is distributed at different levels.
Then the term may be related to other terms.
These relations may lead to simple strings of
texts (as in the given example) or to autonomous
term entries in the dictionary by the use of the
termref attribute. The relation itself is specified
in the relationToTerm attribute. The current
schema allows for the representation of relations
8Strictly speaking, the Alpine Convention does not con-
stitute a legal system per se.
9Also Liechtenstein and Monaco are parties to the Alpine
Convention, however, their legal systems are not terminolog-
ically processed within LexALP.
between concepts (synonymy, hyponymy and hy-
peronymy), as well as relations between graphies
(variant, abbreviation, acronym, etc.).
Then, a definition and a context may be given.
Both should be extracted from legal texts, which
must be identified in the source field.
An interlingual acception (or axie) is a place
holder for relations. Each interlingual acception
may be linked to several term entries in the lan-
guage volumes through termref elements and
to other interlingual acceptions through axieref
elements, as illustrated in figure 5.
<axie id="axi..1011424.e">
<termref
idref="ita.traffico_intraalpino.1010654.e"
lang="ita"/>
<termref
idref="fra.trafic_intra-alpin.1010743.e"
lang="fra"/>
<termref
idref="deu.inneralpiner_Verkehr.1011065.e"
lang="deu"/>
<termref
idref="slo.znotrajalpski_promet.1011132.e"
lang="slo"/>
<axieref idref=""/>
<misc></misc>
</axie>
Figure 5: XML form of the interlingual acception
illustated in figure 2.
4 LexALP Information System
4.1 Overview
Building such a term bank can only be envisaged
as a collaborative work involving terminologists,
translators and legal experts from all the involved
countries. Hence, the LexALP consortium has set
up a centralised information system that is used to
gather all textual and terminological data.
This information system is organized in two
main parts. The first one is dedicated to corpus
management. It allows the users to upload legal
texts that will serve to bootstrap the terminology
work (by way of candidate term extraction) and
to let terminologists find occurrences of the term
they are working on, in order for them to provide
definitions or contexts.
The second part is dedicated to terminology
work per se. It has been developed with the Jibiki
platform described in section 2. In this section, we
show the LexALP Information System functional-
ity, along with the metadata required to implement
it with Jibiki.
940
4.2 Dictionary Browsing
The first main service consists in browsing the cur-
rently developed dictionary. It consists in two dif-
ferent query interfaces (see figures 6 and 7) and a
unique result presentation interface (see figure 10).
Figure 6: Simple search interface present on all
pages of the LexALP Information System
<dictionary-metadata
[...]
d:category="multilingual"
d:fullname="LexALP multilingual Term Base"
d:name="LexALP"
d:owner="LexALP consortium"
d:type="pivot">
<languages>
<source-language d:lang="deu"/>
<source-language d:lang="fra"/>
<target-language d:lang="deu"/>
<target-language d:lang="fra"/>
[...]
</languages>
[...]
<volumes>
<volume-metadata-ref name="LexALP_fra"
source-language="fra"
xlink:href="LexALP_fra-metadata.xml"/>
<volume-metadata-ref name="LexALP_deu"
source-language="deu"
xlink:href="LexALP_deu-metadata.xml"/>
[...]
<volume-metadata-ref name="LexALP_axi"
source-language="axi"
xlink:href="LexALP_axi-metadata.xml"/>
</volumes>
<xsl-stylesheet name="LexALP" default="true"
xlink:href="LexALP-view.xsl"/>
<xsl-stylesheet name="short-list"
xlink:href="short-list-view.xsl"/>
</dictionary-metadata>
Figure 8: Excerpt of the dictionary descriptor
In the provided examples, the user of the sys-
tem specifies an entry (a term), or part of it, and
a language in which the search is to be done. The
expected behaviour may only be achieved if :
? the system knows in which volume the search
is to be performed,
? the system knows where, in the volume entry,
the headword is to be found,
? the system is able to produce a presentation
for the retrieved XML structures.
However, as the Jibiki platform is entirely in-
dependent of the underlying dictionary structure
<volume-metadata
[...]
dbname="lexalpfra"
dictname="LexALP"
name="LexALP_fra"
source-language="fra">
<cdm-elements>
<cdm-entry-id index="true"
xpath="/volume/entry/@id"/>
<cdm-headword d:lang="fra" index="true"
xpath="/volume/entry/term/text()"/>
<cdm-pos d:lang="fra" index="true"
xpath="/volume/entry/grammar/text()"/>
[...]
</cdm-elements>
<xmlschema-ref xlink:href="lexalp.xsd"/>
<template-entry-ref
xlink:href="lexalp_fra-template.xml"/>
<template-interface-ref
xlink:href="lexalp-interface.xhtml"/>
</volume-metadata>
Figure 9: Excerpt of a volume descriptor
(which makes it highly adaptable), the expected
result may only be achieved if additional metadata
is added to the system.
These pieces of information are to be found in
the mandatory dictionary descriptor. It consists
in a structure defined in the Dictionary Metadata
Language (DML), as set of metadata structures
and a specific XML namespace defined in (Man-
geot, 2001).
Figure 8 gives an excerpt of this descriptor. The
metadata first identify the dictionary by giving it
a name and a type. In this example the dictionary
is a pivot dictionary (DML also defines monolin-
gual and bilingual dictionary types). The descrip-
tor also defines the set of source and target lan-
guages. Finally, the dictionary is defined as a set
of volumes, each volume being described in an-
other file. As the LexALP dictionary is a pivot
dictionary, there should be a volume for the artifi-
cial language axi, which is the pivot volume.
Figure 9 shows an excerpt of the description of
the French volume of the LexALP dictionary. Af-
ter specifying the name of the dictionary, the de-
scriptor provides a set of cdm-elements. These el-
ements are used to identify standard dictionary el-
ements (that can be found in several dictionaries)
in the specific dictionary structure. For instance,
the descriptor tells the system that the headword of
the dictionary (cdm-headword) is to be found
by applying the specified xpath10 to the dictionary
structure.
With this set of metadata, the system knows
that:
10an xpath is a standard way to extract a sub-part of any
XML structure
941
Figure 7: Advanced search interface
? requests on French should be directed to the
LexALP fra volume,
? the requested headword will be found in the
text of the term element of the volume
entry element,
Hence, the system can easily perform a request
and retrieve the desired XML entries. The only
remaining step is to produce a presentation for
the user, based on the retrieved entries. This is
achieved by way of a xsl11 stylesheet. This
stylesheet is specified either on the dictionary level
(for common presentations) or on the volume level
(for volume specific presentation).
In the given example, the dictionary adminis-
trator provided two presentations called LexALP
(the default one, as shown in figure 10) and
short-list, both of them defined in the dic-
tionary descriptor.
This mechanism allows for the definition of pre-
sentation outputs in xhtml (for online browsing)
or for presentation output in pdf (for dictionary
export and print).
4.3 Dictionary Edition
The second main service provided by the Jibiki
platform is to allow terminologists to collabora-
tively develop the envisaged dictionary. In this
sense, Jibiki is quite unique as it federates, on the
very same platform the construction and diffusion
of a structured dictionary.
As before, Jibiki may be used to edit any dictio-
nary. Hence, it needs some metadata information
in order to work:
? the complete definition of the dictionary entry
structures by way of an XML schema,
? a template describing an empty entry struc-
ture,
11XSL is a standard way to transform an XML structure
into another structure (XML or not).
Current XML 
structure
Empty 
XHTML form
Instanciate Form
Instanciated 
XHTML form
Online edition
Network
CGI decoding
Figure 11: Basic flow chart of the editing service
? a xhtml form used to edit a dictionary entry
structure (which can be adapted from an au-
tomatically generated one).
When this information is known, the Jibiki plat-
form provides a specific web page to edit a dictio-
nary entry structure. As shown in figure 11, the
XML structure is projected into the given empty
XHTML form. This form is served as a standard
web page on the client browser. After manual edit-
ing, the resulting form is sent back to the Jibiki
platform as CGI12 data. The Jibiki platform de-
codes this data and modifies the edited XML struc-
ture accordingly. Then the process iterates as long
as necessary. Figure 12 shows an example of such
a dynamically created web page.
After each update, the resulting XML structure
is stored in the dictionary database. However, it
is not available to other users until it is marked as
finished by the contributor (by clicking on the
save button). If the contributor leaves the web
page without saving the entry, he will be able to
retrieve it and finish his contribution later.
12Common Gateway Interface
942
Figure 10: Query result presentation interface
Figure 12: Edition interface of a LexALP French entry
943
At each step of the contribution (after each up-
date) and at each step of dictionary editing (after
each save), the previous state is saved and the con-
tributor (or the dictionary administrator) is able to
browse the history of changes and to revert the en-
try to a previous version.
5 Conclusion
In this article we give some details on the way the
Jibiki platform allows the diffusion and the online
editing of a dictionary, regardless of his structure
(monolingual, bilingual (directed or not) or multi-
lingual (multi-bilingual or pivot based)).
Initially developed to support the editing of the
Papillon multilingual dictionary13, the Jibiki plat-
form proved useful for the development of other
very different dictionaries. It is currently used for
the development of the GDEF (Grand Dictionnaire
Estonien-Franc?ais) project14 an Estonian French
bilingual dictionary. This article also shows the
use of the platform for the development of a Eu-
ropean term bank for legal terms on spatial plan-
ning and sustainable development in the LexALP
project.
Adapting the Jibiki platform to a new dictio-
nary requires the definition of several metadata in-
formation, taking the form of several XML files.
While not trivial, this metadata definition does not
require any competence in computer development.
This adaptation may therefore also be done by ex-
perimented linguists. Moreover, when the dictio-
nary microstructure needs to evolve, this evolu-
tion does not require any programming. Hence the
Jibiki platform gives linguists great liberty in their
decisions.
Another positive aspect of Jibiki is that it inte-
grates diffusion and editing services on the same
platform. This allows for a tighter collaboration
between linguists and users and also allows for the
involvement of motivated users to the editing pro-
cess.
The Jibiki platform is freely available for use by
any willing team of lexicographer/terminologists,
provided that the resulting dictionary data will be
freely available for online browsing.
In this article, we also presented the choices
made by the LexALP consortium to structure a
term bank used for the description and harmonisa-
tion of legal terms in the domain of spacial plan-
13http://www.papillon-dictionary.org/
14http://estfra.ee/
ning and sustainable development of the Alpine
Space. In such a domain, classical techniques
used in multilingual terminology cannot be used
as the term cannot be defined by reference to a sta-
ble/shared semantic level (each country having its
own set of non-equivalent legal concepts).
References
Reiner Arntz. 1993. Terminological equivalence
and translation. In H. Sonneveld and K. Loen-
ing, editors, Terminology. Applications in Interdisci-
plinary Communication, pages 5?19. Amsterdam et
Philadelphia, John Benjamins Publishing Company.
Helmut Felber, 1987. Manuel de terminologie. UN-
ESCO, Paris.
Helmut Felber. 1994. Terminology research: Its rela-
tion to the theory of science. ALFA, 8(7):163?172.
Mathieu Mangeot, Gilles Se?rasset, and Mathieu
Lafourcade. 2003. Construction collaborative d?une
base lexicale multilingue, le projet Papillon. TAL,
44(2):151?176.
Mathieu Mangeot. 2001. Environnements centralise?s
et distribue?s pour lexicographes et lexicologues en
contexte multilingue. The`se de nouveau doctorat,
spe?cialite? informatique, Universite? Joseph Fourier
Grenoble I, Septembre.
Gilles Se?rasset. 1994. Interlingual lexical organi-
sation for multilingual lexical databases in nadia.
In Makoto Nagao, editor, COLING-94, volume 1,
pages 278?282, August.
Gilles Se?rasset. 2004. A generic collaborative plat-
form for multilingual lexical database development.
In Gilles Se?rasset, editor, COLING 2004 Multilin-
gual Linguistic Resources, pages 73?79, Geneva,
Switzerland, August 28. COLING.
Oliver Streiter, Leonhard Voltmer, Isabella Ties, and
Natascia Ralli. 2004. BISTRO, the online plat-
form for terminology management: structuring ter-
minology without entry structures. In The transla-
tion of domain specific languages and multilingual
terminology, number 3 in Linguistica Antverpien-
sia New Series. Hoger Instituut voor Vertalers en
Tolken, Hogeschool Antwerpen.
Oliver Streiter, Leonhard Voltmer, Isabella Ties, Natas-
cia Ralli, and Verena Lyding. 2006. BISTRO: Data
structure, term tools and interface. Terminology Sci-
ence and Research, 16.
944
