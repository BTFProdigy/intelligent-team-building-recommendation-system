Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 967?975, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Translation Quality by Discarding Most of the Phrasetable
J Howard Johnson and Joel Martin
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
George Foster and Roland Kuhn
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Abstract
It is possible to reduce the bulk of phrase-
tables for Statistical Machine Translation us-
ing a technique based on the significance
testing of phrase pair co-occurrence in the
parallel corpus. The savings can be quite
substantial (up to 90%) and cause no reduc-
tion in BLEU score. In some cases, an im-
provement in BLEU is obtained at the same
time although the effect is less pronounced
if state-of-the-art phrasetable smoothing is
employed.
1 Introduction
An important part of the process of Statistical Ma-
chine Translation (SMT) involves inferring a large
table of phrase pairs that are translations of each
other from a large corpus of aligned sentences.
These phrase pairs together with estimates of con-
ditional probabilities and useful feature weights,
called collectively a phrasetable, are used to match
a source sentence to produce candidate translations.
The choice of the best translation is made based
on the combination of the probabilities and feature
weights, and much discussion has been made of how
to make the estimates of probabilites, how to smooth
these estimates, and what features are most useful
for discriminating among the translations.
However, a cursory glance at phrasetables pro-
duced often suggests that many of the translations
are wrong or will never be used in any translation.
On the other hand, most obvious ways of reducing
the bulk usually lead to a reduction in translation
quality as measured by BLEU score. This has led to
an impression that these pairs must contribute some-
thing in the grand scheme of things and, certainly,
more data is better than less.
Nonetheless, this bulk comes at a cost. Large ta-
bles lead to large data structures that require more
resources and more time to process and, more im-
portantly, effort directed in handling large tables
could likely be more usefully employed in more fea-
tures or more sophisticated search.
In this paper, we show that it is possible to prune
phrasetables using a straightforward approach based
on significance testing, that this approach does not
adversely affect the quality of translation as mea-
sured by BLEU score, and that savings in terms of
number of discarded phrase pairs can be quite sub-
stantial. Even more surprising, pruning can actu-
ally raise the BLEU score although this phenomenon
is less prominent if state of the art smoothing of
phrasetable probabilities is employed.
Section 2 reviews the basic ideas of Statistical
Machine Translation as well as those of testing sig-
nificance of associations in two by two contingency
tables departing from independence. From this, a
filtering algorithm will be described that keeps only
phrase pairs that pass a significance test. Section 3
outlines a number of experiments that demonstrate
the phenomenon and measure its magnitude. Sec-
tion 4 presents the results of these experiments. The
paper concludes with a summary of what has been
learned and a discussion of continuing work that
builds on these ideas.
967
2 Background Theory
2.1 Our Approach to Statistical Machine
Translation
We define a phrasetable as a set of source phrases (n-
grams) s? and their translations (m-grams) t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source / tar-
get n,m-grams observed in a word-aligned parallel
corpus. These joint counts are estimated using the
phrase induction algorithm described in (Koehn et
al., 2003), with symmetrized word alignments gen-
erated using IBM model 2 (Brown et al, 1993).
Phrases are limited to 8 tokens in length (n,m ? 8).
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is the
most likely translation of s. To make search more
efficient, we use the Viterbi approximation and seek
the most likely combination of t and its alignment a
with s, rather than just the most likely t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1...t?K ; s?k are source
phrases such that s = s?j1 ...s?jK ; and s?k is the trans-
lation of the kth target phrase t?k.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al , 2001) on a development corpus. The
features used are: the length of t; a single-parameter
distortion penalty on phrase reordering in a, as de-
scribed in (Koehn et al, 2003); phrase translation
model probabilities; and 4-gram language model
probabilities log p(t), using Kneser-Ney smooth-
ing as implemented in the SRILM toolkit (Stolcke,
2002).
Phrase translation model probabilities are features
of the form:
log p(s|t,a) ?
K?
k=1
log p(s?k|t?k)
i.e., we assume that the phrases s?k specified by a are
conditionally independent, and depend only on their
aligned phrases t?k.
The ?forward? phrase probabilities p(t?|s?) are not
used as features, but only as a filter on the set of
possible translations: for each source phrase s? that
matches some ngram in s, only the 30 top-ranked
translations t? according to p(t?|s?) are retained. One
of the reviewers has pointed out correctly that tak-
ing only the top 30 translations will interact with the
subject under study; however, this pruning technique
has been used as a way of controlling the width of
our beam search and rebalancing search parameters
would have complicated this study and taken it away
from our standard practice.
The phrase translation model probabilities are
smoothed according to one of several techniques as
described in (Foster et al, 2006) and identified in the
discussion below.
2.2 Significance testing using two by two
contingency tables
Each phrase pair can be thought of as am n,m-gram
(s?, t?) where s? is an n-gram from the source side of
the corpus and t? is an m-gram from the target side
of the corpus.
We then define: C(s?, t?) as the number of parallel
sentences that contain one or more occurrences of
s? on the source side and t? on the target side; C(s?)
the number of parallel sentences that contain one or
more occurrences of s? on the source side; and C(t?)
the number of parallel sentences that contain one or
more occurrences of t? on the target side. Together
with N , the number of parallel sentences, we have
enough information to draw up a two by two contin-
gency table representing the unconditional relation-
ship between s? and t?. This table is shown in Table
1.
A standard statistical technique used to assess the
importance of an association represented by a con-
tingency table involves calculating the probability
that the observed table or one that is more extreme
could occur by chance assuming a model of inde-
pendence. This is called a significance test. Intro-
ductory statistics texts describe one such test called
the Chi-squared test.
There are other tests that more accurately apply
to our small tables with only two rows and columns.
968
Table 1: Two by two contingency table for s? and t?
C(s?, t?) C(s?)? C(s?, t?) C(s?)
C(t?)? C(s?, t?) N ? C(s?)? C(t?) + C(s?, t?) N ? C(s?)
C(t?) N ? C(t?) N
In particular, Fisher?s exact test calculates probabil-
ity of the observed table using the hypergeometric
distibution.
ph(C(s?, t?)) =
(
C(s?)
C(s?, t?)
)(
N ? C(s?)
C(t?)? C(s?, t?)
)
(
N
C(t?)
)
The p-value associated with our observed table is
then calculated by summing probabilities for tables
that have a larger C(s?, t?)).
p-value(C(s?, t?)) =
??
k=C(s?,t?)
ph(k)
This probability is interpreted as the probability
of observing by chance an association that is at least
as strong as the given one and hence its significance.
Agresti (1996) provides an excellent introduction to
this topic and the general ideas of significance test-
ing in contingency tables.
Fisher?s exact test of significance is considered a
gold standard since it represents the precise proba-
bilities under realistic assumptions. Tests such as the
Chi-squared test or the log-likelihood-ratio test (yet
another approximate test of significance) depend on
asymptotic assumptions that are often not valid for
small counts.
Note that the count C(s?, t?) can be larger or
smaller than c(s?, t?) discussed above. In most cases,
it will be larger, because it counts all co-occurrences
of s? with t? rather than just those that respect the
word alignment. It can be smaller though because
multiple co-occurrences can occur within a single
aligned sentence pair and be counted multiple times
in c(s?, t?). On the other hand, C(s?, t?) will not count
all of the possible ways that an n,m-grammatch can
occur within a single sentence pair; it will count the
match only once per sentence pair in which it occurs.
Moore (2004) discusses the use of signifi-
cance testing of word associations using the log-
likelihood-ratio test and Fisher?s exact test. He
shows that Fisher?s exact test is often a practical
method if a number of techniques are followed:
1. approximating the logarithms of factorials us-
ing commonly available numerical approxima-
tions to the log gamma function,
2. using a well-known recurrence for the hyperge-
ometic distribution,
3. noting that few terms usually need to be
summed, and
4. observing that convergence is usually rapid.
2.3 Significance pruning
The idea behind significance pruning of phrasetables
is that not all of the phrase pairs in a phrasetable are
equally supported by the data and that many of the
weakly supported pairs could be removed because:
1. the chance of them occurring again might be
low, and
2. their occurrence in the given corpus may be the
result of an artifact (a combination of effects
where several estimates artificially compensate
for one another). This concept is usually re-
ferred to as overfit since the model fits aspects
of the training data that do not lead to improved
prediction.
Phrase pairs that cannot stand on their own by
demonstrating a certain level of significance are sus-
pect and removing them from the phrasetable may
969
be beneficial in terms of reducing the size of data
structures. This will be shown to be the case in rather
general terms.
Note that this pruning may and quite often will
remove all of the candidate translations for a source
phrase. This might seem to be a bad idea but it must
be remembered that deleting longer phrases will al-
low combinations of shorter phrases to be used and
these might have more and better translations from
the corpus. Here is part of the intuition about how
phrasetable smoothing may interact with phrasetable
pruning: both are discouraging longer but infrequent
phrases from the corpus in favour of combinations of
more frequent, shorter phrases.
Because the probabilities involved below will be
so incredibly tiny, we will work instead with the neg-
ative of the natural logs of the probabilities. Thus
instead of selecting phrase pairs with a p-value less
than exp(?20), we will select phrase pairs with a
negative-log-p-value greater than 20. This has the
advantage of working with ordinary-sized numbers
and the happy convention that bigger means more
pruning.
2.4 C(s?, t?) = 1, 1-1-1 Tables and the ?
Threshold
An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and
each of the component phrases occurs exactly once
in its side of the parallel corpus.
These phrase pairs will be referred to as 1-1-1
phrase pairs and the corresponding tables will be
called 1-1-1 contingency tables because C(s?) = 1,
C(t?) = 1, and C(s?, t?) = 1.
Moore (2004) comments that the p-value for these
tables under Fisher?s exact test is 1/N . Since we are
using thresholds of the negative logarithm of the p-
value, the value ? = log(N) is a useful threshold to
consider.
In particular, ? +  (where  is an appropriately
small positive number) is the smallest threshold that
results in none of the 1-1-1 phrase pairs being in-
cluded. Similarly, ? ?  is the largest threshold that
results in all of the 1-1-1 phrase pairs being included.
Because 1-1-1 phrase pairs can make up a large part
of the phrase table, this is important observation for
its own sake.
Since the contingency table with C(s?, t?) = 1 hav-
ing the greatest significance (lowest p-value) is the
1-1-1 table, using the threshold of ?+  can be used
to exclude all of the phrase pairs occurring exactly
once (C(s?, t?) = 1).
The common strategy of deleting all of the 1-
count phrase pairs is very similar in effect to the use
of the ? +  threshold.
3 Experiments
3.1 WMT06
The corpora used for most of these experiments are
publicly available and have been used for a num-
ber of comparative studies (Workshop on Statisti-
cal Machine Translation, 2006). Provided as part of
the materials for the shared task are parallel corpora
for French?English, Spanish?English, and German?
English as well as language models for English,
French, Spanish, and German. These are all based
on the Europarl resources (Europarl, 2003).
The only change made to these corpora was to
convert them to lowercase and to Unicode UTF-8.
Phrasetables were produced by symmetrizing IBM2
conditional probabilities as described above.
The phrasetables were then used as a list of
n,m-grams for which counts C(s?, t?), C(s?), and
C(t?) were obtained. Negative-log-p-values under
Fisher?s exact test were computed for each of the
phrase pairs in the phrasetable and the entry was
censored if the negative-log-p-value for the test was
below the pruning threshold. The entries that are
kept are ones that are highly significant.
A number of combinations involving many differ-
ent pruning thresholds were considered: no pruning,
10, ??, ?+, 15, 20, 25, 50, 100, and 1000. In ad-
dition, a number of different phrasetable smoothing
algorithms were used: no smoothing, Good-Turing
smoothing, Kneser-Ney 3 parameter smoothing and
the loglinear mixture involving two features called
Zens-Ney (Foster et al, 2006).
3.2 Chinese
To test the effects of significance pruning on larger
corpora, a series of experiments was run on a much
larger corpus based on that distributed for MT06
Chinese?English (NIST MT, 2006). Since the ob-
jective was to assess how the method scaled we used
our preferred phrasetable smoothing technique of
970
1000100101
BLEU by Pruning Threshold
no smoothing
3
3
333 3
3
3
3
GT (+1)
+ +
+++ +
+
+
+
KN3 (+2)
2 2222 2
2
2
2
ZN (+3)
? ???? ?
?
?
?
107
106
105
1000100101
Phrasetable Size by Pruning Threshold
size3 3
333
3
3
3
3
107106105
BLEU by Phrasetable Size
no smoothing
3
3
3333
3
3
3
GT (+1)
++
++++
+
+
+
KN3 (+2)
222222
2
2
2
ZN (+3)
??????
?
?
?
Figure 1: WMT06: Results for French ?? English.
[to separate the curves, graphs for smoothed meth-
ods are shifted by +1, +2, or +3 BLEU points]
Table 2: Corpus Sizes and ? Values
number of
parallel sentences ?
WMT06: fr?? en 688,031 13.4415892
WMT06: es?? en 730,740 13.501813
WMT06: de?? en 751,088 13.5292781
Chinese?English: best 3,164,228 14.9674197
Chinese?English: UN-v2 4,979,345 15.4208089
Zens-Ney and separated our corpus into two phrase-
tables, one based on the UN corpus and the other
based on the best of the remaining parallel corpora
available to us.
Different pruning thresholds were considered: no
pruning, 14, 16, 18, 20, and 25. In addition, another
more aggressive method of pruning was attempted.
Moore points out, correctly, that phrase pairs that oc-
cur in only one sentence pair, (C(s?, t?) = 1 ), are less
reliable and might require more special treatment.
These are all pruned automatically at thresholds of
16 and above but not at threshold of 14. A spe-
cial series of runs was done for threshold 14 with all
of these singletons removed to see whether at these
thresholds it was the significance level or the prun-
ing of phrase pairs with (C(s?, t?) = 1 ) that was more
important. This is identified as 14? in the results.
4 Results
The results of the experiments are described in Ta-
bles 2 through 6.
Table 2 presents the sizes of the various parallel
corpora showing the number of parallel sentences,
N , for each of the experiments, together with the ?
thresholds (? = log(N)).
Table 3 shows the sizes of the phrasetables that
result from the various pruning thresholds described
for the WMT06 data. It is clear that this is extremely
aggressive pruning at the given levels.
Table 4 shows the corresponding phrasetable sizes
for the large corpus Chinese?English data. The
pruning is not as aggressive as for the WMT06 data
but still quite sizeable.
Tables 5 and 6 show the main results for the
WMT06 and the Chinese?English large corpus ex-
periments. To make these results more graphic, Fig-
ure 1 shows the French ?? English data from the
WMT06 results in the form of three graphs. Note
971
Table 3: WMT06: Distinct phrase pairs by pruning threshold
threshold fr?? en es?? en de?? en
none 9,314,165 100% 11,591,013 100% 6,954,243 100%
10 7,999,081 85.9% 10,212,019 88.1% 5,849,593 84.1%
??  6,014,294 64.6% 7,865,072 67.9% 4,357,620 62.7%
? +  1,435,576 15.4% 1,592,655 13.7% 1,163,296 16.7%
15 1,377,375 14.8% 1,533,610 13.2% 1,115,559 16.0%
20 1,152,780 12.4% 1,291,113 11.1% 928,855 13.4%
25 905,201 9.7% 1,000,264 8.6% 732,230 10.5%
50 446,757 4.8% 481,737 4.2% 365,118 5.3%
100 235,132 2.5% 251,999 2.2% 189,655 2.7%
1000 22,873 0.2% 24,070 0.2% 16,467 0.2%
Table 4: Chinese?English: Distinct phrase pairs by pruning threshold
threshold best UN-v2
none 18,858,589 100% 20,228,273 100%
14 7,666,063 40.7% 13,276,885 65.6%
16 4,280,845 22.7% 7,691,660 38.0%
18 4,084,167 21.7% 7,434,939 36.8%
20 3,887,397 20.6% 7,145,827 35.3%
25 3,403,674 18.0% 6,316,795 31.2%
also pruning C(s?, t?) = 1
14? 4,477,920 23.7% 7,917,062 39.1%
that an artificial separation of 1 BLEU point has
been introduced into these graphs to separate them.
Without this, they lie on top of each other and hide
the essential point. In compensation, the scale for
the BLEU co-ordinate has been removed.
These results are summarized in the following
subsections.
4.1 BLEU as a function of threshold
In tables 5 and 6, the largest BLEU score for each
set of runs has been marked in bold font. In addition,
to highlight that there are many near ties for largest
BLEU, all BLEU scores that are within 0.1 of the
best are also marked in bold.
When this is done it becomes clear that pruning
at a level of 20 for the WMT06 runs would not re-
duce BLEU in most cases and in many cases would
actually increase it. A pruning threshold of 20 cor-
responds to discarding roughly 90% of the phrase-
table.
For the Chinese?English large corpus runs, a level
of 16 seems to be about the best with a small in-
crease in BLEU and a 60% ? 70% reduction in the
size of the phrasetable.
4.2 BLEU as a function of depth of pruning
Another view of this can be taken from Tables 5
and 6. The fraction of the phrasetable retained is
a more or less simple function of pruning threshold
as shown in Tables 3 and 4. By including the per-
centages in Tables 5 and 6, we can see that BLEU
goes up as the fraction approaches between 20% and
30%.
This seems to be a relatively stable observation
across the experiments. It is also easily explained by
its strong relationship to pruning threshold.
4.3 Large corpora
Table 6 shows that this is not just a small corpus phe-
nomenon. There is a sizeable benefit both in phrase-
table reduction and a modest improvement to BLEU
even in this case.
4.4 Is this just the same as phrasetable
smoothing?
One question that occurred early on was whether this
improvement in BLEU is somehow related to the
improvement in BLEU that occurs with phrasetable
smoothing.
972
It appears that the answer is, in the main, yes, al-
though there is definitely something else going on.
It is true that the benefit in terms of BLEU is less-
ened for better types of phrasetable smoothing but
the benefit in terms of the reduction in bulk holds. It
is reassuring to see that no harm to BLEU is done by
removing even 80% of the phrasetable.
4.5 Comment about C(s?, t?) = 1
Another question that came up is the role of phrase
pairs that occur only once: C(s?, t?) = 1. In particu-
lar as discussed above, the most significant of these
are the 1-1-1 phrase pairs whose components also
only occur once: C(s?) = 1, and C(t?) = 1. These
phrase pairs are amazingly frequent in the phrase-
tables and are pruned in all of the experiments ex-
cept when pruning threshold is equal to 14.
The Chinese?English large corpus experiments
give us a good opportunity to show that significance
level seems to be more an issue than the case that
C(s?, t?) = 1.
Note that we could have kept the phrase pairs
whose marginal counts were greater than one but
most of these are of lower significance and likely
are pruned already by the threshold. The given con-
figuration was considered the most likely to yield a
benefit and its poor performance led to the whole
idea being put aside.
5 Conclusions and Continuing Work
To sum up, the main conclusions are five in number:
1. Phrasetables produced by the standard Diag-
Andmethod (Koehn et al, 2003) can be aggres-
sively pruned using significance pruning with-
out worsening BLEU.
2. If phrasetable smoothing is not done, the BLEU
score will improve under aggressive signifi-
cance pruning.
3. If phrasetable smoothing is done, the improve-
ment is small or negligible but there is still no
loss on aggressive pruning.
4. The preservation of BLEU score in the pres-
ence of large-scale pruning is a strong effect in
small and moderate size phrasetables, but oc-
curs also in much larger phrasetables.
5. In larger phrasetables based on larger corpora,
the percentage of the table that can be dis-
carded appears to decrease. This is plausible
since a similar effect (a decrease in the benefit
of smoothing) has been noted with phrasetable
smoothing (Foster et al, 2006). Together these
results suggest that, for these corpus sizes, the
increase in the number of strongly supported
phrase pairs is greater than the increase in the
number of poorly supported pairs, which agrees
with intuition.
Although there may be other approaches to prun-
ing that achieve a similar effect, the use of Fisher?s
exact test is mathematically and conceptually one of
the simplest since it asks a question separately for
each phrase pair: ?Considering this phase pair in
isolation of any other analysis on the corpus, could it
have occurred plausibly by purely random processes
inherent in the corpus construction?? If the answer
is ?Yes?, then it is hard to argue that the phrase pair
is an association of general applicability from the
evidence in this corpus alone.
Note that the removal of 1-count phrase pairs is
subsumed by significance pruning with a threshold
greater than ? and many of the other simple ap-
proaches (from an implementation point of view)
are more difficult to justify as simply as the above
significance test. Nonetheless, there remains work
to do in determining if computationally simpler ap-
proaches do as well. Moore?s work suggests that
log-likelihood-ratio would be a cheaper and accurate
enough alternative, for example.
We will now return to the interaction of the se-
lection in our beam search of the top 30 candidates
based on forward conditional probabilities. This will
affect our results but most likely in the following
manner:
1. For very small thresholds, the beam will be-
come much wider and the search will take
much longer. In order to allow the experiments
to complete in a reasonable time, other means
will need to be employed to reduce the choices.
This reduction will also interact with the sig-
nificance pruning but in a less understandable
manner.
2. For large thresholds, there will not be 30
973
choices and so there will be no effect.
3. For intermediate thresholds, the extra prun-
ing might reduce BLEU score but by a small
amount because most of the best choices are
included in the search.
Using thresholds that remove most of the phrase-
table would no doubt qualify as large thresholds so
the question is addressing the true shape of the curve
for smaller thresholds and not at the expected operat-
ing levels. Nonetheless, this is a subject for further
study, especially as we consider alternatives to our
?filter 30? approach for managing beam width.
There are a number of important ways that this
work can and will be continued. The code base for
taking a list of n,m-grams and computing the re-
quired frequencies for signifance evaluation can be
applied to related problems. For example, skip-n-
grams (n-grams that allow for gaps of fixed or vari-
able size) may be studied better using this approach
leading to insight about methods that weakly ap-
proximate patterns.
The original goal of this work was to better un-
derstand the character of phrasetables, and it re-
mains a useful diagnostic technique. It will hope-
fully lead to more understanding of what it takes
to make a good phrasetable especially for languages
that require morphological analysis or segmentation
to produce good tables using standard methods.
The negative-log-p-value promises to be a useful
feature and we are currently evaluating its merits.
6 Acknowledgement
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).?
References
Alan Agresti. 1996. An Introduction to Categorical Data
Analysis. Wiley.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?312,
June.
Philipp Koehn 2003. Europarl: A Mul-
tilingual Corpus for Evaluation of Ma-
chine Translation. Unpublished draft. see
http://www.iccs.inf.ed.ac.uk/?pkoehn
/publications/europarl.pdf
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Machine
Translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing (ICASSP) 1995, pages
181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Eduard
Hovy, editor, Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?133, Edmonton, Alberta, Canada, May.
NAACL.
Robert C. Moore. 2004. On Log-Likelihood-Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, Barcelona, Spain.
NIST. 2006. NIST MT Benchmark Test. see
http://www.nist.gov/speech/tests/mt/
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the Association for Computa-
tional Linguistics(ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Report
RC22176, IBM, September.
NAACL Workshop on Statistical Machine Translation.
2006. see http://www.statmt.org/wmt06/
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP) 2002, Denver, Colorado, September.
Richard Zens and Hermann Ney. 2004. Improvements in
phrase-based statistical machine translation. In Pro-
ceedings of Human Language Technology Conference
/ North American Chapter of the ACL, Boston, May.
974
Table 5: WMT06 Results: BLEU by type of smoothing and pruning threshold
threshold phrasetable % fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
relative frequency: no smoothing
none 100% 25.39 27.26 20.74 27.29 27.17 14.71
10 84?88% 25.97 27.81 21.08 27.82 27.71 15.09
??  63?68% 26.32 28.00 21.27 28.11 28.09 15.19
? +  14?17% 26.34 28.27 21.22 28.16 28.08 15.24
15 13?15% 26.36 28.50 21.14 28.20 28.18 15.29
20 11?13% 26.51 28.45 21.36 28.28 28.06 15.28
25 8?10% 26.50 28.38 21.28 28.32 27.97 15.25
50 4?5% 26.26 27.88 20.87 28.05 27.90 15.08
100 2% 25.66 27.07 20.07 27.38 27.11 14.66
1000 0.2% 20.49 21.66 15.23 22.51 22.31 11.36
Good-Turing
none 100% 25.96 28.14 21.17 27.84 27.95 15.13
10 84?88% 26.33 28.33 21.38 28.18 28.27 15.22
??  63?68% 26.54 28.63 21.50 28.36 28.39 15.31
? +  14?17% 26.24 28.49 21.15 28.22 28.16 15.28
15 13?15% 26.48 28.03 21.21 28.27 28.21 15.31
20 11?13% 26.65 28.45 21.41 28.36 28.14 15.25
25 8?10% 26.54 28.56 21.31 28.35 28.04 15.28
50 4?5% 26.26 27.78 20.94 28.07 27.95 15.08
100 2% 25.70 27.07 20.12 27.41 27.13 14.66
1000 0.2% 20.49 21.66 15.52 22.53 22.31 11.37
Kneser-Ney (3 parameter)
none 100% 26.89 28.70 21.78 28.64 28.71 15.50
10 84?88% 26.79 28.78 21.71 28.63 28.41 15.35
15 13?15% 26.49 28.69 21.34 28.60 28.57 15.52
20 11?13% 26.73 28.67 21.54 28.56 28.44 15.41
25 8?10% 26.84 28.70 21.29 28.54 28.21 15.42
50 4?5% 26.44 28.16 20.93 28.17 28.05 15.17
100 2% 25.72 27.27 20.11 27.50 27.26 14.58
1000 0.2% 20.48 21.70 15.28 22.58 22.36 11.33
Zens-Ney
none 100% 26.87 29.07 21.55 28.75 28.54 15.50
10 84?88% 26.81 29.00 21.65 28.72 28.52 15.54
15 13?15% 26.92 28.67 21.74 28.79 28.32 15.44
20 11?13% 26.93 28.47 21.72 28.69 28.42 15.45
25 8?10% 26.85 28.79 21.58 28.59 28.27 15.37
50 4?5% 26.51 27.96 20.96 28.30 27.96 15.27
100 2% 25.82 27.34 20.02 27.57 27.30 14.51
1000 0.2% 20.50 21.76 15.46 22.68 22.33 11.56
Table 6: Chinese Results: BLEU by pruning threshold
threshold phrasetable % nist04 nist05 nist06-GALE nist06-NIST
Zens-Ney Smoothing applied to all phrasetables
none 100% 32.14 30.69 13.06 27.97
14 40?65% 32.66 31.14 13.11 28.35
16 22?38% 32.73 30.97 13.14 28.00
18 21?36% 31.56 30.45 12.49 27.03
20 20?35% 32.00 30.73 12.50 27.33
25 18?31% 30.54 29.58 11.68 26.12
also pruning C(s?, t?) = 1
14? 23?39% 32.08 30.99 12.75 27.66
975
Unsupervised Learning of Morphology for English and Inuktitut
Howard Johnson
Institute for Information Technology,
National Research Council
Howard.Johnson@nrc.gc.ca
Joel Martin
Institute for Information Technology
National Research Council
Joel.Martin@nrc.gc.ca
Abstract
We describe a simple unsupervised technique
for learning morphology by identifying hubs
in an automaton.  For our purposes, a hub is a
node in a graph with in-degree greater than
one and out-degree greater than one.   We cre-
ate a word-trie, transform it into a minimal
DFA, then identify hubs.  Those hubs mark
the boundary between root and suffix,
achieving similar performance to more com-
plex mixtures of techniques.
1 Introduction
To recognize a morpheme boundary, for example be-
tween a root and a suffix, a learner must have seen at
least two roots with that suffix and at least two suffixes
with that root.  For instance, 'helpful', 'helpless', 'harm-
ful?, and 'harmless' would be enough evidence to guess
that those words could be divided as 'help/ful',
'help/less', 'harm/ful', and 'harm/less'.  Without seeing
varying roots and varying suffixes, there is no reason to
prefer one division to another.
We can represent a language's morphology as a
graph or automaton, with the links labeled by characters
and the nodes organizing which characters can occur
after specific prefixes.  In such an automaton, the mor-
pheme boundaries would be hubs, that is, nodes with in-
degree greater than one and out-degree greater than one.
Furthermore, this automaton could be simplified by path
compression to remove all nodes with in-degree and
out-degree of one.  The remaining automaton could be
further modified to produce a graph with one source,
one sink, and all other nodes would be hubs.
A hub-automaton, as described above, matches the
intuitive idea that a language's morphology allows one
to assemble a word by chaining morphemes together.
This representation highlights the morphemes while also
representing morphotactic information.  Phonological
information can be represented in the same graph but
may be more economically represented in a separate
transducer that can be composed with the hub-
automaton.
For identifying the boundary between roots and suf-
fixes, the idea of hubs is essentially the same as Gold-
smith?s (2001) signatures or the variations between
Gaussier?s (1999) p-similarity words.  A signature is a
set of suffixes, any of which can be added to several
roots to create a word.  For example, in English any
suffix in the set: NULL, ?s?, ?ed?, ?ing?, can be added to
?want? or ?wander? to form a word.  Here, NULL means
the empty suffix.
In a hub automaton, the idea is more general than in
previous work and applies to more complex morpholo-
gies, such as those for agglutinative or polysynthetic
languages.  In particular, we are interested in unsuper-
vised learning of Inuktitut morphology in which a single
lexical unit can often include a verb, two pronouns, ad-
verbs, and temporal information.
In this paper, we describe a very simple technique
for identifying hubs as a first step in building a hub-
automaton.  We show that, for English, this technique
does as well as more complex collections of techniques
using signatures.  We then show that the technique also
works, in a limited way, for Inuktitut.  We close with a
discussion of the limitations and our plans for more
complete learning of hub-automata.
2 Searching for hubs
The simplest way to build a graph from a raw corpus of
words is to construct a trie.  A trie is a tree representa-
tion of the distinct words with a character label on each
branch.  The trie can be transformed into a minimal,
acyclic DFA (deterministic finite automaton), sharing
nodes that have identical continuations.  There are well
known algorithms for doing this (Hopcroft & Ullman,
1969).  For example, suppose that, in a given corpus, the
prefix ?friend? occurs only with the suffixes ?NULL?,
?s?, and ?ly? and the word ?kind? occurs only with the
same suffixes.  The minimal DFA has merged the nodes
that represent those suffixes, and as a result has fewer
links and fewer nodes than the original trie.
In this DFA, some hubs will be obvious, such as for
the previous example.  These are morpheme boundaries.
There will be other nodes that are not obvious hubs.
Some may have high out-degree but an in-degree of
one; others will have high in-degree but an out-degree
of one.
Many researchers, including Schone and Jurafsky
(2000), Harris (1958), and D?jean (1998), suggest
looking for nodes with high branching (out-degree) or a
large number of continuations.  That technique is also
used as the first step in Goldsmith?s (2001) search for
signatures.  However, without further processing, such
nodes are not reliable morpheme boundaries.
Other candidate hubs are those nodes with high out-
degree that are direct descendants, along a single path,
of a node with high in-degree.  In essence, these are
stretched hubs.  Figure 1 shows an idealized view of a
hub and a stretched hub.
Figure 1: An idealized view of a hub and a
stretched hub.  The lines are links in the automaton
and each would be labeled with a character.  The
ovals are nodes and are only branching points.
In a minimized DFA of the words in a corpus, we
can identify hubs and the last node in stretched hubs as
morpheme boundaries.  These roughly correspond to the
signatures found by other methods.
The above-mentioned technique for hub searching
misses boundaries if a particular signature only appears
once in a corpus.  For instance, the signature for ?help?
might be ?ed?, ?s?, ?less?, ?lessly?, and NULL; and sup-
pose there is no other word in the corpus with the same
signature.  The morpheme boundaries ?help-less? and
?help-ed? will not be found.
The way to generalize the hub-automaton to include
words that were never seen is to merge hubs.  This is a
complex task in general.  In this paper, we propose a
very simple method.  We suggest merging each node
that is a final state (at the end of a word) with each hub
or stretched hub that has in-degree greater than two.
Doing so sharply increases the number of words ac-
cepted by the automaton.  It will identify more correct
morpheme boundaries at the expense of including some
non-words.
These two techniques, hub searching and simple
node merging, were implemented in a program called
?HubMorph? (hub-automaton morphology).
3 Related Work
Most previous work in unsupervised learning of mor-
phology has focused on learning the division between
roots and suffixes (e.g., Sproat, 1992; Gaussier, 1999;
D?jean, 1996; Goldsmith, 2001).  The hope is that the
same techniques will work for extracting prefixes.
However, even that will not handle the complex combi-
nations of infixes that are possible in agglutinative lan-
guages like Turkish or polysynthetic languages like
Inuktitut.
This paper presents a generalization of one class of
techniques that search for signatures or positions in a
trie with a large branching factor.   Goldsmith (2001)
presents a well-developed and robust version of this
class and has made his system, Linguistica, freely avail-
able (Goldsmith, 2002).
Linguistica applies a wide array of techniques in-
cluding heuristics and the application of the principle of
Minimum Description Length (MDL) to find the best
division of words into roots and suffixes, as well as pre-
fixes in some cases.  The first of these techniques finds
the points in a word with the highest number of possible
successors in other words. With all these techniques,
Linguistica seeks optimal breakpoints in each word.  In
this case, optimal means the minimal number of bits
necessary to encode the whole collection.
There are also techniques that attempt to use seman-
tic cues, arguing that knowing the signatures is not suf-
ficient for the task.  For example, Yarowsky and
Wicentowski (2000; cf. Schone & Jurafsky, 2000) pre-
sent a method for determining whether singed can be
split into sing and ed based on whether singed and sing
appear in the same contexts.  Adopting a technique like
this would increase the precision of HubMorph.  In ad-
dition, some semantic approach is absolutely essential
for identifying fusional morphology, where the word
(sang) is not a simple composition of a root (sing) and
morphemes.
4 Evaluation
As noted above, Linguistica uses many techniques to
learn morphology, including a fairly complex system for
counting bits.  We tested whether the two techniques
presented in this paper, hub searching and simple node
merging, achieve the same performance as Linguistica.
If so, the simpler techniques might be preferred. Also,
we would be justified using them for more complex
morphologies.
The input to Linguistica and HubMorph was the text
of Tom Sawyer.  The performance of both was com-
pared against a gold standard division of the distinct
words in that novel.  The gold standard was based on
dictionary entries and the judgment of two English
speakers.
In matching the gold standard words to divisions
predicted by either system, we made the following as-
sumptions. a) Words with hyphens are split at the hy-
phen to match Linguistica?s assumption. b) If the gold
standard has a break before and after a single character,
to capture non-concatenative modification, either break
matches.  An example would be ?mud-d-y?. c) An apos-
trophe at a morpheme boundary is ignored for compari-
son matching to allow it to stick to the root or to the
suffix. d) The suffix split proposed must result in a suf-
fix of 5 or fewer characters, again to match Linguis-
tica?s assumption.
Table 1 show the results of this comparison for Lin-
guistica, hub-searching alone, and HubMorph (both hub
searching and node merging).  Hub-searching alone is
sufficient to achieve the same precision as Linguistica
and nearly the same recall.  Both of the techniques to-
gether are sufficient to achieve the same precision and
recall as Linguistica.  The recall for all is low because
the list of words in Tom Sawyer is not long enough to
include most acceptable combinations of roots and suf-
fixes.  A longer input word list would improve this
score.
System Recall Precision
Linguistica 0.5753 0.9059
Hub-Searching 0.4451 0.9189
HubMorph 0.5904 0.9215
Table 1: The recall and precision of Linguistica,
Hub-searching alone, and HubMorph.  Recall is the
proportion of distinct words from Tom Sawyer that
are correctly divided into root and suffix.  Precision
is the proportion of predicted divisions that are cor-
rect.
5 Discussion
HubMorph achieves the same performance as Linguis-
tica on the words in Tom Sawyer.  It does so with a
general technique based on building a hub-automaton.
In addition to being simple, HubMorph can be general-
ized to deal with more complex morphologies.
We have applied HubMorph to Inuktitut for dividing
such words as ikajuqtaulauqsimajunga (?I was helped in
the recent past?, ikajuq-tau-lauq-sima-junga).  The path
in a hub automaton for most Inuktitut words would have
many hubs, because the words have many divisions.
Currently, there are many limitations.  The search
for hubs in the middle of words is very difficult and
requires merging nodes to induce new words.  This will
be necessary because Inuktitut theoretically has billions
of words and only a small fraction of them has occurred
in our source (the Nunavut, Canada Hansards).
Also, because each word has many morphemes, it is
difficult to correctly detect the divisions for roots and
suffixes.  In general, there are no prefixes in Inuktitut,
only infixes and suffixes.
Finally, there are many dialects of Inuktitut and
many spelling variations.  In general, the written lan-
guage is phonetic and the spelling reflects all the varia-
tions in speech.
When HubMorph performs unsupervised learning of
Inuktitut roots, it achieves a precision of 31.8% and a
recall of 8.1%.    It will be necessary to learn more of
the infixes and suffixes to improve these scores.
We believe that hub-automata will be the basis of a
general solution for IndoEuropean languages as well as
for Inuktitut.
References
D?jean, H. 1998. Morphemes as necessary concepts for
structures: Discovery from untagged corpora. Uni-
v e r s i t y  o f  Caen-Basse Normandie.
http://citeseer.nj.nec.com/19299.html
Gaussier E. (1999). Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In:
Kehler A and Stolcke A, eds, ACL workshop on Un-
supervised Methods in Natural Language Learning,
College Park, MD.
Goldsmith, J.A. (2001). Unsupervised Learning of the
Morphology of a Natural Language.  Computational
Linguistics, 27:2 pp. 153-198.
Goldsmith, J.A. (2002). Linguistica software.
http://humanities.uchicago.edu/faculty/goldsmith/Lin
guistica2000/.
Harris, Z. (1951). Structural Linguistics. University of
Chicago Press.
Hopcroft, J.E. & Ullman, J.D. (1969). Formal Lan-
guages and their Relation to Automata. Addison-
Wesley, Reading, MA.
Schone, P., & Jurafsky, D. (2000). Knowledge-free in-
duction of morphology using latent semantic analy-
sis. In Proceedings of CoNLL-2000 and LLL-2000,
pp. 67--72 Lisbon, Portugal.
Sproat, R. (1992). Morphology and Computation, Cam-
bridge, MA, MIT Press.
Yarowsky, D. & Wicentowski, R. (2000). Minimally
supervised morphological analysis by multimodal
alignment. In K. Vijay-Shanker and Chang-Ning
Huang, editors, Proceedings of the 38th Meeting of
the Association for Computational Linguistics, pages
207-216, Hong Kong.
Aligning and Using an English-Inuktitut Parallel Corpus
Joel Martin, Howard Johnson, Benoit Farley and Anna Maclachlan
Institute for Information Technology
National Research Council Canada
firstname.lastname@nrc-cnrc.gc.ca
Abstract
A parallel corpus of texts in English and
in Inuktitut, an Inuit language, is presented.
These texts are from the Nunavut Hansards.
The parallel texts are processed in two phases,
the sentence alignment phase and the word cor-
respondence phase. Our sentence alignment
technique achieves a precision of 91.4% and
a recall of 92.3%. Our word correspondence
technique is aimed at providing the broadest
coverage collection of reliable pairs of Inuktitut
and English morphemes for dictionary expan-
sion. For an agglutinative language like Inuk-
titut, this entails considering substrings, not
simply whole words. We employ a Pointwise
Mutual Information method (PMI) and attain a
coverage of 72.3% of English words and a pre-
cision of 87%.
1 Introduction
We present an aligned parallel corpus of Inuktitut and
English from the Nunavut Hansards. The alignment at
the sentence level and the word correspondence follow
techniques described in the literature with augmentations
suggested by the specific properties of this language pair.
The lack of lexical resources for Inuktitut, the unrelated-
ness of the two languages, the fact that the languages use
a different script and the richness of the morphology in
Inuktitut have guided our choice of technique. Sentences
have been aligned using the length-based dynamic pro-
gramming approach of Gale and Church (1993) enhanced
with a small number of lexical and non-alphabetic an-
chors. Word correspondences have been identified with
the goal of finding an extensive high quality candidate
glossary for English and Inuktitut words. Crucially, the
algorithm considers not only full word correspondences,
as most approaches do, but also multiple substring corre-
spondences resulting in far greater coverage.
2 An English-Inuktitut Corpus
2.1 The Parallel Texts
The corpus of parallel texts we present consists of
3,432,212 words of English and 1,586,423 words of Inuk-
titut from the Nunavut Hansards. These Hansards are
available to the public in electronic form in both English
and Inuktitut (www.assembly.nu.ca). The Legislative As-
sembly of the newly created territory of Nunavut began
sitting on April 1, 1999. Our corpus represents 155 days
of transcribed proceedings of the Nunavut Legislative As-
sembly from that first session through to November 1,
2002, which was part way through the sixth session of
the assembly.
We gather and process these 155 documents in vari-
ous ways described in the rest of this paper and make
available a sentence-aligned version of the parallel texts
(www.InuktitutComputing.ca/NunavutHansards). Like
the French-English Canadian Hansards of parliamentary
proceedings, this corpus represents a valuable resource
for Machine Translation research and corpus research as
well as for the development of language processing tools
for Inuktitut. The work reported here takes some first
steps toward these ends, and it is hoped that others will
find ways to expand on this work. One reason that the
Canadian Hansards, a large parallel corpus of English-
French, are particularly useful for research is that they
are comparatively noise free as parallel text collections
go (Simard and Plamondon, 1996). This should be true
of the Nunavut Hansard collection as well. The Canadian
Hansard is transcribed in both languages so what was said
in English is transcribed in English and then translated
into French and vice versa. For the Nunavut Hansard, in
contrast, a complete English version of the proceedings
is prepared and then this is translated into Inuktitut, even
when the original proceedings were spoken in Inuktitut.
2.2 The Inuktitut Language
Inuktitut is the language of the Inuit living in North East-
ern Canada, that is, Nunavut (Keewatin and Baffin Is-
land), Nunavik and Labrador. It includes six closely
related spoken dialects: Kivalliq, Aivilik, North Baffin,
South Baffin, Arctic Quebec (Nunavik), and Labrador.
Inuktitut is a highly agglutinative language. Noun and
verb roots occur with two main types of suffixes and there
are many instantiations of these suffixes. The seman-
tic suffixes modify the meaning of the root (over 250 of
these in North Baffin dialect) and the grammatical suf-
fixes indicate features like agreement and mood (approx-
imately 700 verbal endings and over 300 nominal endings
in North Baffin dialect).
A single word in Inuktitut is often translated with
multiple English words, sometimes corresponding to a
full English clause. For example, the Inuktitut word
r???'???rk?'n
n
c
?k???rk?rk (which is transliterated as
qaisaaliniaqquunngikkaluaqpuq) corresponds to these
eight English words: ?Actually he will probably not come
early today?. The verbal root is qai ?come?, the semantic
suffixes are -saali-, -niaq-, -qquu-, -nngit- and -galuaq-
meaning ?soon?, ?a little later today or tomorrow?, ?proba-
bility?, ?negation?, and ?actuality? respectively, and finally
the grammatical suffix -puq expresses the 3rd person sin-
gular of the indicative mood. This frequently occurring
one-to-many correspondence represents a challenge for
word correspondence. The opposite challenging situa-
tion, namely instances of many-to-one correspondences,
also arises for this language pair but less frequently. The
latter is therefore not addressed in this paper.
Yet another challenge is the morphophonological com-
plexity of Inuktitut as reflected in the orthography, which
has two components. First, the sheer number of pos-
sible suffixes mentioned above is problematic. Second,
the shape of these suffixes is variable. That is, there are
significant orthographic changes to the individual mor-
phemes when they occur in the context of a word. This
type of variability can be seen in the above example at the
interface of -nngit- and -galuaq-, which together become
-nngikkaluaq-.
Finally, it is important to note that Inuktitut has a syl-
labic script for which there is a standard Romanization.
To give an idea of how the scripts compare, our corpus
of parallel texts consists of 20,124,587 characters of En-
glish and 13,457,581 characters in Inuktitut syllabics as
compared to 21,305,295 characters of Inuktitut in Roman
script.
3 Sentence Alignment
3.1 Sentence Alignment Approach
The algorithm used to align English-Inuktitut sentences is
an extension of that presented in Gale and Church (1993).
It does not identify crossing alignments where the sen-
tence order within paragraphs in the parallel texts differs.
Sentence alignments typically involve one English sen-
tence matching one Inuktitut sentence (a 1-to-1 bead),
but may also involve 2-to-1, 1-to-2, 0-to-1, 1-to-0 and
2-to-2 sentence matching patterns, or beads. Using such
a length-based approach where the length of sentences
is measured in characters is appropriate for our language
pair since the basic assumption generally holds. Namely,
longer English sentences typically correspond to longer
Inuktitut sentences as measured in characters.
One problem with the approach, as pointed out by
Macklovitch and Hannan (1998), is that from the point
where a paragraph is misaligned, it is difficult to ensure
proper alignment for the remainder of the paragraph. We
observed this effect in our alignment. We also observed
that the large number of small paragraphs with almost
identical length caused problems for the algorithm.
Many alignment approaches have addressed such prob-
lems by making use of additional linguistic clues specific
to the languages to be aligned. For our language pair,
it was not feasible to use most of these. For example,
some alignment techniques make good use of cognates
(Simard and Plamondon, 1996). The assumption is that
words in the two languages that share the first few let-
ters are usually translations of each other. English and
Inuktitut, however, are too distantly related to have many
cognates. Even the translation of a proper name does not
usually result in a cognate for our language pair, since the
translation between scripts induces a phonetic translation
rather than a character-preserving translation of the name,
as these pairs illustrate Peter, Piita; Canada, Kanata;
McLean, Makalain.
Following a suggestion in Gale and Church (1993),
the alignment was aided by the use of additional an-
chors that were available for the language pair. These
anchors consisted of non-alphabetic sequences (such as
9:00, 42-1(1) and 1999) and 8 reliable word cor-
respondences that occurred frequently in the corpus, in-
cluding words beginning with these character sequences
speaker/uqaqti and motion/pigiqati, for ex-
ample.
3.2 Steps in Sentence Alignment
Preprocessing: Preprocessing the Inuktitut and the En-
glish raised separate issues. For English, the main is-
sue was ensuring that illegal or unusual characters are
mapped to other characters to simplify later processing.
For Inuktitut the main issue was the array of encodings
used for the syllabic script. Inuktitut syllabics can be rep-
resented using a 7-bit encoding called ProSyl, which is
in many cases extended to an 8-bit encoding Tunngavik.
Each syllabic character can be encoded in multiple ways
that need to be mapped into a uniform scheme, such as
Unicode. Each separate file was converted to HTML us-
ing a commercial product LogicTran r2net. Then, the
Perl package HTML::TreeBuilder was used to purge the
text of anomalies and set up the correct mappings. The
output of this initial preprocessing step was a collection
of HTML files in pure Unicode UTF8.
Boundary Identification: The next step was to iden-
tify the paragraph and sentence boundaries for the Inuk-
titut and English texts. Sentences were split at periods,
question marks, colons and semi-colons except where the
following character was a lower case letter or a number.
This resulted in a number of errors but was quite accurate
in general. Paragraph boundaries were inserted where
such logical breaks occurred as signaled in the HTML
and generally correspond to natural breaks in the orig-
inal document. Using HTML indicators contributed to
the number of very short paragraphs, especially toward
the beginning of each document. As mentioned in sec-
tion 3.1, these short paragraphs were problematic for the
alignment algorithm. The collection consists of 348,619
sentences in 112,346 paragraphs in English and 352,486
sentences in 118,733 paragraphs in Inuktitut. After this
step, document, paragraph and sentence boundaries were
available to use as hard and soft boundaries for the Gale
and Church algorithm.
Syllabic Script Conversion: The word correspon-
dence phase required a Roman script representation of
the Inuktitut texts. The conversion from unicode syllab-
ics to Roman characters was performed at this stage in the
sentence alignment process using the standard ICI con-
version method.
Anchors: The occurrences of the lexical anchors men-
tioned above were found and used with a dynamic pro-
gramming search to find the path with the largest number
of alignments. This algorithm was written in Perl and re-
quired about two hours to process the whole corpus. All
alignments that occurred in the first two sentences of each
paragraph were marked as hard boundaries for the Gale
and Church (1993) program as provided in their paper.
3.3 Sentence Alignment Evaluation
Three representative days of Hansard (1999/04/01,
2001/02/21 and 2002/10/29) were selected and manually
aligned at the sentence level as a gold standard. Precision
and recall were then measured as suggested in Isabelle
and Simard (1996).
Results: The number of sentence alignments in the
gold standard was 3424. The number automatically
aligned by our method was 3459. The number of
those automatic alignments that were correct as measured
against the gold standard was 3161. This represents a pre-
cision of 91.4% and a recall rate of 92.3%. For compari-
son, the Gale and Church (1993) program, which did not
make use of additional anchors, had poorer results over
our corpus. Their one-pass approach, which ignores para-
graph boundaries, had a precision of 66.7% and a recall
of 71.5%. Their two-pass approach, which aligns para-
graphs in one pass and then aligns sentences in a second
pass, had a precision of 85.6% and a recall of 87.0%.
4 Word Correspondence
Having built a sentence-aligned parallel corpus, we next
attempted to use that corpus. Our goal was to extract
as many reliable word associations as possible to aid in
developing a morphological analyzer and in expanding
Inuktitut dictionaries. The output of this glossary discov-
ery phase is a list of suggested pairings that a human can
consider for inclusion in a dictionary. Inuktitut dictio-
naries often disagree because of spelling and dialectical
differences. As well, many contemporary words are not
in the existing dictionaries. The parallel corpus presented
here can be used to augment the dictionaries with current
words, thereby providing an important tool for students,
translators, and others.
In our approach, a glossary is populated with pairs of
words that are consistent translations of each other. For
many language pairs, considering whole word to whole
word correspondences for inclusion in a glossary would
yield good results. However, because Inuktitut is aggluti-
native, the method must discover pairs of an English word
and the corresponding root of the Inuktitut word, or the
corresponding Inuktitut suffix, or sometimes the whole
Inuktitut word. In other words, it is essential to consider
substrings of words for good coverage for a language pair
like ours.
4.1 Substring Correspondence Method
Searching for substring correspondences is reduced to a
counting exercise. For any pair of substrings, you need to
know how many parallel regions contained the pair, how
many regions in one language contained the first, how
many regions in the other language contained the second,
and how many regions there are in total. For example,
the English word ?today? and the Inuktitut word ?ullumi?
occur in 2092 parallel regions. The word ?today? appears
in a total of 3065 English regions; and ?ullumi? appears
in 2702 Inuktitut regions. All together, there are 332,154
aligned regions. It is fairly certain that these two words
should be a glossary pair because each usually occurs as
a translation of the other.
The PMI Measure: We measure the degree of asso-
ciation between any two substrings, one in the English
and one in the Inuktitut, using Pointwise Mutual Infor-
mation (PMI). PMI measures the amount of information
that each substring conveys about the occurrence of the
other. We recognize that PMI is badly behaved when the
counts are near 1. To protect against that problem, we
compute the 99.99999% confidence intervals around the
PMI (Lin, 1999), and use the lower bound as a measure
of association. This lower bound rises as the PMI rises
or as the amount of data increases. Many measures of
association would likely work as well as the lower confi-
dence bound on PMI. We used that bound as a metric in
this study for three reasons. First, that metric led to bet-
ter performance than Chi-squared on this data. Second, it
addressed the problem of low frequency events. Third, it
makes the correct judgment on Gale and Church?s well-
known chambre-communes problem (Gale and Church,
1991).
The decision to include pairs of substrings in the glos-
sary proceeds as follows. Include the highest PMI scoring
pairs if neither member of the pair has yet been included.
If two pairs are tied, check whether the Inuktitut members
of the pairs are in a substring relation. If they are, then
add the pair with the longer substring to the glossary; if
not, then add neither pair.
Many previous efforts have used a similar methodol-
ogy but were only able to focus on word to word cor-
respondences (Gale and Church, 1991). Here, the En-
glish words can correspond to any substring in any Inuk-
titut word in the aligned region. This means that statis-
tics have to be maintained for many possible pairs. Un-
der our approach, we maintain all these statistics for all
English words, all Inuktitut words as well as substrings
with length of between one and 10 Roman characters, and
all co-occurrences that have frequency greater than three.
This approach thereby addresses the challenge of Inuk-
titut roots and multiple semantic suffixes corresponding
to individual English words. It also addresses the chal-
lenge of orthographic variation at morpheme boundaries
to some degree since it will truncate morphemes appro-
priately in many cases.
4.2 Glossary Evaluation
This method suggested 4362 word-substring pairs for in-
clusion in a glossary. This represents a 72.3% coverage of
English word occurrences in the corpus (omitting words
of fewer than 3 characters). One hundred of these word-
substring pairs were chosen at random and judged for ac-
curacy using two existing dictionaries and a partial suffix
list. An Inuktitut substring was said to match an English
word exactly if the Inuktitut root plus all the suffixes car-
ried the same meaning as the English word and conveyed
the same grammatical features (e.g., grammatical number
and case). The correspondence was said to be good if the
Inuktitut root plus the left-most lexical suffixes conveyed
the same meaning as the English word. In those cases, the
Inuktitut word conveyed additional semantic or grammat-
ical information.
About half of the exact matches were uninflected
proper nouns. A typical example of the other exact
matches is the pair inuup and person?s. In this pair, inu-
means person and -up is the singular genitive case. A typ-
ical example of a good match is the pair pigiaqtitara and
deal. In this pair, pigiaqti- means deal and -tara conveys
first person singular subject and third person singular ob-
ject. For example, ?I deal with him?.
Of the 100 pairs, 43 were deemed exact matches and
44 were deemed good matches. The remaining 13 were
incorrect. Taken together 87% of the pairs in the sample
were useful to include in a glossary. This level of perfor-
mance will improve as we introduce morphological anal-
ysis to both the Inuktitut and English words.
5 Conclusion
We have shown that aligning an English text with a highly
agglutinative language text can have very useful out-
comes. The alignment of the corpus to the sentence level
was achieved accurately enough to build a usable parallel
corpus. This is demonstrated by the fact that we could
create a glossary tool on the basis of this corpus that
suggested glossary pairings for 72.3% of English words
in the text with a precision of 87%. We hope that our
work will generate further interest in this newly available
English-Inuktitut parallel corpus.
Acknowledgements We would like to thank Gavin
Nesbitt of the Legislative Assembly of Nunavut for pro-
viding the Hansards, Peter Turney for useful sugges-
tions, and Canadian Heritage for financial support of this
project.
References
William A. Gale and Kenneth Ward Church. 1991. Iden-
tifying word correspondance in parallel text. In Pro-
ceedings of the DARPA NLP Workshop.
William A. Gale and Kenneth Ward Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?103.
Pierre Isabelle and Michel Simard. 1996. Propo-
sitions pour la repre?sentation et l?e?valuation des
alignements de textes paralle`les. [http://www.lpl.univ-
aix.fr/projects/arcade/2nd/sent/metrics.html]. In Rap-
port technique, CITI.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the ACL.
Elliot Macklovitch and Marie-Louise Hannan. 1998.
Line ?em up: Advances in alignment technology and
their impact on translation support tools. Machine
Translation, 13(1).
Michel Simard and Pierre Plamondon. 1996. Bilingual
sentence alignment: Balancing robustness and accu-
racy. In Proceedings of the Conference of the Associa-
tion for Machine Translation in the Americas (AMTA).
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 129?132,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
PORTAGE: A Phrase-based Machine Translation System 
 
Fatiha Sadat+, Howard Johnson++, Akakpo Agbago+, George Foster+,               
Roland Kuhn+, Joel Martin++ and Aaron Tikuisis?
 
+ NRC Institute for Information 
Technology 
101 St-Jean-Bosco Street  
Gatineau, QC K1A 0R6, Canada 
++ NRC Institute for Information 
Technology 
1200 Montreal Road  
Ottawa, ON K1A 0R6, Canada 
?University of Waterloo 
200 University Avenue W., 
Waterloo, Ontario, Canada 
 
firstname.lastname@cnrc-nrc.gc.ca aptikuis@uwaterloo.ca  
 
Abstract 
This paper describes the participation of 
the Portage team at NRC Canada in the 
shared task1 of ACL 2005 Workshop on 
Building and Using Parallel Texts. We dis-
cuss Portage, a statistical phrase-based 
machine translation system, and present 
experimental results on the four language 
pairs of the shared task. First, we focus on 
the French-English task using multiple re-
sources and techniques. Then we describe 
our contribution on the Finnish-English, 
Spanish-English and German-English lan-
guage pairs using the provided data for the 
shared task.  
1 Introduction 
The rapid growth of the Internet has led to a rapid 
growth in the need for information exchange among 
different languages. Machine Translation (MT) and 
related technologies have become essential to the 
information flow between speakers of different lan-
guages on the Internet. Statistical Machine Transla-
tion (SMT), a data-driven approach to producing 
translation systems, is becoming a practical solution 
to the longstanding goal of cheap natural language 
processing.  
In this paper, we describe Portage, a statistical 
phrase-based machine translation system, which we 
evaluated on all different language pairs that were 
provided for the shared task.  As Portage is a very 
                                                           
1 http://www.statmt.org/wpt05/mt-shared-task/ 
new system, our main goal in participating in the 
workshop was to test it out on different language 
pairs, and to establish baseline performance for the 
purpose of comparison against other systems and 
against future improvements.  To do this, we used a 
fairly standard configuration for phrase-based SMT, 
described in the next section. 
Of the language pairs in the shared task, French-
English is particularly interesting to us in light of 
Canada?s demographics and policy of official bilin-
gualism. We therefore divided our participation into 
two parts: one stream for French-English and an-
other for Finnish-, German-, and Spanish-English. 
For the French-English stream, we tested the use of 
additional data resources along with hand-coded 
rules for translating numbers and dates. For the 
other streams, we used only the provided resources 
in a purely statistical framework (although we also 
investigated several automatic methods of coping 
with Finnish morphology). 
The remainder of the paper is organized as fol-
lows. Section 2 describes the architecture of the 
Portage system, including its hand-coded rules for 
French-English.  Experimental results for the four 
pairs of languages are reported in Section 3. Section 
4 concludes and gives pointers to future work. 
2 Portage  
Portage operates in three main phases: preprocess-
ing of raw data into tokens, with translation sugges-
tions for some words or phrases generated by rules; 
decoding to produce one or more translation hy-
potheses; and error-driven rescoring to choose the 
best final hypothesis. (A fourth postprocessing 
phase was not needed for the shared task.) 
129
2.1 Preprocessing 
Preprocessing is a necessary first step in order to 
convert raw texts in both source and target lan-
guages into a format suitable for both model train-
ing and decoding (Foster et al, 2003).  For the 
supplied Europarl corpora, we relied on the existing 
segmentation and tokenization, except for French, 
which we manipulated slightly to bring into line 
with our existing conventions (e.g., converting l ? 
an  into l? an).  For the Hansard corpus used to 
supplement our French-English resources (de-
scribed in section 3 below), we used our own 
alignment based on Moore?s algorithm (Moore, 
2002), segmentation, and tokenization procedures. 
Languages with rich morphology are often prob-
lematic for statistical machine translation because 
the available data lacks instances of all possible 
forms of a word to efficiently train a translation sys-
tem. In a language like German, new words can be 
formed by compounding (writing two or more 
words together without a space or a hyphen in be-
tween). Segmentation is a crucial step in preproc-
essing languages such as German and Finnish texts.
In addition to these simple operations, we also 
developed a rule-based component to detect num-
bers and dates in the source text and identify their 
translation in the target text. This component was 
developed on the Hansard corpus, and applied to the 
French-English texts (i.e. Europarl and Hansard), on 
the development data in both languages, and on the 
test data. 
2.2 Decoding 
Decoding is the central phase in SMT, involving a 
search for the hypotheses t that have highest prob-
abilities of being translations of the current source 
sentence s according to a model for P(t|s). Our 
model for P(t|s) is a log-linear combination of four 
main components: one or more trigram language 
models, one or more phrase translation models, a 
distortion model, and a word-length feature. The 
trigram language model is implemented in the 
SRILM toolkit (Stolcke, 2002). The phrase-based 
translation model is similar to the one described in 
(Koehn, 2004), and relies on symmetrized IBM 
model 2 word-alignments for phrase pair induction. 
The distortion model is also very similar to 
Koehn?s, with the exception of a final cost to ac-
count for sentence endings.  
s
To set weights on the components of the log-
linear model, we implemented Och?s algorithm 
(Och, 2003).  This essentially involves generating, 
in an iterative process, a set of nbest translation hy-
potheses that are representative of the entire search 
space for a given set of source sentences. Once this 
is accomplished, a variant of Powell?s algorithm is 
used to find weights that optimize BLEU score 
(Papineni et al 2002) over these hypotheses, com-
pared to reference translations. Unfortunately, our 
implementation of this algorithm converged only 
very slowly to a satisfactory final nbest list, so we 
used two different ad hoc strategies for setting 
weights: choosing the best values encountered dur-
ing
, with the exception of a 
ch as the ability to decode either 
w ards.  
 transla-
 
rent language pairs of the 
sha d t
hared t
- 
 the iterations of Och?s algorithm (French-
English), and a grid search (all other languages).  
To perform the actual translation, we used our 
decoder, Canoe, which implements a dynamic-
programming beam search algorithm based on that 
of Pharaoh (Koehn, 2004). Canoe is input-output 
compatible with Pharaoh
few extensions su
back ards or forw
2.3 Rescoring 
To improve raw output from Canoe, we used a 
rescoring strategy: have Canoe generate a list of 
nbest translations rather than just one, then reorder 
the list using a model trained with Och?s method to 
optimize BLEU score. This is identical to the final 
pass of the algorithm described in the previous sec-
tion, except for the use of a more powerful log-
linear model than would have been feasible to use 
inside the decoder. In addition to the four basic fea-
tures of the initial model, our rescoring model in-
cluded IBM2 model probabilities in both directions 
(i.e., P(s|t) and P(t|s)); and an IBM1-based feature 
designed to detect whether any words in one lan-
guage seemed to be left without satisfactory
tions in the other language. This missing-word
feature was also applied in both directions. 
3 Experiments on the Shared Task 
We conducted experiments and evaluations on 
Portage using the diffe
re ask. The training data was provided for the 
ask as follows:  
Training data of 688,031 sentences in 
French and English. A similarly sized cor-
130
pus is provided for Finnish, Spanish and 
German with matched English translations. 
orpus was used to generate both 
lan
e translations into English, was 
 
 Portage for a comparative study ex-
ploiting and combining different resources and 
tec
 
3. arl corpus 
4. 
rd corpora as training data and 
 
t  mod est  
p icipation at th h-English tas 9.53. 
od D  Decoding+Rescoring
- Development test data of 2,000 sentences in 
the four languages.  
In addition to the provided data, a set of 
6,056,014 sentences extracted from Hansard corpus, 
the official record of Canada?s parliamentary de-
bates, was used in both French and English lan-
guages. This c
guage and translation models for use in decoding 
and rescoring. 
The development test data was split into two 
parts: The first part that includes 1,000 sentences in 
each language with reference translations into Eng-
lish served in the optimization of weights for both 
the decoding and rescoring models. In this study, 
number of n-best lists was set to 1,000. The second 
part, which includes 1,000 sentences in each lan-
guage with referenc
used in the evaluation of the performance of the
translation models. 
3.1 Experiments on the French-English Task 
Our goal for this language pair was to conduct ex-
periments on
hniques:  
1. Method E is based on the Europarl corpus 
as training data, 
2. Method E-H is based on both Europarl and 
Hansard corpora as training data, 
Method E-p is based on the Europ
as training data and parsing numbers and 
dates in the preprocessing phase, 
Method E-H-p is based on both Europarl 
and Hansa
parsing numbers and date in the preprocess-
ing phase. 
Results are shown in Table 1 for the French-
English task. The first column of Table 1 indicates 
the method, the second column gives results for 
decoding with Canoe only, and the third column for 
decoding and rescoring with Canoe. For comparison 
between the four methods, there was an improve-
ment in terms of BLEU scores when using two lan-
guage models and two translation models generated 
from Europarl and Hansard corpora; however, pars-
ing numbers and dates had a negative impact on the
ranslation els. The b  BLEU score for our
art e Frenc k was 2
Meth ecoding
E 27.71 29.22 
E-H 28.71 29.53 
E-p 26.45 28.21 
E-H-p 28.29 28.56 
Ta
ed 
f 
of increased trade within North 
merica but also functions as a good counterpoint 
for French-English. 
 
ble 1. BLEU scores for the French-English test 
sentences  
 
A noteworthy feature of these results is that the 
improvement given by the out-of-domain Hansard 
corpus was very slight. Although we suspect that 
somewhat better performance could have been 
achieved by better weight optimization, this result 
clearly underscores the importance of matching 
training and test domains. A related point is that our 
number and date translation rules actually caused a 
performance drop due to the fact that they were op-
timized for typographical conventions prevalent in 
Hansard, which are quite different from those used 
in Europarl. 
Our best result ranked third in the shared 
WPT05 French-English task , with a difference of 
0.74 in terms of BLEU score from the first rank
participant, and a difference of 0.67 in terms o
BLEU score from the second ranked participant. 
3.2 Experiments on other Pairs of Languages 
The WPT05 workshop provides a good opportunity 
to achieve our benchmarking goals with corpora 
that provide challenging difficulties. German and 
Finnish are languages that make considerable use of 
compounding. Finnish, in addition, has a particu-
larly complex morphology that is organized on 
principles that are quite different from any in Eng-
lish. This results in much longer word forms each of 
which occurs very infrequently. 
Our original intent was to propose a number of pos-
sible statistical approaches to analyzing and split-
ting these word forms and improving our results. 
Since none of these yielded results as good as the 
baseline, we will continue this work until we under-
stand what is really needed. We also care very 
much about translating between French and English 
in Canada and plan to spend a lot of extra effort on 
difficulties that occur in this case. Translation be-
tween Spanish and English is also becoming more 
mportant as a result i
A
131
Language Pair Decoding+Rescoring
Finnish-English 20.95 
German-English 23.21 
Spanish English 29.08 
Ta
and 1.56 in 
m ores, respectively, compared to 
the first ranked participant.   
l 
ation, greater use of morphological 
R
Fr
Meeting of the Association for Computational 
Fr
Statistical Machine Transla-
Ge
id 
Ke
e 
Ki
al Meeting of the Association for Com-
M
ne Trans-
Oc
 of the 40th Annual Meet-
Fr
 Proceedings of 
Ph parl: A multilingual corpusfor 
 P
ation 
Models. In Proceedings of the Association for Ma-
chine Translation in the Americas AMTA 2004. 
ble 2 BLEU scores for the Finnish-English, Ger-
man-English and Spanish-English test sentences  
 
To establish our baseline, the only preprocessing 
we did was lowercasing (using the provided tokeni-
zation). Canoe was run without any special settings, 
although weights for distortion, word penalty, lan-
guage model, and translation model were optimized 
using a grid search, as described above. Rescoring 
was also done, and usually resulted in at least an 
extra BLEU point.  
Our final results are shown in Table 2. Ranks at 
the shared WPT05 Finnish-, German-, and Spanish-
English tasks were assigned as second, third and 
fourth, with differences of 1.06, 1.87 
ter s of BLEU sc
4 Conclusion 
We have reported on our participation in the shared 
task of the ACL 2005 Workshop on Building and 
Using Parallel Texts, conducting evaluations of 
Portage, our statistical machine translation system, 
on all four language pairs. Our best BLEU scores 
for the French-, Finnish-, German-, and Spanish-
English at this stage were 29.5, 20.95, 23.21 and 
29.08, respectively. In total, eleven teams took part 
at the shared task and most of them submitted re-
sults for all pairs of languages.  Our results distin-
guished the NRC team at the third, second, third 
and fourth ranks with slight differences with the 
first ranked participants. 
A major goal of this work was to evaluate Port-
age at its first stage of implementation on different 
pairs of languages. This evaluation has served to 
identify some problems with our system in the areas 
of weight optimization and number and date rules. 
It has also indicated the limits of using out-of-
domain corpora, and the difficulty of morphologi-
cally complex languages like Finnish. 
Current and planned future work includes the 
exploitation of comparable corpora for statistica
machine transl
knowledge, and better features for nbest rescoring. 
eferences 
Andreas Stolcke. 2002. SRILM - an Extensible Language 
Modeling Toolkit. In ICSLP-2002, 901-904. 
anz Josef Och, Hermann Ney. 2000. Improved Statisti-
cal Alignment Models. In Proceedings of the 38th An-
nual 
Linguistics, Hong Kong, China, October 2000, 440-
447. 
anz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, Dragomir Radev. 2004. A Smor-
gasbord of Features for 
tion. In Proceeding of the HLT/NAACL 2004, 
Boston, MA, May 2004. 
orge Foster, Simona Gandrabur, Philippe Langlais, 
Pierre Plamondon, Graham Russell and Michel Si-
mard. 2003. Statistical Machine Translation: Rap
Development with Limited Resources. In Proceedings 
of MT Summit IX 2003, New Orleans, September.  
vin Knight, Ishwar Chander, Matthew Haines, Va-
sileios Hatzivassiloglou, Eduard Hovy, Masayo Iida, 
Steve K. Luk, Richard Whitney, and Kenji Yamada. 
1995. Filling Knowledge Gaps in a Broad-Coverag
MT System. In Proceedings of the International Joint 
Conference on Artificial Intelligence (IJCAI), 1995. 
shore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
the 40th Annu
putational Linguistics ACL, Philadelphia, July 2002, 
pp. 311-318. 
oore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Machine Transla-
tion: From Research to Real Users (Proceedings of the 
5th Conference of the Association for Machi
lation in the Americas, Tiburon, California), Springer-
Verlag, Heidelberg, Germany, pp. 135-244. 
h, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Machine 
Translation. In Proceedings
ing of the Association for Computational Linguistics, 
Philadelphia, pp. 295?302. 
anz Josef Och, 2003. Minimum Error Rate Training 
for Statistical Machine Translation. In
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, July. 
ilipp Koehn. 2002. Euro
evaluation of machine translation. Ms., University of 
Southern California. 
hilipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrase-based Statistical Machine Transl
132
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Phrasetable Smoothing for Statistical Machine Translation
George Foster and Roland Kuhn and Howard Johnson
National Research Council Canada
Ottawa, Ontario, Canada
firstname.lastname@nrc.gc.ca
Abstract
We discuss different strategies for smooth-
ing the phrasetable in Statistical MT, and
give results over a range of translation set-
tings. We show that any type of smooth-
ing is a better idea than the relative-
frequency estimates that are often used.
The best smoothing techniques yield con-
sistent gains of approximately 1% (abso-
lute) according to the BLEU metric.
1 Introduction
Smoothing is an important technique in statistical
NLP, used to deal with perennial data sparseness
and empirical distributions that overfit the training
corpus. Surprisingly, however, it is rarely men-
tioned in statistical Machine Translation. In par-
ticular, state-of-the-art phrase-based SMT relies
on a phrasetable?a large set of ngram pairs over
the source and target languages, along with their
translation probabilities. This table, which may
contain tens of millions of entries, and phrases of
up to ten words or more, is an excellent candidate
for smoothing. Yet very few publications describe
phrasetable smoothing techniques in detail.
In this paper, we provide the first system-
atic study of smoothing methods for phrase-based
SMT. Although we introduce a few new ideas,
most methods described here were devised by oth-
ers; the main purpose of this paper is not to in-
vent new methods, but to compare methods. In
experiments over many language pairs, we show
that smoothing yields small but consistent gains in
translation performance. We feel that this paper
only scratches the surface: many other combina-
tions of phrasetable smoothing techniques remain
to be tested.
We define a phrasetable as a set of source
phrases (ngrams) s? and their translations t?, along
with associated translation probabilities p(s?|t?) and
p(t?|s?). These conditional distributions are derived
from the joint frequencies c(s?, t?) of source/target
phrase pairs observed in a word-aligned parallel
corpus.
Traditionally, maximum-likelihood estimation
from relative frequencies is used to obtain con-
ditional probabilities (Koehn et al, 2003), eg,
p(s?|t?) = c(s?, t?)/?s? c(s?, t?) (since the estimation
problems for p(s?|t?) and p(t?|s?) are symmetrical,
we will usually refer only to p(s?|t?) for brevity).
The most obvious example of the overfitting this
causes can be seen in phrase pairs whose con-
stituent phrases occur only once in the corpus.
These are assigned conditional probabilities of 1,
higher than the estimated probabilities of pairs for
which much more evidence exists, in the typical
case where the latter have constituents that co-
occur occasionally with other phrases. During de-
coding, overlapping phrase pairs are in direct com-
petition, so estimation biases such as this one in
favour of infrequent pairs have the potential to sig-
nificantly degrade translation quality.
An excellent discussion of smoothing tech-
niques developed for ngram language models
(LMs) may be found in (Chen and Goodman,
1998; Goodman, 2001). Phrasetable smoothing
differs from ngram LM smoothing in the follow-
ing ways:
? Probabilities of individual unseen events are
not important. Because the decoder only
proposes phrase translations that are in the
phrasetable (ie, that have non-zero count), it
never requires estimates for pairs s?, t? having
53
c(s?, t?) = 0.1 However, probability mass is
reserved for the set of unseen translations,
implying that probability mass is subtracted
from the seen translations.
? There is no obvious lower-order distribution
for backoff. One of the most important tech-
niques in ngram LM smoothing is to com-
bine estimates made using the previous n? 1
words with those using only the previous n?i
words, for i = 2 . . . n. This relies on the
fact that closer words are more informative,
which has no direct analog in phrasetable
smoothing.
? The predicted objects are word sequences
(in another language). This contrasts to LM
smoothing where they are single words, and
are thus less amenable to decomposition for
smoothing purposes.
We propose various ways of dealing with these
special features of the phrasetable smoothing
problem, and give evaluations of their perfor-
mance within a phrase-based SMT system.
The paper is structured as follows: section 2
gives a brief description of our phrase-based SMT
system; section 3 presents the smoothing tech-
niques used; section 4 reviews previous work; sec-
tion 5 gives experimental results; and section 6
concludes and discusses future work.
2 Phrase-based Statistical MT
Given a source sentence s, our phrase-based SMT
system tries to find the target sentence t? that is
the most likely translation of s. To make search
more efficient, we use the Viterbi approximation
and seek the most likely combination of t and its
alignment a with s, rather than just the most likely
t:
t? = argmax
t
p(t|s) ? argmax
t,a
p(t,a|s),
where a = (s?1, t?1, j1), ..., (s?K , t?K , jK); t?k are tar-
get phrases such that t = t?1 . . . t?K ; s?k are source
phrases such that s = s?j1 . . . s?jK ; and s?k is the
translation of the kth target phrase t?k.
1This is a first approximation; exceptions occur when dif-
ferent phrasetables are used in parallel, and when rules are
used to translate certain classes of entities.
To model p(t,a|s), we use a standard loglinear
approach:
p(t,a|s) ? exp
[
?
i
?ifi(s, t,a)
]
where each fi(s, t,a) is a feature function, and
weights ?i are set using Och?s algorithm (Och,
2003) to maximize the system?s BLEU score (Pa-
pineni et al, 2001) on a development corpus. The
features used in this study are: the length of t;
a single-parameter distortion penalty on phrase
reordering in a, as described in (Koehn et al,
2003); phrase translation model probabilities; and
trigram language model probabilities log p(t), us-
ing Kneser-Ney smoothing as implemented in the
SRILM toolkit (Stolcke, 2002).
Phrase translation model probabilities are fea-
tures of the form:
log p(s|t,a) ?
K
?
k=1
log p(s?k|t?k)
ie, we assume that the phrases s?k specified by a
are conditionally independent, and depend only on
their aligned phrases t?k. The ?forward? phrase
probabilities p(t?|s?) are not used as features, but
only as a filter on the set of possible translations:
for each source phrase s? that matches some ngram
in s, only the 30 top-ranked translations t? accord-
ing to p(t?|s?) are retained.
To derive the joint counts c(s?, t?) from which
p(s?|t?) and p(t?|s?) are estimated, we use the phrase
induction algorithm described in (Koehn et al,
2003), with symmetrized word alignments gener-
ated using IBM model 2 (Brown et al, 1993).
3 Smoothing Techniques
Smoothing involves some recipe for modifying
conditional distributions away from pure relative-
frequency estimates made from joint counts, in or-
der to compensate for data sparsity. In the spirit of
((Hastie et al, 2001), figure 2.11, pg. 38) smooth-
ing can be seen as a way of combining the relative-
frequency estimate, which is a model with high
complexity, high variance, and low bias, with an-
other model with lower complexity, lower vari-
ance, and high bias, in the hope of obtaining bet-
ter performance on new data. There are two main
ingredients in all such recipes: some probability
distribution that is smoother than relative frequen-
cies (ie, that has fewer parameters and is thus less
54
complex) and some technique for combining that
distribution with relative frequency estimates. We
will now discuss both these choices: the distribu-
tion for carrying out smoothing and the combina-
tion technique. In this discussion, we use p?() to
denote relative frequency distributions.
Choice of Smoothing Distribution
One can distinguish between two approaches to
smoothing phrase tables. Black-box techniques do
not look inside phrases but instead treat them as
atomic objects: that is, both the s? and the t? in the
expression p(s?|t?) are treated as units about which
nothing is known except their counts. In contrast,
glass-box methods break phrases down into their
component words.
The black-box approach, which is the sim-
pler of the two, has received little attention in
the SMT literature. An interesting aspect of
this approach is that it allows one to implement
phrasetable smoothing techniques that are analo-
gous to LM smoothing techniques, by treating the
problem of estimating p(s?|t?) as if it were the prob-
lem of estimating a bigram conditional probabil-
ity. In this paper, we give experimental results
for phrasetable smoothing techniques analogous
to Good-Turing, Fixed-Discount, Kneser-Ney, and
Modified Kneser-Ney LM smoothing.
Glass-box methods for phrasetable smoothing
have been described by other authors: see sec-
tion 3.3. These authors decompose p(s?|t?) into a
set of lexical distributions p(s|t?) by making inde-
pendence assumptions about the words s in s?. The
other possibility, which is similar in spirit to ngram
LM lower-order estimates, is to combine estimates
made by replacing words in t? with wildcards, as
proposed in section 3.4.
Choice of Combination Technique
Although we explored a variety of black-box and
glass-box smoothing distributions, we only tried
two combination techniques: linear interpolation,
which we used for black-box smoothing, and log-
linear interpolation, which we used for glass-box
smoothing.
For black-box smoothing, we could have used a
backoff scheme or an interpolation scheme. Back-
off schemes have the form:
p(s?|t?) =
{
ph(s?|t?), c(s?, t?) ? ?
pb(s?|t?), else
where ph(s?|t?) is a higher-order distribution,
pb(s?|t?) is a smooth backoff distribution, and ? is
a threshold above which counts are considered re-
liable. Typically, ? = 1 and ph(s?|t?) is version of
p?(s?|t?) modified to reserve some probability mass
for unseen events.
Interpolation schemes have the general form:
p(s?|t?) = ?(s?, t?)p?(s?|t?) + ?(s?, t?)pb(s?|t?), (1)
where ? and ? are combining coefficients. As
noted in (Chen and Goodman, 1998), a key
difference between interpolation and backoff is
that the former approach uses information from
the smoothing distribution to modify p?(s?|t?) for
higher-frequency events, whereas the latter uses
it only for low-frequency events (most often 0-
frequency events). Since for phrasetable smooth-
ing, better prediction of unseen (zero-count)
events has no direct impact?only seen events are
represented in the phrasetable, and thus hypoth-
esized during decoding?interpolation seemed a
more suitable approach.
For combining relative-frequency estimates
with glass-box smoothing distributions, we em-
ployed loglinear interpolation. This is the tradi-
tional approach for glass-box smoothing (Koehn
et al, 2003; Zens and Ney, 2004). To illustrate the
difference between linear and loglinear interpola-
tion, consider combining two Bernoulli distribu-
tions p1(x) and p2(x) using each method:
plinear(x) = ?p1(x) + (1? ?)p2(x)
ploglin(x) =
p1(x)?p2(x)
p1(x)?p2(x) + q1(x)?q2(x)
where qi(x) = 1 ? pi(x). Setting p2(x) = 0.5
to simulate uniform smoothing gives ploglin(x) =
p1(x)?/(p1(x)? + q1(x)?). This is actually less
smooth than the original distribution p1(x): it pre-
serves extreme values 0 and 1, and makes inter-
mediate values more extreme. On the other hand,
plinear(x) = ?p1(x) + (1 ? ?)/2, which has the
opposite properties: it moderates extreme values
and tends to preserve intermediate values.
An advantage of loglinear interpolation is that
we can tune loglinear weights so as to maximize
the true objective function, for instance BLEU; re-
call that our translation model is itself loglinear,
with weights set to minimize errors. In fact, a lim-
itation of the experiments described in this paper
is that the loglinear weights for the glass-box tech-
niques were optimized for BLEU using Och?s al-
gorithm (Och, 2003), while the linear weights for
55
black-box techniques were set heuristically. Ob-
viously, this gives the glass-box techniques an ad-
vantage when the different smoothing techniques
are compared using BLEU! Implementing an al-
gorithm for optimizing linear weights according to
BLEU is high on our list of priorities.
The preceding discussion implicitly assumes a
single set of counts c(s?, t?) from which conditional
distributions are derived. But, as phrases of differ-
ent lengths are likely to have different statistical
properties, it might be worthwhile to break down
the global phrasetable into separate phrasetables
for each value of |t?| for the purposes of smooth-
ing. Any similar strategy that does not split up
{s?|c(s?, t?) > 0} for any fixed t? can be applied to
any smoothing scheme. This is another idea we
are eager to try soon.
We now describe the individual smoothing
schemes we have implemented. Four of them
are black-box techniques: Good-Turing and three
fixed-discount techniques (fixed-discount inter-
polated with unigram distribution, Kneser-Ney
fixed-discount, and modified Kneser-Ney fixed-
discount). Two of them are glass-box techniques:
Zens-Ney ?noisy-or? and Koehn-Och-Marcu IBM
smoothing. Our experiments tested not only these
individual schemes, but also some loglinear com-
binations of a black-box technique with a glass-
box technique.
3.1 Good-Turing
Good-Turing smoothing is a well-known tech-
nique (Church and Gale, 1991) in which observed
counts c are modified according to the formula:
cg = (c + 1)nc+1/nc (2)
where cg is a modified count value used to replace
c in subsequent relative-frequency estimates, and
nc is the number of events having count c. An
intuitive motivation for this formula is that it ap-
proximates relative-frequency estimates made by
successively leaving out each event in the corpus,
and then averaging the results (Na?das, 1985).
A practical difficulty in implementing Good-
Turing smoothing is that the nc are noisy for large
c. For instance, there may be only one phrase
pair that occurs exactly c = 347, 623 times in a
large corpus, and no pair that occurs c = 347, 624
times, leading to cg(347, 623) = 0, clearly not
what is intended. Our solution to this problem
is based on the technique described in (Church
and Gale, 1991). We first take the log of the ob-
served (c, nc) values, and then use a linear least
squares fit to log nc as a function of log c. To en-
sure that the result stays close to the reliable values
of nc for large c, error terms are weighted by c, ie:
c(log nc? log n?c)2, where n?c are the fitted values.
Our implementation pools all counts c(s?, t?) to-
gether to obtain n?c (we have not yet tried separate
counts based on length of t? as discussed above). It
follows directly from (2) that the total count mass
assigned to unseen phrase pairs is cg(0)n0 = n1,
which we approximate by n?1. This mass is dis-
tributed among contexts t? in proportion to c(t?),
giving final estimates:
p(s?|t?) = cg(s?, t?)?
s cg(s?, t?) + p(t?)n?1
,
where p(t?) = c(t?)/?t? c(t?).
3.2 Fixed-Discount Methods
Fixed-discount methods subtract a fixed discount
D from all non-zero counts, and distribute the re-
sulting probability mass according to a smoothing
distribution (Kneser and Ney, 1995). We use an
interpolated version of fixed-discount proposed by
(Chen and Goodman, 1998) rather than the origi-
nal backoff version. For phrase pairs with non-
zero counts, this distribution has the general form:
p(s?|t?) = c(s?, t?)?D?
s? c(s?, t?)
+ ?(t?)pb(s?|t?), (3)
where pb(s?|t?) is the smoothing distribution. Nor-
malization constraints fix the value of ?(t?):
?(t?) = D n1+(?, t?)/
?
s?
c(s?, t?),
where n1+(?, t?) is the number of phrases s? for
which c(s?, t?) > 0.
We experimented with two choices for the
smoothing distribution pb(s?|t?). The first is a plain
unigram p(s?), and the second is the Kneser-Ney
lower-order distribution:
pb(s?) = n1+(s?, ?)/
?
s?
n1+(s?, ?),
ie, the proportion of unique target phrases that s? is
associated with, where n1+(s?, ?) is defined anal-
ogously to n1+(?, t?). Intuitively, the idea is that
source phrases that co-occur with many different
56
target phrases are more likely to appear in new
contexts.
For both unigram and Kneser-Ney smoothing
distributions, we used a discounting coefficient de-
rived by (Ney et al, 1994) on the basis of a leave-
one-out analysis: D = n1/(n1 + 2n2). For the
Kneser-Ney smoothing distribution, we also tested
the ?Modified Kneser-Ney? extension suggested
in (Chen and Goodman, 1998), in which specific
coefficients Dc are used for small count values
c up to a maximum of three (ie D3 is used for
c ? 3). For c = 2 and c = 3, we used formu-
las given in that paper.
3.3 Lexical Decomposition
The two glass-box techniques that we considered
involve decomposing source phrases with inde-
pendence assumptions. The simplest approach as-
sumes that all source words are conditionally in-
dependent, so that:
p(s?|t?) =
J?
?
j=1
p(sj|t?)
We implemented two variants for p(sj|t?) that
are described in previous work. (Zens and Ney,
2004) describe a ?noisy-or? combination:
p(sj |t?) = 1? p(s?j |t?)
? 1?
I?
?
i=1
(1? p(sj |ti))
where s?j is the probability that sj is not in the
translation of t?, and p(sj|ti) is a lexical proba-
bility. (Zens and Ney, 2004) obtain p(sj|ti) from
smoothed relative-frequency estimates in a word-
aligned corpus. Our implementation simply uses
IBM1 probabilities, which obviate further smooth-
ing.
The noisy-or combination stipulates that sj
should not appear in s? if it is not the translation
of any of the words in t?. The complement of this,
proposed in (Koehn et al, 2005), to say that sj
should appear in s? if it is the translation of at least
one of the words in t?:
p(sj|t?) =
?
i?Aj
p(sj |ti)/|Aj |
where Aj is a set of likely alignment connections
for sj . In our implementation of this method,
we assumed that Aj = {1, . . . , I?}, ie the set of
all connections, and used IBM1 probabilities for
p(s|t).
3.4 Lower-Order Combinations
We mentioned earlier that LM ngrams have a
naturally-ordered sequence of smoothing distribu-
tions, obtained by successively dropping the last
word in the context. For phrasetable smoothing,
because no word in t? is a priori less informative
than any others, there is no exact parallel to this
technique. However, it is clear that estimates made
by replacing particular target (conditioning) words
with wildcards will be smoother than the original
relative frequencies. A simple scheme for combin-
ing them is just to average:
p(s?|t?) =
?
i=I?
c?i (s?, t?)
?
s? c?i (s?, t?)
/I?
where:
c?i (s?, t?) =
?
ti
c(s?, t1 . . . ti . . . tI?).
One might also consider progressively replacing
the least informative remaining word in the target
phrase (using tf-idf or a similar measure).
The same idea could be applied in reverse, by
replacing particular source (conditioned) words
with wildcards. We have not yet implemented
this new glass-box smoothing technique, but it has
considerable appeal. The idea is similar in spirit to
Collins? backoff method for prepositional phrase
attachment (Collins and Brooks, 1995).
4 Related Work
As mentioned previously, (Chen and Goodman,
1998) give a comprehensive survey and evalua-
tion of smoothing techniques for language mod-
eling. As also mentioned previously, there is
relatively little published work on smoothing for
statistical MT. For the IBM models, alignment
probabilities need to be smoothed for combina-
tions of sentence lengths and positions not encoun-
tered in training data (Garc??a-Varea et al, 1998).
Moore (2004) has found that smoothing to cor-
rect overestimated IBM1 lexical probabilities for
rare words can improve word-alignment perfor-
mance. Langlais (2005) reports negative results
for synonym-based smoothing of IBM2 lexical
probabilities prior to extracting phrases for phrase-
based SMT.
For phrase-based SMT, the use of smoothing to
avoid zero probabilities during phrase induction is
reported in (Marcu and Wong, 2002), but no de-
tails are given. As described above, (Zens and
57
Ney, 2004) and (Koehn et al, 2005) use two dif-
ferent variants of glass-box smoothing (which they
call ?lexical smoothing?) over the phrasetable, and
combine the resulting estimates with pure relative-
frequency ones in a loglinear model. Finally, (Cet-
tollo et al, 2005) describes the use of Witten-Bell
smoothing (a black-box technique) for phrasetable
counts, but does not give a comparison to other
methods. As Witten-Bell is reported by (Chen and
Goodman, 1998) to be significantly worse than
Kneser-Ney smoothing, we have not yet tested this
method.
5 Experiments
We carried out experiments in two different set-
tings: broad-coverage ones across six European
language pairs using selected smoothing tech-
niques and relatively small training corpora; and
Chinese to English experiments using all im-
plemented smoothing techniques and large train-
ing corpora. For the black-box techniques,
the smoothed phrase table replaced the original
relative-frequency (RF) phrase table. For the
glass-box techniques, a phrase table (either the
original RF phrase table or its replacement after
black-box smoothing) was interpolated in loglin-
ear fashion with the smoothing glass-box distribu-
tion, with weights set to maximize BLEU on a de-
velopment corpus.
To estimate the significance of the results across
different methods, we used 1000-fold pairwise
bootstrap resampling at the 95% confidence level.
5.1 Broad-Coverage Experiments
In order to measure the benefit of phrasetable
smoothing for relatively small corpora, we used
the data made available for the WMT06 shared
task (WMT, 2006). This exercise is conducted
openly with access to all needed resources and
is thus ideal for benchmarking statistical phrase-
based translation systems on a number of language
pairs.
The WMT06 corpus is based on sentences ex-
tracted from the proceedings of the European Par-
liament. Separate sentence-aligned parallel cor-
pora of about 700,000 sentences (about 150MB)
are provided for the three language pairs hav-
ing one of French, Spanish and German with En-
glish. SRILM language models based on the same
source are also provided for each of the four lan-
guages. We used the provided 2000-sentence dev-
sets for tuning loglinear parameters, and tested on
the 3064-sentence test sets.
Results are shown in table 1 for relative-
frequency (RF), Good-Turing (GT), Kneser-Ney
with 1 (KN1) and 3 (KN3) discount coefficients;
and loglinear combinations of both RF and KN3
phrasetables with Zens-Ney-IBM1 (ZN-IBM1)
smoothed phrasetables (these combinations are
denoted RF+ZN-IBM1 and KN3+ZN-IBM1).
It is apparent from table 1 that any kind of
phrase table smoothing is better than using none;
the minimum improvement is 0.45 BLEU, and
the difference between RF and all other meth-
ods is statistically significant. Also, Kneser-
Ney smoothing gives a statistically significant im-
provement over GT smoothing, with a minimum
gain of 0.30 BLEU. Using more discounting co-
efficients does not appear to help. Smoothing
relative frequencies with an additional Zens-Ney
phrasetable gives about the same gain as Kneser-
Ney smoothing on its own. However, combining
Kneser-Ney with Zens-Ney gives a clear gain over
any other method (statistically significant for all
language pairs except en?es and en?de) demon-
strating that these approaches are complementary.
5.2 Chinese-English Experiments
To test the effects of smoothing with larger
corpora, we ran a set of experiments for
Chinese-English translation using the corpora
distributed for the NIST MT05 evaluation
(www.nist.gov/speech/tests/mt). These are sum-
marized in table 2. Due to the large size of
the out-of-domain UN corpus, we trained one
phrasetable on it, and another on all other parallel
corpora (smoothing was applied to both). We also
used a subset of the English Gigaword corpus to
augment the LM training material.
corpus use sentences
non-UN phrasetable1 + LM 3,164,180
UN phrasetable2 + LM 4,979,345
Gigaword LM 11,681,852
multi-p3 dev 993
eval-04 test 1788
Table 2: Chinese-English Corpora
Table 3 contains results for the Chinese-English
experiments, including fixed-discount with uni-
gram smoothing (FDU), and Koehn-Och-Marcu
smoothing with the IBM1 model (KOM-IBM1)
58
smoothing method fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
RF 25.35 27.25 20.46 27.20 27.18 14.60
GT 25.95 28.07 21.06 27.85 27.96 15.05
KN1 26.83 28.66 21.36 28.62 28.71 15.42
KN3 26.84 28.69 21.53 28.64 28.70 15.40
RF+ZN-IBM1 26.84 28.63 21.32 28.84 28.45 15.44
KN3+ZN-IBM1 27.25 29.30 21.77 29.00 28.86 15.49
Table 1: Broad-coverage results
as described in section 3.3. As with the
broad-coverage experiments, all of the black-box
smoothing techniques do significantly better than
the RF baseline. However, GT appears to work
better in the large-corpus setting: it is statistically
indistinguishable from KN3, and both these meth-
ods are significantly better than all other fixed-
discount variants, among which there is little dif-
ference.
Not surprisingly, the two glass-box methods,
ZN-IBM1 and KOM-IBM1, do poorly when used
on their own. However, in combination with an-
other phrasetable, they yield the best results, ob-
tained by RF+ZN-IBM1 and GT+KOM-IBM1,
which are statistically indistinguishable. In con-
strast to the situation in the broad-coverage set-
ting, these are not significantly better than the
best black-box method (GT) on its own, although
RF+ZN-IBM1 is better than all other glass-box
combinations.
smoothing method BLEU score
RF 29.85
GT 30.66
FDU 30.23
KN1 30.29
KN2 30.13
KN3 30.54
ZN-IBM1 29.55
KOM-IBM1 28.09
RF+ZN-IBM1 30.95
RF+KOM-IBM1 30.10
GT+ZN-IBM1 30.45
GT+KOM-IBM1 30.81
KN3+ZN-IBM1 30.66
Table 3: Chinese-English Results
A striking difference between the broad-
coverage setting and the Chinese-English setting
is that in the former it appears to be beneficial
to apply KN3 smoothing to the phrasetable that
gets combined with the best glass-box phrasetable
(ZN), whereas in the latter setting it does not. To
test whether this was due to corpus size (as the
broad-coverage corpora are around 10% of those
for Chinese-English), we calculated Chinese-
English learning curves for the RF+ZN-IBM1 and
KN3-ZN-IBM1 methods, shown in figure 1. The
results are somewhat inconclusive: although the
KN3+ZN-IBM1 curve is perhaps slightly flatter,
the most obvious characteristic is that this method
appears to be highly sensitive to the particular cor-
pus sample used.
 0.25
 0.255
 0.26
 0.265
 0.27
 0.275
 0.28
 0.285
 0.29
 0.295
 0.3
 0  10  20  30  40  50  60  70  80
B
L
E
U
proportion of corpus
Learning curves for smoothing methods
RF+ZN-IBM1
KN3+ZN-IBM1
Figure 1: Learning curves for two glass-box com-
binations.
6 Conclusion and Future Work
We tested different phrasetable smoothing tech-
niques in two different translation settings: Eu-
ropean language pairs with relatively small cor-
pora, and Chinese to English translation with large
corpora. The smoothing techniques fall into two
59
categories: black-box methods that work only on
phrase-pair counts; and glass-box methods that de-
compose phrase probabilities into lexical proba-
bilities. In our implementation, black-box tech-
niques use linear interpolation to combine relative
frequency estimates with smoothing distributions,
while glass-box techniques are combined in log-
linear fashion with either relative-frequencies or
black-box estimates.
All smoothing techniques tested gave statisti-
cally significant gains over pure relative-frequency
estimates. In the small-corpus setting, the best
technique is a loglinear combination of Kneser-
Ney count smoothing with Zens-Ney glass-box
smoothing; this yields an average gain of 1.6
BLEU points over relative frequencies. In the
large-corpus setting, the best technique is a log-
linear combination of relative-frequency estimates
with Zens-Ney smoothing, with a gain of 1.1
BLEU points. Of the two glass-box smoothing
methods tested, Zens-Ney appears to have a slight
advantage over Koehn-Och-Marcu. Of the black-
box methods tested, Kneser-Ney is clearly bet-
ter for small corpora, but is equivalent to Good-
Turing for larger corpora.
The paper describes several smoothing alterna-
tives which we intend to test in future work:
? Linear versus loglinear combinations (in our
current work, these coincide with the black-
box versus glass-box distinction, making it
impossible to draw conclusions).
? Lower-order distributions as described in sec-
tion 3.4.
? Separate count-smoothing bins based on
phrase length.
7 Acknowledgements
The authors would like to thank their colleague
Michel Simard for stimulating discussions. The
first author would like to thank all his colleagues
for encouraging him to taste a delicacy that was
new to him (shredded paper with maple syrup).
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are
those of the author(s) and do not necessarily re-
flect the views of the Defense Advanced Research
Projects Agency (DARPA).
References
Peter F. Brown, Stephen A. Della Pietra, Vincent
Della J. Pietra, and Robert L. Mercer. 1993. The
mathematics of Machine Translation: Parameter es-
timation. Computational Linguistics, 19(2):263?
312, June.
M. Cettollo, M. Federico, N. Bertoldi, R. Cattoni, and
B. Chen. 2005. A look inside the ITC-irst SMT
system. In Proceedings of MT Summit X, Phuket,
Thailand, September. International Association for
Machine Translation.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
K. Church and W. Gale. 1991. A comparison of the
enhanced Good-Turing and deleted estimation meth-
ods for estimating probabilities of English bigrams.
Computer speech and language, 5(1):19?54.
M. Collins and J. Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Proceed-
ings of the 3rd ACL Workshop on Very Large Cor-
pora (WVLC), Cambridge, Massachusetts.
Ismael Garc??a-Varea, Francisco Casacuberta, and Her-
mann Ney. 1998. An iterative, DP-based search al-
gorithm for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Spo-
ken Language Processing (ICSLP) 1998, volume 4,
pages 1135?1138, Sydney, Australia, December.
Joshua Goodman. 2001. A bit of progress in language
modeling. Computer Speech and Language.
Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP) 1995,
pages 181?184, Detroit, Michigan. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Ed-
uard Hovy, editor, Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 127?133, Edmonton, Alberta,
Canada, May. NAACL.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, D. Talbot, and M. White. 2005. Ed-
inburgh system description for the 2005 NIST MT
evaluation. In Proceedings of Machine Translation
Evaluation Workshop.
Philippe Langlais, Guihong Cao, and Fabrizio Gotti.
2005. RALI: SMT shared task system description.
60
In Proceedings of the 2nd ACL workshop on Build-
ing and Using Parallel Texts, pages 137?140, Uni-
versity of Michigan, Ann Arbor, June.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, PA.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 10:1?38.
Arthur Na?das. 1985. On Turing?s formula for
word probabilities. IEEE Transactions on Acous-
tics, Speech and Signal Processing (ASSP), ASSP-
33(6):1415?1417, December.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of Machine Translation. Technical Re-
port RC22176, IBM, September.
Andreas Stolcke. 2002. SRILM - an extensi-
ble language modeling toolkit. In Proceedings of
the 7th International Conference on Spoken Lan-
guage Processing (ICSLP) 2002, Denver, Colorado,
September.
WMT. 2006. The NAACL Workshop on Statistical
Machine Translation (www.statmt.org/wmt06), New
York, June.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the
ACL, Boston, May.
61
Proceedings of the Workshop on Statistical Machine Translation, pages 134?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
PORTAGE: with Smoothed Phrase Tables
and Segment Choice Models
Howard Johnson
National Research Council
Institute for Information Technology
Interactive Information
1200 Montreal Road
Ottawa, ON, Canada K1A 0R6
Howard.Johnson@cnrc-nrc.gc.ca
Fatiha Sadat, George Foster, Roland Kuhn,
Michel Simard, Eric Joanis and Samuel Larkin
National Research Council
Institute for Information Technology
Interactive Language Technologies
101 St-Jean-Bosco Street
Gatineau, QC, Canada K1A 0R6
firstname.lastname@cnrc-nrc.gc.ca
Abstract
Improvements to Portage and its partici-
pation in the shared task of NAACL 2006
Workshop on Statistical Machine Trans-
lation are described. Promising ideas in
phrase table smoothing and global dis-
tortion using feature-rich models are dis-
cussed as well as numerous improvements
in the software base.
1 Introduction
The statistical machine translation system Portage is
participating in the NAACL 2006 Workshop on Sta-
tistical Machine Translation. This is a good opportu-
nity to do benchmarking against a publicly available
data set and explore the benefits of a number of re-
cently added features.
Section 2 describes the changes that have been
made to Portage in the past year that affect the par-
ticipation in the 2006 shared task. Section 3 outlines
the methods employed for this task and extensions
of it. In Section 4 the results are summarized in tab-
ular form. Following these, there is a conclusions
section that highlights what can be gleaned of value
from these results.
2 Portage
Because this is the second participation of Portage in
such a shared task, a description of the base system
can be found elsewhere (Sadat et al 2005). Briefly,
Portage is a research vehicle and development pro-
totype system exploiting the state-of-the-art in sta-
tistical machine translation (SMT). It uses a custom
built decoder followed by a rescoring module that
adjusts weights based on a number of features de-
fined on the source sentence. We will devote space
to discussing changes made since the 2005 shared
task.
2.1 Phrase-Table Smoothing
Phrase-based SMT relies on conditional distribu-
tions p(s|t) and p(t|s) that are derived from the joint
frequencies c(s, t) of source/target phrase pairs ob-
served in an aligned parallel corpus. Traditionally,
relative-frequency estimation is used to derive con-
ditional distributions, ie p(s|t) = c(s, t)/
?
s c(s, t).
However, relative-frequency estimation has the
well-known problem of favouring rare events. For
instance, any phrase pair whose constituents occur
only once in the corpus will be assigned a probabil-
ity of 1, almost certainly higher than the probabili-
ties of pairs for which much more evidence exists.
During translation, rare pairs can directly compete
with overlapping frequent pairs, so overestimating
their probabilities can significantly degrade perfor-
mance.
To address this problem, we implemented two
simple smoothing strategies. The first is based on
the Good-Turing technique as described in (Church
and Gale, 1991). This replaces each observed joint
frequency c with cg = (c + 1)nc+1/nc, where nc
is the number of distinct pairs with frequency c
(smoothed for large c). It also assigns a total count
mass of n1 to unseen pairs, which we distributed
in proportion to the frequency of each conditioning
134
phrase. The resulting estimates are:
pg(s|t) =
cg(s, t)
?
s cg(s, t) + p(t)n1
,
where p(t) = c(t)/
?
t c(t). The estimates for
pg(t|s) are analogous.
The second strategy is Kneser-Ney smoothing
(Kneser and Ney, 1995), using the interpolated vari-
ant described in (Chen and Goodman., 1998):1
pk(s|t) =
c(s, t) ? D + D n1+(?, t) pk(s)
?
s c(s, t)
where D = n1/(n1 + 2n2), n1+(?, t) is the num-
ber of distinct phrases s with which t co-occurs, and
pk(s) = n1+(s, ?)/
?
s n1+(s, ?), with n1+(s, ?)
analogous to n1+(?, t).
Our approach to phrase-table smoothing contrasts
to previous work (Zens and Ney, 2004) in which
smoothed phrase probabilities are constructed from
word-pair probabilities and combined in a log-linear
model with an unsmoothed phrase-table. We believe
the two approaches are complementary, so a combi-
nation of both would be worth exploring in future
work.
2.2 Feature-Rich DT-based distortion
In a recent paper (Kuhn et al 2006), we presented a
new class of probabilistic ?Segment ChoiceModels?
(SCMs) for distortion in phrase-based systems. In
some situations, SCMs will assign a better distortion
score to a drastic reordering of the source sentence
than to no reordering; in this, SCMs differ from the
conventional penalty-based distortion, which always
favours less rather than more distortion.
We developed a particular kind of SCM based on
decision trees (DTs) containing both questions of a
positional type (e.g., questions about the distance
of a given phrase from the beginning of the source
sentence or from the previously translated phrase)
and word-based questions (e.g., questions about the
presence or absence of given words in a specified
phrase).
The DTs are grown on a corpus consisting of
segment-aligned bilingual sentence pairs. This
1As for Good-Turing smoothing, this formula applies only
to pairs s, t for which c(s, t) > 0, since these are the only ones
considered by the decoder.
segment-aligned corpus is obtained by training a
phrase translation model on a large bilingual cor-
pus and then using it (in conjunction with a distor-
tion penalty) to carry out alignments between the
phrases in the source-language sentence and those
in the corresponding target-language sentence in a
second bilingual corpus. Typically, the first corpus
(on which the phrase translation model is trained) is
the same as the second corpus (on which alignment
is carried out). To avoid overfitting, the alignment
algorithm is leave-one-out: statistics derived from
a particular sentence pair are not used to align that
sentence pair.
Note that the experiments reported in (Kuhn et
al, 2006) focused on translation of Chinese into En-
glish. The interest of the experiments reported here
onWMT data was to see if the feature-rich DT-based
distortion model could be useful for MT between
other language pairs.
3 Application to the Shared Task: Methods
3.1 Restricted Resource Exercise
The first exercise that was done is to replicate the
conditions of 2005 as closely as possible to see the
effects of one year of research and development.
The second exercise was to replicate all three of
these translation exercises using the 2006 language
model, and to do the three exercises of translat-
ing out of English into French, Spanish, and Ger-
man. This was our baseline for other studies. A
third exercise involved modifying the generation
of the phrase-table to incorporate our Good-Turing
smoothing. All six language pairs were re-processed
with these phrase-tables. The improvement in the
results on the devtest set were compelling. This be-
came the baseline for further work. A fourth ex-
ercise involved replacing penalty-based distortion
modelling with the feature-rich decision-tree based
distortion modelling described above. A fifth ex-
ercise involved the use of a Kneser-Ney phrase-
table smoothing algorithm as an alternative to Good-
Turing.
For all of these exercises, 1-best results after de-
coding were calculated as well as rescoring on 1000-
best lists of results using 12 feature functions (13
in the case of decision-tree based distortion mod-
elling). The results submitted for the shared task
135
were the results of the third and fourth exercises
where rescoring had been applied.
3.2 Open Resource Exercise
Our goal in this exercise was to conduct a com-
parative study using additional training data for the
French-English shared task. Results of WPT 2005
showed an improvement of at least 0.3 BLEU point
when exploiting different resources for the French-
English pair of languages. In addition to the training
resources used in WPT 2005 for the French-English
task, i.e. Europarl and Hansard, we used a bilingual
dictionary, Le Grand Dictionnaire Terminologique
(GDT) 2 to train translation models and the English
side of the UN parallel corpus (LDC2004E13) to
train an English language model. Integrating termi-
nological lexicons into a statistical machine transla-
tion engine is not a straightforward operation, since
we cannot expect them to come with attached prob-
abilities. The approach we took consists on view-
ing all translation candidates of each source term or
phrase as equiprobable (Sadat et al 2006).
In total, the data used in this second part of our
contribution to WMT 2006 is described as follows:
(1) A set of 688,031 sentences in French and En-
glish extracted from the Europarl parallel corpus (2)
A set of 6,056,014 sentences in French and English
extracted from the Hansard parallel corpus, the offi-
cial record of Canada?s parliamentary debates. (3) A
set of 701,709 sentences in French and English ex-
tracted from the bilingual dictionary GDT. (4) Lan-
guage models were trained on the French and En-
glish parts of the Europarl and Hansard. We used
the provided Europarl corpus while omitting data
from Q4/2000 (October-December), since it is re-
served for development and test data. (5) An addi-
tional English language model was trained on 128
million words of the UN Parallel corpus.
For the supplied Europarl corpora, we relied on
the existing segmentation and tokenization, except
for French, which we manipulated slightly to bring
into line with our existing conventions (e.g., convert-
ing l ? an into l? an, aujourd ? hui into aujourd?hui).
For the Hansard corpus used to supplement our
French-English resources, we used our own align-
ment based on Moore?s algorithm, segmentation,
2http://www.granddictionnaire.com/
and tokenization procedures. English preprocessing
simply included lower-casing, separating punctua-
tion from words and splitting off ?s.
4 Results
The results are shown in Table 1. The numbers
shown are BLEU scores. The MC rows correspond
to the multi-corpora results described in the open re-
source exercise section above. All other rows are
from the restricted resource exercise.
The devtest results are the scores computed be-
fore the shared-task submission and were used to
drive the choice of direction of the research. The
test results were computed after the shared-task sub-
mission and serve for validation of the conclusions.
We believe that our use of multiple training cor-
pora as well as our re-tokenization for French and
an enhanced language model resulted in our overall
success in the English-French translation track. The
results for the in-domain test data puts our group at
the top of the ranking table drawn by the organizers
(first on Adequacy and fluency and third on BLEU
scores).
5 Conclusion
Benchmarking with same language model and pa-
rameters as WPT05 reproduces the results with a
tiny improvement. The larger language model used
in 2006 for English yields about half a BLEU. Good-
Turing phrase table smoothing yields roughly half
a BLEU point. Kneser-Ney phrase table smooth-
ing yields between a third and half a BLEU point
more than Good-Turing. Decision tree based distor-
tion yields a small improvement for the devtest set
when rescoring was not used but failed to show im-
provement on the test set.
In summary, the results from phrase-table
smoothing are extremely encouraging. On the other
hand, the feature-rich decision tree distortion mod-
elling requires additional work before it provides a
good pay-back. Fortunately we have some encour-
aging avenues under investigation. Clearly there is
more work needed for both of these areas.
Acknowledgements
We wish to thank Aaron Tikuisis and Denis Yuen
for important contributions to the Portage code base
136
Table 1: Restricted and open resource results
fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
devtest: with rescoring
WPT05 29.32 29.08 23.21
LM-2005 29.30 29.21 23.41
LM-2006 29.88 29.54 23.94 30.43 28.81 17.33
GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62
GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84
KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78
MC WPT05 29.63
MC 30.09 31.30
MC+GT-PTS 30.75 31.37
devtest: 1-best after decoding
LM-2006 28.59 28.45 23.22 29.22 28.30 16.94
GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32
GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42
KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78
MC WPT05 28.71
MC 29.63 31.01
MC+GT-PTS 29.90 31.22
test: with rescoring
LM-2006 26.64 28.43 21.33 28.06 28.01 15.19
GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38
GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45
KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64
MC 26.95 29.12
MC+GT-PTS 27.10 29.46
test: 1-best after decoding
LM-2006 25.35 27.25 20.46 27.20 27.18 14.60
GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05
GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92
KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42
MC 26.70 28.74
MC+GT-PTS 26.81 29.03
and the OQLF (Office Que?be?cois de la Langue
Franc?aise) for permission to use the GDT.
References
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
K. Church and W. Gale. 1991. A comparison of the en-
hanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Com-
puter speech and language, 5(1):19?54.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-
nis and J. H. Johnson. 2006. Segment Choice Models:
Feature-Rich Models for Global Distortion in Statisti-
cal Machine Translation (accepted for publication in
HLT-NAACL conference, to be held June 2006).
F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin and A. Tikuisis. 2005. PORTAGE: A
Phrase-based Machine Translation System In Proc.
ACL 2005 Workshop on building and using parallel
texts. Ann Arbor, Michigan.
F. Sadat, G. Foster and R. Kuhn. 2006. Syste`me de tra-
duction automatique statistique combinant diffe?rentes
ressources. In Proc. TALN 2006 (Traitement Automa-
tique des Langues Naturelles). Leuven, Belgium, April
10-13, 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conference / North American
Chapter of the ACL, Boston, May.
137
Proceedings of the Second Workshop on Statistical Machine Translation, pages 185?188,
Prague, June 2007. c?2007 Association for Computational Linguistics
NRC?s PORTAGE system for WMT 2007
Nicola Ueffing, Michel Simard, Samuel Larkin
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Howard Johnson
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
Howard.Johnson@nrc.gc.ca
Abstract
We present the PORTAGE statistical
machine translation system which par-
ticipated in the shared task of the ACL
2007 Second Workshop on Statistical
Machine Translation. The focus of this
description is on improvements which
were incorporated into the system over
the last year. These include adapted lan-
guage models, phrase table pruning, an
IBM1-based decoder feature, and rescor-
ing with posterior probabilities.
1 Introduction
The statistical machine translation (SMT) sys-
tem PORTAGE was developed at the National
Research Council Canada and has recently been
made available to Canadian universities and
research institutions. It is a state-of-the-art
phrase-based SMT system. We will shortly de-
scribe its basics in this paper and then high-
light the new methods which we incorporated
since our participation in the WMT 2006 shared
task. These include new scoring methods for
phrase pairs, pruning of phrase tables based
on significance, a higher-order language model,
adapted language models, and several new de-
coder and rescoring models. PORTAGE was
also used in a joint system developed in coop-
eration with Systran. The interested reader is
referred to (Simard et al, 2007).
Throughout this paper, let sJ1 := s1 . . . sJ de-
note a source sentence of length J , tI1 := t1 . . . tI
a target sentence of length I, and s? and t? phrases
in source and target language, respectively.
2 Baseline
As baseline for our experiments, we used a ver-
sion of PORTAGE corresponding to its state at
the time of the WMT 2006 shared task. We pro-
vide a basic description of this system here; for
more details see (Johnson et al, 2006).
PORTAGE implements a two-stage transla-
tion process: First, the decoder generates N -
best lists, using a basic set of models which are
then rescored with additional models in a sec-
ond step. In the baseline system, the decoder
uses the following models (or feature functions):
? one or several phrase table(s), which model
the translation direction p(s? | t?). They are
generated from the training corpus via the
?diag-and? method (Koehn et al, 2003)
and smoothed using Kneser-Ney smooth-
ing (Foster et al, 2006),
? one or several n-gram language model(s)
trained with the SRILM toolkit (Stolcke,
2002); in the baseline experiments reported
here, we used a trigram model,
? a distortion model which assigns a penalty
based on the number of source words which
are skipped when generating a new target
phrase,
? a word penalty.
These different models are combined log-
linearly. Their weights are optimized
w.r.t. BLEU score using the algorithm de-
scribed in (Och, 2003). This is done on the
provided development corpus. The search
algorithm implemented in the decoder is a
dynamic-programming beam-search algorithm.
185
After the decoding step, rescoring with addi-
tional models is performed. The baseline system
generates a 1,000-best list of alternative trans-
lations for each source sentence. These lists
are rescored with the different models described
above, a character penalty, and three different
features based on IBM Models 1 and 2 (Brown
et al, 1993) calculated in both translation di-
rections. The weights of these additional models
and of the decoder models are again optimized
to maximize BLEU score.
Note that we did not use the decision-tree-
based distortion models described in (Johnson
et al, 2006) here because they did not improve
translation quality.
In the following subsections, we will describe
the new models added to the system for our
WMT 2007 submissions.
3 Improvements in PORTAGE
3.1 Phrase translation models
Whereas the phrase tables used in the baseline
system contain only one score for each phrase
pair, namely conditional probabilities calculated
using Kneser-Ney smoothing, our current sys-
tem combines seven different phrase scores.
First, we used several types of phrase table
smoothing in the WMT 2007 system because
this proved helpful on other translation tasks:
relative frequency estimates, Kneser-Ney- and
Zens-Ney-smoothed probabilities (Foster et al,
2006). Furthermore, we added normalized joint
probability estimates to the phrase translation
model. The other three scores will be explained
at the end of this subsection.
We pruned the generated phrase tables fol-
lowing the method introduced in (Johnson et
al., 2007). This approach considers all phrase
pairs (s?, t?) in the phrase table. The count C(s?, t?)
of all sentence pairs containing (s?, t?) is deter-
mined, as well as the count of all source/target
sentences containing s?/t?. Using these counts,
Fisher?s exact test is carried out to calculate
the significance of the phrase pair. The phrase
tables are then pruned based on the p-value.
Phrase pairs with low significance, i.e. which are
only weakly supported by the training data, are
pruned. This reduces the size of the phrase ta-
bles to 8-16% on the different language pairs.
See (Johnson et al, 2007) for details.
Three additional phrase scores were derived
from information on which this pruning is based:
? the significance level (or p-value),
? the number C(s?, t?) of sentence pairs con-
taining the phrase pair, normalized by the
number of source sentences containing s?,
? C(s?, t?), normalized by the number of target
sentences containing t?.
For our submissions, we used the last three
phrase scores only when translating the Eu-
roParl data. Initial experiments showed that
they do not improve translation quality on the
News Commentary data. Apart from this, the
systems for both domains are identical.
3.2 Adapted language models
Concerning the language models, we made two
changes to our system since WMT 2006. First,
we replaced the trigram language model by a 4-
gram model trained on the WMT 2007 data. We
also investigated the use of a 5-gram, but that
did not improve translation quality. Second,
we included adapted language models which
are specific to the development and test cor-
pora. For each development or test corpus, we
built this language model using information re-
trieval1 to find relevant sentences in the train-
ing data. To this end, we merged the train-
ing corpora for EuroParl and News Commen-
tary. The source sentences from the develop-
ment or test corpus served as individual queries
to find relevant training sentence pairs. For
each source sentence, we retrieved 10 sentence
pairs from the training data and used their tar-
get sides as language model training data. On
this small corpus, we trained a trigram lan-
guage model, again using the SRILM toolkit.
The feature function weights in the decoder and
the rescoring model were optimized using the
adapted language model for the development
corpus. When translating the test corpus, we
kept these weights, but replaced the adapted
1We used the lemur toolkit for querying, see
http://www.lemurproject.org/
186
language model by that specific to the test cor-
pus.
3.3 New decoder and rescoring features
We integrated several new decoder and rescoring
features into PORTAGE. During decoding, the
system now makes use of a feature based on IBM
Model 1. This feature calculates the probability
of the (partial) translation over the source sen-
tence, using an IBM1 translation model in the
direction p(tI1 | sJ1 ).
In the rescoring process, we additionally in-
cluded several types of posterior probabilities.
One is the posterior probability of the sentence
length over the N -best list for this source sen-
tence. The others are determined on the level
of words, phrases, and n-grams, and then com-
bined into a value for the whole sentence. All
posterior probabilities are calculated over theN -
best list, using the sentence probabilities which
the baseline system assigns to the translation
hypotheses. For details on the posterior prob-
abilities, see (Ueffing and Ney, 2007; Zens and
Ney, 2006). This year, we increased the length
of the N -best lists from 1,000 to 5,000.
3.4 Post-processing
For truecasing the translation output, we used
the model described in (Agbago et al, 2005).
This model uses a combination of statisti-
cal components, including an n-gram language
model, a case mapping model, and a special-
ized language model for unknown words. The
language model is a 5-gram model trained on
the WMT 2007 data. The detokenizer which we
used is the one provided for WMT 2007.
4 Experimental results
We submitted results for six of the translation
directions of the shared task: French ? English,
German ? English, and Spanish ? English.
Table 1 shows the improvements result-
ing from incorporating new techniques into
PORTAGE on the Spanish ? English EuroParl
task. The baseline system is the one described
in section 2. Trained on the 2007 training cor-
pora, this yields a BLEU score of 30.48. Adding
the new phrase scores introduced in section 3.1
yields a slight improvement in translation qual-
ity. This improvement by itself is not signifi-
cant, but we observed it consistently across all
evaluation metrics and across the different devel-
opment and test corpora. Increasing the order
of the language model and adding an adapted
language model specific to the translation input
(see section 3.2) improves the BLEU score by
0.6 points. This is the biggest gain we observe
from introducing a new method. The incorpora-
tion of the IBM1-based decoder feature causes
a slight drop in translation quality. This sur-
prised us because we found this feature to be
very helpful on the NIST Chinese ? English
translation task. Adding the posterior proba-
bilities presented in section 3.3 in rescoring and
increasing the length of the N -best lists yielded
a small, but consistent gain in translation qual-
ity. The overall improvement compared to last
year?s system is around 1 BLEU point. The gain
achieved from introducing the new methods by
themselves are relatively small, but they add up.
Table 2 shows results on all six language pairs
we translated for the shared task. The trans-
lation quality achieved on the 2007 test set is
similar to that on the 2006 test set. The system
clearly performs better on the EuroParl domain
than on News Commentary.
Table 2: Translation quality in terms of
BLEU[%] and NIST score on all tasks. True-
cased and detokenized translation output.
test2006 test2007
task BLEU NIST BLEU NIST
Eu D?E 25.27 6.82 26.02 6.91
E?D 19.36 5.86 18.94 5.71
S?E 31.54 7.55 32.09 7.67
E?S 30.94 7.39 30.92 7.41
F?E 30.90 7.51 31.90 7.68
E?F 30.08 7.26 30.06 7.26
NC D?E 20.23 6.19 23.17 7.10
E?D 13.84 5.38 16.30 5.95
S?E 31.07 7.68 31.08 8.11
E?S 30.79 7.73 32.56 8.25
F?E 24.97 6.78 26.84 7.47
E?F 24.91 6.79 26.60 7.24
187
Table 1: Effect of integrating new models and methods into the PORTAGE system. Translation
quality in terms of BLEU and NIST score, WER and PER on the EuroParl Spanish?English 2006
test set. True-cased and detokenized translation output. Best results printed in boldface.
system BLEU[%] NIST WER[%] PER[%]
baseline 30.48 7.44 58.62 42.74
+ new phrase table features 30.66 7.48 58.25 42.46
+ 4-gram LM + adapted LM 31.26 7.53 57.93 42.26
+ IBM1-based decoder feature 31.18 7.51 58.13 42.53
+ refined rescoring 31.54 7.55 57.81 42.24
5 Conclusion
We presented the PORTAGE system with which
we translated six language pairs in the WMT
2007 shared task. Starting from the state of
the system during the WMT 2006 evaluation,
we analyzed the contribution of new methods
which were incorporated over the last year in
detail. Our experiments showed that most of
these changes result in (small) improvements in
translation quality. In total, we gain about 1
BLEU point compared to last year?s system.
6 Acknowledgments
Our thanks go to the PORTAGE team at NRC
for their contributions and valuable feedback.
References
A. Agbago, R. Kuhn, and G. Foster. 2005. True-
casing for the Portage system. In Recent Ad-
vances in Natural Language Processing, pages 21?
24, Borovets, Bulgaria, September.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311,
June.
G. Foster, R. Kuhn, and J. H. Johnson. 2006.
Phrasetable smoothing for statistical machine
translation. In Proc. of the Conf. on Empir-
ical Methods for Natural Language Processing
(EMNLP), pages 53?61, Sydney, Australia, July.
J. H. Johnson, F. Sadat, G. Foster, R. Kuhn,
M. Simard, E. Joanis, and S. Larkin. 2006.
Portage: with smoothed phrase tables and seg-
ment choice models. In Proc. HLT/NAACL
Workshop on Statistical Machine Translation
(WMT), pages 134?137, New York, NY, June.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
2007. Improving translation quality by discard-
ing most of the phrasetable. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing and Conf. on Computational Natural
Language Learning (EMNLP-CoNLL), to appear,
Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Human
Language Technology Conf. (HLT-NAACL), pages
127?133, Edmonton, Canada, May/June.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
M. Simard, J. Senellart, P. Isabelle, R. Kuhn,
J. Stephan, and N. Ueffing. 2007. Knowledge-
based translation with statistical phrase-based
post-editing. In Proc. ACL Second Workshop on
Statistical Machine Translation (WMT), to ap-
pear, Prague, Czech Republic, June.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Int. Conf. on Spoken
Language Processing (ICSLP), volume 2, pages
901?904, Denver, CO.
N. Ueffing and H. Ney. 2007. Word-level confi-
dence estimation for machine translation. Com-
putational Linguistics, 33(1):9?40, March.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation.
In Proc. HLT/NAACL Workshop on Statistical
Machine Translation (WMT), pages 72?77, New
York, NY, June.
188
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?32,
New York, June 2006. c?2006 Association for Computational Linguistics
Segment Choice Models: Feature-Rich Models for Global  
Distortion in Statistical Machine Translation 
 
 
Roland Kuhn, Denis Yuen, Michel Simard, Patrick Paul,  
George Foster, Eric Joanis, and Howard Johnson 
 
Institute for Information Technology, National Research Council of Canada 
Gatineau, Qu?bec, CANADA  
Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, 
Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com  
 
 
 
 
Abstract 
This paper presents a new approach to 
distortion (phrase reordering) in phrase-
based machine translation (MT). Distor-
tion is modeled as a sequence of choices 
during translation. The approach yields 
trainable, probabilistic distortion models 
that are global: they assign a probability 
to each possible phrase reordering. These 
?segment choice? models (SCMs) can be 
trained on ?segment-aligned? sentence 
pairs; they can be applied during decoding 
or rescoring. The approach yields a metric 
called ?distortion perplexity? (?disperp?) 
for comparing SCMs offline on test data, 
analogous to perplexity for language 
models. A decision-tree-based SCM is 
tested on Chinese-to-English translation, 
and outperforms a baseline distortion 
penalty approach at the 99% confidence 
level. 
1 Introduction: Defining SCMs  
The work presented here was done in the context 
of phrase-based MT (Koehn et al, 2003; Och and 
Ney, 2004). Distortion in phrase-based MT occurs 
when the order of phrases in the source-language 
sentence changes during translation, so the order of 
corresponding phrases in the target-language trans-
lation is different. Some MT systems allow arbi-
trary reordering of phrases, but impose a distortion 
penalty proportional to the difference between the 
new and the original phrase order (Koehn, 2004). 
Some interesting recent research focuses on reor-
dering within a narrow window of phrases (Kumar 
and Byrne, 2005; Tillmann and Zhang, 2005; Till-
mann, 2004). The (Tillmann, 2004) paper intro-
duced lexical features for distortion modeling. A 
recent paper (Collins et al, 2005) shows that major 
gains can be obtained by constructing a parse tree 
for the source sentence and then applying hand-
crafted reordering rules to rewrite the source in 
target-language-like word order prior to MT.  
 
Our model assumes that the source sentence is 
completely segmented prior to distortion. This 
simplifying assumption requires generation of hy-
potheses about the segmentation of the complete 
source sentence during decoding. The model also 
assumes that each translation hypothesis grows in a 
predetermined order. E.g., Koehn?s decoder 
(Koehn 2004) builds each new hypothesis by add-
ing phrases to it left-to-right (order is deterministic 
for the target hypothesis). Our model doesn?t re-
quire this order of operation ? it would support 
right-to-left or inwards-outwards hypothesis con-
struction ? but it does require a predictable order. 
 
One can keep track of how segments in the 
source sentence have been rearranged during de-
coding for a given hypothesis, using what we call a 
?distorted source-language hypothesis? (DSH). A 
similar concept appears in (Collins et al, 2005) 
(this paper?s preoccupations strongly resemble 
25
ours, though our method is completely different: 
we don?t parse the source, and use only automati-
cally generated rules). Figure 1 shows an example 
of a DSH for German-to-English translation (case 
information is removed). Here, German ?ich habe 
das buch gelesen .? is translated into English ?i 
have read the book .? The DSH shows the distor-
tion of the German segments into an English-like 
word order that occurred during translation (we 
tend to use the word ?segment? rather than the 
more linguistically-charged  ?phrase?). 
Figure 1. Example of German-to-English DSH 
From the DSH, one can reconstruct the series of 
segment choices. In Figure 1 - given a left-to-right 
decoder - ?[ich]? was chosen from five candidates 
to be the leftmost segment in the DSH. Next, 
?[habe]? was chosen from four remaining candi-
dates, ?[gelesen]? from three candidates, and ?[das 
buch]? from two candidates. Finally, the decoder 
was forced to choose ?[.]?.   
 
Segment Choice Models (SCMs) assign 
probabilities to segment choices made as the DSH 
is constructed. The available choices at a given 
time are called the ?Remaining Segments? (RS). 
Consider a valid (though stupid) SCM that assigns 
equal probabilities to all segments in the RS. This 
uniform SCM assigns a probability of 1/5! to the 
DSH in Figure 1: the probability of choosing 
?[ich]? from among 5 RS was 1/5, then the 
probability of ?[habe]? among 4 RS was  1/4 , etc. 
The uniform SCM would be of little use to an MT 
system. In the next two sections we describe some 
more informative SCMs, define the ?distortion 
perplexity? (?disperp?) metric for comparing 
SCMs offline on a test corpus, and show how to 
construct this corpus.  
2 Disperp and Distortion Corpora 
2.1 Defining Disperp 
The ultimate reason for choosing one SCM over 
another will be the performance of an MT system 
containing it, as measured by a metric like BLEU 
(Papineni et al, 2002). However, training and 
testing a large-scale MT system for each new SCM 
would be costly. Also, the distortion component?s 
effect on the total score is muffled by other 
components (e.g., the phrase translation and target 
language models). Can we devise a quick 
standalone metric for comparing SCMs? 
 
There is an offline metric for statistical language 
models: perplexity (Jelinek, 1990). By analogy, the 
higher the overall probability a given SCM assigns 
to a test corpus of representative distorted sentence 
hypotheses (DSHs), the better the quality of the 
SCM. To define distortion perplexity (?disperp?), 
let PrM(dk) = the probability an SCM M assigns to 
a DSH for sentence k, dk. If T is a test corpus 
comprising numerous DSHs, the probability of the 
corpus according to M is PrM(T) =   k PrM(dk).  
Let S(T) = total number of segments in T. Then 
disperp(M,T) = PrM(T)-1/S(T). This gives the mean 
number of choices model M allows; the lower the 
disperp for corpus T, the better M is as a model for 
T (a model X that predicts segment choice in T 
perfectly would have disperp(X,T) = 1.0).  
2.2 Some Simple A Priori SCMs 
The uniform SCM assigns to the DSH dk that has 
S(dk) segments the probability 1/[S(dk)!] . We call 
this Model A. Let?s define some other illustrative 
SCMs. Fig. 2 shows a sentence that has 7 segments 
with 10 words (numbered 0-9 by original order). 
Three segments in the source have been used; the 
decoder has a choice of four RS. Which of the RS 
has the highest probability of being chosen? Per-
haps [2 3], because it is the leftmost RS: the ?left-
most? predictor. Or, the last phrase in the DSH will 
be followed by the phrase that originally followed 
it, [8 9]: the ?following? predictor. Or, perhaps 
positions in the source and target should be close, 
so since the next DSH position to be filled is 4, 
phrase [4] should be favoured: the ?parallel? pre-
dictor. 
 
 
Figure 2. Segment choice prediction example 
Model B will be based on the ?leftmost? predic-
tor, giving the leftmost segment in the RS twice the 
probability of the other segments, and giving the 
Original German:   [ich] [habe] [das buch] [gelesen]    [.] 
DSH for German:  [ich] [habe]  [gelesen]    [das buch] [.] 
(English:                [i]     [have]   [read]        [the book] [.]) 
original:  [0 1] [2 3] [4] [5] [6] [7] [8 9] 
DSH:  [0 1] [5] [7],   RS:  [2 3], [4], [6], [8 9] 
26
others uniform probabilities. Model C will be 
based on the ?following? predictor, doubling the 
probability for the segment in the RS whose first 
word was the closest to the last word in the DSH, 
and otherwise assigning uniform probabilities. Fi-
nally, Model D combines ?leftmost? and ?follow-
ing?: where the leftmost and following segments 
are different, both are assigned double the uniform 
probability; if they are the same segment, that 
segment has four times the uniform probability. Of 
course, the factor of 2.0 in these models is arbi-
trary. For Figure 2, probabilities would be: 
? Model A: PrA([2 3])= PrA([4])= PrA([6])= 
PrA([8 9]) = 1/4; 
? Model B: PrB ([2 3])= 2/5, PrB([4])= 
PrB([6])= PrB([8 9]) = 1/5; 
? Model C: PrC ([2 3])= PrC ([4])= PrC([6]) 
= 1/5, PrC([8 9]) = 2/5; 
? Model D: PrD ([2 3]) = PrD([8 9]) = 1/3, 
PrD([4])= PrD([6]) = 1/6.  
 
Finally, let?s define an SCM derived from the 
distortion penalty used by systems based on the 
?following? predictor, as in (Koehn, 2004). Let ai = 
start position of source phrase translated into ith 
target phrase, bi -1= end position of source phrase 
that?s translated into (i-1)th target phrase. Then 
distortion penalty d(ai, bi-1) =   ?ai? bi-1 -1?; the total 
distortion is the product of the phrase distortion 
penalties. This penalty is applied as a kind of non-
normalized probability in the decoder. The value of 
   for given (source, target) languages is optimized 
on development data. 
To turn this penalty into an SCM, penalties are 
normalized into probabilities, at each decoding 
stage; we call the result Model P (for ?penalty?). 
Model P with    = 1.0 is the same as uniform 
Model A. In disperp experiments, Model P with    
optimized on held-out data performs better than 
Models A-D (see Figure 5), suggesting that dis-
perp is a realistic measure.  
Models A-D are models whose parameters were 
all defined a priori; Model P has one trainable pa-
rameter,  . Next, let?s explore distortion models 
with several trainable parameters.  
2.3 Constructing a Distortion Corpus 
To compare SCMs using disperp and to train 
complex SCMs, we need a corpus of representative 
examples of DSHs. There are several ways of ob-
taining such a corpus. For the experiments de-
scribed here, the MT system was first trained on a 
bilingual sentence-aligned corpus. Then, the sys-
tem was run in a second pass over its own training 
corpus, using its phrase table with the standard dis-
tortion penalty to obtain a best-fit phrase alignment 
between each (source, target) sentence pair. Each 
such alignment yields a DSH whose segments are 
aligned with their original positions in the source; 
we call such a source-DSH alignment a ?segment 
alignment?. We now use a leave-one-out procedure 
to ensure that information derived from a given 
sentence pair is not used to segment-align that sen-
tence pair. In our initial experiments we didn?t do 
this, with the result that the segment-aligned cor-
pus underrepresented the case where words or N-
grams not in the phrase table are seen in the source 
sentence during decoding.  
3 A Trainable Decision Tree SCM 
Almost any machine learning technique could be 
used to create a trainable SCM. We implemented 
one based on decision trees (DTs), not because 
DTs necessarily yield the best results but for soft-
ware engineering reasons: DTs are a quick way to 
explore a variety of features, and are easily inter-
preted when grown (so that examining them can 
suggest further features). We grew N DTs, each 
defined by the number of choices available at a 
given moment. The highest-numbered DT has a 
?+? to show it handles N+1 or more choices. E.g., 
if we set N=4, we grow a ?2-choice?, a ?3-choice?, 
a ?4-choice?, and a ?5+-choice tree?. The 2-choice 
tree handles cases where there are 2 segments in 
the RS, assigning a probability to each; the 3-
choice tree handles cases where there are 3 seg-
ments in the RS, etc. The 5+-choice tree is differ-
ent from the others: it handles cases where there 
are 5 segments in the RS to choose from, and 
cases where there are more than 5. The value of N 
is arbitrary; e.g., for N=8, the trees go from ?2-
choice? up to ?9+-choice?.  
Suppose a left-to-right decoder with an N=4 
SCM is translating a sentence with seven phrases. 
Initially, when the DSH is empty, the 5+-choice 
tree assigns probabilities to each of these seven. It 
27
will use the 5+-choice tree twice more, to assign 
probabilities to six RS, then to five. To extend the 
hypothesis, it will then use the 4-choice tree, the 3-
choice tree, and finally the 2-choice tree. Disperps 
for this SCM are calculated on test corpus DSHs in 
the same left-to-right way, using the tree for the 
number of choices in the RS to find the probability 
of each segment choice. 
Segments need labels, so the N-choice DT can 
assign probabilities to the N segments in the RS. 
We currently use a ?following? labeling scheme. 
Let X be the original source position of the last 
word put into the DSH, plus 1. In Figure 2, this 
was word 7, so X=8. In our scheme, the RS seg-
ment whose first word is closest to X is labeled 
?A?; the second-closest segment is labeled ?B?, 
etc. Thus, segments are labeled in order of the 
(Koehn, 2004) penalty; the ?A? segment gets the 
lowest penalty. Ties between segments on the right 
and the left of X are broken by first labeling the 
right segment. In Figure 2, the labels for the RS 
are ?A? = [8 9], ?B? = [6], ?C? = [4], ?D? = [2 3].  
 
 
 
 
 
 
 
 
Figure 3. Some question types for choice DTs 
Figure 3 shows the main types of questions used 
for tree-growing, comprising position questions 
and word-based questions. Position questions 
pertain to location, length, and ordering of seg-
ments. Some position questions ask about the dis-
tance between the first word of a segment and the 
?following? position X: e.g., if the answer to 
?pos(A)-pos(X)=0?? is yes, then segment A comes 
immediately after the last DSH segment in the 
source, and is thus highly likely to be chosen. 
There are also questions relating to the ?leftmost? 
and ?parallel? predictors (above, sec. 2.2). The 
fseg() and bseg() functions count segments in the 
RS from left to right and right to left respectively, 
allowing, e.g., the question whether a given seg-
ment is the second last segment in the RS. The 
only word-based questions currently implemented 
ask whether a given word is contained in a given 
segment (or anywhere in the DSH, or anywhere in 
the RS). This type could be made richer by allow-
ing questions about the position of a given word in 
a given segment, questions about syntax, etc.  
Figure 4 shows an example of a 5+-choice DT. 
The ?+? in its name indicates that it will handle 
cases where there are 5 or more segments in the 
RS. The counts stored in the leaves of this DT rep-
resent the number of training data items that ended 
up there; the counts are used to estimate probabili-
ties. Some smoothing will be done to avoid zero 
probabilities, e.g., for class C in node 3.  
 
Figure 4. Example of a 5+-choice tree 
For ?+? DTs, the label closest to the end of the 
alphabet (?E? in Figure 4) stands for a class that 
can include more than one segment. E.g., if this 
5+-choice DT is used to estimate probabilities for a 
7-segment RS, the segment closest to X is labeled 
?A?, the second closest ?B?, the third closest ?C?, 
and the fourth closest ?D?. That leaves 3 segments, 
all labeled ?E?. The DT shown yields probability 
Pr(E) that one of these three will be chosen. Cur-
rently, we apply a uniform distribution within this 
?furthest from X? class, so the probability of any 
one of the three ?E? segments is estimated as 
Pr(E)/3.  
To train the DTs, we generate data items from 
the second-pass DSH corpus. Each DSH generates 
several data items. E.g., moving across a seven-
segment DSH from left to right, there is an exam-
ple of the seven-choice case, then one of the six-
choice case, etc. Thus, this DSH provides three 
items for training the 5+-choice DT and one item 
     pos(A)-pos(X)<0? 
A:27 B:23 C:20 D:11 E:19  
        today    DSH? 
A:10 B:8 C:10 D:6 E:5 
A:8 B:6 C:0 D:2 E:4 A:2 B:2 C:10 D:4 E:1 
A:17 B:15 C:10 D:5 E:14 
yes no 
yes no 
1. 
3. 
2. 5. 
4. 
1. Position Questions 
Segment Length Questions 
E.g., ?lgth(DSH)<5??, ?lgth(B)=2??, ?lgth(RS)<6??, etc.  
Questions about Original Position 
Let pos(seg) = index of seg?s first word in source sentence 
E.g., ?pos(A)=9??, ?pos(C) <17??, etc.  
Questions With X (?following? word position)  
E.g., ?pos(X)=9??, ?pos(C) ? pos(X) <0??, etc.  
Segment Order Questions  
Let fseg = segment # (forward), bseg = segment # (back-
ward) 
E.g., ?fseg(D) = 1??, ?bseg(A) <5??, etc.  
2. Word-Based Questions  
E.g., ?and   DSH??, ?November   B??, etc.  
28
each for training the 4-choice, 3-choice, and 2-
choice DTs. The DT training method was based on 
Gelfand-Ravishankar-Delp expansion-pruning 
(Gelfand et al, 1991), for DTs whose nodes con-
tain probability distributions (Lazarid?s et al, 
1996).  
4 Disperp Experiments 
We carried out SCM disperp experiments for the 
English-Chinese task, in both directions. That is, 
we trained and tested models both for the distortion 
of English into Chinese-like phrase order, and the 
distortion of Chinese into English-like phrase or-
der. For reasons of space, details about the ?dis-
torted English? experiments won?t be given here. 
Training and development data for the distorted 
Chinese experiments were taken from the NIST 
2005 release of the FBIS corpus of Xinhua news 
stories. The training corpus comprised 62,000 
FBIS segment alignments, and the development 
?dev? corpus comprised a disjoint set of 2,306 
segment alignments from the same FBIS corpus. 
All disperp results are obtained by testing on ?dev? 
corpus. 
 
Distorted Chinese: Models A-D, P, & a four-DT 
Model
1
2
3
4
5
6
7
8
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
 
Model A
Model B
Model C
Model D
Model P (alpha =
0.77)
Four DTs: pos +
100-wd qns
 
Figure 5. Several SCMs for distorted Chinese 
Figure 5 shows disperp results for the models 
described earlier. The y axis begins at 1.0 (mini-
mum value of disperp). The x axis shows number 
of alignments (DSHs) used to train DTs, on a log 
scale. Models A-D are fixed in advance; Model P?s 
single parameter    was optimized once on the en-
tire training set of 62K FBIS alignments (to 0.77) 
rather than separately for each amount of training 
data. Model P, the normalized version of  Koehn?s 
distortion penalty, is superior to Models A-D, and 
the DT-based SCM is superior to Model P.  
The Figure 5 DT-based SCM had four trees (2-
choice, 3-choice, 4-choice, and 5+-choice) with 
position-based and word-based questions. The 
word-based questions involved only the 100 most 
frequent Chinese words in the training corpus. The 
system?s disperp drops from 3.1 to 2.8 as the num-
ber of alignments goes from 500 to 62K. 
Figure 6 examines the effect of allowing word-
based questions. These questions provide a signifi-
cant disperp improvement, which grows with the 
amount of training data. 
Distorted Chinese: effect of allowing word qns 
(four- DT models)
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale) 
Di
sp
er
p 
o
n
 
"
de
v
"
 
Four DTs: pos qns
only
Four DTs: pos +
100-wd qns
 
Figure 6. Do word-based questions help? 
In the ?four-DT? results above, examples with 
five or more segments are handled by the same 
?5+-choice? tree. Increasing the number of trees 
allows finer modeling of multi-segment cases 
while spreading the training data more thinly. 
Thus, the optimal number of trees depends on the 
amount of training data. Fixing this amount to 32K 
alignments, we varied the number of trees. Figure 
7 shows that this parameter has a significant im-
pact on disperp, and that questions based on the 
most frequent 100 Chinese words help perform-
ance for any number of trees.  
29
Distorted Chinese: Disperp vs. # of trees (all 
trees grown on 32K alignments)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3 4 5 6 7 8 9 10 11 12 13 14
# of trees
Di
sp
er
p 
o
n
 
"
de
v"
 
pos qns only
pos + 100-wd qns
 
Figure 7. Varying the number of DTs  
In Figure 8 the number of the most frequent 
Chinese words for questions is varied (for a 13-DT 
system trained on 32K alignments). Most of the 
improvement came from the 8 most frequent 
words, especially from the most frequent, the 
comma ?,?. This behaviour seems to be specific to 
Chinese. In our ?distorted English? experiments, 
questions about the 8 most frequent words also 
gave a significant improvement, but each of the 8 
words had a fairly equal share in the improvement. 
Distorted Chinese: Disperp vs. #words (all trees 
grown on 32K alignments)
2.58
2.6
2.62
2.64
2.66
2.68
2.7
2.72
0 2 8 32 12
8
51
2
# words tried for qns (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
Performance of 13-
DT system
 
Figure 8. Varying #words (13-DT system) 
Finally, we grew the DT system used for the MT 
experiments: one with 13 trees and questions about 
the 25 most frequent Chinese words, grown on 
88K alignments. Its disperp on the ?dev? used for 
the MT experiments (a different ?dev? from the 
one above ? see Sec. 5.2) was 2.42 vs. 3.48 for the 
baseline Model P system: a 30% drop.  
5 Machine Translation Experiments 
5.1 SCMs for Decoding 
SCMs assume that the source sentence is fully 
segmented throughout decoding. Thus, the system 
must guess the segmentation for the unconsumed 
part of the source (?remaining source?: RS). For 
the results below, we used a simple heuristic: RS is 
broken into one-word segments. In future, we will 
apply a more realistic segmentation model to RS 
(or modify DT training to reflect accurately RS 
treatment during decoding).  
5.2 Chinese-to-English MT Experiments  
The training corpus for the MT system?s phrase 
tables consists of all parallel text available for the 
NIST MT05 Chinese-English evaluation, except 
the Xinhua corpora and part 3 of LDC's ?Multiple-
Translation Chinese Corpus? (MTCCp3). The Eng-
lish language model was trained on the same cor-
pora, plus 250M words from Gigaword. The DT-
based SCM was trained and tuned on a subset of 
this same training corpus (above). The dev corpus 
for optimizing component weights is MTCCp3. 
The experimental results below were obtained by 
testing on the evaluation set for MTeval NIST04.  
Phrase tables were learned from the training cor-
pus using the ?diag-and? method (Koehn et al, 
2003), and using IBM model 2 to produce initial 
word alignments (these authors found this worked 
as well as IBM4). Phrase probabilities were based 
on unsmoothed relative frequencies. The model 
used by the decoder was a log-linear combination 
of a phrase translation model (only in the 
P(source|target) direction), trigram language 
model, word penalty (lexical weighting), an op-
tional segmentation model (in the form of a phrase 
penalty) and distortion model. Weights on the 
components were assigned using the (Och, 2003) 
method for max-BLEU training on the develop-
ment set. The decoder uses a dynamic-
programming beam-search, like the one in (Koehn, 
2004). Future-cost estimates for all distortion mod-
els are assigned using the baseline penalty model. 
5.3 Decoding Results 
30
29,40
29,60
29,80
30,00
30,20
30,40
30,60
30,80
31,00
31,20
no PP PP no PP PP
DP DT
BL
EU
 
sc
o
re
1x beam
4x beam
 
Figure 9. BLEU on NIST04 (95% conf. = ?0.7) 
Figure 9 shows experimental results. The ?DP? 
systems use the distortion penalty in (Koehn, 2004) 
with    optimized on ?dev?, while ?DT? systems 
use the DT-based SCM. ?1x? is the default beam 
width, while ?4x? is a wider beam (our notation 
reflects decoding time, so ?4x? takes four times as 
long as ?1x?). ?PP? denotes presence of the phrase 
penalty component. The advantage of DTs as 
measured by difference between the score of the 
best DT system and the best DP system is 0.75 
BLEU at 1x and 0.5 BLEU at 4x. With a 95% 
bootstrap confidence interval of ?0.7 BLEU (based 
on 1000-fold resampling), the resolution of these 
results is too coarse to draw firm conclusions. 
Thus, we carried out another 1000-fold bootstrap 
resampling test on NIST04, this time for pairwise 
system comparison. Table 1 shows results for 
BLEU comparisons between the systems with the 
default (1x) beam. The entries show how often the 
A system (columns) had a better score than the B 
system (rows), in 1000 observations.  
   A    
vs. B   
DP,  
no PP 
DP, PP DT,  
no PP 
DT, PP 
DP,  
no PP 
x 2.95% 99.45% 99.55% 
DP, PP 97.05% x 99.95% 99.95% 
DT,  
no PP 
0.55% 0.05% x 65.68% 
DT, PP 0.45% 0.05% 34.32% x 
Table 1. Pairwise comparison for 1x systems 
The table shows that both DT-based 1x systems 
performed better than either of the DP systems 
more than 99% of the time (underlined results). 
Though not shown in the table, the same was true 
with 4x beam search. The DT 1x system with a 
phrase penalty had a higher score than the DT 1x 
system without one about 66% of the time. 
6 Summary and Discussion 
In this paper, we presented a new class of probabil-
istic model for distortion, based on the choices 
made during translation. Unlike some recent dis-
tortion models (Kumar and Byrne, 2005; Tillmann 
and Zhang, 2005; Tillmann, 2004) these Segment 
Choice Models (SCMs) allow phrases to be moved 
globally, between any positions in the sentence. 
They also lend themselves to quick offline com-
parison by means of a new metric called disperp. 
We developed a decision-tree (DT) based SCM 
whose parameters were optimized on a ?dev? cor-
pus via disperp. Two variants of the DT system 
were experimentally compared with two systems 
with a distortion penalty on a Chinese-to-English 
task. In pairwise bootstrap comparisons, the sys-
tems with DT-based distortion outperformed the 
penalty-based systems more than 99% of the time. 
The computational cost of training the DTs on 
large quantities of data is comparable to that of 
training phrase tables on the same data - large but 
manageable ? and increases linearly with the 
amount of training data. However, currently there 
is a major problem with DT training: the low pro-
portion of Chinese-English sentence pairs that can 
be fully segment-aligned and thus be used for DT 
training (about 27%). This may result in selection 
bias that impairs performance. We plan to imple-
ment an alignment algorithm with smoothed phrase 
tables (Johnson et al 2006) to achieve segment 
alignment on 100% of the training data. 
Decoding time with the DT-based distortion 
model is roughly proportional to the square of the 
number of tokens in the source sentence. Thus, 
long sentences pose a challenge, particularly dur-
ing the weight optimization step. In experiments on 
other language pairs reported elsewhere (Johnson 
et al 2006), we applied a heuristic: DT training 
and decoding involved source sentences with 60 or 
fewer tokens, while longer sentences were handled 
with the distortion penalty. A more principled ap-
31
proach would be to divide long source sentences 
into chunks not exceeding 60 or so tokens, within 
each of which reordering is allowed, but which 
cannot themselves be reordered.  
The experiments above used a segmentation 
model that was a count of the number of source 
segments (sometimes called ?phrase penalty?), but 
we are currently exploring more sophisticated 
models. Once we have found the best segmentation 
model, we will improve the system?s current na?ve 
single-word segmentation of the remaining source 
sentence during decoding, and construct a more 
accurate future cost function for beam search. An-
other obvious system improvement would be to 
incorporate more advanced word-based features in 
the DTs, such as questions about word classes 
(Tillmann and Zhang 2005, Tillmann 2004).  
We also plan to apply SCMs to rescoring N-best 
lists from the decoder. For rescoring, one could 
apply several SCMs, some with assumptions dif-
fering from those of the decoder. E.g., one could 
apply right-to-left SCMs, or ?distorted target? 
SCMs which assume a target hypothesis generated 
the source sentence, instead of vice versa.  
Finally, we are contemplating an entirely differ-
ent approach to DT-based SCMs for decoding. In 
this approach, only one DT would be used, with 
only two output classes that could be called ?C? 
and ?N?. The input to such a tree would be a par-
ticular segment in the remaining source sentence, 
with contextual information (e.g., the sequence of 
segments already chosen). The DT would estimate 
the probability Pr(C) that the specified segment is 
?chosen? and the probability Pr(N) that it is ?not 
chosen?. This would eliminate the need to guess 
the segmentation of the remaining source sentence.  
References  
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. ?The Mathematics of Statistical Machine 
Translation: Parameter Estimation?. Computational 
Linguistics, 19(2), pp. 263-311.  
 
M. Collins, P. Koehn, and I. Ku   erov?. 2005. ?Clause 
Restructuring for Statistical Machine Translation?. 
Proc. ACL, Ann Arbor, USA, pp. 531-540. 
 
S. Gelfand, C. Ravishankar, and E. Delp. 1991. ?An 
Iterative Growing and Pruning Algorithm for Clas-
sification Tree Design?. IEEE Trans. Patt. Analy. 
Mach. Int. (IEEE PAMI), V. 13, no. 2, pp. 163-174.  
 
F. Jelinek. 1990. ?Self-Organized Language Modeling 
for Speech Recognition? in Readings in Speech 
Recognition (ed. A. Waibel and K. Lee, publ. Mor-
gan Kaufmann), pp. 450-506.  
 
H. Johnson, F. Sadat, G. Foster, R. Kuhn, M. Simard, E. 
Joanis, and S. Larkin. 2006. ?PORTAGE: with 
Smoothed Phrase Tables and Segment Choice Mod-
els?.  Submitted to NAACL 2006 Workshop on Statis-
tical Machine Translation, New York City. 
P. Koehn. 2004. ?Pharaoh: a Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els?. Assoc. Machine Trans. Americas (AMTA04). 
 
P. Koehn, F.-J. Och and D. Marcu. 2003. ?Statistical 
Phrase-Based Translation?. Proc. Human Lang. 
Tech. Conf. N. Am. Chapt. Assoc. Comp. Ling. 
(NAACL03), pp. 127-133.  
 
S. Kumar and W. Byrne. 2005. ?Local Phrase Reorder-
ing Models for Statistical Machine Translation?. 
HLT/EMNLP, pp. 161-168, Vancouver, Canada.  
 
A. Lazarid?s, Y. Normandin, and R. Kuhn. 1996. ?Im-
proving Decision Trees for Acoustic Modeling?. 
Int. Conf. Spoken Lang. Proc. (ICSLP96), V. 2, pp. 
1053-1056, Philadelphia, Pennsylvania, USA. 
 
F. Och and H. Ney. 2004. ?The Alignment Template 
Approach to Statistical Machine Translation?. 
Comp. Linguistics, V. 30, Issue 4, pp. 417-449.  
 
Franz Josef Och. 2003. ?Minimum Error Rate Training 
for Statistical Machine  Translation?. Proc. ACL, 
Sapporo, Japan. 
 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
?BLEU: A method for automatic evaluation of ma-
chine translation?. Proc. ACL, pp. 311-318. 
 
C. Tillmann and T. Zhang. 2005. ?A Localized Predic-
tion Model for Statistical Machine Translation?. 
Proc. ACL.  
 
C. Tillmann. 2004. ?A Block Orientation Model for 
Statistical Machine Translation?. HLT/NAACL. 
 
S. Vogel, H. Ney, and C. Tillmann. 1996. ?HMM-Based 
Word Alignment in Statistical Translation?. 
COLING, pp. 836-841. 
32
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 127?132,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Lessons from NRC?s Portage System at WMT 2010 
 
 
Samuel Larkin, Boxing Chen, George Foster, Ulrich Germann, Eric Joanis, 
Howard Johnson, and Roland Kuhn  
National Research Council of Canada (NRC) 
Gatineau, Qu?bec, Canada. 
Firstname.Lastname@cnrc-nrc.gc.ca 
 
  
 
Abstract 
 
NRC?s Portage system participated in the Eng-
lish-French (E-F) and French-English (F-E) 
translation tasks of the ACL WMT 2010 eval-
uation. The most notable improvement over 
earlier versions of Portage is an efficient im-
plementation of lattice MERT. While Portage 
has typically performed well in Chinese to 
English MT evaluations, most recently in the 
NIST09 evaluation, our participation in WMT 
2010 revealed some interesting differences be-
tween Chinese-English and E-F/F-E transla-
tion, and alerted us to certain weak spots in 
our system. Most of this paper discusses the 
problems we found in our system and ways of 
fixing them. We learned several lessons that 
we think will be of general interest.  
1 Introduction 
Portage, the statistical machine translation sys-
tem of the National Research Council of Canada 
(NRC), is a two-pass phrase-based system. The 
translation tasks to which it is most often applied 
are Chinese to English, English to French (hen-
ceforth ?E-F?), and French to English (hence-
forth ?F-E?): in recent years we worked on Chi-
nese-English translation for the GALE project 
and for NIST evaluations, and English and 
French are Canada?s two official languages. In 
WMT 2010, Portage scored 28.5 BLEU (un-
cased) for F-E, but only 27.0 BLEU (uncased) 
for E-F. For both language pairs, Portage tru-
ecasing caused a loss of 1.4 BLEU; other WMT 
systems typically lost around 1.0 BLEU after 
truecasing. In Canada, about 80% of translations 
between English and French are from English to 
French, so we would have preferred better results 
for that direction. This paper first describes the 
version of Portage that participated in WMT 
2010. It then analyzes problems with the system 
and describes the solutions we found for some of 
them.  
2 Portage system description 
2.1 Core engine and training data 
The NRC system uses a standard two-pass 
phrase-based approach. Major features in the 
first-pass loglinear model include phrase tables 
derived from symmetrized IBM2 alignments and 
symmetrized HMM alignments, a distance-based 
distortion model, a lexicalized distortion model, 
and language models (LMs) that can be either 
static or else dynamic mixtures. Each phrase ta-
ble used was a merged one, created by separately 
training an IBM2-based and an HMM-based 
joint count table on the same data and then add-
ing the counts. Each includes relative frequency 
estimates and lexical estimates (based on Zens 
and Ney, 2004) of forward and backward condi-
tional probabilities. The lexicalized distortion 
probabilities are also obtained by adding IBM2 
and HMM counts. They involve 6 features (mo-
notone, swap and discontinuous features for fol-
lowing and preceding phrase) and are condi-
tioned on phrase pairs in a model similar to that 
of Moses (Koehn et al, 2005); a MAP-based 
backoff smoothing scheme is used to combat 
data sparseness when estimating these probabili-
ties. Dynamic mixture LMs are linear mixtures 
of ngram models trained on parallel sub-corpora 
with weights set to minimize perplexity of the 
current source text as described in (Foster and 
Kuhn, 2007); henceforth, we?ll call them ?dy-
namic LMs?.  
Decoding uses the cube-pruning algorithm of 
(Huang and Chiang, 2007) with a 7-word distor-
tion limit. Contrary to the usual implementation 
of distortion limits, we allow a new phrase to end 
127
more than 7 words past the first non-covered 
word, as long as the new phrase starts within 7 
words from the first non-covered word. Notwith-
standing the distortion limit, contiguous phrases 
can always be swapped. Out-of-vocabulary 
(OOV) source words are passed through un-
changed to the target. Loglinear weights are 
tuned with Och's max-BLEU algorithm over lat-
tices (Macherey et al, 2008); more details about 
lattice MERT are given in the next section. The 
second pass rescores 1000-best lists produced by 
the first pass, with additional features including 
various LM and IBM-model probabilities; ngram, 
length, and reordering posterior probabilities and 
frequencies; and quote and parenthesis mismatch 
indicators. To improve the quality of the maxima 
found by MERT when using large sets of partial-
ly-overlapping rescoring features, we use greedy 
feature selection, first expanding from a baseline 
set, then pruning. 
We restricted our training data to data that was 
directly available through the workshop's web-
site; we didn?t use the LDC resources mentioned 
on the website (e.g., French Gigaword, English 
Gigaword). Below, ?mono? refers to all mono-
lingual data (Europarl, news-commentary, and 
shuffle); ?mono? English is roughly three times 
bigger than ?mono? French (50.6 M lines in 
?mono? English, 17.7 M lines in ?mono? French). 
?Domain? refers to all WMT parallel training 
data except GigaFrEn (i.e., Europarl, news-
commentary, and UN).   
2.2 Preprocessing and postprocessing 
We used our own English and French pre- and 
post-processing tools, rather than those available 
from the WMT web site. For training, all English 
and French text is tokenized with a language-
specific tokenizer and then mapped to lowercase. 
Truecasing uses an HMM approach, with lexical 
probabilities derived from ?mono? and transition 
probabilities from a 3-gram LM trained on tru-
ecase ?mono?. A subsequent rule-based pass ca-
pitalizes sentence-initial words. A final detokeni-
zation step undoes the tokenization. 
2.3 System configurations for WMT 2010 
In the weeks preceding the evaluation, we tried 
several ways of arranging the resources available 
to us. We picked the configurations that gave the 
highest BLEU scores on WMT2009 Newstest. 
We found that tuning with lattice MERT rather 
than N-best MERT allowed us to employ more 
parameters and obtain better results.  
E-F system components: 
1. Phrase table trained on ?domain?;  
2. Phrase table trained on GigaFrEn;  
3. Lexicalized distortion model trained on 
?domain?;  
4. Distance-based distortion model; 
5. 5-gram French LM trained on ?mono?;  
6. 4-gram LM trained on French half of 
GigaFrEn;  
7. Dynamic LM composed of 4 LMs, each 
trained on the French half of a parallel 
corpus (5-gram LM trained on ?domain?, 
4-gram LM on GigaFrEn, 5-gram LM on 
news-commentary and 5-gram LM on 
UN). 
 
The F-E system is a mirror image of the E-F sys-
tem.  
3 Details of lattice MERT (LMERT) 
Our system?s implementation of LMERT (Ma-
cherey et al, 2008) is the most notable recent 
change in our system. As more and more features 
are included in the loglinear model, especially if 
they are correlated, N-best MERT (Och, 2003) 
shows more and more instability, because of 
convergence to local optima (Foster and Kuhn, 
2009). We had been looking for methods that 
promise more stability and better convergence. 
LMERT seemed to fit the bill. It optimizes over 
the complete lattice of candidate translations af-
ter a decoding run. This avoids some of the prob-
lems of N-best lists, which lack variety, leading 
to poor local optima and the need for many de-
coder runs. 
Though the algorithm is straightforward and is 
highly parallelizable, attention must be paid to 
space and time resource issues during implemen-
tation. Lattices output by our decoder were large 
and needed to be shrunk dramatically for the al-
gorithm to function well. Fortunately, this could 
be achieved via the finite state equivalence algo-
rithm for minimizing deterministic finite state 
machines. The second helpful idea was to sepa-
rate out the features that were a function of the 
phrase associated with an arc (e.g., translation 
length and translation model probability fea-
tures). These features could then be stored in a 
smaller phrase-feature table. Features associated 
with language or distortion models could be han-
dled in a larger transition-feature table. 
The above ideas, plus careful coding of data 
structures, brought the memory footprint down 
sufficiently to allow us to use complete lattices 
from the decoder and optimize over the complete 
128
development set for NIST09 Chinese-English. 
However, combining lattices between decoder 
runs again resulted in excessive memory re-
quirements. We achieved acceptable perfor-
mance by searching only the lattice from the lat-
est decoder run; perhaps information from earlier 
runs, though critical for convergence in N-best 
MERT, isn?t as important for LMERT.  
Until a reviewer suggested it, we had not 
thought of pruning lattices to a specified graph 
density as a solution for our memory problems. 
This is referred to in a single sentence in (Ma-
cherey et al, 2008), which does not specify its 
implementation or its impact on performance, 
and is an option of OpenFst (we didn?t use 
OpenFst). We will certainly experiment with lat-
tice pruning in future.  
Powell's algorithm (PA), which is at the core 
of MERT, has good convergence when features 
are mostly independent and do not depart much 
from a simple coordinate search; it can run into 
problems when there are many correlated fea-
tures (as with multiple translation and language 
models). Figure 1 shows the kind of case where 
PA works well. The contours of the function be-
ing optimized are relatively smooth, facilitating 
learning of new search directions from gradients. 
Figure 2 shows a more difficult case: there is 
a single optimum, but noise dominates and PA 
has difficulty finding new directions. Search of-
ten iterates over the original co-ordinates, miss-
ing optima that are nearby but in directions not 
discoverable from local gradients. Probes in ran-
dom directions can do better than iteration over 
the same directions (this is similar to the method 
proposed for N-best MERT by Cer et al, 2008). 
Each 1-dimensional MERT optimization is exact, 
so if our probe stabs a region with better scores, 
it will be discovered. Figures 1 and 2 only hint 
at the problem: in reality, 2-dimensional search 
isn?t a problem. The difficulties occur as the di-
mension grows: in high dimensions, it is more 
important to get good directions and they are 
harder to find. 
For WMT 2010, we crafted a compromise 
with the best properties of PA, yet alowing for a 
more aggressive search in more directions. We 
start with PA. As long as PA is adding new di-
rection vectors, it is continued. When PA stops 
adding new directions, random rotation (ortho-
gonal transformation) of the coordinates is per-
formed and PA is restarted in the new space. PA 
almost always fails to introduce new directions 
within the new coordinates, then fails again, so 
another set of random coordinates is chosen. This 
process repeats until convergence. In future 
work, we will look at incorporating random res-
tarts into the algorithm as additional insurance 
against premature convergence.  
Our LMERT implementation has room for 
improvement: it may still run into over-fitting 
problems with many correlated features. Howev-
er, during preparation for the evaluation, we no-
ticed that LMERT converged better than N-best 
MERT, allowing models with more features and 
higher BLEU to be chosen.  
After the WMT submission, we discovered 
that our LMERT implementation had a bug; our 
submission was tuned with this buggy LMERT. 
Comparison between our E-F submission tuned 
with N-best MERT and the same system tuned 
with bug-fixed LMERT shows BLEU gains of 
+1.5-3.5 for LMERT (on dev, WMT2009, and 
WMT2010, with no rescoring). However, N-best 
MERT performed very poorly in this particular 
case; we usually obtain a gain due to LMERT of 
+0.2-1.0 (e.g., for the submitted F-E system).  
 
 
Figure 1: Convergence for PA (Smooth Feature 
Space)  
 
 
Figure 2: Convergence for PA with Random Rotation 
(Rough Feature Space) 
129
4 Problems and Solutions 
4.1 Fixing LMERT  
Just after the evaluation, we noticed a discrepan-
cy for E-F between BLEU scores computed dur-
ing LMERT optimization and scores from the 1-
best list immediately after decoding. Our 
LMERT code had a bug that garbled any ac-
cented word in the version of the French refer-
ence in memory; previous LMERT experiments 
had English as target language, so the bug hadn?t 
showed up. The bug didn?t affect characters in 
the 7-bit ASCII set, such as English ones, only 
accented characters. Words in candidate transla-
tions were not garbled, so correct translations 
with accents received a lower BLEU score than 
they should have. As Table 1 shows, this bug 
cost us about 0.5 BLEU for WMT 2010 E-F after 
rescoring (according to NRC?s internal version 
of BLEU, which differs slightly from WMT?s 
BLEU). Despite this bug, the system tuned with 
buggy LMERT (and submitted) was still better 
than the best system we obtained with N-best 
MERT. The bug didn?t affect F-E scores.  
 
 Dev WMT2009 WMT2010 
LMERT (bug) 25.26 26.85 27.55 
LMERT 
 (no bug) 
25.43 26.89 28.07 
 
Table 1: LMERT bug fix (E-F BLEU after rescoring) 
4.2 Fixing odd translations 
After the evaluation, we carefully studied the 
system outputs on the WMT 2010 test data, par-
ticularly for E-F. Apart from truecasing errors, 
we noticed two kinds of bad behaviour: transla-
tions of proper names and apparent passthrough 
of English words to the French side.  
Examples of E-F translations of proper names 
from our WMT 2010 submission (each from a 
different sentence): 
 
Mr. Onderka ? M. Roman, Luk?? Marvan ? G. 
Luk??, Janey ? The, Janette Tozer ? Janette, 
Aysel Tugluk ? joints tugluk, Tawa Hallae ? 
Ottawa, Oleson ?  production,  Alcobendas ?  ; 
 
When the LMERT bug was fixed, some but 
not all of these bad translations were corrected 
(e.g., 3 of the 8 examples above were corrected). 
Our system passes OOV words through un-
changed. Thus, the names above aren?t OOVs, 
but words that occur rarely in the training data, 
and for which bad alignments have a dispropor-
tionate effect. We realized that when a source 
word begins with a capital, that may be a signal 
that it should be passed through. We thus de-
signed a passthrough feature function that applies 
to all capitalized forms not at the start of a sen-
tence (and also to forms at the sentence start if 
they?re capitalized elsewhere). Sequences of one 
or more capitalized forms are grouped into a 
phrase suggestion (e.g., Barack Obama ? bar-
rack obama) which competes with phrase table 
entries and is assigned a weight by MERT. 
The passthrough feature function yields a tiny 
improvement over the E-F system with the bug-
fixed LMERT on the dev corpus (WMT2008): 
+0.06 BLEU (without rescoring). It yields a larg-
er improvement on our test corpus: +0.27 BLEU 
(without rescoring). Furthermore, it corrects all 
the examples from the WMT 2010 test shown 
above (after the LMERT bug fix 5 of the 8 ex-
amples above still had problems, but when the 
passthrough function is incorporated all of them 
go away). Though the BLEU gain is small, we 
are happy to have almost eradicated this type of 
error, which human beings find very annoying.  
The opposite type of error is apparent pass-
through. For instance, ?we?re? appeared 12 times 
in the WMT 2010 test data, and was translated 6 
times into French as ?we?re? - even though better 
translations had higher forward probabilities. The 
source of the problem is the backward probabili-
ty P(E=?we?re?|F=?we?re?), which is 1.0; the 
backward probabilities for valid French transla-
tions of ?we?re? are lower. Because of the high 
probability P(E=?we?re?|F=?we?re?) within the 
loglinear combination, the decoder often chooses 
?we?re? as the French translation of ?we?re?. 
The (E=?we?re?, F=?we?re?) pair in WMT 
2010 phrase tables arose from two sentence pairs 
where the ?French? translation of an English sen-
tence is a copy of that English sentence. In both, 
the original English sentence contains ?we?re?. 
Naturally, the English words on the ?French? 
side are word-aligned with their identical twins 
on the English side. Generally, if the training 
data has sentence pairs where the ?French? sen-
tence contains words from the English sentence, 
those words will get high backward probabilities 
of being translated as themselves. This problem 
may not show up as an apparent passthrough; 
instead, it may cause MERT to lower the weight 
of the backward probability component, thus 
hurting performance.  
We estimated English contamination of the 
French side of the parallel training data by ma-
130
nually inspecting a random sample of ?French? 
sentences containing common English function 
words. Manual inspection is needed for accurate 
estimation: a legitimate French sentence might 
contain mostly English words if, e.g., it is short 
and cites the title of an English work (this 
wouldn?t count as contamination). The degree of 
contamination is roughly 0.05% for Europarl, 
0.5% for news-commentary, 0.5% for UN, and 
1% for GigaFrEn (in these corpora the French is 
also contaminated by other languages, particular-
ly German). Foreign contamination of English 
for these corpora appears to be much less fre-
quent.  
Contamination can take strange forms. We ex-
pected to see English sentences copied over in-
tact to the French side, and we did, but we did 
not expect to see so many ?French? sentences 
that interleaved short English word sequences 
with short French word sequences, apparently 
because text with an English and a French col-
umn had been copied by taking lines from alter-
nate columns. We found many of these inter-
leaved ?French? sentences, and found some of 
them in exactly this form on the Web (i.e., the 
corruption didn?t occur during WMT data collec-
tion). The details may not matter: whenever the 
?French? training sentence contains words from 
its English twin, there can be serious damage via 
backward probabilities. 
To test this hypothesis, we filtered all parallel 
and monolingual training data for the E-F system 
with a language guessing tool called text_cat 
(Cavnar and Trenkle, 1994). From parallel data, 
we filtered out sentence pairs whose French side 
had a high probability of not being French; from 
LM training data, sentences with a high non-
French probability. We set the filtering level by 
inspecting the guesser?s assessment of news-
commentary sentences, choosing a rather aggres-
sive level that eliminated 0.7% of news-
commentary sentence pairs. We used the same 
level to filter Europarl (0.8% of sentence pairs 
removed), UN (3.4%), GigaFrEn (4.7%), and 
?mono? (4.3% of sentences).  
 
 Dev WMT2009 WMT2010 
Baseline 25.23 26.47 27.72 
Filtered 25.45 26.66 27.98 
 
Table 2: Data filtering (E-F BLEU, no rescoring) 
 
Table 2 shows the results: a small but consis-
tent gain (about +0.2 BLEU without rescoring). 
We have not yet confirmed the hypothesis that 
copies of source-language words in the paired 
target sentence within training data can damage 
system performance via backward probabilities.  
4.3 Fixing problems with LM training   
Post-evaluation, we realized that our arrange-
ment of the training data for the LMs for both 
language directions was flawed. The grouping 
together of disparate corpora in ?mono? and 
?domain? didn?t allow higher-quality, truly in-
domain corpora to be weighted more heavily 
(e.g., the news corpora should have higher 
weights than Europarl, but they are lumped to-
gether in ?mono?). There are also potentially 
harmful overlaps between LMs (e.g., GigaFrEn 
is used both inside and outside the dynamic LM).  
We trained a new set of French LMs for the E-
F system, which replaced all the French LMs 
(#5-7) described in section 2.3 in the E-F system: 
1. 5-gram LM trained on news-commentary 
and shuffle;  
2. Dynamic LM based on 4 5-gram LMs 
trained on French side of parallel data 
(LM trained on GigaFrEn, LM on UN, 
LM on Europarl, and LM on news-
commentary). 
We did not apply the passthrough function or 
language filtering (section 4.2) to any of the 
training data for any component (LMs, TMs, dis-
tortion models) of this system; we did use the 
bug-fixed version of LMERT (section 4.1). 
The experiments with these new French LMs 
for the E-F system yielded a small decrease of 
NRC BLEU on dev (-0.15) and small increases 
on WMT Newstest 2009 and Newstest 2010 
(+0.2 and +0.4 respectively without rescoring). 
We didn?t do F-E experiments of this type.  
4.4 Pooling improvements   
The improvements above were (individual un-
cased E-F BLEU gains without rescoring in 
brackets): LMERT bug fix (about +0.5); pass-
through feature function (+0.1-0.3); language 
filtering for French (+0.2). There was also a 
small gain on test data by rearranging E-F LM 
training data, though the loss on ?dev? suggests 
this may be a statistical fluctuation. We built 
these four improvements into the evaluation E-F 
system, along with quote normalization: in all 
training and test data, diverse single quotes were 
mapped onto the ascii single quote, and diverse 
double quotes were mapped onto the ascii double 
quote. The average result on WMT2009 and 
WMT2010 was +1.7 BLEU points compared to 
the original system, so there may be synergy be-
131
tween the improvements. The original system 
had gained +0.3 from rescoring, while the final 
improved system only gained +0.1 from rescor-
ing: a post-evaluation rescored gain of +1.5.  
An experiment in which we dropped lexica-
lized distortion from the improved system 
showed that this component yields about +0.2 
BLEU. Much earlier, when we were still training 
systems with N-best MERT, incorporation of the 
6-feature lexicalized distortion often caused 
scores to go down (by as much as 2.8 BLEU). 
This illustrates how LMERT can make incorpo-
ration of many more features worthwhile.  
4.5 Fixing truecasing  
Our truecaser doesn?t work as well as truecasers 
of other WMT groups: we lost 1.4 BLEU by tru-
ecasing in both language directions, while others 
lost 1.0 or less. To improve our truecaser, we 
tried: 1. Training it on all relevant data and 2. 
Collecting 3-gram case-pattern statistics instead 
of unigrams. Neither of these helped significant-
ly. One way of improving the truecaser would be 
to let case information from source words influ-
ence the case of the corresponding target words. 
Alternatively, one of the reviewers stated that 
several labs involved in WMT have no separate 
truecaser and simply train on truecase text. We 
had previously tried this approach for NIST Chi-
nese-English and discarded it because of its poor 
performance. We are currently re-trying it on 
WMT data; if it works better than having a sepa-
rate truecaser, this was yet another area where 
lessons from Chinese-English were misleading. 
5 Lessons  
LMERT is an improvement over N-best MERT. 
The submitted system was one for which N-best 
MERT happened to work very badly, so we got 
ridiculously large gains of +1.5-3.5 BLEU for 
non-buggy LMERT over N-best MERT. These 
results are outliers: in experiments with similar 
configurations, we typically get +0.2-1.0 for 
LMERT over N-best MERT. Post-evaluation, 
four minor improvements ? a case-based pass-
through function, language filtering, LM rear-
rangement, and quote normalization ? collective-
ly gave a nice improvement. Nothing we tried 
helped truecaser performance significantly, 
though we have some ideas on how to proceed. 
We learned some lessons from WMT 2010. 
Always test your system on the relevant lan-
guage pair. Our original version of LMERT was 
developed on Chinese-English and worked well 
there, but had a bug that surfaced only when the 
target language had accents.  
European language pairs are more porous to 
information than Chinese-English. Our WMT 
system reflected design decisions for Chinese-
English, and thus didn?t exploit case information 
in the source: it passed through OOVs to the tar-
get, but didn?t pass through upper-case words 
that are likely to be proper nouns.  
It is beneficial to remove foreign-language 
contamination from the training data.  
When entering an evaluation one hasn?t parti-
cipated in for several years, always read system 
papers from the previous year. Some of the 
WMT 2008 system papers mention passthrough 
of some non-OOVs, filtering out of noisy train-
ing data, and using the case of a source word to 
predict the case of the corresponding target word. 
References  
William Cavnar and John Trenkle. 1994. N-Gram-
Based Text Categorization. Proc. Symposium on 
Document Analysis and Information Retrieval, 
UNLV Publications/Reprographics, pp. 161-175. 
Daniel Cer, Daniel Jurafsky, and Christopher D. 
Manning. 2008. Regularization and search for min-
imum error rate training. Proc. Workshop on 
SMT, pp. 26-34. 
George Foster and Roland Kuhn. 2009. Stabilizing 
Minimum Error Rate Training. Proc. Workshop 
on SMT, pp. 242-249. 
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. Proc. Workshop on 
SMT, pp. 128-135. 
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language 
Models.  Proc. ACL, pp.  144-151. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Transcription Evalua-
tion. MT Eval. Workshop. 
Wolfgang Macherey, Franz Josef Och, Ignacio Thay-
er, and Jakob Uszkoreit. 2008. Lattice-based Min-
imum Error Rate Training for Statistical Machine-
Translation. Conf. EMNLP, pp. 725-734. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation.  Proc. ACL, 
pp. 160-167.  
Richard Zens and Hermann Ney. 2004. Improvements 
in Phrase-Based Statistical Machine Translation. 
Proc. HLT/NAACL, pp. 257-264. 
132
