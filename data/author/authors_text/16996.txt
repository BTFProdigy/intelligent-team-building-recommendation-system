Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87?91,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Query log analysis with GALATEAS LangLog
Marco Trevisan and Luca Dini
CELI
trevisan@celi.it
dini@celi.it
Eduard Barbu
Universita` di Trento
eduard.barbu@unitn.it
Igor Barsanti
Gonetwork
i.barsanti@gonetwork.it
Nikolaos Lagos
Xerox Research Centre Europe
Nikolaos.Lagos@xrce.xerox.com
Fre?de?rique Segond and Mathieu Rhulmann
Objet Direct
fsegond@objetdirect.com
mruhlmann@objetdirect.com
Ed Vald
Bridgeman Art Library
ed.vald@bridgemanart.co.uk
Abstract
This article describes GALATEAS
LangLog, a system performing Search Log
Analysis. LangLog illustrates how NLP
technologies can be a powerful support
tool for market research even when the
source of information is a collection of
queries each one consisting of few words.
We push the standard Search Log Analysis
forward taking into account the semantics
of the queries. The main innovation of
LangLog is the implementation of two
highly customizable components that
cluster and classify the queries in the log.
1 Introduction
Transaction logs become increasingly important
for studying the user interaction with systems
likeWeb Searching Engines, Digital Libraries, In-
tranet Servers and others (Jansen, 2006). Var-
ious service providers keep log files recording
the user interaction with the searching engines.
Transaction logs are useful to understand the user
search strategy but also to improve query sugges-
tions (Wen and Zhang, 2003) and to enhance
the retrieval quality of search engines (Joachims,
2002). The process of analyzing the transaction
logs to understand the user behaviour and to as-
sess the system performance is known as Transac-
tion Log Analysis (TLA). Transaction Log Anal-
ysis is concerned with the analysis of both brows-
ing and searching activity inside a website. The
analysis of transaction logs that focuses on search
activity only is known as Search Log Analysis
(SLA). According to Jansen (2008) both TLA
and SLA have three stages: data collection, data
preparation and data analysis. In the data collec-
tion stage one collects data describing the user
interaction with the system. Data preparation is
the process of loading the collected data in a re-
lational database. The data loaded in the database
gives a transaction log representation independent
of the particular log syntax. In the final stage
the data prepared at the previous step is analyzed.
One may notice that the traditional three levels
log analyses give a syntactic view of the infor-
mation in the logs. Counting terms, measuring
the logical complexity of queries or the simple
procedures that associate queries with the ses-
sions in no way accesses the semantics of queries.
LangLog system addreses the semantic problem
performing clustering and classification for real
query logs. Clustering the queries in the logs al-
lows the identification of meaningful groups of
queries. Classifying the queries according to a
relevant list of categories permits the assessment
of how well the searching engine meets the user
needs. In addition the LangLog system address
problems like automatic language identification,
Name Entity Recognition, and automatic query
translation. The rest of the paper is organized
as follows: the next section briefly reviews some
systems performing SLA. Then we present the
data sources the architecture and the analysis pro-
cess of the LangLog system. The conclusion sec-
tion concludes the article summarizing the work
and presenting some new possible enhancements
of the LangLog.
87
2 Related work
The information in the log files is useful in many
ways, but its extraction raises many challenges
and issues. Facca and Lanzi (2005) offer a sur-
vey of the topic. There are several commercial
systems to extract and analyze this information,
such as Adobe web analytics1, SAS Web Analyt-
ics2, Infor Epiphany3, IBM SPSS4. These prod-
ucts are often part of a customer relation manage-
ment (CRM) system. None of those showcases
include any form of linguistic processing. On the
other hand, Web queries have been the subject
of linguistic analysis, to improve the performance
of information retrieval systems. For example, a
study (Monz and de Rijke, 2002) experimented
with shallow morphological analysis, another (Li
et al 2006) analyzed queries to remove spelling
mistakes. These works encourage our belief that
linguistic analysis could be beneficial for Web log
analysis systems.
3 Data sources
LangLog requires the following information from
the Web logs: the time of the interaction, the
query, click-through information and possibly
more. LangLog processes log files which con-
form to the W3C extended log format. No other
formats are supported. The system prototype is
based on query logs spanning one month of inter-
actions recorded at the Bridgeman Art Library5.
Bridgeman Art library contains a large repository
of images coming from 8000 collections and rep-
resenting more than 29.000 artists.
4 Analyses
LangLog organizes the search log data into units
called queries and hits. In a typical search-
ing scenario a user submits a query to the con-
tent provider?s site-searching engine and clicks
on some (or none) of the search results. From
now on we will refer to a clicked item as a hit,
and we will refer to the text typed by the user as
the query. This information alone is valuable to
the content provider because it allows to discover
1http://www.omniture.com/en/products/analytics
2http://www.sas.com/solutions/webanalytics/index.html
3http://www.infor.com
4http://www-01.ibm.com/software/analytics/spss/
5http://www.bridgemanart.com
which queries were served with results that satis-
fied the user, and which queries were not.
LangLog extracts queries and hits from the log
files, and performs the following analyses on the
queries:
? language identification
? tokenization and lemmatization
? named entity recognition
? classification
? cluster analysis
Language information may help the content
provider decide whether to translate the content
into new languages.
Lemmatization is especially important in lan-
guages like German and Italian that have a rich
morphology. Frequency statistics of keywords
help understand what users want, but they are bi-
ased towards items associated with words with
lesser ortographic and morpho-syntactic varia-
tion. For example, two thousand queries for
?trousers?, one thousand queries for ?handbag?
and another thousand queries for ?handbags?
means that handbags are twice as popular as
trousers, although statistics based on raw words
would say otherwise.
Named entities extraction helps the content
provider for the same reasons lemmatization does.
Named entities are especially important because
they identify real-world items that the content
provider can relate to, while lemmas less often do
so. The name entities and the most important con-
cepts can be linked afterwards with resources like
Wikipedia which offer a rich specification of their
properties.
Both classification and clustering allow the
content provider to understand what kind of the
users look for and how this information is targeted
by means of queries.
Classification consists of classifying queries
into categories drawn from a classification
schema. When the schema used to classify
is different from the schema used in the con-
tent provider?s website, classification may provide
hints as to what kind of queries are not matched
by items in the website. In a similar way, cluster
analysis can be used to identify new market seg-
ments or new trends in the user?s behaviour. Clus-
88
ter analysis provide more flexybility than classifi-
cation, but the information it produces is less pre-
cise. Many trials and errors may be necessary be-
fore finding interesting results. One hopes that the
final clustering solution will give insights into the
patterns of users? searches. For example an on-
line book store may discover that one cluster con-
tains many software-related terms, altough none
of those terms is popular enough to be noticeable
in the statistics.
5 Architecture
LangLog consists of three subsystems: log ac-
quisition, log analysis, log disclosure. Periodi-
cally the log acquisition subsystem gathers new
data which it passes to the log analyses compo-
nent. The results of the analyses are then available
through the log disclosure subsystem.
Log acquisition deals with the acquisition and
normalization and anonymization of the data con-
tained in the content provider?s log files. The
data flows from the content provider?s servers to
LangLog?s central database. This process is car-
ried out by a series of Pentaho Data Integration6
procedures.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
Log disclosure is actually a collection of inde-
pendent systems that allow the content providers
to access their information and the analyses. Log
disclosure systems are also concerned with access
control and protection of privacy. The content
provider can access the output of LangLog using
AWStats, QlikView, or JPivot.
? AWStats7 is a widely used log analysis sys-
tem for websites. The logs gathered from the
websites are parsed by AWStats, which gen-
erates a complete report about visitors, vis-
its duration, visitor?s countries and other data
to disclose useful information about the visi-
tor?s behavior.
6http://kettle.pentaho.com
7http://awstats.sourceforge.net
? QlikView8 is a business intelligence (BI)
platform. A BI platform provides histori-
cal, current, and predictive views of busi-
ness operations. Usually such tools are used
by companies to have a clear view of their
business over time. In LangLog, QlickView
does not display sales or costs evolution over
time. Instead, it displays queries on the con-
tent provider?s website over time. A dash-
board with many elements (input selections,
tables, charts, etc.) provides a wide range of
tools to visualize the data.
? JPivot9 is a front-end for Mondrian. Mon-
drian10 is an Online Analytical Processing
(OLAP) engine, a system capable of han-
dling and analyzing large quantities of data.
JPivot allows the user to explore the output
of LangLog, by slicing the data along many
dimensions. JPivot allows the user to display
charts, export results to Microsoft Excel or
CSV, and use custom OLAP MDX queries.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
5.1 Language Identification
The system uses a language identification sys-
tem (Bosca and Dini, 2010) which offers language
identification for English, French, Italian, Span-
ish, Polish and German. The system uses four
different strategies:
? N-gram character models: uses the distance
between the character based models of the
input and of a reference corpus for the lan-
guage (Wikipedia).
? Word frequency: looks up the frequency of
the words in the query with respect to a ref-
erence corpus for the language.
? Function words: searches for particles
highly connoting a specific language (such
as prepositions, conjunctions).
8http://www.qlikview.com
9http://jpivot.sourceforge.net
10http://mondrian.pentaho.com
89
? Prior knowledge: provides a default guess
based on a set of hypothesis and heuristics
like region/browser language.
5.2 Lemmatization
To perform lemmatization, Langlog uses general-
purpose morpho-syntactic analysers based on the
Xerox Incremental Parser (XIP), a deep robust
syntactic parser (Ait-Mokhtar et al 2002). The
system has been adapted with domain-specific
part of speech disambiguation grammar rules, ac-
cording to the results a linguistic study of the de-
velopment corpus.
5.3 Named entity recognition
LangLog uses the Xerox named entity recogni-
tion web service (Brun and Ehrmann, 2009) for
English and French. XIP includes also a named
entity detection component, based on a combina-
tion of lexical information and hand-crafted con-
textual rules. For example, the named entity
recognition system was adapted to handle titles
of portraits, which were frequent in our dataset.
While for other NLP tasks LangLog uses the same
system for every content provider, named entity
recognition is a task that produces better analyses
when it is tailored to the domain of the content.
Because LangLog uses a NER Web service, it is
easy to replace the default NER system with a dif-
ferent one. So if the content provider is interested
in the development of a NER system tailored for
a specific domain, LangLog can accomodate this.
5.4 Clustering
We developed two clustering systems: one per-
forms hierarchical clustering, another performs
soft clustering.
? CLUTO: the hierarchical clustering system
relies on CLUTO411, a clustering toolkit.
To understand the main ideas CLUTO is
based on one might consult Zhao and
Karypis (2002). The clustering process pro-
ceeds as follows. First, the set of queries to
be clustered is partitioned in k groups where
k is the number of desired clusters. To do
so, the system uses a partitional clustering
algorithm which finds the k-way clustering
solution making repeated bisections. Then
11http://glaros.dtc.umn.edu/gkhome/views/cluto
the system arranges the clusters in a hierar-
chy by successively merging the most similar
clusters in a tree.
? MALLET: the soft clustering system we
developed relies on MALLET (McCallum,
2002), a Latent Dirichlet Allocation (LDA)
toolkit (Steyvers and Griffiths, 2007).
Our MALLET-based system considers that
each query is a document and builds a topic
model describing the documents. The result-
ing topics are the clusters. Each query is as-
sociated with each topic according to a cer-
tain strenght. Unlike the system based on
CLUTO, this system produces soft clusters,
i.e. each query may belong to more than one
cluster.
5.5 Classification
LangLog allows the same query to be classified
many times using different classification schemas
and different classification strategies. The result
of the classification of an input query is always a
map that assigns each category a weight, where
the higher the weight, the more likely the query
belongs to the category. If NER performs bet-
ter when tailored to a specific domain, classifi-
cation is a task that is hardly useful without any
customization. We need a different classification
schema for each content provider. We developed
two classification system: an unsupervised sys-
tem and a supervised one.
? Unsupervised: this system does not require
any training data nor any domain-specific
corpus. The output weight of each category
is computed as the cosine similarity between
the vector models of the most representa-
tive Wikipedia article for the category and
the collection of Wikipedia articles most rel-
evant to the input query. Our evaluation in
the KDD-Cup 2005 dataset results in 19.14
precision and 22.22 F-measure. For com-
parison, the state of the art in the competi-
tion achieved a 46.1 F-measure. Our system
could not achieve a similar score because it
is unsupervised, and therefore it cannot make
use of the KDD-Cup training dataset. In ad-
dition, it uses only the query to perform clas-
sification, whereas KDD-Cup systems were
also able to access the result sets associated
to the queries.
90
? Supervised: this system is based on the
Weka framework. Therefore it can use any
machine learning algorithm implemented in
Weka. It uses features derived from the
queries and from Bridgeman metadata. We
trained a Naive Bayes classifier on a set of
15.000 queries annotated with 55 categories
and hits and obtained a F-measure of 0.26.
The results obtained for the classification
are encouraging but not yet at the level of
the state of the art. The main reason for
this is the use of only in-house meta-data in
the feature computation. In the future we
will improve both components by providing
them with features from large resources like
Wikipedia or exploiting the results returned
by Web Searching engines.
6 Demonstration
Our demonstration presents:
? The setting of our case study: the Bridgeman
Art Library website, a typical user search,
and what is recorded in the log file.
? The conceptual model of the results of the
analyses: search episodes, queries, lemmas,
named entities, classification, clustering.
? The data flow across the parts of the system,
from content provider?s servers to the front-
end through databases, NLP Web services
and data marts.
? The result of the analyses via QlikView.
7 Conclusion
In this paper we presented the LangLog system,
a customizable system for analyzing query logs.
The LangLog performs language identification,
lemmatization, NER, classification and clustering
for query logs. We tested the LangLog system on
queries in Bridgeman Library Art. In the future
we will test the system on query logs in differ-
ent domains (e.g. pharmaceutical, hardware and
software, etc.) thus increasing the coverage and
the significance of the results. Moreover we will
incorporate in our system the session information
which should increase the precision of both clus-
tering and classification components.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod and Claude
Roux 2002. Robustness Beyond Shallowness: In-
cremental Deep Parsing. Journal of Natural Lan-
guage Engineering 8, 2-3, 121-144.
Alessio Bosca and Luca Dini. 2010. Language Identi-
fication Strategies for Cross Language Information
Retrieval. CLEF 2010 Working Notes.
C. Brun and M. Ehrmann. 2007. Adaptation of
a Named Entity Recognition System for the ES-
TER 2 Evaluation Campaign. In proceedings of
the IEEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering.
F. M. Facca and P. L. Lanzi. 2005. Mining interesting
knowledge from weblogs: a survey. Data Knowl.
Eng. 53(3):225241.
Jansen, B. J. 2006. Search log analysis: What is it;
what?s been done; how to do it. Library and Infor-
mation Science Research 28(3):407-432.
Jansen, B. J. 2008. The methodology of search log
analysis. In B. J. Jansen, A. Spink and I. Taksa (eds)
Handbook of Web log analysis 100-123. Hershey,
PA: IGI.
Joachims T. 2002. Optimizing search engines us-
ing clickthrough data. In proceedings of the 8th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining 133-142.
M. Li, Y. Zhang, M. Zhu, and M. Zhou. 2006. Ex-
ploring distributional similarity based models for
query spelling correction. In proceedings of In ACL
06: the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of
the ACL 10251032, 2006.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
C. Monz and M. de Rijke. 2002. Shallow Morpholog-
ical Analysis in Monolingual Information Retrieval
for Dutch, German and Italian. In Proceedings of
CLEF 2001. Springer
M. Steyvers and T. Griffiths. 2007. Probabilistic
Topic Models. In T. Landauer, D McNamara, S.
Dennis and W. Kintsch (eds), Handbook of Latent
Semantic Analysis, Psychology Press.
J. R. Wen and H.J. Zhang 2003. Query Clustering
in the Web Context. In Wu, Xiong and Shekhar
(eds) Information Retrieval and Clustering 195-
226. Kluwer Academic Publishers.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
91
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 696?700,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
CELI: An Experiment with Cross Language Textual Entailment
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino, Italy
bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval Cross-lingual Textual Entailment for
Content Synchronization task.
1 Introduction
The Cross-Lingual Textual Entailment task (CLTE)
is a new task that addresses textual entailment (TE)
(Bentivogli et. al., 2011), targeting the cross-
lingual content synchronization scenario proposed
in (Mehdad et. al., 2011) and (Negri et. al., 2011).
The task has interesting application scenarios that
can be investigated. Some of them are content syn-
chronization and cross language query alignment.
The task is defined by the organizers as follows:
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs.
Example for Spanish English pairs:
? bidirectional
Mozart naci en la ciudad de Salzburgo
Mozart was born in Salzburg.
? forward
Mozart naci en la ciudad de Salzburgo
Mozart was born on the 27th January 1756 in
Salzburg.
? backward
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born in 1756 in the city of Salzburg
? no entailment
Mozart naci el 27 de enero de 1756 en
Salzburgo
Mozart was born to Leopold and Anna Maria
Pertl Mozart.
2 Our Approach to CLTE
In our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Negri et. al., 2012) we have developed
an approach based on cross-language text similarity.
We have modified our cross-language query similar-
ity system TLike to handle longer texts.
Our approach is based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.
696
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
compute mutual information scores between candi-
date terms.
More technically, Random Indexing exploits an
algebraic model in order to represent the seman-
tics of terms in a Nth dimensional space (a vector
of length N); approaches falling into this category,
actually create a Terms By Contexts matrix where
each cell represents the degree of memberships of a
given term to the different contexts. The algorithm
assigns a random signature to each context (a highly
sparse vector of length N , with few, randomly cho-
sen, non-zero elements) and then generates the vec-
tor space model by performing a statistical analysis
of the documents in the domain corpus and by ac-
cumulating on terms rows all the signatures of the
contexts where terms appear.
According to this approach if two different terms
have a similar meaning they should appear in similar
contexts (within the same documents or surrounded
by the same words), resulting into close coordinates
in the so generated semantic space.
In our case study semantic vectors have been gen-
erated taking as corpus the set of metadata available
via the CACAO project (Cacao Project, 2007) fed-
eration (about 6 millions records). After processing
for each word in the corpus we have:
? A vector of float from 0 to 1 representing its
contextual meaning;
? A set of neighbors terms selected among the
terms with a higher semantic similarity, calcu-
lated as cosine distance among vectors.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The basic idea behind our algorithm is to detect
the probability for two texts to be one a translation
of the other. In the simple case we expect that if all
the words in text TS have a translation in text TT and
if TS and TT have the same number of terms, then
TS and TT are entailed. Things are of course more
complex than this, due to the following facts:
? The presence of compound words make the
constraints on cardinality of search terms not
feasible (e.g. the Italian Carta di Credito vs.
the German KreditCarte).
? One or more words in TS could be absent from
translation dictionaries.
? One or more words in TS could be present
in the translation dictionaries, but contextually
correct translation might be missing.
? There might be items which do not need to be
translated, notably Named Entities.
The first point, compounding, is only partially
an obstacle. NLP technology developed during
CACAO Project, which adopted translation dictio-
naries, deals with compound words both in terms
of identification and translation. Thus the Italian
?Carta di Credito? would be recognized and cor-
rectly translated into ?KreditCarte?. So, in an ideal
1http://lucene.apache.org
697
word, the cardinality principle could be considered
strict. In reality, however, there are many com-
pounding phenomena which are not covered by our
dictionaries, and this forces us to consider that a mis-
match in text term cardinality decrease the probabil-
ity that the two translations are translation of each
other, without necessarily setting it to zero.
Concerning the second aspect, the absence of
source (T1) words in translation dictionaries, it is
dealt with by accessing the semantic repository de-
scribed in the previous section. We first obtain the
list of neighbor terms for the untranslatable source
word. This list is likely to contain many words that
have one or more translations. For each translation,
again, we consult our semantic repository and we
obtain its semantic vector.
Finally, we compose all vectors of all available
translations and we search in the target text (T2) for
the word whose semantic vector best matches the
composed one (cosine distance). Of course we can-
not assume that the best matching vector is a transla-
tion of the original word, but we can use the distance
between the two vectors as a further weight for de-
ciding whether the two texts are translations one of
the other.
There are of course cases when the source word
is correctly missing in the source dictionary. This
is typically the case of most named entities, such
as geographical and person names. These entities
should be appropriately recognized and searched as
exact matches in the target text, thus by-passing any
dictionary look-up and any semantic based match-
ing. Notice that the recognition of named entities
it is not just a matter of generalizing the statement
according to which ?if something is not in the dic-
tionaries, it is a named entity?. Indeed there are well
known cases where the named entity is homograph
with common words (e.g. the French author ?La
Fontaine?), and in these cases we must detect them
in order to avoid the rejection of likely translation
pairs. In other words we must avoid that the two
texts ?La fontaine fables? and ?La Fontaine fav-
ole? are rejected as translation pairs, just by virtue
of the fact that ?La fontaine? is treated as a com-
mon word, thus generating the Italian translation?La
fontana?. Fortunately CACAO disposes of a quite
accurate subsystem for recognizing named entities
in texts, mixing standard NLP technologies with sta-
tistical processing and other corpus-oriented heuris-
tics.
We concentrated our work on handling cases
where two texts are candidates to be mutual trans-
lations, but one or more words receive a translation
which is not contained in the target text. Typically
these cases are a symptom of a non-optimal quality
in translation dictionaries: the lexicographer prob-
ably did not consider some translation candidate.
To address this problem we have created a solution
based on a weighting scheme. For each word of the
source language we assign a weight that reflects its
importance to the semantic interpretation of the text.
We define a matchweight of a word using the for-
mula represented in Figure 2.In this formula wis is
a word from the source text, wkt is a word from the
target text, w is a word in the source language and
trans is a boolean function that searches in the dic-
tionary for translations between two words.
The matchweight is relevant to the matching of a
translation of a word from the source with one of
the words of the target. If the system finds a direct
correspondence the weight is 0. If the match was
made using random indexing the weight is inverse
to the cosine similarity between the vectors.
In order to make an approximation of the signif-
icance of the word to the meaning of the phrase we
have used as its cost the inverse document frequency
(IDF) of the word calculated using Wikipedia as a
corpus. IDF is a most popular measure (a measure
commonly used in Information Retrieval) for calcu-
lating the importance of a word to a text. If N is the
number of documents in a text collection and Nw is
the number of documents of the collection that con-
tain w then the IDF of w is given by the formula:
weight(wis) = idf(w) = log(
N
Nw
) (2)
Using the matchweight and weight we define the
matchscore of a source target pair as:
matchscore(Ts, Tt) =
?
matchweigth(wis)
?
weight(wis)
(3)
If all the words of the source text have a transla-
tion in the target text the score is 0. If none is found
the score is 1. We have calculated the scores for each
698
matchweight(wis) =
?
??
??
0 ?wkt trans(wis) = wkt
w ? (wis) ? (1? d) ?w &wkt distance(wis, w) = d&trans(w) = wkt
w ? (wis) otherwise
(1)
Figure 1: Match Weight of a Word
pair taking t1 as a source and t2 as a target and vice
versa.
3 Systems
We have submitted four runs in the SemEval CLTE
challenge. We used the NaiveBayse algorithm im-
plemented in Mallet2 to create a classifier that will
produce the output for each of the four categories
Forward , Backward , Bidirectional and No Entail-
ment.
System 1 As our first system we have created a
binary classifier in the classical RTE (Bentivogli et.
al., 2011) classification (YES & NO) for each direc-
tion Forward and Backward. We assigned the Bidi-
rectional category if both classifiers returned YES.
As features the classifiers used only the match scores
obtained for the corresponding direction as one and
only numeric feature.
System 2 For the second system we trained a clas-
sifier using all four categories as output. Apart of the
scores obtained matching the texts in both directions
we have included also a set of eight simple surface
measures. Some of these are:
? The length of the two texts.
? The number of common words without transla-
tions.
? The cosine similarity between the tokens of the
two texts without translation.
? Levenshtein distance between the texts.
System 3 For the third system we trained a classi-
fier using all four categories as output. We used as
features scores obtained matching the texts in both
directions without the surface features used in the
System 2.
2http://mallet.cs.umass.edu/
System 4 In the last system we trained a classifier
using all four categories as output. We used as fea-
tures the simple surface measures used in System 2.
The results obtained are shown in Table 1.
4 Analysis
Analyzing the results of our participation we have
reached several important conclusions.
The dataset provided by the organizers presented
a significant challenge for our system which was
adapted from a query similarity approach. The re-
sults obtained demonstrate that only a similarity
based approach will not provide good results for this
task. This fact is also confirmed by the poor perfor-
mance of the simple similarity measures by them-
selves (System 4) and by their contribution to the
combined run (System 2).
The poor performance of our system can be par-
tially explained also by the small dimensions of the
cross-language dictionaries we used. Expanding
them with more words and phrases can potentially
increase our results.
The classifier with four categories clearly outper-
forms the two directional one (System 1 vs. System
3).
Overall we are not satisfied with our experi-
ment. A radically new approach is needed to address
the problem of Cross-Language Textual Entailment,
which our similarity based system could not model
correctly.
In the future we intend to integrate our approach
in our RTE open source system EDITS (Kouylekov
et. al., 2011) (Kouylekov and Negri, 2010) available
at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
699
SPA-ENG ITA-ENG FRA-ENG DEU-ENG
System 1 0.276 0.278 0.278 0.280
System 2 0.336 0.336 0.300 0.352
System 3 0.322 0.334 0.298 0.350
System 4 0.268 0.280 0.280 0.274
Table 1: Results obtained.
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Cacao Project CACAO - project supported by the eCon-
tentplus Programme of the European Commission.
http://www.cacaoproject.eu/
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
700
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 592?597, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Celi: EDITS and Generic Text Pair Classification
Milen Kouylekov
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
kouylekov@celi.it
Luca Dini
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
dini@celi.it
Alessio Bosca
Celi S.R.L.
via San Quintino 31
Torino,10121, Italy
alessio.bosca@celi.it
Marco Trevisan
Celi S.R.L.
via San Quintino 31
Torino, Italy
trevisan@celi.it
Abstract
This paper presents CELI?s participation in the
SemEval The Joint Student Response Anal-
ysis and 8th Recognizing Textual Entailment
Challenge (Task7) and Cross-lingual Textual
Entailment for Content Synchronization task
(Task 8).
1 Introduction
Recognizing an existing relation between two text
fragments received a significant interest as NLP task
in the recent years. A lot of the approaches were
focused in the filed of Textual Entailment(TE). TE
has been proposed as as a comprehensive frame-
work for applied semantics (Dagan and Glickman,
2004), where the need for an explicit mapping be-
tween linguistic objects can be, at least partially,
bypassed through the definition of semantic infer-
ences at the textual level. In the TE framework, a
text (T ) is said to entail the hypothesis (H) if the
meaning of H can be derived from the meaning of
T . Initially defined as binary relation between texts
(YES/NO there is an entailment or there is not) the
TE evolved in the third RTE3 (Giampiccolo et al,
2007) challenge into a set of three relations between
texts: ENTAILMENT, CONTRADICTION and
UNKNOWN. These relations are interpreted as fol-
lows:
? ENTAILMENT - The T entails the H .
? CONTRADICTION - The H contradicts the T
? UNKNOWN - There is no semantic connection
between T and H .
With more and more applications available for
recognizing textual entailment the researches fo-
cused their efforts in finding practical applications
for the developed systems. Thus the Cross-Lingual
Textual Entailment task (CLTE) was created using
textual entailment (TE) to define cross-lingual con-
tent synchronization scenario proposed in (Mehdad
et. al., 2011), (Negri et. al., 2011) (Negri et. al.,
2012). The task is defined by the organizers as fol-
lows: Given a pair of topically related text fragments
(T1 and T2) in different languages, the CLTE task
consists of automatically annotating it with one of
the following entailment judgments:
? Bidirectional: the two fragments entail each
other (semantic equivalence)
? Forward: unidirectional entailment from T1 to
T2
? Backward: unidirectional entailment from T2
to T1
? No Entailment: there is no entailment between
T1 and T2
The textual entailment competition also evolved.
In this year SEMEVAL The Joint Student Response
Analysis and 8th Recognizing Textual Entailment
Challenge - JRSA-RTE8 (Task7) the textual entail-
ment was defined in three subtasks:
5-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? Correct, if the student answer is a complete and
correct paraphrase of the reference answer;
592
? Partially correct incomplete, if the student an-
swer is a partially correct answer containing
some but not all information from the reference
answer;
? Contradictory, if the student answer explicitly
contradicts the reference answer;
? Irrelevant, if the student answer is ?irrelevant?,
talking about domain content but not providing
the necessary information;
? Non domain, if the student answer expresses a
request for help, frustration or lack of domain
knowledge - e.g., ?I don?t know?, ?as the book
says?, ?you are stupid?.
3-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? contradictory
? incorrect, conflating the categories of par-
tially correct incomplete, irrelevant or
non domain in the 5-way classification
2-way task , where the system is required to clas-
sify the student answer according to one of the fol-
lowing judgments:
? correct
? incorrect, conflating the categories of contra-
dictory and incorrect in the 3-way classifica-
tion.
Following the overall trend, we have decided to
convert our system for recognizing textual entail-
ment EDITS from a simple YES/NO recognition
system into a generic system capable of recognizing
multiple semantic relationships between two texts.
EDITS (Kouylekov and Negri, 2010) and
(Kouylekov et. al., 2011) is an open source pack-
age for recognizing textual entailment, which offers
a modular, flexible, and adaptable working environ-
ment to experiment with the RTE task over different
datasets. The package allows to: i) create an en-
tailment engine by defining its basic components ii)
train such entailment engine over an annotated RTE
corpus to learn a model; and iii) use the entailment
engine and the model to assign an entailment judg-
ments and a confidence score to each pair of an un-
annotated test corpus.
We define the recognition of semantic relations
between two texts as a classification task. In this
task the system takes as an input two texts and clas-
sifies them in one of a set of predefined relations.
We have modified EDITS in order to handle the so
defined task.
Having this in mind we have participated in
JRSA-RTE8 (task 7) and CLTE2 (task 8) with the
same approach. We have merged EDITS with some
features from the TLike system described in our last
participation in CLTE (Kouylekov et. al., 2011). For
each of the tasks we have created a specialized com-
ponents that are integrated in EDITS as one of the
system?s modules.
2 EDITS and Generic Text Pair
Classification
As in the previous versions, the core of EDITS im-
plements a distance-based framework. Within this
framework the system implements and harmonizes
different approaches to distance computation be-
tween texts, providing both edit distance algorithms,
and similarity algorithms. Each algorithm returns
a normalized distance score (a number between 0
and 1). Each algorithm algorithm depends on two
generic modules defined by the system?s user:
? Matcher - a module that is used to align text
fragments. This module uses semantic tech-
niques and entailment rules to find equivalent
textfragments.
? Weight Calculator - a module that is used to
give weight to text fragments. The weights are
used to determine the importance of a text por-
tion to the overall meaning of the text.
In the previous versions of the system at the train-
ing stage, distance scores calculated over annotated
T-H pairs are used to estimate a threshold that best
separates positive (YES) from negative (NO) exam-
ples. The calculated threshold was used at a test
stage to assign an entailment judgment and a con-
fidence score to each test pair. In the new version
593
of the system we used a machine learning classifier
to classify the T-H pairs in the appropriate category.
The overall architecture of the system is shown in
Figure 1.
The new architecture is divided in two sets of
modules: Machine Learning and Edit Distance. In
the Edit Distance set various distance algorithms are
used to calculate the distance between the two texts.
Each of these algorithms have a custom matcher and
weight calculator. The distances calculated by each
of these algorithms are used as features for the clas-
sifiers of the Machine Leaning modules. The ma-
chine learning modules are structured in two levels:
? Binary Classifiers - for each semantic relation
we create a binary classifier that distinguishes
between the members of the relation and the
members of the other relations. For example:
For 3way task (Task 7) the system created 3 bi-
nary classifiers one for each relation.
? Classifier - a module that makes final decision
for the text pair taking the output (decision and
confidence) of the binary classifiers as an input.
We have experimented with other configurations
of the machine leaning modules and selected this
one as the best performing on the available datasets
of the previous RTE competitions. In the version
of EDITS avalble online other configurations of the
machine leaning modules will be available using the
flexibility of the system configuration.
We have used the algorithms implemented in
WEKA (Hall et al, 2009) for the classification mod-
ules. The binary modules use SMO algorithm. The
top classifier uses NaiveBayes.
The input to the system is a corpus of text pairs
each classified with one semantic relation. We have
used the format of the previous RTE competitions
in order to be compliant. The goal of the system is
to create classifier that is capable of recognizing the
correct relation for an un-annotated pair of texts.
The new version of EDITS package allows to:
? Create an Classifier by defining its basic com-
ponents (i.e. algorithms, matchers, and weight
calculators);
? Train such Classifier over an annotated corpus
(containing T-H pairs annotated in terms of en-
tailment) to learn a Model;
? Use the Classifier and the Model to assign an
entailment judgment and a confidence score to
each pair of an un-annotated test corpus.
3 Resources
Like our participation in the 2012 SemEval Cross-
lingual Textual Entailment for Content Synchroniza-
tion task (Kouylekov et. al., 2011), our approach is
based on four main resources:
? A system for Natural Language Processing able
to perform for each relevant language basic
tasks such as part of speech disambiguation,
lemmatization and named entity recognition.
? A set of word based bilingual translation mod-
ules.(Employed only for Task 8)
? A semantic component able to associate a se-
mantic vectorial representation to words.
? We use Wikipedia as multilingual corpus.
NLP modules are described in (Bosca and Dini,
2008), and will be no further detailed here.
Word-based translation modules are composed by
a bilingual lexicon look-up component coupled with
a vector based translation filter, such as the one de-
scribed in (Curtoni and Dini, 2008). In the context of
the present experiments, such a filters has been de-
activated, which means that for any input word the
component will return the set of all possible transla-
tions. For unavailable pairs, we make use of trian-
gular translation (Kraaij, 2003).
As for the semantic component we experimented
with a corpus-based distributional approach capable
of detecting the interrelation between different terms
in a corpus; the strategy we adopted is similar to La-
tent Semantic Analysis (Deerwester et. al., 1990)
although it uses a less expensive computational solu-
tion based on the Random Projection algorithm (Lin
et. al., 2003) and (Bingham et. al., 2001). Different
works debate on similar issues: (Turney, 2001) uses
LSA in order to solve synonymy detection questions
from the well-known TOEFL test while the method
presented by (Inkpen, 2001) or by (Baroni and Bisi,
2001) proposes the use of the Web as a corpus to
594
Figure 1: EDITS Architecture
compute mutual information scores between candi-
date terms.
We use Wikipedia as a corpus for calculating
word statistics in different languages. We have in-
dexed using Lucene1 the English, Italian, French,
German, Spanish distributions of the resource.
The semantic component and the translation2
modules are used as core components in the matcher
module. IDF calculated on Wikipedia is used as
weight for the words by the weight calculator model.
4 JRSA-RTE8
In the JRSA-RTE8 we consider the reference an-
swers as T (text) and the student answer as H (hy-
pothesis). As the reference answers are often more
than one, we considered as input to the machine
learning algorithms the distance between the student
answer and the closest reference answer. We define
the closest reference answer as the reference answer
with minimum distance according to the distance al-
gorithm.
1http://lucene.apache.org
2Translation module is used only for Task 8.
4.1 Systems
We have submitted two runs in the SemEval JRSA-
RTE8 challenge (Task 7). The systems were exe-
cuted on each of the sub tasks of the main task.
System 1 The distance algorithm used in the first
system is Word Overlap. The algorithm tries to find
the words of a source text between the words of the
target text. We have created two features for each
binary classifier: 1) Feature 1 - word overlap of H
into T (words of H are matched by the words in T;
2) Feature 2 - word overlap T into H (Words of T are
matched by the words in H).
System 2 In the second system the we have used
only Feature 1.
We have created separate models for the Beatle
dataset and the sciEntsBank dataset. The results ob-
tained are shown in Table 1.
4.2 Analysis
The results obtained are in line with our previous
participations in the RTE challenges (Kouylekov et.
al., 2011). Of course as we described before in our
papers (Kouylekov et. al., 2011) the potential of the
edit distance algorithm is limited. Still it provides a
595
Task Beatle Q Beatle A sciEntsBank Q sciEntsBank A sciEntsBank D
2way
run 1 0.6400 0.6570 0.5930 0.6280 0.6160
run 2 0.4620 0.4480 0.5560 0.5930 0.5710
3way
run 1 0.5510 0.4950 0.5240 0.5780 0.5490
run 2 0.4150 0.4400 0.4390 0.5030 0.4770
5way
run 1 0.4830 0.4470 0.4130 0.4340 0.4170
run 2 0.3850 0.4320 0.2330 0.2370 0.2540
Table 1: Task 7 Results obtained. (Accuracy)
good performance and provides a solid potential for
some close domain tasks as described in (Negri and
Kouylekov, 2009). We were quite content with the
new machine learning based core. The selected con-
figuration performed in an acceptable manner. The
results obtained were in line with the cross accuracy
obtained by our system on the training set which
shows that it is not susceptible to over-training.
5 CLTE
5.1 Systems
We have submitted two runs in the CLTE task (Task
8).
System 1 The distance algorithm used in the first
system is Word Overlap as we did for task 7. We
have created two features for each binary classifier:
1) Feature 1 - word overlap of H into T (words of H
are matched by the words in T; 2) Feature 2 - word
overlap T into H (Words of T are matched by the
words in H).
System 2 In the second system we have made a
slight modification of the matcher that handled num-
bers.
The matcher module for this task used the transla-
tion modules defined in Section 3. We have created
a model for each language pair.
The results obtained are shown in Table 2.
5.2 Analysis
The results obtained are quite disappointing. Our
system obtained on the test set of the last CLTE com-
petition (CLTE1) quite satisfactory results (clte1-
test). All the results obtained for this competition
are near or above the medium of the best systems.
Our algorithm did not show signs of over-training
(the accuracy of the system on the test and on the
training of CLTE1 were almost equal). Having this
in mind we expected to obtain scores at least in the
margins of 0.45 to 0.5. This does not happen ac-
cording us due to the fact that this year dataset has
characteristics quite different than the last year. To
test this hypothesis we have trained our system on
half of the dataset (clte2-half-training) ,given for test
this year, and test it on the rest (clte-half-test). The
results obtained demonstrate that the dataset given
is more difficult for our system than the last years
one. The results also prove that our system is prob-
ably too conservative when learning from examples.
If the test set is similar to the training it performs
in consistent manner on both, otherwise it demon-
strates severe over-training problems.
6 Conclusions
In this paper we have presented a generic system for
text pair classification. This system was evaluated
on task 7 and task 8 of Semeval 2013 and obtained
satisfactory results. The new machine learning mod-
ule of the system needs improvement and we plan to
focus our future efforts in it.
We plan to release the newly developed system as
version 4 of the open source package EDITS avail-
able at http://edits.sf.net.
Acknowledgments
This work has been partially supported by the
ECfunded project Galateas (CIP-ICT PSP-2009-3-
250430).
596
Run Spanish Italian French German
run1 0.34 0.324 0.346 0.349
run2 0.342 0.324 0.34 0.349
clte2-half-training 0.41 0.43 0.40 0.44
clte2-half-test 0.43 0.44 0.41 0.43
clte1-test 0.52 0.51 0.54 0.55
Table 2: Task 8. Results obtained. (Accuracy)
References
Baroni M., Bisi S. 2004. Using cooccurrence statistics
and the web to discover synonyms in technical lan-
guage In Proceedings of LREC 2004
Bentivogli L., Clark P., Dagan I., Dang H, Giampic-
colo D. 2011. The Seventh PASCAL Recognizing
Textual Entailment Challenge In Proceedings of TAC
2011
Bingham E., Mannila H. 2001. Random projection in
dimensionality reduction: Applications to image and
text data. In Knowledge Discovery and Data Mining,
ACM Press pages 245250
Bosca A., Dini L. 2008. Query expansion via library
classification system. In CLEF 2008. Springer Verlag,
LNCS
Curtoni P., Dini L. 2006. Celi participation at clef 2006
Cross language delegated search. In CLEF2006 Work-
ing notes.
Dagan I. and Glickman O. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. Learning Methods for Text Understanding
and Mining Workshop.
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K.,
Harshman R. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science 41 391407
Giampiccolo; Bernardo Magnini; Ido Dagan; Bill Dolan.
2007. The Third PASCAL Recognizing Textual
Entailment Challenge. Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing. June 2007, Prague, Czech Republic
Hall M., Frank E., Holmes G., Pfahringer B., Reute-
mann P., Witten I. 2009 The WEKA Data Mining
Software: An Update; SIGKDD Explorations, Vol-
ume 11, Issue 1.
Inkpen D. 2007. A statistical model for near-synonym
choice. ACM Trans. Speech Language Processing
4(1)
Kouylekov M., Negri M. An Open-Source Package for
Recognizing Textual Entailment. 48th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2010) ,Uppsala, Sweden. July 11-16, 2010
Kouylekov M., Bosca A., Dini L. 2011. EDITS 3.0 at
RTE-7. Proceedings of the Seventh Recognizing Tex-
tual Entailment Challenge (2011).
Kouylekov M., Bosca A., Dini L., Trevisan M. 2012.
CELI: An Experiment with Cross Language Textual
Entailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Kouylekov M., Mehdad Y. and Negri M. 2011 Is it Worth
Submitting this Run? Assess your RTE System with a
Good Sparring Partner Proceedings of the TextInfer
2011 Workshop on Textual Entailment
Kraaij W. 2003. Exploring transitive translation meth-
ods. In Vries, A.P.D., ed.: Proceedings of DIR 2003.
Lin J., Gunopulos D. 2003. Dimensionality reduction
by random projection and latent semantic indexing. In
proceedings of the Text Mining Workshop, at the 3rd
SIAM International Conference on Data Mining.
Mehdad Y.,Negri M., Federico M.. 2011. Using Paral-
lel Corpora for Cross-lingual Textual Entailment. In
Proceedings of ACL-HLT 2011.
Negri M., Bentivogli L., Mehdad Y., Giampiccolo D.,
Marchetti A. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. In Proceedings of EMNLP 2011.
Negri M., Kouylekov M., 2009 Question Answer-
ing over Structured Data: an Entailment-Based Ap-
proach to Question Analysis. RANLP 2009 - Re-
cent Advances in Natural Language Processing, 2009
Borovets, Bulgaria
Negri M., Marchetti A., Mehdad Y., Bentivogli L., Gi-
ampiccolo D. Semeval-2012 Task 8: Cross-lingual
Textual Entailment for Content Synchronization. In
Proceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012). 2012.
Turney P.D. 2001. Mining the web for synonyms: Pmi-
ir versus lsa on toefl. In EMCL 01: Proceedings of
the 12th European Conference on Machine Learning,
London, UK, Springer-Verlag pages 491502
597
