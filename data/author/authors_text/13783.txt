Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912?920,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multi-document Summarization via
Budgeted Maximization of Submodular Functions
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We treat the text summarization problem as
maximizing a submodular function under a
budget constraint. We show, both theoretically
and empirically, a modified greedy algorithm
can efficiently solve the budgeted submodu-
lar maximization problem near-optimally, and
we derive new approximation bounds in do-
ing so. Experiments on DUC?04 task show
that our approach is superior to the best-
performing method from the DUC?04 evalu-
ation on ROUGE-1 scores.
1 Introduction
Automatically generating summaries from large text
corpora has long been studied in both information
retrieval and natural language processing. There
are several types of text summarization tasks. For
example, if an input query is given, the generated
summary can be query-specific, and otherwise it is
generic. Also, the number of documents to be sum-
marized can vary from one to many. The constituent
sentences of a summary, moreover, might be formed
in a variety of different ways ? summarization can
be conducted using either extraction or abstraction,
the former selects only sentences from the origi-
nal document set, whereas the latter involves natu-
ral language generation. In this paper, we address
the problem of generic extractive summaries from
clusters of related documents, commonly known as
multi-document summarization.
In extractive text summarization, textual units
(e.g., sentences) from a document set are extracted
to form a summary, where grammaticality is as-
sured at the local level. Finding the optimal sum-
mary can be viewed as a combinatorial optimiza-
tion problem which is NP-hard to solve (McDon-
ald, 2007). One of the standard methods for
this problem is called Maximum Marginal Rele-
vance (MMR) (Dang, 2005)(Carbonell and Gold-
stein, 1998), where a greedy algorithm selects the
most relevant sentences, and at the same time avoids
redundancy by removing sentences that are too sim-
ilar to already selected ones. One major problem
of MMR is that it is non-optimal because the deci-
sion is made based on the scores at the current it-
eration. McDonald (2007) proposed to replace the
greedy search of MMR with a globally optimal for-
mulation, where the basic MMR framework can be
expressed as a knapsack packing problem, and an
integer linear program (ILP) solver can be used to
maximize the resulting objective function. ILP Al-
gorithms, however, can sometimes either be expen-
sive for large scale problems or themselves might
only be heuristic without associated theoretical ap-
proximation guarantees.
In this paper, we study graph-based approaches
for multi-document summarization. Indeed, several
graph-based methods have been proposed for extrac-
tive summarization in the past. Erkan and Radev
(2004) introduced a stochastic graph-based method,
LexRank, for computing the relative importance of
textual units for multi-document summarization. In
LexRank the importance of sentences is computed
based on the concept of eigenvector centrality in
the graph representation of sentences. Mihalcea and
Tarau also proposed an eigenvector centrality algo-
rithm on weighted graphs for document summariza-
tion (Mihalcea and Tarau, 2004). Mihalcea et al
later applied Google?s PageRank (Brin and Page,
1998) to natural language processing tasks ranging
912
from automatic keyphrase extraction and word sense
disambiguation, to extractive summarization (Mi-
halcea et al, 2004; Mihalcea, 2004). Recent work
in (Lin et al, 2009) presents a graph-based approach
where an undirected weighted graph is built for the
document to be summarized, and vertices represent
the candidate sentences and edge weights represent
the similarity between sentences. The summary ex-
traction procedure is done by maximizing a submod-
ular set function under a cardinality constraint.
Inspired by (Lin et al, 2009), we perform summa-
rization by maximizing submodular functions under
a budget constraint. A budget constraint is natural
in summarization task as the length of the summary
is often restricted. The length (byte budget) limita-
tion represents the real world scenario where sum-
maries are displayed using only limited computer
screen real estate. In practice, the candidate tex-
tual/linguistic units might not have identical costs
(e.g., sentence lengths vary). Since a cardinality
constraint is a special case (a budget constraint with
unity costs), our approach is more general than (Lin
et al, 2009). Moreover, we propose a modified
greedy algorithm (Section 4) and both theoretically
(Section 4.1) and empirically (Section 5.1) show that
the algorithm solves the problem near-optimally,
thanks to submodularity. Regarding summarization
performance, experiments on DUC?04 task show
that our approach is superior to the best-performing
method in DUC?04 evaluation on ROUGE-1 scores
(Section 5).
2 Background on Submodularity
Consider a set function f : 2V ? R, which maps
subsets S ? V of a finite ground set V to real num-
bers. f(?) is called normalized if f(?) = 0, and
is monotone if f(S) ? f(T ) whenever S ? T .
f(?) is called submodular (Lovasz, 1983) if for any
S, T ? V , we have
f(S ? T ) + f(S ? T ) ? f(S) + f(T ). (1)
An equivalent definition of submodularity is the
property of diminishing returns, well-known in the
field of economics. That is, f(?) is submodular if for
any R ? S ? V and s ? V \ S,
f(S ? {s})? f(S) ? f(R ? {s})? f(R). (2)
Eqn. 2 states that the ?value? of s never increases
in the contexts of ever larger sets, exactly the prop-
erty of diminishing returns. This phenomenon arises
naturally in many other contexts as well. For ex-
ample, the Shannon entropy function is submodu-
lar in the set of random variables. Submodular-
ity, moreover, is a discrete analog of convexity (Lo-
vasz, 1983). As convexity makes continuous func-
tions more amenable to optimization, submodular-
ity plays an essential role in combinatorial optimiza-
tion.
Many combinatorial optimization problems can
be solved optimally or near-optimally in polynomial
time only when the underlying function is submod-
ular. It has been shown that any submodular func-
tion can be minimized in polynomial time (Schri-
jver, 2000)(Iwata et al, 2001). Maximization of sub-
modular functions, however, is an NP-complete op-
timization problem but fortunately, some submod-
ular maximization problems can be solved near-
optimally. A famous result is that the maximization
of a monotone submodular function under a cardi-
nality constraint can be solved using a greedy al-
gorithm (Nemhauser et al, 1978) within a constant
factor (0.63) of being optimal. A constant-factor ap-
proximation algorithm has also been obtained for
maximizing monotone submodular function with a
knapsack constraint (see Section 4.2). Feige et.al.
(2007) studied unconstrained maximization of a ar-
bitrary submodular functions (not necessarily mono-
tone). Kawahara et.al. (2009) proposed a cutting-
plane method for optimally maximizing a submod-
ular set function under a cardinality constraint, and
Lee et.al. (2009) studied non-monotone submodu-
lar maximization under matroid and knapsack con-
straints.
3 Problem Setup
In this paper, we study the problem of maximizing a
submodular function under budget constraint, stated
formally below:
max
S?V
{
f(S) :
?
i?S
ci ? B
}
(3)
where V is the ground set of all linguistic units (e.g.,
sentences) in the document, S is the extracted sum-
mary (a subset of V ), ci is the non-negative cost of
913
selecting unit i and B is our budget, and submodular
function f(?) scores the summary quality.
The budgeted constraint arises naturally since of-
ten the summary must be length limited as men-
tioned above. In particular, the budget B could be
the maximum number of words allowed in any sum-
mary, or alternatively the maximum number of bytes
of any summary, where ci would then be either num-
ber of words or the number of bytes in sentence i.
To benefit from submodular optimization, the
objective function measuring the summary quality
must be submodular. In general, there are two ways
to apply submodular optimization to any application
domain. One way is to force submodularity on an
application, leading to an artificial and poorly per-
forming objective function even if it can be opti-
mized well. The alternative is to address applica-
tions where submodularity naturally applies. We are
fortunate in that, like convexity in the continuous do-
main, submodularity seems to arise naturally in a va-
riety of discrete domains, and as we will see below,
extractive summarization is one of them. As men-
tioned in Section 1, our approach is graph-based,
not only because a graph is a natural representation
of the relationships and interactions between textual
units, but also because many submodular functions
are well defined on a graph and can naturally be used
in measuring the summary quality.
Suppose certain pairs (i, j) with i, j ? V are sim-
ilar and the similarity of i and j is measured by a
non-negative value wi,j . We can represent the en-
tire document with a weighted graph (V,E), with
non-negative weights wi,j associated with each edge
ei,j , e ? E. One well-known graph-based submod-
ular function that measures the similarity of S to the
remainder V \ S is the graph-cut function:
fcut(S) =
?
i?V \S
?
j?S
wi,j . (4)
In multi-document summarization, redundancy is a
particularly important issue since textual units from
different documents might convey the same infor-
mation. A high quality (small and meaningful) sum-
mary should not only be informative about the re-
mainder but also be compact (non-redundant). Typ-
ically, this goal is expressed as a combination of
maximizing the information coverage and minimiz-
ing the redundancy (as used in MMR (Carbonell and
Goldstein, 1998)). Inspired by this, we use the fol-
lowing objective by combining a ?-weighted penalty
term with the graph cut function:
fMMR(S) =
?
i?V \S
?
j?S
wi,j??
?
i,j?S:i?=j
wi,j , ? ? 0.
Luckily, this function is still submodular as both the
graph cut function and the redundancy term are sub-
modular. Neither objective, however, is monotone,
something we address in Theorem 3. Although sim-
ilar to the MMR objective function, our approach is
different since 1) ours is graph-based and 2) we for-
malize the problem as submodular function maxi-
mization under the budget constraint where a simple
greedy algorithm can solve the problem guaranteed
near-optimally.
4 Algorithms
Algorithm 1 Modified greedy algorithm
1: G? ?
2: U ? V
3: while U ?= ? do
4: k ? argmax??U f(G?{?})?f(G)(c?)r
5: G ? G ? {k} if
?
i?G ci + ck ? B and
f(G ? {k})? f(G) ? 0
6: U ? U \ {k}
7: end while
8: v? ? argmaxv?V,cv?B f({v})
9: return Gf = argmaxS?{{v?},G} f(S)
Inspired by (Khuller et al, 1999), we propose
Algorithm 1 to solve Eqn. (3). The algorithm se-
quentially finds unit k with the largest ratio of ob-
jective function gain to scaled cost, i.e., (f(G ?
{?})? f(G))/cr? , where r > 0 is the scaling factor.
If adding k increases the objective function value
while not violating the budget constraint, it is then
selected and otherwise bypassed. After the sequen-
tial selection, setG is compared to the within-budget
singleton with the largest objective value, and the
larger of the two becomes the final output.
The essential aspect of a greedy algorithm is
the design of the greedy heuristic. As discussed
in (Khuller et al, 1999), a heuristic that greedily se-
lects the k that maximizes (f(G?{k})? f(G))/ck
has an unbounded approximation factor. For ex-
ample, let V = {a, b}, f({a}) = 1, f({b}) = p,
914
ca = 1, cb = p + 1, and B = p + 1. The solution
obtained by the greedy heuristic is {a} with objec-
tive function value 1, while the true optimal objec-
tive function value is p. The approximation factor
for this example is then p and therefore unbounded.
We address this issue by the following two mod-
ifications to the naive greedy algorithms. The first
one is the final step (line 8 and 9) in Algorithm 1
where set G and singletons are compared. This step
ensures that we could obtain a constant approxima-
tion factor for r = 1 (see the proof in the Appendix).
The second modification is that we introduce a
scaling factor r to adjust the scale of the cost. Sup-
pose, in the above example, we scale the cost as
ca = 1r, cb = (p+1)r, then selecting a or b depends
also on the scale r, and we might get the optimal so-
lution using a appropriate r. Indeed, the objective
function values and the costs might be uncalibrated
since they might measure different units. E.g., it is
hard to say if selecting a sentence of 15 words with
an objective function gain of 2 is better than select-
ing sentence of 10 words with gain of 1. Scaling
can potentially alleviate this mismatch (i.e., we can
adjust r on development set). Interestingly, our the-
oretical analysis of the performance guarantee of the
algorithm also gives us guidance about how to scale
the cost for a particular problem (see Section 4.1).
4.1 Analysis of performance guarantee
Although Algorithm 1 is essentially a simple greedy
strategy, we show that it solves Eqn. (3) globally and
near-optimally, by exploiting the structure of sub-
modularity. As far as we know, this is a new result
for submodular optimization, not previously stated
or published before.
Theorem 1. For normalized monotone submodular
function f(?), Algorithm 1 with r = 1 has a constant
approximation factor as follows:
f(Gf ) ?
(
1? e?
1
2
)
f(S?), (5)
where S? is an optimal solution.
Proof. See Appendix.
Note that an ?-approximation algorithm for an
optimization problem is a polynomial-time algo-
rithm that for all instances of the problem produces
a solution whose value is within a factor of ? of the
value of the an optimal solution. So Theorem 1 ba-
sically states that the solution found by Algorithm 1
can be at least as good as (1 ? 1/
?
e)f(S?) ?
0.39f(S?) even in the worst case. A constant ap-
proximation bound is good since it is true for all in-
stances of the problem, and we always know how
good the algorithm is guaranteed to be without any
extra computation. For r ?= 1, we resort to instance-
dependent bound where the approximation can be
easily computed per problem instance.
Theorem 2. With normalized monotone submodu-
lar f(?), for i = 1, . . . , |G|, let vi be the ith unit
added intoG andGi is the set after adding vi. When
0 ? r ? 1,
f(Gi) ?
(
1?
i
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?)
(6)
?
(
1?
i
?
k=1
(
1?
crvk
Br|V |1?r
)
)
f(S?) (7)
and when r ? 1,
f(Gi) ?
(
1?
i
?
k=1
(
1?
(cvk
B
)r)
)
f(S?). (8)
Proof. See Appendix.
Theorem 2 gives bounds for a specific instance of
the problem. Eqn. (6) requires the size |S?|, which
is unknown, requiring us to estimate an upper bound
of the cardinality of the optimal set S?. Obviously,
|S?| ? |V |, giving us Eqn. (7). A tighter upper
bound is obtained, however, by sorting the costs.
That is, let c[1], c[2], . . . , c[|V |] be the sorted sequence
of costs in nondecreasing order, giving |S?| < m
where
?m?1
k=1 c[i] ? B and
?m
k=1 c[i] > B. In this
case, the computation cost for the bound estimation
is O(|V | log |V |), which is quite feasible.
Note that both Theorem 1 and 2 are for mono-
tone submodular functions while our practical ob-
jective function, i.e. fMMR, is not guaranteed every-
where monotone. However, our theoretical results
still holds for fMMR with high probability in prac-
tice. Intuitively, in summarization tasks, the sum-
mary is usually small compared to the ground set
size (|S| ? |V |). When |S| is small, fMMR is
915
monotone and our theoretical results still hold. Pre-
cisely, assume that all edge weights are bounded:
wi,j ? [0, 1] (which is the case for cosine simi-
larity between non-negative vectors). Also assume
that edges weights are independently identically dis-
tributed with mean ?, i.e. E(wi,j) = ?. Given a
budget B, assume the maximum possible size of a
solution is K. Let ? = 2? + 1, and ? = 2K ? 1.
Notice that ? ? |V | for our summarization task. We
have the following theorem:
Theorem 3. Algorithm 1 solves the summarization
problem near-optimally (i.e. Theorem 1 and Theo-
rem 2 hold) with high probability of at least
1? exp
{
?2(|V | ? (? + 1)?)
2?2
|V |+ (?2 ? 1)?
+ lnK
}
Proof. Omitted due to space limitation.
4.2 Related work
Algorithms for maximizing submodular function
under budget constraint (Eqn. (3)) have been stud-
ied before. Krause (2005) generalized the work by
Khuller et al(1999) on budgeted maximum cover
problem to the submodular framework, and showed
a 12(1 ? 1/e)-approximation algorithm. The algo-
rithm in (Krause and Guestrin, 2005) and (Khuller
et al, 1999) is actually a special case of Algorithm 1
when r = 1, and Theorem 1 gives a better bound
(i.e., (1? 1/
?
e) > 12(1? 1/e)) in this case. There
is also a greedy algorithm with partial enumerations
(Sviridenko, 2004; Krause and Guestrin, 2005) fac-
tor (1? 1/e). This algorithm, however, is too com-
putationally expensive and thus not practical for real
world applications (the computation cost is O(|V |5)
in general). When each unit has identical cost, the
budget constraint reduces to cardinality constraint
where a greedy algorithm is known to be a (1?1/e)-
approximation algorithm (Nemhauser et al, 1978)
which is the best that can be achieved in polyno-
mial time (Feige, 1998) if P ?= NP. Recent work
(Takamura and Okumura, 2009) applied the maxi-
mum coverage problem to text summarization (with-
out apparently being aware that their objective is
submodular) and studied a similar algorithm to ours
when r = 1 and for the non-penalized graph-cut
function. This problem, however, is a special case
of constrained submodular function maximization.
5 Experiments
We evaluated our approach on the data set of
DUC?04 (2004) with the setting of task 2, which
is a multi-document summarization task on English
news articles. In this task, 50 document clusters
are given, each of which consists of 10 documents.
For each document cluster, a short multi-document
summary is to be generated. The summary should
not be longer than 665 bytes including spaces and
punctuation, as required in the DUC?04 evaluation.
We used DUC?03 as our development set. All docu-
ments were segmented into sentences using a script
distributed by DUC. ROUGE version 1.5.5 (Lin,
2004), which is widely used in the study of summa-
rization, was used to evaluate summarization perfor-
mance in our experiments 1. We focus on ROUGE-
1 (unigram) F-measure scores since it has demon-
strated strong correlation with human annotation
(Lin, 2004).
The basic textual/linguistic units we consider in
our experiments are sentences. For each document
cluster, sentences in all the documents of this cluster
forms the ground set V . We built semantic graphs
for each document cluster based on cosine similar-
ity, where cosine similarity is computed based on
the TF-IDF (term frequency, inverse document fre-
quency) vectors for the words in the sentences. The
cosine similarity measures the similarity between
sentences, i.e., wi,j .
Here the IDF values were calculated using all the
document clusters. The weighted graph was built
by connecting vertices (corresponding to sentences)
with weight wi,j > 0. Any unconnected vertex was
removed from the graph, which is equivalent to pre-
excluding certain sentences from the summary.
5.1 Comparison with exact solution
In this section, we empirically show that Algo-
rithm 1 works near-optimally in practice. To deter-
mine how much accuracy is lost due to approxima-
tions, we compared our approximation algorithms
with an exact solution. The exact solutions were ob-
tained by Integer Linear Programming (ILP). Solv-
ing arbitrary ILP is an NP-hard problem. If the size
of the problem is not too large, we can sometimes
find the exact solution within a manageable time
1Options used: -a -c 95 -b 665 -m -n 4 -w 1.2
916
using a branch-and-bound method. In our experi-
ments, MOSEK was used as our ILP solver.
We formalize Eqn. (3) as an ILP by introducing
indicator (binary) variables xi,j , yi,j , i ?= j and zi
for i, j ? V . In particular, zi = 1 indicates that
unit i is selected, i.e., i ? S, xi,j = 1 indicates that
i ? S but j /? S, and yi,j = 1 indicates both i and
j are selected. Adding constraints to ensure a valid
solution, we have the following ILP formulation for
Eqn. (3) with objective function fMMR(S):
max
?
i?=j,i,j?V
wi,jxi,j ? ?
?
i?=j,i,j?V
wi,jyi,j
subject to:
?
i?V
cizi ? B,
xi,j ? zi ? 0, xi,j + zj ? 1, zi ? zj ? xi,j ? 0,
yi,j ? zi ? 0, yi,j ? zj ? 0, zi + zj ? yi,j ? 1,
xi,j , yi,j , zi ? {0, 1},?i ?= j, i, j ? V
Note that the number of variables in the ILP for-
mulation is O(|V |2). For a document cluster with
hundreds of candidate textual units, the scale of the
problem easily grows involving tens of thousands
of variables, making the problem very expensive to
solve. For instance, solving the ILP exactly on a
document cluster with 182 sentences (as used in Fig-
ure 1) took about 17 hours while our Algorithm 1
finished in less than 0.01 seconds.
We tested both approximate and exact algorithms
on DUC?03 data where 60 document clusters were
used (30 TDT document clusters and 30 TREC doc-
ument clusters), each of which contains 10 docu-
ments on average. The true approximation factor
was computed by dividing the objective function
value found by Algorithm 1 over the optimal ob-
jective function value (found by ILP). The average
approximation factors over the 58 document clus-
ters (ILP on 2 of the 60 document clusters failed to
finish) are shown in Table 1, along with other statis-
tics. On average Algorithm 1 finds a solution that is
over 90% as good as the optimal solution for many
different r values, which backs up our claim that
the modified greedy algorithm solves the problem
near-optimally, even occasionally optimally (Figure
1 shows one such example).
The higher objective function value does not al-
ways indicate higher ROUGE-1 score. Indeed,
0
20
40
60
80
100
120
140
0 2 4 6 8 10 12
op mal
r=0
r=0.5
r=1
r=1.5
number of sentences in the summary
Ob
jec
tiv
e f
un
cti
on
 va
lue
exact solution
Figure 1: Application of Algorithm 1 when summariz-
ing document cluster d30001t in the DUC?04 dataset with
summary size limited to 665 bytes. The objective func-
tion was fMMR with ? = 2. The plots show the achieved
objective function as the number of selected sentences
grows. The plots stop when in each case adding more
sentences violates the budget. Algorithm 1 with r = 1
found the optimal solution exactly.
rather than directly optimizing ROUGE, we opti-
mize a surrogate submodular function that indicates
the quality of a summary. Optimality in the submod-
ular function does not necessary indicate optimality
in ROUGE score. Nevertheless, we will show that
our approach outperforms several other approaches
in terms of ROUGE. We note that ROUGE is itself
a surrogate for true human-judged summary quality,
it might possibly be that fMMR is a still better surro-
gate ? we do not consider this possibility further in
this work, however.
5.2 Summarization Results
We used DUC?03 (as above) for our development
set to investigate how r and ? relate to the ROUGE-
1 score. From Figure 2, the best performance is
achieved with r = 0.3, ? = 4. Using these settings,
we applied our approach to the DUC?04 task. The
results, along with the results of other approaches,
are shown in Table 2. All the results in Table 2 are
presented as ROUGE-1 F-measure scores. 2
We compared our approach to two other well-
2When the evaluation was done in 2004, ROUGEwas still in
revision 1.2.1, so we re-evaluated the DUC?04 submissions us-
ing ROUGE v1.5.5 and the numbers are slightly different from
the those reported officially.
917
Table 1: Comparison of Algorithm 1 to exact algorithms
on DUC?03 dataset. All the numbers shown in the ta-
ble are the average statistics (mean/std). The ?true? ap-
proximation factor is the ratio of objective function value
found by Algorithm 1 over the ILP-derived true-optimal
objective value, and the approximation bounds were esti-
mated using Theorem 2.
Approx. factor ROUGE-1
true bound (%)
exact 1.00 - 33.60/5.05
r = 0.0 0.65/0.15 ?0.19/0.08 33.50/5.94
r = 0.1 0.71/0.15 ?0.24/0.08 33.68/6.03
r = 0.3 0.88/0.11 ?0.37/0.06 34.77/5.49
r = 0.5 0.96/0.04 ?0.48/0.05 34.33/5.94
r = 0.7 0.98/0.02 ?0.56/0.05 34.08/5.41
r = 1.0 0.98/0.02 ?0.65/0.04 33.32/5.14
r = 1.2 0.97/0.02 ?0.48/0.05 32.54/4.69
32.0%
32.5%
33.0%
33.5%
34.0%
34.5%
35.0%
0 5 10 15
r=0
r=0.3
r=0.5
r=0.7
r=1
RO
UG
E-1
 F-
me
as
ur
e
Figure 2: Different combinations of r and ? for fMMR
related to ROUGE-1 score on DUC?03 task 1.
known graph-based approaches, LexRank and
PageRank. LexRank was one of the participat-
ing system in DUC?04, with peer code 104. For
PageRank, we implemented the recursive graph-
based ranking algorithm ourselves. The importance
of sentences was estimated in an iterative way as
in (Brin and Page, 1998)(Mihalcea et al, 2004).
Sentences were then selected based on their impor-
tance rankings until the budget constraint was vi-
olated. The graphs used for PageRank were ex-
actly the graphs in our submodular approaches (i.e.,
an undirected graph). In both cases, submodu-
lar summarization achieves better ROUGE-1 scores.
The improvement is statistically significant by the
Wilcoxon signed rank test at level p < 0.05. Our
approach also outperforms the best system (Conroy
et al, 2004), peer code 65 in the DUC?04 evalua-
tion although not as significant (p < 0.08). The rea-
son might be that DUC?03 is a poor representation
of DUC?04 ? indeed, by varying r and ? over the
ranges 0 ? r ? 0.2 and 5 ? ? ? 9 respectively, the
DUC?04 ROUGE-1 scores were all > 38.8% with
the best DUC?04 score being 39.3%.
Table 2: ROUGE-1 F-measure results (%)
Method ROUGE-1 score
peer65 (best system in DUC04) 37.94
peer104 (LexRank) 37.12
PageRank 35.37
Submodular (r = 0.3, ? = 4) 38.39
6 Appendix
We analyze the performance guarantee of Algorithm 1.
We use the following notation: S? is the optimal solu-
tion; Gf is the final solution obtained by Algorithm 1;
G is the solution obtained by the greedy heuristic (line
1 to 7 in Algorithm 1); vi is the ith unit added to G,
i = 1, . . . , |G|;Gi is the set obtained by greedy algorithm
after adding vi (i.e., Gi = ?ik=1{vk}, for i = 1, . . . , |G|,
with G0 = ? and G|G| = G); f(?) : 2V ? R is a
monotone submodular function; and ?k(S) is the gain of
adding k to S, i.e., f(S ? {k})? f(S).
Lemma 1. ?X,Y ? V ,
f(X) ? f(Y ) +
?
k?X\Y
?k(Y ). (9)
Proof. See (Nemhauser et al, 1978)
Lemma 2. For i = 1, . . . , |G|, when 0 ? r ? 1,
f(S?)? f(Gi?1) ?
Br|S?|1?r
crvi
(f(Gi)? f(Gi?1)),
(10)
and when r ? 1,
f(S?)? f(Gi?1) ?
(
B
cvi
)r
(f(Gi)? f(Gi?1))
(11)
Proof. Based on line 4 of Algorithm 1, we have
?u ? S? \Gi?1,
?u(Gi?1)
cru
? ?vi(Gi?1)
crvi
.
918
Thus when 0 ? r ? 1,
?
u?S?\Gi?1
?u(Gi?1) ?
?vi(Gi?1)
crvi
?
u?S?\Gi?1
cru
? ?vi(Gi?1)
crvi
|S? \Gi?1|
(
?
u?S?\Gi?1 cu
|S? \Gi?1|
)r
? ?vi(Gi?1)
crvi
|S?|1?r
?
?
?
u?S?\Gi?1
cu
?
?
r
? ?vi(Gi?1)
crvi
|S?|1?rBr,
where the second inequality is due to the concavity of
g(x) = xr, x > 0, 0 ? r ? 1. The last inequality uses
the fact that
?
u?S? cu ? B. Similarly, when r ? 1,
?
u?S?\Gi?1
?u(Gi?1) ?
?vi(Gi?1)
crvi
?
u?S?\Gi?1
cru
? ?vi(Gi?1)
crvi
?
?
?
u?S?\Gi?1
cu
?
?
r
? ?vi(Gi?1)
crvi
Br.
Applying Lemma 1, i.e., let X = S? and Y = Gi?1, the
lemma immediately follows.
The following is a proof of Theorem 2.
Proof. Obviously, the theorem is true when i = 1 by
applying Lemma 2.
Assume that the theorem is true for i?1, 2 ? i ? |G|,
we show that it also holds for i. When 0 ? r ? 1,
f(Gi) = f(Gi?1) + (f(Gi)? f(Gi?1))
? f(Gi?1) +
crvi
Br|S?|1?r
(f(S?)? f(Gi?1))
=
(
1?
crvi
Br|S?|1?r
)
f(Gi?1) +
crvi
Br|S?|1?r
f(S?)
?
(
1?
crvi
Br|S?|1?r
)
(
1?
i?1
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?) +
crvi
Br|S?|1?r
f(S?)
=
(
1?
i
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?).
The case when r ? 1 can be proven similarly.
Now we are ready to prove Theorem 1.
Proof. Consider the following two cases:
Case 1: ?v ? V such that f({v}) > 12f(S
?). Then it
is guaranteed that f(Gf ) ? f({v})) > 12f(S
?) due line
9 of Algorithm 1.
Case 2: ?v ? V , we have f({v}) ? 12f(S
?). We
consider the following two sub-cases, namely Case 2.1
and Case 2.2:
Case 2.1: If
?
v?G cv ?
1
2B, then we know that
?v /? G, cv > 12B since otherwise we can add a v /? G
into G to increase the objective function value without
violating the budget constraint. This implies that there is
at most one unit in S? \ G since otherwise we will have
?
v?S? cv > B. By assumption, we have f(S? \ G) ?
1
2f(S
?). Submodularity of f(?) gives us:
f(S? \G) + f(S? ?G) ? f(S?),
which implies f(S? ?G) ? 12f(S
?). Thus we have
f(Gf ) ? f(G) ? f(S? ?G) ?
1
2
f(S?),
where the second inequality follows from monotonicity.
Case 2.2: If
?
v?G cv >
1
2B, for 0 ? r ? 1, using
Theorem 2, we have
f(G) ?
?
?1?
|G|
?
k=1
(
1?
crvk
Br|S?|1?r
)
?
? f(S?)
?
?
?1?
|G|
?
k=1
?
?1?
crvk |S
?|r?1
2r
(
?|G|
k=1 cvk
)r
?
?
?
? f(S?)
?
(
1?
(
1? |S
?|r?1
2r|G|r
)|G|)
f(S?)
?
(
1? e?
1
2
?
|S?|
2|G|
?r?1)
f(S?)
where the third inequality uses the fact (provable using
Lagrange multipliers) that for a1, . . . , an ? R+ such that
?n
i=1 ai = ?, function
1?
n
?
i=1
(
1? ?a
r
i
?r
)
achieves its minimum of 1 ? (1 ? ?/nr)n when a1 =
? ? ? = an = ?/n for?, ? > 0. The last inequality follows
from e?x ? 1? x.
In all cases, we have
f(Gf ) ? min
{
1
2
, 1? e?
1
2
?
|S?|
2|G|
?r?1}
f(S?)
In particular, when r = 1, we obtain the constant approx-
imation factor, i.e.
f(Gf ) ?
(
1? e? 12
)
f(S?)
919
Acknowledgments
This work is supported by an ONR MURI grant
(No. N000140510388), the Companions project
(IST programme under EC grant IST-FP6-034434),
and the National Science Foundation under grant
IIS-0535100. We also wish to thank the anonymous
reviewers for their comments.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer networks
and ISDN systems, 30(1-7):107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR.
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P.
O?leary. 2004. Left-brain/right-brain multi-document
summarization. In Proceedings of the Document Un-
derstanding Conference (DUC 2004).
H.T. Dang. 2005. Overview of DUC 2005. In Proceed-
ings of the Document Understanding Conference.
2004. Document understanding conferences (DUC).
http://www-nlpir.nist.gov/projects/duc/index.html.
G. Erkan and D.R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
U. Feige, V. Mirrokni, and J. Vondrak. 2007. Maximiz-
ing non-monotone submodular functions. In Proceed-
ings of 48th Annual IEEE Symposium on Foundations
of Computer Science (FOCS).
U. Feige. 1998. A threshold of ln n for approximating set
cover. Journal of the ACM (JACM), 45(4):634?652.
G. Goel, , C. Karande, P. Tripathi, and L. Wang.
2009. Approximability of Combinatorial Problems
with Multi-agent Submodular Cost Functions. FOCS.
S. Iwata, L. Fleischer, and S. Fujishige. 2001. A
combinatorial strongly polynomial algorithm for min-
imizing submodular functions. Journal of the ACM,
48(4):761?777.
Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, and
Jeff Bilmes. 2009. Submodularity cuts and appli-
cations. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39?45.
A. Krause and C. Guestrin. 2005. A note on the bud-
geted maximization of submodular functions. Techni-
cal Rep. No. CMU-CALD-05, 103.
J. Lee, V.S. Mirrokni, V. Nagarajan, and M. Sviridenko.
2009. Non-monotone submodular maximization un-
der matroid and knapsack constraints. In Proceedings
of the 41st annual ACM symposium on Symposium on
theory of computing, pages 323?332. ACM New York,
NY, USA.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graph-
based submodular selection for extractive summariza-
tion. In Proc. IEEE Automatic Speech Recognition
and Understanding (ASRU), Merano, Italy, December.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop.
L. Lovasz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds.
A. Bachem, M. Grotschel and B. Korte) Springer,
pages 235?257.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. Lecture
Notes in Computer Science, 4425:557.
R. Mihalcea and P. Tarau. 2004. TextRank: bringing or-
der into texts. In Proceedings of EMNLP, Barcelona,
Spain.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on
semantic networks, with application to word sense dis-
ambiguation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING-
04).
R. Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the ACL 2004 (companion volume).
2006. Mosek.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978.
An analysis of approximations for maximizing sub-
modular set functions I. Mathematical Programming,
14(1):265?294.
A. Schrijver. 2000. A combinatorial algorithm mini-
mizing submodular functions in strongly polynomial
time. Journal of Combinatorial Theory, Series B,
80(2):346?355.
M. Sviridenko. 2004. A note on maximizing a submod-
ular set function subject to a knapsack constraint. Op-
erations Research Letters, 32(1):41?43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781?789. Association for
Computational Linguistics.
920
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 510?520,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Class of Submodular Functions for Document Summarization
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We design a class of submodular functions
meant for document summarization tasks.
These functions each combine two terms,
one which encourages the summary to be
representative of the corpus, and the other
which positively rewards diversity. Critically,
our functions are monotone nondecreasing
and submodular, which means that an efficient
scalable greedy optimization scheme has
a constant factor guarantee of optimality.
When evaluated on DUC 2004-2007 corpora,
we obtain better than existing state-of-art
results in both generic and query-focused
document summarization. Lastly, we show
that several well-established methods for
document summarization correspond, in fact,
to submodular function optimization, adding
further evidence that submodular functions are
a natural fit for document summarization.
1 Introduction
In this paper, we address the problem of generic and
query-based extractive summarization from collec-
tions of related documents, a task commonly known
as multi-document summarization. We treat this task
as monotone submodular function maximization (to
be defined in Section 2). This has a number of criti-
cal benefits. On the one hand, there exists a simple
greedy algorithm for monotone submodular func-
tion maximization where the summary solution ob-
tained (say S?) is guaranteed to be almost as good
as the best possible solution (say Sopt) according to
an objective F . More precisely, the greedy algo-
rithm is a constant factor approximation to the car-
dinality constrained version of the problem, so that
F(S?) ? (1 ? 1/e)F(Sopt) ? 0.632F(Sopt). This
is particularly attractive since the quality of the so-
lution does not depend on the size of the problem,
so even very large size problems do well. It is also
important to note that this is a worst case bound, and
in most cases the quality of the solution obtained will
be much better than this bound suggests.
Of course, none of this is useful if the objective
function F is inappropriate for the summarization
task. In this paper, we argue that monotone nonde-
creasing submodular functionsF are an ideal class of
functions to investigate for document summarization.
We show, in fact, that many well-established methods
for summarization (Carbonell and Goldstein, 1998;
Filatova and Hatzivassiloglou, 2004; Takamura and
Okumura, 2009; Riedhammer et al, 2010; Shen and
Li, 2010) correspond to submodular function opti-
mization, a property not explicitly mentioned in these
publications. We take this fact, however, as testament
to the value of submodular functions for summariza-
tion: if summarization algorithms are repeatedly de-
veloped that, by chance, happen to be an instance of a
submodular function optimization, this suggests that
submodular functions are a natural fit. On the other
hand, other authors have started realizing explicitly
the value of submodular functions for summarization
(Lin and Bilmes, 2010; Qazvinian et al, 2010).
Submodular functions share many properties in
common with convex functions, one of which is that
they are closed under a number of common combi-
nation operations (summation, certain compositions,
restrictions, and so on). These operations give us the
tools necessary to design a powerful submodular ob-
jective for submodular document summarization that
extends beyond any previous work. We demonstrate
this by carefully crafting a class of submodular func-
510
tions we feel are ideal for extractive summarization
tasks, both generic and query-focused. In doing so,
we demonstrate better than existing state-of-the-art
performance on a number of standard summarization
evaluation tasks, namely DUC-04 through to DUC-
07. We believe our work, moreover, might act as a
springboard for researchers in summarization to con-
sider the problem of ?how to design a submodular
function? for the summarization task.
In Section 2, we provide a brief background on sub-
modular functions and their optimization. Section 3
describes how the task of extractive summarization
can be viewed as a problem of submodular function
maximization. We also in this section show that many
standard methods for summarization are, in fact, al-
ready performing submodular function optimization.
In Section 4, we present our own submodular func-
tions. Section 5 presents results on both generic and
query-focused summarization tasks, showing as far
as we know the best known ROUGE results for DUC-
04 through DUC-06, and the best known precision
results for DUC-07, and the best recall DUC-07 re-
sults among those that do not use a web search engine.
Section 6 discusses implications for future work.
2 Background on Submodularity
We are given a set of objects V = {v1, . . . , vn} and a
functionF : 2V ? R that returns a real value for any
subset S ? V . We are interested in finding the subset
of bounded size |S| ? k that maximizes the function,
e.g., argmaxS?V F(S). In general, this operation
is hopelessly intractable, an unfortunate fact since
the optimization coincides with many important ap-
plications. For example, F might correspond to the
value or coverage of a set of sensor locations in an
environment, and the goal is to find the best locations
for a fixed number of sensors (Krause et al, 2008).
If the function F is monotone submodular then the
maximization is still NP complete, but it was shown
in (Nemhauser et al, 1978) that a greedy algorithm
finds an approximate solution guaranteed to be within
e?1
e ? 0.63 of the optimal solution, as mentioned
in Section 1. A version of this algorithm (Minoux,
1978), moreover, scales to very large data sets. Sub-
modular functions are those that satisfy the property
of diminishing returns: for anyA ? B ? V \v, a sub-
modular functionF must satisfyF(A+v)?F(A) ?
F(B + v)?F(B). That is, the incremental ?value?
of v decreases as the context in which v is considered
grows from A to B. An equivalent definition, useful
mathematically, is that for any A,B ? V , we must
have that F(A) +F(B) ? F(A ?B) +F(A ?B).
If this is satisfied everywhere with equality, then
the function F is called modular, and in such case
F(A) = c +
?
a?A
~fa for a sized |V | vector ~f of
real values and constant c. A set function F is mono-
tone nondecreasing if ?A ? B, F(A) ? F(B). As
shorthand, in this paper, monotone nondecreasing
submodular functions will simply be referred to as
monotone submodular.
Historically, submodular functions have their roots
in economics, game theory, combinatorial optimiza-
tion, and operations research. More recently, submod-
ular functions have started receiving attention in the
machine learning and computer vision community
(Kempe et al, 2003; Narasimhan and Bilmes, 2005;
Krause and Guestrin, 2005; Narasimhan and Bilmes,
2007; Krause et al, 2008; Kolmogorov and Zabin,
2004) and have recently been introduced to natural
language processing for the tasks of document sum-
marization (Lin and Bilmes, 2010) and word align-
ment (Lin and Bilmes, 2011).
Submodular functions share a number of proper-
ties in common with convex and concave functions
(Lova?sz, 1983), including their wide applicability,
their generality, their multiple options for their repre-
sentation, and their closure under a number of com-
mon operators (including mixtures, truncation, com-
plementation, and certain convolutions). For exam-
ple, if a collection of functions {Fi}i is submodular,
then so is their weighted sum F =
?
i ?iFi where
?i are nonnegative weights. It is not hard to show
that submodular functions also have the following
composition property with concave functions:
Theorem 1. Given functions F : 2V ? R and
f : R? R, the composition F ? = f ? F : 2V ? R
(i.e., F ?(S) = f(F(S))) is nondecreasing sub-
modular, if f is non-decreasing concave and F is
nondecreasing submodular.
This property will be quite useful when defining sub-
modular functions for document summarization.
511
3 Submodularity in Summarization
3.1 Summarization with knapsack constraint
Let the ground set V represents all the sentences
(or other linguistic units) in a document (or docu-
ment collection, in the multi-document summariza-
tion case). The task of extractive document sum-
marization is to select a subset S ? V to represent
the entirety (ground set V ). There are typically con-
straints on S, however. Obviously, we should have
|S| < |V | = N as it is a summary and should
be small. In standard summarization tasks (e.g.,
DUC evaluations), the summary is usually required
to be length-limited. Therefore, constraints on S
can naturally be modeled as knapsack constraints:
?
i?S ci ? b, where ci is the non-negative cost of
selecting unit i (e.g., the number of words in the sen-
tence) and b is our budget. If we use a set function
F : 2V ? R to measure the quality of the summary
set S, the summarization problem can then be for-
malized as the following combinatorial optimization
problem:
Problem 1. Find
S? ? argmax
S?V
F(S) subject to:
?
i?S
ci ? b.
Since this is a generalization of the cardinality
constraint (where ci = 1,?i), this also constitutes
a (well-known) NP-hard problem. In this case as
well, however, a modified greedy algorithm with par-
tial enumeration can solve Problem 1 near-optimally
with (1?1/e)-approximation factor ifF is monotone
submodular (Sviridenko, 2004). The partial enumer-
ation, however, is too computationally expensive for
real world applications. In (Lin and Bilmes, 2010),
we generalize the work by Khuller et al (1999) on
the budgeted maximum cover problem to the gen-
eral submodular framework, and show a practical
greedy algorithm with a (1? 1/?e)-approximation
factor, where each greedy step adds the unit with the
largest ratio of objective function gain to scaled cost,
while not violating the budget constraint (see (Lin
and Bilmes, 2010) for details). Note that in all cases,
submodularity and monotonicity are two necessary
ingredients to guarantee that the greedy algorithm
gives near-optimal solutions.
In fact, greedy-like algorithms have been widely
used in summarization. One of the more popular
approaches is maximum marginal relevance (MMR)
(Carbonell and Goldstein, 1998), where a greedy
algorithm selects the most relevant sentences, and
at the same time avoids redundancy by removing
sentences that are too similar to ones already selected.
Interestingly, the gain function defined in the original
MMR paper (Carbonell and Goldstein, 1998) satisfies
diminishing returns, a fact apparently unnoticed until
now. In particular, Carbonell and Goldstein (1998)
define an objective function gain of adding element
k to set S (k /? S) as:
?Sim1(sk, q)? (1? ?) max
i?S
Sim2(si, sk), (1)
where Sim1(sk, q) measures the similarity between
unit sk to a query q, Sim2(si, sk) measures the simi-
larity between unit si and unit sk, and 0 ? ? ? 1 is
a trade-off coefficient. We have:
Theorem 2. Given an expression forFMMR such that
FMMR(S ?{k})?FMMR(S) is equal to Eq. 1, FMMR
is non-monotone submodular.
Obviously, diminishing-returns hold since
max
i?S
Sim2(si, sk) ? max
i?R
Sim2(si, sk)
for all S ? R, and therefore FMMR is submodular.
On the other hand,FMMR, would not be monotone, so
the greedy algorithm?s constant-factor approximation
guarantee does not apply in this case.
When scoring a summary at the sub-sentence
level, submodularity naturally arises. Concept-based
summarization (Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009; Riedhammer et al,
2010; Qazvinian et al, 2010) usually maximizes the
weighted credit of concepts covered by the summary.
Although the authors may not have noticed, their ob-
jective functions are also submodular, adding more
evidence suggesting that submodularity is natural for
summarization tasks. Indeed, let S be a subset of
sentences in the document and denote ?(S) as the
set of concepts contained in S. The total credit of the
concepts covered by S is then
Fconcept(S) ,
?
i??(S)
ci,
where ci is the credit of concept i. This function is
known to be submodular (Narayanan, 1997).
512
Similar to the MMR approach, in (Lin and Bilmes,
2010), a submodular graph based objective function
is proposed where a graph cut function, measuring
the similarity of the summary to the rest of document,
is combined with a subtracted redundancy penalty
function. The objective function is submodular but
again, non-monotone. We theoretically justify that
the performance guarantee of the greedy algorithm
holds for this objective function with high probability
(Lin and Bilmes, 2010). Our justification, however,
is shown to be applicable only to certain particular
non-monotone submodular functions, under certain
reasonable assumptions about the probability distri-
bution over weights of the graph.
3.2 Summarization with covering constraint
Another perspective is to treat the summarization
problem as finding a low-cost subset of the document
under the constraint that a summary should cover
all (or a sufficient amount of) the information in the
document. Formally, this can be expressed as
Problem 2. Find
S? ? argmin
S?V
?
i?S
ci subject to: F(S) ? ?,
where ci are the element costs, and set function F(S)
measure the information covered by S. When F
is submodular, the constraint F(S) ? ? is called
a submodular cover constraint. When F is mono-
tone submodular, a greedy algorithm that iteratively
selects k with minimum ck/(F(S ? {k}) ? F(S))
has approximation guarantees (Wolsey, 1982). Re-
cent work (Shen and Li, 2010) proposes to model
document summarization as finding a minimum dom-
inating set and a greedy algorithm is used to solve
the problem. The dominating set constraint is also
a submodular cover constraint. Define ?(S) be the
set of elements that is either in S or is adjacent to
some element in S. Then S is a dominating set if
|?(S)| = |V |. Note that
Fdom(S) , |?(S)|
is monotone submodular. The dominating set
constraint is then also a submodular cover constraint,
and therefore the approaches in (Shen and Li, 2010)
are special cases of Problem 2. The solutions found
in this framework, however, do not necessarily
satisfy a summary?s budget constraint. Consequently,
a subset of the solution found by solving Problem 2
has to be constructed as the final summary, and the
near-optimality is no longer guaranteed. Therefore,
solving Problem 1 for document summarization
appears to be a better framework regarding global
optimality. In the present paper, our framework is
that of Problem 1.
3.3 Automatic summarization evaluation
Automatic evaluation of summary quality is impor-
tant for the research of document summarization as
it avoids the labor-intensive and potentially inconsis-
tent human evaluation. ROUGE (Lin, 2004) is widely
used for summarization evaluation and it has been
shown that ROUGE-N scores are highly correlated
with human evaluation (Lin, 2004). Interestingly,
ROUGE-N is monotone submodular, adding further
evidence that monotone submodular functions are
natural for document summarization.
Theorem 3. ROUGE-N is monotone submodular.
Proof. By definition (Lin, 2004), ROUGE-N is the
n-gram recall between a candidate summary and a
set of reference summaries. Precisely, let S be the
candidate summary (a set of sentences extracted from
the ground set V ), ce : 2V ? Z+ be the number of
times n-gram e occurs in summary S, and Ri be the
set of n-grams contained in the reference summary i
(suppose we have K reference summaries, i.e., i =
1, ? ? ? ,K). Then ROUGE-N can be written as the
following set function:
FROUGE-N(S) ,
?K
i=1
?
e?Ri
min(ce(S), re,i)
?K
i=1
?
e?Ri
re,i
,
where re,i is the number of times n-gram e occurs
in reference summary i. Since ce(S) is monotone
modular and min(x, a) is a concave non-decreasing
function of x, min(ce(S), re,i) is monotone sub-
modular by Theorem 1. Since summation preserves
submodularity, and the denominator is constant, we
see that FROUGE-N is monotone submodular.
Since the reference summaries are unknown, it is
of course impossible to optimize FROUGE-N directly.
Therefore, some approaches (Filatova and Hatzivas-
siloglou, 2004; Takamura and Okumura, 2009; Ried-
hammer et al, 2010) instead define ?concepts?. Alter-
513
natively, we herein propose a class of monotone sub-
modular functions that naturally models the quality of
a summary while not depending on an explicit notion
of concepts, as we will see in the following section.
4 Monotone Submodular Objectives
Two properties of a good summary are relevance and
non-redundancy. Objective functions for extractive
summarization usually measure these two separately
and then mix them together trading off encouraging
relevance and penalizing redundancy. The redun-
dancy penalty usually violates the monotonicity of
the objective functions (Carbonell and Goldstein,
1998; Lin and Bilmes, 2010). We therefore propose
to positively reward diversity instead of negatively
penalizing redundancy. In particular, we model the
summary quality as
F(S) = L(S) + ?R(S), (2)
where L(S) measures the coverage, or ?fidelity?,
of summary set S to the document, R(S) rewards
diversity in S, and ? ? 0 is a trade-off coefficient.
Note that the above is analogous to the objectives
widely used in machine learning, where a loss
function that measures the training set error (we
measure the coverage of summary to a document),
is combined with a regularization term encouraging
certain desirable (e.g., sparsity) properties (in
our case, we ?regularize? the solution to be more
diverse). In the following, we discuss how both L(S)
andR(S) are naturally monotone submodular.
4.1 Coverage function
L(S) can be interpreted either as a set function that
measures the similarity of summary set S to the docu-
ment to be summarized, or as a function representing
some form of ?coverage? of V by S. Most naturally,
L(S) should be monotone, as coverage improves
with a larger summary. L(S) should also be submod-
ular: consider adding a new sentence into two sum-
mary sets, one a subset of the other. Intuitively, the
increment when adding a new sentence to the small
summary set should be larger than the increment
when adding it to the larger set, as the information
carried by the new sentence might have already been
covered by those sentences that are in the larger sum-
mary but not in the smaller summary. This is exactly
the property of diminishing returns. Indeed, Shan-
non entropy, as the measurement of information, is
another well-known monotone submodular function.
There are several ways to define L(S) in our
context. For instance, we could use L(S) =
?
i?V,j?S wi,j where wi,j represents the similarity
between i and j. L(S) could also be facility
location objective, i.e., L(S) =
?
i?V maxj?S wi,j ,
as used in (Lin et al, 2009). We could also use
L(S) =
?
i??(S) ci as used in concept-based
summarization, where the definition of ?concept?
and the mechanism to extract these concepts become
important. All of these are monotone submodular.
Alternatively, in this paper we propose the follow-
ing objective that does not reply on concepts. Let
L(S) =
?
i?V
min {Ci(S), ? Ci(V )} , (3)
where Ci : 2V ? R is a monotone submodular func-
tion and 0 ? ? ? 1 is a threshold co-efficient. Firstly,
L(S) as defined in Eqn. 3 is a monotone submodular
function. The monotonicity is immediate. To see that
L(S) is submodular, consider the fact that f(x) =
min(x, a) where a ? 0 is a concave non-decreasing
function, and by Theorem 1, each summand in Eqn. 3
is a submodular function, and as summation pre-
serves submodularity, L(S) is submodular.
Next, we explain the intuition behind Eqn. 3. Basi-
cally, Ci(S) measures how similar S is to element i,
or how much of i is ?covered? by S. Then Ci(V ) is
just the largest value that Ci(S) can achieve. We call
i ?saturated? by S when min{Ci(S), ?Ci(V )} =
?Ci(V ). When i is already saturated in this way,
any new sentence j can not further improve the
coverage of i even if it is very similar to i (i.e.,
Ci(S ? {j}) ? Ci(S) is large). This will give other
sentences that are not yet saturated a higher chance
of being better covered, and therefore the resulting
summary tends to better cover the entire document.
One simple way to define Ci(S) is just to use
Ci(S) =
?
j?S
wi,j (4)
where wi,j ? 0 measures the similarity between i
and j. In this case, when ? = 1, Eqn. 3 reduces
to the case where L(S) =
?
i?V,j?S wi,j . As we
will see in Section 5, having an ? that is less than
514
1 significantly improves the performance compared
to the case when ? = 1, which coincides with our
intuition that using a truncation threshold improves
the final summary?s coverage.
4.2 Diversity reward function
Instead of penalizing redundancy by subtracting from
the objective, we propose to reward diversity by
adding the following to the objective:
R(S) =
K?
i=1
? ?
j?Pi?S
rj . (5)
where Pi, i = 1, ? ? ?K is a partition of the ground
set V (i.e.,
?
i Pi = V and the Pis are disjoint) into
separate clusters, and ri ? 0 indicates the singleton
reward of i (i.e., the reward of adding i into the empty
set). The value ri estimates the importance of i to
the summary. The functionR(S) rewards diversity
in that there is usually more benefit to selecting a
sentence from a cluster not yet having one of its
elements already chosen. As soon as an element
is selected from a cluster, other elements from the
same cluster start having diminishing gain, thanks
to the square root function. For instance, consider
the case where k1, k2 ? P1, k3 ? P2, and rk1 = 4,
rk2 = 9, and rk3 = 4. Assume k1 is already in the
summary set S. Greedily selecting the next element
will choose k3 rather than k2 since
?
13 < 2 + 2. In
other words, adding k3 achieves a greater reward as it
increases the diversity of the summary (by choosing
from a different cluster). Note,R(S) is distinct from
L(S) in that R(S) might wish to include certain
outlier material that L(S) could ignore.
It is easy to show that R(S) is submodular by
using the composition rule from Theorem 1. The
square root is non-decreasing concave function.
Inside each square root lies a modular function
with non-negative weights (and thus is monotone).
Applying the square root to such a monotone sub-
modular function yields a submodular function, and
summing them all together retains submodularity, as
mentioned in Section 2. The monotonicity ofR(S)
is straightforward. Note, the form of Eqn. 5 is similar
to structured group norms (e.g., (Zhao et al, 2009)),
recently shown to be related to submodularity (Bach,
2010; Jegelka and Bilmes, 2011).
Several extensions to Eqn. 5 are discussed
next: First, instead of using a ground set partition,
intersecting clusters can be used. Second, the
square root function in Eqn. 5 can be replaced with
any other non-decreasing concave functions (e.g.,
f(x) = log(1 + x)) while preserving the desired
property ofR(S), and the curvature of the concave
function then determines the rate that the reward
diminishes. Last, multi-resolution clustering (or
partitions) with different sizes (K) can be used, i.e.,
we can use a mixture of components, each of which
has the structure of Eqn. 5. A mixture can better
represent the core structure of the ground set (e.g.,
the hierarchical structure in the documents (Celiky-
ilmaz and Hakkani-tu?r, 2010)). All such extensions
preserve both monotonicity and submodularity.
5 Experiments
The document understanding conference (DUC)
(http://duc.nist.org) was the main forum
providing benchmarks for researchers working
on document summarization. The tasks in DUC
evolved from single-document summarization to
multi-document summarization, and from generic
summarization (2001?2004) to query-focused sum-
marization (2005?2007). As ROUGE (Lin, 2004)
has been officially adopted for DUC evaluations
since 2004, we also take it as our main evaluation
criterion. We evaluated our approaches on DUC
data 2003-2007, and demonstrate results on both
generic and query-focused summarization. In all
experiments, the modified greedy algorithm (Lin and
Bilmes, 2010) was used for summary generation.
5.1 Generic summarization
Summarization tasks in DUC-03 and DUC-04 are
multi-document summarization on English news
articles. In each task, 50 document clusters are
given, each of which consists of 10 documents.
For each document cluster, the system generated
summary may not be longer than 665 bytes including
spaces and punctuation. We used DUC-03 as
our development set, and tested on DUC-04 data.
We show ROUGE-1 scores1 as it was the main
evaluation criterion for DUC-03, 04 evaluations.
1ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n 4
-w 1.2
515
Documents were pre-processed by segmenting sen-
tences and stemming words using the Porter Stemmer.
Each sentence was represented using a bag-of-terms
vector, where we used context terms up to bi-grams.
Similarity between sentence i and sentence j, i.e.,
wi,j , was computed using cosine similarity:
wi,j =
?
w?si
tfw,i ? tfw,j ? idf2w
??
w?si
tf2w,si idf
2
w
??
w?sj
tf2w,j idf
2
w
,
where tfw,i and tfw,j are the numbers of times that
w appears in si and sentence sj respectively, and
idfw is the inverse document frequency (IDF) of
term w (up to bigram), which was calculated as the
logarithm of the ratio of the number of articles that
w appears over the total number of all articles in the
document cluster.
Table 1: ROUGE-1 recall (R) and F-measure (F) results
(%) on DUC-04. DUC-03 was used as development set.
DUC-04 R F
P
i?V
P
j?S wi,j 33.59 32.44
L1(S) 39.03 38.65
R1(S) 38.23 37.81
L1(S) + ?R1(S) 39.35 38.90
Takamura and Okumura (2009) 38.50 -
Wang et al (2009) 39.07 -
Lin and Bilmes (2010) - 38.39
Best system in DUC-04 (peer 65) 38.28 37.94
We first tested our coverage and diversity re-
ward objectives separately. For coverage, we use a
modular Ci(S) =
?
j?S wi,j for each sentence i, i.e.,
L1(S) =
?
i?V
min
?
?
?
?
j?S
wi,j , ?
?
k?V
wi,k
?
?
?
. (6)
When ? = 1, L1(S) reduces to
?
i?V,j?S wi,j ,
which measures the overall similarity of summary
set S to ground set V . As mentioned in Section 4.1,
using such similarity measurement could possibly
over-concentrate on a small portion of the document
and result in a poor coverage of the whole document.
As shown in Table 1, optimizing this objective
function gives a ROUGE-1 F-measure score 32.44%.
On the other hand, when using L1(S) with an ? < 1
(the value of ? was determined on DUC-03 using
a grid search), a ROUGE-1 F-measure score 38.65%
36.2
36.4
36.6
36.8
37
37.2
37.4
37.6
0 5 10 15 20
RO
UG
E-1
 F-
me
asu
re 
(%
)
K=0.05N
K=0.1N
K=0.2N
a
Figure 1: ROUGE-1 F-measure scores on DUC-03 when
? and K vary in objective function L1(S) + ?R1(S),
where ? = 6 and ? = aN .
is achieved, which is already better than the best
performing system in DUC-04.
As for the diversity reward objective, we define
the singleton reward as ri = 1N
?
j wi,j , which is
the average similarity of sentence i to the rest of the
document. It basically states that the more similar to
the whole document a sentence is, the more reward
there will be by adding this sentence to an empty
summary set. By using this singleton reward, we
have the following diversity reward function:
R1(S) =
K?
k=1
?
?
j?S?Pk
1
N
?
i?V
wi,j . (7)
In order to generate Pk, k = 1, ? ? ?K, we used
CLUTO2 to cluster the sentences, where the IDF-
weighted term vector was used as feature vector, and
a direct K-mean clustering algorithm was used. In
this experiment, we set K = 0.2N . In other words,
there are 5 sentences in each cluster on average.
And as we can see in Table 1, optimizing the
diversity reward function alone achieves comparable
performance to the DUC-04 best system.
Combining L1(S) and R1(S), our system outper-
forms the best system in DUC-04 significantly, and
it also outperforms several recent systems, including
a concept-based summarization approach (Takamura
and Okumura, 2009), a sentence topic model based
system (Wang et al, 2009), and our MMR-styled
submodular system (Lin and Bilmes, 2010). Figure 1
illustrates how ROUGE-1 scores change when ? and
K vary on the development set (DUC-03).
2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
516
Table 2: ROUGE-2 recall (R) and F-measure (F) results
(%) on DUC-05, where DUC-05 was used as training set.
DUC-05 R F
L1(S) + ?RQ(S) 8.38 8.31
Daume? III and Marcu (2006) 7.62 -
Extr, Daume? et al (2009) 7.67 -
Vine, Daume? et al (2009) 8.24 -
Table 3: ROUGE-2 recall (R) and F-measure (F) results
on DUC-05 (%). We used DUC-06 as training set.
DUC-05 R F
L1(S) + ?RQ(S) 7.82 7.72
Daume? III and Marcu (2006) 6.98 -
Best system in DUC-05 (peer 15) 7.44 7.43
5.2 Query-focused summarization
We evaluated our approach on the task of query-
focused summarization using DUC 05-07 data. In
DUC-05 and DUC-06, participants were given 50
document clusters, where each cluster contains 25
news articles related to the same topic. Participants
were asked to generate summaries of at most 250
words for each cluster. For each cluster, a title and
a narrative describing a user?s information need are
provided. The narrative is usually composed of a
set of questions or a multi-sentence task description.
The main task in DUC-07 is the same as in DUC-06.
In DUC 05-07, ROUGE-2 was the primary
criterion for evaluation, and thus we also report
ROUGE-23 (both recall R, and precision F). Docu-
ments were processed as in Section 5.1. We used both
the title and the narrative as query, where stop words,
including some function words (e.g., ?describe?) that
appear frequently in the query, were removed. All
queries were then stemmed using the Porter Stemmer.
Note that there are several ways to incorporate
query-focused information into both the coverage
and diversity reward objectives. For instance, Ci(S)
could be query-dependent in how it measures how
much query-dependent information in i is covered
by S. Also, the coefficient ? could be query and sen-
tence dependent, where it takes larger value when a
sentence is more relevant to query (i.e., a larger value
of ? means later truncation, and therefore more pos-
sible coverage). Similarly, sentence clustering and
singleton rewards in the diversity function can also
3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4
-u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250
Table 4: ROUGE-2 recall (R) and F-measure (F) results
(%) on DUC-06, where DUC-05 was used as training set.
DUC-06 R F
L1(S) + ?RQ(S) 9.75 9.77
Celikyilmaz and Hakkani-tu?r (2010) 9.10 -
Shen and Li (2010) 9.30 -
Best system in DUC-06 (peer 24) 9.51 9.51
Table 5: ROUGE-2 recall (R) and F-measure (F) re-
sults (%) on DUC-07. DUC-05 was used as training
set for objective L1(S) + ?RQ(S). DUC-05 and DUC-
06 were used as training sets for objective L1(S) +?
? ??RQ,?(S).
DUC-07 R F
L1(S) + ?RQ(S) 12.18 12.13
L1(S) +
P3
?=1 ??RQ,?(S) 12.38 12.33
Toutanova et al (2007) 11.89 11.89
Haghighi and Vanderwende (2009) 11.80 -
Celikyilmaz and Hakkani-tu?r (2010) 11.40 -
Best system in DUC-07 (peer 15) 12.45 12.29
be query-dependent. In this experiment, we explore
an objective with a query-independent coverage func-
tion (R1(S)), indicating prior importance, combined
with a query-dependent diversity reward function,
where the latter is defined as:
RQ(S) =
K?
k=1
?
?
?
?
?
j?S?Pk
(
?
N
?
i?V
wi,j + (1? ?)rj,Q
)
,
where 0 ? ? ? 1, and rj,Q represents the rel-
evance between sentence j to query Q. This
query-dependent reward function is derived by
using a singleton reward that is expressed as a
convex combination of the query-independent score
( 1N
?
i?V wi,j) and the query-dependent score (rj,Q)
of a sentence. We simply used the number of
terms (up to a bi-gram) that sentence j overlaps the
query Q as rj,Q, where the IDF weighting is not
used (i.e., every term in the query, after stop word
removal, was treated as equally important). Both
query-independent and query-dependent scores were
then normalized by their largest value respectively
such that they had roughly the same dynamic range.
To better estimate of the relevance between query
and sentences, we further expanded sentences with
synonyms and hypernyms of its constituent words. In
particular, part-of-speech tags were obtained for each
sentence using the maximum entropy part-of-speech
tagger (Ratnaparkhi, 1996), and all nouns were then
517
expanded with their synonyms and hypernyms using
WordNet (Fellbaum, 1998). Note that these expanded
documents were only used in the estimation rj,Q, and
we plan to further explore whether there is benefit to
use the expanded documents either in sentence sim-
ilarity estimation or in sentence clustering in our fu-
ture work. We also tried to expand the query with syn-
onyms and observed a performance decrease, presum-
ably due to noisy information in a query expression.
While it is possible to use an approach that is
similar to (Toutanova et al, 2007) to learn the
coefficients in our objective function, we trained all
coefficients to maximize ROUGE-2 F-measure score
using the Nelder-Mead (derivative-free) method.
Using L1(S)+?RQ(S) as the objective and with the
same sentence clustering algorithm as in the generic
summarization experiment (K = 0.2N ), our system,
when both trained and tested on DUC-05 (results in
Table 2), outperforms the Bayesian query-focused
summarization approach and the search-based
structured prediction approach, which were also
trained and tested on DUC-05 (Daume? et al, 2009).
Note that the system in (Daume? et al, 2009) that
achieves its best performance (8.24% in ROUGE-2
recall) is a so called ?vine-growth? system, which
can be seen as an abstractive approach, whereas our
system is purely an extractive system. Comparing
to the extractive system in (Daume? et al, 2009), our
system performs much better (8.38% v.s. 7.67%).
More importantly, when trained only on DUC-06 and
tested on DUC-05 (results in Table 3), our approach
outperforms the best system in DUC-05 significantly.
We further tested the system trained on DUC-05
on both DUC-06 and DUC-07. The results on
DUC-06 are shown in Table. 4. Our system outper-
forms the best system in DUC-06, as well as two
recent approaches (Shen and Li, 2010; Celikyilmaz
and Hakkani-tu?r, 2010). On DUC-07, in terms of
ROUGE-2 score, our system outperforms PYTHY
(Toutanova et al, 2007), a state-of-the-art supervised
summarization system, as well as two recent systems
including a generative summarization system based
on topic models (Haghighi and Vanderwende,
2009), and a hybrid hierarchical summarization
system (Celikyilmaz and Hakkani-tu?r, 2010). It
also achieves comparable performance to the best
DUC-07 system. Note that in the best DUC-07
system (Pingali et al, 2007; Jagarlamudi et al, 2006),
an external web search engine (Yahoo!) was used
to estimate a language model for query relevance. In
our system, no such web search expansion was used.
To further improve the performance of our system,
we used both DUC-05 and DUC-06 as a training
set, and introduced three diversity reward terms
into the objective where three different sentence
clusterings with different resolutions were produced
(with sizes 0.3N, 0.15N and 0.05N ). Denoting
a diversity reward corresponding to clustering ?
as RQ,?(S), we model the summary quality as
L1(S) +
?3
?=1 ??RQ,?(S). As shown in Table 5,
using this objective function with multi-resolution
diversity rewards improves our results further, and
outperforms the best system in DUC-07 in terms of
ROUGE-2 F-measure score.
6 Conclusion and discussion
In this paper, we show that submodularity naturally
arises in document summarization. Not only do
many existing automatic summarization methods cor-
respond to submodular function optimization, but
also the widely used ROUGE evaluation is closely
related to submodular functions. As the correspond-
ing submodular optimization problem can be solved
efficiently and effectively, the remaining question
is then how to design a submodular objective that
best models the task. To address this problem, we
introduce a powerful class of monotone submodular
functions that are well suited to document summariza-
tion by modeling two important properties of a sum-
mary, fidelity and diversity. While more advanced
NLP techniques could be easily incorporated into our
functions (e.g., language models could define a better
Ci(S), more advanced relevance estimations for the
singleton rewards ri, and better and/or overlapping
clustering algorithms for our diversity reward), we
already show top results on standard benchmark eval-
uations using fairly basic NLP methods (e.g., term
weighting and WordNet expansion), all, we believe,
thanks to the power and generality of submodular
functions. As information retrieval and web search
are closely related to query-focused summarization,
our approach might be beneficial in those areas as
well.
518
References
F. Bach. 2010. Structured sparsity-inducing norms
through submodular functions. Advances in Neural
Information Processing Systems.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proc. of SIGIR.
A. Celikyilmaz and D. Hakkani-tu?r. 2010. A hybrid hier-
archical model for multi-document summarization. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 815?824,
Uppsala, Sweden, July. Association for Computational
Linguistics.
H. Daume?, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine learning,
75(3):297?325.
H. Daume? III and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, page 312.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL Work-
shop on Summarization, volume 111.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
J. Jagarlamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006.
S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond
submodular energies: coupling edges in graph cuts.
In Computer Vision and Pattern Recognition (CVPR),
Colorado Springs, CO, June.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39?45.
V. Kolmogorov and R. Zabin. 2004. What energy func-
tions can be minimized via graph cuts? IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
26(2):147?159.
A. Krause and C. Guestrin. 2005. Near-optimal nonmy-
opic value of information in graphical models. In Proc.
of Uncertainty in AI.
A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta.
2008. Robust submodular observation selection. Jour-
nal of Machine Learning Research, 9:2761?2801.
H. Lin and J. Bilmes. 2010. Multi-document summariza-
tion via budgeted maximization of submodular func-
tions. In North American chapter of the Association
for Computational Linguistics/Human Language Tech-
nology Conference (NAACL/HLT-2010), Los Angeles,
CA, June.
H. Lin and J. Bilmes. 2011. Word alignment via submod-
ular maximization over matroids. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT), Port-
land, OR, June.
H. Lin, J. Bilmes, and S. Xie. 2009. Graph-based submod-
ular selection for extractive summarization. In Proc.
IEEE Automatic Speech Recognition and Understand-
ing (ASRU), Merano, Italy, December.
C.-Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop.
L. Lova?sz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds. A.
Bachem, M. Grotschel and B. Korte) Springer, pages
235?257.
M. Minoux. 1978. Accelerated greedy algorithms for
maximizing submodular set functions. Optimization
Techniques, pages 234?243.
M. Narasimhan and J. Bilmes. 2005. A submodular-
supermodular procedure with applications to discrimi-
native structure learning. In Proc. Conf. Uncertainty in
Artifical Intelligence, Edinburgh, Scotland, July. Mor-
gan Kaufmann Publishers.
M. Narasimhan and J. Bilmes. 2007. Local search for
balanced submodular clusterings. In Twentieth Inter-
national Joint Conference on Artificial Intelligence (IJ-
CAI07), Hyderabad, India, January.
H. Narayanan. 1997. Submodular functions and electrical
networks. North-Holland.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An
analysis of approximations for maximizing submodular
set functions I. Mathematical Programming, 14(1):265?
294.
P. Pingali, K. Rahul, and V. Varma. 2007. IIIT Hyderabad
at DUC 2007. Proceedings of DUC 2007.
V. Qazvinian, D.R. Radev, and A. Ozgu?r. 2010. Cita-
tion Summarization Through Keyphrase Extraction. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 895?
903.
519
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP, volume 1, pages
133?142.
K. Riedhammer, B. Favre, and D. Hakkani-Tu?r. 2010.
Long story short-Global unsupervised models for
keyphrase based meeting summarization. Speech Com-
munication.
C. Shen and T. Li. 2010. Multi-document summarization
via the minimum dominating set. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 984?992, Beijing, China,
August. Coling 2010 Organizing Committee.
M. Sviridenko. 2004. A note on maximizing a submodu-
lar set function subject to a knapsack constraint. Oper-
ations Research Letters, 32(1):41?43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781?789. Association for
Computational Linguistics.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at DUC
2007. In the proceedings of Document Understanding
Conference.
D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multi-
document summarization using sentence-based topic
models. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 297?300, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
L.A. Wolsey. 1982. An analysis of the greedy algorithm
for the submodular set covering problem. Combinator-
ica, 2(4):385?393.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hier-
archical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468?3497.
520
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 170?175,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Word Alignment via Submodular Maximization over Matroids
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We cast the word alignment problem as max-
imizing a submodular function under matroid
constraints. Our framework is able to express
complex interactions between alignment com-
ponents while remaining computationally ef-
ficient, thanks to the power and generality of
submodular functions. We show that submod-
ularity naturally arises when modeling word
fertility. Experiments on the English-French
Hansards alignment task show that our ap-
proach achieves lower alignment error rates
compared to conventional matching based ap-
proaches.
1 Introduction
Word alignment is a key component in most statisti-
cal machine translation systems. While classical ap-
proaches for word alignment are based on generative
models (e.g., IBM models (Brown et al, 1993) and
HMM (Vogel et al, 1996)), word alignment can also
be viewed as a matching problem, where each word
pair is associated with a score reflecting the desirabil-
ity of aligning that pair, and the alignment is then the
highest scored matching under some constraints.
Several matching-based approaches have been
proposed in the past. Melamed (2000) introduces
the competitive linking algorithm which greedily
constructs matchings under the one-to-one mapping
assumption. In (Matusov et al, 2004), matchings
are found using an algorithm for constructing
a maximum weighted bipartite graph matching
(Schrijver, 2003), where word pair scores come from
alignment posteriors of generative models. Similarly,
Taskar et al (2005) cast word alignment as a
maximum weighted matching problem and propose a
framework for learning word pair scores as a function
of arbitrary features of that pair. These approaches,
however, have two potentially substantial limitations:
words have fertility of at most one, and interactions
between alignment decisions are not representable.
Lacoste-Julien et al (2006) address this issue by
formulating the alignment problem as a quadratic
assignment problem, and off-the-shelf integer linear
programming (ILP) solvers are used to solve to op-
timization problem. While efficient for some median
scale problems, ILP-based approaches are limited
since when modeling more sophisticated interactions,
the number of variables (and/or constraints) required
grows polynomially, or even exponentially, making
the resultant optimization impractical to solve.
In this paper, we treat the word alignment problem
as maximizing a submodular function subject to
matroid constraints (to be defined in Section 2).
Submodular objective functions can represent
complex interactions among alignment decisions,
and essentially extend the modular (linear) objectives
used in the aforementioned approaches. While our
extensions add expressive power, they do not result
in a heavy computational burden. This is because
maximizing a monotone submodular function under
a matroid constraint can be solved efficiently using
a simple greedy algorithm. The greedy algorithm,
moreover, is a constant factor approximation
algorithm that guarantees a near-optimal solution.
In this paper, we moreover show that submodularity
naturally arises in word alignment problems when
modeling word fertility (see Section 4). Experiment
results on the English-French Hansards alignment
task show that our approach achieves lower align-
ment error rates compared to the maximum weighted
matching approach, while being at least 50 times
170
faster than an ILP-based approach.
2 Background
Matroids and submodularity both play important
roles in combinatorial optimization. We briefly in-
troduce them here, referring the reader to (Schrijver,
2003) for details.
Matroids are combinatorial structures that general-
ize the notion of linear independence in matrices. A
pair (V, I) is called a matroid if V is a finite ground
set and I is a nonempty collection of subsets of V
that are independent. In particular, I must satisfy (i)
if X ? Y and Y ? I then X ? I, (ii) if X,Y ? I
and |X| < |Y | thenX?{e} ? I for some e ? Y \X .
We typically refer to a matroid by listing its ground
set and its family of independent sets:M = (V, I).
A set function f : 2V ? R is called submodu-
lar (Edmonds, 1970) if it satisfies the property of
diminishing returns: for any X ? Y ? V \ v, a sub-
modular function f must satisfy f(X+v)?f(X) ?
f(Y + v)? f(Y ). That is, the incremental ?value?
of v decreases as the context in which v is considered
grows from X to Y . If this is satisfied everywhere
with equality, then the function f is called modu-
lar. A set function f is monotone nondecreasing if
?X ? Y , f(X) ? f(Y ). As shorthand, in this pa-
per, monotone nondecreasing submodular functions
will simply be referred to as monotone submodular.
Historically, submodular functions have their roots
in economics, game theory, combinatorial optimiza-
tion, and operations research. More recently, submod-
ular functions have started receiving attention in the
machine learning and computer vision community
(Kempe et al, 2003; Narasimhan and Bilmes, 2004;
Narasimhan and Bilmes, 2005; Krause and Guestrin,
2005; Narasimhan and Bilmes, 2007; Krause et al,
2008; Kolmogorov and Zabin, 2004; Jegelka and
Bilmes, 2011) and have recently been introduced
to natural language processing for the task of docu-
ment summarization (Lin and Bilmes, 2010; Lin and
Bilmes, 2011).
3 Approach
We are given a source language (English) string eI1 =
e1, ? ? ? , ei, ? ? ? , eI and a target language (French)
string fJ1 = f1, ? ? ? , fj , ? ? ? , fJ that have to be
aligned. Define the word positions in the English
string as set E , {1, ? ? ? , I} and positions in the
French string as set F , {1, ? ? ? , J}. An alignment
A between the two word strings can then be seen as
a subset of the Cartesian product of the word posi-
tions, i.e., A ? {(i, j) : i ? E, j ? F} , V, and
V = E ? F is the ground set. For convenience, we
refer to element (i, j) ? A as an edge that connects i
and j in alignment A.
Restricting the fertility of word fj to be at most kj
is mathematically equivalent to having |A ? PEj | ?
kj , whereA ? V is an alignment and PEj = E?{j}.
Intuitively, PEj is the set of all possible edges in the
ground set that connect to j, and the cardinality of
the intersection between A and PEj indicates how
many edges in A are connected to j. Similarly, we
can impose constraints on the fertility of English
words by constraining the alignment A to satisfy
|A ? PFi | ? ki for i ? E where P
F
i = {i} ? F .
Note that either of {PEj : j ? F} or {P
F
i : i ? E}
constitute a partition of V . Therefore, alignments A
that satisfy |A ? PEj | ? kj ,?j ? F , are independent
in the partition matroidME = (V, IE) with
IE = {A ? V : ?j ? F, |A ? P
E
j | ? kj},
and alignmentsA that satisfy |A?PFi | ? ki, ?i ? E,
are independent in matroidMF = (V, IF ) with
IF = {A ? V : ?i ? E, |A ? P
F
i | ? ki}.
Suppose we have a set function f : 2V ? R+ that
measures quality (or scores) of an alignment A ? V ,
then when also considering fertility constraints, we
can treat the word alignment problem as maximizing
a set function subject to matroid constraint:
Problem 1. maxA?V f(A), subject to: A ? I,
where I is the set of independent sets of a matroid (or
it might be the set of independent sets simultaneously
in two matroids, as we shall see later).
Independence in partition matroids generalizes
the typical matching constraints for word alignment,
where each word aligns to at most one word (kj =
1,?j) in the other sentence (Matusov et al, 2004;
Taskar et al, 2005). Our matroid generalizations pro-
vide flexibility in modeling fertility, and also strate-
gies for solving the word alignment problem effi-
ciently and near-optimally. In particular, when f
is monotone submodular, near-optimal solutions for
Problem 1 can be efficiently guaranteed.
171
For example, in (Fisher et al, 1978), a simple
greedy algorithm for monotone submodular function
maximization with a matroid constraint is shown
to have a constant approximation factor. Precisely,
the greedy algorithm finds a solution A such that
f(A) ? 1m+1f(A
?) whereA? is the optimal solution
and m is number of matroid constraints. When there
is only one matroid constraint, we get an approxima-
tion factor 12 . Constant factor approximation algo-
rithms are particularly attractive since the quality of
the solution does not depend on the size of the prob-
lem, so even very large size problems do well. It is
also important to note that this is a worst case bound,
and in most cases the quality of the solution obtained
will be much better than this bound suggests.
Vondra?k (2008) shows a continuous greedy al-
gorithm followed by pipage rounding with approx-
imation factor 1 ? 1/e (? 0.63) for maximizing
a monotone submodular function subject to a ma-
troid constraint. Lee et al (2009) improve the 1m+1 -
approximation result in (Fisher et al, 1978) by show-
ing a local-search algorithm has approximation guar-
antee of 1m+ for the problem of maximizing a mono-
tone submodular function subject to m matroid con-
straints (m ? 2 and  > 0). In this paper, however,
we use the simple greedy algorithm for the sake of
efficiency. We outline our greedy algorithm for Prob-
lem 1 in Algorithm 1, which is slightly different from
the one in (Fisher et al, 1978) as in line 4 of Al-
gorithm 1, we have an additional requirement on a
such that the increment of adding a is strictly greater
than zero. This additional requirement is to main-
tain a higher precision word alignment solution. The
theoretical guarantee still holds as f is monotone ?
i.e., Algorithm 1 is a 12 -approximation algorithm for
Problem 1 (only one matroid constraint) when f is
monotone submodular.
Algorithm 1: A greedy algorithm for Problem 1.
input : A = ?, N = V .
begin1
while N 6= ? do2
a? argmaxe?N f(A ? {e})? f(A);3
if A ? {a} ? I and f(A ? {a})? f(A) > 04
then
A? A ? {a}5
N ? N \ {a}.6
end7
Algorithm 1 requires O(|V |2) evaluations of f . In
practice, the argmax in Algorithm 1 can be efficient
implemented with priority queue when f is submod-
ular (Minoux, 1978), which brings the complexity
down to O(|V | log |V |) oracle function calls.
4 Submodular Fertility
We begin this section by demonstrating that submod-
ularity arises naturally when modeling word fertility.
To do so, we borrow an example of fertility from
(Melamed, 2000). Suppose a trained model estimates
s(e1, f1) = .05, s(e1, f2) = .02 and s(e2, f2) = .01,
where s(ei, fj) represents the score of aligning ei and
fj . To find the correct alignment (e1, f1) and (e2, f2),
the competitive linking algorithm in (Melamed, 2000)
poses a one-to-one assumption to prevent choosing
(e1, f2) over (e2, f2). The one-to-one assumption,
however, limits the algorithm?s capability of handling
models with fertility larger than one. Alternatively,
we argue that the reason of choosing (e2, f2) rather
than (e1, f2) is that the benefit of aligning e1 and f2
diminishes after e1 is already aligned with f1 ? this
is exactly the property of diminishing returns, and
therefore, it is natural to use submodular functions to
model alignment scores.
To illustrate this further, we use another real
example taken from the trial set of English-French
Hansards data. The scores estimated from the data
for aligning word pairs (the, le), (the, de) and (of,
de) are 0.68, 0.60 and 0.44 respectively. Given
an English-French sentence pair: ?I have stressed
the CDC as an example of creative, aggressive
effective public ownership? and ?je le ai cite? comme
exemple de proprie?te? publique cre?atrice, dynamique
et efficace?, an algorithm that allows word fertility
larger than 1 might choose alignment (the, de) over
(of, de) since 0.68 + 0.60 > 0.68 + 0.44, regardless
the fact that the is already aligned with le. Now if
we use a submodular function to model the score of
aligning an English word to a set of French words,
we might obtain the correct alignments (the, le) and
(of, de) by incorporating the diminishing returns
property (i.e., the score gain of (the, de), which is
0.60 out of context, could diminish to something less
than 0.44 when evaluated in the context of (the, le)).
Formally, for each i in E, we define a mapping
172
?i : 2V ? 2F with
?i(A) = {j ? F |(i, j) ? A}, (1)
i.e., ?i(A) is the set of positions in F that are aligned
with position i in alignment A.
We use function fi : 2F ? R+ to represent the
benefit of aligning position i ? E to a set of positions
in F . Given score si,j of aligning i and j, we could
have, for S ? F ,
fi(S) =
?
?
?
j?S
si,j
?
?
?
, (2)
where 0 < ? ? 1, i.e., we impose a concave function
over a modular function, which produces a submod-
ular function. The value of ? determines the rate
that the marginal benefit diminishes when aligning
a word to more than one words in the other string.
Summing over alignment scores in all positions in
E, we obtain the total score of an alignment A:
f(A) =
?
i?E
fi(?i(A)), (3)
which is again, monotone submodular. By diminish-
ing the marginal benefits of aligning a word to more
than one words in the other string, f(A) encourages
the common case of low fertility while allowing fer-
tility larger than one. For instance in the aforemen-
tioned example, when ? = 12 , the score for aligning
both le and de to the is
?
0.68 + 0.60 ? 1.13, while
the score of aligning the to le and of to de is
?
0.68 +?
0.44 ? 1.49, leading to the correct alignment.
5 Experiments
We evaluated our approaches using the English-
French Hansards data from the 2003 NAACL shared
task (Mihalcea and Pedersen, 2003). This corpus con-
sists of 1.1M automatically aligned sentences, and
comes with a test set of 447 sentences, which have
been hand-aligned and are marked with both ?sure?
and ?possible? alignments (Och and Ney, 2003). Us-
ing these alignments, alignment error rate (AER) is
calculated as:
AER(A,S, P ) = 1?
|A ? S|+ |A ? P |
|A|+ |S|
(4)
where S is the set of sure gold pairs, and P is the
set of possible gold pairs. We followed the work
in (Taskar et al, 2005) and split the original test set
into 347 test examples, and 100 training examples
for parameters tuning.
In general, the score of aligning i to j can be
modeled as a function of arbitrary features. Although
parameter learning in our framework would be
another interesting topic to study, we focus herein on
the inference problem. Therefore, only one feature
(Eq. 5) was used in our experiments in order for no
feature weight learning to be required. In particular,
we estimated the score of aligning i to j as
si,j =
p(fj |ei) ? p(i|j, I)
?
j??F p(fj? |ei) ? p(i|j
?, I)
, (5)
where the translation probability p(fj |ei) and
alignment probability p(i|j, I) were obtained from
IBM model 2 trained on the 1.1M sentences. The
IBM 2 models gives an AER of 21.0% with French
as the target, in line with the numbers reported in
Och and Ney (2003) and Lacoste-Julien et al (2006).
We tested two types of partition matroid con-
straints. The first is a global matroid constraint:
A ? {A? ? V : ?j ? F, |A? ? PEj | ? b}, (6)
which restricts fertility of all words on F side to be at
most b. This constraint is denoted as FertF (A) ? b
in Table 1 for simplicity. The second type, denoted
as FertF (A) ? kj , is word-dependent:
A ? {A? ? V : ?j ? F, |A? ? PEj | ? kj}, (7)
where the fertility of word on j is restricted to be
at most kj . Here kj = max{b : pb(f) ? ?, b ?
{0, 1, . . . , 5}}, where ? is a threshold and pb(f) is
the probability that French word f was aligned to at
most b English words based on the IBM 2 alignment.
As mentioned in Section 3, matroid constraints
generalize the matching constraint. In particular,
when using two matroid constraints, FertE(A) ? 1
and FertF (A) ? 1, we have the matching constraint
where fertility for both English and French words
are restricted to be at most one. Our setup 1 (see Ta-
ble 1) uses these two constraints along with a modular
objective function, which is equivalent to the max-
imum weighted bipartite matching problem. Using
173
Table 1: AER results
ID Objective function Constraint AER(%)
1 FertF (A) ? 1, FertE(A) ? 1 21.0
2 FertF (A) ? 1 23.1
3
modular: f(A) =
P
i?E
P
j??i(A)
si,j
FertF (A) ? kj 22.1
4 FertF (A) ? 1 19.8
5
submodular: f(A) =
P
i?E
?P
j??i(A)
si,j
??
FertF (A) ? kj 18.6
Generative model (IBM 2, E?F) 21.0
Maximum weighted bipartite matching 20.9
Matching with negative penalty on fertility (ILP) 19.3
greedy algorithm to solve this problem, we get AER
21.0% (setup 1 in Table 1) ? no significant difference
compared to the AER (20.9%) achieved by the ex-
act solution (maximum weighted bipartite matching
approach), illustrating that greedy solutions are near-
optimal. Note that the bipartite matching approach
does not improve performance over IBM 2 model,
presumably because only one feature was used here.
When allowing fertility of English words to be
more than one, we see a significant AER reduction
using a submodular objective (setup 4 and 5) instead
of a modular objective (setup 2 and 3), which verifies
our claim that submodularity lends itself to modeling
the marginal benefit of growing fertility. In setup
2 and 4, while allowing larger fertility for English
words, we restrict the fertility of French words to
be most one. To allow higher fertility for French
words, one possible approach is to use constraint
FertF (A) ? 2, in which all French words are
allowed to have fertility up to 2. This approach, how-
ever, results in a significant increase of false positive
alignments since all French words tend to collect
as many matches as permitted. This issue could be
alleviated by introducing a symmetric version of
the objective function in Eq. 3 such that marginal
benefit of higher fertility of French words are also
compressed. Alternatively, we use the second type
of matroid constraint in which fertility upper bounds
of French words are word-dependent instead of
global. With ? = .8, about 10 percent of the French
words have kj equal to 2 or greater. By using the
word-dependent matroid constraint (setup 3 and 5),
AERs are reduced compared to those using global
matroid constraints. In particular, 18.6% AER is
achieved by setup 5, which significantly outperforms
the maximum weighted bipartite matching approach.
We also compare our method with model of
Lacoste-Julien et al (2006) which also allows fer-
tility larger than one by penalizing different levels of
fertility. We used si,j as an edge feature and pb(f) as
a node feature together with two additional features:
a bias feature and the bucketed frequency of the word
type. The same procedures for training and decoding
as in (Lacoste-Julien et al, 2006) were performed
where MOSEK was used as the ILP solver. As shown
in Table 1, performance of setup 5 outperforms this
model and moreover, our approach is at least 50 times
faster: it took our approach only about half a second
to align all the 347 test set sentence pairs whereas
using the ILP-based approach took about 40 seconds.
6 Discussion
We have presented a novel framework where word
alignment is framed as submodular maximization
subject to matroid constraints. Our framework
extends previous matching-based frameworks
in two respects: submodular objective functions
generalize modular (linear) objective functions, and
matroid constraints generalize matching constraints.
Moreover, such generalizations do not incur a
prohibitive computational price since submodular
maximization over matroids can be efficiently solved
with performance guarantees. As it is possible to
leverage richer forms of submodular functions that
model higher order interactions, we believe that the
full potential of our approach has yet to be explored.
Our approach might lead to novel approaches for
machine translation as well.
Acknowledgment
We thank Simon Lacoste-Julien for sharing his code
and features from (Lacoste-Julien et al, 2006), and
the anonymous reviewers for their comments. This
work was supported by NSF award 0905341.
174
References
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational linguistics,
19(2):263?311.
J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69?87. Gordon and Breach.
ML Fisher, GL Nemhauser, and LA Wolsey. 1978. An
analysis of approximations for maximizing submodular
set functions?II. Polyhedral combinatorics, pages
73?87.
S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond
submodular energies: coupling edges in graph cuts.
In Computer Vision and Pattern Recognition (CVPR),
Colorado Springs, CO, June.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
V. Kolmogorov and R. Zabin. 2004. What energy func-
tions can be minimized via graph cuts? IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
26(2):147?159.
A. Krause and C. Guestrin. 2005. Near-optimal nonmy-
opic value of information in graphical models. In Proc.
of Uncertainty in AI.
A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta.
2008. Robust submodular observation selection. Jour-
nal of Machine Learning Research, 9:2761?2801.
S. Lacoste-Julien, B. Taskar, D. Klein, and M.I. Jordan.
2006. Word alignment via quadratic assignment. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 112?119. Association for Computational
Linguistics.
J. Lee, M. Sviridenko, and J. Vondra?k. 2009. Submodular
maximization over multiple matroids via generalized
exchange properties. Approximation, Randomization,
and Combinatorial Optimization. Algorithms and Tech-
niques, pages 244?257.
H. Lin and J. Bilmes. 2010. Multi-document summariza-
tion via budgeted maximization of submodular func-
tions. In North American chapter of the Association
for Computational Linguistics/Human Language Tech-
nology Conference (NAACL/HLT-2010), Los Angeles,
CA, June.
H. Lin and J. Bilmes. 2011. A class of submodular func-
tions for document summarization. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT), Port-
land, OR, June.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation. In
Proceedings of the 20th international conference on
Computational Linguistics, page 219. Association for
Computational Linguistics.
I.D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop on Building and using parallel
texts: data driven machine translation and beyond-
Volume 3, pages 1?10. Association for Computational
Linguistics.
M. Minoux. 1978. Accelerated greedy algorithms for
maximizing submodular set functions. Optimization
Techniques, pages 234?243.
Mukund Narasimhan and Jeff Bilmes. 2004. PAC-
learning bounded tree-width graphical models. In Un-
certainty in Artificial Intelligence: Proceedings of the
Twentieth Conference (UAI-2004). Morgan Kaufmann
Publishers, July.
M. Narasimhan and J. Bilmes. 2005. A submodular-
supermodular procedure with applications to discrimi-
native structure learning. In Proc. Conf. Uncertainty in
Artifical Intelligence, Edinburgh, Scotland, July. Mor-
gan Kaufmann Publishers.
M. Narasimhan and J. Bilmes. 2007. Local search for
balanced submodular clusterings. In Twentieth Inter-
national Joint Conference on Artificial Intelligence (IJ-
CAI07), Hyderabad, India, January.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
A. Schrijver. 2003. Combinatorial optimization: polyhe-
dra and efficiency. Springer Verlag.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 73?80. Association for Com-
putational Linguistics.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
J. Vondra?k. 2008. Optimal approximation for the sub-
modular welfare problem in the value oracle model. In
Proceedings of the 40th annual ACM symposium on
Theory of computing, pages 67?74. ACM.
175
