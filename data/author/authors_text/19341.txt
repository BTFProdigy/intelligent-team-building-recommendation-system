Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255?265,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Learning Abstract Concept Embeddings from Multi-Modal Data:
Since You Probably Can?t See What I Mean
Felix Hill
Computer Laboratory
University of Cambridge
felix.hill@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
University of Cambridge
anna.korhonen@cl.cam.ac.uk
Abstract
Models that acquire semantic represen-
tations from both linguistic and percep-
tual input are of interest to researchers
in NLP because of the obvious parallels
with human language learning. Perfor-
mance advantages of the multi-modal ap-
proach over language-only models have
been clearly established when models are
required to learn concrete noun concepts.
However, such concepts are comparatively
rare in everyday language. In this work,
we present a new means of extending
the scope of multi-modal models to more
commonly-occurring abstract lexical con-
cepts via an approach that learns multi-
modal embeddings. Our architecture out-
performs previous approaches in combin-
ing input from distinct modalities, and
propagates perceptual information on con-
crete concepts to abstract concepts more
effectively than alternatives. We discuss
the implications of our results both for op-
timizing the performance of multi-modal
models and for theories of abstract con-
ceptual representation.
1 Introduction
Multi-modal models that learn semantic represen-
tations from both language and information about
the perceptible properties of concepts were orig-
inally motivated by parallels with human word
learning (Andrews et al., 2009) and evidence that
many concepts are grounded in perception (Barsa-
lou and Wiemer-Hastings, 2005). The perceptual
information in such models is generally mined di-
rectly from images (Feng and Lapata, 2010; Bruni
et al., 2012) or from data collected in psychologi-
cal studies (Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013).
By exploiting the additional information en-
coded in perceptual input, multi-modal models
can outperform language-only models on a range
of semantic NLP tasks, including modelling sim-
ilarity (Bruni et al., 2014; Kiela et al., 2014) and
free association (Silberer and Lapata, 2012), pre-
dicting compositionality (Roller and Schulte im
Walde, 2013) and concept categorization (Silberer
and Lapata, 2014). However, to date, these pre-
vious approaches to multi-modal concept learning
focus on concrete words such as cat or dog, rather
than abstract concepts, such as curiosity or loyalty.
However, differences between abstract and con-
crete processing and representation (Paivio, 1991;
Hill et al., 2013; Kiela et al., 2014) suggest that
conclusions about concrete concept learning may
not necessarily hold in the general case. In this pa-
per, we therefore focus on multi-modal models for
learning both abstract and concrete concepts.
Although concrete concepts might seem more
basic or fundamental, the vast majority of open-
class, meaning-bearing words in everyday lan-
guage are in fact abstract. 72% of the noun or
verb tokens in the British National Corpus (Leech
et al., 1994) are rated by human judges
1
as more
abstract than the noun war, for instance, a con-
cept many would already consider to be quite
abstract. Moreover, abstract concepts by defi-
nition encode higher-level (more general) princi-
ples than concrete concepts, which typically re-
side naturally in a single semantic category or do-
main (Crutch and Warrington, 2005). It is there-
fore likely that abstract representations may prove
highly applicable for multi-task, multi-domain or
transfer learning models, which aim to acquire
?general-purpose? conceptual knowledge without
reference to a specific objective or task (Collobert
and Weston, 2008; Mesnil et al., 2012).
In a recent paper, Hill et al. (2014) investigate
whether the multi-modal models cited above are
1
Contributors to the USF dataset (Nelson et al., 2004).
255
effective for learning concepts other than concrete
nouns. They observe that representations of cer-
tain abstract concepts can indeed be enhanced in
multi-modal models by combining perceptual and
linguistic input with an information propagation
step. Hill et al. (2014) propose ridge regression as
an alternative to the nearest-neighbour averaging
proposed by Johns and Jones (2012) for such prop-
agation, and show that it is more robust to changes
in the type of concept to be learned. However, both
methods are somewhat inelegant, in that they learn
separate linguistic and ?pseudo-perceptual? repre-
sentations, which must be combined via a separate
information combination step. Moreover, for the
majority of abstract concepts, the best performing
multi-modal model employing these techniques
remains less effective than conventional text-only
representation learning model.
Motivated by these observations, we introduce
an architecture for learning both abstract and con-
crete representations that generalizes the skipgram
model of Mikolov et al. (2013) from text-based to
multi-modal learning. Aspects of the model de-
sign are influenced by considering the process of
human language learning. The model moderates
the training input to include more perceptual infor-
mation about commonly-occurring concrete con-
cepts and less information about rarer concepts.
Moreover, it integrates the processes of combin-
ing perceptual and linguistic input and propagat-
ing information from concrete to abstract concepts
into a single representation update process based
on back-propagation.
We train our model on running-text language
and two sources of perceptual descriptors for con-
crete nouns: the ESPGame dataset of annotated
images (Von Ahn and Dabbish, 2004) and the
CSLB set of concept property norms (Devereux
et al., 2013). We find that our model combines in-
formation from the different modalities more ef-
fectively than previous methods, resulting in an
improved ability to model the USF free associa-
tion gold standard (Nelson et al., 2004) for con-
crete nouns. In addition, the architecture propa-
gates the extra-linguistic input for concrete nouns
to improve representations of abstract concepts
more effectively than alternative methods. While
this propagation can effectively extend the advan-
tage of the multi-modal approach to many more
concepts than simple concrete nouns, we observe
that the benefit of adding perceptual input appears
to decrease as target concepts become more ab-
stract. Indeed, for the most abstract concepts of
all, language-only models still provide the most
effective learning mechanism.
Finally, we investigate the optimum quantity
and type of perceptual input for such models. Be-
tween the most concrete concepts, which can be
effectively represented directly in the perceptual
modality, and the most abstract concepts, which
cannot, we identify a set of concepts that cannot
be represented effectively directly in the percep-
tual modality, but still benefit from perceptual in-
put propagated in the model via concrete concepts.
The motivation in designing our model and ex-
periments is both practical and theoretical. Taken
together, the empirical observations we present are
potentially important for optimizing the learning
of representations of concrete and abstract con-
cepts in multi-modal models. In addition, they of-
fer a degree of insight into the poorly understood
issue of how abstract concepts may be encoded in
human memory.
2 Model Design
Before describing how our multi-modal architec-
ture encodes and integrates perceptual informa-
tion, we first describe the underlying corpus-based
representation learning model.
Language-only Model Our multi-modal archi-
tecture builds on the continuous log-linear skip-
gram language model proposed by Mikolov et
al. (2013). This model learns lexical representa-
tions in a similar way to neural-probabilistic lan-
guage models (NPLM) but without a non-linear
hidden layer, a simplification that facilitates the
efficient learning of large vocabularies of dense
representations, generally referred to as embed-
dings (Turian et al., 2010). Embeddings learned
by the model achieve state-of-the-art performance
on several evaluations including sentence comple-
tion and analogy modelling (Mikolov et al., 2013).
For each word type w in the vocabulary V , the
model learns both a ?target-embedding? r
w
? R
d
and a ?context-embedding? r?
w
? R
d
such that,
given a target word, its ability to predict nearby
context words is maximized. The probability of
seeing context word c given target w is defined as:
p(c|w) =
e
r?
c
?r
w
?
v?V
e
r?
v
?r
w
256
  
w
n
Target Representation
Score: p(c|w)
Context Representations  Information Source
w
n+2
pw
n+2
w
n+1
pw
n+1
w
n-1
pw
n-1
w
n-2
pw
n-2
Linguistic
Text8 Corpus
Perceptual
P
ESP  
P
CSBL
Figure 1: Our multi-modal model architecture. Light boxes are elements of the original Mikolov et
al. (2013) model. For target words w
n
in the domain of P (concrete concepts), the model updates its
representations based on corpus context wordsw
n?i
, then on words p
w
n?i
in perceptual pseudo-sentences.
For w
n
not in the domain of P (abstract concepts), updates are based solely on the w
n?i
.
The model learns from a set of target-word,
context-word pairs, extracted from a corpus of
sentences as follows. In a given sentence S (of
length N ), for each position n ? N , each word
w
n
is treated in turn as a target word. An inte-
ger t(n) is then sampled from a uniform distribu-
tion on {1, . . . k}, where k > 0 is a predefined
maximum context-window parameter. The pair to-
kens {(w
n
, w
n+j
) : ?t(n) ? j ? t(n), w
i
? S}
are then appended to the training data. Thus, tar-
get/context training pairs are such that (i) only
words within a k-window of the target are selected
as context words for that target, and (ii) words
closer to the target are more likely to be selected
than those further away.
The training objective is then to maximize the
sum of the log probabilities T across of all such
examples from S and across all sentences in the
corpus, where T is defined as follows:
T =
1
N
N
?
n=1
?
?t(n)?j?t(n),j 6=0
log(p(w
n+j
|w
n
))
The model free parameters (target-embeddings
and context-embeddings of dimension d for each
word in the corpus with frequency above a certain
threshold f ) are updated according to stochastic
gradient descent and backpropation, with learning
rate controlled by Adagrad (Duchi et al., 2011).
For efficiency, the output layer is encoded as a
hierarchical softmax function based on a binary
Huffman tree (Morin and Bengio, 2005).
As with other distributional architectures, the
model captures conceptual semantics by exploit-
ing the fact that words appearing in similar lin-
guistic contexts are likely to have similar mean-
ings. Informally, the model adjusts its embeddings
to increase the ?probability? of seeing the language
in the training corpus. Since this probability in-
creases with the p(c|w), and the p(c|w) increase
with the dot product r?
c
? r
w
, the updates have the
effect of moving each target-embedding incremen-
tally ?closer? to the context-embeddings of its col-
locates. In the target-embedding space, this results
in embeddings of concept words that regularly oc-
cur in similar contexts moving closer together.
Multi-modal Extension We extend the Mikolov
et al. (2013) architecture via a simple means of in-
troducing perceptual information that aligns with
human language learning. Based on the assump-
tion that frequency in domain-general linguistic
corpora correlates with the likelihood of ?experi-
encing? a concept in the world (Bybee and Hop-
per, 2001; Chater and Manning, 2006), perceptual
information is introduced to the model whenever
designated concrete concepts are encountered in
the running-text linguistic input. This has the ef-
fect of introducing more perceptual input for com-
monly experienced concrete concepts and less in-
put for rarer concrete concepts.
To implement this process, perceptual informa-
tion is extracted from external sources and en-
coded in an associative array P, which maps (typ-
ically concrete) words w to bags of perceptual fea-
tures b(w). The construction of this array depends
on the perceptual information source; the process
for our chosen sources is detailed in Section 2.1.
Training our model begins as before on running-
text. When a sentence S
m
containing a word w in
the domain of P is encountered, the model finishes
training on S
m
and begins learning from a per-
ceptual pseudo-sentence
?
S
m
(w).
?
S
m
(w) is con-
structed by alternating the token w with a fea-
257
?S(crocodile) = Crocodile legs crocodile teeth crocodile
teeth crocodile scales crocodile green crocodile.
?
S(screwdriver) = Screwdriver handle screwdriver flat
screwdriver long screwdriver handle screwdriver head.
Figure 2: Example pseudo-sentences generated by
our model.
ture sampled at random from b(w) until
?
S
m
(w)
is the same length as S
m
(see Figure 2). Because
we want the ensuing perceptual learning process
to focus on how w relates to its perceptual prop-
erties (rather than how those properties relate to
each other), we insert multiple instances of w into
?
S
m
(w). This ensures that the majority of train-
ing cases derived from
?
S
m
(w) are instances of (w,
feature) rather than (feature, feature) pairs. Once
training on
?
S
m
(w) is complete, the model reverts
to the next ?genuine? (linguistic) sentence S
m+1
,
and the process continues. Thus, when a concrete
concept is encountered in the corpus, its embed-
ding is first updated based on language (moved in-
crementally closer to concepts appearing in sim-
ilar linguistic contexts), and then on perception
(moved incrementally closer to concepts with the
same or similar perceptual features).
For greater flexibility, we introduce a parameter
? reflecting the raw quantity of perceptual infor-
mation relative to linguistic input. When ? = 2,
two pseudo-sentences are generated and inserted
for every corpus occurrence of a token from the
domain of P. For non-integral ?, the number of
sentences inserted is b?c, and a further sentence is
added with probability ?? b?c.
In all experiments reported in the following sec-
tions we set the window size parameter k = 5 and
the minimum frequency parameter f = 3, which
guarantees that the model learns embeddings for
all concepts in our evaluation sets. While the
model learns both target and context-embeddings
for each word in the vocabulary, we conduct our
experiments with the target embeddings only. We
set the dimension parameter d = 300 as this pro-
duces high quality embeddings in the language-
only case (Mikolov et al., 2013).
2.1 Information Sources
We construct the associative array of perceptual
information P from two sources typical of those
used for multi-modal semantic models.
ESPGame Dataset The ESP-Game dataset
(ESP) (Von Ahn and Dabbish, 2004) consists of
100,000 images, each annotated with a list of lex-
ical concepts that appear in that image.
For any concept w identified in an ESP im-
age, we construct a corresponding bag of features
b(w). For each ESP image I that contains w, we
append the other concept tokens identified in I to
b(w). Thus, the more frequently a concept co-
occurs with w in images, the more its correspond-
ing lexical token occurs in b(w). The array P
ESP
in this case then consists of the (w,b(w)) pairs.
CSLB Property Norms The Centre for Speech,
Language and the Brain norms (CSLB) (Devereux
et al., 2013) is a recently-released dataset contain-
ing semantic properties for 638 concrete concepts
produced by human annotators. The CSLB dataset
was compiled in the same way as the McRae et
al. (2005) property norms used widely in multi-
modal models (Silberer and Lapata, 2012; Roller
and Schulte im Walde, 2013); we use CSLB be-
cause it contains more concepts. For each concept,
the proportion of the 30 annotators that produced
a given feature can also be employed as a measure
of the strength of that feature.
When encoding the CSLB data in P, we first
map properties to lexical forms (e.g. is green
becomes green). By directly identifying percep-
tual features and linguistic forms in this way,
we treat features observed in the perceptual data
as (sub)concepts to be acquired via the same
multi-modal input streams and stored in the same
domain-general memory as the evaluation con-
cepts. This design decision in fact corresponds
to a view of cognition that is sometimes disputed
(Fodor, 1983). In future studies we hope to com-
pare the present approach to architectures with
domain-specific conceptual memories.
For each concept w in CSLB, we then con-
struct a feature bag b(w) by appending lexical
forms to b(w) such that the count of each fea-
ture word is equal to the strength of that feature
for w. Thus, when features are sampled from
b(w) to create pseudo-sentences (as detailed pre-
viously) the probability of a feature word occur-
ring in a sentence reflects feature strength. The
array P
CSLB
then consists of all (w,b(w)) pairs.
Linguistic Input The linguistic input to all
models is the 400m word Text8 Corpus
2
of
2
From http://mattmahoney.net/dc/textdata.html
258
ESPGame CSLB
Image 1 Image 2 Crocodile Screwdriver
red wreck has 4 legs (7) has handle (28)
chihuaua cyan has tail (18) has head (5)
eyes man has jaw (7) is long (9)
little crash has scales (8) is plastic (18)
ear accident has teeth (20) is metal (28)
nose street is green (10)
small is large (10)
Table 1: Concepts identified in images in the ESP
Game (left) and features produced for concepts by
human annotators in the CSLB dataset (with fea-
ture strength, max=30).
Concept 1 Concept 2 Assoc.
abdomen (6.83) stomach (6.04) 0.566
throw (4.05) ball (6.08) 0.234
hope (1.18) glory (3.53) 0.192
egg (5.79) milk (6.66) 0.012
Table 2: Example concept pairs (with mean con-
creteness rating) and free-association scores from
the USF dataset.
Wikipedia text, split into sentences and with punc-
tuation removed.
2.2 Evaluation
We evaluate the quality of representations by how
well they reflect free association scores, an em-
pirical measure of cognitive conceptual proxim-
ity. The University of South Florida Norms
(USF) (Nelson et al., 2004) contain free associa-
tion scores for over 40,000 concept pairs, and have
been widely used in NLP to evaluate semantic rep-
resentations (Andrews et al., 2009; Feng and La-
pata, 2010; Silberer and Lapata, 2012; Roller and
Schulte im Walde, 2013). Each concept that we
extract from the USF database has also been rated
for conceptual concreteness on a Likert scale of
1-7 by at least 10 human annotators. Following
previous studies (Huang et al., 2012; Silberer and
Lapata, 2012), we measure the (Spearman ?) cor-
relation between association scores and the cosine
similarity of vector representations.
We create separate abstract and concrete con-
cept lists by ranking the USF concepts accord-
ing to concreteness and sampling at random from
the first and fourth quartiles respectively. We also
introduce a complementary noun/verb dichotomy,
Concept Type List Pairs Examples
concrete nouns 541 1418 yacht, cup
abstract nouns 100 295 fear, respect
all nouns 666 1815 fear, cup
concrete verbs 50 66 kiss, launch
abstract verbs 50 127 differ, obey
all verbs 100 221 kiss, obey
Table 3: Details the subsets of USF data used in
our evaluations, downloadable from our website.
on the intuition that information propagation may
occur differently from noun to noun or from noun
to verb (because of their distinct structural rela-
tionships in sentences). POS-tags are not assigned
as part of the USF data, so we draw the noun/verb
distinction based on the majority POS-tag of USF
concepts in the lemmatized British National Cor-
pus (Leech et al., 1994). The abstract/concrete
and noun/verb dichotomies yield four distinct con-
cept lists. For consistency, the concrete noun list
is filtered so that each concrete noun concept w
has a perceptual representation b(w) in both P
ESP
and P
CSLB
. For the four resulting concept lists
C (concrete/abstract, noun/verb), a correspond-
ing set of evaluation pairs {(w
1
, w
2
) ? USF :
w
1
, w
2
? C} is extracted (see Table 3 for details).
3 Results and Discussion
Our experiments were designed to answer four
questions, outlined in the following subsec-
tions: (1) Which model architectures perform best
at combining information pertinent to multiple
modalities when such information exists explicitly
(as common for concrete concepts)? (2) Which
model architectures best propagate perceptual in-
formation to concepts for which it does not exist
explicitly (as is common for abstract concepts)?
(3) Is it preferable to include all of the perceptual
input that can be obtained from a given source, or
to filter this input stream in some way? (4) How
much perceptual vs. linguistic input is optimal for
learning various concept types?
3.1 Combining information sources
To evaluate our approach as a method of in-
formation combination we compared its perfor-
mance on the concrete noun evaluation set against
three alternative methods. The first alternative
is simple concatenation of these perceptual vec-
tors with linguistic vectors embeddings learned
259
by the Mikolov et al. (2013) model on the Text8
Corpus. In the second alternative (proposed
for multi-modal models by Silberer and Lapata
(2012)), canonical correlation analysis (CCA)
(Hardoon et al., 2004) was applied to the vec-
tors of both modalities. CCA yields reduced-
dimensionality representations that preserve un-
derlying inter-modal correlations, which are then
concatenated. The final alternative, proposed by
Bruni et al. (2014) involves applying Singular
Value Decomposition (SVD) to the matrix of con-
catenated multi-modal representations, yielding
smoothed representations.
3
When implementing the concatenation, CCA
and SVD methods, we first encoded the percep-
tual input directly into sparse feature vectors, with
coordinates for each of the 2726 features in CSLB
and for each of the 100,000 images in ESP. This
sparse encoding matches the approach taken by
Silberer and Lapata (2012), for CCA and concate-
nation, and by Hill et al. (2014) for the ridge re-
gression method of propagation (see below).
We compare these alternatives to our proposed
model with ? = 1. In The CSLB and ESP models,
all training pseudo-sentences are generated from
the arrays P
CSLB
and P
ESP
respectively. In the
models classed as CSLB&ESP, a random choice
between P
CSLB
and P
ESP
is made every time
perceptual input is included (so that the overall
quantity of perceptual information is the same).
As shown in Figure 2 (left side), the embed-
dings learned by our model achieve a higher cor-
relation with the USF data than simple concatena-
tion, CCA and SVD regardless of perceptual input
source. With the optimal perceptual source (ESP
only), for instance, the correlation is 11% higher
that the next best alternative method, CCA.
One possible factor behind this improvement
is that, in our model, the learned representations
fully integrate the two modalities, whereas for
both CCA and the concatenation method each rep-
resentation feature (whether of reduced dimension
or not) corresponds to a particular modality. This
deeper integration may help our architecture to
overcome the challenges inherent in information
combination such as inter-modality differences in
information content and representation sparsity. It
is also important to note that Bruni et al. (2014) ap-
3
CCA was implemented using the CCA package in
R. SVD was implemented using SVDLIBC (http://
tedlab.mit.edu/
?
dr/SVDLIBC/), with truncation
factor k = 1024 as per (Bruni et al., 2014).
plied their SVD method with comparatively dense
perceptual representations extracted from images,
whereas our dataset-based perceptual vectors were
sparsely-encoded.
3.2 Propagating input to abstract concepts
To test the process of information propagation in
our model, we evaluated the learned embeddings
of more abstract concepts. We compared our
approach with two recently-proposed alternative
methods for inferring perceptual features when ex-
plicit perceptual information is unavailable.
Johns and Jones In the method of Johns and
Jones (2012), pseudo-perceptual representations
for target concepts without a perceptual repre-
sentations (uni-modal concepts) are inferred as a
weighted average of the perceptual representations
of concepts that do have such a representation (bi-
modal concepts).
In the first step of their two-step method, for
each uni-modal concept k, a quasi-perceptual rep-
resentation is computed as an average of the
perceptual representations of bi-modal concepts,
weighted by the proximity between each of these
concepts and k
k
p
=
?
c?
?
C
S(k
l
, c
l
)
?
? c
p
where
?
C is the set of bi-modal concepts, c
p
and k
p
are the perceptual representations for c and k re-
spectively, and c
l
and k
l
the linguistic representa-
tions. The exponent parameter ? reflects the learn-
ing rate.
In step two, the initial quasi-perceptual repre-
sentations are inferred for a second time, but with
the weighted average calculated over the percep-
tual or initial quasi-perceptual representations of
all other words, not just those that were originally
bi-modal. As with Johns and Jones (2012), we set
the learning rate parameter ? to be 3 in the first
step and 13 in the second.
Ridge Regression An alternative, proposed for
the present purpose by Hill et al. (2014), uses ridge
regression (Myers, 1990). Ridge regression is a
variant of least squares regression in which a reg-
ularization term is added to the training objective
to favor solutions with certain properties.
For bi-modal concepts of dimension n
p
, we ap-
ply ridge regression to learn n
p
linear functions
260
fi
: R
n
l
? R that map the linguistic represen-
tations (of dimension n
l
) to a particular percep-
tual feature i. These functions are then applied
together to map the linguistic representations of
uni-modal concepts to full quasi-perceptual repre-
sentations.
Following Hill et al. (2014), we take the Euclid-
ian l
2
norm of the inferred parameter vector as the
regularization term. This ensures that the regres-
sion favors lower coefficients and a smoother so-
lution function, which should provide better gen-
eralization performance than simple linear regres-
sion. The objective for learning the f
i
is then to
minimize
?aX ? Y
i
?
2
2
+ ?a?
2
2
where a is the vector of regression coefficients, X
is a matrix of linguistic representations and Y
i
a
vector of the perceptual feature i for the set of bi-
modal concepts.
Comparisons We applied the Johns and Jones
method and ridge regression starting from linguis-
tic embeddings acquired by the Mikolov et al.
(2013) model on the Text8 Corpus, and concate-
nated the resulting pseudo-perceptual and linguis-
tic representations. As with the implementation
of our model, the perceptual input for these alter-
native models was limited to concrete nouns (i.e.
concrete nouns were the only bi-modal concepts
in the models).
Figure 3 (right side) shows the propagation per-
formance of the three models. While the corre-
lations overall may seem somewhat low, this is
a consequence of the difficulty of modelling the
USF data. In fact, the performance of both the
language-only model and our multi-modal exten-
sion across the concept types (from 0.18 to 0.36) is
equal to or higher than previous models evaluated
on the same data (Feng and Lapata, 2010; Silberer
and Lapata, 2012; Silberer et al., 2013).
For learning representations of concrete verbs,
our approach achieves a 69% increase in perfor-
mance over the next best alternative. The perfor-
mance of the model on abstract verbs is marginally
inferior to Johns and Jones? method. Neverthe-
less, the clear advantage for concrete verbs makes
our model the best choice for learning represen-
tations of verbs in general, as shown by perfor-
mance on the set all verbs, which also includes
mixed abstract-concrete pairs.
Our model is also marginally inferior to alterna-
tive approaches in learning representations of ab-
stract nouns. However, in this case, no method
improves on the linguistic-only baseline. It is
possible that perceptual information is simply so
removed from the core semantics of these con-
cepts that they are best acquired via the linguis-
tic medium alone, regardless of learning mecha-
nism. The moderately inferior performance of our
method in such cases is likely caused by its greater
inherent inter-modal dependence compared with
methods that simply concatenate uni-modal rep-
resentations. When the perceptual signal is of
low quality, this greater inter-modal dependence
allows the linguistic signal to be obscured.
The trade-off, however, is generally higher-
quality representations when the perceptual signal
is stronger, exemplified by the fact that our pro-
posed approach outperforms alternatives on pairs
generated from both abstract and concrete nouns
(all nouns). Indeed, the low performance of the
Johns and Jones method on all nouns is strik-
ing given that: (a) It performs best on abstract
nouns (? = .282), and (b) For concrete nouns it
reverts to simple concatenation, which also per-
forms comparatively well (? = .249). The poor
performance of the Jobns and Jones method on
all nouns must therefore derive its comparisons
of mixed abstract-concrete or concrete-abstract
pairs. This suggests that the pseudo-perceptual
representations inferred by this method for ab-
stract concepts method may not be compatible
with the directly-encoded perceptual representa-
tions of concrete concepts, rendering the compar-
ison computation between items of differing con-
creteness inaccurate.
3.3 Direct representation vs. propagation
Although property norm datasets such as the
CSLB data typically consist of perceptual fea-
ture information for concrete nouns only, image-
based datasets such as ESP do contain informa-
tion on more abstract concepts, which was omit-
ted from the previous experiments. Indeed, im-
age banks such as Google Images contain millions
of photographs portraying quite abstract concepts,
such as love or war. On the other hand, encod-
ings or descriptions of abstract concepts are gen-
erally more subjective and less reliable than those
of concrete concepts (Wiemer-Hastings and Xu,
2005). We therefore investigated whether or not
it is preferable to include this additional informa-
tion as model input or to restrict perceptual input
261
0.203
0.22
0.15
0.239
0.259 0.271 0.256
0.301
0.249 0.24 0.231
0.296
0.0
0.1
0.2
0.3
0.4
CSLB ESP CSLB & ESPConcrete nouns ? information combination
Cor
rela
tion
Combination Method
Vector ConcatenationCCASVDOur Model (?=1)
0.282
0.265 0.25
0.07
0.236
0.364
0.06
0.116
0.197
0.177 0.172 0.175 0.167 0.175
0.225
0.0
0.1
0.2
0.3
0.4
abstract nouns all nouns concrete verbs abstract verbs all verbsMore abstract concepts ? information propagation (CSLB & ESP
Cor
rela
tion
Propagation Method
Johns and JonesRidge RegressionOur Model (?=1)
Figure 3: The proposed approach compared with other methods of information combination (left) and
propagation. Dashed lines indicate language-only model baseline. For brevity we include both perceptual
input sources ESP and CSLB when comparing means of propagation; results with individual information
sources were similar.
to concrete nouns as previously.
Of our evaluation sets, it was possible to con-
struct from ESP (and add to P
ESP
) representa-
tions for all of the concrete verbs, and for ap-
proximately half of the abstract verbs and abstract
nouns. Figure 4 (top), shows the performance of
a our model trained on all available perceptual in-
put versus the model in which the perceptual input
was restricted to concrete nouns.
The results reflect a clear manifestation of the
abstract/concrete distinction. Concrete verbs be-
have similarly to concrete nouns, in that they can
be effectively represented directly from perceptual
information sources. The information encoded in
these representations is beneficial to the model and
increases performance. In contrast, constructing
?perceptual? representations of abstract verbs and
abstract nouns directly from perceptual informa-
tion sources is clearly counter-productive (to the
extent that performance also degrades on the com-
bined sets all nouns and all verbs). It appears in
these cases that the perceptual input acts to ob-
scure or contradict the otherwise useful signal in-
ferred from the corpus.
As shown in the previous section, the inclusion
of any form of perceptual input inhibits the learn-
ing of abstract nouns. However, this is not the case
for abstract verbs. Our model learns higher qual-
ity representations of abstract verbs if perceptual
input is restricted to concrete nouns than if no per-
ceptual input is included at all and when percep-
tual input is included for both concrete nouns and
abstract verbs. This supports the idea of a grad-
ual scale of concreteness: The most concrete con-
cepts can be effectively represented directly in the
perceptual modality; somewhat more abstract con-
cepts cannot be represented directly in the percep-
tual modality, but have representations that are im-
proved by propagating perceptual input from con-
crete concepts via language; and the most abstract
concepts are best acquired via language alone.
3.4 Source and quantity of perceptual input
For different concept types, we tested the effect of
varying the proportion of perceptual to linguistic
input (the parameter ?). Perceptual input was re-
stricted to concrete nouns as in Sections 3.1-3.2.
As shown in Figure 4, performance on concrete
nouns improves (albeit to a decreasing degree) as
? increases. When learning concrete noun rep-
resentations, linguistic input is apparently redun-
dant if perceptual input is of sufficient quality and
quantity. For the other concept types, in each case
there is an optimal value for ? in the range .5?2,
above which perceptual input obscures the linguis-
tic signal and performance degrades. The prox-
imity of these optima to 1 suggests that for op-
timal learning, when a concrete concept is experi-
enced approximately equal weight should be given
to available perceptual and linguistic information.
4 Conclusions
Motivated by the notable prevalence of abstract
concepts in everyday language, and their likely
importance to flexible, general-purpose represen-
tation learning, we have investigated how abstract
and concrete representations can be acquired by
multi-modal models. In doing so, we presented a
simple and easy-to-implement architecture for ac-
quiring semantic representations of both types of
262
0.1
0.2
0.3
0 1 2 3 4 5?
Cor
rela
tion
Concrete Nouns
0.1
0.2
0.3
0 1 2 3 4 5?
Abstract Nouns
0.1
0.2
0.3
0 1 2 3 4 5?
Concrete Verbs
0.1
0.2
0.3
0 1 2 3 4 5?
Perceptual Input
CSLB
ESP
CSLB & 
 ESP
Text?only
Abstract Verbs
0.267 0.295
0.136
0.249
0.335 0.364 0.337
0.176
0.087
0.166 0.201
0.225
0.0
0.1
0.2
0.3
0.4
concrete 
 nouns
abstract 
 nouns
all nouns concrete 
 verbs abstract  verbs all verbsConcept Type
Cor
rela
tion
Perceptual Information Source Direct representation Propagation
Our Model ? = 1
Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual in-
formation where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of
increasing ? on correlation with USF pairs (Spearman ?) for each concept type. Horizontal dashed lines
indicate language-only model baseline.
concept from linguistic and perceptual input.
While neuro-probabilistic language models
have been applied to the problem of multi-modal
representation learning previously (Srivastava and
Salakhutdinov, 2012; Wu et al., 2013; Silberer and
Lapata, 2014) our model and experiments develop
this work in several important ways. First, we ad-
dress the problem of learning abstract concepts.
By isolating concepts of different concreteness
and part-of-speech in our evaluation sets, and sep-
arating the processes of information combination
and propagation, we demonstrate that the multi-
modal approach is indeed effective for some, but
perhaps not all, abstract concepts. In addition, our
model introduces a clear parallel with human lan-
guage learning. Perceptual input is introduced pre-
cisely when concrete concepts are ?experienced?
by the model in the corpus text, much like a lan-
guage learner experiencing concrete entities via
sensory perception.
Taken together, our findings indicate the utility
of distinguishing three concept types when learn-
ing representations in the multi-modal setting.
Type I Concepts that can be effectively repre-
sented directly in the perceptual modality. For
such concepts, generally concrete nouns or con-
crete verbs, our proposed approach provides a sim-
ple means of combining perceptual and linguistic
input. The resulting multi-modal representations
are of higher quality than those learned via other
approaches, resulting in a performance improve-
ment of over 10% in modelling free association.
Type II Concepts, including abstract verbs, that
cannot be effectively represented directly in the
perceptual modality, but whose representations
can be improved by joint learning from linguis-
tic input and perceptual information about related
concepts. Our model can effectively propagate
perceptual input (exploiting the relations inferred
from the linguistic input) from Type I concepts to
enhance the representations of Type II concepts
above the language-only baseline. Because of the
frequency of abstract concepts, such propagation
extends the benefit of the multi-modal approach to
a far wider range of language than models based
solely in the concrete domain.
Type III Concepts that are more effectively
learned via language-only models than multi-
modal models, such as abstract nouns. Neither
263
our proposed approach nor alternative propagation
methods achieve an improvement in representa-
tion quality for these concepts over the language-
only baseline. Of course, it is an empirical ques-
tion whether a multi-modal approach could ever
enhance the representation learning of these con-
cepts, one with potential implications for cognitive
theories of grounding (a topic of much debate in
psychology (Grafton, 2009; Barsalou, 2010)).
Additionally, we investigated the optimum type
and quantity of perceptual input for learning con-
cepts of different types. We showed that too much
perceptual input can result in degraded represen-
tations. For concepts of type I and II, the op-
timal quantity resulted from setting ? = 1; i.e.
whenever a concrete concept was encountered, the
model learned from an equal number of language-
based and perception-based examples. While we
make no formal claims here, such observations
may ultimately provide insight into human lan-
guage learning and semantic memory.
In future we will address the question of
whether Type III concepts can ever be enhanced
via multi-modal learning, and investigate multi-
modal models that optimally learn concepts of
each type. This may involve filtering the percep-
tual input stream for concepts according to con-
creteness, and possibly more elaborate model ar-
chitectures that facilitate distinct representational
frameworks for abstract and concrete concepts.
Acknowledgements
Thanks to the Royal Society and St John?s College
for supporting this research, and to Yoshua Bengio
and Diarmuid
?
O S?eaghdha for helpful discussions.
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.
Lawrence W Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. Grounding Cog-
nition: The Role of Perception and Action in Mem-
ory, Language, and Thought, pages 129?163.
Lawrence W Barsalou. 2010. Grounded cognition:
past, present, and future. Topics in Cognitive Sci-
ence, 2(4):716?724.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Joan L Bybee and Paul J Hopper. 2001. Frequency and
the Emergence of Linguistic Structure, volume 45.
John Benjamins Publishing.
Nick Chater and Christopher D Manning. 2006. Prob-
abilistic models of language processing and acquisi-
tion. Trends in Cognitive Sciences, 10(7):335?344.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, pages 160?167. ACM.
Sebastian J Crutch and Elizabeth K Warrington. 2005.
Abstract and concrete concepts have structurally
different representational frameworks. Brain,
128(3):615?627.
Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,
and Billi Randall. 2013. The centre for speech, lan-
guage and the brain (cslb) concept property norms.
Behavior Research Methods, pages 1?9.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Jerry A Fodor. 1983. The modularity of mind: An
essay on faculty psychology. MIT press.
Scott T Grafton. 2009. Embodied cognition and the
simulation of action to understand others. Annals of
the New York Academy of Sciences, 1156(1):97?117.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical correlation analysis:
An overview with application to learning methods.
Neural Computation, 16(12):2639?2664.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive Science.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for abstract and concrete con-
cept semantics. Transactions of the Association for
Computational Linguistics.
264
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Brendan T Johns and Michael N Jones. 2012. Per-
ceptual inference through global lexical similarity.
Topics in Cognitive Science, 4(1):103?120.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In Proceedings of the annual meeting of the
Association for Computational Linguistics. ACL.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the British National
Corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Ken McRae, George S Cree, Mark S Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547?
559.
Gr?egoire Mesnil, Yann Dauphin, Xavier Glorot, Salah
Rifai, Yoshua Bengio, Ian J Goodfellow, Erick
Lavoie, Xavier Muller, Guillaume Desjardins, David
Warde-Farley, et al. 2012. Unsupervised and trans-
fer learning challenge: a deep learning approach.
Journal of Machine Learning Research-Proceedings
Track, 27:97?110.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international Workshop on Arti-
ficial Intelligence and Statistics, pages 246?252.
Raymond H Myers. 1990. Classical and Modern
Regression with Applications, volume 2. Duxbury
Press Belmont, CA.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychology,
45(3):255.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of the annual meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with vi-
sual attributes. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria, August.
Nitish Srivastava and Ruslan Salakhutdinov. 2012.
Multimodal learning with deep boltzmann ma-
chines. In NIPS, pages 2231?2239.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on human factors in computing
systems, pages 319?326. ACM.
Katja Wiemer-Hastings and Xu Xu. 2005. Content
differences for abstract and concrete concepts. Cog-
nitive Science, 29(5):719?736.
Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,
Dayong Wang, and Chunyan Miao. 2013. On-
line multimodal deep similarity learning with ap-
plication to image retrieval. In Proceedings of the
21st ACM International Conference on Multimedia,
pages 153?162. ACM.
265
Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 11?16,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Beauty Before Age? 
Applying Subjectivity to Automatic English Adjective Ordering  
 
Felix Hill 
Dept. of Theoretical & Applied Linguistics  
and Computer Laboratory 
University of Cambridge 
Cambridge CB3 9DA, UK 
fh295@cam.ac.uk 
 
 
 
 
Abstract 
The preferred order of pre-nominal adjectives 
in English is determined primarily by seman-
tics.  Nevertheless, Adjective Ordering (AO) 
systems do not generally exploit semantic fea-
tures.  This paper describes a system that or-
ders adjectives with significantly above-
chance accuracy (73.0%) solely on the basis 
of semantic features pertaining to the cogni-
tive-semantic dimension of subjectivity.  The 
results indicate that combining such semantic 
approaches with current methods could result 
in more accurate and robust AO systems. 
1 Introduction 
As a significant body of linguistic research has 
observed (see e.g. Quirk et al (1985)), English pre-
nominal adjective strings exhibit subtle order re-
strictions.  Although example (2), below, does not 
represent a clear-cut violation of established 
grammatical principles, it would sound distinctly 
unnatural to native speakers in the majority of con-
texts, in contrast to the entirely unproblematic (1).  
(1)    He poked it with a long metal fork   
(2) ? He poked it with a metal long fork 
The problem of determining the principles that go-
vern Adjective Ordering (henceforth, AO) in Eng-
lish has been studied from a range of academic 
perspectives, including philosophy, linguistics, 
psychology and neuroscience. AO is also of inter-
est in the field of Natural Language Processing 
(NLP), since a method that consistently selects 
felicitous orders would serve to improve the output 
of language modeling and generation systems.  
Previous NLP approaches to AO infer the 
ordering of adjective combinations from instances 
of the same, or superficially similar, combinations 
in training corpora (Shaw & Hatzivassiloglou, 
1999) (Malouf, 2000), or from distributional ten-
dencies of the adjectives in multiple-modifier 
strings (Mitchell, 2009) (Dunlop, Mitchell, & 
Roark, 2010).  Such methods are susceptible to 
data sparseness, since the combinations from 
which they learn are rare in everyday language.  
By contrast, the approach taken here deter-
mines AO based on semantic features of adjec-
tives, guided by the theoretical observation that the 
cognitive notion of subjectivity governs ordering in 
the general case (Adamson, 2000).  The semantic 
features developed are each highly significant pre-
dictors of AO, and they combine to classify com-
binations with 73.0% accuracy. These preliminary 
results indicate that semantic AO systems can per-
form comparably to existing systems, and that 
classifiers exploiting semantic and direct evidence 
might surpass the current best-performing systems.  
2 Previous research 
The subtle nature of human ordering preferences 
makes AO a particularly challenging NLP task.  In 
perhaps the first specific attempt to address the 
problem, Shaw and Hatzivassiloglou (1999) apply 
a direct evidence method.  For a given adjective 
combination in the test data, their system searches 
a training corpus and selects the most frequent or-
dering of that combination.  Because there is no 
basis to determine the order of adjective combina-
tions that are not in the training data, Shaw and 
Hatzivassiloglou extend the domain of the classifi-
11
er by assuming transitivity in the order relation, 
increasing the coverage with only a small reduc-
tion in accuracy.  Nevertheless, the system remains 
highly dependent on the domain and quantity of 
training data.  For example, accuracy is 92% when 
training and test data are both within the medical 
domain but only 54% in cross-domain contexts.     
Malouf (2000) combines a direct evidence 
approach with an alternative method for extending 
the domain of his classifier.  His system infers the 
order of unseen combinations from ?similar? seen 
combinations, where similarity is defined purely in 
terms of morphological form.  The method works 
by exploiting a degree of correlation between form 
and order (e.g. capital letters indicate nominal 
modifiers, which typically occur to the right). 
Mitchell (2009) applies a less ?direct? ap-
proach, clustering adjectives based on their posi-
tion in multiple-modifier strings. Although 
Mitchell?s classifier requires no direct evidence, 
data sparseness is still an issue because the strings 
from which the system learns are relatively infre-
quent in everyday language.  Dunlop et al (2010) 
apply Multiple Sequence Alignment (MSA), a sta-
tistical technique for automatic sequence ordering, 
which, as with Malouf?s system, quantifies word-
similarity based solely on morphological features.  
Despite the greater sophistication of these more 
recent approaches, Mitchell et al (2011) showed 
that a simple n-gram (direct evidence) classifier 
trained on 170 million words of New York Times 
and Wall Street Journal text and tested on the 
Brown Corpus  (82.3% accuracy) outperforms both 
the clustering (69.0%) and MSA (81.8%) methods. 
Wulff (2003) uses Linear Discriminant 
Analysis (LDA) to quantify the effects of various 
potential AO correlates, and confirms that seman-
tic features are better predictors than morphologi-
cal and syntactic features.  The features, extracted 
from the 10-million word Spoken British National 
Corpus (BNC) and weighted by LDA, combine to 
predict unseen adjective orders with 72% accuracy.   
Wulff?s study is unique in applying seman-
tics to the problem, although her focus is theoreti-
cal and several features are implemented manually.  
The next section describes the theoretical basis for 
a fully-automated semantic approach to AO that 
could help to resolve the issues of data sparsity and 
domain dependence associated with the direct evi-
dence methods described above.  
 
2.1 The subjectivity hypothesis 
Although phonetic, morphological and syntactic 
factors influence AO in specific contexts, there is 
consensus in the theoretical literature that seman-
tics is the determining factor in the general case 
(see Quirk et al (1985) for further discussion).  
Several semantic theories of AO make use of the 
cognitive linguistic notion of subjectivity (Quirk et 
al. 1985; Hetzron, 1978; Adamson 2000).  Subjec-
tivity in this context refers to the degree to which 
an utterance can or cannot be interpreted indepen-
dently of the speaker?s perspective (Langacker, 
1991).  For example, the deictic utterance (3) is 
more subjective than (4) since its truth depends on 
the speaker?s location at the time of utterance.   
(3) James is sitting across the table 
(4) James is sitting opposite Sam 
In relation to AO, Quirk et al Hetzron and Adam-
son each support some form of the subjectivity hy-
pothesis: that more subjective modifiers generally 
occur to the left of less subjective modifiers in pre-
nominal strings.  For example, in (5) the adjective 
big tells us about the relation between the car and 
the speaker?s idea of typical car size.  This ascrip-
tion is less objectively verifiable than that of car 
color, so big occurs further from the head noun.  
The position of oncoming in (6) reflects the high 
inherent subjectivity of deictic modifiers.   
(5)  A big red Italian car        (BNC) 
(6)  An oncoming small black car (BNC) 
 
 
 
 
 
 
 
 
Figure 1:  Diachronic variation of preferred AO 
 
To illustrate a process of changing AO preferences 
that can be explained in a compelling way by the 
subjectivity hypothesis, the 1 trillion-word Google 
n-Gram Viewer was queried (Figure 1).  The two 
Frequency 
(% corpus)  
Year  
?gay young man?  
?young gay man?  
12
lines indicate the frequency of the strings ?gay 
young man? and ?young gay man? in the Corpus 
from 1950 to 2000, as the pre-eminent meaning of 
gay evolved from the subjective merry to the cate-
gorical, well-defined homosexual.  As the graph 
shows, this reduction in subjectivity has been ac-
companied by a marked increase in the tendency of 
gay to appear closer to the noun in such strings.  
3 System design 
The AO system described below applies the theo-
retical findings presented above by extracting from 
training data various subjectivity features of adjec-
tives and applying this information to classify in-
put orderings as correct or incorrect.1  System 
operation and evaluation consisted of 5 stages.  
Extracting feature profiles:  The 200 highest-
frequency adjectives in the BNC were extracted.  
Following Wulff (2003, p. 6), three items, other, 
only and very were removed from this list because 
they occur in right-branching structures.  For the 
remaining adjectives, a ?profile? of feature values 
(c.f. Table 1, below), was extracted from 24 mil-
lion words (Sections A-C) of the written BNC. 
Generating gold-standard orderings:  From the 
197 adjectives, 19,306 unordered pairs  ,  
were generated. The bigram frequencies of the 
strings  , 	 and  , 	 were then extracted 
from the 1 billion-word Google n-gram Corpus.  
From this data, the 12,000 pairs  ,  with the 
largest proportional difference in frequency be-
tween  , 	 and  , 	 were selected. 
Defining test and training sets: A set of 12,000 
ordered triples 
 ,  ,  ,	 was generated, 
where  ,	 is an indicator function taking the 
value 1 if  , 	 is the preferred ordering in the 
Google corpus and 0 if  , 	 is preferred.  
Some of the triples were re-ordered at random to 
leave an equal number of preferred and dispre-
ferred orderings in the data.  These triples were 
populated with feature profiles, to create vectors 

 , ? ,  ,  , ?  ,  ,	  
                                                           
1 The system operates on adjectival and nominal modifiers but 
not on articles, determiners, degree modifiers and other non-
adjectival pre-modifiers.   
where  is the value of the  feature of the ad-
jective , and n is the total number of features.  
The set of vectors was then randomly partitioned in 
the ratio 80:20 for training and testing respectively.  
Training the classifier:  A logistic regression was 
applied to the set of training vectors, in which the 
first 2n elements of the vectors were independent 
variables and the final element was the dependent 
variable.    Logistic regression has been shown to 
be preferable to alternatives such as Ordinary Least 
Squares and LDA for binary outcome classification 
if, as in this case, the independent variables are not 
normally distributed (Press & Wilson, 1978). 
Evaluation: Performance was determined by the 
number of pairs in the test data correctly ordered 
by the classifier.  Steps 3-5 were repeated 4 times 
(5-fold cross-validation), with the scores averaged.   
3.1 The Features 
Of the features included in the model, 
COMPARABILITY and POLARITY are shown to 
correlate with human subjectivity judgments by 
Wiebe and colleagues (see e.g. Hatzivassiloglou & 
Wiebe, 2000). The remainder are motivated by 
observations in the theoretical literature.   
MODIFIABILITY:  Gradable adjectives, such as 
hot or happy, tend to be more subjective than pro-
totypically categorical adjectives, such as square 
or black (Hetzron, 1978).  Unlike categorical ad-
jectives they admit modification by intensifiers 
(Paradis, 1997).  Therefore, the feature 
MODIFIABILITY is defined as the conditional 
probability that an adjective occurs immediately 
following an intensifier given that it occurs at all.2   
 
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 725?731,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Concreteness and Subjectivity as Dimensions of Lexical Meaning
Felix Hill
Computer Laboratory
Cambridge University
felix.hill@cl.cam.ac.uk
Anna Korhonen
Computer Laboratory
Cambridge University
anna.korhonen@cl.cam.ac.uk
Abstract
We quantify the lexical subjectivity of ad-
jectives using a corpus-based method, and
show for the first time that it correlates
with noun concreteness in large corpora.
These cognitive dimensions together influ-
ence how word meanings combine, and
we exploit this fact to achieve performance
improvements on the semantic classifica-
tion of adjective-noun pairs.
1 Introduction
Concreteness, the degree to which language has
a perceptible physical referent, and subjectivity,
the extent to which linguistic meaning depends on
the perspective of the speaker, are well established
cognitive and linguistic notions. Recent results
suggest that they could also be useful knowledge
for natural language processing systems that aim
to extract and represent the meaning of language.
Insight into concreteness can help systems to
classify adjective-noun pairs according to their se-
mantics. In the non-literal expressions kill the pro-
cess or black comedy, a verb or adjective that oc-
curs with a concrete argument in literal phrases
takes an abstract argument. Turney et al (2011)
present a supervised model that exploits this effect
to correctly classify 79% of adjective-noun pairs
as having literal or non-literal meaning.
Subjectivity analysis has already proved highly
applicable to a range of NLP applications, includ-
ing sentiment analysis, information extraction and
text categorization (Pang and Lee, 2004; Riloff
and Wiebe, 2003). For such applications, subjec-
tivity is analyzed at the phrasal or document level.
However, recent work has highlighted the applica-
tion of subjectivity analysis to lexical semantics,
for instance to the tasks of disambiguating words
according to their usage or sense (Wiebe and Mi-
halcea, 2006; Banea et al, 2014).
The importance of concreteness to NLP systems
is likely to grow with the emergence of multi-
modal semantic models (Bruni et al, 2012; Roller
and Schulte im Walde, 2013). Such models, which
learn representations from both linguistic and per-
ceptual input, outperform text-only models on a
range of evaluations. However, while multi-modal
models acquire richer representations of concrete
concepts, their ability to represent abstract con-
cepts can be weaker than text-only models (Hill et
al., 2013). A principled treatment of concreteness
is thus likely to be important if the multi-modal
approach is to prove effective on a wider range of
concepts. In a similar vein, interest in subjectiv-
ity analysis is set to grow with interest in extract-
ing sentiment and opinion from the web and social
media (Benson et al, 2011). Moreover, given that
humans seem to exploit both concreteness (Paivio,
1990) and subjectivity (Canestrelli et al, 2013)
clues when processing language, it is likely that
the same clues should benefit computational mod-
els aiming to replicate human-level performance
in this area.
In this paper, we show how concreteness and
subjectivity can be applied together to produce
performance improvements on two classification
problems: distinguishing literal and non-literal
adjective-noun pairs (Turney et al, 2011), and
classifying the modification type exhibited by
such pairs (Boleda et al, 2012). We describe an
unsupervised corpus-based method to quantify ad-
jective subjectivity, and show that it effectively
predicts the labels of a hand-coded subjectivity
lexicon. Further, we show for the first time that
adjective subjectivity correlates with noun con-
creteness in large corpora. In addition, we anal-
yse the effect of noun concreteness and adjective
subjectivity on meaning combination, illustrating
how the interaction of these dimensions enables
the accurate classification of adjective-noun pairs
according to their semantics. We conclude by dis-
725
cussing other potential applications of concrete-
ness and subjectivity to NLP.
2 Dimensions of meaning
Concreteness A large and growing body of em-
pirical evidence indicates clear differences be-
tween concrete concepts, such as donut or hot-
dog and abstract concepts, such as guilt or obesity.
Concrete words are more easily learned, remem-
bered and processed than abstract words (Paivio,
1991), while differences in brain activity (Binder
et al, 2005) and cognitive representation (Hill et
al., 2013) have also been observed. In linguistic
conctructions, concreteness appears to influence
compound and phrasal semantics (Traugott, 1985;
Bowdle and Gentner, 2005; Turney et al, 2011).
Together with the practical applications outlined in
Section 1, these facts indicate the potential value
of concreteness for models aiming to replicate hu-
man performance in language processing tasks.
While automatic methods have been proposed
for the quantification of lexical concreteness, they
each rely on dictionaries or similar hand-coded
resources (Kwong, 2008; Turney et al, 2011).
We instead extract scores from a recently released
dataset of lexical concepts rated on a 1-5 scale for
concreteness by 20 annotators in a crowdsourcing
experiment (Brysbaert et al, 2013).
1
Subjectivity Subjectivity is the degree to which
language is interpretable independently of the
speaker?s perspective (Langacker, 2002). For ex-
ample, the utterance he sits across the table is
more subjective than he sits opposite Sam as its
truth depends on the speaker?s position. Language
may also be subjective because it conveys evalua-
tions or opinions (Mihalcea et al, 2007).
Computational applications of subjectivity, in-
cluding sentiment analysis and information ex-
traction, have focused largely on phrase or doc-
ument meaning.
2
In contrast, here we present six
corpus-based features designed to quantify the lex-
ical subjectivity of adjectives. The features Com-
parability and Modifiability are identified as pre-
dictors of subjectivity by Wiebe (2000). The re-
mainder are motivated by corpus studies and/or
observations from the theoretical literature.
3
1
Available at http://crr.ugent.be/archives/1330.
2
See e.g. (Wiebe and Riloff, 2011).
3
Several of the features here were applied by Hill (2012),
to the task of ordering multiple-modifier strings.
Adverbiability: Quirk et al (1985) theorizes that
subjective adjectives tend to develop derived ad-
verbial forms, whereas more objective adjectives
do not. We thus define adverbiability as the fre-
quency of derived adverbial forms relative to the
frequency of their base form, e.g.
?
hotly
?
hot+
?
hotly
Comparability: Wiebe (2000) oberve that grad-
able are more likely to be subjective. Following
Wiebe, we note that the existence of comparative
forms for an adjective are indicative of gradabil-
ity. We thus define comparability as the frequency
of comparative or superlative forms relative to the
frequency of the base form, e.g.
?
hotter +
?
hottest
?
hot+
?
hotter +
?
hottest
LeftTendency: Adamson (2000) proposes that
more subjective adjectives typically occur furthest
from the noun in multiple-modifier strings such as
(hot crossed buns). We consequently extract the
LeftTendency of our adjectives, defined as the fre-
quency of occurrence as the leftmost of two ad-
jectives as a proportion of the overall frequency of
occurrence in multiple-modifier strings.
Modifiability: Another characteristic of gradable
adjectives noted by Wiebe (2000) is that they ad-
mit degree modifiers (very/quite delicious). We
therefore extract the relative frequency of occur-
rence with one of a hand-coded list of English de-
gree modifiers.
Predicativity: Bolinger (1967) proposed that sub-
jective adjectives occur in predicative construc-
tions (the cake is sweet), rather than attribu-
tive constructions (the German capital) more fre-
quently than objective adjectives. We therefore ex-
tract the relative frequency of occurrence in such
constructions.
Non-nominality: Many adjectives also function
as nouns (sweet cake vs. (boiled sweet). Un-
like nouns, many adjectives are inherently subjec-
tive, and the number of adjectives in texts corre-
lates with human judgements of their subjectivity
(Hatzivassiloglou and Wiebe, 2000). We there-
fore extract the frequency with which concepts are
tagged as adjectives relative to as nouns, on the
726
assumption that ?pure? adjectives are on average
more subjective than nominal-style adjectives.
Concreteness meets Subjectivity Demonstra-
ble commonalities in how different people per-
ceive the physical world suggest that concrete lan-
guage may be more objective than abstract lan-
guage (Langacker, 1997). Intuitively, adjectives
ascribing physical properties (wooden shed) are
more objective than those conveying abstract traits
(suspicious man). Indeed, in certain cases the
original, apparently objective, senses of polyse-
mous adjectives are not modifiable (very wooden
shed?), while their more abstract sense extensions
are (very wooden personality).
Motivated by these observations, in the follow-
ing sections we test two hypotheses. (1) Subjec-
tive / objective adjectives are more likely to mod-
ify abstract / concrete nouns respectively. (2) Sub-
jectivity and concreteness can predict aspects of
how adjective and noun concepts combine.
3 Analysis
In addressing (1), we extracted the 2,000 highest-
frequency nouns from the Brysbaert et al (2013)
concreteness dataset. We denote by CONC(n)
the mean concreteness rating for noun n. For the
24,908 adjectives that occur in some adjective-
noun pair with one of these nouns in the British
National Corpus (BNC) (Leech et al, 1994), we
extracted subjectivity features from the Google
Books Corpus (Goldberg and Orwant, 2013).
Since each of the six features takes values on [0, 1],
we define the overall subjectivity of an adjective a
with feature vector s
a
= [s
a
1
. . . s
a
6
] as
SUBJ(a) =
6
?
i=1
s
a
i
.
To verify the quality of our subjectivity features,
we measured their performance as predictors in a
logistic regression classifying the 3,250 adjectives
labelled as subjective or not in the Wilson et al
(2005) lexicon.
4
The combination of all features
produced an overall classifiction accuracy of 79%.
The performance of individual features as predic-
tors in isolation is shown in Figure 1 (top).
We first tested the relationship between con-
creteness and subjectivity with a correlation anal-
ysis over noun concepts. For each noun n we de-
4
Available at http://mpqa.cs.pitt.edu/
79.1
74.2
72.8
70.1
69.4
71.2
73.7
Combined
Adverbiability
Modifiability
Predicativity
Comparability
LeftTendency
NonNominality
0 20 40 60 80Feature Prediction Performance (% correct)
?0.1
0.0
0.1
0.2
0.3
0.4
2 4 6CONC (noun concreteness)S
UB
J (ad
jecti
ve s
ubje
ctivi
ty)
Figure 1: Top: Performance of features in
predicting subjectivity labels from the Wilson
et al (2005) lexicon. Bottom: Concreteness-
subjectivity correlation in adj-noun pairs.
a SUBJ(a) n CONC(n)
flashy 1.98 umbrella 5
honest 1.63 flask 5
good 1.59 automobile 5
Siberian 6.9? 10
?
4 virtue 1.49
interglacial 6.3? 10
?
4 pride 1.46
Soviet 1.9? 10
?
4 hope 1.18
Table 1: The most and least subjective adjectives
and most and least concrete nouns in our data.
fined its subjectivity profile as the mean of the sub-
jectivity vectors of its modifying adjectives
SUBJpfl(n) =
1
|A
n
|
?
a?A
n
s
a
where the bag A
n
contains an adjective a for each
occurrence of the pair (a, n) in the BNC. As hy-
pothesized, CONC(n) was a significant predictor
of the magnitude of the subjectivity profile (Pear-
son r = ?0.421, p < 0.01). This effect is illus-
trated in Figure 1 (bottom).
To explore the relationship between concrete-
ness, subjectivity and meaning, we plotted the
20,000 highest frequency (a, n) pairs in the BNC
in the CONC-SUBJ semantic space (Figure 2,
top). In addition, to examine the effect of con-
creteness alone on adjective-noun semantics, we
727
(a, n) ? (a, n) ?
white hope 4.61 mature attitude 4.05
fresh hope 4.34 injured pride 4.03
male pride 4.28 black mood 3.99
wild hope 4.06 white spirit 3.93
Table 2: The eight pairs with highest ? =
ExpCONC(a)? CONC(n) in our data.
defined a new adjective feature
ExpCONC(a) =
1
|N
a
|
?
n?N
a
CONC(n)
where the bag N
a
contains noun n for each occur-
rence of the pair (a, n) in the BNC. Figure 2 (bot-
tom) illustrates the the CONC-ExpCONC space.
In both spaces, the extremities reflect particular
meaning combination types. Pairs in the bottom-
left region of the CONC-SUBJ space (objective
adjectives with abstract nouns, such as green pol-
itics) seem to exhibit a non-literal, or at least non
prototypical modification type. In contrast, for
pairs in the objective+concrete corner, the adjec-
tives appear to perform a classifying or categoriz-
ing function (baptist minister).
In the CONC-ExpCONC space, on the diag-
onal, where noun-concreteness is ?as expected?,
pairings appear to combine literally. Away from
the diagonal, meaning composition is less pre-
dictable. In the top-left, where ExpCONC is
greater than CONC, the combinations are almost
all non-literal, as shown in Table 2.
In this section we have outlined a set of corpus
features that, taken together, enable effective ap-
proximation of adjective subjectivity. The results
of our analyses also demonstrate a clear interac-
tion between subjectivity and concreteness scores
for nouns attributed by human raters. Specifi-
cally, objective adjectives are more likely to mod-
ify concrete nouns and subjective adjectives are
more likely to modify abstract nouns. Qualita-
tive investigations further suggest the interaction
between these dimensions to be useful in the se-
mantic characterization of adjective-noun pairs, a
proposition we test formally in the next section.
4 Evaluation
We evaluate the potential of our adjective subjec-
tivity features, together with noun concreteness,
to predict adjective-noun semantics, based on two
existing classification tasks.
middle:way
mass:democracy
green:politics
hardy:soullyric:fantasy
soviet:attitude
public:moneypop:music
relative:peacelow:intelligence
expensive:carserious:condition
honest:man
baptist:ministerhindu:temple
delicious:food
sincere:persondutiful:son
?1
0
1
2 4 6CONC (noun concreteness)
SUB
J (ad
jectiv
e s
ubje
ctivit
y)
easy:meat
naked:truthtired:mind
chief:objectivehot:issue fresh:air
blue:sky
public:information local:prisonfuture:home
expeditionary:force
regular:contactfunny:shape
unsettling:effect
diplomatic:bag
powdered:milk
salty:water
2
3
4
5
6
7
2 4 6CONC (noun concreteness)Ex
pCO
NC (a
djecti
ve ex
pect
ed c
oncr
eten
ess)
Figure 2: Adjective-noun pairs in two semantic
spaces. Selected pairs are labelled for illustration,
italics indicate non-literal meaning combinations.
4.1 Non-literal Composition Task
To evaluate their model of lexical concreteness,
Turney et al (2011) developed a list of 100 com-
mon adjective-noun pairs classified as either deno-
tative (used literally) or connotative (non-literal)
by five annotators. Using an identical supervised
learning procedure to Turney et al (logistic re-
gression with 10-fold cross-validation), we test
whether our lexical representations based on sub-
jectivity and concreteness convey sufficient infor-
mation to perform the same classification.
4.2 Modification-type Classification
Boleda et al (2012) introduce a set of 370
adjective-noun pairs grouped into modification
types by human judges. Because a red car is
both a car and red, the pair is classed as intersec-
tive, whereas dark humour, which is not literally
dark, is classed as subsective. To create a distinct
but analogous binary categorization problem to the
composition task, we filtered out pairs not unani-
mously allocated to either class. We again aim to
classify the remaining 211 intersective and 93 sub-
sective pairs with a logistic regression.
728
Feature type Composition Modification
Baseline 55.0 69.4
Concreteness 83.0 72.7
Subjectivity 64.0 70.4
Combined 85.0 75.0
Turney et al 79.0 -
Table 3: Prediction accuracy (%) of models with
different features on the two tasks. The baseline
method allocates all test pairs to the majority class.
4.3 Results
Models were trained with concreteness features
(CONC and ExpCONC), subjectivity features
(SUBJ and SUBJpfl) and the combination of both
types (Combined). The performance of each
model is presented in Table 3, along with a base-
line score reflecting the strategy of allocating all
pairs to the largest class.
On the non-literal composition task, the con-
creteness (83.0) and combined (85.0) models out-
perform that of Turney et al (79.0). The concrete-
ness model performance represents further confir-
mation of the link between concreteness and com-
position. The improvement of this model over
Turney et al (2011) is perhaps to be expected,
since our model exploits the wide scope of the
new Brysbaert et al (2013) crowdsourced data
whereas Turney et al infer concreteness scores
from a smaller training set. Notably, our combined
model improves on the concreteness-only model,
confirming that the interaction of concreteness and
subjectivity provides additional information perti-
nent to meaning composition.
The modification-type task has no performance
benchmark since Boleda et al (2012) do not use
their data for classification. Although all models
improve on the majority-class baseline, the com-
bined model was again most effective. Additive
improvement over the baseline in each case was
lower than for the composition task, which may
reflect the greater subtlety inherent in the sub-
sective/intersective classification. Indeed, inter-
annotator agreement for this goldstandard (Co-
hen?s ? = 0.87) was lower than for the composi-
tion task (0.95), implying a less cognitively salient
distinction.
5 Conclusion
We have shown that objective adjectives are most
likely to modify concrete nouns, and that non-
literal combinations can emerge when this princi-
ple is violated (Section 3). Indeed, the occurrence
of an adjective with a more abstract noun than
those it typically modifies is a strikingly consistent
indicator of metaphoricity (Table 2). In addition,
we showed that both concreteness and subjectivity
improve the automatic classification of adjective-
noun pairs according to compositionality or mod-
ification type (Section 4). Importantly, a classifier
with both subjectivity and concreteness features
outperforms concreteness-only classifiers, includ-
ing those proposed in previous work.
The results underline the relevance of both sub-
jectivity and concreteness to lexical and phrasal
semantics, and their application to language pro-
cessing tasks. We hypothesize that concreteness
and subjectivity are fundamental to human lan-
guage processing because language is precisely
the conveyance of information about the world
from one party to another. In decoding this sig-
nal, it is clearly informative to understand to what
extent the information refers directly to the world,
and also to what extent it reports a fact versus an
opinion. We believe these dimensions will ulti-
mately prove essential for computational systems
aiming to replicate human performance in inter-
preting language. As the results suggest, they may
be particularly important for capturing the intrica-
cies of semantic composition and thus extending
representations beyond the lexeme.
Of course, two dimensions alone are not suf-
ficient to reflect all of the subtleties of adjective
and noun semantics. For instance, our model clas-
sifies white spirit, a transparent cleaning product,
as non-literal, since the lexical concreteness score
does not allow for strong noun polysemy. Further,
it makes no allowance for wider sentential context,
which can be an important clue to modification
type in such cases.
We aim to address these limitations in future
work by integrating subjectivity and concreteness
with conventially acquired semantic representa-
tions, and, ultimately, models that integrate input
corresponding to the perceptual modalities.
6 Acknowledgements
The authors are supported by St John?s College,
Cambridge and The Royal Society.
729
References
Sylvia Adamson. 2000. A lovely little example. In
Olga Fischer, Annette Rosenbach, and Deiter Stein,
editors, Pathways of change: Grammaticalization in
English. John Benjamins.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2014. Sense-level subjectivity in a multilingual set-
ting. Computer Speech & Language, 28(1):7?19.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
Jeffrey R Binder, Chris F Westbury, Kristen A McK-
iernan, Edward T Possing, and David A Medler.
2005. Distinct brain systems for processing concrete
and abstract concepts. Journal of Cognitive Neuro-
science, 17(6):905?917.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1223?1233. Association for Computational Linguis-
tics.
Dwight Bolinger. 1967. Adjectives in english: attribu-
tion and predication. Lingua, 18:1?34.
Brian F Bowdle and Dedre Gentner. 2005. The career
of metaphor. Psychological review, 112(1):193.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known English word lemmas. Be-
havior research methods, pages 1?8.
Anneloes R Canestrelli, Willem M Mak, and Ted JM
Sanders. 2013. Causal connectives in discourse
processing: How differences in subjectivity are re-
flected in eye movements. Language and Cognitive
Processes, 28(9):1394?1413.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics, Associa-
tion for Computational Linguistics, pages 241?247.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Janyce M Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of the 18th
conference on Computational linguistics-Volume 1,
pages 299?305. Association for Computational Lin-
guistics.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013.
Concreteness and corpora: A theoretical and prac-
tical analysis. ACL 2013 Workshop on Cognitive
Modelling and Computational Linguistics, CMCL
2013, page 75.
Felix Hill. 2012. Beauty before age? Applying sub-
jectivity to automatic english adjective ordering. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies:
Student Research Workshop, pages 11?16. Associa-
tion for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Ronald W Langacker. 1997. Consciousness, construal
and subjectivity. Language structure, discourse and
the access to consciousness. Advances in Conscious-
ness Research. John Benjamins, pages 49?57.
Ronald W Langacker. 2002. Deixis and subjectiv-
ity. In Frank Brisard, editor, Grounding: The epis-
temic footing of deixis and reference, pages 1?28.
De Gruyter.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of ACL, pages 622?628. As-
sociation for Computational Linguistics.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Annual Meeting of
the Association for Computational Linguistics, vol-
ume 45, page 976.
Allan Paivio. 1990. Mental Representations: A Dual
Coding Approach. Oxford University Press.
Allan Paivio. 1991. Dual coding theory: Retrospect
and current status. Canadian Journal of Psychol-
ogy/Revue canadienne de psychologie, 45(3):255.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Randolph Quirk, David Crystal, and Pearson Educa-
tion. 1985. A Comprehensive Grammar of the
English Language, volume 397. Cambridge Univ
Press.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 conference on Empirical methods in
730
natural language processing, pages 105?112. Asso-
ciation for Computational Linguistics.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA. Association for Computational
Linguistics.
Elizabeth C Traugott. 1985. On regularity in semantic
change. Journal of literary semantics, 14(3):155?
173.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 1065?1072. Asso-
ciation for Computational Linguistics.
Janyce Wiebe and Ellen Riloff. 2011. Finding mutual
benefit between subjectivity analysis and informa-
tion extraction. Affective Computing, IEEE Trans-
actions on, 2(4):175?191.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347?354. Association for Computational Linguis-
tics.
731
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835?841,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Multi-Modal Representations Using Image Dispersion:
Why Less is Sometimes More
Douwe Kiela*, Felix Hill*, Anna Korhonen and Stephen Clark
University of Cambridge
Computer Laboratory
{douwe.kiela|felix.hill|anna.korhonen|stephen.clark}@cl.cam.ac.uk
Abstract
Models that learn semantic representations
from both linguistic and perceptual in-
put outperform text-only models in many
contexts and better reflect human concept
acquisition. However, experiments sug-
gest that while the inclusion of perceptual
input improves representations of certain
concepts, it degrades the representations
of others. We propose an unsupervised
method to determine whether to include
perceptual input for a concept, and show
that it significantly improves the ability of
multi-modal models to learn and represent
word meanings. The method relies solely
on image data, and can be applied to a va-
riety of other NLP tasks.
1 Introduction
Multi-modal models that learn semantic concept
representations from both linguistic and percep-
tual input were originally motivated by parallels
with human concept acquisition, and evidence that
many concepts are grounded in the perceptual sys-
tem (Barsalou et al, 2003). Such models extract
information about the perceptible characteristics
of words from data collected in property norming
experiments (Roller and Schulte im Walde, 2013;
Silberer and Lapata, 2012) or directly from ?raw?
data sources such as images (Feng and Lapata,
2010; Bruni et al, 2012). This input is combined
with information from linguistic corpora to pro-
duce enhanced representations of concept mean-
ing. Multi-modal models outperform language-
only models on a range of tasks, including mod-
elling conceptual association and predicting com-
positionality (Bruni et al, 2012; Silberer and Lap-
ata, 2012; Roller and Schulte im Walde, 2013).
Despite these results, the advantage of multi-
modal over linguistic-only models has only been
demonstrated on concrete concepts, such as
chocolate or cheeseburger, as opposed to abstract
concepts such as such as guilt or obesity. Indeed,
experiments indicate that while the addition of
perceptual input is generally beneficial for repre-
sentations of concrete concepts (Hill et al, 2013a;
Bruni et al, 2014), it can in fact be detrimental
to representations of abstract concepts (Hill et al,
2013a). Further, while the theoretical importance
of the perceptual modalities to concrete represen-
tations is well known, evidence suggests this is not
the case for more abstract concepts (Paivio, 1990;
Hill et al, 2013b). Indeed, perhaps the most influ-
ential characterization of the abstract/concrete dis-
tinction, the Dual Coding Theory (Paivio, 1990),
posits that concrete representations are encoded
in both the linguistic and perceptual modalities
whereas abstract concepts are encoded only in the
linguistic modality.
Existing multi-modal architectures generally
extract and process all the information from their
specified sources of perceptual input. Since per-
ceptual data sources typically contain information
about both abstract and concrete concepts, such in-
formation is included for both concept types. The
potential effect of this design decision on perfor-
mance is significant because the vast majority of
meaning-bearing words in everyday language cor-
respond to abstract concepts. For instance, 72% of
word tokens in the British National Corpus (Leech
et al, 1994) were rated by contributors to the Uni-
versity of South Florida dataset (USF) (Nelson et
al., 2004) as more abstract than the noun war, a
concept that many would consider quite abstract.
In light of these considerations, we propose
a novel algorithm for approximating conceptual
concreteness. Multi-modal models in which per-
ceptual input is filtered according to our algorithm
learn higher-quality semantic representations than
previous approaches, resulting in a significant per-
formance improvement of up to 17% in captur-
835
ing the semantic similarity of concepts. Further,
our algorithm constitutes the first means of quan-
tifying conceptual concreteness that does not rely
on labor-intensive experimental studies or annota-
tors. Finally, we demonstrate the application of
this unsupervised concreteness metric to the se-
mantic classification of adjective-noun pairs, an
existing NLP task to which concreteness data has
proved valuable previously.
2 Experimental Approach
Our experiments focus on multi-modal models
that extract their perceptual input automatically
from images. Image-based models more natu-
rally mirror the process of human concept acquisi-
tion than those whose input derives from exper-
imental datasets or expert annotation. They are
also more scalable since high-quality tagged im-
ages are freely available in several web-scale im-
age datasets.
We use Google Images as our image source,
and extract the first n image results for each con-
cept word. It has been shown that images from
Google yield higher-quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011). Other potential sources, such as
ImageNet (Deng et al, 2009) or the ESP Game
Dataset (Von Ahn and Dabbish, 2004), either do
not contain images for abstract concepts or do not
contain sufficient images for the concepts in our
evaluation sets.
2.1 Image Dispersion-Based Filtering
Following the motivation outlined in Section 1, we
aim to distinguish visual input corresponding to
concrete concepts from visual input correspond-
ing to abstract concepts. Our algorithm is moti-
vated by the intuition that the diversity of images
returned for a particular concept depends on its
concreteness (see Figure 1). Specifically, we an-
ticipate greater congruence or similarity among a
set of images for, say, elephant than among im-
ages for happiness. By exploiting this connection,
the method approximates the concreteness of con-
cepts, and provides a basis to filter the correspond-
ing perceptual information.
Formally, we propose a measure, image disper-
sion d of a concept word w, defined as the aver-
age pairwise cosine distance between all the image
representations { ~w
1
. . . ~w
n
} in the set of images
for that concept:
Figure 1: Example images for a concrete (elephant
? little diversity, low dispersion) and an abstract
concept (happiness ? greater diversity, high dis-
persion).
Figure 2: Computation of PHOW descriptors us-
ing dense SIFT for levels l = 0 to l = 2 and the
corresponding histogram representations (Bosch
et al, 2007).
d(w) =
1
2n(n? 1)
?
i<j?n
1?
~w
i
? ~w
j
| ~w
i
|| ~w
j
|
(1)
We use an average pairwise distance-based met-
ric because this emphasizes the total variation
more than e.g. the mean distance from the cen-
troid. In all experiments we set n = 50.
Generating Visual Representations Visual
vector representations for each image were ob-
tained using the well-known bag of visual words
(BoVW) approach (Sivic and Zisserman, 2003).
BoVW obtains a vector representation for an
836
image by mapping each of its local descriptors
to a cluster histogram using a standard clustering
algorithm such as k-means.
Previous NLP-related work uses SIFT (Feng
and Lapata, 2010; Bruni et al, 2012) or SURF
(Roller and Schulte im Walde, 2013) descriptors
for identifying points of interest in an image,
quantified by 128-dimensional local descriptors.
We apply Pyramid Histogram Of visual Words
(PHOW) descriptors, which are particularly well-
suited for object categorization, a key component
of image similarity and thus dispersion (Bosch et
al., 2007). PHOW is roughly equivalent to run-
ning SIFT on a dense grid of locations at a fixed
scale and orientation and at multiple scales (see
Fig 2), but is both more efficient and more accu-
rate than regular (dense) SIFT approaches (Bosch
et al, 2007). We resize the images in our dataset
to 100x100 pixels and compute PHOW descriptors
using VLFeat (Vedaldi and Fulkerson, 2008).
The descriptors for the images were subse-
quently clustered using mini-batch k-means (Scul-
ley, 2010) with k = 50 to obtain histograms of
visual words, yielding 50-dimensional visual vec-
tors for each of the images.
Generating Linguistic Representations We
extract continuous vector representations (also of
50 dimensions) for concepts using the continu-
ous log-linear skipgram model of Mikolov et al
(2013a), trained on the 100M word British Na-
tional Corpus (Leech et al, 1994). This model
learns high quality lexical semantic representa-
tions based on the distributional properties of
words in text, and has been shown to outperform
simple distributional models on applications such
as semantic composition and analogical mapping
(Mikolov et al, 2013b).
2.2 Evaluation Gold-standards
We evaluate models by measuring the Spearman
correlation of model output with two well-known
gold-standards reflecting semantic proximity ? a
standard measure for evaluating the quality of rep-
resentations (see e.g. Agirre et al (2009)).
To test the ability of our model to capture
concept similarity, we measure correlations with
WordSim353 (Finkelstein et al, 2001), a selec-
tion of 353 concept pairs together with a similar-
ity rating provided by human annotators. Word-
Sim has been used as a benchmark for distribu-
tional semantic models in numerous studies (see
e.g. (Huang et al, 2012; Bruni et al, 2012)).
As a complementary gold-standard, we use the
University of South Florida Norms (USF) (Nelson
et al, 2004). This dataset contains scores for free
association, an experimental measure of cognitive
association, between over 40,000 concept pairs.
The USF norms have been used in many previous
studies to evaluate semantic representations (An-
drews et al, 2009; Feng and Lapata, 2010; Sil-
berer and Lapata, 2012; Roller and Schulte im
Walde, 2013). The USF evaluation set is partic-
ularly appropriate in the present context because
concepts in the dataset are also rated for concep-
tual concreteness by at least 10 human annotators.
We create a representative evaluation set of USF
pairs as follows. We randomly sample 100 con-
cepts from the upper quartile and 100 concepts
from the lower quartile of a list of all USF con-
cepts ranked by concreteness. We denote these
sets C, for concrete, and A for abstract respec-
tively. We then extract all pairs (w
1
, w
2
) in the
USF dataset such that bothw
1
andw
2
are inA?C.
This yields an evaluation set of 903 pairs, of which
304 are such that w
1
, w
2
? C and 317 are such
that w
1
, w
2
? A.
The images used in our experiments and
the evaluation gold-standards can be down-
loaded from http://www.cl.cam.ac.uk/
?
dk427/dispersion.html.
3 Improving Multi-Modal
Representations
We apply image dispersion-based filtering as fol-
lows: if both concepts in an evaluation pair have
an image dispersion below a given threshold, both
the linguistic and the visual representations are in-
cluded. If not, in accordance with the Dual Cod-
ing Theory of human concept processing (Paivio,
1990), only the linguistic representation is used.
For both datasets, we set the threshold as the
median image dispersion, although performance
could in principle be improved by adjusting this
parameter. We compare dispersion filtered rep-
resentations with linguistic, perceptual and stan-
dard multi-modal representations (concatenated
linguistic and perceptual representations). Sim-
ilarity between concept pairs is calculated using
cosine similarity.
As Figure 3 shows, dispersion-filtered multi-
modal representations significantly outperform
837
0.145
0.532
0.477
0.542
0.189
0.229
0.203
0.247
0.0
0.1
0.2
0.3
0.4
0.5
Similarity ? WordSim 353 Free association ? USF (903)Evaluation Set
Cor
rela
tion
Model Representations
Linguistic onlyImage onlyStandard multi?modalDispersion filtered
Figure 3: Performance of conventional multi-
modal (visual input included for all concepts) vs.
image dispersion-based filtering models (visual in-
put only for concepts classified as concrete) on the
two evaluation gold-standards.
standard multi-modal representations on both
evaluation datasets. We observe a 17% increase in
Spearman correlation on WordSim353 and a 22%
increase on the USF norms. Based on the corre-
lation comparison method of Steiger (1980), both
represent significant improvements (WordSim353,
t = 2.42, p < 0.05; USF, t = 1.86, p < 0.1). In
both cases, models with the dispersion-based filter
also outperform the purely linguistic model, which
is not the case for other multi-modal approaches
that evaluate on WordSim353 (e.g. Bruni et al
(2012)).
4 Concreteness and Image Dispersion
The filtering approach described thus far improves
multi-modal representations because image dis-
persion provides a means to distinguish concrete
concepts from more abstract concepts. Since re-
search has demonstrated the applicability of con-
creteness to a range of other NLP tasks (Turney et
al., 2011; Kwong, 2008), it is important to exam-
ine the connection between image dispersion and
concreteness in more detail.
4.1 Quantifying Concreteness
To evaluate the effectiveness of image dispersion
as a proxy for concreteness we evaluated our al-
gorithm on a binary classification task based on
the set of 100 concrete and 100 abstract concepts
A?C introduced in Section 2. By classifying con-
0.184
0.257
0.29
0.054
0.189
0.167
0.0
0.1
0.2
0.3
0.4
'concrete' pairs (304) 'abstract' pairs (317)Concept Type
Cor
rela
tion
Representation Modality
LinguisticVisualLinguistic+Visual
Figure 4: Visual input is valuable for representing
concepts that are classified as concrete by the im-
age dispersion algorithm, but not so for concepts
classified as abstract. All correlations are with the
USF gold-standard.
cepts with image dispersion below the median as
concrete and concepts above this threshold as ab-
stract we achieved an abstract-concrete prediction
accuracy of 81%.
While well-understood intuitively, concreteness is
not a formally defined notion. Quantities such as
the USF concreteness score depend on the sub-
jective judgement of raters and the particular an-
notation guidelines. According to the Dual Cod-
ing Theory, however, concrete concepts are pre-
cisely those with a salient perceptual representa-
tion. As illustrated in Figure 4, our binary clas-
sification conforms to this characterization. The
importance of the visual modality is significantly
greater when evaluating on pairs for which both
concepts are classified as concrete than on pairs of
two abstract concepts.
Image dispersion is also an effective predic-
tor of concreteness on samples for which the ab-
stract/concrete distinction is less clear. On a differ-
ent set of 200 concepts extracted by random sam-
pling from the USF dataset stratified by concrete-
ness rating (including concepts across the con-
creteness spectrum), we observed a high correla-
tion between abstractness and dispersion (Spear-
man ? = 0.61, p < 0.001). On this more diverse
sample, which reflects the range of concepts typi-
cally found in linguistic corpora, image dispersion
is a particularly useful diagnostic for identifying
838
Concept Image Dispersion Conc. (USF)
shirt .488 6.05
bed .495 5.91
knife .560 6.08
dress .578 6.59
car .580 6.35
ego 1.000 1.93
nonsense .999 1.90
memory .999 1.78
potential .997 1.90
know .996 2.70
Table 1: Concepts with highest and lowest image
dispersion scores in our evaluation set, and con-
creteness ratings from the USF dataset.
the very abstract or very concrete concepts. As
Table 1 illustrates, the concepts with the lowest
dispersion in this sample are, without exception,
highly concrete, and the concepts of highest dis-
persion are clearly very abstract.
It should be noted that all previous approaches
to the automatic measurement of concreteness rely
on annotator ratings, dictionaries or manually-
constructed resources. Kwong (2008) proposes
a method based on the presence of hard-coded
phrasal features in dictionary entries correspond-
ing to each concept. By contrast, S?anchez et al
(2011) present an approach based on the position
of word senses corresponding to each concept in
the WordNet ontology (Fellbaum, 1999). Turney
et al (2011) propose a method that extends a large
set of concreteness ratings similar to those in the
USF dataset. The Turney et al algorithm quanti-
fies the concreteness of concepts that lack such a
rating based on their proximity to rated concepts
in a semantic vector space. In contrast to each of
these approaches, the image dispersion approach
requires no hand-coded resources. It is therefore
more scalable, and instantly applicable to a wide
range of languages.
4.2 Classifying Adjective-Noun Pairs
Finally, we explored whether image dispersion
can be applied to specific NLP tasks as an effec-
tive proxy for concreteness. Turney et al (2011)
showed that concreteness is applicable to the clas-
sification of adjective-noun modification as either
literal or non-literal. By applying a logistic regres-
sion with noun concreteness as the predictor vari-
able, Turney et al achieved a classification accu-
racy of 79% on this task. This model relies on sig-
nificant supervision in the form of over 4,000 hu-
man lexical concreteness ratings.
1
Applying im-
age dispersion in place of concreteness in an iden-
tical classifier on the same dataset, our entirely un-
supervised approach achieves an accuracy of 63%.
This is a notable improvement on the largest-class
baseline of 55%.
5 Conclusions
We presented a novel method, image dispersion-
based filtering, that improves multi-modal repre-
sentations by approximating conceptual concrete-
ness from images and filtering model input. The
results clearly show that including more percep-
tual input in multi-modal models is not always bet-
ter. Motivated by this fact, our approach provides
an intuitive and straightforward metric to deter-
mine whether or not to include such information.
In addition to improving multi-modal represen-
tations, we have shown the applicability of the im-
age dispersion metric to several other tasks. To
our knowledge, our algorithm constitutes the first
unsupervised method for quantifying conceptual
concreteness as applied to NLP, although it does,
of course, rely on the Google Images retrieval al-
gorithm. Moreover, we presented a method to
classify adjective-noun pairs according to modi-
fication type that exploits the link between image
dispersion and concreteness. It is striking that this
apparently linguistic problem can be addressed
solely using the raw data encoded in images.
In future work, we will investigate the precise
quantity of perceptual information to be included
for best performance, as well as the optimal filter-
ing threshold. In addition, we will explore whether
the application of image data, and the interaction
between images and language, can yield improve-
ments on other tasks in semantic processing and
representation.
Acknowledgments
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St John?s College, Cambridge.
AK is supported by The Royal Society. SC is sup-
ported by ERC Starting Grant DisCoTex (306920)
and EPSRC grant EP/I037512/1. We thank the
anonymous reviewers for their helpful comments.
1
The MRC Psycholinguistics concreteness ratings (Colt-
heart, 1981) used by Turney et al (2011) are a subset of those
included in the USF dataset.
839
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 19?27, Boulder, Colorado.
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological review, 116(3):463.
Lawrence W Barsalou, W Kyle Simmons, Aron K Bar-
bey, and Christine D Wilson. 2003. Grounding
conceptual knowledge in modality-specific systems.
Trends in cognitive sciences, 7(2):84?91.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399?405.
Anna Bosch, Andrew Zisserman, and Xavier Munoz.
2007. Image classification using random forests and
ferns. In Proceedings of ICCV.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136?145. Asso-
ciation for Computational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1?47.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hi-
erarchical image database. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Con-
ference on, pages 248?255. IEEE.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91?99. Asso-
ciation for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406?
414. ACM.
Felix Hill, Douwe Kiela, and Anna Korhonen. 2013a.
Concreteness and corpora: A theoretical and practi-
cal analysis. CMCL 2013.
Felix Hill, Anna Korhonen, and Christian Bentz.
2013b. A quantitative empirical analysis of the ab-
stract/concrete distinction. Cognitive science, 38(1).
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Oi Yee Kwong. 2008. A preliminary study on the im-
pact of lexical concreteness on word sense disam-
biguation. In PACLIC, pages 235?244.
Geoffrey Leech, Roger Garside, and Michael Bryant.
1994. Claws4: the tagging of the british national
corpus. In Proceedings of the 15th conference
on Computational linguistics-Volume 1, pages 622?
628. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of Inter-
national Conference of Learning Representations,
Scottsdale, Arizona, USA.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, & Com-
puters, 36(3):402?407.
Allan Paivio. 1990. Mental representations: A dual
coding approach. Oxford University Press.
Stephen Roller and Sabine Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1146?1157, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David S?anchez, Montserrat Batet, and David Isern.
2011. Ontology-based information content compu-
tation. Knowledge-Based Systems, 24(2):297?303.
D Sculley. 2010. Web-scale k-means clustering. In
Proceedings of the 19th international conference on
World wide web, pages 1177?1178. ACM.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
840
in Natural Language Processing and Computational
Natural Language Learning, pages 1423?1433. As-
sociation for Computational Linguistics.
J. Sivic and A. Zisserman. 2003. Video Google: a text
retrieval approach to object matching in videos. In
Proceedings of the Ninth IEEE International Con-
ference on Computer Vision, volume 2, pages 1470?
1477, Oct.
James H Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empiri-
cal Methods in Natural Language Processing, pages
680?690.
A. Vedaldi and B. Fulkerson. 2008. VLFeat: An open
and portable library of computer vision algorithms.
http://www.vlfeat.org/.
Luis Von Ahn and Laura Dabbish. 2004. Labeling
images with a computer game. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 319?326. ACM.
841
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75?83,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Concreteness and Corpora: A Theoretical and Practical Analysis  
 
Felix Hill 
Computer Laboratory 
University of Cambridge 
fh295@cam.ac.uk 
Douwe Kiela 
Computer Laboratory 
University of Cambridge 
dlk427@cam.ac.uk 
Anna Korhonen 
Computer Laboratory 
University of Cambridge 
alk23@cam.ac.uk 
 
  
Abstract 
An increasing body of empirical evidence 
suggests that concreteness is a fundamental 
dimension of semantic representation. By im-
plementing both a vector space model and a 
Latent Dirichlet Allocation (LDA) Model, we 
explore the extent to which concreteness is re-
flected in the distributional patterns in corpora.  
In one experiment, we show that that vector 
space models can be tailored to better model 
semantic domains of particular degrees of 
concreteness.   In a second experiment, we 
show that the quality of the representations of 
abstract words in LDA models can be im-
proved by supplementing the training data 
with information on the physical properties of 
concrete concepts.  We conclude by discussing 
the implications for computational systems 
and also for how concrete and abstract con-
cepts are represented in the mind 
1 Introduction 
A growing body of theoretical evidence empha-
sizes the importance of concreteness to semantic 
representations.  This fact has not been widely 
exploited in NLP systems, despite its clear theo-
retical relevance to tasks such as word-sense in-
duction and compositionality modeling.  In this 
paper, we take a first step towards integrating 
concreteness into NLP by testing the extent to 
which it is reflected by the superficial (distribu-
tional) patterns in corpora.  The motivation is 
both theoretical and practical: We consider the 
implications for the development of computa-
tional systems and also for how concrete and ab-
stract concepts are represented in the human 
mind.  Experimenting with two popular methods 
of extracting lexical representations from text, 
we show both that these approaches are sensitive 
to concreteness and that their performance can be 
improved by adapting their implementation to 
the concreteness of the domain of application.  In 
addition, our findings offer varying degrees of 
support to several recent proposals about concep-
tual representation.   
In the following section we review recent 
theoretical and practical work. In Section 3 we 
explore the extent to which concreteness is re-
flected by Vector-Space Models of meaning 
(VSMs), and in Section 4 we conduct a similar 
analysis for (Bayesian) Latent Dirichlet Alloca-
tion (LDA) models.   We conclude, in Section 5, 
by discussing practical and theoretical implica-
tions.     
2 Related work 
2.1 Concreteness 
Empirical evidence indicates important cognitive 
differences between abstract concepts, such as 
guilt or obesity, and concrete concepts, such as 
chocolate or cheeseburger.  It has been shown 
that concrete concepts are more easily learned 
and remembered than abstract concepts, and that 
language referring to concrete concepts is more 
easily processed (Schwanenflugel, 1991).  There 
are cases of brain damage in which either ab-
stract or concrete concepts appear to be specifi-
cally impaired (Warrington, 1975), and function-
al magnetic resonance imaging (fMRI) studies 
implicate overlapping but partly distinct neural 
systems in the processing of the two concept 
types (Binder et al, 2005).  Further, there is in-
creasing evidence that concrete concepts are 
represented via intrinsic properties whereas ab-
stract representations encode extrinsic relations 
to other concepts (Hill et al, in press). However, 
while these studies together suggest that con-
creteness is fundamental to human conceptual 
representation, much remains to be understood 
about the precise cognitive basis of the ab-
stract/concrete distinction.  Indeed, the majority 
of theoretically motivated studies of conceptual 
representation focus on concrete domains, and 
75
comparatively little has been established empiri-
cally about abstract concepts. 
Despite this support for the cognitive impor-
tance of concreteness, its application to computa-
tional semantics has been limited to date.  One 
possible reason for this is the difficulty in mea-
suring lexical concreteness using corpora alone 
(Kwong, 2008).  Turney et al (2011) overcome 
this hurdle by applying a semi-supervised me-
thod to quantify noun concreteness.  Using this 
data, they show that a disparity in the concrete-
ness between elements of a construction can faci-
litate metaphor identification. For instance, in the 
expressions kill the process or black comedy, a 
verb or adjective that generally occurs with a 
concrete argument takes an abstract argument. 
Turney et al show that a supervised classifier 
can exploit this effect to correctly identify 79% 
of adjective-noun and verb-object constructions 
as literal or metaphorical.  Although these results 
are clearly promising, to our knowledge Turney 
et al?s paper is unique in integrating corpus-
based methods and concreteness in NLP systems.   
1.2 Association / similarity 
A proposed distinction between abstract and 
concrete concepts that is particularly important 
for the present work relates to the semantic rela-
tions association and (semantic) similarity (see 
e.g. Crutch et al 2009; Resnik, 1995). The dif-
ference between these relations is exemplified by 
the concept pairs {car, petrol} and {car, van}.  
Car is said to be (semantically) similar to van, 
and associated with (but not similar to) petrol.  
Intuitively, the basis for the similarity of car and 
bike may be their common physical features 
(wheels) or the fact that they fall within a clearly 
definable category (modes of transport).  In con-
trast, the basis for the association between car 
and petrol may be that they are often found to-
gether or the clear functional relationship be-
tween them.  The two relations are neither mu-
tually exclusive nor independent; bike and car 
are related to some degree by both association 
and similarity.  
Based on fresults of behavioral experiments, 
Crutch et al (2009) make the following proposal 
concerning how association and similarity inte-
ract with concreteness: 
 
(C) The conceptual organization of abstract con-
cepts is governed by association, whereas the 
organization of concrete concepts is governed by 
similarity.   
 
Crutch et al?s hypothesis derives from experi-
ments in which participants selected the odd-one-
out from lists of five words appearing on a 
screen. The lists comprised either concrete or 
abstract words (based on ratings of six infor-
mants) connected either by similarity (e.g. dog, 
wolf, fox etc.; theft, robbery, stealing etc.) or 
association (dog, bone, collar etc.; theft, law, vic-
tim etc.), with an unrelated odd-one-out item in 
each list. Controlling for frequency and position, 
subjects were both significantly faster and more 
accurate if the related words were either abstract 
and associated or concrete and similar. These 
results support (C) on the basis that decision 
times are faster when the related items form a 
more coherent group, rendering the odd-one out 
more salient.  Hill et al (in press) tested the same 
hypothesis on a larger scale, analyzing over 
18,000 concept pairs scored by human annotators 
for concreteness as well as the strength of associ-
ation between them.  They found a moderate in-
teraction between concreteness and the correla-
tion between association strength and similarity 
(as measured using WordNet), but concluded 
that the strength of the effect was not sufficiently 
strong to either confirm or refute (C). 
Against this backdrop, the present work ex-
amines how association, similarity and concrete-
ness are reflected in LDA models and, first, 
VSMs.  In both cases we test Hypothesis (C) and 
related theoretical proposals, and discuss whether 
these findings can lead to better performing se-
mantic models.   
3 Vector Space Models 
Vector space models (VSMs) are perhaps the 
most common general method of extracting se-
mantic representations from corpora (Sahlgren, 
2006; Turney & Pantel, 2010).  Words are 
represented in VSMs as points in a (geometric) 
vector space. The dimensions of the space cor-
respond to the model features, which in the sim-
plest case are high frequency words from the 
corpus.  In such models, the position of a word 
representation along a given feature dimension 
depends on how often that word occurs within a 
specified proximity to tokens of the feature word 
in the corpus.  The exact proximity required is an 
important parameter for model implementation, 
and is referred to as the context window.  Finally, 
the degree to which two word representations are 
related can be calculated as some function of the 
distance between the corresponding points in the 
semantic space.   
76
3.1 Motivation 
VSMs are well established as a method of quan-
tifying relations between word concepts and have 
achieved impressive performance in related NLP 
tasks (Sahlgren, 2006; Turney & Pantel, 2010).  
In these studies, however, it is not always clear 
exactly which semantic relation is best reflected 
by the implemented models.  Indeed, research 
has shown that by changing certain parameter 
settings in the standard VSM architecture, mod-
els can be adapted to better reflect one relation 
type or another.  Specifically, models with 
smaller context windows are reportedly better at 
reflecting similarity, whereas models with larger 
windows better reflect association. (Agirre et al, 
2009; Peirsman et al, 2008) 
Our experiments in this section aim first to 
corroborate these findings by testing how models 
of varying context window sizes perform on em-
pirical data of both association and similarity.  
We then test if this effect differentially affects 
performance on concrete and abstract words.   
3.2 Method  
We employ a conventional VSM design, extract-
ing representations from the (unlemmatised) 
British National Corpus (Leech et al, 1994) with 
stopwords removed.   In the vector representation 
of each noun, our dimension features are the 
50,000 most frequently occurring (non-
stopword) words in the corpus.    We experiment 
with window sizes of three, five and nine (one, 
two and four words either side of the noun, 
counting stopwords).  Finally, we apply point-
wise mutual information (PMI) weighting of our 
co-occurrence frequencies, and measure similari-
ty between weighted noun vectors by the cosine 
of the angle between them in the vector space.    
To evaluate modeling of association, we use 
the University of South Florida (USF) Free-
association Norms (Nelson & McEvoy, 2012).  
The USF data consist of over 5,000 words paired 
with their free associates.  To elicit free asso-
ciates, more than 6,000 participants were pre-
sented with cue words and asked to ?write the 
first word that comes to mind that is meaningful-
ly related or strongly associated to the presented 
word?.  For a cue word c and an associate a, the 
forward association strength (association) from 
c to a is the proportion of participants who pro-
duced a when presented with c.  association is 
thus a measure of the strength of an associate 
relative to other associates of that cue.  The USF 
data is well suited to our purpose because many 
cues and associates in the data have a concrete-
ness score, taken from either the norms of Paivio, 
Yuille and Madigan (1968) or Toglia and Battig 
(1978).  In both cases contributors were asked to 
rate words based on a scale of 1 (very abstract) to 
7 (very concrete).1  We extracted the all 2,230 
nouns from the USF data for which concreteness 
scores were known, yielding a total of 15,195 
noun-noun pairs together with concreteness and 
association values.   
Although some empirical word-similarity da-
tasets are publically available, they contain few if 
any abstract words (Finkelstein et al, 2002; Ru-
benstein & Goodenough, 1965).  Therefore to 
evaluate similarity modeling, we use Wu-Palmer 
Similarity (similarity) (Wu & Palmer, 1994), a 
word similarity metric based on the position of 
the senses of two words in the WordNet taxono-
my (Felbaum, 1998).  similarity can be applied to 
both abstract and concrete nouns and achieves a 
high correlation, with human similarity judg-
ments (Wu & Palmer, 1994).2     
3.3 Results 
In line with previous studies, we observed that 
VSMs with smaller window sizes were better 
able to predict similarity.  The model with win-
dow size 3 achieves a higher correlation with 
similarity (Spearman rank rs  = -0.29) than the 
model with window size 9 (rs  = -0.25).  Howev-
er, the converse effect for association was not 
observed: Model correlation with association 
was approximately constant over all window siz-
es.  These effects are illustrated in Fig. 1.  
 
                                                 
1Although concreteness is well understood intuitively, it 
lacks a universally accepted definition.  It is often described 
in terms of reference to sensory experience (Paivio et al, 
1968), but also connected to specificity; rose is often consi-
dered more concrete than flora.  The present work does not 
address this ambiguity.     
2 similarity achieves a Pearson correlation of r  = .80 on 
the  30 concrete word pairs in the Miller & Charles (1991) 
data.   
0.123 0.125
0.12
0.286
0.29
0.241
0.0
0.1
0.2
0.3
3 5 9
Window
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
77
Figure 1:  Spearman correlations between VSM out-
put and association and similarity for different win-
dow sizes. 
 
In addressing the theoretical Hypothesis (C) we 
focused on the output of our VSM of window 
size five, although the same trends were ob-
served over all three models.  Over all 18,195 
noun-noun pairs the correlation between the 
model output and association was significant (rs  
= 0.13, p < 0.001) but notably lower than the cor-
relation with similarity (rs  = -0.29, p < 0.001).  
To investigate the effect of concreteness, we 
ranked each pair in our sample by the total con-
creteness of both nouns, and restricted our analy-
sis to the 1000 most concrete and 1000 most ab-
stract pairs.  The models captured association 
better over the abstract pairs than concrete con-
cepts, but reflected similarity better over the con-
crete concepts.  The strength of this effect is illu-
strated in Fig. 2.   
 
 
Figure 2: Spearman correlation values between VSM 
output and similarity and association over subsets of 
concrete and abstract pairs. 
 
Given that small window sizes are optimal for 
modeling similarity, and that WSMs appear to 
model similarity better over concrete concepts 
than over abstract concepts, we explored whether 
different window sizes were optimal for either 
abstract or concrete word pairs. When comparing 
the model output to association, no interaction 
between window size and concreteness was ob-
served.  However, there was a notable interaction 
when considering performance in modeling simi-
larity.  As illustrated in Fig. 3, performance on 
concrete word pairs is better for smaller window 
sizes, whereas with abstract word pairs a larger 
window size is preferable.   
 
 
Figure 3:  Spearman correlation values between VSM 
output and similarity and association for different 
window sizes over abstract and concrete word pair 
subsets 
3.4 Conclusion 
Our results corroborate the body of VSM re-
search that reports better performance from small 
window sizes in modeling similarity.  A likely 
explanation for this finding is that similarity is a 
paradigmatic relation: Two similar entities can 
be plausibly exchanged in most linguistic con-
texts.  Small context windows emphasize prox-
imity, which loosely reflects structural relation-
ships such as verb-object, ensuring that paradig-
matically related entities score highly.  Models 
with larger context windows cannot discern pa-
radigmatically and syntagmatically related enti-
ties in this way.  The performance of our models 
on the association dataset did not support the 
converse conclusion that larger window sizes 
perform better.  Overall, each of the three models 
was notably better at capturing similarity than 
association.  This suggests that the core architec-
ture of WSMs is not well suited to modeling as-
sociation.  Indeed, ?first order? models that di-
rectly measure word co-occurrences, rather than 
connecting them via features, seem to perform 
better at this task (Chaudhari et al, 2011).  This 
fact is consistent with the view that association is 
a more basic or fundamental semantic relation 
from which other more structured relations are 
derived.  
The fact that the USF association data re-
flects the instinctive first response of participants 
when presented with a cue word is important for 
interpreting the results with respect to Hypothe-
sis (C).  Our findings suggest that VSMs are bet-
ter able to model this data for abstract word pairs 
than for concrete word pairs.  This is consistent 
with the idea that language fundamentally deter-
mines which abstract concepts come to be asso-
ciated or connected in the mind.  Conversely, the 
0.125
0.108
0.215
0.297
0.0
0.1
0.2
0.3
Association Similarity
Relation Type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
0.202
0.272
0.223
0.254
0.14
0.086
0.104
0.099
0.0
0.1
0.2
3 - Similarity 9 - Similarity 3 - Assoc. 9 - Assoc.
Window size - Relation type
C
o
r
r
e
l
a
t
i
o
n
Abstract
Concrete
78
fact that the model reflects associations between 
concrete words less well suggests that the impor-
tance of extra-linguistic information is lower for 
connecting concrete concepts in this instinctive 
way.  Indeed, it seems plausible that the process 
by which concrete concepts become associated 
involves visualization or some other form of per-
ceptual reconstruction. Consistent with Hypothe-
sis (C), this reconstruction, which is not possible 
for abstract concepts, would naturally reflect si-
milarity to a greater extent than linguistic context 
alone.   
Finally, when modeling similarity, the ad-
vantage of a small window increases as the 
words become more concrete.  Similarity be-
tween concrete concepts is fundamental to cogni-
tive theories involving the well studied notions 
of prototype and categorization (Rosch, 1975; 
Rogers & McClelland, 2003).   In contrast, the 
computation of abstract similarity is intuitively a 
more complex cognitive operation.  Although the 
accurate quantification of abstract similarity may 
be beyond existing corpus-based methods, our 
results suggest that a larger context window 
could in fact be marginally preferable should 
VSMs be applied to this task.   
Overall, our findings show that the design of 
VSMs can be tailored to reflect particular seman-
tic relations and that this in turn can affect their 
performance on different semantic domains, par-
ticularly with respect to concreteness.  In the 
next section, we investigate whether the same 
conclusions should apply to a different class of 
distributional model.        
4 Latent Dirichlet Allocation Models 
LDA models are trained on corpora that are di-
vided into sections (typically documents), ex-
ploiting the principle that words appearing in the 
same document are likely to have similar mean-
ings.  In an LDA model, the sections are viewed 
as having been generated by random sampling 
from unknown latent dimensions, which are 
represented as probability distributions (Dirichlet 
distributions) over words.  Each document can 
then be represented by a probability distribution 
over these dimensions, and by considering the 
meaning of the dimensions, the meaning of the 
document can be effectively characterized.  More 
importantly, because each latent dimension clus-
ters words of a similar meaning, the output of 
such models can be exploited to provide high 
quality lexical representations (Griffiths et al, 
2007).  Such a word representation encodes the 
extent to which each of the latent dimensions 
influences the meaning of that word, and takes 
the form of a probability distribution over these 
dimensions.  The degree to which two words are 
related can then be approximated by any function 
that measures the similarity or difference be-
tween distributions.        
4.1 Motivation 
In recent work, Andrews et al (2009) explore 
ways in which LSA models can be modified to 
improve the quality of their lexical representa-
tions.  They propose that concepts are acquired 
via two distinct information sources: experiential 
data ? the perceptible properties of objects, and 
distributional data ? the superficial patterns of 
language.  To test this hypothesis, Andrews et al 
construct three different LDA models, one 
trained on experiential data, one trained in the 
conventional manner on running text, and one 
trained on the same text but with the experiential 
data appended.  They evaluate the quality of the 
lexical representations in the three models by 
calculating the Kulback-Leibler divergence be-
tween the representation distributions to measure 
how closely related two words are (Kullback & 
Leibler, 1951).  When this data was compared 
with the USF association data, the combined 
model performed better than the corpus-based 
model, which in turn performed better than the 
features-only model.  Andrews et al concluded 
that both experiential and distributional data are 
necessary for the acquisition of good quality lex-
ical representations. 
     As well as suggesting a way to improve the 
performance of LDA models on NLP tasks by 
supplementing the training data, the approach 
taken by Andrews et al may be useful for better 
understanding the nature of the abstract/concrete 
distinction.  In recent work, Hill et al (in press) 
present empirical evidence that concrete con-
cepts are represented in terms of intrinsic fea-
tures or properties whereas abstract concepts are 
represented in terms of connections to other 
(concrete and abstract) concepts.  For example, 
the features [legs], [tail], [fur], [barks] are all 
central aspects of the concrete representation of 
dog, whereas the representation of the abstract 
concept love encodes connections to other con-
cepts such as heart, rose, commitment and hap-
piness etc.  If a feature-based representation is 
understood to be constructed from physical or 
perceptible properties (which themselves may be 
basic or fundamental concrete representations), 
Hill et al?s characterization of concreteness can 
be summarized as follows:  
79
 
(H) Concreteness correlates with the degree to 
which conceptual representations are feature-
based 
 
Because such differences in representation struc-
ture would in turn entail differences in the com-
putation of similarity, (H) is closely related to a 
proposal of Markman and Stilwell (2001; see 
also Gentner & Markman, 2007):  
 
(M) Computing similarity among concrete con-
cepts involves a feature-comparison operation, 
whereas similarity between abstract concepts is 
a structural, analogy-like, comparison.   
 
The findings of Andrews et al do not address 
(H) or (M) directly, for two reasons. Firstly, they 
evaluate their model on a set that includes no 
abstract concepts.  Secondly, they compare their 
model output to association data without testing 
how well it reflects similarity.  In this section we 
therefore reconstruct the Andrews models and 
evaluate how well they reflect both association 
and similarity across a larger set of abstract and 
concrete concepts.   
4.2  Method/materials 
We reconstruct two of the three models devel-
oped by Andrews et al (2009), excluding the 
features-only model because of the present focus 
on corpus-based approaches.  However, while 
the experiential data applied in the Andrews et 
al. combined model was that collected by Vig-
liocco et al (2004), we use the publicly available 
McRae feature production norms (McRae et al, 
2005).  The McRae data consist of 541 concrete 
noun concepts together with features for each 
elicited from 725 participants.  In the data collec-
tion, feature was understood in a very loose 
sense, so that participants were asked to list both 
physical and functional properties of the nouns in 
addition to encyclopedic facts.  However, for the 
present work, we filter out those features that 
were not perceptual properties using McRae et 
al.?s feature classes, leaving a total of 1,285 fea-
ture types, such as [has_claws] and 
[made_of_brass].  The importance of each fea-
ture to the representation of a given concept is 
reflected by the proportion of participants who 
named that feature in the elicitation experiment.  
For each noun concept we therefore extract a 
corresponding probability distribution over fea-
tures. 
The model design and inference are identical 
to those applied by Andrews et al  Our distribu-
tional model contains 250 latent dimensions and 
was trained using a Gibbs Sampling algorithm on 
approximately 7,500 sections of the BNC with 
stopwords removed.3  The combined model con-
tains 350 latent dimensions, and was trained on 
the same BNC data.  However, for each instance 
of one of the 541 McRae concept words, a fea-
ture is drawn at random from the probability dis-
tribution corresponding to that word and ap-
pended to the training data.  The latent dimen-
sions in the combined model therefore corres-
pond to probability distributions both over words 
and over features. This leads to an important dif-
ference between how words come to be related in 
the distributional model and in the combined 
model.  Both models infer connections between 
words by virtue of their occurrence either in the 
same document or in pairs of documents for 
which the same latent dimensions are prominent.  
In the distributional model, it is the words in a 
document that determines which latent dimen-
sions are ultimately prominent, whereas the in 
combined model it is both the words and the fea-
tures in that document.  Therefore, in the com-
bined model, two words can come to be related 
because they occur not only in documents whose 
words are related, but also in documents whose 
features are related.  For words in the McRae 
data, this has the effect of strengthening the rela-
tionship between words with common features.  
More interestingly, because it alters which latent 
dimensions are most prominent for each docu-
ment, it should also influence the relationship 
between words not in the McRae data.   
We evaluate the performance of our models in 
reflecting free association (association) and simi-
larity (similarity).  To obtain test items we rank 
the 18,195 noun-noun pairs from the USF data 
by the product of the two (BNC) word frequen-
cies and select the 5,000 highest frequency pairs.   
4.3 Results 
As expected, the correlation of the combined 
model output with association was greater than 
the correlation of the distributional model output.  
Notably, however, as illustrated in Fig. 4, we 
observed far greater differences between the 
combined and the distributional models when 
comparing to similarity.  Over all noun pairs, the 
addition of features in the combined model im-
                                                 
3 Code for model implementation was taken from Mark 
Andrews : http://www.mjandrews.net/code/index.html  
80
proved the correlation with similarity from 
Spearman rs  =  0.09  to  rs  =  0.15.   
 
Figure 4:  Spearman correlations between distribu-
tional and combined model outputs, similarity and 
association  
 
In order to address Hypothesis (C) (Section 2.2), 
we analyzed the output of the combined model 
on subsets of the 1000 most abstract and concrete 
word pairs in our data as before.  Perhaps surpri-
singly, as shown in Fig. 5, when comparing with 
similarity, the model performed better over ab-
stract pairs, whereas when comparing with asso-
ciation the model performed better over concrete 
pairs.  However, when these concrete pairs were 
restricted to those for which at least one of the 
two words was in the McRae data, and hence to 
which features had been appended in the corpus, 
the ability of the model to reflect similarity in-
creased significantly.          
 
Figure 5:  Spearman correlations between combined 
model output and similarity and association on differ-
ent word pair subsets  
 
Finally, to address hypotheses (H) and (M) we 
compared the previous analysis of the combined 
model output to the equivalent output from the 
distributional model.  Surprisingly, as shown in 
Fig. 6, the ability of the model to reflect associa-
tion over abstract pairs seemed to reduce with the 
addition of features to the training data.  Never-
theless, in all other cases the combined model 
outperformed the distributional model.  Interes-
tingly, the combined model advantage when 
comparing with similarity was roughly the same 
over both abstract and concrete pairs.  However, 
when these pairs contained at least one word 
from the McRae data, the combined model was 
indeed significantly better at modeling similarity, 
consistent with Hypotheses (M) and (H). 
 
Figure 6:  Comparison between distributional 
model and combined model output correlations with 
similarity and association over different word pair 
subsets 
4.4 Conclusion 
Our findings corroborate the main conclusion of 
Andrews et al, that the addition of experiential 
data improves the performance of the LDA mod-
el in reflecting association.  However, they also 
indicate that the advantage of feature-based LDA 
models is far more significant when the objective 
is to model similarity. 
 The findings are also consistent with, if 
not suggestive of, the theoretical hypotheses (H) 
and (M).  Clearly, the property features in the 
combined model training data enable it to better 
model both similarity and association between 
those concepts to which the features correspond.  
However, this benefit is greater when modeling 
similarity than when modeling association.  This 
suggests that the similarity operation is indeed 
based on features to a greater extent than associa-
tion.  Moreover, this effect is far greater for the 
concrete words for which the features were add-
ed than over the other words pairs we tested.  
Whilst this is not a sound test of hypothesis (H) 
(no attempt was made to add ?features? of ab-
stract concepts to the model), it is certainly con-
sistent with the idea that features or properties 
are a more important aspect of concrete represen-
tations than of abstract representations. 
0.13
0.09
0.14
0.15
0.00
0.05
0.10
0.15
Distributional Combined
Model type
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete McRae
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
0.03
0.093
0.15
0.018
0.11
0.025
0.01
0.16
0.2
0.08
0.14
0.36
0.0
0.1
0.2
0.3
Abstract Concrete 
 Distributional model
McRae Abstract_ Concrete_
 Combined model
McRae_
Word pair category
C
o
r
r
e
l
a
t
i
o
n
Association
Similarity
81
Perhaps the most interesting aspect of the 
combined model is how the addition of feature 
information in the training data for certain words 
influences performance on words for which fea-
tures were not added.  In this case, our findings 
suggest that the benefit when modeling similarity 
is marginally greater than when modeling associ-
ation, an observation consistent with Hypothesis 
(M).  A less expected observation is that, be-
tween words for which features were not added, 
the advantage of the combined model over the 
distributional model in modeling similarity was 
equal if not greater for abstract than for concrete 
concepts.  We hypothesize that this is because 
abstract representations naturally inherit any re-
liance on feature information from the concrete 
concepts with which they participate.  In con-
trast, highly concrete representations do not en-
code relations to other concepts and therefore 
cannot inherit relevant feature information in the 
same way.  Under this interpretation, the con-
crete information from the McRae words would 
propagate more naturally to abstract concepts 
than to other concrete concepts.  As a result, the 
highest quality representations in the combined 
model would be those of the McRae words, fol-
lowed by those of the abstract concepts to which 
they closely relate.    
5 Discussion  
This study has investigated how concreteness is 
reflected in the distributional patterns found in 
running text corpora. Our results add to the body 
of evidence that abstract and concrete concepts 
are represented differently in the mind.  The fact 
that VSMs with small windows are particularly 
adept at modeling relations between concrete 
concepts supports the view that similarity go-
verns the conceptual organization of concrete 
concepts to a greater extent than for abstract con-
cepts.  Further, the performance of our LSA 
models on different tasks and across different 
word pairs is consistent with the idea that con-
crete representations are built around features, 
whereas abstract concepts are not.  
More practically, we have demonstrated that 
vector space models can be tailored to reflect 
either similarity or association by adjusting the 
size of the context window.  This in turn indi-
cates a way in which VSMs might be optimized 
to either abstract or concrete domains.  Our expe-
riments with Latent Dirichlet Allocation corrobo-
rate a recent proposal that appending training 
data with perceptible feature or property infor-
mation for a subset of concrete nouns can signif-
icantly improve the quality of the model?s lexical 
representations.  As expected, this effect was 
particularly salient for representations of words 
for which features were appended to the training 
data.  However, the results show that this infor-
mation can propagate to words for which fea-
tures were not appended, in particular to abstract 
words.   
The fact that certain perceptible aspects of 
meaning are not exhaustively reflected in linguis-
tic data is a potentially critical obstacle for cor-
pus-based semantic models.  Our findings sug-
gest that existing machine learning techniques 
may be able to overcome this by adding the re-
quired information for words that refer to con-
crete entities and allowing this information to 
propagate to other elements of language.  In fu-
ture work we aim to investigate specifically 
whether this hypothesis holds for particular parts 
of speech.  For example, we would hypothesize 
that verbs inherit a good degree of their meaning 
from their prototypical nominal arguments.     
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J. 
Pasca, K,. & Soroa,A. 2009. A Study on Similarity 
and Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL-HLT 
2009. 
Andrews, M., Vigliocco, G. & Vinson, D. 2009. 
Integrating experiential and distributional data to 
learn semantic represenations. Psychological Review, 
116(3), 463-498. 
Barsalou, L. 1999. Perceptual symbol systems. Be-
havioral and Brain Sciences, 22, 577-609. 
Binder, J., Westbury, C., McKiernan, K., Possing, 
E., & Medler, D. 2005. Distinct brain systems for 
processing concrete and abstract concepts. Journal of 
Cognitive Neuroscience 17(6), 905-917. 
Chaudhari, D., Damani, O., & Laxman, S. 2011. 
Lexical Co-occurrence, Statistical Significance, and 
Word Association. EMNLP 2011, 1058-1068. 
Crutch, S., Connell, S., & Warrington, E. 2009. 
The different representational frameworks underpin-
ning abstract and concrete knowledge: evidence from 
odd-one-out judgments. Quarterly Journal of Experi-
mental Psychology, 62(7), 1377-1388. 
Felbaum, C. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
Finkelstein, L., Gabrilovich, Matias, Rivlin, Solan, 
Wolfman & Ruppin. 2002. Placing Search in Context: 
The Concept Revisited. ACM Transactions on Infor-
mation Systems, 20(1):116-131. 
Gentner, D., & Markman, A. 1997. Structure map-
ping in analogy and similarity. American Psycholo-
gist, 52. 45-56. 
82
Griffiths, T., Steyvers, M., & Tenembaum, J. 2007. 
Topics in semantic representation. Psychological Re-
view, 114 (2), 211-244. 
Hill, F., Korhonen, A., & Bentz, C. A quantitative 
empricial analysis of the abstract/concrete distinction. 
Cognitive Science. In press.  
Kullback, S., & Leibler, R.A. 1951. On Informa-
tion and Sufficiency. Annals of Mathematical Statis-
tics 22 (1): 79?86. 
Kwong, O, Y. 2008. A Preliminary study on induc-
ing lexical concreteness from dictionary definitions. 
22nd Pacific Asia Conference on Language, In-
formation and Computation, 235?244. 
Leech, G., Garside, R. & Bryant, R. 1994. Claws4: 
The tagging of the British National Corpus. COL-ING 
94, Lancaster: UK. 
Markman, A, & Stilwell, C. 2001. Role-governed 
categories. Journal of Theoretical and Experimental 
Artificial Intelligence, 13, 329-358. 
McRae, K., Cree, G. S., Seidenberg, M. S., & 
McNorgan, C. 2005. Semantic feature production 
norms for a large set of living and nonliving things. 
Behavior Research Methods, 37, 547-559 
Miller, G., & Charles, W. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive 
Processes, 6(1). 
Nelson, D., & McEvoy, C. 2012. The University of 
South Florida Word Association, Rhyme and Word 
Fragment Norms. Retrieved online from: 
http://web.usf.edu/FreeAssociation/Intro.html. 
Paivio, A., Yuille, J., & Madigan, S. 1968. Con-
creteness, imagery, and meaningfulness values for 
925 nouns. Journal of Experimental Psychology Mo-
nograph Supplement, 76(1, Pt. 2).  
Peirsman, y., Heylen, K. & Geeraerts, D. 2008. 
Size Matters. Tight and Loose Context Definitions in 
English Word Space Models. In Proceedings of the 
ESSLLI Workshop on Distributional Lexical Seman-
tics, Hamburg, Germany 
Resnik, P. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. Pro-
ceedings of IJCAI-95. 
Rogers, T., & McLelland, J. 2003. Semantic Cog-
nition. Cambridge, Mass: MIT Press. 
Rosch, E. 1975. Cognitive representations of se-
mantic categories. Journal of Experimental Psycholo-
gy: General, 104(3), (September 1975), pp. 192?233. 
Rubenstein, H., & Goodenough, J. 1965. Contex-
tual correlates of synonymy. Communications of the 
ACM 8(10), 627-633. 
Sahlgren, M. 2006. The Word-Space Model: Using 
distributional analysis to represent syntagmatic and 
paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. dissertation, De-
partment of Linguistics, Stockholm University.  
Schwanenflugel, P. 1991.  Why are abstract con-
cepts hard to understand? In P.  Schwanenflugel.  
The psychology of word meanings (pp.  223-250).  
Hillsdale, NJ: Erlbaum. 
Toglia, M., & Battig, W. 1978. Handbook of se-
mantic word norms. Hillsdale, N.J: Erlbaum. 
Turney, P, & Pantel, P. 2010. From frequency to 
meaning: Vector space models of semantics. Journal 
of Artificial Intelligence Research (JAIR), 37, 141-
188. 
Turney,P., Neuman, Y., Assaf,.D, Cohen, Y. 2011. 
Literal and Metaphorical Sense Identification through 
Concrete and Abstract Context. EMNLP 2011: 680-
690 
Vigliocco, G., Vinson, D. P., Lewis, W., & Garrett, 
M. F. 2004. Reprssenting the meanings of object and 
action words: The featural and unitary semantic space 
hypothesis. Cognitive Psychology, 48, 422?488. 
Warrington, E. (1975). The selective impairment of 
semantic memory. Quarterly Journal of Experimental 
Psychology 27(4), 635-657. 
Wu, Z., Palmer, M. 1994. Verb semantics and lexi-
cal selection. In: Proceedings of the 32nd Annual 
Meeting of the Associations for Computational Lin-
guistics. 133?138. 
 
 
83
