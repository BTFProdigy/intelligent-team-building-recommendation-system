The Effects of Word  Order  and Segmentat ion  on Trans la t ion  
Retr ieva l  Per fo rmance  
T imothy  Ba ldwin  and Hozumi  Tanaka  
27okyo \]nstil;ul;e ()I "~ I e thno logy  
2-1.2-1 Ooka,yama, Meguro -ku ,  qlbkyo 1.52-8552 , JAPAN 
{t im,  tanaka}@cl ,  ca .  t i tech ,  ac .  jp  
Abst ract  
This research looks at tim cIt'ccts of word order 
mL(t scgm(mtation on l;ra.nslation retri(~val t)(~rfor- 
III~\[.11C( ~. lot" ~.111 eXl)erim(:nta.1 Jal>an(>s(>English (;rm>- 
lation memory system. We iml)lem('.nt a num- 
ber of both bag-of-words and word order-s(msitiv(~ 
s;imilarity metrics, and test each over charact(u- 
l/ased m~d word-based indexing. Tim translation 
r(%rieval )elt'ormmm(~ of ca(:h sysi;em (:ontiguration 
is (~valuat(~(1 (mq)iri(:ally through tlm n()ti(>n of word 
edit distan(:(~ \])(}(;W(}(}IL translation (:ml(li(lal;(~ ()ul;lml;s 
mid tim mo(hd translation. Ore resull;s in(li('.at(~ 
(;hat(; (:hm'act(!r-l)as(!d indexing is (:(msislxmtly sup(> 
riot (;() wor(l-bas(:d in(l(:xing, sugg(:sl;ing (;hal; s(:glncn- 
l;al;ion is ;m mm('.cessary luxury in th(', giv(m domain. 
\?or(1 ord(:r-s(:nsi(;iv(: al)i)roach('s at(: do.monsl;rat(:d 
to generally OUtlt(~rform bag-of-words methods, with 
som'(:c bmguagc segment-lev(d e it distan(:o, proving 
th(: most; (:fl'(:(;l;iv(~ similarity m(,,l;ric. 
1 I n t roduct ion  
Transla(.ioll m(unorio,q (TM's) m'c a w(~ll-(!slal)lished 
I,(:(:\]uloliigy wil,llilL (,h(! hlunalL and n|a(:hilm ld'an,qla 
(;ion t'rat('.rnii;i(:s, duo. to the high (raiLslat;ion lit(! - 
(;isioIL (;lmy a flbrd. Esstml;ially, TM's me a list 
of t rans la t ion  records (source la.nguage strings 
paired with a unique target language translation), 
which the TM system accesses in suggcsl;ing a list 
of target languag(', t rans lat ion  cand idates  which 
may l)(,. hell)tiff to (;h(: translator in translating a 
given source language inputJ 
Naturally, TM systems h~w('~ no way of accessing 
the (;a.rgcl; la.nguagc quiv;fl(m(; of tit(: soltr(:(: lan- 
guage input, and hence (;lm list of tautc.l, lanquagc 
tnmslation cmMi(lat(:s is det(:rntined base(l on source 
language similarity between tim (:urr(mt input and 
trmlslation examples within the TM, with transla- 
tion equivalent(s) of maximally similar source lan- 
guage string(s) given as the translation candidate(s). 
This is based on the assumption that structural att(t 
semantic similarities 1)etwe(m targ(:t language trans- 
lations will be reflected in the original source lan- 
guage cquivalenl;s. 
One reason tbr the popularity of TM's is the low 
operational burden they t)(LS(~ to tim user, in that 
translation pairs are largely acquired automatically 
1See \])lanas (1998) for a thorough review of commercial 
TM systems. 
from observai;ion of l;lm incremental (;rmlsl&Lion pro- 
(:(:ss, and translation cml(lidates cml \]m l)roduced on 
(hunand almost insf;ani;ancously. To support his low 
()vt}rlma(1, TM systems must allow first access into 
the l)Oixmtially la.l'g(,.-s(:ah} TM, lint at the stone time 
I)e al)lc to 1)rc(lict .ranslation similarity with high ac- 
curacy. Ilere, th(n'(~ is clearly a trade-off between ac- 
( :ess / re t r i cva l  speed anti predict ive accuracy  of 
(,he retriewfl m(,.ctmnism. 2haditiomflly, resemch on 
TM r(~trieval nmthods has focused on Slme(l, with lit- 
(;1(~ (:ross-(~vahml;ion f (;he accuracy of differ(mr mclh- 
otis. \Vc t>r(~t'(u to focus on ac(:tlracy, and t)r(~s(~ll(; 
(~mlfiLical data (~vid(!ncing tim relative l)r(~di(:l;ivc l>O- 
((u~iial of difl'<u'(mt similarity metrics over different 
l)aram(:t(,.risations. 
In tiffs l)almr, we focus on comparison of differ(mr 
retrieval algorithms for non-segmenting la.nguag(~s, 
1)ascd around a TI~,I sysi;cm from .\]almnese to En- 
glish. Non-s(!gm(ml;ing languages are those which (Io 
not involve d(:limii;ers (e.g. spaces) tmtwe(m words, 
and in(:lude .lapmms(:, (Jhines(: and Thai. W(: are 
tmrticularly int(~'r(~st(:(l in the part tim orlhog(mal 1m- 
rmnet(~rs of s(.,gmentnl;ion and word order play in the 
st)(!cd/a(:(:uracy trad(!-oti'. That is, 1)3" doing away 
with segnl(:ntai;ion in relying soMy on ch\[/t'}lc\[(}l- 
h~v(~l comparis(m (character-1)ased indexing),  do 
w(: signiti(:mitly degrade match tmrt'ormance, ascom- 
pared to word-level comparison (word-based in- 
dexing)? Similm'ly, by ignoring word order and 
treating each sour(:e language string as a "bag of 
words", do \re genuinely lose out over word order- 
s(msitive apl)roacho.s? The. In;fin objective of this 
research is thus (;o (teJ;ermine whether the COmlmi,a- 
tioiml overlmad associated with more stringent ap- 
proaches (i.e. word-based indexing and word order- 
sensitive alH)roaches) is commensura.te with the per- 
formancc gains they ott'er. 
To l)rccmpt what tollows, the major contrilmtions 
of this research are: (a) empirical evaluation of dif- 
thrcnt comparison methods over actual Japanese- 
English TM data, focusing on four orthogonal re- 
triewfl paradigms; (b) the finding that, over tile tar- 
get; data, character-based indexing is consistently 
superior to word-based indexing in identii\[ying the 
translation candidate most sinfilar to tile optimal 
translation for a given inlmt; and (c) empirical ver- 
ification of tim supremacy of word order-sensitive 
exhaustiv(: string comparison methods over boolean 
inal;ch methods. 
In the %llowing sections we discuss the effects 
35 
of segmentation and word order (~ 2) and preseut 
a number of both bag-el;words and word order- 
sensitive sinfilarity metrics (? 3), before going on to 
evaluate the difl'crent lnethods with character-based 
and word-based indexing (? 4). We then conclude 
the paper in Section 5. 
2 Segmentation and word order 
Using segmentat ion  to divide strings into compo- 
nent words or nlori)helnes has tile obvious advml- 
tage of clustering characters into senlantic units, 
which in the case of ideogrmn-based languages uch 
as Japanese (in the fern1 of kanji characters) and 
Chinese, generally disatnbiguates character tnean- 
ing. The kanji character ' J  \[', for example, can be 
used to mean any of "to discern/discriminate", "to 
speak/argue" and "a valve", but word context easily 
resolves uch mnbiguity, hi this sense, our intuition 
is that segmented strings should produce better re- 
sults than non-segmented strings. 
Looking to past research on similarity metrics for 
TM systelns, ahnost all systems involving aal)anese 
as the source language rely on segnlentation (e.g. 
(Nakanmra, 1989; Sulnita and Tsutsumi, 1991; Ki- 
talnura and Yamamoto, 1996; Tmtaka, 19971), with 
Sate (1992) and Sate and Kawase (1994) providing 
rare instances of character-based systelnS. 
By avoiding tile need to segment text;, we: (a) al- 
leviate computational overhead; (b) avoid the need 
to commit ourselves to a particular analysis type in 
the case of ambiguity; (c) avoi(1 the issue of' how 
to deal with unknown words; (d) avoid the need 
for stemming/lenlmatisation; a d (e) to a large ex- 
tent get around problems related to the nornmlisa- 
tion of lexical alternation (see Baldwin and Tanaka 
(1999) for a discussion of problems related to lexical 
alternation in Jal)anese). Additionally, we can use 
the conmlonly anlbiguous na.ture of individual kanji 
characters to our advantage, in modelling seinan- 
tic similarity between related words with character 
overlap. With word-based indexing, this would only 
be possible with tile aid of a thesaurus. 
Similarly for word order,  we would expect hat 
translation records that preserve the word (seg- 
ment) order observed in the inImt string would pro- 
vide closer-matching translations than translation 
records containing those stone segnlents in a differ- 
ent order. Natur~dly, enforcing preservation of word 
order is going to place a significant burden on the 
matching mechanism, in that a number of different 
substring match schenlata re inevitably going to 
be produced between rely two strings, each of which 
nmst be considered on its own merits. 
To the authors' knowledge, there is no TM sys- 
tem operating from Japanese that does not rely 
on word/segment/character order to some degree. 
Tanaka (1997) uses pivotal content words identified, 
by the user to search through the TM and locate 
translation records which contain those same con- 
tent words in the stone order and preferably the stone 
segment distance apart. Nakamura (1989) similarly 
gives preference to translation records in which the 
content words contained in the original input occur 
in the same linear order, although there is tile scope 
to back off to translation records which do not I)re- 
serve the original word order. Sumita and Tsutsmni 
(19911 take the opposite tack in iteratively filter- 
ing out NPs and adverbs to leave only functional 
words and nlatrix-level predicates, and find trmlsla- 
tion records which contain those same key words in 
the same ordering, preferably with the same segment 
types between them in the same numbers. Niren- 
burg et al (1993) propose a word order-sensitive 
metric based on "string composition discrepancy", 
and increlnentally relax the restriction on the qual- 
ity of match required to inehlde word lenmlata, word 
synonynls and then word hyt)ernylns , increasing the 
match penalty as they go. Sate and Kawase (1994) 
employ a more local model of character order in 
modelling similarity according to N-grams fashioned 
from the original string. 
The greatest advantage in ignoring word/segnlent 
order is computational, in that we significantly re- 
duce the search space and require only a single over- 
all comparison per string pair. Below, we analyse 
whether this gain in speed outweighs any losses in 
retrieval perfbrmance. 
3 S imi la r i ty  metr i cs  
Due to o111" interest in the efli~cts of both word order 
and seglnentation, we must have a selection of sim- 
ilarity lnetrics compatible with the various permu- 
tations of these two 1)arameter types. We choose to 
look at a nunlber of bag-of-words and word order- 
sensitive methods which are compatible with both 
character-based and word-based indexing, and vary 
the intmt to model tile etl~ects of the two indexing 
paradigms. The particular bag-of-word approactles 
we target are tlm vector space model (Manning and 
Schiitze, 1.999, p300) and "token intersection", a
silnple ratio-based similarity nletric. For word order- 
sensitive approaches, we test edit distance (Wagner 
and Fisher, 1974; Planas and Furuse, 1999), "se- 
quential correspondence" and "weigllted sequential 
correspondence". 
Each of tile similarity metrics eillpirically de- 
scribes the sintilarity between two inlmt strings tmi 
mid i~., 2 where we define tmi as a source language 
string taken fl'om the TM and i~. as the input string 
which we are seeking to 1hatch within the TM. 
One featnre of all similarity metrics given here is 
that they have fine-grained iscriminatory potential 
and are able to narrow down the final set of trans- 
lation candidates to a handfld of, and in nlost cases 
one, outlmt. This was a deliberate design decision, 
and aimed at example-based machine translation ap- 
plications, where human judgement cannot be relied 
upon to single out the most appropriate translation 
from multiple system outputs. In this, we set our- 
selves apart from the research of Sunlita and Tsut- 
sumi (1.991), for example, who judge the system to 
have been successful if there are a total of 100 or less 
outputs, aud a useful translation is contained within 
them. Note that it would be a relatively simple pro- 
2Note that the ordering here is arbitrary, and that all the 
similarity metrics described herein are commutative for the 
given implementations. 
36 
cedure  to fall ()lit the 11111111)e1" of Olltt)lltS to it ill o l l r  
case, tly taking tim top n ranking outputs. 
For all silnitarity metrics, we weight different 
.\]ai)mmse gment tyl)es according to their exl)ected 
impact on translation, in the form of the sweigh, t 
f l lnct iol l :  
Segment ype s,wcight 
punctuation 0 
other segments 1 
W(' exl)erinlentally trialled intermediate swcight set- 
tings tbr ditt'erent character tyl)es (in the case of 
character-based indexing) or segment yl)eS (in the 
case of word-based indexing), none of which was 
fomtd to apl)reciat)ly iml)rove performance. :~ 
a.1 Simi lar i ty metr ics used in this research 
Vector  space model  
Within our imt)lenmntation of the reactor space 
Inodol (VSM), the segment content of each string 
is (lescril)('.(l as a vector, ma(le u l) of 3 s ingle  dimen- 
sion for each segment tok(,n occurring within tmi or 
in. The. value of each vector eo lnt )onent  is given as 
the weighted frequen(-y of that token accor(ling to 
its sweiqht vahle,  such that any nulnber of 3 given 
i)un(:tuation mark will produce a fl'e(luen(:y of 0. The 
string sinfilarity of t?H, i and in is then detined sis tim 
cosine of the angle l/etween vectors t\[\[~.i and iT\[t, re- 
Sl)ectivety, calculated as: 
tT~,i, i~5, 
cos(t,fi,,,i;4 - It, ll l 0) 
where  dot  l ) roduct  and vect()r length  (:oin(:i(le wil;h 
l;he standard detlnitions. 
The strings tmi of maximal similarity are th()se 
whi(:h i ) roduce the  nmxinuun v3hw, for th(! v(~ctor 
cosine. 
Not(; that  VSM c(msi(lers (inly s(' .gment f re(tueney 
and is insensitive to word order. 
Token  intersect ion 
The token intersection of tmi 3nd in is defined as 
the cumulative intersecting fl'equency of tokens ap- 
pearing in each of the strings, normalised according 
to the combined segment lengths of tm, i and in. For- 
really, this equates to: 
tint(tm~, in) : e ? ~_~, l ' l i l l  (f,'{?(htnl (\[),frcqilz(,)) " m~(l,,,~)+>.,,(i,,) (2) 
where each t is a token (iccurring in e.ither tmi or 
in, freq,(t) is detined as the swei.qht-l)ased fi'equency 
of token t occurring in string s, and Ion(s) is tlm 
aIf anything, weighting down hi,agana characters, fin" ex- 
ample, due to their common occurrence as intlectional suffices 
or particles (as per Fujii and Croft (1993)) led to a significant 
drop in 1)eribrmanee. Simihwly, weighting down stop word- 
like flmetional parts-of-sf)eech in ,lat)anese had little eltiect, 
unlike weighting down stop words in the case of English (see 
below). 
segment length of string s, that is the swcight-1)ased 
COllllt Of segl l lel l ts (:(nltained ill .s'. 
As tbr VSM, the string(s) tmi most similar t;(i in 
arc thos(; which general;e the nlaximum value tbr 
tint(tmi, in). 
Note that word order does not take any part in 
calculation. 
Edit  d istance 
The first of the word order-sensitive methods is edit 
dist3nce (Wagner and Fisher, 1974; l?hmas and Fu- 
ruse, 1999). Essentially, the segment-lmsed it dis- 
tance 1)etwecn strings t'ln, i and in is the minimunl 
numl/er of prilnitive edit operations on single seg- 
ments required to transtbrm tmi into in (and vice 
versa), 1)ased Ul)On the ol)erations of segment equal- 
ity (segments tmi,m and in ,  are identical), segment 
deletion (delete segment a fl'OlIl a given 1)osition in 
string .s') and scgmc'nt insertion (insert segmen~ (t
into a given position in string .s). The cost asso- 
ciated with each ol)eration on segment a is defined 
~/S: 4 
Operat ion Cost 
segment equality () 
segment deletion swcigh, t(a ) 
s(;gment insertion swcigh, t(a) 
Unlike other similarity metrics, smaller v31ues in- 
dicate greater similarity for edit distance, and iden- 
tical strings have edit distmme 0. 
The woM order sensitivity of edit distance is per- 
\]ml)S t)est exeml)litie(l tly way of the following exam- 
1)le, where segment delimiters are given as :.'. 
(1) E - SN-  14-':winter r3in" 
(2a) 2F- $51. l+"summer  rain" 
(21)) 1+" SN- 2F "a rainy summer" 
Itere, the edit distance from (1) to (2a) is 1 -t- 1 = 2, 
as one deletion ol/eration is required to remove E 
\[\]:uyu\] "winter" and one insertion ol)eration required 
to 3dd 2F \[natu\] "summer". The edit distance from 
(1) to (21/), on the other hand, is 1 + 1 + 1 + 1 = 4 
despite (2b) being identical in segment content to 
(2a). In terms of edit distance, therefore, (23) is 
adjudged more similm" to (1) than (21)). 
Sequent ia l  cor respondence 
Sequential corresI)ondence is 3 measure of the m3x- 
innun subsl;ring sinlilarity lmtween tmi and in, nor- 
malised acc(irding to the comt)ined segment lengths 
h'.n(tmi) and len(in). Essentially, this method re- 
quires th3t all substring matches ubmatch (tmi, in) 
between tmi and in be calculated, and the maximum 
scqcorr ratio returned, where scqcorr is delined as: 
, . , 2?max\[su?,mateh(tml,in)\[ 
~ ~ " m~It.,,.)+t~.(~,) (3) 
1Note that dm costs  for deletion and insertioil must be 
equal to maintain commutativity. 
37 
IIere, tile cardinality operator applied to 
submatch(tmi,in) returns tile combined seg- 
ment length of matching substrings, weighted 
according to swcight. That is: 
I~,~ ..... t~(~., ,~.~,~)I=~,j  ~ .... igl~t(s,~j,,~) (4) 
for each segment ssj,t~ of each matching substring 
ssj G submatch(tmi, in). 
Returning to our exmnple from above, the simi- 
larity for (1) and (2a) is 2x2 2 whereas that for 
? 3+3 - -  g 
(1) and (2b)is ')x~ , 
3+3 ~ :~" 
Weighted sequential correspondence 
Weighted sequential correspondence--the lastof the 
word order-sensitive methods--~is an extension of se- 
quential correspondence. It attempts to sut)plement 
the deficiency of sequential correspondence that the 
contiguity of substring matches is not taken into 
consideration. Given input string a~ a2a.~a/,, for 
example, sequential correspondence would suggest 
equal similarity (of ~)  with strings a~ ba~ca:~da/, 
and aj ap. a3 a 4 cfg, despite the second of these being 
more likely to produce a translation at; least partially 
resembling tlmt of the intmt string. 
We get around this by associating all incremen- 
tal weight with each matelfing segment assessing 
the contiguity of left-neighl)ouring segments, in the 
manner (Inscribed by Sato (1992) for chaxactcr- 
based matclfing. Namely, the kth segment of a 
matched substring is given the multiplicative weight 
rain(k, Max), where Max was set to 4 in evaluation 
after Sato. I submatch,(tmi,iu,)l fi'om equation (3) 
thus t)ecomes: 
~ssj ~t,  rain ( k ? swcight(.ssj,~.),Ma, z) (5) 
tbr each sul)string ssj ~ submatch(tmi, i77,). \?e siln- 
ilarly modify tile definition of the lea flmction for a 
string s to: 
lea(s) =- E jmin  (j x sweight(.,'j),Max ) (6) 
for each segment .sj of s. 
3.2 Retrieval speed optirnisation 
While this paper is mainly concerned with accuracy, 
we take a moment out here to discuss the potential 
to accelerate the proposed methods, to get a feel for 
their relative speeds in actual retrieval. 
One immediate and effective way in which we can 
limit the search space for all methods is to use the 
current op-ranking score in establishing upper and 
lower t)ounds on the length of strings which have 
the potential to better that score. For token inter- 
section, for example, fi'om the fixed length lea(in) 
of input string in and current top score a, we can 
calculate the following bounds based on the greatest 
possible degree of lnatch between in and tmi: 
Upper bout, d: le,~(t.~d </(~-~)~n(~'~)J (7) L CZ 
_ F alen('in) 7 Lower bound: len(tmi) >,  2 - ( , ,  (8) 
In a similar fashion, we can stipulate a corridor of al- 
lowable segment lengths for tin i, for sequential corre- 
spondence and weighted sequential correspondence. 
For edit distance, we make the observation that tbr 
a current minimum edit distance of a, the following 
inequality over Icn(tmi) inust be satisfied for tmi to 
have a chance of bettering ct: 
len(in) - ~ < len(tmi) < len(in) + a (9) 
We can also limit the numl)er of string compar- 
isons required to reach the optimal match with in, 
by indexing each tmi by its component segments and 
working through the component segments of in in as- 
cending order of global fi'equency. At each iteration, 
we consider each previously unmatched translation 
record containing the current segment token, adjust- 
ing the upper and lower bounds as we go, given that 
translation records for a given iteration caiulot hmre 
contained segment okens already processed. The 
maxinmm possible segment correspondence b tween 
the strings is therefore decreasing on each iteration. 
We are also able to completely discomlt strings wit}l 
no segment component conunon with iTt in this way. 
Through these two methods, we were able to 
greatly reduce the number of string comparisons in 
word-based indexing evaluation for VSM, token in- 
tersection, sequential correspondence and weighted 
sequential correspondence methods in particular, 
and edit distance to a lesser degree. The degree of 
reduction for character-based indexing was not as 
marked, due to the massive increase in numbers of 
l;ranslation records sharing some character content 
with in. 
There is also considerable scope to accelerate 
the matching mechanisms used by the word order- 
sensitive approaches. Currently, all approaches are 
implemented in Perl 5, and the word order-sensitive 
approaches use a naive, highly recursive method to 
exhaustively generate all substring matches and de- 
ternfine the sinfilarity for each. One obvious way in 
which we could enhance this implelnentation would 
be to use an N-gram index as proposed by Nagao 
and Mori (1.994). Dynamic Programming (DP) tech- 
niques would undoubtedly lead to greater efficiency, 
as suggested by Crmfias et al (1995, 1997) and also 
Planas and Furuse (this volume). 
4 Eva luat ion  
4.1 Evaluation specifications 
Evaluation was partitioned off into character-based 
and word-based indexing for the vm'ious similarity 
methods. For word-based indexing, seginentation 
was carried out with ChaSen v2.0b (Matsmnoto et 
al., 1999). No attempt was made to post-edit he 
segmented outtmt, in interests of maintaining con- 
sistency in the data. Segmented and non-segmented 
strings were tested using a single program, with 
segment length set to a single character for non- 
segmented strings. 
As test data, we used 2336 unique translation 
records deriving fi'om technical field reports on con- 
struction machinery translated from Japanese into 
English. Translation records varied in size from 
38 
CIIAI{ACTEI{- 
BASEl) 
1NI)EXING 
\~)~() 1/J )- 
IL,\SEI) 
INI)I'iXING 
Similarity metric 
Vector space model (0.5) 
Token intersection (0.4) 
Edit distance (/cn(in))- 
Sequential corr. (0.4) 
Weighted seq. (:orr. (0.2) 
Vector sllace model (0.5) 
Token intersection (0.4) 
Edit distmme (h,n(in~- 
Sequential corr, (0.4) 
Weighted seq. corr. (0.2) 
Accuracy 
44.0 
44.3 
Edit 
diserep. 
4.86 
3.25 
1.82 
2.92 
2.89 
Ave,  
outputs 
1.04 (0.97) 
1.01 (0.99) 
1.39 (0.80) 
1.02 (0.98) 
1.04 (0.97) 
50.2 
46.6 
45.6 
43.7 (-0.8%) 
43.0 (-2.9%) 
47.3 (-5.9%) 
43.1 (-7.4%) 
40.7 (-10.7%) 
5.21 
3.12 
2.03 
3.06 
3.30 
1.17 (0.91) 
1.01 (0.99) 
1.90 (0.69) 
1.01 (0.99) 
1.14 (0.92) 
Ave.  
time 
2.14 
2.24 
4.75 
3.20 
4.10 
0.76 
0.88 
1.00 
1.10 
1.24 
Table 1: Results for the different similarity metri(:s under character-1)ased and word-based indexing 
single-word technical terms taken f1'Ol12 SI~ technical 
glossary, to multiple-sentence strings, at an average 
se.glnent length of 13.4 and average character length 
of 26.1. All .lapane, se strings of length 6 chara(:ters 
or more (a l;ol;al of 1802 strings) were extracted fl'om 
the Ix;st da.ta, leaving a resi(hle gh)ssary of te(:hni(:al 
1;erltls (533 strings) as we w(nfld not CXl)e('t to find 
use, hll nlat(:hes in the TM. The retrie, val a(:curacy 
()\,or the 1802 hmger strings was then vcritied t)y \] 0- 
fokt (:ross wflidation, including the glossary in the 
test TM on each iteration. 
Not(; that the test data was llre-1)artitioned into 
single technical terms, single sentences or sen- 
tence clusters, each constitut;i21g a single translation 
record. Partitions were taken as given in evaluation, 
whereas for reM-worhl TM systems, tim automal;i(m 
of th is  i)2"()cess (;Oltll)l'ises ;tll il211)ortalll; COlill)()ll(1Ilt 
of the (/verall sysI;(mL 1)re(',eding translation rel,ri(;val. 
While ackn()wh;(lging the i lnl)ort;an(:(; ()f this step and 
its int(;ra(:l;ion with r(?ri(;val 1)or\[ormall(:(;, we (:boost, 
to sideste l) it for the lmri)os(~s of this pal)c.r , and 
leave it for hltm(; resc.m(:h. 
In an effort to make evaluation as ol)jeci;ive and 
empirical as l)ossibh;, apl)r()i)riatencss of transla- 
tion candidate(s) l rOl)OSed by the different metri(:s 
was evahmted according to the mil2inlunl edit dis- 
tahoe between the translation candidate(s) and the 
unique model translation. In this, we transferred 1,t2(; 
edit distance, method described M)ove directly across 
to the ta.rg(% langustge, (English), with segments its 
words and the fl)lh)wing s'weight schema: 
Segment ype 
tmnctuation 
stop \VOl'dS 
other words 
swcight 
0 
0.2 
1 
Stol) words are defined as those containcd within the 
SMART (Salton, 197\].) stop word l ist) The system 
output was judged to be correct if it contained a 
translation optimally close to the model trmMation; 
the average ol)timal edit distance h'onl the model 
translation was 4.73. 
'5 \[tp://  fl, p.corne, ll.cs.ed U/l)U b/smar t/english,stop 
We set; the additional criterion that the difl'erent 
metrics hould be able to determine whether the top- 
ranking translation (:mMida.te is likeJy to be useflfl to 
the translator, and that no outlmt shouhl lm given if' 
the chlsest nmt('hing translation record was outside 
a certain l '~/Ilg( ~. Of "transla.ti(m uscflflness'. In p2"ac- 
tice, this was set to the, edit distance between the 
model translation and the empty string (i.e. the e.dit 
(:()st; of creating th(; model translation fl'(nn s(:ratch). 
This cut;off' 1)oint vlts realised for the different sim- 
ilarity metrics by thrcshohling over the similarit.y 
scores. The ditferent hresholds ettled Ull(m experi- 
mentally for all similarity metrics are given ill t)ra(:k- 
cts in the second column of Table 1, with the thresh- 
ohl for (;(lit, distance dynamicMly set t(/the edit dis- 
lane(; l~etween the input and tim eml)ty string. 
\Ve set (mrs(;\]ves al)art \]'IX)211 COIlV(;21I;i()IIsll 2'(~S(;D.l'('h 
()n TM r(;hieval lmrl'o2unan(:(; in a(lol)ting this ()l/- 
.i(;(:li\'(; mmmrical (~vahmti()n method. Traditionally, 
r(:i.ri(~val l)erformalm(~ has 1)(!e,n gauged 1)y tlm sub- 
j(~(:t;iv(; useflfln(;ss of the closest matching e.lenmnt of 
the syst;(~lll OUtlmt (as judged 1)y a. hunm,d, mid de- 
scribed by way of a dis(:rete set; of transla.tion (lualit;y 
des('ril)tors ((;.g. (Nakm2mra, 1989; Smnita and Tsut- 
smni, 1991; Sato, 1992)). Perhaps the closest evalua- 
tion a.tte2nt)ts o what we prol)ose are those of' Planas 
and Nn'use (1.999) in s(!tting a mechanical cutoff for 
"translation usability" as the al/ility to generate the 
model translation from a given translation candidate 
1)y editing less than half the component words, and 
Nirenburg et al (1993) ill calculating the weighted 
mmtber of key strokes r(;quirexl to convert he system 
outllut into ;m apl)ropriate translation for the orig- 
inal inllut. Tile method of Nirenburg et al (1993) 
is certainly more indicative of t:rue target language 
useflllness, but is dependent 022 the coml)etence of 
the translator editing the TM system output, and 
not automated to the degree our method is. 
4.2 Resu l ts  
The results for the different similarity metrics with 
character-based and word-based indexing are given 
in Tal)le 1, with the two bag-of-words al)t)roaches 
partitioned off from the three word order-s(msitive 
al)I)roaches tor ea(:h indexing paradigm. "Accuracy" 
is an indication of the prol)ortion of intmts fbr whi(:h 
39 
an optimal translation was produced; character- 
based indexing accuracies in bold indicate a signifi- 
cant ~ advantage over the corresponding wprd-based 
indexing accuracy, and figures in brackets for word- 
based indexing indicate the relative pert'ormaime 
gain over the corresponding character-based index- 
ing configuration. "Edit discrep." refers to the mean 
minimum edit distance discrepancy between trans- 
lation candidate(s) and optimal translation(s) in the 
case of the translation candidate set containiug uo 
optimal translations. "Ave. outputs" describes the 
average number of translation candidates output by 
the system, with the figure in brackets being the 
proportion of int)uts for which a unique translation 
candidate was produced. "Ave. time" describes the 
average time taken to deterlnine the translation era> 
didate(s) for a single output, relative to the time 
taken tbr word-based edit distance retrieval. 
Perhaps the most striking result is ttmt character- 
based indexing produces a superior match accuracy 
to word-based indexing tbr all similarity metrics, at; 
a significant margin tbr all three word order-based 
methods. This is the complete opposite of what we 
had expected, although it does fit in with the find- 
ings of Fujii and Croft (1993) that character-based 
indexing performs comparably with word-based in- 
dexing in Japanese information retrieval. 
Looking to word order, we see that edit distance 
outperforms all other methods for t)oth character- 
and word-based indexing, peaking at just over 50% 
for character-based indexing. Tile relative perfor- 
mance of the remaining methods is variable, with 
the two bag-of-words methods being superior to or 
roughly equivalent to sequential correspondence and 
weighted sequential correspondence tbr word-based 
indexing, but tile word order-based methods having 
a cleat' advantage over the bag-of-words methods for 
character-based indexing. It is thus difticult to draw 
any hard and fast conclusion as to the relative merits 
of word order-based versus bag-of words methods, 
other than to say that edist distance would appear 
to have a clear advantage over other methods. 
The figures for edit discrepancy in the case of non- 
optimal translation candidate(s) are equally inter- 
esting, and suggest hat on the whole, the various 
methods err more conservatively for character-based 
than word-based indexing. The most robust method 
is (source language) edit distance, at all edit dis- 
crepancy of 1.82 and 2.O3 for character-based and 
word-based indexing, respectively. 
All methods were able to produce just over one 
translation candidate on average, with all other than 
edit distance returning a unique translation candi- 
date over 90% of the time. The greater number of 
outtmts for the edit distance method can certainly 
be viewed as one reason for its inflated performance, 
although the lower level of mnbiguity for character- 
based indexing but higher accuracy, would tend to 
suggest otherwise. 
Lastly, word-based indexing was found to be faster 
than character-based indexing across the board, for 
the simple reason that the immber of character seg- 
~As determined by the paired t test (p < 0.05). 
ments is always going to be greater than or equal 
to the number of word segments. The average seg- 
ment lengths quoted above (26.1 characters vs. 13.4 
words) indicate that we generally have twice as many 
characters as words in a given striug. Additionally, 
tile acceleration technique described in ? 3.2 of se- 
quentially working through the segment component 
of the input string in increasing order of global fre- 
quency, has a greater ett>ct for word-tmsed index- 
ing than character-based indexing, accentuating any 
speed disparity. 
4.3 Ref lec t ions  on  the  resul ts  
An immediate xlflanation tbr character-based in- 
dexing's empirical edge over word-based iudexing is 
the semantic smoothing effects of individual kanji 
characters, alluded to above (? 2). To take an exam- 
ple, the single-segment ouns A': n \[s6sa\] and : ng0 
\[sadS\] both mean "operation", but would not match 
under word-based indexing. Character-based index- 
ing, on the other hand, would recogifise the overlap 
in character content, and in the process pick up on 
the semantic orresi)ondenee b tween the two words. 
To take tile opposite tack, one reason wily word- 
based indexing may have been disadvantaged is the 
we did not stem or lemmatise words in word-based 
indexing. Having said this, the. output fl'om ChaSen 
is such that stems of inflecting words are given as 
a single segment, with inflectional morphemes each 
presented as sel)arate segments. In this sense, stem- 
ruing would only act to delete the inflectional mor- 
phemes, and not add allything new. 
Another way in which the outlmt of ChaSen 
could conceivably have atlbcted retrieval perfor- 
iilance is that technical terms tended to be over- 
segmented. Experilnentally combining recognised 
technical terms into a single segment (particularly 
in the case of contiguous katakana segments in the 
manner of Nljii and Croft (1993)), however, de- 
graded rather than lint)roved retrieval performance 
for both character-based and word-based indexing. 
As such, this side-etfect of ChaSen would not appear 
to have impinged on retriewfl accuracy. 
One other plausible reason for tile unexpected re- 
sults is that the test data could have been ill some 
way inherently better suited to character-based in-
dexing than word-based indexing, although the fact 
that the results were cross-wtlidatcd would tend to 
rule out this possibility. 
A surprising result was the lacklustre performance 
of the weighted sequential correspondence method as 
compared to simple sequential correspondence. We 
have no explanation for the drop in accuracy, other 
than to speculate that either the proposed formu- 
lation is in some way flawed or contiguity of match 
does not impinge on translation similarity to the de- 
gree we had expected. 
To return to the original question posed above of 
retrieval speed vs. accuracy, the word order-sensitive 
edit distance approach would seem to hold a gen- 
uine edge over the other methods, to an order that 
would suggest he extra computational overhead is 
warranted, ill both accuracy and translation discrep- 
ancy. It must be said that the TM used in evalua- 
40 
tion was too small to get a gemfine f(;el for the com- 
t)ul;ational overhead that would 1)e cxp(,,ri(;ncc, d in 
~ real-world TM system context of t)ot;entially mil- 
lions rath(;r than thousands of translation records. 
A C the saint', (tim(;, however, coding Ul) the c(lit dis- 
tan(:(; l)roc(',dure in a language fasto, r than Perl using 
chara(;l;(?r ~d;h(~,r \[;\]lall SI;t'illg COIlq)arisol~ 1)roc(?(hlrcs 
mid ai)l)lying (lynami(" 1)rogl'amming t(whni(lu(,,s or 
similar, may well oIl~set h('. large \]nero.as(; in number 
of comparisons dcmand(',d of the system. 
5 Concluding remarks 
This research is concerned with l;}m r(;lativ(~ iml)orl; 
ot7 word order and segm(mta.1;ion n translation re- 
l;rieval i)erformmlc(~ tbr a TM system. Wc mo(Ml('xl 
the elthcts of word order s(msitivity vs. 1)ag-of-wol'dS 
word order ins(msit;ivity 1)y iml)l(mmnl,ing a total of 
live similarity mcla'ics: two bag-of-words al)proach(',s 
(lhe v(',(:tor spa(:(; model and "tol?('.n int(us(!(:tion") 
and tin'('.(', w()r(l ord(',r-s(;nsitive al)l)roach(',s ((',(lit; dis- 
tan(:('., "s(;quential corr(',Sl)ond(',nce" and "wcight(',d 
sequential corr(~st)ondenc(?'). Ea(:h of th(;s(; nw, tri(',s 
was then l;(~sl;e(t Hll(ler (:har;~cl;(;r-1)as(~(\[ al~(t word- 
based in(h'~xing, to deto, rmin(,~ what (;tt'c(:t s(~gm(',nta- 
l;ion wouhl have, on r('.trieval 1)(~rl'orman(:(h Eml)iri- 
c~d evaluation })asc, d }l l 'O l l l ld  \[,h( ~, l;alg(!l, languag(', (;(tit 
distance of t)rot)osed traiMa.tion can(lidal(',s r(~vcaicd 
that (:hara(:tcr-1)ascd indexing consist(mtly produ(:ed 
gr('~atcr accuracy than wordq)ased in(lexiltg; and thai; 
the word or(l(~r-s('atsitivo~ (;(lit distain:(', m(;tri(: clearly 
outl)(',rforme(1 all other methods un(h',r 1)oth in(l(',xing 
paradigms. 
The main area in wlfi(',h we, fc!d this r(~s(!ar(:h c(mht 
1)c, (mhan(:(~d is to validate th(~ findings of this 1)a- 
per in (~Xlmn(ling evahlati()n 1o olh(w domains mid 
l;esl; Set,q, whi(:h wc h'av(', as ;lll il:(?lll 1'()1 t'ulm(~ re- 
s(mr(:h. We also skirl;ed m'(mnd lira issu(~ ()f lrmls- 
lation record partitioning, and wish 11)inv(!stigale 
how difl'(;r(mt 1)mtitioning m(~'tho(ls lmrfl)rm againsl; 
c,;mh other. One important area in which w(; hop(~ 
to eXl)and our resem'ch is to look at tim etl'(~(:ts of 
character type on chm'act(',r-bas(~d in exing, t(anji 
would a,ppear to be helping the case of character- 
based indexing at t)rc, s(mt, ;rod it woul(\[ 1)e highly 
r(;vcaling to look at wh(',th(',r COml)ara,1)l(', ro, sults to 
t\]losc 1)r(:s(;nt('d h(;r(~ would 1)(', t)ro(ht(:ed \[or full 
kaim-basc'd (alphal)c, ti(:) ,lal)an(',sc input, or otlmr 
all)hal)ct-1)ased n(m-s(~gm(ulting languages such as 
Thai. 
Acknowledgements  
Vital input into this research was rcc(~ivcd t?om 
Francis Bond (NTT), Emmanu(;1 Planas (NTT), and 
three anonylllOUS reviewers. 
References 
% Baldwin mul Ill. Tanaka. 1999. The applications of 
unsul)crvised learning to ,I~tl)mmsc gral)\]mmc,-1)honcin(~ 
aligmnent,. In l'roc, of th.e. AUL I.Vortc.d~op oa Uu- 
supervised Learning in Natu'ral Language l~roccs.sin9, 
pages 9 16. 
L. Cranias, H. Ibqmgr.orgiou, and S. Pilmridis. 1995. 
A Matching Technique in Example-Based Machine 
Translation. cmp-lg/9508005. 
L. Cranias, H. Papageorgiou, and S. Piper\]dis. 1997. E?- 
amt)h~ retrieval from a trmlslation memory. Natwral 
Language \]'Jngine.ering, 3(4):255 77. 
1t. Fuji\] and W.B. Croft. 1.993. A comparison of index- 
ing tc(:lmiqu(~s fl)r .lal)ancsc t;c.x|; r('.trieval. In Proc. 
of 161h International ACM-SIGH~, Cot@fence on Re- 
search and Dc'vclopmcnt in Information Ib:tricval (SI- 
GIR'93), pages 237 46. 
It;. Kitamura and II. "~Smmmoto. 1996. Translation 
retrieval systo.m using alignment data flom parallc.l 
texts, in P~wc. of the 5&'d Annual Mccting of tit(" 
II'S,I, volmne 2, pages 385 6. (Ill Ja.t)ancsc ). 
C. Manning and II. S(:hiil;ze. 1999. Foundations of Sta- 
tistical Natural La'ngurtgc P~vccssing. MIT Press. 
Y. Matsmnoto, A. I(i/,auchi, T. Yamashita, and Y. IIi- 
rano. 1999. ,\]apancsc Moudtolo.qical Analysis S?/s- 
l, cm UttaScn Version 2.0 Manual. ~lt'~chnical l/.eporl; 
NAISqUIS-Tl199009, NAIST. 
M. Nagao and S. Mort. 1994. A new method of N-grant 
statist;its tbr large mmflmr of N and ;mtonmtic ex- 
\[;ra(;1;ion of words and l)hrases front large text; data 
of .lapanese. In Proc. of the 15th, lntc~'~u~tioual Con- 
Jcrcncc on Computational Linguistics (COLING '9/~), 
pages 611- 5. 
N. Nakamma. 1989. ~l?~mslat, ion supl)orf by retrieving 
bilingual texts. In l'~wc, of the 38th Annual Mcctin 9 
of the IPSJ, volume 1, pagt;s 357 8. (In Jai)ancs(; ).
S. Nirelflmrg, C. l)omashnc.v, and \]).J. Gramms. 1993. 
Two apt)roa(:hes to mat;thing in eXaml)h>bas(~d rim- 
chin(', translation. In Proc. of the 5th International 
CoT@:rc'ncc on 771corctical and Mcthodologic(d lasucs 
i'tl. Math, inc. 7!ransl,,tio'a 151'M1-93), pages d7 57. 
E. Planas and (). l:uruse. 1999. F(wmalizing translation 
m(m,n'ies. In l)Twc, o.f Math\]n(: Translation ,%m'mit 
VII, pages 331 9. 
1'2. Planas. 1998. A Case, Study on Memory Based Ma- 
chine ~}'anslation 7bols. Phi) Felkm~ \Vorking 1)al)c.r, 
Unil;ed Nations University. 
G. Salton. 1971. The SMAR, T It, err\]oval Sy.stevt: E:rpcr- 
ime.nt.s in Automatic Document Processing. Prentice- 
Hall. 
S. Sato and 3'. Kawase. 1994. A ltigh-Spc.ed B(:st Match 
i{e.tricval Method fin" ,\]apancsc ~}:'a;t. Tct:lulical Rctmrt; 
1S-11R-94-9I, JAIST. 
S. Sato. 1992. CTM: An examlfl('A)ased translation aid 
system. In l"~vc, of the 141h International Confcrc.ncc 
on Computational Linguistics (COLING '92), pages 
1259 63. 
E. Smnit;}~ mtd Y. Tsutsumi. 1991. A 1)ract,ical method 
of retrieving similar examples 1or trmMation aid. 
7Yansaction,s of the IEICE, J74-D-II(10):1437 47. (In 
Japanese). 
It. Tanaka. 1.997. An efficient way of gauging siinilar- 
ity lmtwcen hmg .lalmnc, so, expressions. In Informa- 
tion l~roccssin9 ,%ciety of Japan SIG Notes, vohun(! 
,1t7, no. 85, 1)ages 69 74. (In .l~q)aneso,). 
A. Wagner and M. Fisher. 1974. The' string-to-string 
correction 1)roblcm. Journal of the A CM, 21(1):168 
73. 
41 
Low-cost, High-performance Translation Retrieval:
Dumber is Better
Timothy Baldwin
Department of Computer Science
Tokyo Institute of Technology
2-12-1 O-okayama, Meguro-ku, Tokyo 152-8552 JAPAN
tim@cl.cs.titech.ac.jp
Abstract
In this paper, we compare the rela-
tive effects of segment order, segmen-
tation and segment contiguity on the
retrieval performance of a translation
memory system. We take a selec-
tion of both bag-of-words and segment
order-sensitive string comparison meth-
ods, and run each over both character-
and word-segmented data, in combina-
tion with a range of local segment con-
tiguity models (in the form of N-grams).
Over two distinct datasets, we find that
indexing according to simple character
bigrams produces a retrieval accuracy
superior to any of the tested word N-
gram models. Further, in their optimum
configuration, bag-of-words methods are
shown to be equivalent to segment order-
sensitive methods in terms of retrieval
accuracy, but much faster. We also pro-
vide evidence that our findings are scal-
able.
1 Introduction
Translation memories (TMs) are a list of
translation records (source language strings
paired with a unique target language translation),
which the TM system accesses in suggesting a
list of target language (L2) translation candi-
dates for a given source language (L1) input (Tru-
jillo, 1999; Planas, 1998). Translation retrieval
(TR) is a description of this process of selecting
from the TM a set of translation records (TRecs)
of maximum L1 similarity to a given input. Typi-
cally in example-based machine translation, either
a single TRec is retrieved from the TM based on
a match with the overall L1 input, or the input
is partitioned into coherent segments, and indi-
vidual translations retrieved for each (Sato and
Nagao, 1990; Nirenburg et al, 1993); this is the
first step toward generating a customised transla-
tion for the input. With stand-alone TM systems,
on the other hand, the system selects an arbitrary
number of translation candidates falling within a
certain empirical corridor of similarity with the
overall input string, and simply outputs these for
manual manipulation by the user in fashioning the
final translation.
A key assumption surrounding the bulk of past
TR research has been that the greater the match
stringency/linguistic awareness of the retrieval
mechanism, the greater the final retrieval accu-
racy will become. Naturally, any appreciation in
retrieval complexity comes at a price in terms of
computational overhead. We thus follow the lead
of Baldwin and Tanaka (2000) in asking the ques-
tion: what is the empirical effect on retrieval per-
formance of different match approaches? Here,
retrieval performance is defined as the combina-
tion of retrieval speed and accuracy, with the ideal
method offering fast response times at high accu-
racy.
In this paper, we choose to focus on retrieval
performance within a Japanese?English TR con-
text. One key area of interest with Japanese
is the effect that segmentation has on retrieval
performance. As Japanese is a non-segmenting
language (does not explicitly delimit words or-
thographically), we can take the brute-force ap-
proach in treating each string as a sequence of
characters (character-based indexing), or al-
ternatively call upon segmentation technology in
partitioning each string into words (word-based
indexing). Orthogonal to this is the question of
sensitivity to segment order. That is, should our
match mechanism treat each string as an unor-
ganised multiset of terms (the bag-of-words ap-
proach), or attempt to find the match that best
preserves the original segment order in the in-
put (the segment order-sensitive approach)?
We tackle this issue by implementing a sample
of representative bag-of-words and segment order-
sensitive methods and testing the retrieval per-
formance of each. As a third orthogonal param-
eter, we consider the effects of segment contigu-
ity. That is, do matches over contiguous segments
provide closer overall translation correspondence
than matches over displaced segments? Segment
contiguity is either explicitly modelled within the
string match mechanism, or provided as an add-in
in the form of segment N-grams.
To preempt the major findings of this pa-
per, over a series of experiments we find that
character-based indexing is consistently superior
to word-based indexing. Furthermore, the bag-
of-words methods we test are equivalent in re-
trieval accuracy to the more expensive segment
order-sensitive methods, but superior in retrieval
speed. Finally, segment contiguity models provide
benefits in terms of both retrieval accuracy and
retrieval speed, particularly when coupled with
character-based indexing. We thus provide clear
evidence that high-performance TR is achievable
with naive methods, and moreso that such meth-
ods outperform more intricate, expensive meth-
ods. That is, the dumber the retrieval mechanism,
the better.
Below, we review the orthogonal parameters of
segmentation, segment order and segment conti-
guity (? 2). We then present a range of both bag-
of-words and segment order-sensitive string com-
parison methods (? 3) and detail the evaluation
methodology (? 4). Finally, we evaluate the dif-
ferent methods in a Japanese?English TR context
(? 5), before concluding the paper (? 6).
2 Basic Parameters
In this section, we review three parameter types
that we suggest impinge on TR performance,
namely segmentation, segment order, and segment
contiguity.
2.1 Segmentation
Despite non-segmenting languages such as
Japanese not making use of segment delimiters,
it is possible to artificially partition off a given
string into constituent morphemes through the
process of segmentation. We will collectively
term the resultant segments as words for the
remainder of this paper.
Looking to past research on string compari-
son methods for TM systems, almost all sys-
tems involving Japanese as the source lan-
guage rely on segmentation (Nakamura, 1989;
Sumita and Tsutsumi, 1991; Kitamura and Ya-
mamoto, 1996; Tanaka, 1997), with Sato (1992)
and Sato and Kawase (1994) providing rare in-
stances of character-based systems. This
is despite Fujii and Croft (1993) providing evi-
dence from Japanese information retrieval that
character-based indexing performs comparably to
word-based indexing. In analogous research,
Baldwin and Tanaka (2000) compared character-
and word-based indexing within a Japanese?
English TR context and found character-based in-
dexing to hold a slight empirical advantage.
The most obvious advantage of character-based
indexing over word-based indexing is that there
is no pre-processing overhead. Other arguments
for character-based indexing over word-based in-
dexing are that we: (a) avoid the need to com-
mit ourselves to a particular analysis type in the
case of ambiguity or unknown words; (b) avoid
the need for stemming/lemmatisation; and (c) to
a large extent get around problems related to the
normalisation of lexical alternation.
Note that all methods described below are ap-
plicable to both word- and character-based index-
ing. To avoid confusion between the two lexeme
types, we will collectively refer to the elements of
indexing as segments.
2.2 Segment Order
Our expectation is that TRecs that preserve the
segment order observed in the input string will
provide closer-matching translations than TRecs
containing those same segments in a different or-
der.
As far as we are aware, there is no TM sys-
tem operating from Japanese that does not rely
on word/segment/character order to some degree.
Tanaka (1997) uses pivotal content words identi-
fied by the user to search through the TM and
locate TRecs which contain those same content
words in the same order and preferably the same
segment distance apart. Nakamura (1989) simi-
larly gives preference to TRecs in which the con-
tent words contained in the original input occur in
the same linear order, although there is the scope
to back off to TRecs which do not preserve the
original word order. Sumita and Tsutsumi (1991)
take the opposite tack in iteratively filtering
out NPs and adverbs to leave only functional
words and matrix-level predicates, and find TRecs
which contain those same key words in the
same ordering, preferably with the same seg-
ment types between them in the same num-
bers. Sato and Kawase (1994) employ a more lo-
cal model of character order in modelling similar-
ity according to N-grams fashioned from the orig-
inal string.
2.3 Segment contiguity
Given the input ?1?2?3?4 , we would expect that
of ?1?1?2?2?3?3?4 and ?1?2?3?4?1?2?3 , the
latter would provide a translation more reflective
of the translation for the input. This intuition
is captured either by embedding some contiguity
weighting facility within the string match mecha-
nism (in the case of weighted sequential correspon-
dence ? see below), or providing an independent
model of segment contiguity in the form of seg-
ment N-grams.
The particular N-gram orders we test are simple
unigrams (1-grams), pure bigrams (2-grams), and
mixed unigrams/bigrams. These N-gram models
are implemented as a pre-processing stage, fol-
lowing segmentation (where applicable). All this
involves is mutating the original strings into N-
grams of the desired order, while preserving the
original segment order and segmentation schema.
From the Japanese string? ?? ?? [natu?no?ame]
?summer rain?,1 for example, we would generate
the following variants (common to both character-
and word-based indexing):
1-gram: ? ?? ??
2-gram: ?? ???
Mixed 1/2-gram: ? ??? ?? ??? ??
3 String Comparison Methods
As the starting point for evaluation of the
three parameter types targeted in this re-
search, we take two bag-of-words (segment order-
oblivious) and three segment order-sensitive meth-
ods, thereby modelling the effects of segment or-
der (un)awareness. We then run each method over
both segmented and unsegmented data in combi-
nation with the various N-gram models proposed
above, to capture the full range of parameter set-
tings.
The particular bag-of-word approaches we tar-
get are the vector space model (Manning and
Schu?tze, 1999, p300) and ?token intersection?.
For segment order-sensitive approaches, we test
3-operation edit distance and similarity, and also
?weighted sequential correspondence?.
All methods are formulated to operate over an
arbitrary wt schemata, although in L1 string com-
parison throughout this paper, we assume that
any segment made up entirely of punctuation is
given a wt of 0, and any other segment a wt of 1.
1 Character boundaries (which double as word
boundaries in this case) indicated by ???.
All methods are subject to a threshold on
translation utility, and in the case that the
threshold is not achieved, the null string is re-
turned. The various thresholds are as follows:
Comparison method Threshold
Vector space model 0.5
Token intersection 0.4
3-operation edit distance len(IN)
3-operation edit similarity 0.4
Weighted seq. correspondence 0.2
where IN is the input string, and len is the con-
ventional segment length operator.
Various optimisations were made to each string
comparison method to reduce retrieval time, of the
type described by Baldwin and Tanaka (2000).
While the details are beyond the scope of this pa-
per, suffice to say that the segment order-sensitive
methods benefited from the greatest optimisation,
and that little was done to accelerate the already
quick bag-of-words methods.
3.1 Bag-of-Words Methods
Vector Space Model
Within our implementation of the vector
space model (VSM), the segment content of each
string is described as a vector, made up of a single
dimension for each segment type occurring within
S or T . The value of each vector component is
given as the weighted frequency of that type ac-
cording to its wt value. The string similarity of S
and T is then defined as the cosine of the angle
between vectors S and T , respectively, calculated
as:
cos(S, T ) =
S ? T
|
S||T |
=
?
j
sjtj
?
?
j
s2j
?
?
j
t2j
Token Intersection
The token intersection of S and T is de-
fined as the cumulative intersecting frequency of
segment types appearing in each of the strings,
normalised according to the combined segment
lengths of S and T using Dice?s coefficient. For-
mally, this equates to:
tint(S,T ) =
2 ?
?
e?S,T
min
(
freqS(e), freqT (e)
)
len(S) + len(T )
where each e is a segment occurring in either S or
T , freqS(e) is defined as the wt-based frequency of
segment type e occurring in string S, and len(S)
is the segment length of string S, that is the wt-
based count of segments contained in S (similarly
for T ).
3.2 Segment Order-sensitive Methods
3-op Edit Distance and Similarity
Essentially, the segment-based 3-operation
edit distance between strings S and T is the min-
imum number of primitive edit operations on sin-
gle segments required to transform S into T (and
vice versa). The three edit operations are seg-
ment equality (segments si and tj are identical),
segment deletion (delete segment si) and segment
insertion (insert segment a into a given position
in string S). The cost associated with each opera-
tion is determined by the wt values of the operand
segments, with the exception of segment equality
which is defined to have a fixed cost of 0.
Dynamic programming (DP) techniques are
used to determine the minimum edit distance
between a given string pair, following the clas-
sic 4-operation edit distance formulation of
Wagner and Fisher (1974).2 For 3-operation edit
distance, the edit distance between strings S =
s1s2...sm and T = t1t2...tn is defined as
D3op(S, T ):
D
3op(S,T ) = d3(m,n)
d
3
(i, j) =
?
?
?
?
?
?
0 if i = 0 ? j = 0
d
3
(0, j ? 1) + wt(tj) if i = 0 ? j = 0
d
3
(i ? 1, 0) + wt(si) if i = 0 ? j = 0
min
(
d
3
(i ? 1, j) + wt(si),
d
3
(i, j ? 1) + wt(tj),
m
3
(i, j)
)
otherwise
m
3
(i, j) =
{
d
3
(i ? 1, j ? 1) if si = sj
? otherwise
It is possible to normalise operation edit dis-
tance D3op into 3-operation edit similarity
S3op by way of:
S3op(S, T ) = 1 ?
D3op(S, T )
len(S) + len(T )
Weighted Sequential Correspondence
Weighted sequential correspondence (originally
proposed in Baldwin and Tanaka (2000)) goes one
step further than edit distance in analysing not
only segment sequentiality, but also the contiguity
of matching segments.
Weighted sequential correspondence associates
an incremental weight (orthogonal to our wt
weights) with each matching segment assessing the
contiguity of left-neighbouring segments, in the
manner described by Sato (1992) for character-
based matching. Namely, the kth segment of
a matched substring is given the multiplicative
weight min(k,Max ), where Max is a positive in-
teger. This weighting up of contiguous matches
is facilitated through the DP algorithm given be-
low:
Sw(S, T ) = s(m,n)
s(i, j) =
{
0 if i = 0 ? j = 0
max
(
s(i ? 1, j),
s(i, j ? 1),
s(i ? 1, j ? 1) + mw(i, j)
)
otherwise
mw(i, j) =
{
cm(i, j) ? wt(i) if si = sj
0 otherwise
cm(i, j) =
{
0 if i = 0 ? j = 0 ? si = tj
min(Max , cm(i? 1, j ? 1) + 1) otherwise
2 The fourth operator in 4-operation edit distance
is segment substitution.
The final similarity is determined as:
WSC (S,T ) =
2 ? Sw(S,T )
lenWSC (S) + lenWSC (T )
where lenWSC (S) is the weighted length of S, de-
fined as:
lenWSC (S) =
?m
i=1
wt(si) ? min(Max , i)
4 Evaluation Specifications
4.1 Details of the Dataset
As our main dataset, we used 3033 unique
Japanese?English TRecs extracted from construc-
tion machinery field reports for the purposes of
this research. Most TRecs comprise a single sen-
tence, with an average Japanese character length
of 27.7 and English word length of 13.3. Impor-
tantly, our dataset constitutes a controlled lan-
guage, that is, a given word will tend to be trans-
lated identically across all usages, and only a lim-
ited range of syntactic constructions are employed.
In secondary evaluation of retrieval performance
over differing data sizes, we extracted 61,236
Japanese?English TRecs from the JEIDA parallel
corpus (Isahara, 1998), which is made up of gov-
ernment white papers. The alignment granular-
ity of this second corpus is much coarser than for
the first corpus, with a single TRec often extend-
ing over multiple sentences. The average Japanese
character length of each TRec is 76.3, and the av-
erage English word length is 35.7. The language
used in the JEIDA corpus is highly constrained,
although not as controlled as that in the first cor-
pus.
The construction of TRecs from both corpora
was based on existing alignment data, and no fur-
ther effort was made to subdivide partitions.
For Japanese word-based indexing, segmenta-
tion was carried out primarily with ChaSen v2.0
(Matsumoto et al, 1999), and where specifically
mentioned, JUMAN v3.5 (Kurohashi and Nagao,
1998) and ALTJAWS3 were also used.
4.2 Semi-stratified Cross Validation
Retrieval accuracy was determined by way of
10-fold semi-stratified cross validation over the
dataset. As part of this, all Japanese strings of
length 5 characters or less were extracted from
the dataset, and cross validation was performed
over the residue, including the shorter strings in
the training data (i.e. TM) on each iteration.
In N-fold stratified cross validation, the dataset
is divided into N equally-sized partitions of uni-
form class distribution. Evaluation is then carried
out N times, taking each partition as the held-
out test data, and the remaining partitions as the
training data on each iteration; the overall accu-
racy is averaged over the N data configurations.
As our dataset is not pre-classified according to a
discrete class description, we are not able to per-
form true data stratification over the class distri-
bution. Instead, we carry out ?semi-stratification?
over the L1 segment lengths of the TRecs.
3
http://www.kecl.ntt.co.jp/icl/mtg/resources/altjaws.html
4.3 Evaluation of the Output
Evaluation of retrieval accuracy is carried out ac-
cording to a modified version of the method pro-
posed by Baldwin and Tanaka (2000). The first
step in this process is to determine the set of ?op-
timal? translations by way of the same basic TR
procedure as described above, except that we use
the held-out translation for each input to search
through the L2 component of the TM. As for L1
TR, a threshold on translation utility is then ap-
plied to ascertain whether the optimal translations
are similar enough to the model translation to be
of use, and in the case that this threshold is not
achieved, the empty string is returned as the sole
optimal translation.
Next, we proceed to ascertain whether the ac-
tual system output coincides with one of the opti-
mal translations, and rate the accuracy of each
method according to the proportion of optimal
outputs. If multiple outputs are produced, we se-
lect from among them randomly. This guaran-
tees a unique translation output and differs from
the methodology of Baldwin and Tanaka (2000),
who judged the system output to be ?correct? if
the potentially multiple set of top-ranking outputs
contained an optimal translation, placing methods
with greater fan-out of outputs at an advantage.
So as to filter out any bias towards a given string
comparison method in TR, we determine transla-
tion optimality based on both 3-operation edit dis-
tance (operating over English word bigrams) and
also weighted sequential correspondence (operat-
ing over English word unigrams). We then de-
rive the final translation accuracy as the average
of the accuracies from the respective evaluation
sets. Here again, our approach differs from that
of Baldwin and Tanaka (2000), who based deter-
mination of translation optimality exclusively on
3-operation edit distance (operating over word un-
igrams), a method which we found to produce a
strong bias toward 3-operation edit distance in L1
TR.
In determining translation optimality, all punc-
tuation and stop words were first filtered out of
each L2 (English) string, and all remaining seg-
ments scored at a wt of 1. Stop words are defined
as those contained within the SMART (Salton,
1971) stop word list.4
Perhaps the main drawback of our approach
to evaluation is that we assume a unique model
translation for each input, where in fact, multiple
translations of equivalent quality could reasonably
be expected to exist. In our case, however, both
corpora represent relatively controlled languages
and language use is hence highly predictable. The
proposed evaluation methodology is thus justified.
5 Results and Supporting Evidence
5.1 Basic evaluation
In this section, we test our five string comparison
methods over the construction machinery corpus,
under both character- and word-based indexing,
and with each of unigrams, bigrams and mixed
unigrams/bigrams. The retrieval accuracies and
times for the different string comparison meth-
ods are presented in Figs. 1 and 2, respectively.
4
ftp://ftp.cornell.cs.edu/pub/smart/english.stop
50
52
54
56
58
60
62
V
S
M
T
IN
T
3o
p
D
3o
p
S
W
S
C
V
S
M
T
IN
T
3o
p
D
3o
p
S
V
S
M
T
IN
T
3o
p
D
3o
p
S
R
et
ri
ev
a
l 
a
cc
u
ra
cy
 (
%
)
String comparison method
Word-based indexingChar-based indexing
*
* *
*
*
*
1-gram 2-gram 1/2-gram
Figure 1: Basic retrieval accuracies
Here and in subsequent graphs, ?VSM? refers to
the vector space model, ?TINT? to token inter-
section, ?3opD? to 3-op edit distance, ?3opS? to
3-op edit similarity, and ?WSC? to weighted se-
quential correspondence; the bag-of-words meth-
ods are labelled in italics and the segment order-
sensitive methods in bold. In Figs. 1 and 2, results
for the three N-gram models are presented sepa-
rately, within each of which, the data is sectioned
off into the different string comparison methods.
Weighted sequential correspondence was tested
with a unigram model only, due to its inbuilt mod-
elling of segment contiguity. Bars marked with an
asterisk indicate a statistically significant5 gain
over the corresponding indexing paradigm (i.e.
character-based indexing vs. word-based indexing
for a given string comparison method and N-gram
order). Times in Fig. 2 are calibrated relative to
3-operation edit distance with word unigrams, and
plotted against a logarithmic time axis.
Results to come from these figures can be sum-
marised as follows:
? Character-based indexing is consistently su-
perior to word-based indexing, particularly
when combined with bigrams or mixed uni-
grams/bigrams.
? In terms of raw translation accuracy, there is
very little to separate the best of the bag-of-
words methods from the best of the segment
order-sensitive methods.
? With character-based indexing, bigrams offer
tangible gains in translation accuracy at the
same time as greatly accelerating the retrieval
process. With word-based indexing, mixed
unigrams/bigrams offer the best balance of
translation accuracy and computational cost.
? Weighted sequential correspondence is mod-
erately successful in terms of accuracy, but
grossly expensive.
Based on the above results, we judge bi-
grams to be the best segment contiguity model
for character-based indexing, and mixed uni-
grams/bigrams to be the best segment contiguity
5 As determined by the paired t test (p < 0.05).
1
10
100
V
S
M
T
IN
T
3o
p
D
3o
p
S
W
S
C
V
S
M
T
IN
T
3o
p
D
3o
p
S
V
S
M
T
IN
T
3o
p
D
3o
p
S
R
el
a
ti
v
e 
re
tr
ie
v
a
l 
ti
m
e
String comparison method
Word-based indexing
Char-based indexing1-gram
2-gram 1/2-gram
Figure 2: Basic unit retrieval times
model for word-based indexing, and for the re-
mainder of this paper, present only these two sets
of results.
While we have been able to confirm the find-
ing of Baldwin and Tanaka (2000) that character-
based indexing is superior to word-based indexing,
we are no closer to determining why this should be
the case. In the following sections we look to shed
some light on this issue by considering each of: (i)
the retrieval accuracy for other segmentation sys-
tems, (ii) the effects of lexical normalisation, and
(iii) the scalability and reproducibility of the given
results over different datasets. Finally, we present
a brief qualitative explanation for the overall re-
sults.
5.2 The effects of segmentation and
lexical normalisation
Above, we observed that segmentation consis-
tently brought about a degradation in translation
retrieval for the given dataset. Automated seg-
mentation inevitably leads to errors, which could
possibly impinge on the accuracy of word-based
indexing. Alternatively, the performance drop
could simply be caused somehow by our particular
choice of segmentation module, that is ChaSen.
First, we used JUMAN to segment the con-
struction machinery corpus, and evaluated the re-
sultant dataset in the exact same manner as for
the ChaSen output. Similarly, we ran a devel-
opment version of ALTJAWS over the same cor-
pus to produce two datasets, the first simply seg-
mented and the second both segmented and lex-
ically normalised. By lexical normalisation, we
mean that each word is converted to its canonical
form. The main segment types that normalisation
has an effect on are verbs and adjectives (conju-
gating words), and also loan-word nouns with an
optional long final vowel (e.g.monita? ?monitor??
monita) and words with multiple inter-replaceable
kanji realisations (e.g. ?? [zyu?buN] ?sufficient?
? ??).
The retrieval accuracies for JUMAN, and ALT-
JAWS with and without lexical normalisation
are presented in Fig. 3, juxtaposed against
the retrieval accuracies for character-based in-
dexing (bigrams) and also ChaSen (mixed uni-
grams/bigrams) from Section 5.1. Asterisked bars
50
52
54
56
58
60
62
VSM TINT 3opD 3opS WSC
R
et
ri
ev
a
l 
a
cc
u
ra
cy
 (
%
)
String comparison method
ChaSen
Char-based JUMAN
ALTJAWS (?norm)
ALTJAWS (+norm)
*
*
*
* *
*
*
*
* *
*
Figure 3: Results using different segmentation
modules
indicate a statistically significant gain in accuracy
over ChaSen.
Looking first to the results for JUMAN, there is
a gain in accuracy over ChaSen for all string com-
parison methods. With ALTJAWS, also, a con-
sistent gain in performance is evident with simple
segmentation, the degree of which is significantly
higher than for JUMAN. The addition of lexi-
cal normalisation enhances this effect marginally.
Notice that character-based indexing (based on
character bigrams) holds a clear advantage over
the best of the word-based indexing results for all
string comparison methods.
Based on the above, we can state that the choice
of segmentation system does have a modest im-
pact on retrieval accuracy, but that the effects of
lexical normalisation are highly localised. In the
following, we look to quantify the relationship be-
tween retrieval and segmentation accuracy.
In the next step of evaluation, we took a random
sample of 200 TRecs from the original dataset, and
ran each of ChaSen, JUMAN and ALTJAWS over
the Japanese component of each. We then man-
ually evaluated the output in terms of segment
precision and recall, defined respectively as:
Segment precision =
# correct segs in output
Total # segs in output
Segment recall =
# correct segs in output
Total # segs in model data
One slight complication in evaluating the out-
put of the three systems is that they adopt in-
congruent models of conjugation. We thus made
allowance for variation in the analysis of verb and
adjective complexes, and focused on the segmen-
tation of noun complexes.
A performance breakdown for ChaSen (CS),
JUMAN (JM) and ALTJAWS (AJ) is presented in
Tab. 1. ALTJAWS was found to outperform the
remaining two systems in terms of segment pre-
cision, while ChaSen and JUMAN performed at
the exact same level of segment precision. Look-
ing next to segment recall, ChaSen significantly
outperformed both ALTJAWS and JUMAN. The
source of almost all errors in recall, and roughly
half of errors in precision for both ChaSen and
CS JM AJ
Ave. segs/TRec 13.0 12.0 11.7
Segment precision 98.3% 98.3% 98.6%
Segment recall 98.1% 96.2% 97.7%
Sentence accuracy 70.5% 59.0% 72.0%
Total segment types 650 656 634
Table 1: Segmentation performance
JUMAN was katakana sequences such as ge?to-
rokku-barubu ?gate-lock valve?, transcribed from
English. ALTJAWS, on the other hand, was re-
markably successful at segmenting katakana word
sequences, achieving a segment precision of 100%
and segment recall approaching 99%. This is
thought to have been the main cause for the dis-
parity in retrieval accuracy for the three systems,
aggravated by the fact that most katakana se-
quences were key technical terms.
To gain an insight into consistency in the case
of error, we further calculated the total number
of segment types in the output, expecting to find
a core set of correctly-analysed segments, of rel-
atively constant size across the different systems,
plus an unpredictable component of segment er-
rors, of variable size. The system generating the
fewest segment types can thus be said to be the
most consistent.
Based on the segment type counts in Tab. 1,
ALTJAWS errs more consistently than the re-
maining two systems, and there is very little to
separate ChaSen and JUMAN. This is thought to
have had some impact on the inflated retrieval ac-
curacy for ALTJAWS.
To summarise, there would seem to be a di-
rect correlation between segmentation accuracy
and retrieval performance, with segmentation ac-
curacy on key terms (katakana sequences) having
a particularly keen effect on translation retrieval.
In this respect, ALTJAWS is superior to both
ChaSen and JUMAN for the target domain. Ad-
ditionally, complementing segmentation with lex-
ical normalisation would seem to produce meager
performance gains. Lastly, despite the slight gains
to word-based indexing with the different segmen-
tation systems, it is still significantly inferior to
character-based indexing.
5.3 Scalability of performance
All results to date have arisen from evaluation over
a single dataset of fixed size. In order to validate
the basic findings from above and observe how
increases in the data size affect retrieval perfor-
mance, we next ran the string comparison meth-
ods over differing-sized subsets of the JEIDA cor-
pus.
We simulate TMs of differing size by randomly
splitting the JEIDA corpus into ten partitions,
and running the various methods first over par-
tition 1, then over the combined partitions 1 and
2, and so on until all ten partitions are combined
together into the full corpus. We tested all string
comparison methods other than weighted sequen-
tial correspondence over the ten subsets of the
JEIDA corpus. Weighted sequential correspon-
dence was excluded from evaluation due to its
overall sub-standard retrieval performance. The
translation accuracies for the different methods
40
50
60
70
80
90
5976 11952 17937 23922 29898 35874 41859 47835 53820 61236
A
cc
u
ra
cy
 (
%
)
Dataset size (# translation records)
1/2-gram 3opS +seg
2-gram 3opS ?seg
1/2-gram 3opD +seg
2-gram 3opD ?seg
1/2-gram VSM +seg
2-gram VSM ?seg
Figure 4: Retrieval accuracies over datasets of in-
creasing size
over the ten datasets of varying size, are indicated
in Fig. 4, with each string comparison method
tested under character bigrams (?2-gram ?seg?)
and mixed word unigrams/bigrams (?1/2-gram
+seg?) as above. The results for token intersec-
tion have been omitted from the graph due to their
being almost identical to those for VSM.
A striking feature of the graph is that it is right-
decreasing, which is essentially an artifact of the
inflated length of each TRec (see Section 4.1) and
resultant data sparseness. That is, for smaller
datasets, in the bulk of cases, no TRec in the TM
is similar enough to the input to warrant consid-
eration as a translation candidate (i.e. the trans-
lation utility threshold is generally not achieved).
For larger datasets, on the other hand, we are hav-
ing to make more subtle choices as to the final
translation candidate.
One key trend in Fig. 4 is the superiority of
character- over word-based indexing for each of
the three string comparison methods, at a rela-
tively constant level as the TM size grows. Also
of interest is the finding that there is very little
to distinguish bag-of-words from segment order-
sensitive methods in terms of retrieval accuracy
in their respective best configurations.
As with the original dataset from above, 3-
operation edit similarity was the strongest per-
former just nosing out (character bigram-based)
VSM for line honours, with 3-operation edit dis-
tance lagging well behind.
Next, we turn to consider the mean unit re-
trieval times for each method, under the two in-
dexing paradigms. Times are presented in Fig. 5,
plotted once again on a logarithmic scale in order
to fit the full fan-out of retrieval times onto a single
graph. VSM and 3-operation edit distance were
the most consistent performers, both maintaining
retrieval speeds in line with those for the original
dataset at around or under 1.0 (i.e. the same re-
trieval time per input as 3-operation edit distance
run over word unigrams for the construction ma-
chinery dataset). Most importantly, only minor
increases in retrieval speed were evident as the
TM size increased, which were then reversed for
the larger datasets. All three string comparison
methods displayed this convex shape, although
the final running time for 3-operation edit simi-
larity under character- and word-based indexing
1
10
100
5976 11952 17937 23922 29898 35874 41859 47835 53820 61236
R
el
a
ti
v
e 
re
tr
ie
v
a
l 
ti
m
e
Dataset size (# translation records)
2-gram VSM ?seg
1/2-gram VSM +seg
2-gram 3opD ?seg
1/2-gram 3opD +seg
1/2-gram 3opD +seg
2-gram 3opD ?seg
Figure 5: Relative unit retrieval times over
datasets of increasing size
was, respectively, around 10 and 100 times slower
than that for VSM or 3-operation edit distance
over the same dataset.
To combine the findings for accuracy and speed,
VSM under character-based indexing suggests it-
self as the pick of the different system configura-
tions, combining both speed and consistent accu-
racy. That is, it offers the best overall retrieval
performance.
5.4 Qualitative evaluation
Above, we established that character-based index-
ing is superior to word-based indexing for distinct
datasets and a range of segmentation modules,
even when segmentation is coupled with lexical
normalisation. Additionally, we provided evidence
to the effect that bag-of-words methods offer supe-
rior translation retrieval performance to segment
order-sensitive methods. We are still no closer,
however, to determining why this should be the
case. Here, we seek to provide an explanation for
these intriguing results.
First comparing character- and word-based in-
dexing, we found that the disparity in retrieval
accuracy was largely related to the scoring of
katakana words, which are significantly longer in
character length than native Japanese words. For
the construction machinery dataset as analysed
with ChaSen, for example, the average charac-
ter length of katakana words is 3.62, as com-
pared to 2.05 overall. Under word-based index-
ing, all words are treated equally and character
length does not enter into calculations. Thus
a katakana word is treated identically to any
other word type. Under character-based index-
ing, on the other hand, the longer the word, the
more segments it generates, and a single matching
katakana sequence thus tends to contribute more
heavily to the final score than other words. Ef-
fectively, therefore, katakana sequences receive a
higher score than kanji and other sequences, pro-
ducing a preference for TRecs which incorporate
the same katakana sequences as the input. As
noted above, katakana sequences generally repre-
sent key technical terms, and such weighting thus
tends to be beneficial to retrieval accuracy.
We next examine the reason for the high corre-
lation in retrieval accuracy between bag-of-words
and segment order-sensitive methods in their op-
timum configurations (i.e. when coupled with
character bigrams). Essentially, the probabil-
ity of a given segment set permuting in differ-
ent string contexts diminishes as the number of
co-occurring segments decreases. That is, for a
given string pair, the greater the segment over-
lap between them (relative to the overall string
lengths), the lower the probability that those seg-
ments are going to occur in different orderings.
This is particularly the case when local segment
contiguity is modelled within the segment de-
scription, as occurs for the character bigram and
mixed word uni/bigram models. For high-scoring
matches, therefore, segment order sensitivity be-
comes largely superfluous, and the slight edge
in retrieval accuracy for segment order-sensitive
methods tends to come for mid-scoring matches,
in the vicinity of the translation utility threshold.
6 Conclusion
This research has been concerned with the rela-
tive import of segmentation, segment order and
segment contiguity on translation retrieval per-
formance. We simulated the effects of word or-
der sensitivity vs. bag-of-words word order insen-
sitivity by implementing a total of five compar-
ison methods: two bag-of-words approaches and
three word order-sensitive approaches. Each of
these methods was then tested under character-
based and word-based indexing and in combina-
tion with a range of N-gram models, and the rel-
ative performance of each such system configu-
ration evaluated. Character-based indexing was
found to be superior to word-based indexing, par-
ticularly when supplemented with a character bi-
gram model.
We went on to discover a strong correlation be-
tween retrieval accuracy and segmentation accu-
racy/consistency, and that lexical normalisation
produces marginal gains in retrieval performance.
We further tested the effects of incremental in-
creases in data on retrieval performance, and con-
firmed our earlier finding that character-based in-
dexing is superior to word-based indexing. At the
same time, we discovered that in their best con-
figurations, the retrieval accuracies of our bag-of-
words and segment order sensitive string compar-
ison methods are roughly equivalent, but that the
computational overhead for bag-of-words methods
to achieve that accuracy is considerably lower than
that for segment order sensitive methods.
References
T. Baldwin and H. Tanaka. 2000. The effects of
word order and segmentation on translation re-
trieval performance. In Proc. of the 18th Inter-
national Conference on Computational Linguistics
(COLING 2000), pages 35?41.
H. Fujii andW.B. Croft. 1993. A comparison of index-
ing techniques for Japanese text retrieval. In Proc.
of 16th International ACM-SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR?93), pages 237?46.
H. Isahara. 1998. JEIDA?s English?Japanese bilin-
gual corpus project. In Proc. of the 1st Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), pages 471?81.
E. Kitamura and H. Yamamoto. 1996. Translation
retrieval system using alignment data from parallel
texts. In Proc. of the 53rd Annual Meeting of the
IPSJ, volume 2, pages 385?6. (In Japanese).
S. Kurohashi and M. Nagao. 1998. Nihongo keitai-
kaiseki sisutemu JUMAN [Japanese morphological
analysis system JUMAN] version 3.5. Technical re-
port, Kyoto University. (In Japanese).
C. Manning and H. Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
Y. Matsumoto, A. Kitauchi, T. Yamashita, and Y. Hi-
rano. 1999. Japanese Morphological Analysis Sys-
tem ChaSen Version 2.0 Manual. Technical Report
NAIST-IS-TR99009, NAIST.
N. Nakamura. 1989. Translation support by retrieving
bilingual texts. In Proc. of the 38th Annual Meeting
of the IPSJ, volume 1, pages 357?8. (In Japanese).
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993.
Two approaches to matching in example-based ma-
chine translation. In Proc. of the 5th International
Conference on Theoretical and Methodological Is-
sues in Machine Translation (TMI-93), pages 47?
57.
E. Planas. 1998. A Case Study on Memory Based
Machine Translation Tools. PhD Fellow Working
Paper, United Nations University.
G. Salton. 1971. The SMART Retrieval System:
Experiments in Automatic Document Processing.
Prentice-Hall.
S. Sato and T. Kawase. 1994. A High-Speed Best
Match Retrieval Method for Japanese Text. Techni-
cal Report IS-RR-94-9I, JAIST.
S. Sato and M. Nagao. 1990. Toward memory-
based translation. In Proc. of the 13th International
Conference on Computational Linguistics (COL-
ING ?90), pages 247?52.
S. Sato. 1992. CTM: An example-based transla-
tion aid system. In Proc. of the 14th International
Conference on Computational Linguistics (COL-
ING ?92), pages 1259?63.
E. Sumita and Y. Tsutsumi. 1991. A practical method
of retrieving similar examples for translation aid.
Transactions of the IEICE, J74-D-II(10):1437?47.
(In Japanese).
H. Tanaka. 1997. An efficient way of gauging similar-
ity between long Japanese expressions. In Informa-
tion Processing Society of Japan SIG Notes, volume
97, no. 85, pages 69?74. (In Japanese).
A. Trujillo. 1999. Translation Engines: Techniques
for Machine Translation. Springer Verlag.
A. Wagner and M. Fisher. 1974. The string-to-
string correction problem. Journal of the ACM,
21(1):168?73.
Bringing the Dictionary to the User: the FOKS system
Slaven Bilac?, Timothy Baldwin? and Hozumi Tanaka?
? Tokyo Institute of Technology
2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552 JAPAN
{sbilac,tanaka}@cl.cs.titech.ac.jp
? CSLI, Ventura Hall, Stanford University
Stanford, CA 94305-4115 USA
tbaldwin@csli.stanford.edu
Abstract
The dictionary look-up of unknown words is partic-
ularly difficult in Japanese due to the complicated
writing system. We propose a system which allows
learners of Japanese to look up words according to
their expected, but not necessarily correct, reading.
This is an improvement over previous systems which
provide no handling of incorrect readings. In prepro-
cessing, we calculate the possible readings each kanji
character can take and different types of phonolog-
ical and conjugational changes that can occur, and
associate a probability with each. Using these prob-
abilities and corpus-based frequencies we calculate a
plausibility measure for each generated reading given
a dictionary entry, based on the naive Bayes model.
In response to a reading input, we calculate the plau-
sibility of each dictionary entry corresponding to the
reading and display a list of candidates for the user
to choose from. We have implemented our system
in a web-based environment and are currently eval-
uating its usefulness to learners of Japanese.
1 Introduction
Unknown words are a major bottleneck for learners
of any language, due to the high overhead involved in
looking them up in a dictionary. This is particularly
true in non-alphabetic languages such as Japanese,
as there is no easy way of looking up the component
characters of new words. This research attempts to
alleviate the dictionary look-up bottleneck by way
of a comprehensive dictionary interface which allows
Japanese learners to look up Japanese words in an ef-
ficient, robust manner. While the proposed method
is directly transferable to other language pairs, for
the purposes of this paper, we will focus exclusively
on a Japanese?English dictionary interface.
The Japanese writing system consists of the
three orthographies of hiragana, katakana and kanji,
which appear intermingled in modern-day texts
(NLI, 1986). The hiragana and katakana syllabaries,
collectively referred to as kana, are relatively small
(46 characters each), and each character takes a
unique and mutually exclusive reading which can
easily be memorized. Thus they do not present a
major difficulty for the learner. Kanji characters
(ideograms), on the other hand, present a much big-
ger obstacle. The high number of these characters
(1,945 prescribed by the government for daily use,
and up to 3,000 appearing in newspapers and formal
publications) in itself presents a challenge, but the
matter is further complicated by the fact that each
character can and often does take on several different
and frequently unrelated readings. The kanji
 
, for
example, has readings including hatsu and ta(tsu),
whereas  has readings including omote, hyou and
arawa(reru). Based on simple combinatorics, there-
fore, the kanji compound
 
 happyou ?announce-
ment? can take at least 6 basic readings, and when
one considers phonological and conjugational varia-
tion, this number becomes much greater. Learners
presented with the string
 
 for the first time will,
therefore, have a possibly large number of potential
readings (conditioned on the number of component
character readings they know) to choose from. The
problem is further complicated by the occurrence of
character combinations which do not take on com-
positional readings. For example   kaze ?com-
mon cold? is formed non-compositionally from 
kaze/fuu ?wind? and  yokoshima/ja ?evil?.
With paper dictionaries, look-up typically occurs
in two forms: (a) directly based on the reading of the
entire word, or (b) indirectly via component kanji
characters and an index of words involving those
kanji. Clearly in the first case, the correct reading
of the word must be known in order to look it up,
which is often not the case. In the second case, the
complicated radical and stroke count systems make
the kanji look-up process cumbersome and time con-
suming.
With electronic dictionaries?both commercial
and publicly available (e.g. EDICT (2000))?the
options are expanded somewhat. In addition to
reading- and kanji-based look-up, for electronic
texts, simply copying and pasting the desired string
into the dictionary look-up window gives us direct
access to the word.1. Several reading-aid systems
1Although even here, life is complicated by Japanese being
a non-segmenting language, putting the onus on the user to
(e.g. Reading Tutor (Kitamura and Kawamura,
2000) and Rikai2) provide greater assistance by seg-
menting longer texts and outputing individual trans-
lations for each segment (word). If the target text
is available only in hard copy, it is possible to use
kana-kanji conversion to manually input component
kanji, assuming that at least one reading or lexical
instantiation of those kanji is known by the user. Es-
sentially, this amounts to individually inputting the
readings of words the desired kanji appear in, and
searching through the candidates returned by the
kana-kanji conversion system. Again, this is com-
plicated and time inefficient so the need for a more
user-friendly dictionary look-up remains.
In this paper we describe the FOKS (Forgiving
Online Kanji Search) system, that allows a learner
to use his/her knowledge of kanji to the fullest extent
in looking up unknown words according to their ex-
pected, but not necessarily correct, reading. Learn-
ers are exposed to certain kanji readings before oth-
ers, and quickly develop a sense of the pervasiveness
of different readings. We attempt to tap into this
intuition, in predicting how Japanese learners will
read an arbitrary kanji string based on the relative
frequency of readings of the component kanji, and
also the relative rates of application of phonological
processes. An overall probability is attained for each
candidate reading using the naive Bayes model over
these component probabilities. Below, we describe
how this is intended to mimic the cognitive ability
of a learner, how the system interacts with a user
and how it benefits a user.
The remainder of this paper is structured as fol-
lows. Section 2 describes the preprocessing steps of
reading generation and ranking. Section 3 describes
the actual system as is currently visible on the in-
ternet. Finally, Section 4 provides an analysis and
evaluation of the system.
2 Data Preprocessing
2.1 Problem domain
Our system is intended to handle strings both in the
form they appear in texts (as a combination of the
three Japanese orthographies) and as they are read
(with the reading expressed in hiragana). Given a
reading input, the system needs to establish a rela-
tionship between the reading and one or more dictio-
nary entries, and rate the plausibility of each entry
being realized with the entered reading.
In a sense this problem is analogous to kana?kanji
conversion (see, e.g., Ichimura et al (2000) and
Takahashi et al (1996)), in that we seek to deter-
mine a ranked listing of kanji strings that could cor-
respond to the input kana string. There is one major
difference, however. Kana?kanji conversion systems
correctly identify word boundaries.
2http://www.rikai.com
are designed for native speakers of Japanese and as
such expect accurate input. In cases when the cor-
rect or standardized reading is not available, kanji
characters have to be converted one by one. This can
be a painstaking process due to the large number of
characters taking on identical readings, resulting in
large lists of characters for the user to choose from.
Our system, on the other hand, does not assume
100% accurate knowledge of readings, but instead
expects readings to be predictably derived from the
source kanji. What we do assume is that the user
is able to determine word boundaries, which is in
reality a non-trivial task due to Japanese being non-
segmenting (see Kurohashi et al (1994) and Na-
gata (1994), among others, for details of automatic
segmentation methods). In a sense, the problem of
word segmentation is distinct from the dictionary
look-up task, so we do not tackle it in this paper.
To be able to infer how kanji characters can be
read, we first determine all possible readings a kanji
character can take based on automatically-derived
alignment data. Then, we machine learn phonologi-
cal rules governing the formation of compound kanji
strings. Given this information we are able to gen-
erate a set of readings for each dictionary entry that
might be perceived as correct by a learner possessing
some, potentially partial, knowledge of the charac-
ter readings. Our generative method is analogous
to that successfully applied by Knight and Graehl
(1998) to the related problem of Japanese (back)
transliteration.
2.2 Generating and grading readings
In order to generate a set of plausible readings we
first extract all dictionary entries containing kanji,
and for each entry perform the following steps:
1. Segment the kanji string into minimal morpho-
phonemic units3 and align each resulting unit
with the corresponding reading. For this pur-
pose, we modified the TF-IDF based method
proposed by Baldwin and Tanaka (2000) to ac-
cept bootstrap data.
2. Perform conjugational, phonological and mor-
phological analysis of each segment?reading
pair and standardize the reading to canonical
form (see Baldwin et al (2002) for full de-
tails). In particular, we consider gemination
(onbin) and sequential voicing (rendaku) as the
most commonly-occurring phonological alterna-
tions in kanji compound formation (Tsujimura,
1996)4. The canonical reading for a given seg-
3A unit is not limited to one character. For example, verbs
and adjectives commonly have conjugating suffices that are
treated as part of the same segment.
4In the previous example of    happyou ?announcement?
the underlying reading of individual characters are hatsu and
hyou respectively. When the compound is formed, hatsu seg-
ment is the basic reading to which conjugational
and phonological processes apply.
3. Calculate the probability of a given segment be-
ing realized with each reading (P (r|k)), and
of phonological (P
phon
(r)) or conjugational
(P
conj
(r)) alternation occurring. The set of
reading probabilities is specific to each (kanji)
segment, whereas the phonological and conju-
gational probabilities are calculated based on
the reading only. After obtaining the compos-
ite probabilities of all readings for a segment,
we normalize them to sum to 1.
4. Create an exhaustive listing of reading candi-
dates for each dictionary entry s and calculate
the probability P (r|s) for each reading, based
on evidence from step 3 and the naive Bayes
model (assuming independence between all pa-
rameters).
P (r|s) = P (r
1..n
|k
1..n
) (1)
P (r
1..n
|k
1..n
) =
n
?
i=1
P (r
i
|k
i
)?
?P
phon
(r
i
) ? P
conj
(r
i
) (2)
5. Calculate the corpus-based frequency F (s) of
each dictionary entry s in the corpus and then
the string probability P (s), according to equa-
tion (3). Notice that the term
?
i
F (s
i
) de-
pends on the given corpus and is constant for
all strings s.
P (s) =
F (s)
?
i
F (s
i
)
(3)
6. Use Bayes rule to calculate the probability
P (s|r) of each resulting reading according to
equation (4).
P (s|r)
P (s)
=
P (r|s)
P (r)
(4)
Here, as we are only interested in the relative
score for each s given an input r, we can ig-
nore P (r) and the constant
?
i
F (s
i
). The final
plausibility grade is thus estimated as in equa-
tion (5).
Grade(s|r) = P (r|s) ? F (s) (5)
The resulting readings and their scores are stored
in the system database to be queried as necessary.
Note that the above processing is fully automated,
a valuable quality when dealing with a volatile dic-
tionary such as EDICT.
ment undergoes gemination and hyou segment undergoes se-
quential voicing resulting in happyou surface form reading.
3 System Description
The section above described the preprocessing steps
necessary for our system. In this section we describe
the actual implementation.
3.1 System overview
The base dictionary for our system is the publicly-
available EDICT Japanese?English electronic dictio-
nary.5 We extracted all entries containing at least
one kanji character and executed the steps described
above for each. Corpus frequencies were calculated
over the EDR Japanese corpus (EDR, 1995).
During the generation step we ran into problems
with extremely large numbers of generated readings,
particularly for strings containing large numbers of
kanji. Therefore, to reduce the size of generated
data, we only generated readings for entries with
less than 5 kanji segments, and discarded any read-
ings not satisfying P (r|s) ? 5 ? 10?5. Finally, to
complete the set, we inserted correct readings for
all dictionary entries s
kana
that did not contain any
kanji characters (for which no readings were gener-
ated above), with plausibility grade calculated by
equation (6).6
Grade(s
kana
|r) = F (s
kana
) (6)
This resulted in the following data:
Total entries: 97,399
Entries containing kanji: 82,961
Average number of segments: 2.30
Total readings: 2,646,137
Unique readings: 2,194,159
Average entries per reading: 1.21
Average readings per entry: 27.24
Maximum entries per reading: 112
Maximum readings per entry: 471
The above set is stored in a MySQL relational
database and queried through a CGI script. Since
the readings and scores are precalculated, there is no
time overhead in response to a user query. Figure 1
depicts the system output for the query atamajou.7
The system is easily accessible through any
Japanese language-enabled web browser. Currently
we include only a Japanese?English dictionary but
it would be a trivial task to add links to translations
in alternative languages.
3.2 Search facility
The system supports two major search modes: sim-
ple and intelligent. Simple search emulates a
conventional electronic dictionary search (see, e.g.,
5http://www.csse.monash.edu.au/~jwb/edict.html
6Here, P (r|s
kana
) is assumed to be 1, as there is only one
possible reading (i.e. r).
7This is a screen shot of the system as it is visible at
http://tanaka-www.titech.ac.jp/foks/.
Figure 1: Example of system display
Breen (2000)) over the original dictionary, taking
both kanji and kana as query strings and displaying
the resulting entries with their reading and transla-
tion. It also supports wild character and specified
character length searches. These functions enable
lookup of novel kanji combinations as long as at least
one kanji is known and can be input into the dictio-
nary interface.
Intelligent search is over the set of generated
readings. It accepts only kana query strings8 and
proceeds in two steps. Initially, the user is provided
with a list of candidates corresponding to the query,
displayed in descending order of the score calculated
from equation (5). The user must then click on the
appropriate entry to get the full translation. This
search mode is what separates our system from ex-
isting electronic dictionaries.
3.3 Example search
Let us explain the benefit of the system to the
Japanese learner through an example. Suppose the
user is interested in looking up    zujou ?over-
head? in the dictionary but does not know the cor-
rect reading. Both   ?head? and  ?over/above?
are quite common characters but frequently realized
with different readings, namely atama, tou, etc. and
ue, jou, etc., respectively. As a result, the user could
interpret the string    as being read as atamajou
or toujou and query the system accordingly. Tables
1 and 2 show the results of these two queries.9 Note
that the displayed readings are always the correct
readings for the corresponding Japanese dictionary
entry, and not the reading in the original query. For
8In order to retain the functionality offered by the simple
interface, we automatically default all queries containing kanji
characters and/or wild characters into simple search.
9Readings here are given in romanized form, whereas they
appear only in kana in the actual system interface. See Figure
1 for an example of real-life system output.
Entry Reading Grade Translation
   zujou 0.40844 overhead
   tousei 0.00271168 head voice
Table 1: Results of search for atamajou
Entry Reading Grade Translation
 
toujou 73.2344 appearance
   zujou 1.51498 overhead
 	
toujou 1.05935 embarkation

 
toujou 0.563065 cylindrical
 
doujou 0.201829 dojo

 toujou 0.126941 going to Tokyo
 
shimoyake 0.0296326 frostbite
 
toushou 0.0296326 frostbite
 
toushou 0.0144911 swordsmith
   tousei 0.0100581 head voice


toushou 0.00858729 sword wound
 
tousou 0.00341006 smallpox
 
tousou 0.0012154 frostbite
 
toushin 0.000638839 Eastern China
Table 2: Results of search for toujou
all those entries where the actual reading coincides
with the user input, the reading is displayed in bold-
face.
From Table 1 we see that only two results are
returned for atamajou, and that the highest rank-
ing candidate corresponds to the desired string  
 . Note that atamajou is not a valid word in
Japanese, and that a conventional dictionary search
would yield no results.
Things get somewhat more complicated for the
reading toujou, as can be seen from Table 2. A total
of 14 entries is returned, for four of which toujou is
the correct reading (as indicated in bold). The string
   is second in rank, scored higher than three en-
tries for which toujou is the correct reading, due to
the scoring procedure not considering whether the
generated readings are correct or not.
For both of these inputs, a conventional system
would not provide access to the desired translation
without additional user effort, while the proposed
system returns the desired entry as a first-pass can-
didate in both cases.
4 Analysis and Evaluation
To evaluate the proposed system, we first provide
a short analysis of the reading set distribution and
then describe results of a preliminary experiment on
real-life data.
4.1 Reading set analysis
Since we create a large number of plausible read-
ings, one potential problem is that a large number
01
2
3
4
5
6
7
8
9
10
11
12
0 10 20 30 40 50 60 70 80 90 100 110 120
N
um
be
r o
f R
ea
di
ng
s(l
og
)
Results per Reading
Number of results returned per reading
All
Existing
Baseline
Figure 2: Distribution of results returned per read-
ing
0
3
6
9
12
15
18
21
24
27
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Av
er
ag
e 
nu
m
be
r o
f r
es
ul
ts
 re
tu
rn
ed
Length of reading (in characters)
Average number of results returned depending on length of reading
All
Existing
Baseline
Figure 3: Distribution of results for different query
string lengths
of candidates would be returned for each reading, ob-
scuring dictionary entries for which the input is the
correct reading. This could result in a high penalty
for competent users who mostly search the dictio-
nary with correct readings, potentially making the
system unusable.
To verify this, we tried to establish how many
candidates are returned for user queries over read-
ings the system has knowledge of, and also tested
whether the number of results depends on the length
of the query.
The distribution of results returned for different
queries is given in Figure 2, and the average num-
ber of results returned for different-length queries
is given in Figure 3. In both figures, Baseline is
calculated over only those readings in the original
dictionary (i.e. correct readings); Existing is the
subset of readings in the generated set that existed
in the original dictionary; and All is all readings in
the generated set. The distribution of the latter two
sets is calculated over the generated set of readings.
In Figure 2 the x-axis represents the number of
results returned for the given reading and the y-axis
represents the natural log of the number of readings
returning that number of results. It can be seen that
only a few readings return a high number of entries.
308 out of 2,194,159 or 0.014% readings return over
30 results. As it happens, most of the readings re-
turning a high number of results are readings that
existed in the original dictionary, as can be seen from
the fact that Existing and All are almost identical
for x values over 30. Note that the average number
of dictionary entries returned per reading is 1.21 for
the complete set of generated readings.
Moreover, as seen from Figure 3 the number of
results depends heavily on the length of the reading.
In this figure, the x-axis gives the length of the read-
ing in characters and the y-axis the average number
of entries returned. It can be seen that queries con-
taining 4 characters or more are likely to return 3
results or less on average. Here again, the Exist-
ing readings have the highest average of 2.88 results
returned for 4 character queries. The 308 readings
mentioned above were on average 2.80 characters in
length.
From these results, it would appear that the re-
turned number of entries is ordinarily not over-
whelming, and provided that the desired entries are
included in the list of candidates, the system should
prove itself useful to a learner. Furthermore, if a
user is able to postulate several readings for a target
string, s/he is more likely to obtain the translation
with less effort by querying with the longer of the
two postulates.
4.2 Comparison with a conventional system
As the second part of evaluation, we tested to see
whether the set of candidates returned for a query
over the wrong reading, includes the desired entry.
We ran the following experiment. As a data set we
used a collection of 139 entries taken from a web
site displaying real-world reading errors made by
native speakers of Japanese.10 For each entry, we
queried our system with the erroneous reading to
see whether the intended entry was returned among
the system output. To transform this collection of
items into a form suitable for dictionary querying, we
converted all readings into hiragana, sometimes re-
moving context words in the process. Table 3 gives
a comparison of results returned in simple (con-
ventional) and intelligent (proposed system) search
modes. 62 entries, mostly proper names11 and 4-
10http://www.sutv.zaq.ne.jp/shirokuma/godoku.html
11We have also implemented the proposed system with the
ENAMDICT, a name dictionary in the EDICT distribution,
Conventional Our System
In dictionary 77 77
Ave. # Results 1.53 5.42
Successful 10 34
Mean Rank 1.4 4.71
Table 3: Comparison between a conventional dictio-
nary look-up and our system
character proverbs, were not contained in the dictio-
nary and have been excluded from evaluation. The
erroneous readings of the 77 entries that were con-
tained in the dictionary averaged 4.39 characters in
length.
From Table 3 we can see that our system is able
to handle more than 3 times more erroneous read-
ings then the conventional system, representing an
error rate reduction of 35.8%. However, the average
number of results returned (5.42) and mean rank of
the desired entry (4.71 ? calculated only for suc-
cessful queries) are still sufficiently small to make
the system practically useful.
That the conventional system covers any erro-
neous readings at all is due to the fact that those
readings are appropriate in alternative contexts, and
as such both readings appear in the dictionary.
Whereas our system is generally able to return all
reading-variants for a given kanji string and there-
fore provide the full set of translations for the kanji
string, conventional systems return only the transla-
tion for the given reading. That is, with our system,
the learner will be able to determine which of the
readings is appropriate for the given context based
on the translation, whereas with conventional sys-
tems, they will be forced into attempting to contex-
tualize a (potentially) wrong translation.
Out of 42 entries that our system did not handle,
the majority of misreadings were due to the usage of
incorrect character readings in compounds (17) and
graphical similarity-induced error (16). Another 5
errors were a result of substituting the reading of
a semantically-similar word, and the remaining 5 a
result of interpreting words as personal names.
Finally, for the same data set we compared the
relative rank of the correct and erroneous readings
to see which was scored higher by our grading pro-
cedure. Given that the data set is intended to ex-
emplify cases where the expected reading is different
from the actual reading, we would expect the erro-
neous readings to rank higher than the actual read-
ings. An average of 76.7 readings was created for
allowing for name searches on the same basic methodology.
We feel that this part of the system should prove itself useful
even to the native speakers of Japanese who often experience
problems reading uncommon personal or place names. How-
ever, as of yet, we have not evaluated this part of the system
and will not discuss it in detail.
the 34 entries. The average relative rank was 12.8
for erroneous readings and 19.6 for correct readings.
Thus, on average, erroneous readings were ranked
higher than the correct readings, in line with our
prediction above.
Admittedly, this evaluation was over a data set
of limited size, largely because of the difficulty in
gaining access to naturally-occurring kanji?reading
confusion data. The results are, however, promising.
4.3 Discussion
In order to emulate the limited cognitive abilities of
a language learner, we have opted for a simplistic
view of how individual kanji characters combine in
compounds. In step 4 of preprocessing, we use the
naive Bayes model to generate an overall probability
for each reading, and in doing so assume that com-
ponent readings are independent of each other, and
that phonological and conjugational alternation in
readings does not depend on lexical context. Clearly
this is not the case. For example, kanji readings de-
riving from Chinese and native Japanese sources (on
and kun readings, respectively) tend not to co-occur
in compounds. Furthermore, phonological and con-
jugational alternations interact in subtle ways and
are subject to a number of constraints (Vance, 1987).
However, depending on the proficiency level of
the learner, s/he may not be aware of these rules,
and thus may try to derive compound readings in
a more straightforward fashion, which is adequately
modeled through our simplistic independence-based
model. As can be seen from preliminary experi-
mentation, our model is effective in handling a large
number of reading errors but can be improved fur-
ther. We intend to modify it to incorporate further
constraints in the generation process after observ-
ing the correlation between the search inputs and
selected dictionary entries.
Furthermore, the current cognitive model does not
include any notion of possible errors due to graphic
or semantic similarity. But as seen from our pre-
liminary experiment these error types are also com-
mon. For example,    bochi ?graveyard? and 
 kichi ?base? are graphically very similar but read
differently, and  mono ?thing? and  koto ?thing?
are semantically similar but take different readings.
This leads to the potential for cross-borrowing of er-
rant readings between these kanji pairs.
Finally, we are working under the assumption that
the target string is contained in the original dictio-
nary and thus base all reading generation on the
existing entries, assuming that the user will only at-
tempt to look up words we have knowledge of. We
also provide no immediate solution for random read-
ing errors or for cases where user has no intuition as
to how to read the characters in the target string.
4.4 Future work
So far we have conducted only limited tests of cor-
relation between the results returned and the target
words. In order to truly evaluate the effectiveness of
our system we need to perform experiments with a
larger data set, ideally from actual user inputs (cou-
pled with the desired dictionary entry). The reading
generation and scoring procedure can be adjusted by
adding and modifying various weight parameters to
modify calculated probabilities and thus affect the
results displayed.
Also, to get a full coverage of predictable errors,
we would like to expand our model further to in-
corporate consideration of errors due to graphic or
semantic similarity of kanji.
5 Conclusion
In this paper we have proposed a system design
which accommodates user reading errors and supple-
ments partial knowledge of the readings of Japanese
words. Our method takes dictionary entries con-
taining kanji characters and generates a number of
readings for each. Readings are scored depending on
their likeliness and stored in a system database ac-
cessed through a web interface. In response to a user
query, the system displays dictionary entries likely
to correspond to the reading entered. Initial evalua-
tion indicates that the proposed system significantly
increases error-resilience in dictionary searches.
Acknowledgements
This research was supported in part by the Re-
search Collaboration between NTT Communication
Science Laboratories, Nippon Telegraph and Tele-
phone Corporation and CSLI, Stanford University.
We would particularly like to thank Ryo Okumura
for help in the development of the FOKS system,
Prof. Nishina Kikuko of the International Student
Center (TITech) for initially hosting the FOKS sys-
tem, and Francis Bond, Christoph Neumann and
two anonymous referees for providing valuable feed-
back during the writing of this paper.
References
Timothy Baldwin and Hozumi Tanaka. 2000. A
comparative study of unsupervised grapheme-
phoneme alignment methods. In Proc. of the 22nd
Annual Meeting of the Cognitive Science Society
(CogSci 2000), pages 597?602, Philadelphia.
Timothy Baldwin, Slaven Bilac, Ryo Okumura,
Takenobu Tokunaga, and Hozumi Tanaka. 2002.
Enhanced Japanese electronic dictionary look-up.
In Proc. of LREC. (to appear).
Jim Breen. 2000. A WWW Japanese Dictionary.
Japanese Studies, 20:313?317.
EDICT. 2000. EDICT Japanese-English Dictio-
nary File. ftp://ftp.cc.monash.edu.au/pub/
nihongo/.
EDR. 1995. EDR Electronic Dictionary Technical
Guide. Japan Electronic Dictionary Research In-
stitute, Ltd. In Japanese.
Yumi Ichimura, Yoshimi Saito, Kazuhiro Kimura,
and Hideki Hirakawa. 2000. Kana-kanji conver-
sion system with input support based on pre-
diction. In Proc. of the 18th International Con-
ference on Computational Linguistics (COLING
2000), pages 341?347.
Tatuya Kitamura and Yoshiko Kawamura. 2000.
Improving the dictionary display in a reading
support system. International Symposium of
Japanese Language Education. (In Japanese).
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24:599?612.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Improvements
of Japanese morphological analyzer JUMAN. In
SNLR, pages 22?28.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-DP backward-
A? N-best search algorithm. In Proc. of the 15th
International Conference on Computational Lin-
guistics )(COLING 1994, pages 201?207.
NLI. 1986. Character and Writing system Edu-
cation, volume 14 of Japanese Language Educa-
tion Reference. National Language Institute. (in
Japanese).
Masahito Takahashi, Tsuyoshi Shichu, Kenji
Yoshimura, and Kosho Shudo. 1996. Process-
ing homonyms in the kana-to-kanji conversion.
In Proc. of the 16th International Conference
on Computational Linguistics (COLING 1996),
pages 1135?1138.
Natsuko Tsujimura. 1996. An Introduction to
Japanese Linguistics. Blackwell, Cambridge,
Massachusetts, first edition.
Timothy J. Vance. 1987. Introduction to Japanese
Phonology. SUNY Press, New York.
Learning the Countability of English Nouns from Corpus Data
Timothy Baldwin
CSLI
Stanford University
Stanford, CA, 94305
tbaldwin@csli.stanford.edu
Francis Bond
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
Kyoto, Japan
bond@cslab.kecl.ntt.co.jp
Abstract
This paper describes a method for learn-
ing the countability preferences of English
nouns from raw text corpora. The method
maps the corpus-attested lexico-syntactic
properties of each noun onto a feature
vector, and uses a suite of memory-based
classifiers to predict membership in 4
countability classes. We were able to as-
sign countability to English nouns with a
precision of 94.6%.
1 Introduction
This paper is concerned with the task of knowledge-
rich lexical acquisition from unannotated corpora,
focusing on the case of countability in English.
Knowledge-rich lexical acquisition takes unstruc-
tured text and extracts out linguistically-precise cat-
egorisations of word and expression types. By
combining this with a grammar, we can build
broad-coverage deep-processing tools with a min-
imum of human effort. This research is close
in spirit to the work of Light (1996) on classi-
fying the semantics of derivational affixes, and
Siegel and McKeown (2000) on learning verb as-
pect.
In English, nouns heading noun phrases are typ-
ically either countable or uncountable (also called
count and mass). Countable nouns can be modi-
fied by denumerators, prototypically numbers, and
have a morphologically marked plural form: one
dog, two dogs. Uncountable nouns cannot be modi-
fied by denumerators, but can be modified by unspe-
cific quantifiers such as much, and do not show any
number distinction (prototypically being singular):
*one equipment, some equipment, *two equipments.
Many nouns can be used in countable or uncountable
environments, with differences in interpretation.
We call the lexical property that determines which
uses a noun can have the noun?s countability prefer-
ence. Knowledge of countability preferences is im-
portant both for the analysis and generation of En-
glish. In analysis, it helps to constrain the inter-
pretations of parses. In generation, the countabil-
ity preference determines whether a noun can be-
come plural, and the range of possible determin-
ers. Knowledge of countability is particularly im-
portant in machine translation, because the closest
translation equivalent may have different countabil-
ity from the source noun. Many languages, such
as Chinese and Japanese, do not mark countability,
which means that the choice of countability will be
largely the responsibility of the generation compo-
nent (Bond, 2001). In addition, knowledge of count-
ability obtained from examples of use is an impor-
tant resource for dictionary construction.
In this paper, we learn the countability prefer-
ences of English nouns from unannotated corpora.
We first annotate them automatically, and then train
classifiers using a set of gold standard data, taken
from COMLEX (Grishman et al, 1998) and the trans-
fer dictionaries used by the machine translation sys-
tem ALT-J/E (Ikehara et al, 1991). The classifiers
and their training are described in more detail in
Baldwin and Bond (2003). These are then run over
the corpus to extract nouns as members of four
classes ? countable: dog; uncountable: furniture; bi-
partite: [pair of] scissors and plural only: clothes.
We first discuss countability in more detail (? 2).
Then we present the lexical resources used in our ex-
periment (? 3). Next, we describe the learning pro-
cess (? 4). We then present our results and evalu-
ation (? 5). Finally, we discuss the theoretical and
practical implications (? 6).
2 Background
Grammatical countability is motivated by the se-
mantic distinction between object and substance
reference (also known as bounded/non-bounded or
individuated/non-individuated). It is a subject of
contention among linguists as to how far grammat-
ical countability is semantically motivated and how
much it is arbitrary (Wierzbicka, 1988).
The prevailing position in the natural language
processing community is effectively to treat count-
ability as though it were arbitrary and encode it as
a lexical property of nouns. The study of countabil-
ity is complicated by the fact that most nouns can
have their countability changed: either converted by
a lexical rule or embedded in another noun phrase.
An example of conversion is the so-called universal
packager, a rule which takes an uncountable noun
with an interpretation as a substance, and returns a
countable noun interpreted as a portion of the sub-
stance: I would like two beers. An example of em-
bedding is the use of a classifier, e.g. uncountable
nouns can be embedded in countable noun phrases
as complements of classifiers: one piece of equip-
ment.
Bond et al (1994) suggested a division of count-
ability into five major types, based on Allan (1980)?s
noun countability preferences (NCPs). Nouns which
rarely undergo conversion are marked as either fully
countable, uncountable or plural only. Fully countable
nouns have both singular and plural forms, and can-
not be used with determiners such as much, little, a
little, less and overmuch. Uncountable nouns, such
as furniture, have no plural form, and can be used
with much. Plural only nouns never head a singular
noun phrase: goods, scissors.
Nouns that are readily converted are marked as ei-
ther strongly countable (for countable nouns that can
be converted to uncountable, such as cake) or weakly
countable (for uncountable nouns that are readily
convertible to countable, such as beer).
NLP systems must list countability for at least
some nouns, because full knowledge of the refer-
ent of a noun phrase is not enough to predict count-
ability. There is also a language-specific knowl-
edge requirement. This can be shown most sim-
ply by comparing languages: different languages en-
code the countability of the same referent in dif-
ferent ways. There is nothing about the concept
denoted by lightning, e.g., that rules out *a light-
ning being interpreted as a flash of lightning. In-
deed, the German and French translation equivalents
are fully countable (ein Blitz and un e?clair respec-
tively). Even within the same language, the same
referent can be encoded countably or uncountably:
clothes/clothing, things/stuff , jobs/work.
Therefore, we must learn countability classes
from usage examples in corpora. There are several
impediments to this approach. The first is that words
are frequently converted to different countabilities,
sometimes in such a way that other native speak-
ers will dispute the validity of the new usage. We
do not necessarily wish to learn such rare examples,
and may not need to learn more common conver-
sions either, as they can be handled by regular lexi-
cal rules (Copestake and Briscoe, 1995). The second
problem is that some constructions affect the appar-
ent countability of their head: for example, nouns
denoting a role, which are typically countable, can
appear without an article in some constructions (e.g.
We elected him treasurer). The third is that different
senses of a word may have different countabilities:
interest ?a sense of concern with and curiosity? is
normally countable, whereas interest ?fixed charge
for borrowing money? is uncountable.
There have been at several earlier approaches
to the automatic determination of countabil-
ity. Bond and Vatikiotis-Bateson (2002) determine
a noun?s countability preferences from its seman-
tic class, and show that semantics predicts (5-way)
countability 78% of the time with their ontology.
O?Hara et al (2003) get better results (89.5%) using
the much larger Cyc ontology, although they only
distinguish between countable and uncountable.
Schwartz (2002) created an automatic countabil-
ity tagger (ACT) to learn noun countabilities from
the British National Corpus. ACT looks at deter-
miner co-occurrence in singular noun chunks, and
classifies the noun if and only if it occurs with a de-
terminer which can modify only countable or un-
countable nouns. The method has a coverage of
around 50%, and agrees with COMLEX for 68% of
the nouns marked countable and with the ALT-J/E
lexicon for 88%. Agreement was worse for uncount-
able nouns (6% and 44% respectively).
3 Resources
Information about noun countability was obtained
from two sources. One was COMLEX 3.0 (Grish-
man et al, 1998), which has around 22,000 noun
entries. Of these, 12,922 are marked as being count-
able (COUNTABLE) and 4,976 as being uncountable
(NCOLLECTIVE or :PLURAL *NONE*). The remainder
are unmarked for countability.
The other was the common noun part of ALT-
J/E?s Japanese-to-English semantic transfer dictio-
nary (Bond, 2001). It contains 71,833 linked
Japanese-English pairs, each of which has a value
for the noun countability preference of the English
noun. Considering only unique English entries with
different countability and ignoring all other informa-
tion gave 56,245 entries. Nouns in the ALT-J/E dic-
tionary are marked with one of the five major count-
ability preference classes described in Section 2. In
addition to countability, default values for number
and classifier (e.g. blade for grass: blade of grass)
are also part of the lexicon.
We classify words into four possible classes, with
some words belonging to multiple classes. The first
class is countable: COMLEX?s COUNTABLE and ALT-
J/E?s fully, strongly and weakly countable. The sec-
ond class is uncountable: COMLEX?s NCOLLECTIVE or
:PLURAL *NONE* and ALT-J/E?s strongly and weakly
countable and uncountable.
The third class is bipartite nouns. These can only
be plural when they head a noun phrase (trousers),
but singular when used as a modifier (trouser leg).
When they are denumerated they use pair: a pair of
scissors. COMLEX does not have a feature to mark
bipartite nouns; trouser, for example, is listed as
countable. Nouns in ALT-J/E marked plural only with
a default classifier of pair are classified as bipartite.
The last class is plural only nouns: those that only
have a plural form, such as goods. They can nei-
ther be denumerated nor modified by much. Many
of these nouns, such as clothes, use the plural form
even as modifiers (a clothes horse). The word
clothes cannot be denumerated at all. Nouns marked
:SINGULAR *NONE* in COMLEX and nouns in ALT-
J/E marked plural only without the default classifier
pair are classified as plural only. There was some
noise in the ALT-J/E data, so this class was hand-
checked, giving a total of 104 entries; 84 of these
were attested in the training data.
Our classification of countability is a subset of
ALT-J/E?s, in that we use only the three basic ALT-
J/E classes of countable, uncountable and plural only,
(although we treat bipartite as a separate class, not a
subclass). As we derive our countability classifica-
tions from corpus evidence, it is possible to recon-
struct countability preferences (i.e. fully, strongly, or
weakly countable) from the relative token occurrence
of the different countabilities for that noun.
In order to get an idea of the intrinsic difficulty of
the countability learning task, we tested the agree-
ment between the two resources in the form of clas-
sification accuracy. That is, we calculate the average
proportion of (both positive and negative) countabil-
ity classifications over which the two methods agree.
E.g., COMLEX lists tomato as being only countable
where ALT-J/E lists it as being both countable and un-
countable. Agreement for this one noun, therefore, is
3
4 , as there is agreement for the classes of countable,
plural only and bipartite (with implicit agreement as
to negative membership for the latter two classes),
but not for uncountable. Averaging over the total set
of nouns countability-classified in both lexicons, the
mean was 93.8%. Almost half of the disagreements
came from words with two countabilities in ALT-J/E
but only one in COMLEX.
4 Learning Countability
The basic methodology employed in this research is
to identify lexical and/or constructional features as-
sociated with the countability classes, and determine
the relative corpus occurrence of those features for
each noun. We then feed the noun feature vectors
into a classifier and make a judgement on the mem-
bership of the given noun in each countability class.
In order to extract the feature values from corpus
data, we need the basic phrase structure, and partic-
ularly noun phrase structure, of the source text. We
use three different sources for this phrase structure:
part-of-speech tagged data, chunked data and fully-
parsed data, as detailed below.
The corpus of choice throughout this paper is the
written component of the British National Corpus
(BNC version 2, Burnard (2000)), totalling around
90m w-units (POS-tagged items). We chose this be-
cause of its good coverage of different usages of En-
glish, and thus of different countabilities. The only
component of the original annotation we make use
of is the sentence tokenisation.
Below, we outline the features used in this re-
search and methods of describing feature interac-
tion, along with the pre-processing tools and ex-
traction techniques, and the classifier architecture.
The full range of different classifier architectures
tested as part of this research, and the experi-
ments to choose between them are described in
Baldwin and Bond (2003).
4.1 Feature space
For each target noun, we compute a fixed-length
feature vector based on a variety of features intended
to capture linguistic constraints and/or preferences
associated with particular countability classes. The
feature space is partitioned up into feature clusters,
each of which is conditioned on the occurrence of
the target noun in a given construction.
Feature clusters take the form of one- or two-
dimensional feature matrices, with each dimension
describing a lexical or syntactic property of the
construction in question. In the case of a one-
dimensional feature cluster (e.g. noun occurring in
singular or plural form), each component feature
feats in the cluster is translated into the 3-tuple:
Feature cluster
(base feature no.) Countable Uncountable Bipartite Plural only
Head number (2) S,P S P P
Modifier number (2) S,P S S P
Subj?V agreement (2 ? 2) [S,S],[P,P] [S,S] [P,P] [P,P]
Coordinate number (2? 2) [S,S],[P,S],[P,P] [S,S],[S,P] [P,S],[P,P] [P,S],[P,P]
N of N (11 ? 2) [100s,P], . . . [lack,S], . . . [pair,P], . . . [rate,P], . . .
PPs (52 ? 2) [per,-DET], . . . [in,-DET], . . . ? ?
Pronoun (12 ? 2) [it,S],[they,P], . . . [it,S], . . . [they,P], . . . [they,P], . . .
Singular determiners (10) a,each, . . . much, . . . ? ?
Plural determiners (12) many, few, . . . ? ? many, . . .
Neutral determiners (11? 2) [less,P], . . . [BARE,S], . . . [enough,P], . . . [all,P], . . .
Table 1: Predicted feature-correlations for each feature cluster (S=singular, P=plural)
?freq(feats|word),
freq(feats|word)
freq(word) ,
freq(feats|word)
?
ifreq(feati|word)
?
In the case of a two-dimensional feature cluster
(e.g. subject-position noun number vs. verb number
agreement), each component feature feat s,t is trans-
lated into the 5-tuple:
?freq(feats,t|word),
freq(feats,t|word)
freq(word) ,
freq(feats,t|word)
?
i,j freq(feati,j |word)
,
freq(feats,t|word)
?
ifreq(feati,t|word)
,
freq(feats,t|word)
?
j freq(feats,j |word)
?
See Baldwin and Bond (2003) for further details.
The following is a brief description of each fea-
ture cluster and its dimensionality (1D or 2D). A
summary of the number of base features and predic-
tion of positive feature correlations with countability
classes is presented in Table 1.
Head noun number:1D the number of the target
noun when it heads an NP (e.g. a shaggy dog
= SINGULAR)
Modifier noun number:1D the number of the tar-
get noun when a modifier in an NP (e.g. dog
food = SINGULAR)
Subject?verb agreement:2D the number of the tar-
get noun in subject position vs. number agree-
ment on the governing verb (e.g. the dog barks
= ?SINGULAR,SINGULAR?)
Coordinate noun number:2D the number of the
target noun vs. the number of the head
nouns of conjuncts (e.g. dogs and mud =
?PLURAL,SINGULAR?)
N of N constructions:2D the number of the target
noun (N?) vs. the type of the N? in an N?
of N? construction (e.g. the type of dog =
?TYPE,SINGULAR?). We have identified a total
of 11 N? types for use in this feature cluster
(e.g. COLLECTIVE, LACK, TEMPORAL).
Occurrence in PPs:2D the presence or absence of
a determiner (?DET) when the target noun oc-
curs in singular form in a PP (e.g. per dog
= ?per,?DET?). This feature cluster exploits
the fact that countable nouns occur determin-
erless in singular form with only very partic-
ular prepositions (e.g. by bus, *on bus, *with
bus) whereas with uncountable nouns, there are
fewer restrictions on what prepositions a target
noun can occur with (e.g. on furniture, with fur-
niture, ?by furniture).
Pronoun co-occurrence:2D what personal and
possessive pronouns occur in the same sen-
tence as singular and plural instances of the
target noun (e.g. The dog ate its dinner =
?its,SINGULAR?). This is a proxy for pronoun
binding effects, and is determined over a total
of 12 third-person pronoun forms (normalised
for case, e.g. he, their, itself ).
Singular determiners:1D what singular-selecting
determiners occur in NPs headed by the tar-
get noun in singular form (e.g. a dog = a).
All singular-selecting determiners considered
are compatible with only countable (e.g. an-
other, each) or uncountable nouns (e.g. much,
little). Determiners compatible with either are
excluded from the feature cluster (cf. this dog,
this information). Note that the term ?deter-
miner? is used loosely here and below to denote
an amalgam of simplex determiners (e.g. a), the
null determiner, complex determiners (e.g. all
the), numeric expressions (e.g. one), and adjec-
tives (e.g. numerous), as relevant to the partic-
ular feature cluster.
Plural determiners:1D what plural-selecting deter-
miners occur in NPs headed by the target noun
in plural form (e.g. few dogs = few). As
with singular determiners, we focus on those
plural-selecting determiners which are compat-
ible with a proper subset of count, plural only
and bipartite nouns.
Non-bounded determiners:2D what non-bounded
determiners occur in NPs headed by the target
noun, and what is the number of the target noun
for each (e.g. more dogs = ?more,PLURAL?).
Here again, we restrict our focus to non-
bounded determiners that select for singular-
form uncountable nouns (e.g. sufficient furni-
ture) and plural-form countable, plural only
and bipartite nouns (e.g. sufficient dogs).
The above feature clusters produce a combined
total of 1,284 individual feature values.
4.2 Feature extraction
In order to extract the features described above,
we need some mechanism for detecting NP and
PP boundaries, determining subject?verb agreement
and deconstructing NPs in order to recover con-
juncts and noun-modifier data. We adopt three ap-
proaches. First, we use part-of-speech (POS) tagged
data and POS-based templates to extract out the nec-
essary information. Second, we use chunk data
to determine NP and PP boundaries, and medium-
recall chunk adjacency templates to recover inter-
phrasal dependency. Third, we fully parse the data
and simply read off all necessary data from the de-
pendency output.
With the POS extraction method, we first Penn-
tagged the BNC using an fnTBL-based tagger (Ngai
and Florian, 2001), training over the Brown and
WSJ corpora with some spelling, number and hy-
phenation normalisation. We then lemmatised this
data using a version of morph (Minnen et al, 2001)
customised to the Penn POS tagset. Finally, we
implemented a range of high-precision, low-recall
POS-based templates to extract out the features from
the processed data. For example, NPs are in many
cases recoverable with the following Perl-style reg-
ular expression over Penn POS tags: (PDT)* DT
(RB|JJ[RS]?|NNS?)* NNS? [?N].
For the chunker, we ran fnTBL over the lem-
matised tagged data, training over CoNLL 2000-
style (Tjong Kim Sang and Buchholz, 2000) chunk-
converted versions of the full Brown and WSJ cor-
pora. For the NP-internal features (e.g. determin-
ers, head number), we used the noun chunks directly,
or applied POS-based templates locally within noun
chunks. For inter-chunk features (e.g. subject?verb
agreement), we looked at only adjacent chunk pairs
so as to maintain a high level of precision.
As the full parser, we used RASP (Briscoe and
Carroll, 2002), a robust tag sequence grammar-
based parser. RASP?s grammatical relation output
function provides the phrase structure in the form
of lemmatised dependency tuples, from which it is
possible to read off the feature information. RASP
has the advantage that recall is high, although pre-
cision is potentially lower than chunking or tagging
as the parser is forced into resolving phrase attach-
ment ambiguities and committing to a single phrase
structure analysis.
Although all three systems map onto an identi-
cal feature space, the feature vectors generated for a
given target noun diverge in content due to the dif-
ferent feature extraction methodologies. In addition,
we only consider nouns that occur at least 10 times
as head of an NP, causing slight disparities in the
target noun type space for the three systems. There
were sufficient instances found by all three systems
for 20,530 common nouns (out of 33,050 for which
at least one system found sufficient instances).
4.3 Classifier architecture
The classifier design employed in this research is
four parallel supervised classifiers, one for each
countability class. This allows us to classify a sin-
gle noun into multiple countability classes, e.g. de-
mand is both countable and uncountable. Thus,
rather than classifying a given target noun accord-
ing to the unique most plausible countability class,
we attempt to capture its full range of countabilities.
Note that the proposed classifier design is that which
was found by Baldwin and Bond (2003) to be opti-
mal for the task, out of a wide range of classifier
architectures.
In order to discourage the classifiers from over-
training on negative evidence, we constructed the
gold-standard training data from unambiguously
negative exemplars and potentially ambiguous pos-
itive exemplars. That is, we would like classifiers
to judge a target noun as not belonging to a given
countability class only in the absence of positive ev-
idence for that class. This was achieved in the case
of countable nouns, for instance, by extracting all
countable nouns from each of the ALT-J/E and COM-
LEX lexicons. As positive training exemplars, we
then took the intersection of those nouns listed as
countable in both lexicons (irrespective of member-
ship in alternate countability classes); negative train-
ing exemplars, on the other hand, were those con-
tained in both lexicons but not classified as count-
Class Positive data Negative data Baseline
Countable 4,342 1,476 .746
Uncountable 1,519 5,471 .783
Bipartite 35 5,639 .994
Plural only 84 5,639 .985
Table 2: Details of the gold-standard data
able in either.1 The uncountable gold-standard data
was constructed in a similar fashion. We used the
ALT-J/E lexicon as our source of plural only and bi-
partite nouns, using all the instances listed as our
positive exemplars. The set of negative exemplars
was constructed in each case by taking the intersec-
tion of nouns not contained in the given countability
class in ALT-J/E, with all annotated nouns with non-
identical singular and plural forms in COMLEX.
Having extracted the positive and negative exem-
plar noun lists for each countability class, we filtered
out all noun lemmata not occurring in the BNC.
The final make-up of the gold-standard data for
each of the countability classes is listed in Table 2,
along with a baseline classification accuracy for
each class (?Baseline?), based on the relative fre-
quency of the majority class (positive or negative).
That is, for bipartite nouns, we achieve a 99.4% clas-
sification accuracy by arbitrarily classifying every
training instance as negative.
The supervised classifiers were built using
TiMBL version 4.2 (Daelemans et al, 2002), a
memory-based classification system based on the k-
nearest neighbour algorithm. As a result of exten-
sive parameter optimisation, we settled on the de-
fault configuration for TiMBL with k set to 9. 2
5 Results and Evaluation
Evaluation is broken down into two components.
First, we determine the optimal classifier configura-
tion for each countability class by way of stratified
cross-validation over the gold-standard data. We
then run each classifier in optimised configuration
over the remaining target nouns for which we have
feature vectors.
5.1 Cross-validated results
First, we ran the classifiers over the full feature set
for the three feature extraction methods. In each
case, we quantify the classifier performance by way
1Any nouns not annotated for countability in COMLEX were
ignored in this process so as to assure genuinely negative
exemplars.
2We additionally experimented with the kernel-based
TinySVM system, but found TiMBL to be superior in all cases.
Class System Accuracy (e.r.) F-score
Tagger? .928 (.715) .953
Chunker .933 (.734) .956Countable
RASP? .923 (.698) .950
Combined .939 (.759) .960
Tagger .945 (.746) .876
Chunker? .945 (.747) .876Uncountable
RASP? .944 (.743) .872
Combined .952 (.779) .892
Tagger .997 (.489) .752
Chunker .997 (.460) .704Bipartite
RASP .997 (.488) .700
Combined .996 (.403) .722
Tagger .989 (.275) .558
Chunker .990 (.299) .568Plural only
RASP? .989 (.227) .415
Combined .990 (.323) .582
Table 3: Cross-validation results
of 10-fold stratified cross-validation over the gold-
standard data for each countability class. The fi-
nal classification accuracy and F-score3 are averaged
over the 10 iterations.
The cross-validated results for each classifier are
presented in Table 3, broken down into the differ-
ent feature extraction methods. For each, in addi-
tion to the F-score and classification accuracy, we
present the relative error reduction (e.r.) in classifi-
cation accuracy over the majority-class baseline for
that gold-standard set (see Table 2). For each count-
ability class, we additionally ran the classifier over
the concatenated feature vectors for the three basic
feature extraction methods, producing a 3,852-value
feature space (?Combined?).
Given the high baseline classification accuracies
for each gold-standard dataset, the most revealing
statistics in Table 3 are the error reduction and F-
score values. In all cases other than bipartite, the
combined system outperformed the individual sys-
tems. The difference in F-score is statistically sig-
nificant (based on the two-tailed t-test, p < .05) for
the asterisked systems in Table 3. For the bipartite
class, the difference in F-score is not statistically sig-
nificant between any system pairing.
There is surprisingly little separating the tagger-,
chunker- and RASP-based feature extraction meth-
ods. This is largely due to the precision/recall trade-
off noted above for the different systems.
5.2 Open data results
We next turn to the task of classifying all unseen
common nouns using the gold-standard data and the
best-performing classifier configurations for each
3Calculated according to: 2?precision ?recallprecision+recall
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  100  1000  10000
 0.2
 0.4
 0.6
 0.8
 1
precision
recall
P
re
ci
si
on
R
ec
al
l
Mean frequency
Figure 1: Precision?recall curve for countable nouns
countability class (indicated in bold in Table 3).4
Here, the baseline method is to classify every noun
as being uniquely countable.
There were 11,499 feature-mapped common
nouns not contained in the union of the gold-
standard datasets. Of these, the classifiers were able
to classify 10,355 (90.0%): 7,974 (77.0%) as count-
able (e.g. alchemist), 2,588 (25.0%) as uncountable
(e.g. ingenuity), 9 (0.1%) as bipartite (e.g. head-
phones), and 80 (0.8%) as plural only (e.g. dam-
ages). Only 139 nouns were assigned to multiple
countability classes.
We evaluated the classifier outputs in two ways.
In the first, we compared the classifier output to the
combined COMLEX and ALT-J/E lexicons: a lexicon
with countability information for 63,581 nouns. The
classifiers found a match for 4,982 of the nouns. The
predicted countability was judged correct 94.6% of
the time. This is marginally above the level of match
between ALT-J/E and COMLEX (93.8%) and substan-
tially above the baseline of all-countable at 89.7%
(error reduction = 47.6%).
To gain a better understanding of the classifier
performance, we analysed the correlation between
corpus frequency of a given target noun and its pre-
cision/recall for the countable class.5 To do this,
we listed the 11,499 unannotated nouns in increas-
ing order of corpus occurrence, and worked through
the ranking calculating the mean precision and re-
call over each partition of 500 nouns. This resulted
in the precision?recall graph given in Figure 1, from
which it is evident that mean recall is proportional
and precision inversely proportional to corpus fre-
4In each case, the classifier is run over the best-
500 features as selected by the method described in
Baldwin and Bond (2003) rather than the full feature set, purely
in the interests of reducing processing time. Based on cross-
validated results over the training data, the resultant difference
in performance is not statistically significant.
5We similarly analysed the uncountable class and found the
same basic trend.
quency. That is, for lower-frequency nouns, the clas-
sifier tends to rampantly classify nouns as count-
able, while for higher-frequency nouns, the classi-
fier tends to be extremely conservative in positively
classifying nouns. One possible explanation for this
is that, based on the training data, the frequency
of a noun is proportional to the number of count-
ability classes it belongs to. Thus, for the more
frequent nouns, evidence for alternate countability
classes can cloud the judgement of a given classifier.
In secondary evaluation, the authors used BNC
corpus evidence to blind-annotate 100 randomly-
selected nouns from the test data, and tested the cor-
relation with the system output. This is intended
to test the ability of the system to capture corpus-
attested usages of nouns, rather than independent
lexicographic intuitions as are described in the COM-
LEX and ALT-J/E lexicons. Of the 100, 28 were clas-
sified by the annotators into two or more groups
(mainly countable and uncountable). On this set,
the baseline of all-countable was 87.8%, and the
classifiers gave an agreement of 92.4% (37.7% e.r.),
agreement with the dictionaries was also 92.4%.
Again, the main source of errors was the classi-
fier only returning a single countability for each
noun. To put this figure in proper perspective, we
also hand-annotated 100 randomly-selected nouns
from the training data (that is words in our com-
bined lexicon) according to BNC corpus evidence.
Here, we tested the correlation between the manual
judgements and the combined ALT-J/E and COMLEX
dictionaries. For this dataset, the baseline of all-
countable was 80.5%, and agreement with the dic-
tionaries was a modest 86.8% (32.3% e.r.). Based
on this limited evaluation, therefore, our automated
method is able to capture corpus-attested count-
abilities with greater precision than a manually-
generated static repository of countability data.
6 Discussion
The above results demonstrate the utility of the
proposed method in learning noun countability
from corpus data. In the final system configu-
ration, the system accuracy was 94.6%, compar-
ing favourably with the 78% accuracy reported
by Bond and Vatikiotis-Bateson (2002), 89.5% of
O?Hara et al (2003), and also the noun token-based
results of Schwartz (2002).
At the moment we are merely classifying nouns
into the four classes. The next step is to store the
distribution of countability for each target noun and
build a representation of each noun?s countability
preferences. We have made initial steps in this direc-
tion, by isolating token instances strongly support-
ing a given countability class analysis for that target
noun. We plan to estimate the overall frequency of
the different countabilities based on this evidence.
This would represent a continuous equivalent of the
discrete 5-way scale employed in ALT-J/E, tunable to
different corpora/domains.
For future work we intend to: investigate further
the relation between meaning and countability, and
the possibility of using countability information to
prune the search space in word sense disambigua-
tion; describe and extract countability-idiosyncratic
constructions, such as determinerless PPs and role-
nouns; investigate the use of a grammar that distin-
guishes between countable and uncountable uses of
nouns; and in combination with such a grammar, in-
vestigate the effect of lexical rules on countability.
7 Conclusion
We have proposed a knowledge-rich lexical acqui-
sition technique for multi-classifying a given noun
according to four countability classes. The tech-
nique operates over a range of feature clusters draw-
ing on pre-processed corpus data, which are then fed
into independent classifiers for each of the count-
ability classes. The classifiers were able to selec-
tively classify the countability preference of English
nouns with a precision of 94.6%.
Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. BCS-0094638 and also
the Research Collaboration between NTT Communication Sci-
ence Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University. We would like to thank
Leonoor van der Beek, Ann Copestake, Ivan Sag and the three
anonymous reviewers for their valuable input on this research.
References
Keith Allan. 1980. Nouns and countability. Language,
56(3):541?67.
Timothy Baldwin and Francis Bond. 2003. A plethora of meth-
ods for learning English countability. In Proc. of the 2003
Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2003), Sapporo, Japan. (to appear).
Francis Bond and Caitlin Vatikiotis-Bateson. 2002. Using an
ontology to determine English countability. In Proc. of the
19th International Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan.
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994.
Countability and number in Japanese-to-English machine
translation. In Proc. of the 15th International Conference
on Computational Linguistics (COLING ?94), pages 32?8,
Kyoto, Japan.
Francis Bond. 2001. Determiners and Number in English, con-
trasted with Japanese, as exemplified in Machine Transla-
tion. Ph.D. thesis, University of Queensland, Brisbane, Aus-
tralia.
Ted Briscoe and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd International
Conference on Language Resources and Evaluation (LREC
2002), pages 1499?1504, Las Palmas, Canary Islands.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Ann Copestake and Ted Briscoe. 1995. Semi-productive poly-
semy and sense extension. Journal of Semantics, pages 15?
67.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2002. TiMBL: Tilburg memory based
learner, version 4.2, reference guide. ILK technical report
02-01.
Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.
COMLEX Syntax Reference Manual. Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E?. In Proc. of the Third
Machine Translation Summit (MT Summit III), pages 101?
106, Washington DC.
Marc Light. 1996. Morphological cues for lexical semantics.
In Proc. of the 34th Annual Meeting of the ACL, pages 25?
31, Santa Cruz, USA.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?23.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-
der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, Fritz
Lehmann, Matt Smith, David Baxter, Jon Curtis, and Peter
Wagner. 2003. Inducing criteria for mass noun lexical map-
pings using the Cyc KB and its extension to WordNet. In
Proc. of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, the Netherlands.
Lane O.B. Schwartz. 2002. Corpus-based acquisition of head
noun countability features. Master?s thesis, Cambridge Uni-
versity, Cambridge, UK.
Eric V. Siegel and Kathleen McKeown. 2000. Learning meth-
ods to combine linguistic indicators: Improving aspectual
classification and revealing linguistic insights. Computa-
tional Linguistics, 26(4):595?627.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proc.
of the 4th Conference on Computational Natural Language
Learning (CoNLL-2000), Lisbon, Portugal.
Anna Wierzbicka. 1988. The Semantics of Grammar. John
Benjamin.
Extracting the Unextractable: A Case Study on Verb-particles
Timothy Baldwin? and Aline Villavicencio?
? CSLI, Ventura Hall, Stanford University
Stanford, CA 94305-4115 USA
tbaldwin@csli.stanford.edu
? University of Cambridge, Computer Laboratory, William Gates Building
JJ Thomson Avenue, Cambridge CB3 OFD, UK
Aline.Villavicencio@cl.cam.ac.uk
Abstract
This paper proposes a series of techniques for ex-
tracting English verb?particle constructions from
raw text corpora. We initially propose three basic
methods, based on tagger output, chunker output
and a chunk grammar, respectively, with the chunk
grammar method optionally combining with an at-
tachment resolution module to determine the syn-
tactic structure of verb?preposition pairs in ambigu-
ous constructs. We then combine the three methods
together into a single classifier, and add in a number
of extra lexical and frequentistic features, producing
a final F-score of 0.865 over the WSJ.
1 Introduction
There is growing awareness of the pervasiveness and
idiosyncrasy of multiword expressions (MWEs),
and the need for a robust, structured handling
thereof (Sag et al, 2002; Calzolari et al, 2002;
Copestake et al, 2002). Examples of MWEs are
lexically fixed expressions (e.g. ad hoc), idioms (e.g.
see double), light verb constructions (e.g. make a
mistake) and institutionalised phrases (e.g. kindle
excitement).
MWEs pose a challenge to NLP due to their syn-
tactic and semantic idiosyncrasies, which are often
unpredictable from their component parts. Large-
scale manual annotation of MWEs is infeasible due
to their sheer volume (at least equivalent to the num-
ber of simplex words (Jackendoff, 1997)), produc-
tivity and domain-specificity. Ideally, therefore, we
would like to have some means of automatically ex-
tracting MWEs from a given domain or corpus, al-
lowing us to pre-tune our grammar prior to deploy-
ment. It is this task of extraction that we target in
this paper. This research represents a component of
the LinGO multiword expression project,1 which is
targeted at extracting, adequately handling and rep-
resenting MWEs of all types. As a research testbed
and target resource to expand/domain-tune, we use
the LinGO English Resource Grammar (LinGO-
ERG), a linguistically-precise HPSG-based gram-
mar under development at CSLI (Copestake and
Flickinger, 2000; Flickinger, 2000).
The particular MWE type we target for extrac-
tion is the English verb-particle construction.
Verb-particle constructions (?VPCs?) consist of a
1http://lingo.stanford.edu/mwe
head verb and one or more obligatory particles,
in the form of intransitive prepositions (e.g. hand
in), adjectives (e.g. cut short) or verbs (e.g. let
go) (Villavicencio and Copestake, 2002a; Villavicen-
cio and Copestake, 2002b; Huddleston and Pullum,
2002); for the purposes of this paper, we will fo-
cus exclusively on prepositional particles?by far the
most common and productive of the three types?
and further restrict our attention to single-particle
VPCs (i.e. we ignore VPCs such as get alng to-
gether). We define VPCs to optionally select for an
NP complement, i.e. to occur both transitively (e.g.
hand in the paper) and intransitively (e.g. battle on).
One aspect of VPCs that makes them difficult to
extract (cited in, e.g., Smadja (1993)) is that the
verb and particle can be non-contiguous, e.g. hand
the paper in and battle right on. This sets them apart
from conventional collocations and terminology (see,
e.g., Manning and Schu?tze (1999) and McKeown and
Radev (2000)) in that they cannot be captured ef-
fectively using N-grams, due to the variability in the
number and type of words potentially interceding
between the verb and particle.
We are aiming for an extraction technique which
is applicable to any raw text corpus, allowing us to
tune grammars to novel domains. Any linguistic
annotation required during the extraction process,
therefore, is produced through automatic means,
and it is only for reasons of accessibility and compa-
rability with other research that we choose to work
over the Wall Street Journal section of the Penn
Treebank (Marcus et al, 1993). That is, other than
in establishing upper bounds on the performance of
the different extraction methods, we use only the
raw text component of the treebank.
In this paper, we first outline distinguishing fea-
tures of VPCs relevant to the extraction process
(? 2). We then present and evaluate a number of
simple methods for extracting VPCs based on, re-
spectively, POS tagging (? 3), the output of a full
text chunk parser (? 4), and a chunk grammar (? 5).
Finally, we detail enhancements to the basic meth-
ods (? 6) and give a brief description of related re-
search (? 7) before concluding the paper (? 8).
2 Distinguishing Features of VPCs
Here, we review a number of features of VPCs per-
tinent to the extraction task. First, we describe lin-
guistic qualities that characterise VPCs, and second
we analyse the actual occurrence of VPCs in the
WSJ.
2.1 Linguistic features
Given an arbitrary verb?preposition pair, where the
preposition is governed by the verb, a number of
analyses are possible. If the preposition is intransi-
tive, a VPC (either intransitive or transitive) results.
If the preposition is transitive, it must select for an
NP, producing either a prepositional verb (e.g. re-
fer to) or a free verb?preposition combination
(e.g. put it on the table, climb up the ladder).
A number of diagnostics can be used to distinguish
VPCs from both prepositional verbs and free verb?
preposition combinations (Huddleston and Pullum,
2002):
1. transitive VPCs undergo the particle alterna-
tion
2. with transitive VPCs, pronominal objects must
be expressed in the ?split? configuration
3. manner adverbs cannot occur between the verb
and particle
The first two diagnostics are restricted to transitive
VPCs, while the third applies to both intransitive
and transitive VPCs.
The first diagnostic is the canonical test for par-
ticlehood, and states that transitive VPCs take two
word orders: the joined configuration whereby the
verb and particle are adjacent and the NP comple-
ment follows the particle (e.g. hand in the paper),
and the split configuration whereby the NP com-
plement occurs between the verb and particle (e.g.
hand the paper in). Note that prepositional verbs
and free verb?preposition combinations can occur
only in the joined configuration (e.g. refer to the book
vs. *refer the book to). Therefore, the existence of
a verb?preposition pair in the split configuration is
sufficient evidence for a VPC analysis. It is impor-
tant to realise that compatibility with the particle
alternation is a sufficient but not necessary condi-
tion on verb?particlehood. That is, a small number
of VPCs do not readily occur in the split configu-
ration, including carry out (a threat) (cf. ?carry a
threat out).
The second diagnostic stipulates that pronominal
NPs can occur only in the split configuration (hand
it in vs. *hand in it). Note also that heavy NPs tend
to occur in the joined configuration, and that various
other factors interact to determine which configura-
tion a given VPC in context will occur in (see, e.g.,
Gries (2000)).
The third diagnostic states that manner adverbs
cannot intercede between the verb and particle (e.g.
*hand quickly the paper in). Note that this con-
straint is restricted to manner adverbs, and that
there is a small set of adverbs which can pre-modify
particles and hence occur between the verb and par-
ticle (e.g. well in jump well up).
2.2 Corpus occurrence
In order to get a feel for the relative frequency of
VPCs in the corpus targeted for extraction, namely
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60  70
V
P
C
 t
yp
es
 (
%
)
Corpus frequency
Figure 1: Frequency distribution of VPCs in the
WSJ
Tagger correctextracted Prec Rec F?=1
Brill 135135 1.000 0.177 0.301
Penn 667800 0.834 0.565 0.673
Table 1: POS-based extraction results
the WSJ section of the Penn Treebank, we took a
random sample of 200 VPCs from the Alvey Natu-
ral Language Tools grammar (Grover et al, 1993)
and did a manual corpus search for each. In the
case that a VPC was found attested in the WSJ,
we made a note of the frequency of occurrence as:
(a) an intransitive VPC, (b) a transitive VPC in the
joined configuration, and (c) a transitive VPC in the
split configuration. Of the 200 VPCs, only 62 were
attested in the Wall Street Journal corpus (WSJ),
at a mean token frequency of 5.1 and median to-
ken frequency of 2 (frequencies totalled over all 3
usages). Figure 1 indicates the relative proportion
of the 62 attested VPC types which occur with the
indicated frequencies. From this, it is apparent that
two-thirds of VPCs occur at most three times in the
overall corpus, meaning that any extraction method
must be able to handle extremely sparse data.
Of the 62 attested VPCs, 29 have intransitive us-
ages and 45 have transitive usages. Of the 45 at-
tested transitive VPCs, 12 occur in both the joined
and split configurations and can hence be unambigu-
ously identified as VPCs based on the first diagnostic
from above. For the remaining 33 transitive VPCs,
we have only the joined usage, and must find some
alternate means of ruling out a prepositional verb
or free verb?preposition combination analysis. Note
that for the split VPCs, the mean number of words
occurring between the verb and particle was 1.6 and
the maximum 3.
In the evaluation of the various extraction tech-
niques below, recall is determined relative to this
limited set of 62 VPCs attested in the WSJ. That
is, recall is an indication of the proportion of the 62
VPCs contained within the set of extracted VPCs.
3 Method-1: Simple POS-based
Extraction
One obvious method for extracting VPCs is to run a
simple regular expression over the output of a part-
of-speech (POS) tagger, based on the observation
that the Penn Treebank POS tagset, e.g., contains a
dedicated particle tag (RP). Given that all particles
are governed by a verb, extraction consists of simply
locating each particle and searching back (to the left
of the particle, as particles cannot be passivised or
otherwise extraposed) for the head verb of the VPC.
Here and for the subsequent methods, we assume
that the maximum word length for NP complements
in the split configuration for transitive VPCs is 5,2
i.e. that an NP ?heavier? than this would occur more
naturally in the joined configuration. We thus dis-
count all particles which are more than 5 words from
their governing verb. Additionally, we extracted a
set of 73 canonical particles from the LinGO-ERG,
and used this to filter out extraneous particles in the
POS data.
In line with our assumption of raw text to extract
over, we use the Brill tagger (Brill, 1995) to auto-
matically tag the WSJ, rather than making use of
the manual POS annotation provided in the Penn
Treebank. We further lemmatise the data using
morph (Minnen et al, 2001) and extract VPCs based
on the Brill tags. This produces a total of 135 VPCs,
which we evaluate according to the standard metrics
of precision (Prec), recall (Rec) and F-score (F?=1).
Note that here and for the remainder of this pa-
per, precision is calculated according to the man-
ual annotation for the combined total of 4,173 VPC
candidate types extracted by the various methods
described in this paper, whereas recall is relative to
the 62 attested VPCs from the Alvey Tools data as
described above.
As indicated in the first line of Table 1 (?Brill?),
the simple POS-based method results in a precision
of 1.000, recall of 0.177 and F-score of 0.301.
In order to determine the upper bound on per-
formance for this method, we ran the extraction
method over the original tagging from the Penn
Treebank. This resulted in an F-score of 0.774
(?Penn? in Table 1). The primary reason for the
large disparity between the Brill tagger output and
original Penn Treebank annotation is that it is no-
toriously difficult to differentiate between particles,
prepositions and adverbs (Toutanova and Manning,
2000). Over the WSJ, the Brill tagger achieves a
modest tag recall of 0.103 for particles, and tag pre-
cision of 0.838. That is, it is highly conservative in
allocating particle tags, to the extent that it recog-
nises only two particle types for the whole of the
WSJ: out and down.
4 Method-2: Simple Chunk-based
Extraction
To overcome the shortcomings of the Brill tagger
in identifying particles, we next look to full chunk
2Note, this is the same as the maximum span length of 5
used by Smadja (1993), and above the maximum attested NP
length of 3 from our corpus study (see Section 2.2).
WSJ CoNLL
Prec Rec F?=1 Prec Rec F?=1
0.889 0.911 0.900 0.912 0.925 0.919
Table 2: Chunking performance
parsing. Full chunk parsing involves partitioning
up a text into syntactically-cohesive, head-final seg-
ments (?chunks?), without attempting to resolve
inter-chunk dependencies. In the chunk inventory
devised for the CoNLL-2000 test chunking shared
task (Tjong Kim Sang and Buchholz, 2000), a ded-
icated particle chunk type once again exists. It is
therefore possible to adopt an analogous approach to
that from Method-1, in identifying particle chunks
then working back to locate the verb each particle
chunk is associated with.
4.1 Chunk parsing method
In order to chunk parse the WSJ, we first tagged
the full WSJ and Brown corpora using the Brill tag-
ger, and then converted them into chunks based on
the original Penn Treebank parse trees, with the
aid of the conversion script used in preparing the
CoNLL-2000 shared task data.3 We next lemma-
tised the data using morph (Minnen et al, 2000),
and chunk parsed the WSJ with TiMBL 4.1 (Daele-
mans et al, 2001) using the Brown corpus as train-
ing data. TiMBL is a memory-based classification
system based on the k-nearest neighbour algorithm,
which takes as training data a set of fixed-length
feature vectors pre-classified according to an infor-
mation field. For each test instance described over
the same feature vector, it then returns the ?neigh-
bours? at the k-nearest distances to the test instance
and classifies the test instance according to the class
distribution over those neighbours. TiMBL provides
powerful functionality for determining the relative
distance between different values of a given feature
in the form of MVDM, and also supports weighted
voting between neighbours in classifying inputs, e.g.
in the form of inverse distance weighting.
We ran TiMBL based on the feature set described
in Veenstra and van den Bosch (2000), that is using
the 5 word lemmata and POS tags to the left and
3 word lemmata and POS tags to the right of each
focus word, along with the POS tag and lemma for
the focus word. We set k to 5, ran MVDM over only
the POS tags4 and used inverse distance weighting,
but otherwise ran TiMBL with the default settings.
We evaluated the basic TiMBL method over both
the full WSJ data, training on the Brown section
of the Penn Treebank, and over the original shared
task data from CoNLL-2000, the results for which
are presented in Table 2. Note that, similarly to
the CoNLL-2000 shared task, precision, recall and
3Note that the gold standard chunk data for the WSJ was
used only in evaluation of chunking performance, and to es-
tablish upper bounds on the performance of the various ex-
traction methods.
4Based on the results of Veenstra and van den Bosch
(2000) and the observation that MVDM is temperamental
over sparse data (i.e. word lemmata).
Chunker correctextracted Prec Rec F?=1
TiMBL 695854 0.772 0.548 0.641
Penn 651760 0.857 0.694 0.766
Table 3: Chunk tag-based extraction results
F-score are all evaluated at the chunk rather than
the word level. The F-score of 0.919 for the CoNLL-
2000 data is roughly the median score attained by
systems performing in the original task, and slightly
higher than the F-score of 0.915 reported by Veen-
stra and van den Bosch (2000), due to the use of
word lemmata rather than surface forms, and also
inverse distance weighting. The reason for the drop-
off in performance between the CoNLL data and the
full WSJ is due to the CoNLL training and test data
coming from a homogeneous data source, namely a
subsection of the WSJ, but the Brown corpus being
used as the training data in chunking the full extent
of the WSJ.
4.2 Extraction method
Having chunk-parsed the WSJ in the manner de-
scribed above, we next set about extracting VPCs by
identifying each particle chunk, and searching back
for the governing verb. As for Method-1, we allow a
maximum of 5 words to intercede between a particle
and its governing verb, and we apply the additional
stipulation that the only chunks that can occur be-
tween the verb and the particle are: (a) noun chunks,
(b) preposition chunks adjoining noun chunks, and
(c) adverb chunks found in our closed set of particle
pre-modifiers (see ? 2.1). Additionally, we used the
gold standard set of 73 particles to filter out extra-
neous particle chunks, as for Method-1 above.
The results for chunk-based extraction are pre-
sented in Table 3, evaluated over the chunk parser
output (?TiMBL?) and also the gold-standard chunk
data for the WSJ (?Penn?). These results are signifi-
cantly better than those for Method-1 over the Brill
output and Penn data, respectively, both in terms
of the raw number of VPCs extracted and F-score.
One reason for the relative success of extracting over
chunker as compared to tagger output is that our
chunker was considerably more successful than the
Brill tagger at annotating particles, returning an F-
score of 0.737 over particle chunks (precision=0.786,
recall=0.693). The stipulations on particle type and
what could occur between a verb and particle chunk
were crucial in maintaining a high VPC extraction
precision, relative to both particle chunk precision
and the gold standard extraction precision. As can
be seen from the upper bound on recall (i.e. recall
over the gold standard chunk data), however, this
method has limited applicability.
5 Method-3: Chunk Grammar-based
Extraction
The principle weakness of Method-2 was recall, lead-
ing us to implement a rule-based chunk sequencer
which searches for particles in prepositional and ad-
verbial chunks as well as particle chunks. In essence,
Method correctextracted Prec Rec F?=1
Rule?att 6761119 0.604 0.694 0.646
Timbl?att 615823 0.747 0.661 0.702
Penn?att 694927 0.749 0.823 0.784
Rule+att 9513126 0.304 0.823 0.444
Timbl+att 7391049 0.704 0.710 0.707
Penn+att 7501079 0.695 0.871 0.773
Table 4: Chunk grammar-based extraction results
we take each verb chunk in turn, and search to the
right for a single-word particle, prepositional or ad-
verbial chunk which is contained in the gold stan-
dard set of 73 particles. For each such chunk pair,
it then analyses: (a) the chunks which occur be-
tween them to ensure that, maximally, an NP and
particle pre-modifier adverb chunk are found; (b)
the chunks that occur immediately after the parti-
cle/preposition/adverb chunk to check for a clause
boundary or NP; and (c) the clause context of the
verb chunk for possible extraposition of an NP ver-
bal complement, through passivisation or relativisa-
tion. The objective of this analysis is to both deter-
mine the valence of the VPC candidate (intransitive
or transitive) and identify evidence either support-
ing or rejecting a VPC analysis. Evidence for or
against a VPC analysis is in the form of congruence
with the known linguistic properties of VPCs, as de-
scribed in Section 2.1. For example, if a pronominal
noun chunk were found to occur immediately after
the (possibly) particle chunk (e.g. *see off him), a
VPC analysis would not be possible. Alternatively,
if a punctuation mark (e.g. a full stop) were found
to occur immediately after the ?particle? chunk and
nothing interceded between the verb and particle
chunk, then this would be evidence for an intran-
sitive VPC analysis.
The chunk sequencer is not able to furnish posi-
tive or negative evidence for a VPC analysis in all
cases. Indeed, in a high proportion of instances, a
noun chunk (=NP) was found to follow the ?parti-
cle? chunk, leading to ambiguity between analysis as
a VPC, prepositional verb or free verb?preposition
combination (see Section 2.1), or in the case that
an NP occurs between the verb and particle, the
?particle? being the head of a PP post-modifying
an NP. As a case in point, the VP hand the paper in
here could take any of the following structures: (1)
hand [the paper] [in] [here] (transitive VPC hand
in with adjunct NP here), (2) hand [the paper] [in
here] (transitive prepositional verb hand in or sim-
ple transitive verb with PP adjunct), and (3) hand
[the paper in here] (simple transitive verb). In such
cases, we can choose to either (a) avoid committing
ourselves to any one analysis, and ignore all such
ambiguous cases, or (b) use some means to resolve
the attachment ambiguity (i.e. whether the NP is
governed by the verb, resulting in a VPC, or the
preposition, resulting in a prepositional verb or free
verb?preposition combination). In the latter case,
we use an unsupervised attachment disambiguation
method, based on the log-likelihood ratio (?LLR?,
Dunning (1993)). That is, we use the chunker output
to enumerate all the verb?preposition, preposition?
noun and verb?noun bigrams in theWSJ data, based
on chunk heads rather than strict word bigrams. We
then use frequency data to pre-calculate the LLR for
each such type. In the case that the verb and ?par-
ticle? are joined (i.e. no NP occurs between them),
we simply compare the LLR of the verb?noun and
particle?noun pairs, and assume a VPC analysis in
the case that the former is strictly larger than the
latter. In the case that the verb and ?particle? are
split (i.e. we have the chunk sequence VC NC1 PC
NC2),5 we calculate three scores: (1) the product
of the LLR for (the heads of) VC-PC and VC-NC2
(analysis as VPC, with NC2 as an NP adjunct of
the verb); (2) the product of the LLR for NC1-PC
and PC-NC2 (transitive verb analysis, with the PP
modifying NC1); and (3) the product of the LLR for
VC-PC and PC-NC2 (analysis as prepositional verb or
free verb?preposition combination). Only in the case
that the first of these scores is strictly greater than
the other two, do we favour a (transitive) VPC anal-
ysis.
Based on the positive and negative grammatical
evidence from above, for both intransitive and tran-
sitive VPC analyses, we generate four frequency-
based features. The optional advent of data derived
through attachment resolution, again for both in-
transitive and transitive VPC analyses, provides an-
other two features. These features can be combined
in either of two ways: (1) in a rule-based fashion,
where a given verb?preposition pair is extracted out
as a VPC only in the case that there is positive and
no negative evidence for either an intransitive or
transitive VPC analysis (?Rule? in Table 4); and
(2) according to a classifier, using TiMBL to train
over the auto-chunked Brown data, with the same
basic settings as for chunking (with the exception
that each feature is numeric and MVDM is not used
? results presented as ?Timbl? in Table 4). We also
present upper bound results for the classifier-based
method using gold standard chunk data, rather than
the chunker output (?Penn?). For each of these
three basic methods, we present results with and
without the attachment-resolved data (??att?).
Based on the results in Table 4, the classifier-based
method (?Timbl?) is superior to not only the rule-
based method (?Rule?), but also Method-1 and
Method-2. While the rule-based method degrades
significantly when the attachment data is factored
in, the classifier-based method remains at the same
basic F-score value, undergoing a drop in precision
but equivalent gain in recall and gaining more than
120 correct VPCs in the process. Rule+att returns
the highest recall value of all the automatic meth-
ods to date at 0.823, at the cost of low precision at
0.304. This points to the attachment disambigua-
tion method having high recall but low precision.
Timbl?att and Penn?att are equivalent in terms
5Here, VC = verb chunk, NC = noun chunk and PC = (in-
transitive or transitive) preposition chunk.
Method correctextracted Prec Rec F?=1
Combine 719953 0.754 0.710 0.731
M?2 686778 0.882 0.677 0.766
M3?att? 684788 0.868 0.694 0.771
M3+att? 8711020 0.854 0.823 0.838
Combine? 10001164 0.859 0.871 0.865
Combine?Penn 9311047 0.889 0.903 0.896
Table 5: Consolidated extraction results
of precision, but the Penn data leads to considerably
better recall.
6 Improving on the Basic Methods
Comparing the results for the three basic methods,
it is apparent that Method-1 and Method-2 offer
higher precision while Method-3 offers higher recall.
In order to capitalise on the respective strengths of
the different methods, in this section, we investigate
the possibility of combining the outputs of the four
methods into a single consolidated classifier. Sys-
tem combination is achieved by taking the union of
all VPC outputs from all systems, and having a vec-
tor of frequency-based features for each, based on
the outputs of the different methods for the VPC
in question. For each of Method-1 and Method-2,
a single feature is used describing the total number
of occurrences of the given VPC detected by that
method. For Method-3, we retain the 6 features used
as input to Timbl?att, namely the frequency with
which positive and negative evidence was detected
and also the frequency of VPCs detected through at-
tachment resolution, for both intransitive and tran-
sitive VPCs. Training data comes from the output
of the different methods over the Brown corpus, and
the chunking data for Method-2 and Method-3 was
generated using the WSJ gold standard chunk data
as training data, analogously to the method used to
chunk parse the WSJ.
The result of this simple combination process is
presented in the first line of Table 5 (?Combine?).
Encouragingly, we achieved the exact same recall
as the best of the simple methods (Timbl+att) at
0.710, and significantly higher F-score than any in-
dividual method at 0.731.
Steeled by this initial success, we further augment
the feature space with features describing the fre-
quency of occurrence of: (a) the particle in the cor-
pus, and (b) deverbal noun and adjective forms of
the VPC in the corpus (e.g. turnaround, dried-up),
determined through a simple concatenation opera-
tion optionally inserting a hyphen. The first of these
is attempted to reflect the fact that high-frequency
particles (e.g. up, over) are more productive (i.e.
are found in novel VPCs more readily) than low-
frequency particles.6 The deverbal feature is in-
tended to reflect the fact that VPCs have the po-
6We also experimented with a similar feature describing
verb frequency, but found it to either degrade or have no
effect on classifier performance.
tential to undergo deverbalisation whereas prepo-
sitional verbs and free verb?preposition combina-
tions do not.7 We additionally added in features
describing: (a) the number of letters in the verb
lemma, (b) the verb lemma, and (c) the particle
lemma. The first feature was intended to capture
the informal observation that shorter verbs tend
to be more productive than longer verbs (which
offers one possible explanation for the anomalous
call/ring/phone/*telephone up). The second and
third features are intended to capture this same pro-
ductivity effect, but on a individual word-level. Note
that as TiMBL treats all features as fully indepen-
dent, it is not able to directly pick up on the gold
standard verb?particle pairs in the training data to
select in the test data.
The expanded set of features was used to re-
evaluate each of: Method-2 (M?2 in Table 5); theclassifier version of Method-3 with and without
attachment-resolved data (M3?ATT?); and the
simple system combination method (Combine?).
Additionally, we calculated an upper bound for the
expanded feature set based on the gold standard
data for each of the methods (Combine?Penn in Ta-
ble 5). The results for these five consolidated meth-
ods are presented in Table 5.
The addition of the 7 new features leads to an
appreciable gain in both precision and recall for all
methods, with the system combination method once
again proving to be the best performer, at an F-score
of 0.865. The differential between the system com-
bination method when trained over auto-generated
POS and chunk data (Combine?) and that trained
over gold standard data (Combine?Penn) is still tan-gible, but considerably less than for any of the in-
dividual methods. Importantly, Combine? outper-
forms the gold standard results for each of the in-
dividual methods. Examples of false positives (i.e.
verb?prepositions misclassified as VPCs) returned
by this final system configuration are firm away, base
on and very off.
In Section 1, we made the claim that VPCs are
highly productive and domain-specific. We validate
this claim by comparing the 1000 VPCs correctly
extracted by the Combine? method against both
the LinGO-ERG and the relatively broad-coverage
Alvey Tools VPC inventory. The 28 March, 2002
version of the LinGO-ERG contains a total of 300
intransitive and transitive VPC types, of which
195 were contained in the 1000 correctly-extracted
VPCs. Feeding the remaining 805 VPCs into the
grammar (with a lexical type describing their tran-
sitivity) would therefore result in an almost four-
fold increase in the total number of VPCs, and in-
crease the chances of the grammar being able to
parse WSJ-style text. The Alvey Tools data con-
tains a total of 2254 VPC types. Of the 1000 ex-
tracted VPCs, 284 or slightly over 28%, were not
contained in the Alvey data, with examples includ-
ing head down, blend together and bid up. Combin-
ing this result with that for the LinGO-ERG, one can
7Note that only a limited number of VPCs can be dever-
balised in this manner: of the 62 VPCs attested in the WSJ,
only 8 had a deverbal usage.
see that we are not simply extracting information al-
ready at our fingertips, but are accessing significant
numbers of novel VPC types.
7 Related research
There is a moderate amount of research related to
the extraction of VPCs, or more generally phrasal
verbs, which we briefly describe here.
One of the earliest attempts at extracting ?in-
terrupted collocations? (i.e. non-contiguous colloca-
tions, including VPCs), was that of Smadja (1993).
Smadja based his method on bigrams, but unlike
conventional collocation work, described bigrams by
way of the triple of ?word1,word2,posn?, where posn
is the number of words occurring between word 1 and
word2 (up to 4). For VPCs, we can reasonably ex-
pect from 0 to 4 words to occur between the verb
and the particle, leading to 5 distinct variants of
the same VPC and no motivated way of selecting
between them. Smadja did not attempt to evalu-
ate his method other than anecdotally, making any
comparison with our research impossible.
The work of Blaheta and Johnson (2001) is closer
in its objectives to our research, in that it takes a
parsed corpus and extracts out multiword verbs (i.e.
VPCs and prepositional verbs) through the use of
log-linear models. Once again, direct comparison
with our results is difficult, as Blaheta and Johnson
output a ranked list of all verb?preposition pairs,
and subjectively evaluate the quality of different sec-
tions of the list. Additionally, they make no attempt
to distinguish VPCs from prepositional verbs.
The method which is perhaps closest to ours is
that of Kaalep and Muischnek (2002) in extracting
Estonian multiword verbs (which are similar to En-
glish VPCs in that the components of the multiword
verb can be separated by other words). Kaalep and
Muischnek apply the ?mutual expectation? test over
a range of ?positioned bigrams?, similar to those
used by Smadja. They test their method over three
different corpora, with results ranging from a preci-
sion of 0.21 and recall of 0.86 (F-score=0.34) for the
smallest corpus, to a precision of 0.03 and recall of
0.85 (F-score=0.06) for the largest corpus. That is,
high levels of noise are evident in the system output,
and the F-score values are well below those achieved
by our method for English VPCs.
8 Conclusion
In conclusion, this paper has been concerned with
the extraction of English verb?particle construc-
tions from raw text corpora. Three basic meth-
ods were proposed, based on tagger output, chunker
output and a chunk grammar; the chunk grammar
method was optionally combined with attachment
resolution to determine the syntactic structure of
verb?preposition pairs in ambiguous constructs. We
then experimented with combining the output of the
three methods together into a single classifier, and
further complemented the feature space with a num-
ber of lexical and frequentistic features, culminating
in an F-score of 0.865 over the WSJ.
It is relatively simple to adapt the meth-
ods described here to output subcategorisation
types, rather than a binary judgement on verb?
particlehood. This would allow the extracted out-
put to be fed directly into the LinGO-ERG for use
in parsing. We are also interested in extending
the method to extract prepositional verbs, many
of which appear in the attachment resolution data
and are subsequently filtered out by the consolidated
classifier.
Acknowledgements
This research was supported in part by NSF grant
BCS-0094638 and also the Research Collaboration
between NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation and
CSLI, Stanford University. We would like to thank
Francis Bond, Ann Copestake, Dan Flickinger, Di-
ana McCarthy and the three anonymous reviewers
for their valuable input on this research.
References
Don Blaheta and Mark Johnson. 2001. Unsuper-
vised learning of multi-word verbs. In Proc. of
the ACL/EACL 2001 Workshop on the Compu-
tational Extraction, Analysis and Exploitation of
Collocations, pages 54?60.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21:543?65.
Nicoletta Calzolari, Charles J. Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Catherine
MacLeod, and Antonio Zampolli. 2002. Towards
best practice for multiword expressions in compu-
tational lexicons. In Proc. of the 3rd International
Conference on Language Resources and Evalua-
tion (LREC 2002), pages 1934?40.
Ann Copestake and Dan Flickinger. 2000. Open
source grammar development environment and
broad-coverage English grammar using HPSG. In
Proc. of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC 2000),
pages 591?8.
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: linguistic precision and reusability. In Proc.
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), pages
1941?7.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2001. TiMBL: Tilburg
Memory Based Learner, version 4.1, reference
guide. ILK technical report 01-04.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61?74.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Stefan T. Gries. 2000. Towards multifactorial anal-
yses of syntactic variation: The case of particle
placement. Ph.D. thesis, University of Hamburg.
Claire Grover, John Carroll, and Edward Briscoe.
1993. The Alvey Natural Language Tools gram-
mar (4th release). Technical Report 284, Com-
puter Laboratory, Cambridge University, UK.
Rodney Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Lan-
guage. Cambridge: Cambridge University Press.
Ray Jackendoff. 1997. The Architecture of the Lan-
guage Faculty. Cambridge, MA: MIT Press.
Heiki-Jaan Kaalep and Kadri Muischnek. 2002. Us-
ing the text corpus to create a comprehensive list
of phrasal verbs. In Proc. of the 3rd International
Conference on Language Resources and Evalua-
tion (LREC 2002), pages 101?5.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Ann Marcinkiewicz. 1993. Building a large an-
notated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313?30.
Kathleen R. McKeown and Dragomir R. Radev.
2000. Collocations. In Robert Dale, Hermann
Moisl, and Harold Somers, editors, Handbook of
Natural Language Processing, chapter 21. Marcel
Dekker.
Guido Minnen, John Carroll, and Darren Pearce.
2000. Robust, applied morphological generation.
In Proc. of the First International Natural Lan-
guage Generation Conference (INLG), pages 201?
8.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of En-
glish. 7(3).
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In
Proc. of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15.
Frank Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143?78.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proc. of the 4th Confer-
ence on Computational Natural Language Learn-
ing (CoNLL-2000), pages 127?132.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Proc.
of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora (EMNLP/VLC-2000).
Jorn Veenstra and Antal van den Bosch. 2000.
Single-classifier memory-based phrase chunking.
In Proc. of the 4th Conference on Computational
Natural Language Learning (CoNLL-2000), pages
157?9.
Aline Villavicencio and Ann Copestake. 2002a.
Phrasal verbs and the LinGO-ERG. LinGO
Working Paper No. 2002-01.
Aline Villavicencio and Ann Copestake. 2002b.
Verb-particle constructions in a computational
grammar. In Proc. of the 9th International Con-
ference on Head-Driven Phrase Structure Gram-
mar (HPSG-2002).
A Plethora of Methods for Learning English Countability
Timothy Baldwin
CSLI
Stanford University
Stanford, CA 94305 USA
tbaldwin@csli.stanford.edu
Francis Bond
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
Kyoto, Japan
bond@cslab.kecl.ntt.co.jp
Abstract
This paper compares a range of methods
for classifying words based on linguis-
tic diagnostics, focusing on the task of
learning countabilities for English nouns.
We propose two basic approaches to
feature representation: distribution-based
representation, which simply looks at
the distribution of features in the cor-
pus data, and agreement-based represen-
tation which analyses the level of token-
wise agreement between multiple pre-
processor systems. We additionally com-
pare a single multiclass classifier archi-
tecture with a suite of binary classifiers,
and combine analyses from multiple pre-
processors. Finally, we present and evalu-
ate a feature selection method.
1 Introduction
Lexical acquisition can be described as the process
of populating a grammar skeleton with lexical items,
through a process of mapping word lemmata onto
lexical types described in the grammar. Depending
on the linguistic precision of the base grammar, lex-
ical acquisition can range in complexity from sim-
ple part-of-speech tagging (shallow lexical acquisi-
tion) to the acquisition of selectionally-constrained
subcategorisation frame clusters or constructional
compatibilities (deep lexical acquisition). Our par-
ticular interest is in the latter task of deep lexical
acquisition with respect to English nouns.
We are interested in developing learning tech-
niques for deep lexical acquisition which take a fixed
set of linguistic diagnostics, and classify words ac-
cording to corpus data. We propose a range of gen-
eral techniques for this task, as exemplified over the
task of English countability acquisition. Countabil-
ity is the syntactic property that determines whether
a noun can take singular and plural forms, and af-
fects the range of permissible modifiers. Many
nouns have both countable and uncountable lemmas,
with differences in meaning: I submitted two papers
?documents? (countable) vs. Please use white paper
?substance to be written on? (uncountable).
This research complements that described in
Baldwin and Bond (2003), where we present the lin-
guistic foundations and features drawn upon in the
countability classification task, and motivate the
claim that countability preferences can be learned
from corpus evidence. In this paper, we focus on
the methods used to tackle the task of countability
classification based on this fixed feature set.
The remainder of this paper is structured as fol-
lows. Section 2 outlines the countability classes,
resources and pre-processors. Section 3 presents
two methods of representing the feature space. Sec-
tion 4 details the different classifier designs and the
dataset, which are then evaluated in Section 5. Fi-
nally, we conclude the paper with a discussion in
Section 6.
2 Preliminaries
In this section, we describe the countability classes,
the resources used in this research, and the feature
extraction method. These are described in greater
detail in Baldwin and Bond (2003).
2.1 Countability classes
Nouns are classified as belonging to one or more of
four possible classes: countable, uncountable, plural
only and bipartite. Countable nouns can be modi-
fied by denumerators, prototypically numbers, and
have a morphologically marked plural form: one
dog, two dogs. Uncountable nouns cannot be mod-
ified by denumerators, but can be modified by un-
specific quantifiers such as much; they do not show
any number distinction (prototypically being singu-
lar): *one equipment, some equipment, *two equip-
ments. Plural only nouns only have a plural form,
such as goods, and cannot be either denumerated or
modified by much; many plural only nouns, such
as clothes, use the plural form even as modifiers: a
clothes horse. Bipartite nouns are plural when they
head a noun phrase (trousers), but generally singu-
lar when used as a modifier (trouser leg); they can
be denumerated with the classifier pair: a pair of
scissors.
2.2 Gold standard data
Information about noun countability was obtained
from two sources: COMLEX 3.0 (Grishman et
al., 1998) and the common noun part of ALT-
J/E?s Japanese-to-English semantic transfer dictio-
nary (Ikehara et al, 1991). Of the approximately
22,000 noun entries in COMLEX, 13,622 are marked
as countable, 710 as uncountable and the remainder
are unmarked for countability. ALT-J/E has 56,245
English noun types with distinct countability.
2.3 Feature space
Features used in this research are divided up into
feature clusters, each of which is conditioned on
the occurrence of a target noun in a given construc-
tion. Feature clusters are either one-dimensional
(describing a single multivariate feature) or two-
dimensional (describing the interaction between two
multivariate features), with each dimension describ-
ing a lexical or syntactic property of the construc-
tion in question. An example of a one-dimensional
feature cluster is head noun number, i.e. the num-
ber (singular or plural) of the target noun when it oc-
curs as the head of an NP; an example of a two-
dimensional feature cluster in subject?verb agree-
ment, i.e. the number (singular or plural) of the tar-
get noun when it occurs as head of a subject NP
vs. number agreement on the verb (singular or plu-
ral). Below, we provide a basic description of the
10 feature clusters used in this research and their di-
mensionality ([x]=1-dimensional feature cluster with
x unit features, [x?y]=2-dimensional feature cluster
with x ? y unit features). These represent a total of
206 unit features.
Head noun number:[?] the number of the target
noun when it heads an NP
Modifier noun number:[?] the number of the target
noun when a modifier in an NP
Subject?verb agreement:[???] the number of the
target noun in a subject position vs. number
agreement on the governing verb
Coordinate noun number:[???] the number of the
target noun vs. the number of the head nouns of
conjuncts
N of N constructions:[????] the type of the N? (e.g.
COLLECTIVE, TEMPORAL) vs. the number of the
target noun (N?) in an N? of N? construction
Occurrence in PPs:[????] the preposition type vs.
the presence or absence of a determiner when
the target noun occurs in singular form in a PP
Pronoun co-occurrence:[????] what personal, pos-
sessive and reflexive pronouns (e.g. he, their,
itself ) occur in the same sentence as singular
and plural instances of the target noun
Singular determiners:[??] what singular-selecting
determiners (e.g. a, much) occur in NPs headed
by the target noun in singular form
Plural determiners:[??] what plural-selecting de-
terminers (e.g. many, various) occur in NPs
headed by the target noun in plural form
Non-bounded determiners:[????] what non-
bounded determiners (e.g. more, sufficient)
occur in NPs headed by the target noun, and
what is the number of the target noun for each
2.4 Feature extraction
The values for the features described above were ex-
tracted from the written component of the British
National Corpus (BNC, Burnard (2000)) using three
different pre-processors: (a) a POS tagger, (b) a full-
text chunker and (c) a dependency parser. These are
used independently to test the efficacy of the differ-
ent systems at capturing features used in the clas-
sification process, and in tandem to consolidate the
strengths of the individual methods.
With the POS extraction method, we first tagged
the BNC using an fnTBL-based tagger (Ngai and
Florian, 2001) trained over the Brown and WSJ cor-
pora and based on the Penn POS tagset. We then
lemmatised this data using a Penn tagset-customised
version of morph (Minnen et al, 2001). Finally, we
implemented a range of high-precision, low-recall
POS-based templates to extract out the features from
the processed data.
For the chunker, we ran fnTBL over the lem-
matised tagged data, training over CoNLL 2000-
style (Tjong Kim Sang and Buchholz, 2000) chunk-
converted versions of the full Brown and WSJ cor-
pora. For the NP-internal features (e.g. determin-
ers, head number), we used the noun chunks directly,
or applied POS-based templates locally within noun
chunks. For inter-chunk features (e.g. subject?verb
agreement), we looked at only adjacent chunk pairs
so as to maintain a high level of precision.
We read dependency tuples directly off the output
of RASP (Briscoe and Carroll, 2002b) in grammati-
cal relation mode.1 RASP has the advantage that re-
call is high, although precision is potentially lower
1We used the first parse in the experiments reported here.
An alternative method would be to use weighted dependency
tuples, as described in Briscoe and Carroll (2002a).
than chunking or tagging as the parser is forced into
resolving phrase attachment ambiguities and com-
mitting to a single phrase structure analysis.
After generating the different feature vectors for
each noun based on the above configurations, we fil-
tered out all nouns which did not occur at least 10
times in NP head position in the output of all three
systems. This resulted in a total of 20,530 nouns,
of which 9,031 are contained in the combined COM-
LEX and ALT-J/E lexicons. The evaluation is based
on these 9,031 nouns.
3 Feature representation
We test two basic feature representations in this re-
search: distribution-based, which simply looks at
the relative occurrence of different features in the
corpus data, and agreement-based, which analyses
the level of token-wise agreement between multiple
systems.
3.1 Distribution-based feature representation
In the distribution-based feature representation, we
take each target noun in turn and compare its amal-
gamated value for each unit feature with (a) the val-
ues for other target nouns, and (b) the value of other
unit features within that same feature cluster. That
is, we focus on the relative prominence of features
globally within the corpus and locally within each
feature cluster.
In the case of a one-dimensional feature cluster
(e.g. singular determiners), each unit feature f s for
target noun w is translated into 3 separate feature
values:
corpfreq(f s,w) =
freq(f s|w)
freq(?) (1)
wordfreq(f s,w) =
freq(f s|w)
freq(w) (2)
featfreq(f s,w) =
freq(f s|w)?
ifreq(f i|w)
? (3)
where freq(?) is the frequency of all words in the cor-
pus. That is, for each unit feature we capture the rel-
ative corpus frequency, frequency relative to the tar-
get word frequency, and frequency relative to other
features in the same feature cluster. Thus, for an n-
valued one-dimensional feature cluster, we generate
3n independent feature values.
In the case of a two-dimensional feature ma-
trix (e.g. subject-position noun number vs. verb
number agreement), each unit feature f s,t for tar-
get noun w is translated into corpfreq(f s,t,w),
wordfreq(f s,t,w) and featfreq(f s,t,w) as above,
and 2 additional feature values:
featdimfreq?(f s,t,w) =
freq(f s,t|w)?
ifreq(f i,t|w)
(4)
featdimfreq?(f s,t,w) =
freq(f s,t|w)?
j freq(f s,j |w)
(5)
which represent the featfreq values calculated along
each of the two feature dimensions. Additionally,
we calculate cumulative totals for each row and
column of the feature matrix and describe each as
for the one-dimensional features above (in the form
of 3 values). Thus, for an m ? n-valued two-
dimensional feature cluster, we generate a total of
5mn+ 3(m+ n) independent feature values.
The feature clusters produce a combined total of
1284 individual feature values.
3.2 Agreement-based feature representation
The agreement-based feature representation con-
siders the degree of token agreement between the
features extracted using the three different pre-
processors. This allows us to pinpoint the reliable di-
agnostics within the corpus data and filter out noise
generated by the individual pre-processors.
It is possible to identify the features which
are positively-correlated with a unique countability
class (e.g. occurrence of a singular noun with the
determiner a occurs only for countable nouns), and
for each to determine the token-level agreement be-
tween the different systems. The number of diagnos-
tics considered for each of the countability classes
is: 32 for countable nouns, 19 for uncountable nouns
and 1 for each of plural only and bipartite nouns.
The total number of diagnostics we test agreement
across is thus 53.
The token-level correlation for each feature f s is
calculated fourfold according to relative agreement,
the ? statistic, correlated frequency and correlated
weight. The relative agreement between systems
sys? and sys? wrt f s for target noun w is defined to
be:
agr(f s,w)(sys?, sys?) =
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
where tok (f s,w)(sys i) returns the set of token in-
stances of (f s,w). The ? statistic (Carletta, 1996)
is recast as:
?(f s,w)(sys?, sys?) =
agr(f s,w)(sys?, sys?)?
?
agr(f s,?)(sys?,sys?)
N
??
?
agr(f s,?)(sys?,sys?)
N
In this modified form, ?(f s,w) represents the diver-
gence in relative agreement wrt f s for target noun w ,
relative to the mean relative agreement wrt f s over
all words. Correlated frequency is defined to be:
cfreq(f s,w)(sys?, sys?) =
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
freq(w)
It describes the occurrence of tokens in agreement
for (f s,w) relative to the total occurrence of the tar-
get word.
The metrics are used to derive three separate fea-
ture values for each diagnostic over the three pre-
processor system pairings. We additionally calcu-
late the mean value of each metric across the system
pairings and the overall correlated weight for each
countability class C as:
cw(C ,w)(sys?, sys?) =
?
f s?C |tok(f s,w)(sys?) ? tok(f s,w)(sys?)|?
i|tok(f i,w)(sys?) ? tok(f i,w)(sys?)|
Correlated weight describes the occurrence of corre-
lated features in the given countability class relative
to other correlated features.
We test agreement: (a) for each of these diag-
nostics individually and within each countability
class (Agree(Token,?)), and (b) across the amalgam
of diagnostics for each of the countability classes
(Agree(Class,?)). For Agree(Token,?), we calculate
agr , ? and cfreq values for each of the 53 diag-
nostics across the 3 system pairings, and addition-
ally calculate the mean value for each value. We
additionally calculate the overall cw value for each
countability class. This results in a total of 640 fea-
ture values (3? 53? 3 + 53? 3 + 4). In the case
of Agree(Class,?), we average the agr , ? and cfreq
values across each countability class for each of the
three system pairings, and also calculate the mean
value in each case. We further calculate the overall
cw value for each countability class, culminating in
52 feature values (3? 4? 3 + 4? 3 + 4).
4 Classifier Set-up and Evaluation
Below, we outline the different classifiers tested
and describe the process used to generate the gold-
standard data.
4.1 Classifier architectures
We propose a variety of unsupervised and super-
vised classifier architectures for the task of learning
countability, and also a feature selection method. In
all cases, our classifiers are built using TiMBL ver-
sion 4.2 (Daelemans et al, 2002), a memory-based
classification system based on the k-nearest neigh-
bour algorithm. As a result of extensive parame-
ter optimisation, we settled on the default configu-
ration2 for TiMBL with k set to 9.3
2IB1 with weighted overlap, gain ratio-based feature
weighting and equal weighting of neighbours.
3We additionally experimented with the kernel-based
TinySVM system, but found TiMBL to be the marginally supe-
rior performer in all cases, a somewhat surprising result given
the high-dimensionality of the feature space.
Full-feature supervised classifiers
The simplest system architecture applies the su-
pervised learning paradigm to the distribution-based
feature vectors for each of the POS tagger, chun-
ker and RASP (Dist(POS,?), Dist(chunk,?) and
Dist(RASP,?), respectively). For the distribution-
based feature representation, we additionally
combine the outputs of the three pre-processors by:
(a) concatenating the individual distribution-based
feature vectors for the three systems (resulting in
a 3852-element feature vector: Dist(AllCON,?));
and (b) taking the mean over the three systems for
each distribution-based feature value (resulting in
a 1284-element feature vector: Dist(AllMEAN,?)).
The agreement-based feature representation
provides two additional system configurations:
Agree(Class,?) and Agree(Token,?) (see Section
3.2).
Orthogonal to the issue of how to generate the
feature values is the question of how to classify
a given noun according to the different countabil-
ity classes. The two basic options here are to ei-
ther have a single classifier and define multiclasses
according to all observed combinations of count-
ability classes (Dist(?,SINGLE)), or have a suite of
binary classifiers, one for each countability class
(Dist(?,SUITE)). The SINGLE classifier architec-
ture has advantages in terms of speed (a 4? speed-
up over the classifier suite) and simplicity, but runs
into problems with data sparseness for the less-
commonly attested multi-classes given that a single
noun can occur with multiple countabilities. The
SUITE classifier architecture delineates the different
countability classes more directly, but runs the risk
of a noun not being classified according to any of the
four classes.
Feature-selecting supervised classifiers
We improve the performance of the basic classi-
fiers by way of best-N filter-based feature selection.
Feature selection has been shown to improve clas-
sification accuracy over a variety of tasks (Liu and
Motoda, 1988), but in the case of memory-based
learners such as TiMBL, has the additional advan-
tage of accelerating the classification process and re-
ducing memory overhead. The computational com-
plexity of memory-based learners is proportional to
the number of features, so any reduction in the fea-
ture space leads to a proportionate reduction in com-
putational time. For tasks such as countability clas-
sification with a large number of both feature values
and test instances (particularly if we are to classify
all noun types in a given corpus), this speed-up is
vital.
Our feature selection method uses a combined
feature relevance metric to estimate the best-N fea-
tures for each countability class, and then restricts
the classifier to operate over only those N features.
Feature relevance is estimated through analysis of
the correspondence between class and feature val-
ues for a given feature, through metrics including
shared variance and information gain. These indi-
vidual metrics tend to be biased toward particular
features: information gain and gain ratio, e.g., tend
to favour features of higher cardinality (White and
Liu, 1994). In order to minimise such bias, we
generate a feature ranking for each feature selec-
tion metric (based on the relative feature relevance
scores), and simply add the absolute ranks for each
feature together. By re-ranking the features in in-
creasing order of summed rank, we can generate a
generalised feature relevance ranking. We are now
in a position to prune the feature space to a pre-
determined size, by taking the best-N features in the
feature ranking.
The feature selection metrics we combine are
those implemented in TiMBL, namely: shared vari-
ance, chi-square, information gain and gain ratio.
Unsupervised classifier
In order to derive a common baseline for the dif-
ferent systems, we built an unsupervised classifier
which, for each target noun, simply checks to see
if any diagnostic (as used in the agreement-based
feature representation) was detected for each of the
countability classes; even a single occurrence of
a diagnostic is taken to be sufficient evidence for
membership in that countability class. Elementary
system combination is achieved by voting between
the three pre-processor outputs as to whether the tar-
get noun belongs to a given countability class. That
is, the target noun is classified as belonging to a
given countability class iff at least two of the pre-
processors furnish linguistic evidence for member-
ship in that class.
4.2 Training data
Training data was generated independently for the
SINGLE and SUITE classifiers. In each case, we first
extracted all countability-annotated nouns from each
of the ALT-J/E and COMLEX lexicons which are at-
tested at least 10 times in the BNC, and composed
the training data from these pre-filtered sets. In the
case of the SINGLE classifier, we simply classified
words according to the union of all countabilities
from ALT-J/E and COMLEX, resulting in the follow-
ing dataset:
Count Uncount Plural Bipart No. Freq
1 0 0 0 4068 .685
0 1 0 0 1134 .191
0 0 1 0 35 .006
0 0 0 1 10 .002
1 1 0 0 650 .110
1 0 1 0 13 .002
0 1 1 0 13 .002
0 0 1 1 5 .001
1 1 1 0 8 .001
From this, it is evident that some class combinations
(e.g. plural only+bipartite) are highly infrequent, hint-
ing at a problem with data sparseness.
For the SUITE classifier, we generate the positive
exemplars for the countable and uncountable classes
from the intersection of the COMLEX and ALT-J/E
data for that class; negative exemplars, on the other
hand, are those not annotated as belonging to that
class in either lexicon. With the plural only and
bipartite data, COMLEX cannot be used as it does
not describe these two classes. We thus took all
members of each class listed in ALT-J/E as our pos-
itive exemplars, and all remaining nouns with non-
identical singular and plural forms as negative ex-
emplars. This resulted in the following datasets:
Class Positive data Negative data
Countable 4,342 1,476
Uncountable 1,519 5,471
Plural only 84 5,639
Bipartite 35 5,639
5 Evaluation
Evaluation of the supervised classifiers was carried
out based on 10-fold stratified cross-validation over
the relevant dataset, and results presented here are
averaged over the 10 iterations. Classifier perfor-
mance is rated according to classification accuracy
(the proportion of instances classified correctly) and
F-score (? = 1). In the case of the SINGLE classifier,
the class-wise F-score is calculated by decomposing
the multiclass labels into their components. A count-
able+uncountable instance misclassified as countable,
for example, would count as a misclassification in
terms of classification accuracy, a correct classifica-
tion in the calculation of the countable F-score, and a
misclassification in the calculation of the uncountable
F-score. Note that the SINGLE classifier is run over a
different dataset to each member of the SUITE clas-
sifier, and cross-comparison of the classification ac-
curacies is not representative of the relative system
performance (classification accuracies for the SIN-
GLE classifier are given in parentheses to reinforce
this point). Classification accuracies are thus simply
used for classifier comparison within a basic classi-
fier architecture (SINGLE or SUITE), and F-score is
Classifier Accuracy F-score
Majority class .746 .855
Unsupervised .798 .879
Dist(POS,SUITE) .928 .953
Dist(POS,SINGLE) (.850) .940
Dist(chunk,SUITE) .933 .956
Dist(chunk,SINGLE) (.853) .942
Dist(RASP,SUITE) .923 .950
Dist(RASP,SINGLE) (.847) .940
Dist(AllCON,SUITE) .939 .960
Dist(AllCON,SINGLE) (.857) .944
Dist(AllMEAN,SUITE) .937 .959
Agree(Token,SUITE) .902 .936
Agree(Class,SUITE) .911 .941
Table 1: Basic results for countable nouns
Classifier Accuracy F-score
Majority class .783 (.357)
Unsupervised .342 .391
Dist(POS,SUITE) .945 .876
Dist(POS,SINGLE) (.850) .861
Dist(chunk,SUITE) .945 .876
Dist(chunk,SINGLE) (.853) .861
Dist(RASP,SUITE) .944 .872
Dist(RASP,SINGLE) (.847) .851
Dist(AllCON,SUITE) .952 .892
Dist(AllCON,SINGLE) (.857) .873
Dist(AllMEAN,SUITE) .954 .895
Agree(Token,SUITE) .923 .825
Agree(Class,SUITE) .923 .824
Table 2: Basic results for uncountable nouns
the evaluation metric of choice for overall evalua-
tion.
We present the results for two baseline systems
for each countability class: a majority-class clas-
sifier and the unsupervised method. The Majority
class system is run over the binary data used by
the SUITE classifier for the given class, and sim-
ply classifies all instances according to the most
commonly-attested class in that dataset. Irrespective
of the majority class, we calculate the F-score based
on a positive-class classifier, i.e. a classifier which
naively classifies each instance as belonging to the
given class; in the case that the positive class is not
the majority class, the F-score is given in parenthe-
ses.
The results for the different system configurations
over the four countability classes are presented in
Tables 1?4, in which the highest classification accu-
racy and F-score values for each class are presented
in boldface. The classifier Dist(AllCON,SUITE), for
example, applies the distribution-based feature rep-
resentation in a SUITE classifier configuration (i.e.
it tests for binary membership in each countability
class), using the concatenated feature vectors from
each of the tagger, chunker and RASP.
Items of note in the results are:
Classifier Accuracy F-score
Majority class .985 (.023)
Unsupervised .411 .033
Dist(POS,SUITE) .989 .558
Dist(POS,SINGLE) (.850) .479
Dist(chunk,SUITE) .990 .568
Dist(chunk,SINGLE) (.853) .495
Dist(RASP,SUITE) .989 .415
Dist(RASP,SINGLE) (.847) .360
Dist(AllCON,SUITE) .990 .582
Dist(AllCON,SINGLE) (.857) .500
Dist(AllMEAN,SUITE) .990 .575
Agree(Token,SUITE) .988 .409
Agree(Class,SUITE) .988 .401
Table 3: Basic results for plural only nouns
Classifier Accuracy F-score
Majority class .994 (.012)
Unsupervised .931 .137
Dist(POS,SUITE) .997 .752
Dist(POS,SINGLE) (.850) .857
Dist(chunk,SUITE) .997 .704
Dist(chunk,SINGLE) (.853) .865
Dist(RASP,SUITE) .997 .700
Dist(RASP,SINGLE) (.847) .798
Dist(AllCON,SUITE) .996 .723
Dist(AllCON,SINGLE) (.857) .730
Dist(AllMEAN,SUITE) .997 .710
Agree(Token,SUITE) .997 .710
Agree(Class,SUITE) .997 .695
Table 4: Basic results for bipartite nouns
? all system configurations surpass both the
majority-class baseline and unsupervised clas-
sifier in terms of F-score
? for all other than bipartite nouns, the SUITE
classifier outperforms the SINGLE classifier in
terms of F-score
? the best of the distribution-based classifiers
was, without exception, superior to the best of
the agreement-based classifiers
? chunk-based feature extraction generally pro-
duced superior performance to POS tag-based
feature extraction, which was in turn gener-
ally better than RASP-based feature extraction;
statistically significant differences in F-score
(based on the two-tailed t-test, p < .05) were
observed for both chunking and tagging over
RASP for the plural only class, and chunking
over RASP for the countable class
? for the SUITE classifier, system combination
by either concatenation (Dist(AllCON,SUITE))
or averaging over the individual feature val-
ues (Dist(AllMEAN,SUITE)) generally led to a
statistically significant improvement over each
of the individual systems for the countable
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 100  1000
 0.1
 1
 10
 100
F-s
cor
e
Ins
tan
ces
/sec
No. Features (N)
rand-N (countable)best-N (countable)
best-N (uncountable)rand-N (uncountable)
best-N (countable)best-N (uncountable)
Figure 1: Effects of feature selection
and uncountable classes,4 but there was no
statistical difference between these two archi-
tectures for any of the 4 countability classes;
for the SINGLE classifier, system combination
(Dist(AllCON,SUITE)) did not lead to a signifi-
cant performance gain
To evaluate the effects of feature selection, we
graphed the F-score value and processing time (in
instances processed per second5) over values of
N from 25 to the full feature set. We targeted
the Dist(AllCON,SUITE) system for evaluation (3852
features), and ran it over both the countable and un-
countable classes.6 We additionally carried out ran-
dom feature selection as a baseline to compare the
feature selection results against. Note that the x-axis
(N ) and right y-axis (instances/sec) are both log-
arithmic, such that the linear right-decreasing time
curves are indicative of the direct proportionality be-
tween the number of features and processing time.
The differential in F-score for the best-N configura-
tion as compared to the full feature set is statistically
insignificant for N > 100 for countable nouns and
N > 50 for uncountable nouns. That is, feature se-
lection facilitates a relative speed-up of around 30?
without a significant drop in F-score. Comparing the
results for the best-N and rand-N features, the dif-
ference in F-score was statistically significant for all
values of N < 1000. The proposed method of fea-
ture selection thus allows us to maintain the full clas-
sification potential of the feature set while enabling
4No significant performance difference was observed for:
Dist(ChunkMEAN,SUITE) vs. Dist(All?,SUITE) for countable
nouns, and Dist(POSCON,SUITE) vs. Dist(AllCON,SUITE) for
uncountable nouns.
5As evaluated on an AMD Athlon 2100+ CPU with 3GB of
memory.
6We focus exclusively on countable and uncountable nouns
here and in the remainder of supplementary evaluation as these
are by far the most populous countability classes.
Feature COUNTABLE UNCOUNTABLE
space Acc F-score Acc F-score
All features .937 .959 .954 .895
Best-200 .934 .956 .949 .884
Binary .904? .931? .930? .833?
Corpus freq .929 .954 .952 .889
Word freq .933 .956 .954 .896
Feature freq .928 .952? .934? .869?
Table 5: Results for restricted feature sets
a speedup greater than an order of magnitude, po-
tentially making the difference in practical utility for
the proposed method.
To determine the relative impact of the com-
ponent feature values on the performance of the
distribution-based feature representation, we used
the Dist(AllMEAN,SUITE) configuration to build: (a)
a classifier using a single binary value for each
unit feature, based on simple corpus occurrence (Bi-
nary); and (b) 3 separate classifiers based on each of
the corpfreq , wordfreq and featfreq features values
only (without the 2D feature cluster totals). In each
case, the total number of feature values is 206.
The results for each of these classifiers over
countable and uncountable nouns are pre-
sented in Table 5, as compared to the basic
Dist(AllMEAN,SUITE) classifier with all 1,284
features (All features) and also the best-200 features
(Best-200). Results which differ from those for
All features to a level of statistical significance are
asterisked. The binary classifiers performed signif-
icantly worse than All features for both countable
and uncountable nouns, underlining the utility of the
distribution-based feature representation. wordfreq
is marginally superior to corpfreq as a standalone
feature representation, and both of these were on
the whole slightly below the full feature set in
performance (although no significant difference was
observed). featfreq performed slightly worse again,
significantly below the level of the full feature set.
Results for the best-200 classifier were marginally
higher than those for each of the individual feature
representations in the case of the countable class,
but marginally below the results for corpfreq and
wordfreq in the case of the uncountable class. The
differences here are not statistically significant, and
additional evaluation is required to determine the
relative success of feature selection over simply
using wordfreq values, for example.
6 Discussion
There have been at least three earlier approaches
to the automatic determination of countability:
two using semantic cues and one using cor-
pora. Bond and Vatikiotis-Bateson (2002) deter-
mine a noun?s countability preferences?as de-
fined in a 5-way classification?from its se-
mantic class in the ALT-J/E lexicon, and show
that semantics predicts countability 78% of the
time. O?Hara et al (2003) implemented a sim-
ilar approach using the much larger Cyc on-
tology and achieved 89.5% accuracy, mapping
onto the 2 classes of countable and uncount-
able. Schwartz (2002) learned noun countabilities
by looking at determiner occurrence in singular
noun chunks and was able to tag 11.7% of BNC
noun tokens as countable and 39.5% as uncountable,
achieving a noun type agreement of 88% and 44%,
respectively, with the ALT-J/E lexicon. Our results
compare favourably with each of these.
In a separate evaluation, we took the best-
performing classifier (Dist(AllCON,SUITE)) and ran
it over open data, using best-500 feature selection
(Baldwin and Bond, 2003). The output of the
classifier was evaluated relative to hand-annotated
data, and the level of agreement found to be around
92.4%, which is approximately equivalent to the
agreement between COMLEX and ALT-J/E of 93.8%.
In conclusion, we have presented a plethora of
learning techniques for deep lexical acquisition from
corpus data, and applied each to the task of classify-
ing English nouns for countability. We specifically
compared two feature representations, based on rel-
ative feature occurrence and token-level classifica-
tion, and two basic classifier architectures, using a
suite of binary classifiers and a single multi-class
classifier. We also analysed the effects of comb-
ing the output of multiple pre-processors, and pre-
sented a simple feature selection method. Overall,
the best results were obtained using a distribution-
based suite of binary classifiers combining the out-
put of multiple pre-processors.
Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. BCS-0094638 and also
the Research Collaboration between NTT Communication Sci-
ence Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University. We would like to thank
Leonoor van der Beek, Slaven Bilac, Ann Copestake, Ivan Sag
and the three anonymous reviewers for their valuable input on
this research, and John Carroll for providing access to RASP.
References
Timothy Baldwin and Francis Bond. 2003. Learning the count-
ability of English nouns from corpus data. In Proc. of the
41st Annual Meeting of the ACL, Sapporo, Japan. (to ap-
pear).
Francis Bond and Caitlin Vatikiotis-Bateson. 2002. Using an
ontology to determine English countability. In Proc. of the
19th International Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan.
Ted Briscoe and John Carroll. 2002a. High precision extraction
of grammatical relations. In Proc. of the 19th International
Conference on Computational Linguistics (COLING 2002),
pages 134?140, Taipei, Taiwan.
Ted Briscoe and John Carroll. 2002b. Robust accurate sta-
tistical annotation of general text. In Proc. of the 3rd In-
ternational Conference on Language Resources and Evalu-
ation (LREC 2002), pages 1499?1504, Las Palmas, Canary
Islands.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2002. TiMBL: Tilburg memory based
learner, version 4.2, reference guide. ILK technical report
02-01.
Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.
COMLEX Syntax Reference Manual. Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E?. In Proc. of the Third
Machine Translation Summit (MT Summit III), pages 101?
106, Washington DC, USA.
Huan Liu and Hiroshi Motoda. 1988. Feature Extraction, Con-
struction and Selection: A Data Mining Perspective. Kluwer
Academic Publishers.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?23.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-
der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, Fritz
Lehmann, Matt Smith, David Baxter, Jon Curtis, and Peter
Wagner. 2003. Inducing criteria for mass noun lexical map-
pings using the Cyc KB and its extension to WordNet. In
Proc. of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, the Netherlands.
Lane O.B. Schwartz. 2002. Corpus-based acquisition of head
noun countability features. Master?s thesis, Cambridge Uni-
versity, Cambridge, UK.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proc.
of the 4th Conference on Computational Natural Language
Learning (CoNLL-2000), Lisbon, Portugal.
Allan P. White and Wei Zhong Liu. 1994. Bias in information-
based measures in decision tree induction. Machine Learn-
ing, 15(3):321?9.
Noun-Noun Compound Machine Translation:
A Feasibility Study on Shallow Processing
Takaaki Tanaka
Communication Science Laboratories
Nippon Telephone and Telegraph Corporation
Kyoto, Japan
takaaki@cslab.kecl.ntt.co.jp
Timothy Baldwin
CSLI
Stanford University
Stanford, CA 94305 USA
tbaldwin@csli.stanford.edu
Abstract
The translation of compound nouns is a ma-
jor issue in machine translation due to their
frequency of occurrence and high produc-
tivity. Various shallow methods have been
proposed to translate compound nouns, no-
table amongst which are memory-based
machine translation and word-to-word com-
positional machine translation. This paper
describes the results of a feasibility study
on the ability of these methods to trans-
late Japanese and English noun-noun com-
pounds.
1 Introduction
Multiword expressions are problematic in machine
translation (MT) due to the idiomaticity and overgen-
eration problems (Sag et al, 2002). Idiomaticity is
the problem of compositional semantic unpredictabil-
ity and/or syntactic markedness, as seen in expres-
sions such as kick the bucket (= die   ) and by and large,
respectively. Overgeneration occurs as a result of a
system failing to capture idiosyncratic lexical affini-
ties between words, such as the blocking of seemingly
equivalent word combinations (e.g. many thanks vs.
*several thanks). In this paper, we target the particu-
lar task of the Japanese  English machine translation
of noun-noun compounds to outline the various tech-
niques that have been proposed to tackle idiomaticity
and overgeneration, and carry out detailed analysis of
their viability over naturally-occurring data.
Noun-noun (NN) compounds (e.g. web server, car
park) characteristically occur with high frequency and
high lexical and semantic variability. A summary ex-
amination of the 90m-word written component of the
British National Corpus (BNC, Burnard (2000)) un-
earthed over 400,000 NN compound types, with a
combined token frequency of 1.3m;1 that is, over 1%
of words in the BNC are NN compounds. More-
over, if we plot the relative token coverage of the
most frequently-occurring NN compound types, we
find that the low-frequency types account for a sig-
1Results based on the method described in  3.1.
nificant proportion of the type count (see Figure 12).
To achieve 50% token coverage, e.g., we require cov-
erage of the top 5% most-frequent NN compounds,
amounting to roughly 70,000 types with a minimum
token frequency of 10. NN compounds are especially
prevalent in technical domains, often with idiosyn-
cratic semantics: Tanaka and Matsuo (1999) found
that NN compounds accounted for almost 20% of en-
tries in a Japanese-English financial terminological
dictionary.
Various claims have been made about the level of
processing complexity required to translate NN com-
pounds, and proposed translation methods range over
a broad spectrum of processing complexity. There is
a clear division between the proposed methods based
on whether they attempt to interpret the semantics of
the NN compound (i.e. use deep processing), or sim-
ply use the source language word forms to carry out
the translation task (i.e. use shallow processing). It is
not hard to find examples of semantic mismatch in NN
compounds to motivate deep translation methods: the
Japanese 	
 idobata  kaigi ?(lit.) well-side
meeting?,3 e.g., translates most naturally into English
as ?idle gossip?, which a shallow method would be
hard put to predict. Our interest is in the relative oc-
currence of such NN compounds and their impact on
the performance of shallow translation methods. In
particular, we seek to determine what proportion of
NN compounds shallow translation translation meth-
ods can reasonably translate and answer the question:
do shallow methods perform well enough to preclude
the need for deep processing? The answer to this
question takes the form of an estimation of the upper
bound on translation performance for shallow transla-
tion methods.
In order to answer this question, we have selected
the language pair of English and Japanese, due to
the high linguistic disparity between the two lan-
guages. We consider the tasks of both English-to-
Japanese (EJ) and Japanese-to-English (JE) NN com-
pound translation over fixed datasets of NN com-
pounds, and apply representative shallow MT meth-
ods to the data.
2The graph for Japanese NN compounds based on the
Mainichi Corpus is almost identical.
3With all Japanese NN compound examples, we explicitly
segment the compound into its component nouns through the use
of the ?  ? symbol.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
T
ok
en
 c
ov
er
ag
e
Type coverage
Figure 1: Type vs. token coverage (English)
While stating that English and Japanese are highly
linguistically differentiated, we recognise that there
are strong syntactic parallels between the two lan-
guages with respect to the compound noun construc-
tion. At the same time, there are large volumes of sub-
tle lexical and expressional divergences between the
two languages, as evidenced between   
jiteNsha  seNshu ?(lit.) bicycle athelete? and its trans-
lation competitive cyclist. In this sense, we claim that
English and Japanese are representative of the inher-
ent difficulty of NN compound translation.
The remainder of this paper is structured as follows.
In  2, we outline the basic MT strategies that exist
for translating NN compounds, and in  3 we describe
the method by which we evaluate each method. We
then present the results in  4, and analyse the results
and suggest an extension to the basic method in  5.
Finally, we conclude in  6
2 Methods for translating NN compounds
Two basic paradigms exist for translating NN com-
pounds: memory-based machine translation and dy-
namic machine translation. Below, we discuss these
two paradigms in turn and representative instantia-
tions of each.
2.1 Memory-based machine translation
Memory-based machine translation (MBMT) is a
simple and commonly-used method for translating
NN compounds, whereby translation pairs are stored
in a static translation database indexed by their
source language strings. MBMT has the ability to
produce consistent, high-quality translations (condi-
tioned on the quality of the original bilingual dictio-
nary) and is therefore suited to translating compounds
in closed domains. Its most obvious drawback is that
the method can translate only those source language
strings contained in the translation database.
There are a number of ways to populate the transla-
tion database used in MBMT, the easiest of which is
to take translation pairs directly from a bilingual dic-
tionary (dictionary-driven MBMT or MBMTDICT).
MBMTDICT offers an extremist solution to the id-
iomaticity problem, in treating all NN compounds as
being fully lexicalised. Overgeneration is not an issue,
as all translations are manually determined.
As an alternative to a precompiled bilingual dic-
tionary, translation pairs can be extracted from a
parallel corpus (Fung, 1995; Smadja et al, 1996;
Ohmori and Higashida, 1999), that is a bilingual doc-
ument set that is translation-equivalent at the sentence
or paragraph level; we term this MT configuration
alignment-driven MBMT (or MBMTALIGN). While
this method alleviates the problem of limited scalabil-
ity, it relies on the existence of a parallel corpus in
the desired domain, which is often an unreasonable
requirement.
Whereas a parallel corpus assumes translation
equivalence, a comparable corpus is simply a
crosslingual pairing of corpora from the same domain
(Fung and McKeown, 1997; Rapp, 1999; Tanaka and
Matsuo, 1999; Tanaka, 2002). It is possible to extract
translation pairs from a comparable corpus by way of
the following process (Cao and Li, 2002):
1. extract NN compounds from the source language
corpus by searching for NN bigrams (e.g. 	
 

kikai  hoNyaku ?machine translation?)
2. compositionally generate translation candidates
for each NN compound by accessing transla-
tions for each component word and slotting these
into translation templates; example JE transla-
tion templates for source Japanese string [N 
N  ]J are [N  N  ]E and [N  of N  ]E, where the nu-
meric subscripts indicate word coindexation be-
tween Japanese and English (resulting in, e.g.,
machine translation and translation of machine)
3. use empirical evidence from the target language
corpus to select the most plausible translation
candidate
We term this process word-to-word compositional
MBMT (or MBMTCOMP). While the coverage of
MBMTCOMP is potentially higher than MBMTALIGN
due to the greater accessibility of corpus data, it is
limited to some degree by the coverage of the simplex
translation dictionary used in Step 2 of the translation
process. That is, only those NN compounds whose
component nouns occur in the bilingual dictionary can
be translated.
Note that both MBMTALIGN and MBMTCOMP lead
to a static translation database. MBMTCOMP is also
subject to overgeneration as a result of dynamically
generating translation candidates.
2.2 Dynamic machine translation
Dynamic machine translation (DMT) is geared to-
wards translating arbitrary NN compounds. In this pa-
per, we consider two methods of dynamic translation:
word-to-word compositional DMT and interpretation-
driven DMT.
Word-to-word compositional DMT (or
DMTCOMP) differs from MBMTCOMP only in
that the source NN compounds are fed directly into
the system rather than extracted out of a source
language corpus. That is, it applies Steps 2 and 3 of
the method for MBMTCOMP to an arbitrary source
language string.
Interpretation-driven DMT (or DMTINTERP) of-
fers the means to deal with NN compounds where
strict word-to-word alignment does not hold. It gen-
erally does this in two stages:
1. use semantics and/or pragmatics to carry out
deep analysis of the source NN compound,
and map it into some intermediate (i.e. inter-
lingual) semantic representation (Copestake and
Lascarides, 1997; Barker and Szpakowicz, 1998;
Rosario and Hearst, 2001)
2. generate the translation directly from the seman-
tic representation
DMTINTERP removes any direct source/target lan-
guage interdependence, and hence solves the prob-
lem of overgeneration due to crosslingual bias. At the
same time, it is forced into tackling idiomaticity head-
on, by way of interpreting each individual NN com-
pound. As for DMTCOMP, DMTINTERP suffers from
undergeneration.
With DMTINTERP, context must often be called
upon in interpreting NN compounds (e.g. apple
juice seat (Levi, 1978; Bauer, 1979)), and minimal
pairs with sharply-differentiated semantics such as
colour/group photograph illustrate the fine-grained
distinctions that must be made. It is interesting to note
that, while these examples are difficult to interpret, in
an MT context, they can all be translated word-to-
word compositionally into Japanese. That is, apple
juice seat translates most naturally as  
	

  appurujuusu  no  seki ?apple-juice seat?,4
which retains the same scope for interpretation as
its English counterpart; similarly, colour photograph
translates trivially as   karaa  shashiN
?colour photograph? and group photograph as ffA Statistical Approach to the Semantics of Verb-Particles
Colin Bannard
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
c.j.bannard@ed.ac.uk
Timothy Baldwin
CSLI
Stanford University
210 Panama Street
Stanford CA 94305, USA
tbaldwin@csli.stanford.edu
Alex Lascarides
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
alex@inf.ed.ac.uk
Abstract
This paper describes a distributional ap-
proach to the semantics of verb-particle
constructions (e.g. put up, make off ). We
report first on a framework for implement-
ing and evaluating such models. We then go
on to report on the implementation of some
techniques for using statistical models ac-
quired from corpus data to infer the mean-
ing of verb-particle constructions.
1 Introduction
The semantic representation of multiword expres-
sions (MWEs) has recently become the target of re-
newed attention, notably in the area of hand-written
grammar development (Sag et al, 2002; Villavicen-
cio and Copestake, 2002). Such items cause con-
siderable problems for any semantically-grounded
NLP application (including applications where se-
mantic information is implicit, such as information
retrieval) because their meaning is often not sim-
ply a function of the meaning of the constituent
parts. However, corpus-based or empirical NLP has
shown limited interest in the problem. While there
has been some work on statistical approaches to
the semantics of compositional compound nominals
(e.g. Lauer (1995), Barker and Szpakowicz (1998),
Rosario and Hearst (2001)), the more idiosyncratic
items have been largely ignored beyond attempts at
identification (Melamed, 1997; Lin, 1999; Schone and
Jurafsky, 2001). And yet the identification of non-
compositional phrases, while valuable in itself, would
by no means be the end of the matter. The unique
challenge posed by MWEs for empirical NLP is pre-
cisely that they do not fall cleanly into the binary
classes of compositional and non-compositional ex-
pressions, but populate a continuum between the two
extremes.
Part of the reason for the lack of interest by com-
putational linguists in the semantics of MWEs is that
there is no established gold standard data from which
to construct or evaluate models. Evaluation to date has
tended to be fairly ad hoc. Another key problem is the
lack of any firm empirical foundations for the notion
of compositionality. Given this background, this pa-
per has two aims. The first is to put the treatment of
non-compositionality in corpus-based NLP on a firm
empirical footing. As such it describes the develop-
ment of a resource for implementing and evaluating
statistical models of MWE meaning, based on non-
expert human judgements. The second is to demon-
strate the usefulness of such approaches by imple-
menting and evaluating a handful of approaches.
The remainder of this paper is structured as follows.
We outline the linguistic foundations of this research
in Section 2 before describing the process of resource
building in Section 3. Section 4 summarises previ-
ous work on the subject and Section 5 details our pro-
posed models of compositionality. Section 6 lays out
the evaluation of those models over the gold standard
data, and we conclude the paper in Section 7.
2 Verb Particle Constructions
We selected the English verb-particle construction as
our test case MWE in this paper. Verb-particle con-
structions (hereafter referred to as VPCs) consist of a
head verb and one or more obligatory particles, in the
form of intransitive prepositions (e.g. hand in), ad-
jectives (e.g. cut short) or verbs (e.g. let go) . Here,
we focus exclusively on prepositional particles due to
their high productivity and variable compositionality.
Examples of prepositional VPCs are put up, finish up,
gun down and make out as used in the following sen-
tences:
(1) Peter put the picture up
(2) Susan finished up her paper
(3) Philip gunned down the intruder
(4) Barbara and Simon made out
VPCs cause significant problems for NLP sys-
tems. Semantically, they often cannot be understood
through the simple composition of their independent
parts. Compare, for example, sentences (1) and (4).
In (1), the meaning seems to be that Peter put the pic-
ture somewhere and that as a consequence the picture
was up. That is, the verb and the particle make in-
dependent contributions to the sentence. A (partial)
Parsons-style semantic analysis of this might be as
follows:
put(e1, x, y) ? peter(x) ? picture(y) ? up(e1, y)
Sentence (4), on the other hand requires a rather dif-
ferent analysis. Neither Barbara nor Simon can be
said to have made or to be out. The semantic anal-
ysis we would want then might be something like the
following:
make out(e1, e2) ? and(e2, x, y) ? barbara(x) ? simon(y)
How are we to identify whether the first or the second
kind of semantic representation is appropriate for any
given item? If we look at the other two sentences we
can see that the problem is even more complicated.
In (2) it is the case that the paper is finished, but it
would be hard to claim that anything or anyone is up.
Only the verb then seems to be contributing its sim-
plex meaning, and the semantic analysis is (roughly):
finish(e1, x, y) ? susan(x) ? paper(y)
In (3), by contrast, it is the particle that contributes its
simplex meaning and not the verb. As a consequence
of Philip?s action the intruder is down, but since there
is no simplex verb to gun, we would not say that any-
one gunned or was gunned The semantic analysis is
consequently as follows:
gun down(e1, x, y) ? philip(x) ? intruder(y) ? down(e1, y)
In the linguistic literature, the semantics of VPCs
is frequently viewed in rather more complicated terms
than we are suggesting here, with particles often seen
as making significant construction-specific contribu-
tions in terms of aspect (e.g. Brinton (1985)). How-
ever no such existing linguistic account is completely
robust, and for practical NLP purposes we are forced
to adopt a rather straightforward definition of compo-
sitionality as meaning that the overall semantics of the
MWE can be composed from the simplex semantics
of its parts, as described (explicitly or implicitly) in a
finite lexicon.
3 Building the Resource
Rather than attempting to model compositionality by
anchoring word semantics to a given lexicon, our ap-
proach in this work is to defer to an empirical refer-
ence based on human judgements. We define MWE
compositionality to be an entailment relationship be-
tween the whole and its various parts, and solicit en-
tailment judgements based on a handful of example
sentences.
Entailment is conventionally defined for logical
propositions, where a proposition P entails a proposi-
tion Q iff there is no conceivable state of affairs that
could make P true and Q false. This can be gener-
alised to refer to the relationship between two verbs
V1 and V2 that holds when the sentence Someone V1s
entails the sentence Someone V2s (see, e.g., the treat-
ment of verbs in the WordNet hierarchy (Miller et al,
1990)). According to this generalisation we would
then say that the verb run entails the verb move be-
cause the sentence He runs entails the sentence He
moves. The same idea can be generalised to the rela-
tionship between simplex verbs (e.g. walk) and VPCs
(e.g. walk off ). For example, sentence (1) can be said
to entail that Peter put the picture somewhere and so
we can say that put up entails put. The same might be
said of finish up and finish in (2). However, (3) and
(4) produce a rather different result. (4) does not en-
tail that Simon and Barbara made something, and (3)
cannot entail that Philip gunned the intruder because
there is no simplex verb to gun. This is a very useful
way of testing whether the simplex verb contributes to
the meaning of the construction.
We can approach the relationship between VPCs
and particles in this same way. For (1), while it is
not true that Peter was up, it is true that The picture
was up. We can therefore say that the VPC entails the
particle here. For (2), it is not true that either Susan
or the paper were up, and the VPC therefore does not
entail the particle. In the case of (3), while it is not
true that Philip was down it is true that The intruder
was down, and the VPC therefore entails the particle.
Finally, for (4), it is not true that Barbara and Simon
were out, and the VPC therefore does not entail the
particle.
We make the assumption that these relationships
between the component words of the VPC and the
whole are intuitive to non-experts, and aim to use
their ?entailment? judgements accordingly. This
use of entailment in exploring the semantics of
verb and preposition combinations was first pro-
posed by Hawkins (2000), and applied to VPCs by
Lohse et al (in preparation).
3.1 Experimental Materials
In an attempt to normalise the annotators? entailment
judgements, we decided upon an experimental setup
where the subject is, for each VPC type, presented
with a fixed selection of sentential contexts for that
VPC. So as to avoid introducing any bias into the
experiment through artificially-generated sentences,
we chose to extract the sentences from naturally-
occurring text, namely the written component of the
British National Corpus (BNC, Burnard (2000)).
Extraction of the VPCs was based on the method
of Baldwin and Villavicencio (2002). First, we used a
POS tagger and chunker (both built using fnTBL 1.0
(Ngai and Florian, 2001)) to (re)tag the BNC. This al-
lowed us to extract VPC tokens through use of: (a)
the particle POS in the POS tagged output, for each
instance of which we simply then look for the right-
most verb within a fixed window to the left of the
particle, and (b) the particle chunk tag in the chunker
output, where we similarly locate the rightmost verb
associated with each particle chunk occurrence. Fi-
nally, we ran a stochastic chunk-based grammar over
the chunker output to extend extraction coverage to
include mistagged particles and also more reliably de-
termine the valence of the VPC. The token output of
these three methods was amalgamated by weighted
voting.
The above method extracted 461 distinct VPC types
occurring at least 50 times, attested in a total of
110,199 sentences. After partitioning the sentence
data by type, we randomly selected 5 sentences for
each VPC type. We then randomly selected 40 VPC
types (with 5 sentences each) to use in the entailment
experiment. That is, all results described in this paper
are over 40 VPC types.
3.2 Participants
28 participants took part in our initial experiment.
They were all native speakers of English, recruited
by advertisements posted to newsgroups and mailing
lists.
3.3 Experimental Method
Each participant was presented with 40 sets of 5 sen-
tences, where each of the five sentences contained a
particular VPC. The VPC in question was indicated at
the top of the screen, and they were asked two ques-
tions: (1) whether the VPC implies the verb, and (2)
whether the VPC implies the particle. If the VPC
was round up, e.g., the subject would be asked ?Does
round up imply round?? and ?Does round up im-
ply up??, respectively. They were given the option of
three responses: ?Yes?, ?No? or ?Don?t Know?. Once
they had indicated their answer and pressed next, they
advanced to the next VPC and set of 5 sentences. They
were unable to move on until a choice had been indi-
cated.
As with any corpus-based approach to lexical se-
mantics, our study of VPCs is hampered by poly-
semy, e.g. carry outTRANSin the execute and transport
out (from a location) senses.1 Rather than intervene
to customise example sentences to a prescribed sense,
we accepted whatever composition of senses random
sampling produced. Participants were advised that if
they felt more that one meaning was present in the set
of five sentences, they should base their decision on
the sense that had the greatest number of occurrences
in the set.
1The effects of polysemy were compounded by not having any
reliable method for determining valence. We consider that sim-
ply partitioning VPC items into intransitive and transitive usages
would reduce polysemy significantly.
VPC Component word Yes No Don?t Know
get 19 5 2get down down 14 10 2
move 14 12 0
move off
off 19 7 0
throw 20 6 0throw out
out 15 10 1
pay 11 12 3pay off
off 16 8 2
lift 25 1 0lift out
out 26 0 0
roll 13 9 4
roll back back 14 12 0
dig 21 5 0dig up
up 18 7 1
lie 24 2 0lie down down 25 1 0
wear 6 19 1
wear on
on 3 22 1
fall 23 3 0fall off
off 25 1 0
move 22 4 0
move out
out 26 0 0
hand 15 9 2hand out
out 19 7 0
seek 13 13 0
seek out
out 15 11 0
sell 14 12 0sell off
off 16 9 1
trail 8 18 0trail off
off 10 16 0
stay 20 5 1stay up
up 21 5 0
go 18 7 1go down down 22 3 1
hang 22 4 0hang out
out 25 1 0
get 20 6 0get back back 19 6 1
throw 15 9 2throw in in 13 12 1
put 8 17 1put off
off 5 19 2
shake 12 14 0shake off
off 15 11 0
step 25 1 0step off
off 26 0 0
give 12 12 2give off
off 21 5 0
carry 7 17 2carry away
away 6 18 2
throw 18 7 1throw back back 21 4 1
pull 13 10 3pull off
off 13 6 7
carry 0 25 1carry out
out 0 25 1
brighten 9 16 1brighten up
up 16 10 0
map 9 17 0
map out
out 10 16 0
slow 11 14 1slow down down 19 7 0
sort 6 19 1
sort out
out 11 15 0
bite 15 10 1bite off
off 16 8 2
add 12 14 0add up
up 19 6 1
mark 13 13 0
mark out
out 14 12 0
lay 11 14 1lay out
out 10 14 2
catch 6 20 0catch up
up 7 18 1
run 12 13 1
run up
up 13 10 3
stick 20 6 0
stick out
out 15 11 0
play 10 15 1play down down 6 20 0
Table 1: Participant entailment judgements
Overall Verbs only Particles only
Agreement .677 .703 .650
Kappa (?) .376 .372 .352
% Yes .575 .655 .495
% No .393 .319 .467
% Don?t Know .032 .026 .038
Table 2: Summary of judgements for all VPCs
The experiment was conducted remotely over the
Web, using the experimental software package Web-
Exp (Corley et al, 2000). Experimental sessions
lasted approximately 20 minutes and were self-paced.
The order in which the forty sets of sentences were
presented was randomised by the software.
3.4 Annotator agreement
We performed a pairwise analysis of the agreement
between our 28 participants. The overall mean agree-
ment was .655, with a kappa (?) score (Carletta, 1996)
of .329. An initial analysis showed that two partic-
ipants strongly disagreed with the other, achieving a
mean pairwise ? score of less than .1. We decided
therefore to remove these from the set before pro-
ceeding. The overall results for the remaining 26 par-
ticipants can be seen in Table 2. The ? score over
these 26 participants (.376) is classed as fair (0.2?
0.4) and approaching moderate (0.4?0.6) according to
Altman (1991).
As mentioned above, a major problem with lexi-
cal semantic studies is that items tend to occur with
more than one meaning. In order to test the effects of
polysemy in the example sentences on inter-annotator
agreement, we analysed the agreement obtained over
those VPCs which have only one meaning accord-
ing to WordNet (Miller et al, 1990). There was a
total of 14 such items, giving 28 entailment judge-
ments (one for the verb and one for the particle in
each item). For these items, mean agreement and the
? score were .700 and .387, respectively. These are
only very slightly higher than the overall scores, sug-
gesting, although by no means proving, that polysemy
was not a significant confounding factor.
The results for each VPC type can be seen in Ta-
ble 1, broken down into the verb and particle entail-
ment judgements and based on the 26 participants. We
took two approaches to deriving a single judgement
for each test. First, we took the majority judgement
to be the correct one (majority). Second, we identi-
fied the participant who achieved the highest overall ?
score with the other participants, and took their judge-
ments to be correct (centroid annotator). Both sets
of results will be referred to in evaluating our models.
It is interesting to look at the way in which the re-
sults for component entailment are distributed across
the VPCs. According to the majority view, there are
21 fully-compositional items, 10 items where neither
the verb nor the particle is entailed, 9 items where only
the particle is entailed, and 0 items where the verb
alone is entailed. According to the judgements of the
centroid annotator, there are 10 fully-compositional
items, 12 items where neither the verb nor the parti-
cle is entailed, 15 where only the verb is entailed, and
3 where only the particle is entailed. It is surprising
to notice that the majority view holds there to be no
items in which the verb alone is contributing mean-
ing. It could be the case that items where only the
verb contributes meaning are rare, or that they are not
represented in our dataset. Another possible, and to
our minds more likely, conclusion is that the contribu-
tion of the head verb strongly affects the way in which
participants view the whole item. Thus if a verb is
considered to be contributing simplex semantics, the
participant is likely to assume that the VPC is com-
pletely compositional, and conversely if a verb is con-
sidered to not be contributing simplex semantics, the
participant is more likely to assume the VPC to be
non-compositional.
4 Related Work
We devote this section to a description of statistical
NLP work on the non-compositionality of MWEs.
Perhaps the singularly most influential work on
MWE non-compositionality is that of Lin (1999). We
describe Lin?s method in some detail here as it forms
the basis of one of the methods tested in this re-
search. Lin?s method is based on the premise that non-
compositional items have markedly different distribu-
tional characteristics to expressions derived through
synonym substitution over the original word compo-
sition. Lin took his multiword items from a colloca-
tion database (Lin, 1998b). For each collocation, he
substituted each of the component words with a word
with a similar meaning. The list of similar meanings
was obtained by taking the 10 most similar words ac-
cording to a corpus-derived thesaurus, the construc-
tion of which is described in Lin (1998a). The mutual
information value was then found for each item pro-
duced by this substitution by taking a collocation to
consist of three events: the type of dependency rela-
tionship, the head lexical item, and the modifier. A
phrase ? was then said to be non-compositional iff
there exists no phrase ? where: (a) ? can be pro-
duced by substitution of the components of ? as de-
scribed above, and (b) there is an overlap between
the 95% confidence interval of the mutual informa-
tion values of ? and ?. These judgements were eval-
uated by comparison with a dictionary of idioms. If
an item was in the dictionary then it was said to be
non-compositional. Scores of 15.7% for precision and
13.7% for recall are reported.
There are, to our minds, significant problems with
the underlying assumptions of Lin?s method. The
theoretical basis of the technique is that composi-
tional items should have a similar distribution to items
formed by replacing components words with seman-
tically similar ones. The idea presumably is that if an
item is the result of the free combination of words, or
a fully productive lexical rule, then word-substituted
variants should be distributed similarly. This seems a
reasonable basis for modelling productivity but not
compositionality, as Lin claims. There are many ex-
amples in natural language of phrases that are not at
all productive but are still compositional (e.g. frying
pan); we term the process by which these expressions
arise institutionalisation . Similar work to Lin?s has
been done in the area of collocation extraction (e.g.
Pearce (2002)), to pick up on this alternate concept of
institutionalisation.
Schone and Jurafsky (2001) employed Latent Se-
mantic Analysis (LSA, Deerwester et al (1990)) in an
effort to improve on existing techniques for extract-
ing MWEs from corpora. One property they try and
pick up on in doing so is non-compositionality. They
measure the cosine between the vector representation
for the candidate MWE and a weighted vector sum
of its component words, suggesting that a small co-
sine would indicate compositionality. They evaluate
this by comparing the extracted items with those listed
in existing dictionaries, and report that it offers no
improvement in extracting MWEs over existing tech-
niques. The assumption that non-compositionality is
requisite for the presence of a MWE in a dictionary,
while interesting, is not well-founded, and hence it
does not seem to us that the poor results reflect a fail-
ure of the LSA approach in measuring compositional-
ity.
Bannard (2002) used a combination of hand-built
thesauri and corpus statistics to explore the compo-
sitionality of VPCs. The task was to predict whether
the verb and/or the particle were contributing meaning
to a given item, using statistical analysis of a set of
VPCs extracted from the Wall Street Journal section
of the Penn Treebank (Marcus et al, 1993). Two tech-
niques were used. The first of these loosely followed
Lin in measuring the extent to which the component
verb or particle of any VPC could be replaced with
items of a similar semantic class to form a corpus-
attested VPC; WordNet (Miller et al, 1990) was used
as the source for verb substitution candidates, and a
hand-build semantic taxonomy for particles. The sec-
ond technique explored the semantic similarity of a
VPC to its component verb by comparing their subcat-
egorisation preferences, assuming that semantic sim-
ilarity between a VPC and its component verb indi-
cates compositionality. Poor results were put down
to data-sparseness, and the lexical resources not being
well suited to the task. We use a larger corpus and an
automatically-derived thesaurus for the research de-
scribed in this paper, with the hope of overcoming
these problems.
McCarthy et al (2003) carry out research close in
spirit to that described here, in taking VPC tokens
automatically extracted from the BNC and using an
automatically acquired thesaurus to classify their rel-
ative compositionality. One significant divergence
from our research is that they consider composition-
ality to be an indivisible property of the overall VPC,
and not the individual parts. Gold-standard data was
generated by asking human annotators to describe the
compositionality of a given VPC according to a 11-
point scale, based upon which the VPCs were ranked
in order of compositionality. Similarly to this re-
search, McCarthy et al in part used the similarity
measure of Lin (1998a) to model compositionality,
e.g., in taking the top N similar words to each VPC
and looking at overlap with the top N similar words to
the head verb. They also examine the use of statistical
tests such as mutual information in modelling com-
positionality, and find the similarity-based methods to
correlate more highly with the human judgements.
Baldwin et al (2003) use LSA as a technique for
analysing the compositionality (or decomposabil-
ity) of a given MWE. LSA is suggested to be
a construction-inspecific test for compositionality,
which is illustrated by testing its effectivity over both
English noun-noun compounds and VPCs. Baldwin
et al used LSA to calculate the distributional similar-
ity between an MWE and its head word, and demon-
strate a correlation between similarity and composi-
tionality (modelled in terms of endocentricity) by way
of items with higher similarity being more composi-
tional. They do not go as far as to classify MWEs as
being compositional or non-compositional, however.
5 Building a classifier
Having created our gold-standard data, we imple-
mented some statistical techniques for automatic anal-
ysis. In this, we use the VPC tokens with sentential
contexts extracted from the BNC as reported in Sec-
tion 3, i.e. a superset of the data used to annotate the
VPCs. We mapped the gold-standard data onto four
binary (yes/no) classification tasks over VPC items:
TASK 1: The item is completely compositional.
TASK 2: The item includes at least one item that is
compositional.
TASK 3: The verb in the item contributes its simplex
meaning.
TASK 4: The particle in the item contributes its sim-
plex meaning.
Note the partial conditional chaining between these
tests, e.g. an item for which the verb and particle con-
tribute their simplex meaning (i.e. positive exemplars
for TASKS 3 and 4) is completely compositional (i.e.
a positive exemplar for TASK 1).
The following sections describe four methods for
modelling VPC compositionality, each of which is
tested over the 4 individual compositionality classi-
fication tasks. The results for each method are given
in Table 4, in which the baseline for each task is the
score obtained when we assign the most frequent label
to all items. Each method is evaluated in terms of pre-
cision (Prec), Recall (Rec) and F-score (? = 1, FB1),
and all values which exceed the baseline are indicated
in boldface.
5.1 Method 1
We decided to gain a sense of the start-of-the-art on
the task by reimplementing the technique described
in Lin (1999) over VPCs. In our implementation we
replaced Lin?s collocations with our VPCs, treating
the relationship between a verb and a particle as a
kind of grammatical relation. In addition to the binary
compositional/non-compositional judgement that Lin
offers (which seems to be equivalent to TASK 1), we
tested the method over the other three tasks. Ac-
knowledging, as we must, that items can be partially
compositional (i.e. have one component item con-
tributing a conventional meaning), it would seem to
be the case, according to the assumptions made by
the technique, that the substitutability of each item
will give us some insight into its semantic contribu-
tion. The thesaurus used by Lin has been generously
made available online. However this is not adequate
for our purposes since it includes only verbs, nouns
and adjectives/adverbs. We therefore replicated the
approach described in Lin (1998a) to build the the-
saurus, using BNC data and including prepositions.
5.2 Method 2
Method 2 is very similar to Method 1, except that
instead of using a thesaurus based on Lin?s method,
we took a knowledge-free approach to obtaining syn-
onyms. Our technique is very similar to the approach
taken to building a ?context space? by Schu?tze (1998).
We measured the frequency of co-occurrence of our
target words (the 20,000 most frequent words, includ-
ing all of our VPCs2 and all of their component verbs
and prepositions) with a set of 1000 ?content-bearing?
words (we used the 51st to the 1050th most frequent
words, the 50 most frequent being taken to have ex-
tremely low infomation content). A target word was
said to co-occur with a content word if that content
word occurred within a window of 5 words to either
side of it. These co-occurrence figures were stored as
feature vectors. In order to overcome data sparseness,
we used techniques borrowed from Latent Seman-
tic Indexing (LSI, Deerwester et al (1990)). LSI is
an information retrieval technique based on Singular
Value Decomposition (SVD), and works by project-
ing a term-document matrix onto a lower-dimensional
subspace, in which relationships might more easily be
2Concatenated into a single-word item
Majority Centroid 60% Agreement
1.29 4.09 0.48All (p =.255) (p=.043) (p=.488)
2.19 0.01 5.56Monosemous (p=.137) (p=.924) (p =.018)
Table 3: Logistic regression for Method 4
observed between terms which are related but do not
co-occur. We used this technique to reduce the feature
space for our target words from 1000 to 100, allow-
ing relations to be discovered between target words
even if there is not direct match between their context
words. We used the various tools in the GTP software
package, created at the University of Tennessee3 to
build these matrices from the co-occurrence data, and
to perform SVD analysis.
We calculated the similarity between two terms by
finding the cosine of the angle between their vec-
tors. We performed a pairwise comparison between
all verbs and all particles. For each term we then
sorted all of the other items of the same part-of-speech
in descending order of similarity, which gave us the
thesaurus for use in substitution. As with the Lin
method, we performed substitutions by taking the 10
most similar items for the head verb and particle of
each VPC.
5.3 Method 3
We noted in Section 4 that a significant problem
with the substitution approach is that it is sensitive to
institutionalisation rather than non-compositionality.
Method 3 attempts to adapt substitution to more ac-
curately reflect non-compositionality by removing the
assumption that an item formed by substitution should
have the same distributional characteristics as the
original item. Rather than basing the composition-
ality judgement on the relative mutual information
scores of the original items and the items resulting
from substitution, we instead base it on the corpus-
based semantic similarity between the original ex-
pression and word-substituted derivative expressions.
The same method of substitution is used, with each
component being replaced by each of its 10 nearest
neighbours according to the knowledge-free similar-
ity measure described above. We judge a VPC item
to be compositional if an expression formed by sub-
stitution occurs among the nearest 100 verb-particle
items to the original, and failing this, we judge it to be
non-compositional. We experimented with a number
of cut-off points for identifying semantically similar
items, and found that a value of 100 gave the best re-
sults.
5.4 Method 4
While Method 3 softens the reliance upon productiv-
ity as a test for compositionality, it still confuses insti-
3http://www.cs.utk.edu/?lsi/soft.html
TASK 1 TASK 2
(mean agreement = .693) (mean agreement = .750)
Majority Centroid annotator Majority Centroid annotator
Prec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1
Baseline .525 1.000 .680 .250 1.000 .400 .750 1.000 .860 .700 1.000 .820
Method 1 .577 .714 .638 .269 .700 .389 .731 .633 .678 .731 .679 .704
Method 2 .575 .714 .638 .308 .800 .447 .769 .667 .717 .769 .714 .739
Method 3 .558 .905 .690 .235 .800 .360 .765 .866 .810 .735 .892 .810
Method 4 .514 .857 .642 .200 .700 .280 .771 .900 .830 .714 .893 .794
TASK 3 TASK 4
(mean agreement = .729) (mean agreement = .688)
Majority Centroid annotator Majority Centroid annotator
Prec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1
Baseline .525 1.000 .690 .625 1.000 .770 .750 1.000 .857 .670 1.000 .800
Method 1 .474 .429 .450 .632 .480 .546 .818 .300 .442 .454 .385 .417
Method 2 .608 .666 .639 .782 .720 .749 .818 .300 .442 .454 .385 .417
Method 3 .531 .810 .641 .625 .800 .717 .769 .333 .480 .308 .308 .308
Method 4 .666 .286 .400 .666 .240 .353 .758 .833 .793 .303 .769 .435
Table 4: Results for the four methods over the different compositionality classification tasks
tutionalisation with non-compositionality somewhat
in its reliance upon substitution. We now suggest an-
other technique which we claim is based on sounder
principles. The underlying intuition is that identify-
ing the degree of semantic similarity between a VPC
and its component verb and/or particle will indicate
whether that component part contributes independent
semantics. This is similar to the assumption made
in Schone and Jurafsky (2001), except that we make
a distinction between the contribution of the different
component parts. We again used the knowledge-free
semantic similarity measure. We performed a pair-
wise comparison of all VPCs with all verbs and all
particles, obtaining cosine similarity scores for each
pair.
In order to measure the usefulness of this score,
we performed a logistic regression of the similarity
scores and the human judgements as to whether the
given verb or particle is entailed by the VPC. We did
this for the majority human judgements, and also the
centroid annotator scores. We also did the same us-
ing the majority scores but rejecting those items on
which there was less than 60% agreement. In ad-
dition to performing a regression for all items (All),
we also performed a regression for only those items
which have only one meaning according to WordNet
(Monosemous). The results for all of these are shown
in Table 3. The figures shown are chi-squared scores,
with their associated significance values. We observed
significant correlations for a number of the regres-
sions (notably all items vs. the centroid annotator, and
monosemous items vs. 60% agreement). While the
results are far from stable, such variation is perhaps to
be expected on a test like this since the nature of con-
text space models means that rogue items sometimes
get extremely high similarity scores, and we are per-
forming the regression over only 40 VPCs (80 VPC-
component pairs).
In order to build a classifier for making composi-
tionality decisions, we again used a neighbour-based
approach with a cut-off. We said that a verb was
contributing meaning to a VPC if it occurred in the
20 most similar items to the VPC. For particles, we
said that the item was contributing meaning if it was
among the 10 nearest neighbours. We tried out a range
of different cut-offs for each item and found that these
gave the best results.
6 Results
The results in Table 4 show that on all tasks (for the
majority-view based data and three out of four for the
centroid data), at least one of the four statistical meth-
ods offers an improvement in precision over the base-
line, and that there is an improvement in F-score for
TASK 1 on both sets of data. There are swings in the
relative scores obtained over the majority as compared
to centroid annotator data for a given task. In terms
of relative performance, the semantic similarity based
approach of Methods 3 and 4 outperform the distribu-
tion based approach of Methods 1 and 2 in terms of
F-score, on 6 of the 8 sets of results reported.
In order to get a reliable sense for how good these
scores are, we compare them with the level of agree-
ment across human judges. We calculated pairwise
agreement across all participants on the four classifi-
cation tasks, resulting in the figures given in Table 4.
These agreement scores give us an upper bound for
classification accuracy on each task, from which it is
possible to benchmark the classification accuracy of
the classifiers on that same task. On TASK 1, three of
the four classifiers achieved a classification accuracy
of .575. On TASK 2, the highest-performing classi-
fier (Method 4), achieved a classification accuracy of
.725. On TASK 3, Method 2 achieved the highest clas-
sification accuracy at .600, and on TASK 4, Method 4
achieved a classification accuracy of .675. We can see
then that the best classifiers perform only marginally
below the upper bound on at least two of the tasks.
While these results may appear at first glance to be
less than conclusive, we must bear in mind that we are
working with limited amounts of data and relatively
simplistic models of a cognitively intensive task. We
interpret them as very positive indicators of the via-
bility of using empirical methods to analyse VPC se-
mantics.
7 Conclusion
This paper has described the implementation and eval-
uation of four corpus-based approaches to the seman-
tics of verb-particle constructions. We created a set of
gold-standard data, based on non-expert judgements
acquired via a web-based experiment. We then imple-
mented four different techniques and showed that they
offer a significant improvement over a naive approach.
Acknowledgements
We would like to thank Ann Copestake, Maria Lapata, Diana Mc-
Carthy, Aline Villavicencio, Tom Wasow, Dominic Widdows and
the three anonymous reviewers for their valuable input on this
research. Timothy Baldwin is supported by the National Science
Foundation under Grant No. BCS-0094638 and also the Research
Collaboration between NTT Communication Science Laborato-
ries, Nippon Telegraph and Telephone Corporation and CSLI,
Stanford University. Colin Bannard is supported by ESRC Grant
PTA-030-2002-01740
References
Douglas G. Altman. 1991. Practical Statistics for Medical Re-
search. Chapman and Hall.
Timothy Baldwin and Aline Villavicencio. 2002. Extracting the
unextractable: A case study on verb-particles. In Proc. of
the 6th Conference on Natural Language Learning (CoNLL-
2002), Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic
Widdows. 2003. An empirical model of multiword expres-
sion decomposability. In Proc. of the ACL-2003 Workshop on
Multiword Expressions: Analysis, Acquisition and Treatment.
(this volume).
Colin Bannard. 2002. Statistical techniques for automatically
inferring the semantics of verb-particle constructions. LinGO
Working Paper No. 2002-06.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recog-
nition of noun modifier relationships. In Proc. of the 36th An-
nual Meeting of the ACL and 17th International Conference on
Computational Linguistics (COLING/ACL-98), pages 96?102,
Montreal, Canada.
Laurel Brinton. 1985. Verb particles in English: Aspect or ak-
tionsart. Studia Linguistica, 39:157?68.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Jean Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Computational Linguistics, 22(2):249?
254.
Martin Corley, Frank Keller, and Christoph Scheepers. 2000.
Conducting psychological experiments over the world wide
web. Unpublished manuscript, University of Edinburgh and
Saarland University.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990. Indexing
by latent semantic analysis. Journal of the American Society
of Information Science, 41(6).
John A. Hawkins. 2000. The relative order of preposition phrases
in English: Going beyond manner ? place ? time. Language
Variation and Change, 11:231?266.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Macquarie
University.
Dekang Lin. 1998a. Automatic retrieval and clustering of similar
words. In Proc. of the 36th Annual Meeting of the ACL and
17th International Conference on Computational Linguistics
(COLING/ACL-98), Montreal, Canada.
Dekang Lin. 1998b. Extracting collocations from text corpora.
In First Workshop on Computational Terminology.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual Meeting
of the ACL, pages 317?24, College Park, USA.
Barbara Lohse, John A. Hawkins, and Tom Wasow. in prepara-
tion. Domain minimization in English verb-particle construc-
tions.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting
a continuum of compositionality in phrasal verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment. (this volume).
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proceedings of
2nd Conference on Empirical Methods in Natural Language
Processing.
G.A. Miller, R. Beckwith, C. Fellbaum, D Gross, and K.J. Miller.
1990. Introduction to WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4):235?44.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting of
the North American Chapter of Association for Computational
Linguistics (NAACL2001), pages 40?7, Pittsburgh, USA.
Darren Pearce. 2002. A comparative evaluation of collocation
extraction techniques. In Proc. of the 3rd International Con-
ference on Language Resources and Evaluation (LREC 2002),
Las Palmas, Canary Islands.
Barbara Rosario and Marti Hearst. 2001. Classifying the seman-
tic relations in noun compounds via a domain-specific lexical
hierarchy. In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), Pitts-
burgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Conference
on Intelligent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free in-
duction of multiword unit dictionary headwords a solved prob-
lem? In Proc. of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001), pages 100?108.
Hinrich Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
Aline Villavicencio and Ann Copestake. 2002. Phrasal verbs and
the LinGO-ERG. LinGO Working Paper No. 2002-01.
An Empirical Model of Multiword Expression Decomposability
Timothy Baldwin?, Colin Bannard?, Takaaki Tanaka? and Dominic Widdows?
? CSLI
Stanford University
Stanford CA 94305, USA
{tbaldwin,dwiddows}@csli.stanford.edu
? School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
c.j.bannard@ed.ac.uk
? Communication Science
Labs
NTT Corporation
Kyoto, Japan
takaaki@cslab.kecl.ntt.co.jp
Abstract
This paper presents a construction-
inspecific model of multiword expression
decomposability based on latent semantic
analysis. We use latent semantic analysis
to determine the similarity between a
multiword expression and its constituent
words, and claim that higher similarities
indicate greater decomposability. We
test the model over English noun-noun
compounds and verb-particles, and eval-
uate its correlation with similarities and
hyponymy values in WordNet. Based on
mean hyponymy over partitions of data
ranked on similarity, we furnish evidence
for the calculated similarities being corre-
lated with the semantic relational content
of WordNet.
1 Introduction
This paper is concerned with an empirical model of
multiword expression decomposability. Multiword
expressions (MWEs) are defined to be cohesive lex-
emes that cross word boundaries (Sag et al, 2002;
Copestake et al, 2002; Calzolari et al, 2002). They
occur in a wide variety of syntactic configurations
in different languages (e.g. in the case of English,
compound nouns: post office, verbal idioms: pull
strings, verb-particle constructions: push on, etc.).
Decomposability is a description of the degree to
which the semantics of an MWE can be ascribed
to those of its parts (Riehemann, 2001; Sag et al,
2002). Analysis of the semantic correlation between
the constituent parts and whole of an MWE is per-
haps more commonly discussed under the banner of
compositionality (Nunberg et al, 1994; Lin, 1999).
Our claim here is that the semantics of the MWE are
deconstructed and the parts coerced into often id-
iosyncratic interpretations to attain semantic align-
ment, rather than the other way around. One id-
iom which illustrates this process is spill the beans,
where the semantics of reveal?(secret?) are de-
composed such that spill is coerced into the idiosyn-
cratic interpretation of reveal? and beans into the
idiosyncratic interpretation of secret?. Given that
these senses for spill and beans are not readily avail-
able at the simplex level other than in the context
of this particular MWE, it seems fallacious to talk
about them composing together to form the seman-
tics of the idiom.
Ideally, we would like to be able to differ-
entiate between three classes of MWEs: non-
decomposable, idiosyncratically decomposable and
simple decomposable (derived from Nunberg et al?s
sub-classification of idioms (1994)). With non-
decomposable MWEs (e.g. kick the bucket, shoot
the breeze, hot dog), no decompositional anal-
ysis is possible, and the MWE is semantically
impenetrable. The only syntactic variation that
non-decomposable MWEs undergo is verbal in-
flection (e.g. kicked the bucket, kicks the bucket)
and pronominal reflexivisation (e.g. wet oneself ,
wet themselves). Idiosyncratically decomposable
MWEs (e.g. spill the beans, let the cat out of the
bag, radar footprint) are decomposable but co-
erce their parts into taking semantics unavailable
outside the MWE. They undergo a certain degree
of syntactic variation (e.g. the cat was let out of
the bag). Finally, simple decomposable MWEs
(also known as ?institutionalised? MWEs, e.g. kin-
dle excitement, traffic light) decompose into simplex
senses and generally display high syntactic variabil-
ity. What makes simple decomposable expressions
true MWEs rather than productive word combina-
tions is that they tend to block compositional al-
ternates with the expected semantics (termed anti-
collocations by Pearce (2001b)). For example, mo-
tor car cannot be rephrased as *engine car or *mo-
tor automobile. Note that the existence of anti-
collocations is also a test for non-decomposable and
idiosyncratically decomposable MWEs (e.g. hot dog
vs. #warm dog or #hot canine).
Our particular interest in decomposability stems
from ongoing work on grammatical means for cap-
turing MWEs. Nunberg et al (1994) observed that
idiosyncratically decomposable MWEs (in particu-
lar idioms) undergo much greater syntactic variation
than non-decomposable MWEs, and that the vari-
ability can be partially predicted from the decompo-
sitional analysis. We thus aim to capture the decom-
posability of MWEs in the grammar and use this to
constrain the syntax of MWEs in parsing and gen-
eration. Note that it is arguable whether simple de-
composable MWEs belong in the grammar proper,
or should be described instead as lexical affinities
between particular word combinations.
As the first step down the path toward an empir-
ical model of decomposability, we focus on demar-
cating simple decomposable MWEs from idiosyn-
cratically decomposable and non-decomposable
MWEs. This is largely equivalent to classifying
MWEs as being endocentric (i.e., a hyponym of
their head) or exocentric (i.e., not a hyponym of
their head: Haspelmath (2002)).
We attempt to achieve this by looking at the se-
mantic similarity between an MWE and its con-
stituent words, and hypothesising that where the
similarity between the constituents of an MWE and
the whole is sufficiently high, the MWE must be of
simple decomposable type.
The particular similarity method we adopt is la-
tent semantic analysis, or LSA (Deerwester et al,
1990). LSA allows us to calculate the similarity
between an arbitrary word pair, offering the advan-
tage of being able to measure the similarity between
the MWE and each of its constituent words. For
MWEs such as house boat, therefore, we can expect
to capture the fact that the MWE is highly similar in
meaning to both constituent words (i.e. the modifier
house and head noun boat). More importantly, LSA
makes no assumptions about the lexical or syntac-
tic composition of the inputs, and thus constitutes a
fully construction- and language-inspecific method
of modelling decomposability. This has clear advan-
tages over a more conventional supervised classifier-
style approach, where training data would have to be
customised to a particular language and construction
type.
Evaluation is inevitably a difficulty when it comes
to the analysis of MWEs, due to the lack of con-
cise consistency checks on what MWEs should and
should not be incorporated into dictionaries. While
recognising the dangers associated with dictionary-
based evaluation, we commit ourselves to this
paradigm and focus on searching for appropriate
means of demonstrating the correlation between
dictionary- and corpus-based similarities.
The remainder of this paper is structured as fol-
lows. Section 2 describes past research on MWE
compositionality of relevance to this effort. Sec-
tion 3 provides a basic outline of the resources used
in this research, LSA, the MWE extraction methods,
and measures used to evaluate our method. Section 4
then provides evaluation of the proposed method,
and the paper is concluded with a brief discussion
in Section 5.
2 Past research
Although there has been some useful work on com-
positionality in statistical machine translation (e.g.
Melamed (1997)), there has been little work on de-
tecting ?non-compositional? (i.e. non-decomposable
and idiosyncratically decomposable) items of vari-
able syntactic type in monolingual corpora. One in-
teresting exception is Lin (1999), whose approach is
explained as follows:
The intuitive idea behind the method is
that the metaphorical usage of a non-
compositional expression causes it to
have a different distributional characteris-
tic than expressions that are similar to its
literal meaning.
The expressions he uses are taken from a colloca-
tion database (Lin, 1998b). These ?expressions that
are similar to [their] literal meaning? are found by
substituting each of the words in the expression with
the 10 most similar words according to a corpus de-
rived thesaurus (Lin, 1998a). Lin models the dis-
tributional difference as a significant difference in
mutual information. Significance here is defined as
the absence of overlap between the 95% confidence
interval of the mutual information scores. Lin pro-
vides some examples that suggest he has identified
a successful measure of ?compositionality?. He of-
fers an evaluation where an item is said to be non-
compositional if it occurs in a dictionary of idioms.
This produces the unconvincing scores of 15.7% for
precision and 13.7% for recall.
We claim that substitution-based tests are use-
ful in demarcating MWEs from productive word
combinations (as attested by Pearce (2001a) in a
MWE detection task), but not in distinguishing the
different classes of decomposability. As observed
above, simple decomposable MWEs such as mo-
tor car fail the substitution test not because of non-
decomposability, but because the expression is in-
stitutionalised to the point of blocking alternates.
Thus, we expect Lin?s method to return a wide ar-
ray of both decomposable and non-decomposable
MWEs.
Bannard (2002) focused on distributional tech-
niques for describing the meaning of verb-particle
constructions at the level of logical form. The
semantic similarity between a multiword expres-
sion and its head was used as an indicator of
decomposability. The assumption was that if a
verb-particle was sufficiently similar to its head
verb, then the verb contributed its simplex mean-
ing. It gave empirical backing to this assump-
tion by showing that annotator judgements for verb-
particle decomposability correlate significantly with
non-expert human judgements on the similarity be-
tween a verb-particle construction and its head verb.
Bannard et al (2003) extended this research in look-
ing explicitly at the task of classifying verb-particles
as being compositional or not. They successfully
combined statistical and distributional techniques
(including LSA) with a substitution test in analysing
compositionality. McCarthy et al (2003) also tar-
geted verb-particles for a study on compositionality,
and judged compositionality according to the degree
of overlap in the N most similar words to the verb-
particle and head verb, e.g., to determine composi-
tionality.
We are not the first to consider applying LSA to
MWEs. Schone and Jurafsky (2001) applied LSA to
the analysis of MWEs in the task of MWE discov-
ery, by way of rescoring MWEs extracted from a
corpus. The major point of divergence from this re-
search is that Schone and Jurafsky focused specifi-
cally on MWE extraction, whereas we are interested
in the downstream task of semantically classifying
attested MWEs.
3 Resources and Techniques
In this section, we outline the resources used in eval-
uation, give an informal introduction to the LSA
model, sketch how we extracted the MWEs from
corpus data, and describe a number of methods
for modelling decomposability within a hierarchical
lexicon.
3.1 Resources and target MWEs
The particular reference lexicon we use to eval-
uate our technique is WordNet 1.7 (Miller et
al., 1990), due to its public availability, hier-
archical structure and wide coverage. Indeed,
Schone and Jurafsky (2001) provide evidence that
suggests that WordNet is as effective an evaluation
resource as the web for MWE detection methods,
despite its inherent size limitations and static nature.
Two MWE types that are particularly well repre-
sented in WordNet are compound nouns (47,000 en-
tries) and multiword verbs (2,600 entries). Of these,
we chose to specifically target two types of MWE:
noun-noun (NN) compounds (e.g. computer net-
work, work force) and verb-particles (e.g. look on,
eat up) due to their frequent occurrence in both de-
composable and non-decomposable configurations,
and also their disparate syntactic behaviours.
We extracted the NN compounds from the 1996
Wall Street Journal data (WSJ, 31m words), and
the verb-particles from the British National Corpus
(BNC, 90m words: Burnard (2000)). The WSJ data
is more tightly domain-constrained, and thus a more
suitable source for NN compounds if we are to ex-
pect sentential context to reliably predict the seman-
tics of the compound. The BNC data, on the other
hand, contains more colloquial and prosaic texts and
is thus a richer source of verb-particles.
3.2 Description of the LSA model
Our goal was to compare the distribution of differ-
ent compound terms with their constituent words, to
see if this indicated similarity of meaning. For this
purpose, we used latent semantic analysis (LSA) to
build a vector space model in which term-term sim-
ilarities could be measured.
LSA is a method for representing words as points
in a vector space, whereby words which are related
in meaning should be represented by points which
are near to one another, first developed as a method
for improving the vector model for information re-
trieval (Deerwester et al, 1990). As a technique for
measuring similarity between words, LSA has been
shown to capture semantic properties, and has been
used successfully for recognising synonymy (Lan-
dauer and Dumais, 1997), word-sense disambigua-
tion (Schu?tze, 1998) and for finding correct transla-
tions of individual terms (Widdows et al, 2002).
The LSA model we built is similar to that de-
scribed in (Schu?tze, 1998). First, 1000 frequent con-
tent words (i.e. not on the stoplist)1 were chosen
as ?content-bearing words?. Using these content-
bearing words as column labels, the 50,000 most
frequent terms in the corpus were assigned row
vectors by counting the number of times they oc-
1A ?stoplist? is a list of frequent words which have little
independent semantic content, such as prepositions and deter-
miners (Baeza-Yates and Ribiero-Neto, 1999, p167).
curred within the same sentence as a content-bearing
word. Singular-value decomposition (Deerwester et
al., 1990) was then used to reduce the number of
dimensions from 1000 to 100. Similarity between
two vectors (points) was measured using the cosine
of the angle between them, in the same way as the
similarity between a query and a document is often
measured in information retrieval (Baeza-Yates and
Ribiero-Neto, 1999, p28). Effectively, we could use
LSA to measure the extent to which two words or
MWEs x and y usually occur in similar contexts.
Since the corpora had been tagged with parts-of-
speech, we could build syntactic distinctions into the
LSA models ? instead of just giving a vector for
the string test we were able to build separate vec-
tors for the nouns, verbs and adjectives test. This
combination of technologies was also used to good
effect by Widdows (2003): an example of the con-
tribution of part-of-speech information to extracting
semantic neighbours of the word fire is shown in
Table 1. As can be seen, the noun fire (as in the
substance/element) and the verb fire (mainly used
to mean firing some sort of weapon) are related to
quite different areas of meaning. Building a single
vector for the string fire confuses this distinction ?
the neighbours of fire treated just as a string include
words related to both the meaning of fire as a noun
(more frequent in the BNC) and as a verb. The ap-
propriate granularity of syntactic classifications is an
open question for this kind of research: treating all
the possible verbs categories as different (e.g. dis-
tinguishing infinitive from finite from gerund forms)
led to data sparseness, and instead we considered
?verb? as a single part-of-speech type.
3.3 MWE extraction methods
NN compounds were extracted from the WSJ by
first tagging the data with fnTBL 1.0 (Ngai and Flo-
rian, 2001) and then simply taking noun bigrams
(adjoined on both sides by non-nouns to assure the
bigram is not part of a larger compound nominal).
Out of these, we selected those compounds that are
listed in WordNet, resulting in 5,405 NN compound
types (208,000 tokens).
Extraction of the verb-particles was consider-
ably more involved, and drew on the method of
Baldwin and Villavicencio (2002). Essentially, we
used a POS tagger and chunker (both built using
fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag
the BNC. This allowed us to extract verb-particle to-
kens through use of the particle POS and chunk tags
returned by the two systems. This produces high-
precision, but relatively low-recall results, so we
performed the additional step of running a chunk-
based grammar over the chunker output to detect
candidate mistagged particles. In the case that a
noun phrase followed the particle candidate, we per-
formed attachment disambiguation to determine the
transitivity of the particle candidate. These three
methods produced three distinct sets of verb-particle
tokens, which we carried out weighted voting over
to determine the final set of verb-particle tokens. A
total of 461 verb-particles attested in WordNet were
extracted (160,765 tokens).
For both the NN compound and verb-particle
data, we replaced each token occurrence with a
single-word POS-tagged token to feed into the LSA
model.
3.4 Techniques for evaluating correlation with
WordNet
In order to evaluate our approach, we employed the
lexical relations as defined in the WordNet lexical
hierarchy (Miller et al, 1990). WordNet groups
words into sets with similar meaning (known as
?synsets?), e.g. {car, auto, automobile, machine,
motorcar } . These are organised into a hierarchy
employing multiple inheritance. The hierarchy is
structured according to different principles for each
of nouns, verbs, adjectives and adverbs. The nouns
are arranged according to hyponymy or ISA rela-
tions, e.g. a car is a kind of automobile. The verbs
are arranged according to troponym or ?manner-of?
relations, where murder is a manner of killing, so
kill immediately dominates murder in the hierarchy.
We used WordNet for evaluation by way of look-
ing at: (a) hyponymy, and (b) semantic distance.
Hyponymy provides the most immediate way of
evaluating decomposability. With simple decompos-
able MWEs, we can expect the constituents (and
particularly the head) to be hypernyms (ancestor
nodes) or synonyms of the MWE. That is, simple
decomposable MWEs are generally endocentric, al-
though there are some exceptions to this generali-
sation such as vice president arguably not being a
hyponym of president. No hyponymy relation holds
with non-decomposable or idiosyncratically decom-
posable MWEs (i.e., they are exocentric), as even if
the semantics of the head noun can be determined
through decomposition, by definition this will not
correspond to a simplex sense of the word.
We deal with polysemy of the constituent words
and/or MWE by simply looking for the exis-
tence of a sense of the constituent words which
fire (string only) fire nn1 fire vvi
fire 1.000000 fire nn1 1.000000 fire vvi 1.000000
flames 0.709939 flames nn2 0.700575 guns nn2 0.663820
smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778
blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442
firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106
fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572
explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025
burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173
destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767
brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000
arson 0.528909 arson nn1 0.522844 volley nn1 0.458717
accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797
chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523
blast 0.488617 burning vvg 0.502052 alert aj0 0.443610
guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308
damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886
Table 1: Semantic neighbours of fire with different parts-of-speech. The scores are cosine similarities
subsumes a sense of the MWE. The function
hyponym(word i,mwe) thus returns a value of 1 if
some sense of word i subsumes a sense of mwe , and
a value of 0 otherwise.
A more proactive means of utilising the WordNet
hierarchy is to derive a semantic distance based on
analysis of the relative location of senses in Word-
Net. Budanitsky and Hirst (2001) evaluated the per-
formance of five different methods that measure
the semantic distance between words in the Word-
Net Hierarchy, which Patwardhan et al (2003) have
then implemented and made available for general
use as the Perl package distance-0.11.2 We fo-
cused in particular on the following three measures,
the first two of which are based on information the-
oretic principles, and the third on sense topology:
? Resnik (1995) combined WordNet with corpus
statistics. He defines the similarity between
two words as the information content of the
lowest superordinate in the hierarchy, defining
the information content of a concept c (where
a concept is the WordNet class containing the
word) to be the negative of its log likelihood.
This is calculated over a corpus of text.
? Lin (1998c) also employs the idea of corpus-
derived information content, and defines the
similarity between two concepts in the follow-
ing way:
sim(C1, C2) =
2 log P (C0)
log P (C1) + log P (C2)
(1)
where C0 is the lowest class in the hierarchy
that subsumes both classes.
2http://www.d.umn.edu/?tpederse/
distance.html
? Hirst and St-Onge (1998) use a system of ?re-
lations? of different strength to determine the
similarity of word senses, conditioned on the
type, direction and relative distance of edges
separating them.
The Patwardhan et al (2003) implementation that
we used calculates the information values from
SemCor, a semantically tagged subset of the Brown
corpus. Note that the first two similarity measures
operate over nouns only, while the last can be ap-
plied to any word class.
The similarity measures described above calcu-
late the similarity between a pair of senses. In the
case that a given constituent word and/or MWE oc-
cur with more than one sense, we calculate a similar-
ity for sense pairing between them, and average over
them to produce a consolidated similarity value.
4 Evaluation
LSA was used to build models in which MWEs
could be compared with their constituent words.
Two models were built, one from the WSJ corpus
(indexing NN compounds) and one from the BNC
(indexing verb-particles). After removing stop-
words, the 50,000 most frequent terms were indexed
in each model. From the WSJ, these 50,000 terms
included 1,710 NN compounds (with corpus fre-
quency of at least 13) and from the BNC, 461 verb-
particles (with corpus frequency of at least 49).
We used these models to compare different words,
and to find their neighbours. For example, the neigh-
bours of the simplex verb cut and the verb-particles
cut out and cut off (from the BNC model) are shown
in Table 2. As can be seen, several of the neighbours
of cut out are from similar semantic areas as those
of cut, whereas those of cut off are quite different.
cut (verb) cut out (verb) cut off (verb)
cut verb 1.000000 cut out verb 1.000000 cut off verb 1.000000
trim verb 0.529886 fondant nn 0.516956 knot nn 0.448871
slash verb 0.522370 fondant jj 0.501266 choke verb 0.440587
cut nns 0.520345 strip nns 0.475293 vigorously rb 0.438071
cut nn 0.502100 piece nns 0.449555 suck verb 0.413003
reduce verb 0.465364 roll nnp 0.440769 crush verb 0.412301
cut out verb 0.433465 stick jj 0.434082 ministry nn 0.408702
pull verb 0.431929 cut verb 0.433465 glycerol nn 0.395148
fall verb 0.426111 icing nn 0.432307 tap verb 0.383932
hook verb 0.419564 piece nn 0.418780 shake verb 0.381581
recycle verb 0.413206 paste nn 0.416581 jerk verb 0.381284
project verb 0.401246 tip nn 0.413603 put down verb 0.380368
recycled jj 0.396315 hole nns 0.412813 circumference nn 0.378097
prune verb 0.395656 straw nn 0.411617 jn nnp 0.375634
pare verb 0.394991 hook nn 0.402947 pump verb 0.373984
tie verb 0.392964 strip nn 0.399974 nell nnp 0.373768
Table 2: Semantic neighbours of the verbs cut, cut out, and cut off .
Construction Method Pearson R2
Resnik .108 .012
NN compound Lin .101 .010
HSO .072 .005
verb-particle HSO .255 .065
Table 3: Correlation between LSA and WordNet
similarities
This reflects the fact that in most of its instances the
verb cut off is used to mean ?forcibly isolate?.
In order to measure this effect quantitatively, we
can simply take the cosine similarities between these
verbs, finding that sim(cut, cut out) = 0.433 and
sim(cut, cut off) = 0.183 from which we infer di-
rectly that, relative to the sense of cut, cut out is a
clearer case of a simple decomposable MWE than
cut off .
4.1 Statistical analysis
In order to get an initial feel for how well
the LSA-based similarities for MWEs and their
head words correlate with the WordNet-based
similarities over those same word pairs, we
did a linear regression and Pearson?s correla-
tion analysis of the paired data (i.e. the pair-
ing ?simLSA(word i,mwe), simWN(word i,mwe)?
for each WordNet similarity measure simWN). For
both tests, values closer to 0 indicate random distri-
bution of the data, whereas values closer to 1 indi-
cate a strong correlation. The correlation results for
NN compounds and verb-particles are presented in
Table 3, where R2 refers to the output of the linear
regression test and HSO refers to Hirst and St-Onge
similarity measure. In the case of NN compounds,
the correlation with LSA is very low for all tests,
that is LSA is unable to reproduce the relative sim-
ilarity values derived from WordNet with any reli-
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3
M
ea
n
 H
yp
on
ym
y
Partition No.
VPC(head)          
VPC(head)       ALL
HIGH
NN(mod)       
NN(head)       ALL
ALL
NN(head)         LOW
NN(head)          HIGH
VPC(head)         LOW
Figure 1: Hyponymy correlation
ability. With verb-particles, correlation is notably
higher than for NN compounds,3 but still at a low
level.
Based on these results, LSA would appear to
correlate poorly with WordNet-based similarities.
However, our main interest is not in similarity per
se, but how reflective LSA similarities are of the de-
composability of the MWE in question. While tak-
ing note of the low correlation with WordNet simi-
larities, therefore, we move straight on to look at the
hyponymy test.
4.2 Hyponymy-based analysis
We next turn to analysis of correlation between LSA
similarities and hyponymy values. Our expectation
is that for constituent word?MWE pairs with higher
LSA similarities, there is a greater likelihood of the
MWE being a hyponym of the constituent word. We
test this hypothesis by ranking the constituent word?
MWE pairs in decreasing order of LSA similarity,
3Recall that HSO is the only similarity measure which oper-
ates over verbs.
and partitioning the ranking up into m partitions of
equal size. We then calculate the average number of
hyponyms per partition. If our hypothesis is correct,
the earlier partitions (with higher LSA similarities)
will have higher occurrences of hyponyms than the
latter partitions.
Figure 1 presents the mean hyponymy values
across partitions of the NN compound data and verb-
particle data, with m set to 3 in each case. For the
NN compounds, we derive two separate rankings,
based on the similarity between the head noun and
NN compound (NN(head)) and the modifier noun
and the NN compound (NN(mod)). In the case of
the verb-particle data, WordNet has no classification
of prepositions or particles, so we can only calcu-
late the similarity between the head verb and verb-
particle (VPC(head)). Looking to the curves for
these three rankings, we see that they are all fairly
flat, nondescript curves. If we partition the data up
into low- and high-frequency MWEs, as defined by a
threshold of 100 corpus occurrences, we find that the
graphs for the low-frequency data (NN(head)LOW
and VPC(head)LOW) are both monotonically de-
creasing, whereas those for high-frequency data
(NN(head)HIGH and VPC(head)HIGH) are more hap-
hazard in nature. Our hypothesis of lesser instances
of hyponymy for lower similarities is thus supported
for low-frequency items but not for high-frequency
items, suggesting that LSA similarities are more
brittle over high-frequency items for this particu-
lar task. The results for the low-frequency items
are particularly encouraging given that the LSA-
based similarities were found to correlate poorly
with WordNet-derived similarities. The results for
NN(mod) are more erratic for both low- and high-
frequency terms, that is the modifier noun is not as
strong a predictor of decomposability as the head
noun. This is partially supported by the statistics on
the relative occurrence of NN compounds in Word-
Net subsumed by their head noun (71.4%) as com-
pared to NN compounds subsumed by their modifier
(13.7%).
In an ideal world, we would hope that the val-
ues for mean hyponymy were nearly 1 for the first
partition and nearly 0 for the last. Naturally, this
presumes perfect correlation of the LSA similarities
with decomposability, but classificational inconsis-
tencies in WordNet alo work against us. For ex-
ample, vice chairman is an immediate hyponym of
both chairman and president, but vice president is
not a hyponym of president. According to LSA,
however, sim(chairman, vice chairman) = .508 and
sim(president, vice president) = .551.
It remains to be determined why LSA should per-
form better over low-frequency items, although the
higher polysemy of high-frequency items is one po-
tential cause. We intend to further investigate this
matter in future research.
5 Discussion
While evaluation pointed to a moderate correlation
between LSA similarities and occurrences of hy-
ponymy, we have yet to answer the question of
exactly where the cutoffs between simple decom-
posable, idiosyncratically decomposable and non-
decomposable MWEs lie. While it would be pos-
sible to set arbitrary thresholds to artificially parti-
tion up the space of MWEs based on LSA similarity
(or alternatively use statistical tests to derive confi-
dence intervals for similarity values), we feel that
more work needs to be done in establishing exactly
what different LSA similarities for different MWE?
constituent word combinations mean.
One area in which we plan to extend this research
is the analysis of MWEs in languages other than
English. Because of LSA?s independence from lin-
guistic constraints, it is equally applicable to all lan-
guages, assuming there is some way of segmenting
inputs into constituent words.
To summarise, we have proposed a construction-
inspecific empirical model of MWE decomposabil-
ity, based on latent semantic analysis. We evaluated
the method over English NN compounds and verb-
particles, and showed it to correlate moderately with
WordNet-based hyponymy values.
Acknowledgements
This material is partly based upon work supported by the Na-
tional Science Foundation under Grant No. BCS-0094638 and
also the Research Collaboration between NTT Communication
Science Laboratories, Nippon Telegraph and Telephone Corpo-
ration and CSLI, Stanford University. We would like to thank
the anonymous reviewers for their valuable input on this re-
search.
References
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999. Modern
Information Retrieval. Addison Wesley / ACM press.
Timothy Baldwin and Aline Villavicencio. 2002. Extracting
the unextractable: A case study on verb-particles. In Proc. of
the 6th Conference on Natural Language Learning (CoNLL-
2002), Taipei, Taiwan.
Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003.
A statistical approach to the semantics of verb-particles. In
Proc. of the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment. (this volume).
Colin Bannard. 2002. Statistical techniques for automati-
cally inferring the semantics of verb-particle constructions.
LinGO Working Paper No. 2002-06.
Alexander Budanitsky and Graeme Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-oriented
evaluation of five measures. In Workshop on Wordnet and
Other Lexical Resources, Second meeting of the NAACL,
Pittsburgh, USA.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman, Nancy
Ide, Alessandro Lenci, Catherine MacLeod, and Antonio
Zampolli. 2002. Towards best practice for multiword ex-
pressions in computational lexicons. In Proceedings of the
Third International Conference on Language Resources and
Evaluation (LREC 2002), pages 1934?40, Las Palmas, Ca-
nary Islands.
Ann Copestake, Fabre Lambeau, Aline Villavicencio, Francis
Bond, Timothy Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic precision and
reusability. In Proc. of the 3rd International Conference
on Language Resources and Evaluation (LREC 2002), pages
1941?7, Las Palmas, Canary Islands.
Scott Deerwester, Susan Dumais, George Furnas, Thomas Lan-
dauer, and Richard Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for In-
formation Science, 41(6):391?407.
Martin Haspelmath. 2002. Understanding Morphology.
Arnold Publishers.
Graeme Hirst and David St-Onge. 1998. Lexical chains as
representations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 305?32. MIT Press,
Cambridge, USA.
Thomas Landauer and Susan Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory of ac-
quisition. Psychological Review, 104(2):211?240.
Dekang Lin. 1998a. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics (COLING/ACL-98).
Dekang Lin. 1998b. Extracting collocations from text corpora.
In First Workshop on Computational Terminology.
Dekang Lin. 1998c. An information-theoretic definition of
similarity. In Proceedings of the 15th International Confer-
ence on Machine Learning.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual Meeting
of the ACL, pages 317?24, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting
a continuum of compositionality in phrasal verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment. (this volume).
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. of the
2nd Conference on Empirical Methods in Natural Language
Processing (EMNLP-97), Providence, USA.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: an on-line lexical database. International Jour-
nal of Lexicography, 3(4):235?44.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 1994. Id-
ioms. Language, 70:491?538.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the 4th International Con-
ference on Intelligent Text Processing and Computational
Linguistics (CICLing-2003), Mexico City, Mexico.
Darren Pearce. 2001a. Synonymy in collocation extraction. In
Proc. of the NAACL 2001 Workshop on WordNet and Other
Lexical Resources: Applications, Extensions and Customiza-
tions, Pittsburgh, USA.
Darren Pearce. 2001b. Using conceptual similarity for collo-
cation extraction. In Proc. of the 4th UK Special Interest
Group for Computational Linguistics (CLUK4).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity. In Proceedings of the 14th International
Joint Conference on Artificial Intelligence.
Susanne Riehemann. 2001. A Constructional Approach to Id-
ioms and Word Formation. Ph.D. thesis, Stanford.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Confer-
ence on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free
induction of multiword unit dictionary headwords a solved
problem? In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), pages
100?108.
Hinrich Sch u?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan. 2002.
Using parallel corpora to enrich multilingual lexical re-
sources. In Third International Conference on Language Re-
sources and Evaluation, pages 240?245, Las Palmas, Spain,
May.
Dominic Widdows. 2003. Unsupervised methods for develop-
ing taxonomies by combining syntactic and statistical infor-
mation. In Proc. of the 3rd International Conference on Hu-
man Language Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003). (to appear).
Translation by Machine of Complex Nominals: Getting it Right
Timothy Baldwin
CSLI
Stanford University
Stanford, CA 94305 USA
tbaldwin@csli.stanford.edu
Takaaki Tanaka
Communication Science Laboratories
Nippon Telephone and Telegraph Corporation
Kyoto, Japan
takaaki@cslab.kecl.ntt.co.jp
Abstract
We present a method for compositionally translating
noun-noun (NN) compounds, using a word-level
bilingual dictionary and syntactic templates for can-
didate generation, and corpus and dictionary statis-
tics for selection. We propose a support vector
learning-based method employing target language
corpus and bilingual dictionary data, and evaluate it
over a English   Japanese machine translation task.
We show the proposed method to be superior to pre-
vious methods and also robust over low-frequency
NN compounds.
1 Introduction
Noun-noun (NN) compounds (e.g. web server, 

kikai

hoNyaku ?machine translation?,1 the
elements of which we will refer to as N 	 and N 
 in
linear order of occurrence) are a very real problem
for both machine translation (MT) systems and hu-
man translators due to:
constructional variability in the translations:

kikai

hoNyaku ?machine transla-
tion? (N-N) vs.   miNkaN  kigyou
?private company? (Adj-N) vs.  Making Sense of Japanese Relative Clause Constructions
Timothy Baldwin
CSLI
Stanford University
Stanford, CA 94305 USA
tbaldwin@csli.stanford.edu
Abstract
We apply the C4.5 decision tree learner in interpret-
ing Japanese relative clause constructions, based
around shallow syntactic and semantic processing.
In parameterising data for use with C4.5, we pro-
pose and test various means of reducing intra-
clausal interpretational ambiguity, and cross index-
ing the overall analysis of cosubordinated relative
clause constructions. We additionally investigate
the disambiguating effect of the different parame-
ter types used, and establish upper bounds for the
task.
1 Introduction
Japanese relative clause constructions have the gen-
eral structure [[S][NP]], and constitute a noun
phrase. We will term the modifying S the ?relative
clause?, the modified NP the ?head NP?, and the
overall NP a ?relative clause construction? or RCC.
Example RCCs are:1
(1) kino?
yesterday
katta
bought
bo?si
hat
?the hat which ( ) bought yesterday?
(2) bo?si-o
hat-ACC
katta
bought
riyu?
reason
?the reason ( ) bought a hat?
(3) taterareta
built
yokutosi
next year
?the year after ( ) was built?
Different claims have been made as to the roles
of syntax, semantics and pragmatics (or frame se-
mantics) in the construal of Japanese RCCs (e.g.
Teramura (1975?78), Sirai and Gunji (1998), Mat-
sumoto (1997)). We consider two basic syntactico-
semantic selection processes to govern RCC con-
strual: selection of the relative clause by the head
NP and selection of the head NP by the relative
1The following abbreviations are used in glosses: NOM =
nominative, ACC = accusative, PRES = non-past and POT = po-
tential. ( ) is used to indicate zero (anaphoric) arguments.
clause. These processes can be seen to be at play
in the examples above: in (1), the head verb of the
relative clause selects for the head NP, and a di-
rect object case-slot gapping interpretation results
(i.e. bo?si is the direct object of katta); in (2), the
head NP selects for the relative clause, resulting in
an attributive interpretation (i.e. bo?si-o katta is an
attributive modifier of riyu?); and in (3) an attribu-
tive interpretation similarly results, with the quali-
fication that while yokutosi selects for the relative
clause, the relative clause must in turn be able to se-
lect for a temporal modifier (e.g. stative verbs such
as soNzai-suru ?exist? are incompatible with this
construction). There is a close relationship between
syntax and semantics here, in that syntax provides
the basic argument and modifier positions for the
head verb of the relative clause, which semantics
fleshes out by way of selectional restrictions. Prag-
matics also has a role to play in rating the plausibil-
ity of different interpretations (Matsumoto, 1997),
although we ignore its effects, and indeed the im-
pact of context, in this research.
Our objective in this paper is, given a taxonomy
of Japanese RCC semantic types (Baldwin, 1998)
and a gold-standard set of Japanese RCC instances,
to investigate the success of various parameter con-
figurations in interpreting RCCs. One feature of the
proposed method is that it is based on shallow anal-
ysis, centring principally around a basic case frame
and verb class description. That is, we attempt to
make maximum use of surface information in per-
forming a deep semantic task, in the same vein, e.g.,
as Joanis and Stevenson (2003) for English verb
classification and Lapata (2002) in disambiguating
nominalisations.
Relative clause interpretation is a core component
of text understanding, as demonstrated in the con-
text of the MUC conference series (Cardie, 1992;
Hobbs et al, 1997). It also has immediate appli-
cations in, e.g., Japanese?English machine transla-
tion: for case-slot gapping RCCs such as (1), we ex-
trapose the head NP from the appropriate argument
position in the English relative clause (producing,
e.g., ?the hat   [    bought yesterday]?), and for at-
tributive RCCs such as (2), we generate the English
relative clause without extraposition and select the
relative pronoun according to the head NP (produc-
ing, e.g., ?the reason that the hat was bought?).
RCC interpretation is dogged by analytical am-
biguity, in particular for phrase boundary, phrase
head/attachment and word sense ambiguity. The
first two of these concerns can be dealt with by a
parser such as KNP (Kurohashi and Nagao, 1998)
or CaboCha (Kudo and Matsumoto, 2002), or alter-
natively a tag sequence-based technique such as that
proposed by Siddharthan (2002) for English. Word
sense ambiguity is an issue if we wish to determine
the valence of the verb and make use of selectional
restrictions. We sidestep full-on verb sense disam-
biguation by associating a unique case frame with
each verb stem type and encoding common alterna-
tions in the verb class. Even here, however, we must
have some means of dealing with verb homonymy
and integrating analyses for cosubordinated relative
clauses. We investigate various techniques to re-
solve such ambiguity and combine the analysis of
multiple component clauses.
In the following, we define the RCC semantic
types (  2) and outline the parameters used in the
proposed method (  3). We then discuss sources of
ambiguity and disambiguation methods (  4), be-
fore evaluating the proposed methods (  5), and fi-
nally comparing the results with those of previous
research (  6).
2 Definitions
We define relative clause modification as falling into
three major semantic categories, indistinguishable
orthographically: case-slot gapping, attributive and
idiomatic.
Case-slot gapping RCCs (aka ?internal?/?inner
relation? (Teramura, 1975?78) or ?clause host?
RCCs (Matsumoto, 1997)), are characterised by the
head NP having been gapped (or extraposed) from
a case slot subcategorised by the main verb of the
relative clause (see (1)). For our purposes, case-slot
gapping is considered to occur in 19 sub-categories,
which can be partitioned into 8 argument case
slot types (e.g. SUBJECT, DIRECT OBJECT, INDI-
RECT OBJECT) and 11 modifier case slot types
(e.g. INSTRUMENT, TEMPORAL, SOURCE LOCA-
TIVE: Baldwin (1998)). Note that the case marking
on the slot from which gapping has occurred is not
preserved either within the relative clause or on the
head NP.
Attributive RCCs (aka ?external?/?outer rela-
tion? (Teramura, 1975?78) or ?noun host? RCCs
(Matsumoto, 1997)) occur when the relative clause
modifies or restricts the denotatum of the head NP
(see (2)). They come in 7 varieties according to the
nature of modification (e.g. CONTENT, RESULTA-
TIVE, EXCLUSIVE).
Idiomatic RCCs are produced when the overall
RCC produces a constructionally idiomatic reading,
e.g.:
(4) mite
to see
minu
not see
huri
pretend
?looking the other way?
One feature of idiomatic RCCs is that they can be
described by a largely lexicalised construction tem-
plate, and are incompatible with conjugational al-
ternation and modifier case slots. Due to the non-
compositional nature of idiomatic RCCs, we make
no attempt to analyse them by way of the case-slot
gapping/attributive RCC dichotomy, or sub-classify
them further.
Japanese RCC interpretation as defined in this pa-
per is according to the 27 interpretation types sub-
sumed by these 3 basic categories of RCC construal.
It is important to realise that these interpretation
types are lexically indistinguishable. The semantic
type of the RCC is therefore not readily accessible
from a simple structural analysis of the RCC as con-
tained within a standard treebank.
3 Parameter description
Features used in the interpretation of RCCs include
a generalised case frame description, a verb class
characterisation, head noun semantics, morphologi-
cal analysis of the head verb, and various construc-
tional templates. These combine to form the 49-
feature parameter signature of each RCC. Unless
otherwise mentioned, all features are binary.
Case frames are applied in determining which
argument case slots are subcategorised by the head
verb of the relative clause and instantiated?hence
making them unavailable for case-slot gapping?
and conversely which case slots are subcategorised
by the head verb and uninstantiated?making them
available for case slot gapping. The range of argu-
ment case slots coincides exactly with the set of ar-
gument case-slot gapping RCC types from

2 (8
features in total).
Argument case slot instantiation features are set
by comparing a given case frame to the actual input,
and aligning case slots between the two according
to case marker correspondence. In the case frame
dictionary, a single generalised case frame is given
for each verb stem. Case frames were generated
from the Goi-Taikei pattern-based valency dictio-
nary (Ikehara et al, 1997) by manually merging the
major senses for each distinct verb stem. In essence,
case frames are simply a list of the argument case
slots for the verb in question in their canonical or-
dering (case frames include no modifier case slots).
Each case slot is marked for canonical case marking
and case slot type.
Case frames can contain lexicalised case slots,
which must be overtly realised for that case frame to
be triggered. Examples of fixed expressions are ki-o
tukeru (mind-ACC fix/attach) ?to be careful/keep an
eye out for (something)? and yume-o miru (dream-
ACC see) ?to dream?. We manually annotated each
fixed argument for ?gapability?, i.e. the potential
for extraposition to the head NP position such as
with the RCC kino? mita yume ?the dream I had last
night?. If a gapable fixed argument occurs (unmod-
ified) in head NP position, we use the ?gapped fixed
argument head NP? feature to return the argument
type of gapped fixed argument (e.g. DIRECT OB-
JECT).
The unique case frame description is comple-
mented by verb classes. Verb classes are used to
describe such effects as: (1) modifier case slot com-
patibility, e.g. PROXIMAL verbs such as kaeru ?re-
turn? are compatible with target locative modifier
case slots; (2) case slot interaction, e.g. INTER-
PERSONAL verbs such as au ?meet? have two co-
indexed argument slots to indicate the interacting
parties; and (3) potential for valency-modifying al-
ternation, e.g. INCHOATIVE verbs such as kaisi-suru
?start? are listed with the (unaccusative) intansitive
case frame but undergo the causative-inchoative al-
ternation to produce transitive case frames (Jacob-
sen, 1992). A total of 27 verb classes are used in this
research, which incorporate a subset of the verbal
semantic attributes (VSAs) of Nakaiwa and Ikehara
(1997) as well as classes independently developed
for the purposes of this research.
Head noun semantics are used to morpho-
semantically classify the head noun (of the head
NP) into 14 classes (e.g. AGENTIVE, TEMPORAL,
FIRST-PERSON PRONOUN), based on the Goi-Taikei
noun taxonomy. Rather than attempting to disam-
biguate noun sense, the head noun semantic features
are determined as the union of all senses of the head
noun of the head NP. For coordinated head NPs,
we take the intersection of the head noun feature
vectors. One head noun semantic feature particular
to RCCs is the class of functional nouns (e.g. riyu?
?reason?, kekka ?result? and mokuteki ?objective?)
which generally give rise to attributive RCCs.
In processing each unit relative clause, we
carry out morphological analysis of the head
verb of the relative clause, returning a listing
of verb morphemes and tense/aspect affixes: e.g.
the verb okonawareteita ?to have been held? is
analysed as okona-ware-te-ita ?to hold-PASSIVE-
PROGRESSIVE-PAST?. This has applications in case
frame transformation (e.g. passivisation), as trig-
ger conditions in constructional templates, and in
the resolution of case frame ambiguity. Case frame
transformation is carried out prior to matching case
slots between the input and case frame, producing
a description of the surface realisation of the case
frame which reflects the voice, causality, etc. of the
main verb. Case frame transformation can poten-
tially produce fan-out in the number of clause anal-
yses, particularly in the case of the (r)are verb mor-
pheme, which has passive, potential/spontaneous
and honorific readings (Jacobsen, 1992). We pro-
duce all legal case frames in this case, and leave
the selection of the correct verb interpretation for
later processing. Note that the only morphological
verb feature to make an appearance as an indepen-
dent feature is POTENTIALITY, as it combines with
nominalised adjectives to produce COMPARATIVE
RCCs such as tob-eru hirosa (jump-POT size) ?(of)
size big enough to jump (in)?.
In addition to simple features, there are a number
of constructional templates, namely two features
for the attributive RCC types of EXCLUSIVE and IN-
CLUSIVE, and also one feature for idiomatic RCCs.
The constructional template for EXCLUSIVE RCCs
operates over the EXCLUDING verb class (contain-
ing nozoku ?to exclude?, for example), and stipu-
lates simple past or non-past main verb conugation
and the occurrence of only an accusatively-marked
case slot within the relative clause. The satisfaction
of these constraints results in the EXCLUSIVE RCC
compatibility feature being set, as occurs for:
(5) nitiyo?bi-o
Sunday-ACC
nozo-ku
exclude-PRES
mainiti
everyday
?every day except Sundays?
Idiomatic RCC templates constrain the lexical type
and modifiability of the head NP, verbal conju-
gation, case marker alternation and modifier case
slots/adverbials. A total of 11 templates are utilised
in the current system, which are mapped onto a sin-
gle feature value.
4 Analytical ambiguity and
disambiguation
As with any NLP task, ambiguity occurs at various
levels in the data. In this section, we outline sources
of ambiguity and propose disambiguation methods
for each.
4.1 Analytical ambiguity
Analytical ambiguity arises when multiple
clause analyses exist, as a result of verb ho-
mophony/homography or fixed expression compat-
ibility.
For the purposes of our system, verb ho-
mophony occurs when multiple verb entries in the
case frame dictionary share the same kana content
(and hence pronunciation), such that a kana-based
orthography will lead to ambiguity between the dif-
ferent entries. Verb homography, on the other
hand, occurs when multiple verb entries coincide in
kanji content, leading to ambiguity for a kanji-based
orthography. Both verb homophony and homogra-
phy can be either full or partial, i.e. all forms of a
given verb pair can be homophonous/homographic,
or there can be partial overlap for particular types
of verb inflection. For example, the verbs  
kawaru ?change? and  kawaru ?replace? are
fully homophonous, whereas 	 kiru ?wear? and


 kiru ?cut? are partially homophonous (e.g., in
the simple non-past they diverge in kana orthog-
raphy, producing kita and kitta, respectively). For
verb homography, 	 tomeru ?stop? and 
 yameru ?quit? are fully homographic, whereas

okonau ?carry out? and

iku ?go? are par-
tially homographic (with overlap produced for the
simple past tense, e.g., in the form of

, which
can be read as either okonatta or itta). Such over-
lap in lexical form leads to the situation of multiple
verb entries being triggered, producing independent
analyses for the RCC input.
Fixed expressions lead to analytical ambiguity
as, in most cases, the main verb of the expression
will also be compatible with productive usages, by
way of a generalised case frame entry. For example,
in addition to the fixed expression asi-o arau (foot-
ACC wash) ?quit?, arau ?wash? has a (unique) non-
lexicalised case frame entry, which will be compat-
ible with any lexical context satisfying the lexical
constraints on the fixed expression.
4.2 Resolving analytical ambiguity
Here, we present a cascaded system of heuristics
which resolves analytical ambiguity arising from
multiple verb entries, producing a unique feature
vector characterisation.
We select between multiple analyses for a given
relative clause in the first by preferring analyses
stemming from fixed expressions, over those con-
forming to constructional templates, in turn over
those generated through generalised techniques. We
define each such stratum as comprising a dis-
tinct expressional type, similarly to Ikehara et al
(1996).
Expressional type is on the whole a simple but
powerful disambiguation mechanism, but is not in-
fallible. The main area in which it comes unstuck
is in giving fixed expressions absolute priority over
other analyses. Many fixed expressions can also be
interpreted compositionally: e.g. asi-o arau (foot-
ACC wash) ?quit? can mean simply ?wash (one?s)
feet?. In the case of asi-o arau, the case frame
is identical between the fixed and generalised ex-
pression, but the verb classes are significanly differ-
ent, potentially leading to unfortunate side-effects
when trying to interpret an RCC involving the non-
idiomatic sense of the verb.
Fixed expressions and RCCs compatible with
constructional templates tend to be relatively rare,
so in most cases, ambiguity is not resolved through
expressional type preferences. In this case, we ap-
ply a succession of heuristics of decreasing relia-
bility, until we produce a unique analysis and fea-
ture vector characterisation. These heuristics are,
in order of application: minimum verb morpheme
content, best case frame match and representational
preference.
Minimum verb morpheme content involves de-
termining the morphemic content of the head verb
of the relative clause for each verb stem it is com-
patible with, and selecting the verb stem(s) which
are morphologically least complex. Morphologi-
cal complexity is determined by simply counting
the number of morphemes, auxiliary verbs and af-
fixes in the verb composite. Given the verb com-
posite  mieru e.g., we would generate two
analyses: mie-ru ?can see-PRES? and mi-e-ru ?see-
POT-PRES?, of which we would (correctly) select
the first. In essence, this methodology picks up on
more highly stem-lexicalised verb entries, and ef-
fectively blocks more compositional verb entries.
With best case frame match, we analyse the
degree of correspondence between the case frame
listed for each dictionary entry, and the actual case
slot content of the input. In following with the shal-
low processing objective of this research, we simply
calculate the number of case slots in the input which
align with case slots in each case frame (based on
case marker overlap), and divide this by the sum of
the case slots in the case frame and in the input. We
additionally add one to the numerator to give pref-
erence to case frames of lower valency (i.e. fewer
case slots) in the case that there is no overlap with
the input. This can be formalised as:
 	
 
	
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 577?584
Manchester, August 2008
Applying Discourse Analysis and Data Mining Methods to
Spoken OSCE Assessments
Meladel Mistica, Timothy Baldwin
The University of Melbourne
CSSE
{mmistica,tim}
@csse.unimelb.edu.au
Marisa Cordella, Simon Musgrave
Monash University
School of Languages, Cultures and Linguistics
{marisa.cordella,simon.musgrave}
@arts.monash.edu.au
Abstract
This paper looks at the transcribed data of
patient-doctor consultations in an exami-
nation setting. The doctors are interna-
tionally qualified and enrolled in a bridg-
ing course as preparation for their Aus-
tralian Medical Council examination. In
this study, we attempt to ascertain if there
are measurable linguistic features of the
consultations, and to investigate whether
there is any relevant information about
the communicative styles of the qualify-
ing doctors that may predict satisfactory
or non-satisfactory examination outcomes.
We have taken a discourse analysis ap-
proach in this study, where the core unit of
analysis is a ?turn?. We approach this prob-
lem as a binary classification task and em-
ploy data mining methods to see whether
the application of which to richly anno-
tated dialogues can produce a system with
an adequate predictive capacity.
1 Introduction
This paper describes our experimentation with ap-
plying data mining methods to transcribed doctor?
patient consultations. It is in essence a discovery
project: we apply methods to a field and task that
is not ordinarily associated with such approaches
in order to ascertain whether this could make for a
tractable learning task.
The task involves the extraction of dis-
course features from doctor?patient consultations
performed by international medical graduates
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
(IMGs), and what is known as ?simulated pa-
tients? (Vu et al, 1994), respectively. The IMGs
are enrolled in a bridging course in Melbourne as
preparation for their Australian Medical Council
(AMC) examination, the successful completion of
which is one of the pre-requisites to becoming a
fully accredited practitioner in Australia. This par-
tially replicates the AMC examination by studying
in detail how IMGs perform two objective struc-
tured clinical examinations (OSCEs). See Sec-
tion 2 for full details of the examination environ-
ment and participants involved.
The main questions raised when initiating this
study were:
? How objective is the testing?
? What is the importance placed on language
skills in OSCE environments?
? What makes for a successful OSCE?
In this research, we aim to build classifiers
that make reasonable predictions of the data being
tested, and possibly point us in the right direction
with respect to the questions above. From the clas-
sifiers we build, we also hope to ascertain which of
our features best predict a successful examination.
We organise the paper as follows. In Section 2,
we briefly describe the examination environment
and process, the marking scheme, and the partic-
ipants involved in the testing of the IMGs. We
also outline some of the issues that have arisen
with regard to the current methods of IMG testing.
In Section 3, we present details of the data used.
Section 4 describes the features we develop for the
task and discusses the reasoning behind the selec-
tion of features from a discourse analysis perspec-
tive. Section 5 discusses the results of the exper-
iments, with further examination of the data. The
577
last two sections, Sections 6 and 7, comprise a dis-
cussion of the results and concluding remarks.
2 Background
With Western nations becoming increasingly re-
liant on medical professionals trained overseas,
there is, in turn, a growing need to develop a re-
liable means of objectively assessing IMGs. The
shortage of medical doctors is a worldwide phe-
nomenon currently affecting many Western so-
cieties such as the UK, Canada, US and New
Zealand, which compete for the best medical prac-
titioners available around the world. Australia is
not immune to this global phenomenon, and in
the last two decades the shortage of local medi-
cal practitioners in Australia has worsened (Bir-
rell et al, 2004). Challenges to the healthcare
system in the country are particularly evident in
the areas of providing medical care for a grow-
ing elderly population and of servicing rural ar-
eas, where locally trained doctors do not feel par-
ticularly attracted to practise medicine (Han et al,
2006). Currently 35% of the rural medical work-
force and 20% of the total national medical work-
force consist of IMGs (Flynn, 2006). These figures
may increase even further in some regions (Spike,
2006), as preparation of fully educated and trained
local medical graduates takes up to thirteen years
to complete.
There is considerable disparity among IMGs in
their background training, clinical skills, under-
standing of the health system and communication
skills (McGrath, 2004). In order to be registered
to practice in Australia, IMGs must successfully
complete the Australian Medical Council exami-
nations and a period of supervised training. The
medical knowledge of IMGs is assessed in two
ways: by multiple choice examinations and by
clinical examinations. This second form of ex-
amination consists of a series of simulated medi-
cal consultations in which a role-player takes the
part of the patient, and the IMG?s professional
knowledge, lay-cultural knowledge, socio-cultural
assumptions, institutional norms, and values and
personal experiences are all in full display during
the unfolding of the medical event (Roberts et al,
2003). Whenever cultural factors are not shared
with their patients, the interpretative schema and
therefore the comprehension of speech are affected
by this lack of commonality in the participants?
inferences and contextual cues (Gumperz, 1999).
Such effects are likely to cause miscommunica-
tion in medical visits and have a potential nega-
tive effect on patients? satisfaction in the consul-
tation. Identification of the communication diffi-
culties faced by IMGs can therefore inform mod-
ifications to the training provided to IMGs when
they prepare for the Australian Medical Council
examinations, as well as suggesting more nuanced
and targeted procedures for assessing communica-
tive skills within those examinations, all with the
goal of working toward a better equipped medical
workforce for the future. The use of automated an-
alytic procedures to try to establish objective crite-
ria for communicative success is an important step
in this process.
Assessing language knowledge and competence
quantitatively is not a novel concept in second lan-
guage learning assessment. However, the applica-
tion of data mining methods to automatically as-
sess language proficiency in a discourse setting is
novel. Levow et al (1999) propose an architec-
ture to automatically assess language proficiency.
In their paper, they propose an architecture that
employs data mining methods, but do not build
classifiers over their spoken data to test this pro-
posal. A closely related line of research is on the
automatic classification of discourse elements to
assess the quality of a written genre (Burstein et
al., 2001). Like this work, it focuses on extracting
features from the discourse as a whole. But unlike
this study, the authors extract high level features,
such as rhetorical structure, of written discourse.
The study we present in this paper is rather unique
in its approach to language assessment.
3 Data
The data is taken from transcribed recordings of
examinations from students enrolled in a bridg-
ing course at Box Hill Hospital in Melbourne,
Australia. Each candidate was video-recorded en-
acting medical consultation scenarios with what
is known as a standardised or simulated patient
(SP). This method of testing is known as an ob-
jective structured clinical examination (OSCE),
which is an emulation of a doctor?patient consul-
tation, much like a role-play setting.
In this study, the role of the patient (SP) is en-
acted by a qualified doctor who follows a script,
and has well-defined ailment(s) and accompanying
concerns. Even though the SP assumes the same
ailment and disposition with all the candidates, the
578
interaction between the candidate and the SP is un-
cued and free-form. They simply present the infor-
mation in a standardised manner across all candi-
dates and perform the role of the patient as felici-
tously as possible.
For this set of examinations there are 2 types of
OSCE stations referred to as STD (sexually trans-
mitted disease ? genital herpes) and BC (bowel
cancer). The SP for the STD station is played by
a female doctor. The patient she plays has genital
herpes and is concerned about how this will affect
her chances of falling pregnant, and how this con-
dition may also affect her baby. The SP for the BC
station is played by an older male. A tumour is
discovered in his bowel lining and he is reluctant
to undergo any treatment because one of his good
friends suffered a similar condition and his quality
of life was severely diminished.
Even though the consultation is free to be nego-
tiated between doctor (candidate) and patient (sim-
ulated patient), each of the OSCEs cannot exceed
8 minutes, and is terminated by the examiner if it
does so.
3.1 Transcription
The recordings are transcribed in ELAN, a multi-
media annotation tool developed at theMax Planck
Institute, to help encode low-level linguistic fea-
tures such as overlapping and timing information.
The information and features extracted from the
discourse are largely based on a ?turn?.
Here we consider a turn as being normally domi-
nated by one speaker. It can be made up of multiple
intonation units. When there is backchannelling,
overlapping, or any interruption by the other par-
ticipant, then the turn is encoded as ending at the
end of the interrupted intonation unit. Otherwise,
transition pauses commonly signal turn changes,
unless latching occurs.
Given that the OSCE setting aims to emulate
as close as possible a real medical consultation,
this interaction, like all uncued spoken dialogues,
also has evidence of complicated turn-taking ne-
gotiations, disfluent and unintelligible speech, in-
terrupted speech, challenges for the floor, and the
like, all of which must be encoded and noted in
ELAN. Transcribing such data is not a trivial mat-
ter. In addition, transcribing the data in order to
extract these features is also a demanding task in
itself, which makes creating data for such tasks an
involved process.
Disfluencies and repairs are encoded in a limited
way, only by way of marking up truncated or un-
finished words. We also do not take a fine-grained
approach in encoding delaying strategies (Clark et
al., 2002), that is we do not differentiate whether
the uh or ah encoded represents lexical search, a
wish to hold the floor, a wish to give up the floor
or buying time to construct what to say next.
3.2 OSCE scoring
In an OSCE setting, candidates are given an over-
all pass or fail rating for each station by an OSCE
examiner observing the interaction. This overall
evaluation can be based on a number of perfor-
mance criteria which tests the candidates medical,
clinical and communication skills (Grand?Maison
et al, 1992). The OSCE marking scheme used for
this study consists of 5 assessable categories, as
follows:
APPROACH: the ability of the candidate to com-
municate with the patient;
HISTORY: the ability of the candidate to collect
medical history;
INTERPRETATION: how well does the candidate
interpret his or her investigation in order to
formulate an appropriate diagnosis;
MANAGEMENT: how well does the candidate
formulate a management plan for the diagno-
sis;
COUNSELLING: is the candidate able to give ap-
propriate counselling to the patient.
The first category tests language knowledge and
competency both at the lexical and discourse level,
while the remaining four categories test medical
knowledge and clinical competency.
4 Feature Engineering
We extracted a total of 38 features from the tran-
scribed data. Some of these features are based on
what is marked up according to the transcription
scheme, while others are based on timing informa-
tion or lexical information as encoded in ELAN.
These include features such as signals for delaying
speaking or hesitation (Clark et al, 2002), features
of conversational dominance (Itakura, 2000), the
manner in which turn-taking is negotiated (Sacks
et al, 1974), temporal features such as pausing (ten
Bosch et al, 2005), as well as our own features,
579
which include ?lexical introduction?, and ?lexical
repeat?.
In encoding features of conversational
dominance, we focus on participatory domi-
nance (Itakura, 2000), which looks at which
speaker contributes most to the dialogue in terms
of content.
Lexical introduction refers to a non-stop word
that is introduced by the doctor (IMG) or the pa-
tient (SP), while lexical repeat encodes how many
times a word introduced by the other interlocutor
is repeated by the speaker.
Almost all of the features developed are contin-
uous, based on timing information or word counts.
The only binary feature used encodes whether the
doctor initiates the consultation or not.
As mentioned in the previous section, the fea-
tures developed were largely based on turns. This
is to capture, along with other features such as
overlapping and pauses, the interactional aspect of
the communication. For example, conversational
cooperation and speaker reassurance can be cap-
tured with these features. Another aspect to the
development of these features, particularly for the
lexical-based features, is whether the IMG has a
suitable vocabularly and if they employ it appro-
priately in the interaction.
We arrive at 11 feature sets from which we build
our classifiers, as described in Table 1.
Not all features are exclusive to any one feature
set, that is, it is possible for a single feature to be-
long to a number of feature sets.
The sets were designed to isolate possible char-
acteristics of not only the discourse as a whole, but
how the participants negotiated their interaction.
These features sets were developed from observ-
ing each of the consultations with the expectation
that these were salient and determining features of
a successful examination.
5 Experiments
There was a total of 11 OSCE candidates, all of
whom performed an STD and a BC station, giv-
ing us in total 22 instances for this binary classi-
fication task to predict a pass or fail examination
result. Of the 22 instances, we had 5 failures and
17 passes. Given the small number of instances,
we maximised our dataset by employing 10-fold
stratified cross-validation, as well as leave-one-out
cross-validation which uses all but one instance in
training and the held-out instance for testing.
Feature set Example features
all - all 38 features
cooperation - overall word count
- length of interaction
- number of turns
hesitation - number of uh and ah
- number of unfinished words
overlap - number of overlapping words
- length of overlap (time)
pause - transition pauses
- within turn pauses
timeBased - all time-based features
turns - all turn-based features
- number of turns
- longest turn
- single word responses
uniqNrepeat - number of introduced content
words by each speaker
- number of times speaker
uses word introduced by other
wordBased - number of words in dialogue
- longest number of words
in a turn
patient - all SP-based features
doctor - all IMG-based features
Table 1: The 11 feature sets developed
The baseline system we use for comparison is
zero-R, or majority vote. For our supervised clas-
sifier, we employ a lazy learner in the form of the
IB1 algorithm implemented in WEKA.
5.1 Results for Feature Sets
Our initial classifiers held some promise. The clas-
sifier built from all of the features was equivalent
to the baseline system, and the combination of the
word-based features surpassed the baseline?s re-
sults, as shown in Table 2.
To evaluate our system, we employ simple clas-
sification accuracy, in addition to precision, recall
and F-score. Classification accuracy is the propor-
tion of correct predictions by the classifier, irre-
spective of class. Precision gauges how successful
the pass predictions of a given classifier are, while
recall gives us an indication of how successful a
given classifier is at identifying the candidates who
actually passed. Finally, F-score is a composite of
precision and recall, and gives us an overall perfor-
mance rating relative to passed candidates.
The least successful classifier was built on the
580
10-fold cross validation Leave-one-out cross validation
Feature set Accuracy Precision Recall F-score Accuracy Precision Recall F-score
baseline .773 .773 1.00 .872 .773 .773 1.00 .872
all .773 .773 1.00 .872 .773 .773 1.00 .872
cooperation .682 .789 .824 .806 .682 .778 .824 .800
hesitation .636 .737 .824 .778 .636 .737 .824 .778
overlap .773 .833 .882 .857 .773 .833 .882 .857
pause .682 .778 .824 .800 .727 .789 .882 .833
timeBased .500 .647 .688 .667 .545 .706 .706 .706
turns .727 .789 .882 .833 .727 .789 .882 .833
uniqNrepeat .636 .765 .765 .765 .682 .778 .824 .800
wordBased .864 .850 1.00 .919 .864 .850 1.00 .919
patient .727 .867 .765 .813 .727 .867 .765 .813
doctor .733 .800 .941 .865 .727 .789 .882 .833
Table 2: Classification results for STD and BC
feature set based on timing, which contains in-
formation such as the overall length of the dia-
logue, the overall length of transition pauses, in-
turn pauses and other time-based features. This
was most surprising because as a general observa-
tion, candidates who allowed extended pauses and
uncomfortable silences were those who seemed to
perform poorly, and those who did not leave too
many silences, and could maintain the flow of the
dialogue, seemed to perform well.
Given the small number of training instances
each classifier is based on, these first results were
somewhat encouraging. With respect to the base-
line, the overall performance of two of the sys-
tems equalled or surpassed the baseline in terms
of F-score. Most of the classifiers performed well
in terms of precision but less well in terms of re-
call, i.e. when the classifiers predicted a pass they
were generally correct, but there were significant
numbers of candidates who were predicted to have
failed but passed in practice.
5.2 Data Introspection Retrospectively
Although the results show promise, it was ex-
pected that more of the feature sets would return
more favourable results. The possible reasons why
the time-based features, and many of the other fea-
ture sets developed, did not perform as well as ex-
pected may have been because the features used in
building the classifiers could have been combined
in a better way, or because the data itself had too
many anomalies or was too disparate. We would
expect that extra data could iron out such anoma-
lies, but developing additional data is expensive
and more recordings are not always available. The
advantage of having a small dataset is that we are
able to do fine-grained annotation of the data, but
the obvious disadvantage is that we cannot easily
generate extra amounts of training data.
One very noticeable feature of the OSCE sta-
tions was that the STD SP had a very different
communicative style to that of the the BC SP.
Based on this observation we conducted tests given
the hypothesis that the possible bias in the data
could have stemmed from having two very differ-
ent testing approaches from the two SPs. In gen-
eral, the BC SP was more leading and in a sense
more forgiving with the candidates. In contrast to
this, the STD SP tended to be more felicitous in
her role as a patient, allowing awkward silences
and not prompting the candidates for further ex-
ploration.
We conduct the Mann-Whitney test, a rank sum
test, over the data in order to diagnose whether the
poor results were due to the distribution of the data
or whether the classifiers built with the selected
features were simply poor predictors. The Mann-
Whitney test ascertains whether there is a differ-
ence in the population mean of the two samples
given, without making any assumptions about the
distribution of the data.
We sub-sample the data in two ways in exam-
ining its homogeneity: (a) FAIL juxtaposed with
PASS candidates; and (b) BC juxtaposed with STD
stations. Test (a) essentially tests which exam-
inable category contributes the most to a pass or
fail outcome, whilst test (b) examines whether
there is an inherent difference in the way the test-
581
Category OVERALL APPROACH HISTORY INTERPRETATION MANAGEMENT COUNSELLING
z-score 1.84 1.21 -0.03 2.53 0.85 1.64
Table 3: Mann-Whitney z-score for BC and STD samples (OVERALL is the cumulative total of all 5
categories)
Category OVERALL APPROACH HISTORY INTERPRETATION MANAGEMENT COUNSELLING
z-score -3.29 -3.13 -2.43 -2.31 -2.31 -1.57
Table 4: Mann-Whitney z-score for failed and passed samples
ing was conducted between the BC and STD sta-
tions.
BC vs. STD
We use the ranking from the 5 assessable cate-
gories outlined in Section 3 and obtain the Mann-
Whitney z-score for each category. The z-score
gives us an indication of how disparate the two
separated datasets, BC and STD, are. The further
away from 0 the z-score is, the greater the evidence
that BC and STD data are not from the same pop-
ulation, and should be treated as such. The results
of this test, as seen in Table 3, show that these two
groups differ quite markedly: the candidates were
consistently marked differently for all assessable
categories except HISTORY. This is a striking pe-
culiarity because each candidate was tested in both
the STD and BC stations.
Based on the above, we can posit that the dis-
tinct testing styles of the STD and BC SPs were
the reason for our original lacklustre results, and
that the two data samples need to be treated sepa-
rately for the classifiers to perform consistently.
FAIL vs. PASS
In addition to the BC vs. STD test, we also test
how the failing candidates differ from the passing
candidates across the evaluation criteria.
The main idea behind this test is to see which
of the assessable categories contributed the most
in the overall outcome of the examination. For this
test, we would not expect the absolute z-score of
any of the assessment components to exceed the
absolute z-score of the OVERALL category given
that it is the cumulative scores of all categories.
The results in Table 4 suggest that APPROACH
correlates most highly with the pass/fail divide
in the OSCE assessments, followed by HISTORY,
then INTERPRETATION and MANAGEMENT, and
finally COUNSELLING. Recall that APPROACH is
the component that assesses language and commu-
nication skills. In particular, it assesses the style
and appropriateness of the way candidates con-
vey information, from lexical choice to display-
ing empathy through communication style. Given
that APPROACH correlates most strongly with the
assessment result, the decision to focus our fea-
ture engineering efforts on linguistic aspects of the
doctor?patient interaction would appear justified.
5.3 Results for STD & BC Data
Given the results from the Mann-Whitney tests re-
ported in the previous section, we separate the data
into two lots: those from the STD station, and
those from the BC station.
Even though there were very few instances in
the original dataset, we aim to see in these experi-
ments whether this separation improves the perfor-
mance of the classifiers. We build classifiers over
each dataset using the same features as before.
The results of the tests performed over the sep-
arated datasets, as shown in Table 5, show a big
improvement over the baseline for STD, while the
BC dataset is more problematic.
In the STD group, we see that four feature sets,
all, turns, wordBased and patient equal or surpass
the baseline F-score.
In contrast to this, upon examination of the
performance of the classifiers built over the BC
dataset, we do not observe any improvements over
the baseline and the results are markedly worse
than those for the combined dataset. Having said
this, when we combine the outputs of the two com-
ponent classifiers, the F-score for all features is
0.882, an improvement over the original combined
system.
6 Discussion
The OSCE assessment does not merely examine
the language skills of the candidates, but it also as-
582
BC STD
Feature Set Accuracy Precision Recall F-score Accuracy Precision Recall F-score
baseline .818 .818 1.00 .900 .727 .727 1.00 .842
all .727 .875 .778 .824 .909 .889 1.00 .941
cooperation .727 .800 .889 .842 .364 .571 .500 .533
hesitation .636 .778 .778 .778 .636 .700 .875 .778
overlap .727 .800 .889 .842 .636 .700 .875 .778
pause .818 .889 .889 .889 .545 .667 .750 .706
timeBased .636 .857 .667 .750 .545 .667 .750 .706
turns .636 .778 .778 .778 .818 .800 1.00 .889
uniqNrepeat .727 .800 .889 .842 .727 .778 .875 .824
wordBased .636 .778 .778 .778 .909 .889 1.00 .941
patient .727 .875 .778 .824 .818 .875 .875 .875
doctor .818 .818 1.00 .900 .455 .625 .625 .625
Table 5: Results for separated BC and STD datasets (leave-one-out)
sesses the efficacy of their communication skills in
conveying correct and accurate medical informa-
tion within a clinical setting. It can be seen from
Table 4 that there is a high correlation between the
overall pass or fail and the assessable category AP-
PROACH.
The examiners? subjectivity of overall perfor-
mance is minimised by the highly structured exam-
ination setup and well-defined assessment criteria.
However, as shown in Table 3, the communicative
style of the SP is a contributing factor to the per-
ception of successful clinical and communication
skills. The Mann-Whitney tests suggest that an
SP?s approach and their apparent satisfaction dur-
ing the clinical encounter can affect the judgement
of the examiner.
Additional inspection of the data revealed that
the assessment criteria which focused on language
and communication skills correlated highly with
an overall pass grade, moreso than the other cri-
teria. This seems to suggest that more emphasis
should be placed on language skills and communi-
cation style in the assessment of the candidates.
Assessing language competency is no trivial
matter, and capturing the linguistic features of di-
alogues in an attempt to define competence, as we
have done here, is a demanding task in itself. Al-
though many of our features were focused on turn-
taking, speaker response and interaction, we did
not develop features that encompass the informa-
tion structure of the communicative event.
It is assumed that miscommunication between
non-native and native speakers of a language is due
to a lack of language knowledge pertaining to syn-
tax, morphology or lexical semantics. However
many of these communication difficulties arise not
because of this lack of grammatical knowledge,
but through a difference in discourse styles or in-
formation structure as governed by different cul-
tures (Wiberg, 2003; Li, 1999).
Given that the word-based feature sets were
the most successful predictors of an OSCE out-
come, future work of this kind could make use of
medical-based lexicons to gauge whether technical
or non-technical word usage in such environments
is judged favourably. In addition, further work
should be done to test the hypothesis that informa-
tion structure or rhetorical structure does impact on
overall perception of a successful communication,
such as a variation on the methods employed by
Burstein et al (2001).
One obvious improvement to this study would
be to reduce the expense in producing the anno-
tated data. Future work could also be done in auto-
matically extracting features from non-transcribed
data, such as timing information based on pause
length and the turn length of each speaker.
7 Conclusions
In this research, we have built classifiers over
transcribed doctor?patient consultations in an at-
tempt to predict OSCE outcomes. We achieved
encouraging results based on a range of lexical and
discourse-oriented features.
In our first experiments, we combined the data
from two discrete stations in an attempt to max-
imise training data, and achieved modest results.
Subsequent analysis with the Mann-Whitney test
583
indicated both that success in the APPROACH cat-
egory correlates strongly with an overall success-
ful OSCE, and that the data for the two stations is
markedly different in nature. Based on this find-
ing, we conduct tests over the data for the individ-
ual stations with noticeable improvements to the
results.
The results of this exploratory study have been
quite encouraging, given the novel domain and
limited data. We have shown that a data mining
approach to OSCE assessment is feasible, which
we hope will open the way to increased interest in
automated medical assessment based on linguistic
analysis.
References
Birrell, Bob, Lesleyanne Hawthorne. 2004. Medicare
Plus and overseas trained doctors. People and Place,
12(2):83?99.
ten Bosch, Louis, Nelleke Oostdijk, Lou Boves. 2005.
On temporal aspects of turn taking in conversational
dialogues. Speech Communication, 47(2005):80?
86.
Burstein, Jill, Daniel Marcu, Slava Andreyev, Martin
Chodorow. 2001. Towards Automatic Classification
of Discourse Elements in Essays. ACL, 90-97.
Clark, Herber H., Jean E. Fox Tree. 2002. Using
uh and um in spontaneous speaking. Cognition,
84(2002):73?111.
Flynn, Joanna. 2006. Medical Release. Australian
Medical Council, 17(August 2006).
Grand?Maison, Paul, Jo?elle Lescop, Paul Rainsberry,
Carlos A. Brailovsky. 1992. Large-scale use of an
objective, structured clinical examination for licens-
ing family physicians. Canadian Medical Associa-
tion, 146(10):1735?1740.
Gumperz, John. 1999. On Interactional Sociolinguis-
tic Method. In Talk, Work and Institutional Order.
Discourse in Medical, Mediation and Management
Settings S. Sarangi and C. Robers (eds), 453?471.
Han, Gil-Soo, John .S Humphreys. 2006. Integratoin
and retention of international medical graduates in
rural communities. A typological analysis. The Aus-
tralian Sociological Association, 42(2):189?207.
Itakura, Hiroko. 2000. Describing conversational
dominance. Journal of Pragmatics, 33(2001):1859?
1880.
Levow, Gina-Anne, Mari Broman Olsen. 1999. Mod-
eling the language assessment process and result:
Proposed architecture for an automatic oral profi-
ciency assessment. Workshop On Computer Medi-
ated Language Assessment And Evaluation In Natu-
ral Language Processing.
Li, Han Zao. 1999. Comunication Information in Con-
versations: A Cross-cultural Comparison. Interna-
tional Journal of Intercultural Relations, 23(3):387?
409.
McGrath, Barry. 2004. Overseas-trained doctors. Inte-
gration of overseas-trained doctors in the Australian
medical workforce. The Medical Journal of Aus-
tralia, 181(11/12):640?642.
Roberts, Celia, Val Wass, Roger Jones, Srikant Sarangi,
Annie Gillett. 2003. A discourse analysis study
of ?good? and ?poor? communication in an OSCE:
a proposed new framework for teaching students.
Medical Education, 50:192?201.
Sacks, Harvey, Emanuel A. Schegloff, Gail Jefferson.
1974. A Simplest Systematics for the Organiza-
tion of Turn-Taking for Conversation. Language,
50(4):696?735.
Spike, Neil. 2006. International Medical Graduates:
The Australian perspective. Acad Med, 81):842?
846.
Vu, Nu Viet, Howard S. Barrows. 1994. Use of Stan-
dardized Patients in Clinical Assessments: Recents
Developments and Measurement Findings. Educa-
tional Researcher, 23(3):23?30.
Wiberg, Eva. 2003. Interactional context in L2 dia-
logues. Journal of Pragmatics, 35(2003):389?407.
584
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1041?1048
Manchester, August 2008
Measuring and Predicting Orthographic Associations:
Modelling the Similarity of Japanese Kanji
Lars Yencken and Timothy Baldwin
{lljy,tim}@csse.unimelb.edu.au
NICTA Research Lab
University of Melbourne
Abstract
As human beings, our  mental  processes
for  recognising  linguistic  symbols  gen-
erate  perceptual  neighbourhoods  around
such symbols where confusion errors oc-
cur. Such  neighbourhoods  also  pro-
vide  us  with  conscious  mental  associa-
tions between symbols. This  paper  for-
malises orthographic models for similarity
of Japanese kanji, and provides a proof-
of-concept dictionary extension leveraging
the mental associations provided by ortho-
graphic proximity.
1 Introduction
Electronic dictionary interfaces have evolved from
mere digitised forms of their paper ancestors. They
now enhance accessibility by addressing the sep-
arate needs of language consumers and language
producers, of learners from non-speakers to native
speakers, and by targeting the specific difficulties
presented by individual languages.
For languages with logographic orthographies,
such  as  Japanese  and  Chinese, accessibility  re-
mains poor due to the difficulties in looking up
an unknown character in the dictionary. The tra-
ditional method of character lookup in these lan-
guages involves identifying the primary compo-
nent (or ?radical?), counting its strokes, looking it
up in the index, counting the remainder of strokes
in the original character, then finding the character
in a sub-index. This presents several opportunities
for error, but fortunately improvements have been
made, as we discuss in Section 2.
We are interested in the perceptual process of
identifying characters, in particular the behaviour
of perception within dense visual neighbourhoods.
Within the dictionary accessibility space, we are
?2008. Licensed  under  the Creative  Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
motivated by the potential to correct confusion er-
rors, but also to leverage the mental associations
provided by visual proximity to allow advanced
learners  to  find  unknown characters  faster. As
proof-of-concept, we propose a method for look-
ing up unknown words with unfamiliar characters,
based on similarity with known characters.
In  essence, our  method  is  based  on  the  user
plausibly ?mistyping? the word based on closely-
matching kanji they are familiar with (and hence
can readily access via a standard input method ed-
itor), from which we predict the correct kanji com-
bination based on kanji similarity and word fre-
quency. For example, given the input ??, the
system could suggest the word?? [hosa] ?help?
based on similarity between the high-frequency?
and the graphically-similar but low-frequency?.
The  proposed  method  is  combined  with  the
FOKS lookup strategy proposed by Bilac (2002)
for looking up unknown words via plausibly incor-
rect readings.
The contributions of this paper are the proposal
of a range of character similarity models for logo-
graphic scripts, a novel evaluation method for lo-
gographic character confusability, and the incorpo-
ration of kanji similarity into a word-level lookup
model.
The remainder of this paper is structured as fol-
lows. Firstly, we review related lookup systems
(Section 2), and go on to discuss how we measure
and model kanji similarity, including an evalua-
tion of the methods (Section 3). We then focus on
the conversion of similarity models into confusion
models, and their integration into a search interface
(Section 4). Examining both our models and the
interface itself, we discuss our findings (Section 5)
before finally concluding (Section 6).
2 A review of related systems
2.1 Associative lookup systems
Associative  lookup  systems  are  based  on  the
premise that characters and words form a highly
1041
connected lexical network. They focus on finding
and making accessible the mental links provided
by proximity within this network. In contrast, sys-
tems which correct for confusion model plausible
errors in order to recover from them. Examples of
associative systems are as follows:
Semantic Orthographic
(for producers) (for consumers)
Monolingual
Visual WordNet
this paper
Bilingual
standard  bilingual
dictionaries
Pinyomi  dictio-
nary interface
Ferret and Zock (2006) introduce the distinction
between language producers as encoders of seman-
tic  information, and language consumers  as  de-
coders of orthographic (or phonetic) information.
We first consider systems to aid production of lan-
guage.
Systems for production give form and sound to
known semantics. The most common such systems
are bilingual dictionaries which associate words in
one language with their near-synonyms in a second
language. Even within a monolingual context, the
problem of selecting the right word can be difficult,
whether the difficulty is one of limited knowledge
or simply one of access, as in the case of the tip-
of-the-tongue problem. Work in this area (Zock,
2002; Zock and Bilac, 2004) has more recently fo-
cused on extending WordNet with syntagmatic re-
lationships (Ferret and Zock, 2006). Access could
take the form of the Visual WordNet Project.
1
For language consumers, the challenge is to find
the meaning or sound of a word with known form.
For logographic languages, where characters are
entered phonetically
2
using an input method ed-
itor, computer  input  of  an  unknown word  with
known form remains difficult, since the reading is
unknown.
In  a  bilingual  context, the  Pinyomi  Chinese-
Japanese dictionary interface overcomes this ob-
stacle by allowing Japanese speakers to look up
a Chinese word via the Japanese-equivalent char-
acters based on orthographic associations between
similar characters (Yencken et al, 2007).
Our proposed extension to the FOKS dictionary
is functionally similar to Pinyomi, but in a mono-
lingual Japanese context. In our case, a Japanese
word containing unknown characters is found by
1http://kylescholz.com/projects/wordnet/
2
A notable exception is the Wubizixing lookup method for
Chinese.
querying with known characters that are visually
similar. Unlike Pinyomi, which uses an ideogram
transliteration table to determine associations, we
use direct models of character similarity to deter-
mine associations.
2.2 Kanji lookup systems
We next provide a brief review of five kanji lookup
systems in order to situate our proposed interface
appropriately.
The SKIP (System of Kanji Indexing by Pat-
terns)  system  of  lookup  provides  an  indexing
scheme based on a kanji?s overall shape rather than
its primary radical (Halpern, 1999). For example,
? [aka] ?bright? has skip code 1-4-4, with the first
number indicating it is horizontally split into two
parts, and the second and third numbers represent-
ing the respective stroke counts of the two parts.
The Kansuke dictionary simplifies the method of
counting strokes, to form a three-number code rep-
resenting the horizontal, vertical and other strokes
that make up a character (Tanaka-Ishii and Godon,
2006). Characters can also be looked up from their
components. For our earlier example ? consists
of? with code 3-2-0 and? with code 3-1-1.
The  Kanjiru  dictionary  (Winstead, 2006)  at-
tempts  to  interactively  assemble  a  character  by
shape and stroke via mouse movements, providing
the user with structural ways of building up com-
ponents until the desired character is found.
Finally, hand-writing interfaces attempt to cir-
cumvent the computer input problem altogether,
but still suffer from several issues: the awkward-
ness of mouse input for drawing characters; sensi-
tivity to both stroke order and connectivity of com-
ponents; and the difference in hand-writing styles
between learners and native speakers.
These lookup methods contrast  with our pro-
posed similarity-based search in several ways.
Firstly, our  method  combines  word-  and
character-level information directly, yet provides
the means to lookup words with unknown charac-
ters without the use of wildcards. The downside to
this is that the user needs to use kanji in the search
query, limiting potential users to intermediate and
advanced learners with some knowledge of kanji.
Secondly, we are able to cater to both intentional
similarity-based searches, and unintentional input
errors, increasing the accessibility of the base dic-
tionary. This approach shares much with the FOKS
dictionary interface (Bilac, 2002), which provides
1042
error-correcting lookup for reading-based dictio-
nary queries. Suppose, for example, a user wishes
to look up the word ?? ?festival float?, but is
unsure of its pronunciation. FOKS allows them
to guess the pronunciation based on readings they
know for each character in other contexts. In this
case, they might combine ? [yama] ?mountain?
and ? [kuruma] ?car? and guess the word read-
ing as [yamakuruma]. The correct reading [dashi]
cannot be guessed from the word?s parts, but our
educated guess would lead the user to the word
and provide access to both the correct reading and
meaning.
This  approach  is  complementary  to  our  pro-
posed method. Suppose, analogously, that the user
wishes to look up the word?? but is unfamiliar
with the first kanji. A query for ?? would trig-
ger an inference based on the similarity between?
and?, and provide the desired word in the results,
allowing the user to determine both its pronuncia-
tion [h?moN] and its meaning ?visit?.
3 Modelling similarity
3.1 Metric space models
There has been little work on methods for measur-
ing or predicting the similarity between two kanji.
While there have been many psycholinguistic stud-
ies  on  various  specific  aspects  of  perception  of
Chinese and Japanese logographic characters, few
touch directly on orthographic confusion. For a
brief discussion, see Yencken and Baldwin (2006).
Broadly, current  literature  suggests  that  kanji
recognition may be hierarchical, building radicals
from strokes, and whole characters from radicals.
Each point of recognition and combination sug-
gests a potential site for misrecognition or confu-
sion with an orthographic or semantic neighbour.
The most directly relevant study involved two
experiments  by Yeh  and  Li  (2002). In  a  sort-
ing task, subjects tended to categorise characters
by their structure, rather than their shared compo-
nents. In a subsequent search task, presence of
shared structure between target and distractors was
the dominant factor in subjects? response times.
We previously proposed two naive kanji similar-
ity measures: a cosine similarity metric operating
on boolean radical vectors, and the l
1
norm (Man-
hattan distance) between rendered images of kanji
(Yencken and Baldwin, 2006). Evaluating on a
set of human similarity judgements, we determined
that the cosine similarity method outperformed the
d
stroke
d
tree
d
radical
?
?
????
????
l
1
?
?
?
?
3, 11a, 2a, 2a
3, 11a, 2a, 2a, 2a
?
? ?
? ?
? ? ?
? ? ? ? ? ?
?
? ?
? ?
? ? ?
? ? ? ? ? ?
? ?
Figure 1: A summary of our kanji distance metrics
l
1
norm, although it had lower precision for high-
similarity pairs.
3.1.1 Bag of radicals with shape
When learners of Japanese study a new charac-
ter, they do not study its strokes in isolation, but
instead build on prior knowledge of its component
radicals. For example, ? [aka] ?bright? could be
analysed as being made up of the? [sun] ?hi? and
? [moon] ?tsuki? radicals.
Radicals are useful in several ways. The num-
ber of radicals in any kanji is much smaller than
the number of strokes for any kanji, making such
kanji easier to chunk and recall in memory. Fur-
thermore, radicals can provide cues to the mean-
ing and pronunciation of characters which contain
them.
3
The original metric used in Yencken and Bald-
win (2006) simply calculates the cosine similarity
between radical vectors. This ignores the position
of radicals, which is known to be important in sim-
ilarity judgements, and also the number of times
each radical occurs within a kanji. Hence, ?, ?
and ? are all considered identical (radical = ?),
as are? and? (radical =?). The metric is cal-
3
For example, kanji containing the radical ?, such as?
[mune] ?chest? and? [ude] ?arm?, are reliably body parts.
Kanji containing the radical ?, as in ? [d?] ?copper? and
? [d?] ?body?, often have the Chinese or on reading [d?]
amongst their valid pronunciations.
1043
culated by:
d
radical
(x,y) = 1? rx ? ry
|rx||ry|
(1)
To address radical multiplicity, and the findings
of Yeh and Li?s study, we set the above metric to
unit distance whenever the two characters differ
in their basic shape. To approximate shape, we
use the first part of each kanji?s 3-part SKIP code.
which can take values horizontal, vertical, contain-
ment or other. SKIP codes for each kanji are pro-
vided in Kanjidic,
4
and radical membership in the
Radkfile.
5
This change allows the metric to distinguish be-
tween examples with repeated components. The
altered metric aims to capture the visual and se-
mantic salience of radicals in kanji perception, and
to also take into account some basic shape similar-
ity.
3.1.2 Distance of rendered images
In contrast to the previous approach, we can con-
sider kanji as arbitrary symbols rendered in print or
on screen, and then attempt to measure their sim-
ilarity. The simplest way to do this is to simply
render each kanji to an image of fixed size, and to
then use some distance metric over images.
A common and simple distance metric is the l
1
norm, which simply sums the difference in lumi-
nance between pixels of the two images for some
alignment. Fortunately, all kanji are intended to
occupy an identically sized block, so alignment is
via a grid, constant across all kanji. Considering
px(i, j) to be the luminance of the pixel at position
(i, j) of rendered kanji x, we evaluate the l
1
norm
as follows:
l
1
(x,y) =
?
i,j
|px(i, j)?py(i, j)| (2)
This calculation depends on the image representa-
tion chosen, and could differ slightly across fonts,
image sizes and rasterisation methods. We used the
MS Gothic font, rendering to 80x80 images, with
anti-aliasing.
This metric is  aimed at  capturing the general
overlap  of  strokes  between  the  two  characters,
along with the overlap of whitespace, which gives
useful structure information. This metric is known
to be noisy for low-to-medium similarity pairs, but
is very useful in distinguishing near neighbours.
4http://www.csse.monash.edu.au/~jwb/kanjidic.html
5http://www.csse.monash.edu.au/~jwb/kradinf.html
3.1.3 Stroke edit distance
A third possibility is to reduce kanji to the very
strokes used to write them. Two features of the or-
thography make this possible: (1) kanji are not ar-
bitrary symbols, but configurations of strokes cho-
sen from within a finite and limited set; and (2)
each kanji has a precise stroke order which is con-
sistent for reused kanji components, such that if
two or more arbitrary components were combined
to form a new pseudo-character, native speakers
would largely agree on the stroke order.
To define a metric based on strokes, we need
both  a  source  of  stroke  data  and  a  comparison
method. For stroke data, we look to a hierarchi-
cal data set for Japanese kanji created by Apel and
Quint (2004). Each kanji is specified by its strokes,
grouped into common stroke groups (components),
and broken down in a hierarchical manner into rel-
ative positions within the kanji (for example: left
and right, top and bottom). The strokes themselves
are based on a taxonomy of some 26 stroke types
(46 including sub-variants).
For any given kanji, we can flatten its hierarchy
to generate an ordered sequence of strokes: a sig-
nature for that character. The natural distance met-
ric across such sequences is the string edit distance.
This forms our d
stroke
metric.
Much  useful  information  is  preserved  within
stroke signatures. Since radicals are written in se-
quence, they form contiguous blocks in the signa-
ture. The edit distance will thus align shared radi-
cals when their position is similar enough. Since
components are usually drawn in a left-to-right,
top-to-bottom order, the order of components in
a signature also reflects their position as part of
the larger character. Finally, it provides a smooth
blending from stroke similarity to radical similar-
ity, and can recognise the similarity between pairs
like? [hi] ?sun? and? [me] ?eye?.
3.1.4 Tree edit distance
In our previous approach, we discarded much of
the hierarchical information available, relying on
stroke order to approximate it. We can instead use
the full data, and calculate the ordered tree edit dis-
tance between kanji XML representations. Tree
edit distance is defined as the length of the short-
est sequence of inserts, deletions and relabellings
required to convert  one tree into another (Bille,
2005). Though a cost function between labels can
be specified, we gave inserts/deletions and rela-
bellings unit cost.
1044
Figure 1 provides an overview of the structure
of each kanji?s representation. Actual trees also
contain  phonetic  elements, radicals, and  stroke
groups  whose  strokes  are  spread  across  several
non-contiguous blocks. Another motivation for in-
cluding tree edit distance is to determine if this ad-
ditional information is useful in determining kanji
similarity.
3.2 Evaluation
We evaluate our distance metrics over three data
sets.
The first data set is the human similarity judge-
ments from Yencken and Baldwin (2006). This
data set is overly broad in that it weights the abil-
ity to distinguish low and medium similarity pairs
equally with distinguishing medium and high sim-
ilarity pairs. It is clear that for most applications,
determining the high similarity pairs with high pre-
cision is most important. Nevertheless, this data set
is useful for comparing our metrics with those pro-
posed in previous research.
In order to better measure performance on high-
similarity pairs, which we expect to form the basis
of incorrect kanji inputs, we need a set of human-
selected confusion data. The second data set is
drawn from the White Rabbit JLPT Level 3
6
kanji
flashcards. Each flashcard contains either one or
two highly-similar neighbours which might be con-
fused with a given kanji. We use this set to deter-
mine our likely performance in a search task.
Our third data set is based on human confusabil-
ity judgements for kanji pairings.
3.2.1 Similarity experiment data
The first data consists of human similarity judge-
ments to pairs of kanji, scored on a 5 point scale
(Yencken and Baldwin, 2006). The experiment
had 179 participants, covering a broad range of
Japanese proficiency. The key participant group-
ings are: (1) non-speakers of Chinese, Japanese or
Korean (Non-CJK); (2) Japanese second-language
learners  (JSL);  and  (3)  Japanese  first-language
speakers (JFL). Figure 2 gives the rank correlation
? between each metric and a rater, averaged over
all raters in each proficiency group.
For  each  metric, the  mean  rank  correlation
increased  with  the  participants?  knowledge  of
6
Japanese Language Proficiency Test: the standard gov-
ernment test for foreigners learning Japanese.
Non-CJK JSL JFL
0
0.175
0.350
0.525
0.700
Metric agreement within rater groups
d
radical
?SHAPE
d
radical
+SHAPE
l
1
d
tree
d
stroke
Figure 2: Mean value of Spearman?s rank correlation ? over
rater groups for each metric (d
radical
(?shape) is the original
metric, and d
radical
(+shape) is our augmented version)
Japanese (from Non-CJK to JSL to JFL), indicat-
ing that the raters made more motivated and consis-
tent similarity judgements. The d
radical
(+shape)
metric dominates the other metrics, including the
original d
radical
(?shape), at all levels of knowl-
edge. This confirms the salience of radicals and the
tendency for individuals to classify kanji by their
broad shape, as suggested by Yeh and Li (2002).
l
1
, d
stroke
and d
tree
perform poorly in comparison.
Interestingly, these three metrics have large per-
formance differences for non-speakers, but not for
native-speakers.
Despite overall poor performance from our new
metrics, we were able to improve on the original
d
radical
(?shape). We now evaluate over the flash-
card data set for comparison.
3.2.2 Flashcard data set
The flashcard data differs greatly from the previ-
ous experimental data, as it consists of only human-
selected  high-similarity  pairs. Accordingly, we
took two approaches to evaluation.
Firstly, for  each  high-similarity  pair  (a pivot
kanji and its distractor), we randomly select a third
kanji from the j?y? character set
7
and combine it
with the pivot to form a second pair which is highly
likely to be low similarity. We then compare how
well each metric can classify the two pairs by im-
posing the correct ordering on them, in the form of
classification accuracy. The results of this evalua-
tion are shown in Table 1. We include a theoretical
random baseline of 0.500, since any decision has a
7
The  ?common  use?  government  kanji  set, containing
1945 characters.
1045
Metric Accuracy
d
tree
0.979
d
stroke
0.968
l
1
0.957
d
radical
0.648
random baseline 0.500
Table 1: Accuracy at detecting which of two pairs (flashcard
vs. random) has high similarity
Metric MAP p@1 p@5 p@10
d
stroke
0.594 0.313 0.151 0.100
d
tree
0.560 0.313 0.149 0.094
l
1
0.503 0.257 0.139 0.089
d
radical
0.356 0.197 0.087 0.063
Table 2: The mean average precision (MAP), and precision at
N ? {1,5,10} over the flashcard data
50% a priori chance of being successful.
The performance of d
radical
? close to our ran-
dom baseline, despite performing best in the pre-
vious task ? is suggestive of the different charac-
teristics of this task. In particular, a metric which
orders well across the broad spectrum of similarity
pairs may not be well suited to identifying high-
similarity pairs, and vice-versa.
The other  three  metrics  have  accuracy above
0.95 on this task, indicating the ease with which
they can identify high-similarity pairs. However,
this does not guarantee that the neighbourhoods
they generate will  be free from noise, since the
real-world prevalence of highly similar characters
is likely to be very low.
To better determine what dictionary search re-
sults  might  be  like, we consider  each flashcard
kanji as a query, and its high-similarity distractors
as relevant documents (and implicitly all remaining
kanji as irrelevant documents, i.e. dissimilar char-
acters). We can then calculate the Mean Average
Precision (MAP, i.e. the mean area under the preci-
sion?recall curve for a query set) and the precision
at N neighbours, for varied N . The results of this
approach are presented in Table 2.
The precision statistics confirm the ranking of
metrics found in the earlier classification task. The
d
stroke
metric outperforms l
1
by a greater margin in
the MAP statistic and precision at N = 1, but nar-
rows again for greater N . This suggests that it is
more reliable in the upper similarity ranking.
3.2.3 Distractor pool experiment
The flashcard data provides good examples of
high-similarity pairs, but suffers from several prob-
lems. Firstly, the constraints of the flashcard for-
mat limit the number of high-similarity neighbours
that can be presented on each flashcard to at most
two; in some cases we might expect more. Sec-
ondly, the  methodology behind  the  selection  of
these high-similarity neighbours is unclear.
For these reasons, we conducted an experiment
to attempt to replicate the flashcard data. 100 kanji
were randomly chosen from the JLPT 3 set (here-
after pivots). For each pivot kanji, we generated a
pool of possible high-similarity neighbours in the
following way. Firstly, we seeded the pool with
the neighbours from the flashcard data set. We then
added the highest similarity neighbour as given by
each of our similarity metrics. Since these could
overlap, we iteratively continued adding an addi-
tional neighbour from all of our metrics until our
pool contained at least four neighbours.
Native or native-like speakers of Japanese were
solicited as participants. After a dry run, each par-
ticipant was presented with a series of pivot kanji.
For each pivot kanji, they were asked to select from
its pool of neighbours which (if any) might be con-
fused for that kanji based on their graphical simi-
larity. The order of pivots was randomised for each
rater, as was the order of neighbours for each pivot.
Kanji were provided as images using MS Gothic
font for visual consistency across browsers.
Three  participants  completed  the  experiment,
selecting 1.32 neighbours per pivot on average, less
than 1.86 per pivot provided by the flashcard data.
Inter-rater agreement was quite low, with a mean
? of 0.34 across rater pairings, suggesting that par-
ticipants found the task difficult. This is unsurpris-
ing, since as native speakers the participants are
experts at discriminating between characters, and
are unlikely to make the same mistakes as learners.
Comparing their judgements to the flashcard data
set yields a mean ? of 0.37.
Ideally, this data generates a frequency distribu-
tion over potential neighbours based on the num-
ber of times they were rated as similar. However,
since the number of participants was small, we sim-
ply combined the neighbours with high-similarity
judgements for each pivot, yielding an average of
2.45 neighbours per pivot. Re-evaluating our met-
rics on this data gives the figures in Table 3.
1046
Metric MAP p@1 p@5 p@10
d
stroke
1.046 0.530 0.228 0.146
d
tree
1.028 0.540 0.228 0.136
l
1
0.855 0.480 0.200 0.117
d
radical
0.548 0.270 0.122 0.095
Table 3: The mean average precision (MAP), and precision at
N ? {1,5,10} over the distractor data
Compared to the flashcard data set, the ordering
and relative performance of metrics is similar, with
d
stroke
marginally improving on d
tree
, but both sig-
nificantly outperforming l
1
and d
radical
. The near-
doubling of high similarity neighbours from 1.32
to 2.45 is reflected by a corresponding increase in
MAP and precision@N scores, though the effect
is somewhat reduced as N increases.
4 From similarity to search
Having examined several character distance met-
rics, and evaluated them over our three data sets,
we now consider  their  application  to  dictionary
word search.
4.1 Overall model
Our broad probability model for looking up words
based on similar kanji  is  identical  to the FOKS
model for search based on readings, save that we
substitute readings for kanji in our query. A uni-
gram approximation leads us to Equation 3 below,
where q = q
0
. . . qn is the query given by the user,
w = w
0
. . .wn is the desired word, and each qi and
wi is a kanji character:
Pr(w|q) ? Pr(w)Pr(q|w)
= Pr(w)
?
i
Pr(qi|w,q0 . . . qi?1)
? Pr(w)
?
i
Pr(qi|wi) (3)
The final line of Equation 3 requires two models
to be supplied. The first, Pr(w), is the probability
that a word will be looked up. Here we approxi-
mate using corpus frequency over the Nikkei news-
paper data, acknowledging that a newspaper cor-
pus is skewed differently to learner data. The sec-
ond model is our confusion model Pr(qi|wi), inter-
preted either as the probability of confusing kanji
wi with kanji qi, or of the user intentionally select-
ing qi to query for wi. It is this model that we now
focus on.
4.2 Confusion model
Although we can construct a confusion model us-
ing our distance metric alone, it is clear that fre-
quency effects will occur. For example, the like-
lihood of confusion is increased if the target wi is
rare and unknown, but qi is a highly-similar high-
frequency neighbour; certainly this is a typical use
case for intentional similarity-based querying. We
thus propose a generic confusion model based a
similarity measure between kanji:
Pr(qi|wi)?
Pr(qi)s(qi,wi)
?
j Pr(qi,j)s(qi,j ,wi)
(4)
The  confusion  model  uses  a  similarity  function
s(qi,wi) and a kanji frequency model Pr(qi) to de-
termine the relative probability  of  confusing wi
with qi amongst  other  candidates. We convert
the desired distance metric d into s according to
s(x,y) = 1? d(x,y) if the range of d is [0,1], or
s(x,y) = 1
1+d(x,y) if the range of d is [0,?).
To maximise the accessibility of this form of
search, we must find the appropriate trade-off be-
tween providing sufficient candidates and limiting
the noise. We use a thresholding method borrowed
from Clark and Curran (2004), where our thresh-
old is set as a proportion of the first candidate?s
score. For example, using 0.9 as our threshold, if
the first candidate has a similarity score of 0.7 with
the target kanji, we would then accept any neigh-
bours with a similarity greater than 0.63. Using
the d
stroke
metric with a ratio of 0.9, there are on
average 2.65 neighbours for each kanji in the j?y?
character set.
4.3 Evaluating search
Search by similar grapheme has an advantage to
search by word reading: reading results are natu-
rally ambiguous due to homophony in Japanese,
and attempts to perform error correction may in-
terfere with exact matches in the results ranking.
Grapheme-based search may have only one exact
match, so additional secondary candidates are not
in direct competition with existing search practices.
We can estimate the accessibility improvement
given by this form of search as follows. Let us
assume that learners study kanji in frequency or-
der. For each kanji learned, one or more high-
similarity neighbours also become accessible. Tak-
ing all pairings of kanji within the JIS X 0208-
1990 character set, using the d
stroke
metric with a
cutoff ratio of 0.9, and assuming full precision on
the neighbour graph this generates, we get the ac-
cessibility curve found in Figure 3. Our baseline
is a single kanji accessible for each kanji learned.
1047
01500
3000
4500
6000
50 400 800 12501750225027503250375042504750
Accessibility of similarity search
#
 
k
a
n
j
i
 
a
c
c
e
s
s
i
b
l
e
# kanji known
baseline accessible by search
Figure 3: The accessibility improvement of kanji similarity
search
Our actual precision makes the proportion of us-
able neighbours smaller; we will thus need to ex-
pose the user to a larger set of candidates to get this
level of improvement. Improvements in precision
and recall are still needed to reduce noise.
5 Discussion and future work
A current  difficulty  in  evaluating  this  form  of
search is the lack of available query data to objec-
tively evaluate the search before deployment. This
restricts evaluation to longer-term post-hoc analy-
sis based on query logs. Such logs will also provide
additional real-world similarity and confusion data
to improve our metrics.
This form of search is directly extensible to Chi-
nese, and is limited only by the availability of char-
acter data. Indeed, preliminary similarity models
for Chinese already exist (Liu and Lin, 2008). Our
similarity modelling may also suggest approaches
for more general symbol systems that lack ade-
quate indexing schemes, for example heraldry.
There is much potential in the adaption of dic-
tionaries as drill tutors in the context of language
learning (Zock and Quint, 2004). The models pre-
sented in this paper could provide dynamic kanji
drills, to aid early learners to distinguish similar
kanji and provide challenge more advanced learn-
ers.
6 Conclusion
We have proposed a method of searching the dictio-
nary for Japanese words containing unknown kanji,
based on their visual similarity to familiar kanji.
In order to achieve this, we have considered sev-
eral metrics over characters, improved on existing
baselines and evaluated further over a flashcard set.
Of these metrics, the edit distance taken over stroke
descriptions performed the best for high-similarity
cases, and was used to construct similarity-based
search at the word level.
References
[Apel and Quint2004] Apel, Ulrich and Julien Quint. 2004.
Building a graphetic dictionary for Japanese kanji ? char-
acter look up based on brush strokes or stroke groups, and
the display of kanji as path data. In Proc. COLING 2004,
Geneva, Switzerland.
[Bilac2002] Bilac, Slaven. 2002. Intelligent dictionary inter-
face for learners of Japanese. Master?s thesis, Tokyo Insti-
tute of Technology.
[Bille2005] Bille, Philip. 2005. A survey on tree edit dis-
tance and related problems. Theoretical Computer Sci-
ence, 337(1-3):217?239.
[Clark and Curran2004] Clark, Stephen and James R. Curran.
2004. The importance of supertagging for wide-coverage
CCG parsing. In Proc. COLING 2004, page 282?288,
Geneva, Switzerland.
[Ferret and Zock2006] Ferret, Olivier  and  Michael  Zock.
2006. Enhancing electronic dictionaries with an index
based on associations. In Proc. COLING/ACL 2006, pages
281?288, Sydney, Australia.
[Halpern1999] Halpern, Jack, editor. 1999. The  Kodan-
sha Kanji Learner?s Dictionary. Kodansha International,
Tokyo.
[Liu and Lin2008] Liu, Chao-Lin and Jen-Hsiang Lin. 2008.
Using structural information for identifying similar Chi-
nese characters. In Proc. ACL 2008: HLT, Short Papers
(Companion Volume), pages 93?96, Columbus, Ohio.
[Tanaka-Ishii and Godon2006] Tanaka-Ishii, Kumiko and Ju-
lian Godon. 2006. Kansuke: A kanji look-up system based
on a few stroke prototype. In Proc. ICCPOL 2006, Singa-
pore.
[Winstead2006] Winstead, Chris. 2006. Electronic kanji dic-
tionary based on ?Dasher?. Proc. IEEE SMCals 2006 ,
pages 144?148, Logan, USA.
[Yeh and Li2002] Yeh, Su-Ling  and  Jing-Ling  Li. 2002.
Role  of  structure  and  component  in  judgements  of  vi-
sual similarity of Chinese characters. Journal of Experi-
mental Psychology: Human Perception and Performance,
28(4):933?947.
[Yencken and Baldwin2006] Yencken, Lars  and  Timothy
Baldwin. 2006. Modelling  the  orthographic  neigh-
bourhood for  Japanese kanji. In Proc.  ICCPOL 2006,
Singapore.
[Yencken et al2007] Yencken, Lars, Zhihui Jin, and Kumiko
Tanaka-Ishii. 2007. Pinyomi - dictionary lookup via ortho-
graphic associations. In Proc. PACLING 2007, Melbourne,
Australia.
[Zock and Bilac2004] Zock, Michael and Slaven Bilac. 2004.
Word lookup on the basis of associations: from an idea to
a roadmap. In Proc. COLING 2004, pages 89?95, Geneva,
Switzerland.
[Zock and Quint2004] Zock, Michael and Julien Quint. 2004.
Why have them work for peanuts, when it is so easy to pro-
vide reward? Motivations for converting a dictionary into
a drill tutor. In Proc. PAPILLON 2004, Grenoble, France.
[Zock2002] Zock, Michael. 2002. Sorry, what was your name
again, or how to overcome the tip-of-the tongue problem
with the help of a computer? In Proc. COLING 2002,
pages 1?6, Taipei, Taiwan.
1048
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 477?485, Prague, June 2007. c?2007 Association for Computational Linguistics
Word Sense Disambiguation
Incorporating Lexical and Structural Semantic Information
Takaaki Tanaka? Francis Bond? Timothy Baldwin? Sanae Fujita? Chikara Hashimoto?
? {takaaki, sanae}@cslab.kecl.ntt.co.jp ? bond@nict.go.jp
? tim@csse.unimelb.edu.au ? ch@yz.yamagata-u.ac.jp
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? National Institute of Information and Communications Technology
? The University of Melbourne ? Yamagata University
Abstract
We present results that show that incorporat-
ing lexical and structural semantic informa-
tion is effective for word sense disambigua-
tion. We evaluated the method by using pre-
cise information from a large treebank and
an ontology automatically created from dic-
tionary sentences. Exploiting rich semantic
and structural information improves preci-
sion 2?3%. The most gains are seen with
verbs, with an improvement of 5.7% over a
model using only bag of words and n-gram
features.
1 Introduction
Recently, significant improvements have been made
in combining symbolic and statistical approaches
to various natural language processing tasks. In
parsing, for example, symbolic grammars are be-
ing combined with stochastic models (Riezler et al,
2002; Oepen et al, 2002; Malouf and van Noord,
2004). Statistical techniques have also been shown
to be useful for word sense disambiguation (Steven-
son, 2003). However, to date, there have been
few combinations of sense information together with
symbolic grammars and statistical models. Klein
and Manning (2003) show that much of the gain in
statistical parsing using lexicalized models comes
from the use of a small set of function words.
Features based on general relations provide little
improvement, presumably because the data is too
sparse: in the Penn treebank normally used to train
and test statistical parsers stocks and skyrocket never
appear together. They note that this should motivate
the use of similarity and/or class based approaches:
the superordinate concepts capital (? stocks) and
move upward (? sky rocket) frequently appear to-
gether. However, there has been little success in this
area to date. For example, Xiong et al (2005) use se-
mantic knowledge to parse Chinese, but gain only a
marginal improvement. Focusing on WSD, Steven-
son (2003) and others have shown that the use of
syntactic information (predicate-argument relations)
improve the quality of word sense disambiguation
(WSD). McCarthy and Carroll (2003) have shown
the effectiveness of the selectional preference infor-
mation for WSD. However, there is still little work
on combining WSD and parse selection.
We hypothesize that one of the reasons for the
lack of success is that there has been no resource
annotated with both syntactic (or structural seman-
tic information) and lexical semantic information.
For English, there is the SemCor corpus (Fellbaum,
1998) which is annotated with parse trees and Word-
Net senses, but it is fairly small, and does not ex-
plicitly include any structural semantic information.
Therefore, we decided to construct and use a tree-
bank with both syntactic information (e.g. HPSG
parses) and lexical semantic information (e.g. sense
tags): the Hinoki treebank (Bond et al, 2004). This
can be used to train word sense disambiguation and
parse ranking models using both syntactic and lexi-
cal semantic features. In this paper, we discuss only
word sense disambiguation. Parse ranking is dis-
cussed in Fujita et al (2007).
2 The Hinoki Corpus
The Hinoki corpus consists of the Lexeed Seman-
tic Database of Japanese (Kasahara et al, 2004) and
corpora annotated with syntactic and semantic infor-
477
mation.
2.1 Lexeed
Lexeed is a database built from on a dictionary,
which defines word senses used in the Hinoki cor-
pus and has around 49,000 dictionary definition sen-
tences and 46,000 example sentences which are syn-
tactically and semantically annotated. Lexeed con-
sists of all words with a familiarity greater than or
equal to five on a scale of one to seven. This gives
a fundamental vocabulary of 28,000 words, divided
into 46,347 different senses. Each sense has a defi-
nition sentence and example sentence written using
only these 28,000 familiar words (and some function
words). Many senses have more than one sentence
in the definition: there are 75,000 defining sentences
in all.
A (simplified) example of the entry for?U3 un-
tenshu ?chauffeur? is given in Figure 1. Each word
contains the word itself, its part of speech (POS) and
lexical type(s) in the grammar, and the familiarity
score. Each sense then contains definition and ex-
ample sentences, links to other senses in the lexicon
(such as hypernym), and links to other resources,
such as the Goi-Taikei (Ikehara et al, 1997) and
WordNet (Fellbaum, 1998). Each content word in
the definition and example sentences is annotated
with sense tags from the same lexicon.
2.2 Lexical Semantics Annotation
The lexical semantic annotation uses the sense in-
ventory from Lexeed. All words in the fundamental
vocabulary are tagged with their sense. For example,
the word d& ookii ?big? (in ookiku naru ?grow
up?) is tagged as sense 5 in the example sentence
(Figure 1), with the meaning ?elder, older?.
Each word was annotated by five annotators. We
use the majority choice in case of disagreements
(Tanaka et al, 2006). Inter-annotator agreements
among the five annotators range from 78.7% to
83.3%: the lowest agreement is for the Lexeed def-
inition sentences and the highest is for Kyoto cor-
pus (newspaper text). These agreements reflect the
difficulties in disambiguating word sense over each
corpus and can be considered as the upper bound of
precision for WSD.
Table 1 shows the distribution of word senses ac-
cording to the word familiarity in Lexeed.
Fam #Words
Poly-
semous #WS
#Mono-
semous(%)
6.5 - 368 182 4.0 186 (50.5)
6.0 - 4,445 1,902 3.4 2,543 (57.2)
5.5 - 9,814 3,502 2.7 6,312 (64.3)
5.0 - 11,430 3,457 2.5 7,973 (69.8)
Table 1: Word Senses in Lexeed
2.3 Ontology
The Hinoki corpus comes with an ontology semi-
automatically constructed from the parse results of
definitions in Lexeed (Nichols and Bond, 2005). The
ontology includes more than 80 thousand relation-
ships between word senses, e.g. synonym, hyper-
nym, abbreviation, etc. The hypernym relation for
?U3 untenshu ?chauffeur? is shown in Figure 1.
Hypernym or synonym relations exist for almost all
content words.
2.4 Thesaurus
As part of the ontology verification, all nominal and
most verbal word senses in Lexeed were linked to
semantic classes in the Japanese thesaurus, Nihongo
Goi-Taikei (Ikehara et al, 1997). These were then
hand verified. Goi-Taikei has about 400,000 words
including proper nouns, most nouns are classified
into about 2,700 semantic classes. These seman-
tic classes are arranged in a hierarchical structure
(11 levels). The Goi-Taikei Semantic Class for ?
U3 untenshu ?chauffeur? is shown in Figure 1:
?C292:driver? at level 9 which is subordinate to
?C4:person?.
2.5 Syntactic and Structural Semantics
Annotation
Syntactic annotation is done by selecting the best
parse (or parses) from the full analyses derived by
a broad-coverage precision grammar. The gram-
mar is an HPSG implementation (JACY: Siegel and
Bender, 2002), which provides a high level of de-
tail, marking not only dependency and constituent
structure but also detailed semantic relations. As the
grammar is based on a monostratal theory of gram-
mar (HPSG: Pollard and Sag, 1994) it is possible
to simultaneously annotate syntactic and semantic
structure without overburdening the annotator. Us-
ing a grammar enforces treebank consistency ? all
sentences annotated are guaranteed to have well-
478
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ?U3 untenshu
POS noun
LEX-TYPE noun-lex
FAMILIARITY 6.2 [1?7] (? 5)
SENSE 1
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION
[
\1 ??1 k?U1 2d04 a person who drives trains and cars
]
EXAMPLE
[
d&(5 C<8b\1 G?U31 Dod6 G%?3 2
I dream of growing up and becoming a train driver
]
HYPERNYM 04 hito ?person?
SEM. CLASS ?292:driver? (? ?4:person?)
WORDNET motorman1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?
formed parses. The flip side to this is that any sen-
tences which the parser cannot parse remain unan-
notated, at least unless we were to fall back on full
manual mark-up of their analyses. The actual anno-
tation process uses the same tools as the Redwoods
treebank of English (Oepen et al, 2002).
There were 4 parses for the definition sentence
shown in Figure 1. The correct parse, shown as a
phrase structure tree, is shown in Figure 2. The two
sources of ambiguity are the conjunction and the rel-
ative clause. The parser also allows the conjunction
to join to \ densha and 0 hito. In Japanese, rel-
ative clauses can have gapped and non-gapped read-
ings. In the gapped reading (selected here), 0 hito
is the subject of ?U unten ?drive?. In the non-
gapped reading there is some underspecified relation
between the thing and the verb phrase. This is sim-
ilar to the difference in the two readings of the day
he knew in English: ?the day that he knew about?
(gapped) vs ?the day on which he knew (some-
thing)? (non-gapped). Such semantic ambiguity is
resolved by selecting the correct derivation tree that
includes the applied rules in building the tree.
The parse results can be automatically given by
the HPSG parser PET (Callmeier, 2000) with the
Japanese grammar JACY. The current parse ranking
model has an accuracy of 70%: the correct tree is
ranked first 70% of the time (for Lexeed definition
sentences) (Fujita et al, 2007).
The full parse is an HPSG sign, containing both
syntactic and semantic information. A view of the
semantic information is given in Figure 31.
1The specific meaning representation language used in
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 2: Syntactic View of the Definition of ?U
31 untenshu ?chauffeur?
The semantic view shows some ambiguity has
been resolved that is not visible in the purely syn-
tactic view.
The semantic view can be further simplified into a
dependency representation, further abstracting away
from quantification, as shown in Figure 4. One of
the advantages of the HPSG sign is that it contains
all this information, making it possible to extract the
particular view needed. In order to make linking to
other resources (such as the sense annotation) easier,
predicates are labeled with pointers back to their po-
sition in the original surface string. For example, the
predicate densha n 1 links to the surface characters
between positions 0 and 3: \.
JACY is Minimal Recursion Semantics (Copestake et al, 2005).
479
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TEXT \??k?U2d0
TOP h1
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
proposition m rel
LBL h1
ARG0 e2
MARG h3
?
?
?
?
?
?
unknown rel
LBL h4
ARG0 e2
ARG x5
?
?
?
?
?
densha n
LBL h6
ARG0 x7
?
?
?
?
?
?
?
udef rel
LBL h8
ARG0 x7
RSTR h9
BODY h10
?
?
?
?
?
?
?
?
?
?
ya p
LBL h11
ARG0 x13
L-INDEX x7
R-INDEX x12
?
?
?
?
?
?
?
?
?
?
udef rel
LBL h15
ARG0 x12
RSTR h16
BODY h17
?
?
?
?
?
?
?
jidousha n
LBL h18
ARG0 x12
?
?
?
?
?
?
?
udef rel
LBL h19
ARG0 x12
RSTR h20
BODY h21
?
?
?
?
?
?
?
?
?
?
unten s
LBL h22
ARG0 e23 tense=present
ARG1 x5
ARG2 x13
?
?
?
?
?
?
?
hito n
LBL h24
ARG0 x5
?
?
?
?
?
?
?
udef rel
LBL h25
ARG0 x5
RSTR h26
BODY h27
?
?
?
?
?
?
?
?
proposition m rel
LBL h10001
ARG0 e23 tense=present
MARG h28
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HCONS {h3 qeq h4,h9 qeq h6,h16 qeq h11,h20 qeq h18,h26 qeq h24,h28 qeq h22}
ING {h24 ing h10001}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Semantic View of the Definition of ?U31 untenshu ?chauffeur?
_1:proposition_m<0:13>[MARG e2:unknown]
e2:unknown<0:13>[ARG x5:_hito_n]
x7:udef<0:3>[]
x7:densha_n_1<0:3>
x12:udef<4:7>[]
x12:_jidousha_n<4:7>
x13:_ya_p_conj<0:4>[L-INDEX x7:_densha_n_1, R-INDEX x12:_jidousha_n]
e23:_unten_s_2<8:10>[ARG1 x5:_hito_n, ARG2 x13:_ya_p_conj]
x5:udef<12:13>[]
_2:proposition_m<0:13>[MARG e23:_unten_s_2]
Figure 4: Dependency View of the Definition of ?U31 untenshu ?chauffeur?
3 Task
We define the task in this paper as ?allocating the
word sense tags for all content words included in
Lexeed as headwords, in each input sentence?. This
task is a kind of all-words task, however, a unique
point is that we focus on fundamental vocabulary
(basic words) in Lexeed and ignore other words. We
use Lexeed as the sense inventory. There are two
problems in resolving the task: how to build the
model and how to assign the word sense by using
the model for disambiguating the senses. We de-
scribe the word sense selection model we use in sec-
tion 4 and the method of word sense assignment in
section 5.
4 Word Sense Selection Model
All content words (i.e. basic words) in Lexeed are
classified into six groups by part-of-speech: noun,
verb, verbal noun, adjective, adverb, others. We
treat the first five groups as targets of disambiguat-
ing senses. We build five words sense models corre-
sponding to these groups. A model contains senses
for various words, however, features for a word are
discriminated from those for other words so that the
senses irrelevant to a target word are not selected.
For example, an n-gram feature following a target
word ?has-a-tail? for dog is distinct from that for cat.
In the remainder of this section, we describe the
features used in the word sense disambiguation.
First we used simple n-gram collocations, then a bag
of words of all words occurring in the sentence. This
was then enhanced by using ontological information
and predicate argument relations.
4.1 Word Collocations
Word collocations (WORD-Col) are basic and effec-
tive cues for WSD. They can be modelled by n-
gram and bag of words features, which are easily
extracted from a corpus. We used all unigrams, bi-
grams and trigrams which precede and follow the
target words (N-gram) and all content words in the
sentences where the target words occur (BOW).
480
# sample features
C1 ?COLWS:04?
C2 ?COLWSSC:C33:other person?
C3 ?COLWSHYP:0/1?
C4 ?COLWSHYPSC:C5:person?
C1 ?COLWS:\1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:?1?
C4 ?COLWSHYPSC:C988:land vehicle?
C1 ?COLWS:?1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:2?
C4 ?COLWSHYPSC:C988:land vehicle?
Table 2: Example semantic collocation features
(SEM-Col) extracted from the word sense tagged cor-
pus and the dictionary (Lexeed and GoiTaikei) and
the ontology which have the word senses and the se-
mantic classes linked to the semantic tags. The first
column numbers the feature template corresponding
to each example.
4.2 Semantic Features
We use the semantic information (sense tags and on-
tologies) in two ways. One is to enhance the collo-
cations and the other is to enhance dependency rela-
tions.
4.2.1 Semantic Collocations
Word surface features like N-gram and BOW in-
evitably suffer from data sparseness, therefore, we
generalize them to more abstract words or concepts
and also consider words having the same mean-
ings. We used the ontology described in Sec-
tion 2.3 to get hypernyms and synonyms and the
Goi-Taikei thesaurus to abstract the words to the se-
mantic classes. The superordinate classes at level
3, 4 and 5 are also added in addition to the original
semantic class. For example, \ densha ?train?
and ? jidousha ?automobile? are both gener-
alized to the semantic class ?C988:land vehicle?
(level 7). The superordinate classes are also used:
?C706:inanimate? (level 3), ?C760:artifact?
(level 4) and ?C986:vehicle? (level 5).
4.2.2 Semantic Dependencies
The semantic dependency features are based on
a predicate and its arguments taken from the ele-
mentary dependencies. For example, consider the
semantic dependency representation for densha ya
# sample features for ?U2d1
D1 ?PRED:?U2d, ARG1:0?
D1 ?PRED:?U2d, ARG2:\?
D1 ?PRED:?U2d, ARG2:??
D2 ?PRED:?U2d, ARG1:04?
D2 ?PRED:?U2d, ARG2:\1?
D2 ?PRED:?U2d, ARG2:?1?
D3 ?PRED:?U2d, ARG1SC:C33?
D3 ?PRED:?U2d, ARG2SC:C988?
D4 ?PRED:?U2d, ARG2SYN:???1?
D5 ?PRED:?U2d, ARG1HYP:0/1?
D5 ?PRED:?U2d, ARG2HYP:?1?
D5 ?PRED:?U2d, ARG2HYP:2?
D6 ?PRED:?U2d, ARG1HYPSC:C5?
D6 ?PRED:?U2d, ARG2HYPSC:C988?
D11 ?PRED:?U2d, ARG1:0, ARG2:\?
D22 ?PRED:?U2d, ARG1:04, ARG2:\1?
D23 ?PRED:?U2d, ARG1:04, ARG2:C1460 ?
D24 ?PRED:?U2d, ARG1:04, ARG2SYN:???1?
D32 ?PRED:?U2d, ARG1:C5, ARG2:\1?
D33 ?PRED:?U2d, ARG1:C5, ARG2:C988?
D55 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYP:?1?
D56 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYPSC:C988?
D65 ?PRED:?U2d, ARG1HYPSC:C5 , ARG2HYP:?1?
D322 ?PRED:C2003, ARG1:04, ARG2:\1?
Table 3: Example semantic features extracted from
the dependency tree in Figure 4. The first column
numbers the feature template corresponding to each
example.
jidousha-wo unten suru hito ?a person who drives a
train or car? given in Figure 4. The predicate un-
ten ?drive?, has two arguments: ARG1 hito ?person?
and ARG2 ya ?or?. The coordinate conjunction is
expanded out into its children, giving ARG2 densha
?train? and jidousha ?automobile?.
From these, we produce several features, a sam-
ple of them are shown in Table 3. One has all argu-
ments and their labels (D11). We also produce var-
ious back offs, for example the predicate with only
one argument at a time (D1-D3). Each combination
of predicate and its related argument(s) becomes a
feature.
For the next class of features, we used the sense
information from the corpus combined with the se-
mantic classes in the dictionary to replace each pred-
481
icate by its disambiguated sense, its hypernym, its
synonym (if any) and its semantic class. The seman-
tic classes for\1 and?1 are both ?988:land
vehicle?, while ?U1 is ?2003:motion? and 04
is ?4:human?. We also expand ?1 into its syn-
onym ???1 mo?ta?ka? ?motor car?.
The semantic class features provide a seman-
tic smoothing, as words are binned into the 2,700
classes. The hypernym/synonym features provide
even more smoothing. Both have the effect of mak-
ing more training data available for the disambigua-
tor.
4.3 Domain
Domain information is a simple and sometimes
strong cue for disambiguating the target words
(Gliozzo et al, 2005). For instance, the sense of
the word ?record? is likey to be different in the mu-
sical context, which is recalled by domain-specific
words like ?orchestra?, ?guitar?, than in the sport-
ing context. We use 12 domain categories like ?cul-
ture/art?, ?sport?, etc. which are similar to ones used
in directory search web sites. About 6,000 words
are automatically classified into one of 12 domain
categories by distributions in web sites (Hashimoto
and Kurohashi, 2007) and 10% of them are manually
checked. Polysemous words which belong to multi-
ple domains and neutral words are not classified into
any domain.
5 Search Algorithm
The conditional probability of the word sense for
each word is given by the word sense selection
model described in Section 4. In the initial state,
some of the semantic features, e.g. semantic col-
locations (SEM-Col) and word sense extensions for
semantic dependencies (SEM-Dep) are not available,
since no word senses for polysemous words have
been determined. It is not practical to count all com-
binations of word senses for target words, therefore,
we first try to decide the sense for that word which
is most plausible among all the ambiguous words,
then, disambiguate the next word by using the sense.
We use the beam search algorithm, which is sim-
ilar to that used for decoder in statistical machine
translation (Watanabe, 2004), for finding the plausi-
ble combination of word sense tags.
The algorithm is described as follows. For a pol-
ysemous word set in an input sentence {w1, . . . ,wn},
twik is the k-th word sense of word wi, W is a set
having words to be disambiguated, T is a list of re-
solved word senses. A search node N is defined as
[W,T ] and a score of a node N, s(N) is defined as
the probability that the word sense set T occurs in
the context. The beam search can be done as fol-
lows (beam width is b):
1. Create an initial node N0 = [T0,W0] (T0 = {},
W0 = {}) and insert the node into an initial
queue Q0.
2. For each node N in the queue Q, do the follow-
ing steps.
? For each wi (? W ), create W ?i by picking
out wi from W
? Create new lists T ?1, . . . ,T ?l by adding one
of word sense candidates twi1,. . . ,twil for wi
to T
? Create new nodes [W ?i ,T ?0 ], . . . ,[W ?i ,T ?l ] and
insert them into the queue Q?
3. Sort the nodes in Q? by the score s(N)
4. If the top node W in the queue Q? is empty,
adopt T as the combination of word senses and
terminate. Otherwise, pick out the top b nodes
from Q? and insert them into new queue Q, then
go back to 2
6 Evaluation
We trained and tested on the Lexeed Dictionary Def-
inition (LXD-DEF) and Example sections (LXD-EX) of
the Hinoki corpus (Bond et al, 2007). These have
about 75,000 definition and 46,000 example sen-
tences respectively. Some 54,000 and 36,000 sen-
tences of them are treebanked, i.e., they have the
syntactic trees and structural semantic information.
We used these sentences with the complete informa-
tion and selected 1,000 sentences out of each sen-
tence class as test sets (LXD-DEFtest, LXD-EXtest), and
the remainder is combined and used as a training
set (LXD-ALL). We also tested 1,000 sentences from
the Kyoto Corpus of newspaper text (KYOTOtest).
These sentences have between 3.4 (LXD-EXtest) ? 5.2
(KYOTOtest) polysemous words per sentence on av-
erage.
482
We use a maximum entropy / minimum diver-
gence (MEMD) modeler to train the word sense se-
lection model. We use the open-source Maximun
Entropy Modeling Toolkit2 for training, determining
best-performing convergence thresholds and prior
sizes experimentally. The models for five differ-
ent POSs were trained with each training sets: the
base model is word collocation model (WORD-Col),
and the semantic models built by semantic colloca-
tion (SEM-Col), semantic dependency (SEM-Dep) or
domain with WORD-Col (+SEM-Col, +SEM-Dep and
+DOMAIN).
Figure 5: Learning Curve
7 Results and Discussion
Table 4 shows the precision as the results of the word
sense disambiguation on the combination of LXD-
DEF and LXD-EX (LXD-ALL). The baseline method
selects the senses occurring most frequently in the
training corpus. Each row indicates the results us-
ing the baseline, word collocation (WORD-Col), the
combinations of WORD-Col and one of the seman-
tic features (+SEM-Col, +SEM-Dep and +DOMAIN),
e.g, +SEM-Col gives the results using WORD-Col and
SEM-Col, and all features (FULL).
There are significant improvements over the base-
line and the other results on all corpora. Basic word
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
collocation features (WORD-Col) give a vast improve-
ment. Extending this by using the ontological in-
formation (+SEM-Col) gives a further improvement
over the WORD-Col. Adding the predicate-argument
relationships (+SEM-Dep) improves the results even
more.
Table 6 shows the statistics of the target corpora.
The best result of LXD-DEFtest (80.7%) surpasses the
inter-annotator agreement (78.7%) in building the
Hinoki Sensebank. However, there is a wide gap
between the best results of KYOTOtest (60.4%) and
the inter-annotator agreement (83.3%), this suggests
other information such as the semantic classes for
named entities (including proper nouns and multi-
word expressions (MWE)) and broader contexts are
required. However, a model built on dictionary sen-
tences lacks these features. Even, so there is some
improvement.
The domain features (+DOMAIN) give small con-
tribution to the precision, since only intra-sentence
context is counted in this experiment. Unfortunately
dictiory definition and example sentences do not re-
ally have a useful context. We expect broader con-
text should make the domain features more effective
for the newspaper text (e.g. as in Stevenson (2003)),
Table 5 shows comparison of results of different
POSs. The semantic features (+SEM-Col and +SEM-
Dep) are particularly effective for verb and also give
moderate improvements on the results of the other
POSs.
Figure 5 shows the precisions of LXD-DEFtest in
changing the size of a training corpus, which is di-
vided into five partitions. The precision is saturated
in using four partitions (264,000 tokens).
These results of the dictionary sentences are close
to the best published results for the SENSEVAL-2
task (79.3% by Murata et al (2003) using a com-
bination of simple Bayes learners). However, we
are using a different sense inventory (Lexeed not
Iwanami (Nishio et al, 1994)) and testing over a dif-
ferent corpus, so the results are not directly compa-
rable. In future work, we will test over SENSEVAL-
2 data so that we can compare directly.
None of the SENSEVAL-2 systems used onto-
logical information, despite the fact that the dic-
tionary definition sentences were made available,
and there are several algorithms describing how to
extract such information from MRDs (Tsurumaru
483
Model Test Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
LXD-ALL LXD-DEFtest 72.8 78.4 79.8 80.2 78.1 80.7
LXD-EXtest 70.4 75.6 78.7 77.9 76.0 78.8
KYOTOtest 55.6 58.5 60.0 58.8 59.8 60.4
Table 4: The Precision of WSD
POS Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
Noun 65.5 68.7 69.6 69.4 68.9 69.8
Verb 60.3 66.9 71.0 70.6 67.7 72.6
VN 72.6 76.2 77.7 74.6 77.6 77.5
Adj 59.9 67.2 69.5 68.9 68.9 69.5
Adv 74.4 78.6 79.8 79.2 78.6 79.8
Table 5: The Precision of WSD (per Part-of-Speech)
et al, 1991; Wilkes et al, 1996; Nichols et al, 2005).
We hypothesize that this is partly due to the way the
task is presented: there was not enough time to ex-
tract and debug an ontology as well as build a dis-
ambiguation system, and there was no ontology dis-
tributed. The CRL system (Murata et al, 2003) used
a syntactic dependency parser as one source of fea-
tures (KNP: Kurohashi and Nagao (2003)), remov-
ing it decreased performance by around 0.6%.
8 Conclusions
We used the Hinoki corpus to test the importance of
lexical and structural information in word sense dis-
ambiguation. We found that basic n-gram features
and collocations provided a great deal of useful in-
formation, but that better results could be gained by
using ontological information and semantic depen-
dencies.
Acknowledgements
We would like to thank the other members of the
NTT Natural Language Research Group NTT Com-
munication Science laboratories for their support.
We would also like to express gratitude to the re-
viewers for their valuable comments and Professor
Zeng Guangping, Wang Daliang and Shen Bin of
the University of Science and Technology Beijing
(USTB) for building the demo system.
References
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04), pages 554?559. Hainan
Island.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Ulrich Callmeier. 2000. PET - a platform for experimentation
with efficient HPSG processing techniques. Natural Lan-
guage Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281?332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25?32. Prague, Czech Republic.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL 2005). Ann
Arbor, U.S.
Chikara Hashimoto and Sadao Kurohashi. 2007. Construction
of domain dictionary for fundamental vocaburalry. In Pro-
ceedings of the ACL 2007 Main Conference Poster Sessions.
Association for Computational Linguistics, Prague, Czech
Republic.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75?82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423?430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
484
Corpus
Annotated
Tokens #WS
Agreement
token (type) %Other Sense %Homonym %MWE %Proper Noun
LXD-DEF 199,268 5.18 .787 (.850) 4.2 0.084 1.5 0.046
LXD-EX 126,966 5.00 .820 (.871) 2.3 0.035 0.4 0.0018
KYOTO 268,597 3.93 .833 (.828) 9.8 3.3 7.9 5.5
Table 6: Corpus Statistics
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing sys-
tem. In Anne Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, chapter 14, pages 249?260. Kluwer Aca-
demic Publishers.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and HItoshi Isahara. 2003. CRL at Japanese dictionary-
based task of SENSEVAL-2. Journal of Natural Language
Processing, 10(3):115?143. (in Japanese).
Eric Nichols and Francis Bond. 2005. Acquiring ontologies
using deep and shallow processing. In 11th Annual Meeting
of the Association for Natural Language Processing, pages
494?498. Takamatsu.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proceedings of the International Joint Conference on
Artificial Intelligence IJCAI-2005, pages 1111?1116. Edin-
burgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994.
Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dic-
tionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Confer-
ence on Computational Linguistics: COLING-2002, pages
1253?7. Taipei, Taiwan.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing
the Wall Street Journal using a Lexical-Functional Grammar
and discriminative estimation techniques. In 41st Annual
Meeting of the Association for Computational Linguistics:
ACL-2003, pages 271?278.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Mark Stevenson. 2003. Word Sense Disambiguation. CSLI Pub-
lications.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The Hi-
noki sensebank ? a large-scale word sense tagged corpus of
Japanese ?. In Proceedings of the Workshop on Frontiers in
Linguistically Annotated Corpora 2006, pages 62?69. Syd-
ney. URL http://www.aclweb.org/anthology/W/W06/
W06-0608, (ACL Workshop).
Hiroaki Tsurumaru, Katsunori Takesita, Itami Katsuki, Toshi-
hide Yanagawa, and Sho Yoshida. 1991. An approach to
thesaurus construction from Japanese language dictionary.
In IPSJ SIGNotes Natural Language, volume 83-16, pages
121?128. (in Japanese).
Taro Watanabe. 2004. Example-based Statistical Machine
Translation. Ph.D. thesis, Kyoto University.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing ? IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70?81. Springer-Verlag.
485
Semantic Role Labelling of Prepositional Phrases
Patrick Ye1 and Timothy Baldwin1,2
1 Department of Computer Science and Software Engineering,
University of Melbourne, VIC 3010, Australia
2 NICTA Victoria Laboratories,
University of Melbourne, VIC 3010, Australia
{jingy, tim}@cs.mu.oz.au
Abstract. We propose a method for labelling prepositional phrases ac-
cording to two different semantic role classifications, as contained in the
Penn treebank and the CoNLL 2004 Semantic Role Labelling data set.
Our results illustrate the difficulties in determining preposition seman-
tics, but also demonstrate the potential for PP semantic role labelling to
improve the performance of a holistic semantic role labelling system.
1 Introduction
Prepositional phrases (PPs) are both common and semantically varied in open
English text. Learning the semantics of prepositions is not a trivial task in gen-
eral. It may seem that the semantics of a given PP can be predicted with rea-
sonable reliability independent of its context. However, it is actually common for
prepositions or even identical PPs to exhibit a wide range of semantic fuctions
in different open English contexts. For example, consider the PP to the car : this
PP will generally occur as a directional adjunct (e.g. walk to the car), but it can
also occur as an object to the verb (e.g. refer to the car) or contrastive argu-
ment (e.g. the default mode of transport has shifted from the train to the car); to
further complicate the situation, in key to the car it functions as a complement
to the N-bar key. Based on this observation, we may consider the possibility of
constructing a semantic tagger specifically for PPs, which uses the surrounding
context of the PP to arrive at a semantic analysis. It is this task of PP semantic
role labelling that we target in this paper.
A PP semantic role labeller would allow us to take a document and identify
all adjunct PPs with their semantics. We would expect this to include a large
portion of locative and temporal expressions, e.g., in the document, providing
valuable data for tasks such as information extraction and question answering.
Indeed our initial foray into PP semantic role labelling relates to an interest in
geospatial and temporal analysis, and the realisation of the importance of PPs
in identifying and classifying spatial and temporal references.
The contributions of this paper are to propose a method for PP semantic role
labelling, and evaluate its performance over both the Penn treebank (including
comparative evaluation with previous work) and also the data from the CoNLL
Semantic Role Labelling shared task. As part of this process, we identify the
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 779?791, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
780 P. Ye and T. Baldwin
Fig. 1. An example of the preposition semantic roles in Penn Teebank
level of complementarity of a dedicated PP semantic role labeller with a conven-
tional holistic semantic role labeller, suggesting PP semantic role labelling as a
potential avenue for boosting the performance of existing systems.
2 Preposition Semantic Role Disambiguation in Penn
Treebank
Significant numbers of prepositional phrases (PPs) in the Penn treebank [1] are
tagged with their semantic role relative to the governing verb. For example,
Figure 1, shows a fragment of the parse tree for the sentence [Japan?s reserves
of gold, convertible foreign currencies, and special drawing rights] fell by a hefty
$1.82 billion in October to $84.29 billion [the Finance Ministry said], in which
the three PPs governed by the verb fell are tagged as, respectively: PP-EXT
(?extend?), meaning how much of the reserve fell; PP-TMP (?temporal?), meaning
when the reserve fell; and PP-DIR (?direction?), meaning the direction of the fall.
According to our analysis, there are 143 preposition semantic roles in the tree-
bank. However, many of these semantic roles are very similar to one another;
for example, the following semantic roles were found in the treebank: PP-LOC,
PP-LOC-1, PP-LOC-2, PP-LOC-3, PP-LOC-4, PP-LOC-5, PP-LOC-CLR, PP-
LOC-CLR-2, PP-LOC-CLR-TPC-1. Inspection of the data revealed no systematic
semantic differences between these PP types. Indeed, for most PPs, it was im-
possible to distinguish the subtypes of a given superclass (e.g. PP-LOC in our
example). We therefore decided to collapse the PP semantic roles based on their
first semantic feature. For example, all semantic roles that start with PP-LOC
are collapsed to the single class PP-LOC. Table 1 shows the distribution of the
collapsed preposition semantic roles.
[2] describe a system1 for disambiguating the semantic roles of prepositions in
the Penn treebank according to 7 basic semantic classes. In their system, O?Hara
and Weibe used a decision tree classifier, and the following types of features:
? POS tags of surrounding tokens: The POS tags of the tokens before and
after the target preposition within a predefined window size. In O?Hara and
Wiebe?s work, this window size is 2.
1 This system was trained with WEKA?s J48 decision tree implementation.
Semantic Role Labelling of Prepositional Phrases 781
Table 1. Penn treebank semantic role distribution (top-9 roles)
Semantic Role Count Frequency Meaning
PP-LOC 21106 38.2 Locative
PP-TMP 12561 22.7 Temporal
?Closely related? (somewhere between
PP-CLR 11729 21.2
an argument and an adjunct)
PP-DIR 3546 6.4 Direction (from/to X)
PP-MNR 1839 3.3 Manner (incl. instrumentals)
PP-PRD 1819 3.3 Predicate (non-VP)
PP-PRP 1182 2.1 Purpose or reason
PP-CD 654 1.2 Cardinal (numeric adjunct)
PP-PUT 296 0.5 Locative complement of put
? POS tag of the target preposition
? The target preposition
? Word collocation: All the words in the same sentence as the target prepo-
sition; each word is treated as a binary feature.
? Hypernym collocation: The WordNet hypernyms [3] of the open class
words before and after the target preposition within a predefined window
size (set to 5 words); each hypernym is treated as a binary feature.
O?Hara and Wiebe?s system also performs the following pre-classification
filtering on the collocation features:
? Frequency constraint: f(coll) > 1, where coll is either a word from the
word collocation or a hypernym from the hypernym collocation
? Conditional independence threshold: p(c|coll)?p(c)p(c) >= 0.2, where c is a
particular semantic role and coll is from the word collocation or a hypernym
from the hypernym collocation
We began our research by replicating O?Hara and Wiebe?s method and seek-
ing ways to improve it. Our initial investigation revealed that there were around
44000 word and hypernym collocation features even after the frequency con-
straint filter and the conditional independence filter have been applied. We did
not believe all these collocation features were necessary, and we deployed an ad-
ditional ranking-based filtering mechanism over the collocation features to only
select collocation features which occur in the top N frequency bins. Algorithm 1
shows the details of this filtering mechanism.
This ranking-based filtering mechanism allows us to select collocation feature
sets of differing size, and in doing so not only improve the training and tagging
Algorithm 1. Ranking based filtering algorithm
1. Let s be the list that contains the frequency of all the collocation features
2. Sort s in descending order
3. minFrequency = s[N ]
4. Discard all features whose frequency is less than minFrequency
782 P. Ye and T. Baldwin
Table 2. Penn treebank preposition semantic role disambiguation results
Accuracy (%)
Ranking Classifier 1 Classifier 2
10 74.75 81.28
20 76.53 83.52
50 79.21 86.34
100 80.13 87.02
300 81.32 87.62
1000 82.34 87.71
all 82.76 87.45
O?Hara & Wiebe N/A 85.8
speed of the preposition semantic role labelling, but also observe how the number
of collocation features affects the performance of the PP semantic role labeller
and which collocation features are more important.
2.1 Results
Since some of the preposition semantic roles in the treebank have extremely low
frequencies, we decided to build our first classifier using only the top 9 seman-
tic roles, as detailed in Table 1. We also noticed that the semantic roles PP-CLR,
PP-CD and PP-PUT were excluded from O?Hara?s system which only used PP-BNF,
PP-EXT, PP-MNR, PP-TMP, PP-DIR, PP-LOC and PP-PRP, therefore we built a sec-
ond classifier using only the semantic roles used by O?Hara?s system2. The two
classifiers were trained with a maximum entropy [4] learner3.
Table 2 shows the results of our classifier under stratified 10-fold cross val-
idation4 using different parameters for the rank-based filter. We also list the
accuracy reported by O?Hara and Wiebe for comparison.
The results show that the performance of the classifier increases as we add
more collocation features. However, this increase is not linear, and the improve-
ment of performance is only marginal when the number collocation features is
greater than 100. It also can be observed that there is a consistent performance
difference between classifiers 1 and 2, which may suggest that PP-CLR may be
harder to distinguish from other semantic roles. This is not totally surprising
given the relatively vague definition of the semantics of PP-CLR. We return to
analyse these results in greater depth in Section 4.
3 Preposition Semantic Role Labelling over the CoNLL
2004 Dataset
Having built a classifier which has reasonable performance on the task of tree-
bank preposition semantic role disambiguation, we decided to investigate
2 PP-BNF with only 47 counts was not used by the second classifier.
3 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
4 O?Hara?s system was also evaluated using stratified 10-fold cross validation.
Semantic Role Labelling of Prepositional Phrases 783
whether we could use the same feature set to perform PP semantic role labelling
over alternate systems of PP classification. We chose the 2004 CoNLL Semantic
Role Labelling (SRL) dataset [5] because it contained a wide range of semantic
classes of PPs, in part analogous to the Penn treebank data, and also because
we wished to couple our method with a holistic SRL system to demonstrate the
ability of PP semantic role labelling to enhance overall system performance.
Since the focus of the CoNLL data is on SRL relative to a set of pre-
determined verbs for each sentence input,5 our primary objective is to inves-
tigate whether the performance of SRL systems in general can be improved in
any way by an independent preposition SRL system. We achieve this by embed-
ding our PP classification method within an existing holistic SRL system?that
is a system which attempts to tag all semantic role types in the CoNLL 2004
data?through the following three steps:
1. Perform SRL on each preposition in the CoNLL dataset;
2. Merge the output of the preposition SRL with the output of a given verb
SRL system over the same dataset;
3. Perform standard CoNLL SRL evaluation over the merged output.
The details of preposition SRL and combination with the output of a holistic
SRL system are discussed below.
3.1 Breakdown of the Preposition Semantic Role Labelling Problem
Preposition semantic role labelling over the CoNLL dataset is considerably more
complicated than the task of disambiguating preposition semantic roles in the
Penn treebank. There are three separate subtasks which are required to perform
preposition SRL:
1. PP Attachment: determining which verb to attach each preposition to.
2. Preposition Semantic Role Disambiguation
3. Argument Segmentation: determining the boundaries of the semantic
roles.
The three subtasks are not totally independent of each other, as we demon-
strate in the results section, and improved performance over one of the subtasks
does not necessarily correlate with an improvement in the final results.
3.2 PP Attachment Classification
PP attachment (PPA) classification is the first step of preposition semantic role
labelling and involves determining the verb attachment site for a given prepo-
sition, i.e. which of the pre-identified verbs in the sentence the preposition is
5 Note that the CoNLL 2004 data identifies certain verbs as having argument struc-
ture, and that the semantic role annotation is relative to these verbs only. This is
often not the sum total of all verbs in a given sentence: the verbs in relative clauses,
e.g., tend not to be identified as having argument structure.
784 P. Ye and T. Baldwin
governed by. Normally, this task would be performed by a parser. However, since
the CoNLL dataset contains no parsing information6 and we did not want to use
any resources not explicitly provided in the CoNLL data, we had to construct a
PPA classifier to specifically perform this task.
This classifier uses the following features, all of which are derived from infor-
mation provided in the CoNLL data:
? POS tags of surrounding tokens: The POS tags of the tokens before and
after the target preposition within a window size of 2 tokens ([?2, 2]).
? POS tag of the target preposition
? The target preposition
? Verbs and their relative position (VerbRelPos): All the (pre-
identified) verbs in the same sentence as the target preposition and their
relative positions to the preposition are extracted as features. Each (verb,
relative position) tuple is treated as a binary feature. The relative positions
are determined in a way such that the 1st verb before the preposition will
be given the position ?1, the 2nd verb before the preposition will be given
the position ?2, and so on.
? The type of the clause containing the target preposition
? Neighbouring chunk type: The types (NP, PP, VP, etc.) of chunks before
and after the target preposition within a window of 3 chunks.
? Word collocation (WordColl): All the open class words in the phrases
before and after the target preposition within a predefined window of 3
chunks.
? Hypernym collocation (HyperColl): All the hypernyms from the open
class words in the phrases before and after the target preposition within a
predefined window of 3 chunks.
? Named Entity collocation NEColl: All the named entity information
from the phrases before and after the target preposition within a predefined
window of 3 chunks.
The PPA classifier outputs the relative position of the governing verb to the
target preposition, or None if the preposition does not have a semantic role.
We trained the PPA classifier over the CoNLL 2004 training set, and tested it
on the testing set. Table 3 shows the distribution of the classes in the testing set.
The same maximum entropy learner used in the treebank SRL task was used
to train the PPA classifier. The accuracy of this classifier on the CoNLL 2004
testing set is 78.99%.
3.3 Preposition Semantic Role Disambiguation
For the task of preposition semantic role disambiguation (SRD), we constructed
a classifier using the same features as the PPA classifier, with the following
differences:
6 The CoNLL 2005 SRL data does contain parse trees for the sentences, possibly
obviating the need for independent verb attachment classification.
Semantic Role Labelling of Prepositional Phrases 785
Table 3. PPA class distribution
PPA Count Frequency
None 3005 60.71
-1 1454 29.37
1 411 8.30
-2 40 0.81
2 29 0.59
3 8 0.16
-3 2 0.04
-6 1 0.02
Table 4. CoNLL 2004 semantic role distribution in the CoNLL 2004 test dataset(top-
14 roles)
Semantic Role Count Frequency Meaning
A1 424 21.79 Argument 1
A2 355 18.24 Argument 2
AM-TMP 299 15.36 Temporal adjunct
AM-LOC 188 9.66 Locative adjunct
A0 183 9.40 Argument 0
AM-MNR 125 6.42 Manner adjunct
A3 106 5.45 Argument 3
AM-ADV 71 3.65 General-purpose adjunct
A4 44 2.26 Argument 4
AM-CAU 40 2.06 Causal adjunct
AM-PNC 32 1.64 Purpose adjunct
AM-DIS 32 1.64 Discourse marker
AM-DIR 19 0.97 Directional adjunct
AM-EXT 7 0.36 Extent adjunct
1. The window size for the POS tags of surrounding tokens is 5 tokens.
2. The window sizes for the WordColl, the HyperColl and the NeColl fea-
tures are set to include the entire sentence.
We trained the SRD classifier once again on the CoNLL 2004 training set,
and tested it on the testing set. Table 4 shows the distribution of the classes in
the testing set.
We used the same maximum entropy leaner as for the PPA classifier to train
the SRD classifier. The accuracy of the SRD classifier on the CoNLL 2004 testing
set is 58.68%.
3.4 Argument Segmentation
In order to determine the extent of each NP selected for by a given preposition
(i.e. the span of words contained in the NP), we use a simple regular expression
over the chunk parser analysis of the sentence provided in the CoNLL 2004 data,
786 P. Ye and T. Baldwin
namely: PP NP+. We additionally experimented with a robust statistical parser
[6] to determine PP extent, but found that the regular expression-based method
performed equally well or marginally better, without requiring any resources
external to the original task data.
We make no attempt to perform separate evaluation of this particular subtask
because without the semantic role information, no direct comparison can be
made with the CoNLL data.
3.5 Combining the Output of the Subtasks
Once we have identified the association between verbs and prepositions, and dis-
ambiguated the semantic roles of the prepositions, we can begin the process of cre-
ating the final output of the preposition semantic role labelling system. This takes
place by identifying the data column corresponding to the verb governing each
classified PP in the CoNLL data format (as determined by the PPA classifier),
and recording the semantic role of that PP (as determined by the SRD classifier)
over the full extent of the PP (as determined by the segmentation classifier).
3.6 Merging the Output of Preposition SRL and Verb SRL
Once we have generated the output of the preposition SRL system, we can
proceed to the final stage where the semantic roles of the prepositions are merged
with the semantic roles of an existing holistic SRL system.
It is possible, and indeed likely, that the semantic roles produced by the two
systems will conflict in terms of overlap in the extent of labelled constituents
and/or the semantic role labelling of constituents. To address any such conflicts,
we designed three merging strategies to identify the right balance between the
outputs of the two component systems:
S1 When a conflict is encountered, only use the semantic role information from
the holistic SRL system.
S2 When a conflict is encountered, if the start positions of the semantic role
are the same for both SRL systems, then replace the semantic role of the
holistic SRL system with that of the preposition SRL system, but keep the
holistic SRL system?s boundary end.
S3 When a conflict is encountered, only use the semantic role information from
the preposition SRL system.
3.7 Results
To evaluate the performance of our preposition SRL system, we combined its
outputs with the 3 top-performing holistic SRL systems from the CoNLL 2004
SRL shared task.7 The three systems are [7], [8] and [9]. Furthermore, in order
to establish the upper bound of the improvement of preposition SRL on verb
7 Using the test data outputs of the three systems made available at
http://www.lsi.upc.edu/?srlconll/st04/st04.html.
Semantic Role Labelling of Prepositional Phrases 787
Table 5. Preposition SRL results before merging with the holistic SRL systems, (P =
precision, R = recall, F = F-score; above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
VAAUTO 38.77 4.58 8.2 55.12 6.96 12.36 62.68 7.42 13.27 91.41 11.53 20.48
VAORACLE 42.2 6.96 11.95 56.64 10.36 17.51 71.64 11.81 20.28 99.37 18.15 30.69
Table 6. Preposition SRL combined with [7] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49 72.43 66.77 69.49
VAAUTO 72.00 66.84 69.32 72.08 66.91 69.40 72.13 66.95 69.44 72.31 67.11 69.61S1
VAORACLE 71.92 67.02 69.38 71.97 67.30 69.55 72.29 67.39 69.75 72.81 68.12 70.39
VAAUTO 71.34 66.22 68.68 70.66 65.60 68.04 73.12 67.89 70.41 73.42 68.16 70.69S2
VAORACLE 71.01 66.16 68.50 69.78 65.21 67.42 73.68 68.67 71.08 74.35 69.55 71.87
VAAUTO 70.10 65.00 67.46 72.25 66.83 69.43 73.12 67.84 70.38 77.16 71.39 74.16S3
VAORACLE 70.38 65.91 68.07 73.10 68.67 70.81 75.58 70.82 73.12 81.42 76.55 78.91
Table 7. Preposition SRL combined with [8] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39 70.07 63.07 66.39
VAAUTO 68.50 63.79 66.06 69.17 64.44 66.72 69.37 64.60 66.90 70.58 65.73 68.07S1
VAORACLE 68.18 64.59 66.33 68.93 65.57 67.21 69.75 66.09 67.87 71.65 68.18 69.87
VAAUTO 68.21 63.52 65.79 68.31 63.64 65.89 70.53 65.68 68.02 71.87 66.94 69.32S2
VAORACLE 67.77 64.19 65.93 67.50 64.19 65.81 71.43 67.68 69.51 73.51 69.95 71.69
VAAUTO 67.14 62.30 64.63 69.39 64.23 66.71 70.19 65.14 67.57 74.34 68.81 71.47S3
VAORACLE 66.79 63.22 64.96 69.58 66.05 67.76 71.98 68.14 70.01 77.87 73.93 75.85
SRL, and investigate how the three subtasks interact with each other and what
their respective limits are, we also used oracled outputs from each subtask in
combining the final outputs of the preposition SRL system. The oracled outputs
are what would be produced by perfect classifiers, and are emulated by inspection
of the gold-standard annotations for the testing data.
Table 5 shows the results of the preposition SRL systems before they are
merged with the verb SRL systems. These results show that the coverage of our
preposition SRL system is quite low relative to the total number of arguments
788 P. Ye and T. Baldwin
Table 8. Preposition SRL combined with [9] (P = precision, R = recall, F = F-score;
above-baseline results in boldface)
SRDAUTO SRDORACLE
SEGNP SEGORACLE SEGNP SEGORACLE
P R F P R F P R F P R F
ORIG 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03 71.81 61.11 66.03
VAAUTO 70.23 61.87 65.78 70.74 62.43 66.32 71.13 62.65 66.62 72.34 63.83 67.82S1
VAORACLE 69.61 62.63 65.94 70.20 63.60 66.74 71.57 64.38 67.79 73.49 66.60 69.87
VAAUTO 69.92 61.60 65.50 69.91 61.69 65.54 72.10 63.50 67.53 73.39 64.75 68.80S2
VAORACLE 69.14 62.19 65.48 68.84 62.35 65.43 72.79 65.47 68.94 74.83 67.82 71.15
VAAUTO 69.01 60.66 64.57 71.31 62.57 66.65 72.24 63.49 67.58 76.54 67.15 71.54S3
VAORACLE 68.77 61.86 65.13 71.59 64.81 68.03 74.19 66.74 70.27 80.25 72.67 76.27
in the testing data, even when oracled outputs from all three subsystems are
used (recall = 18.15%). However, this is not surprising because we expected the
majority of semantic roles to be noun phrases.
In Tables 6, 7 and 8, we show how our preposition SRL system performs
when merged with the top 3 systems under the 3 merging strategies introduced
in Section 3.6. In each table, ORIG refers to the base system without preposition
SRL merging.
We can make a few observations from the results of the merged systems.
First, out of verb attachment, SRD and segmentation, the SRD module is both:
(a) the component with the greatest impact on overall performance, and (b)
the component with the greatest differential between the oracle performance
and classifier (AUTO) performance. This would thus appear to be the area in
which future efforts should be concentrated in order to boost the performance
of holistic SRLs through preposition SRL.
Second, the results show that in most cases, the recall of the merged system is
higher than that of the original SRL system. This is not surprising given that we
are generally relabelling or adding information to the argument structure of each
verb, although with the more aggressive merging strategies (namely S2 and S3)
it sometimes happens that recall drops, by virtue of the extent of an argument
being aversely affected by relabelling. It does seem to point to a complementarity
between verb-driven SRL and preposition-specific SRL, however.
Finally, it was somewhat disappointing to see that in no instance did a fully-
automated method surpass the base system in precision or F-score. Having said
this, we were encouraged by the size of the margin between the base systems and
the fully oracle-based systems, as it supports our base hypothesis that preposi-
tion SRL has the potential to boost the performance of holistic SRL systems,
up to a margin of 10% in F-score for S3.
4 Analysis and Discussion
In the previous 2 sections, we presented the methodologies and results of two
systems that perform statistical analysis on the semantics of prepositions, each
Semantic Role Labelling of Prepositional Phrases 789
using a different data set. The performance of the 2 systems was very differ-
ent. The SRD system trained on the treebank produced highly credible results,
whereas the SRL system trained on CoNLL 2004 SRL data set produced some-
what negative results. In the remainder of this section, we will analyze these
results and discuss their significance.
There is a significant difference between the results obtained by the tree-
bank classifier and that obtained by the CoNLL SRL classifier. In fact, even
with a very small number of collocation features, the treebank classifier still
outperformed the CoNLL SRL classifier. This suggests that the semantic tag-
ging of prepositions is somewhat artificial. This is evident in three ways. First,
the proportion of prepositional phrases tagged with semantic roles is small ?
around 57,000 PPs out of the million-word Treebank corpus. This small pro-
portion suggests that the preposition semantic roles were tagged only in cer-
tain prototypical situations. Second, we were able to achieve reasonably high
results even when we used a collocation feature set with fewer than 200 fea-
tures. This further suggests that the semantic roles were tagged for only a small
number of verbs in relatively fixed situations. Third, the preposition SRD sys-
tem for the CoNLL data set used a very similar feature set to the treebank
system, but was not able to produce anywhere near comparable results. Since
the CoNLL dataset is aimed at holistic SRL across all argument types, it in-
corporates a much larger set of verbs and tagging scenarios; as a result, the
semantic role labelling of PPs is far more heterogeneous and realistic than is
the case in the treebank. Therefore, we conclude that the results of our tree-
bank preposition SRD system are not very meaningful in terms of predict-
ing the success of the method at identifying and semantically labelling PPs
in open text.
A few interesting facts came out of the results over the CoNLL dataset. The
most important one is that by using an independent preposition SRL system,
the results of a general verb SRL system can be significantly boosted. This
is evident because when the oracled results of all three subtasks were used, the
merged results were around 10% higher than those for the original systems, in all
three cases. Unfortunately, it was also evident from the results that we were not
successful in automating preposition SRL. Due to the strictness of the CoNLL
evaluation, it was not always possible to achieve a better overall performance
by improving just one of the three subsystems. For example, in some cases,
worse results were achieved by using the oracled results for PPA, and the results
produced by SRD classifier than using the PPA classifier and the SRD classifiers
in conjunction. The reason for the worse results is that in our experiments, the
oracled PPA always identifies more prepositions attached to verbs than the PPA
classifier, therefore more prepositions will be given semantic roles by the SRD
classifier. However, since the performance of the SRD classifier is not high, and
the segmentation subsystem does not always produce the same semantic role
boundaries as the CoNLL data set, most of these additional prepositions would
either be given a wrong semantic role or wrong phrasal extent (or both), thereby
causing the overall performance to fall.
790 P. Ye and T. Baldwin
Finally, it is evident that the merging strategy also plays an important role
in determining the performance of the merged preposition SRL and verb SRL
systems: when the performance of the preposition SRL system is high, a more
preposition-oriented merging scheme would produce better overall results, and
vice versa.
5 Conclusion and Future Work
In this paper, we have proposed a method for labelling preposition semantics and
deployed the method over two different data sets involving preposition semantics.
We have shown that preposition semantics is not a trivial problem in general,
and also that has the potential to complement other semantic analysis tasks,
such as semantic role labelling.
Our analysis of the results of the preposition SRL system shows that sig-
nificant improvement in all three stages of preposition semantic role labelling?
namely verb attachment, preposition semantic role disambiguation and argu-
ment segmentation?must be achieved before preposition SRL can make a sig-
nificant contribution to holistic SRL. The unsatisfactory results of our CoNLL
preposition SRL system show that the relatively simplistic feature sets used in
our research are far from sufficient. Therefore, we will direct our future work
towards using additional NLP tools, information repositories and feature engi-
neering to improve all three stages of preposition semantic role labelling.
Acknowledgements
We would like to thank Phil Blunsom and Steven Bird for their suggestions and
encouragement, Tom O?Hara for providing insight into the inner workings of
his semantic role disambiguation system, and the anonymous reviewers for their
comments.
References
1. Marcus, M.P., Marcinkiewicz, M.A., Santorini, B.: Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics 19 (1993) 313?330
2. O?Hara, T., Wiebe, J.: Preposition semantic classification via treebank and
FrameNet. In: Proc. of the 7th Conference on Natural Language Learning (CoNLL-
2003), Edmonton, Canada (2003)
3. Miller, G.A.: WordNet: a lexical database for English. Communications of the ACM
38 (1995) 39?41
4. Berger, A.L., Pietra, V.J.D., Pietra, S.A.D.: A maximum entropy approach to
natural language processing. Computational Linguistics 22 (1996) 39?71
5. Carreras, X., Ma`rquez, L.: Introduction to the CoNLL-2004 shared task: Seman-
tic role labeling. In: Proc. of the 8th Conference on Natural Language Learning
(CoNLL-2004), Boston, USA (2004) 89?97
Semantic Role Labelling of Prepositional Phrases 791
6. Briscoe, T., Carroll, J.: Robust accurate statistical annotation of general text. In:
Proc. of the 3rd International Conference on Language Resources and Evaluation
(LREC 2002), Las Palmas, Canary Islands (2002) 1499?1504
7. Hacioglu, K., Pradhan, S., Ward, W., Martin, J.H., Jurafsky, D.: Semantic role
labeling by tagging syntactic chunks. In: Proc. of the 8th Conference on Natural
Language Learning (CoNLL-2004), Boston, USA (2004)
8. Punyakanok, V., Roth, D., Yih, W.T., Zimak, D., Tu, Y.: Semantic role labeling
via generalized inference over classifiers. In: Proc. of the 8th Conference on Natural
Language Learning (CoNLL-2004), Boston, USA (2004)
9. Carreras, X., Ma`rquez, L., Chrupa, G.: Hierarchical recognition of propositional
arguments with perceptrons. In: Proc. of the 8th Conference on Natural Language
Learning (CoNLL-2004), Boston, USA (2004)
Automatic Interpretation of Noun Compounds
Using WordNet Similarity
Su Nam Kim1,2 and Timothy Baldwin2,3
1 Computer Science, University of Illinois, Chicago, IL 60607 USA
sunamkim@gmail.com
2 Computer Science and Software Engineering,
University of Melbourne, Victoria 3010 Australia
3 NICTA Victoria Lab, University of Melbourne, Victoria 3010 Australia
tim@csse.unimelb.edu.au
Abstract. The paper introduces a method for interpreting novel noun compounds
with semantic relations. The method is built around word similarity with pre-
tagged noun compounds, based on WordNet::Similarity. Over 1,088
training instances and 1,081 test instances from the Wall Street Journal in the
Penn Treebank, the proposed method was able to correctly classify 53.3% of the
test noun compounds. We also investigated the relative contribution of the modi-
fier and the head noun in noun compounds of different semantic types.
1 Introduction
A noun compound (NC) is an ?N made up of two or more nouns, such as golf club or
paper submission; we will refer to the rightmost noun as the head noun and the re-
mainder of nouns in the NC as modifiers. The interpretation of noun compounds is a
well-researched area in natural language processing, and has been applied in applica-
tions such as question answering and machine translation [1,2,3]. Three basic properties
make the interpretation of NCs difficult [4]: (1) the compounding process is extremely
productive; (2) the semantic relationship between head noun and modifier in the noun
compounds is implicit; and (3) the interpretation can be influenced by contextual and
pragmatic factors.
In this paper, we are interested in recognizing the semantic relationship between the
head noun and modifier(s) of noun compounds. We introduce a method based on word
similarity between the component nouns in an unseen test instance NC and annotated
training instance NCs. Due to its simplicity, our method is able to interpret NCs with
significantly reduced cost. We also investigate the relative contribution of the head noun
and modifier in determining the semantic relation.
For the purposes of this paper, we focus exclusively on binary NCs, that is NCs
made up of two nouns. This is partly an empirical decision, in that the majority of
NCs occurring in unrestricted text are binary,1 and also partly due to there being ex-
isting methods for disambiguating the syntactic structure of higher-arity NCs, effec-
tively decomposing them into multiple binary NCs [3]. Note also that in this paper, we
1 We estimate that 88.4% of NCs in the Wall Street Journal section of the Penn Treebank and
90.6% of NCs in the British National Corpus are binary.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 945?956, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
946 S.N. Kim and T. Baldwin
distinguish semantic relations from semantic roles. The semantic relation in an NC is
the underlying relation between the head noun and its modifier, whereas its semantic
role is an indication of its relation to the governing verb and other constituents in the
sentence context.
There is a significant body of closely-related research on interpreting semantic rela-
tions in NCs which relies on hand-written rules. [5] examined the problem of interpre-
tation of NCs and constructed a set of hand-written rules. [6] automatically extracted
semantic information from an on-line dictionary and manipulated a set of hand-written
rules to assign weights to semantic relations. Recently, there has been work on the auto-
matic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is
based on a simplifying assumption as to the scope of semantic relations or the domain
of interpretation, making it difficult to compare the performance of NC interpretation
in a broader context.
In the remainder of the paper, we detail the motivation for our work (Section 2),
introduce the WordNet::Similarity system which we use to calculate word sim-
ilarity (Section 3), outline the set of semantic relations used (Section 4), detail how we
collected the data (Section 5), introduce the proposed method (Section 6), and describe
experimental results (Section 7).
2 Motivation
Most work related to interpreting NCs depends on hand-coded rules [5]. The first at-
tempt at automatic interpretation by [6] showed that it was possible to successfully
interpret NCs. However, the system involved costly hand-written rules involving man-
ual intervention. [9] estimated the amount of world knowledge required to interpret
NCs and claimed that the high cost of data acquisition offsets the benefits of automatic
interpretation of NCs.
Recent work [4,7,8] has investigated methods for interpreting NCs automatically
with minimal human effort. [10] introduced a semi-automatic method for recogniz-
ing noun?modifier relations. [4] examined nominalizations (a proper subset of NCs) in
terms of whether the modifier is a subject or object of the verb the head noun is derived
from (e.g. language understanding = understand language). [7] assigned hierarchical
tags to nouns in medical texts and classified them according to their semantic relations
using neural networks. [8] used the word senses of nouns to classify the semantic re-
lations of NCs. However, in all this work, there has been some underlying simplifying
assumption, in terms of the domain or range of interpretations an NC can occur with,
leading to questions of scalability and portability to novel domains/NC types.
In this paper, we introduce a method which uses word similarity based on WordNet.
Word similarity has been used previously in various lexical semantic tasks, including
word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a con-
text space can be used to disambiguate word senses. [12] measured the relatedness of
concepts using similarity based on WordNet. [13] examined the task of disambiguating
noun groupings with respect to word senses using similarity between nouns in NCs.
Our research uses similarities between nouns in the training and test data to interpret
the semantic relations of novel NCs.
Automatic Interpretation of Noun Compounds 947
apple juice morning milk
chocolate milk
MATERIAL TIME
s12s11 s21 s22
Fig. 1. Similarity between test NC chocolate milk and training NCs apple juice and morning milk
Table 1. WordNet-based similarities for component nouns in the training and test data
Training noun Test noun Sij
t1 apple chocolate 0.71
t2 juice milk 0.83
t1 morning chocolate 0.27
t2 milk milk 1.00
Figure 1 shows the correspondences between two training NCs, apple juice and
morning milk, and a test NC, chocolate milk; Table 1 lists the noun pairings and noun?
noun similarities based on WordNet. Each training noun is a component noun from
the training data, each test noun is a component noun in the input, and Sij provides
a measure of the noun?noun similarity in training and test, where t1 is the modifier
and t2 is the head noun in the NC in question. The similarities in Table 1 were com-
puted by the WUP method [14] as implemented in WordNet::Similarity (see
Section 3).
The simple product of the individual similarities (of each modifier and head noun,
respectively) gives the similarity of the NC pairing. For example, the similarity between
chocolate milk and apple juice is 0.60, while that between chocolate milk and morning
milk is 0.27. Note that although milk in the input NC also occurs in a training exemplar,
the semantic relations for the individual NCs differ. That is, while apple juice is juice
made from apples (MATERIAL), morning milk is milk served in the morning (TIME).
By comparing the similarity of both elements of the input NC, we are able to arrive
at the conclusion that chocolate milk is more closely related to chocolate milk, which
provides the correct semantic relation of MATERIAL (i.e. milk made from/flavored with
chocolate). Unlike word sense disambiguation systems, our method does not need to
determine the particular sense in which each noun is used. The next example (Table 2)
shows how our method interprets NCs containing ambiguous nouns correctly.
One potential pitfall when dealing with WordNet is the high level of polysemy for
many lexemes. We analyze the effects of polysemy with respect to interest. Assume that
we have the two NCs personal interest (POSSESSION) and bank interest (CAUSE/TOPIC)
in the training data. Both contain the noun interest, with the meaning of a state of cu-
948 S.N. Kim and T. Baldwin
Table 2. The effects of polysemy on the similarities between nouns in the training and test data
Training noun Test noun Sij
t1 personal loan 0.32
t2 interest rate 0.84
t1 bank loan 0.75
t2 interest rate 0.84
Table 3. Varying contribution of the head noun and modifier in predicting the semantic relation
Relative contribution of modifier/head noun Relation Example
modifier < head noun PROPERTY elephant seal
modifier = head noun EQUATIVE composer arranger
modifier > head noun TIME morning class
riosity or concern about something in personal interest, and an excess or bonus beyond
what is expected or due in bank interest. Given the test NC loan rate, we would get the
desired result of bank interest being the training instance of highest similarity, leading
to loan rate being classified with the semantic relation of CAUSE/TOPIC. The similar-
ity between the head nouns interest and rate for each pairing of training and test NC is
identical, as the proposed method makes no attempt to disambiguate the sense of a noun
in each NC context, and instead aggregates the overall word-to-word similarity across
the different sense pairings. The determining factor is therefore the similarity between
the different modifier pairings, and the fact that bank is more similar to loan than is the
case for personal.
We also investigate the weight of the head noun and the modifier in determining
overall similarity. We expect for different relations, the weight of the head noun and
the modifier will be different. In the relation EQUATIVE, e.g., we would expect the
significance of the head noun to be the same as that of the modifier. In relations such as
PROPERTY, on the other hand, we would expect the head noun to play a more important
role than the modifier. Conversely, with relations such as TIME, we would expect the
modifier to be more important, as detailed in Table 3.
3 WordNet::Similarity
WordNet::Similarity2 [12] is an open source software package developed at
the University of Minnesota. It allows the user to measure the semantic similarity or
relatedness between a pair of concepts (or word senses), and by extension, between a
pair of words. The system provides six measures of similarity and three measures of
relatedness based on the WordNet lexical database [15]. The measures of similarity are
based on analysis of the WordNet isa hierarchy.
2 www.d.umn.edu/?tpederse/similarity.html
Automatic Interpretation of Noun Compounds 949
The measures of similarity are divided into two groups: path-based and information
content-based. We chose four of the similarity measures in WordNet::Similarity
for our experiments: WUP and LCH as path-based similarity measures, and JCN and
LIN as information content-based similarity measures. LCH finds the shortest path be-
tween nouns [16]; WUP finds the path length to the root node from the least com-
mon subsumer (LCS) of the two word senses that is the most specific word sense
they share as an ancestor [14]; JCN subtracts the information content of the LCS
from the sum [17]; and LIN scales the information content of the LCS relative to the
sum [18].
In WordNet::Similarity, relatedness goes beyond concepts being similar to
each other. That is, WordNet provides additional (non-hierarchical) relations such as
has-part and made-of. It supports our idea of interpretation of NCs by similarity.
However, as [19] point out, information on relatedness has not been developed as ac-
tively as conceptual similarity. Besides, the speed of simulating these relatedness effects
is too slow to use in practice. Hence, we did not use any of the relatedness measures in
this paper.
4 Semantic Relations
A semantic relation in the context of NC interpretation is the relation between the mod-
ifier and the head noun. For instance, family car relates to POSSESSION whereas sports
car relates to PURPOSE. [20] defined complex nominals as expressions that have a
head noun preceded by one or more modifying nouns or denominal adjectives, and
offered nine semantic labels after removing opaque compounds and adding nominal
non-predicating adjectives. [5] produced a diverse set of NC interpretations. Other re-
searchers have identified alternate sets of semantic relations, or conversely cast doubts
on the possibility of devising an all-purpose system of NC interpretations [21]. For our
work, we do not intend to create a new set of semantic relations. Based on our data,
we chose a pre-existing set of semantic relations that had previously been used for au-
tomatic (or semi-automatic) NC interpretation, namely the 20-member classification of
[10] (see Appendix). Other notable classifications include that of [6] which contains 13
relations based on WH questions, making it ideally suited to question answering appli-
cations. However, some relations such as TOPIC are absent. [7] proposed 38 relations for
the medical domain. Such relations are too highly specialized to this domain, and not
suitable for more general applications. [8] defined 35 semantic relations for complex
nominals and adjective phrases.
5 Data Collection
We retrieved binary NCs from the Wall Street Journal component of the Penn treebank.
We excluded proper nouns since WordNet does not contain even high-frequency proper
nouns such as Honda. We also excluded binary NCs that are part of larger NCs. In
tagging the semantic relations of noun compounds, we hired two annotators: two com-
puter science Ph.D students. In many cases, even human annotators disagree on the tag
allocation. For NCs containing more than one semantic relation, the annotators were
950 S.N. Kim and T. Baldwin
judged to have agreed is there was overlap in at least one of the relations specified by
them for a given NC. The initial agreement for the two annotators was 52.31%. From
the disagreement of tagged relations, we observed that decisions between SOURCE and
CAUSE, PURPOSE and TOPIC, and OBJECT and TOPIC frequently have lower agree-
ment. For the NCs where there was no agreement, the annotators decided on a set of
relations through consultation. The distribution of semantic relations is shown in the
Appendix. Overall, we used 1,088 NCs for the training data and 1,081 NCs for the
test data.
6 Method
Figure 2 shows how to compute the similarity between the ith NC in the test data
and jth NC in the training data. We calculate similarities for the component nouns of
the ith NC in the test data with all NCs in the training data. As a result, the modifier
and head noun in the ith test NC are each associated with a total of m similarities,
where m is the number of NCs in the training data. The second step is to multiply
the similarities of the modifier and head noun for all NCs in the training data; we ex-
periment with two methods for calculating the combined similarity. The third step is
to choose the NC in the training data which is most similar to the test instance, and
tag the test instance according to the semantic relation associated with that training
instance.
Formally, SA is the similarity between NCs (Ni,1, Ni,2) and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1) ? ((1 ? ?)S2 + S2))
2
(1)
where S1 is the modifier similarity (i.e. S(Ni,1, Bj1)) and S2 is head noun similarity
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
SB is an analogous similarity function, based on the F-score:
Bj1   Bj2
Bm1 Bm2
B31  B32
B21  B22
B11  B12
Relation2
Relation3
Relation19
Relation_k
Relation3
Ni1   Ni2
Nn1  Nn2
S(Ni1,B11)
S(Ni1,B21)
S(Ni1,Bj1)
S(Ni1,Bm1)
S(Ni2,B12)
S(Ni2,B22)
S(Ni2,Bj2)
S(Ni2,Bm2)
RELATIONNN
N11  N12
N21  N22 Similarity in detail
Fig. 2. Similarity between the ith NC in the test data and jth NC in the training data
Automatic Interpretation of Noun Compounds 951
SB((Ni,1, Ni,2), B(j,1, Bj,2)) = 2 ?
(S1 + ?S1) ? (S2 + (1 ? ?)S2)
(S1 + ?S1) + (S2 + (1 ? ?S2)) (2)
The semantic relation is determined by rel:
rel(Ni,1, Ni,2) = rel(Bm,1, Bm,2) (3)
where m = argmax
j
S((Ni,1, Ni,2), (Bj,1, Bj,2))
7 Experimental Results
7.1 Automatic Tagging Using Similarity
In our first experiment, we tag the test NCs with semantic relations using four different
measures of noun similarity, assuming for the time being that the contribution of the
modifier and head noun is equal (i.e. ? = 0.5). The baseline for this experiment is a
majority-class classifier, in which all NCs are tagged according to the TOPIC class.
Table 4. Accuracy of NC interpretation for the different WordNet-based similarity measures
Basis Method SA SB
majority class Baseline 465 (43.0%) 465 (43.0%)
path-based WUP 576 (53.3%) 557 (51.5%)
path-based LCH 572 (52.9%) 565 (52.3%)
information content-based JCN 505 (46.7%) 470 (43.5%)
information content-based LIN 512 (47.4%) 455 (42.1%)
human annotation Inter-annotator agreement 565 (52.3%) 565 (52.3%)
Table 4 shows that WUP, using the SA multiplicative method of combination, pro-
vides the highest NC interpretation accuracy, significantly above the majority-class
baseline. It is particularly encouraging to see that WUP performs at or above the level
of inter-annotator agreement (52.3%), which could be construed as a theoretical upper
bound for the task as defined here. Using the F-score measure of similarity, LCH has
nearly the same performance as WUP. Among the four measures of similarity used in
this first experiment, the path-based similarity measures have higher performance than
the information content-based methods over both similarity combination methods.
Compared to prior work on the automatic interpretation of NCs, our method
achieves relatively good results. [7] achieved about 60% performance over the medical
domain. [8] used a word sense disambiguation system to achieve around 43% accuracy
interpreting NCs in the open domain. Our accuracy of 53% compares favourably to both
of these sets of results, given that we are operating over open domain data.
7.2 Relative Contribution of Modifier and Head Noun
In the second experiment, we investigated the relative impact of the modifier and head
noun in determining the overall similarity of the NC. While tagging the NCs, we got
952 S.N. Kim and T. Baldwin
(accuracy %)
alpha value
 48.5
 49
 49.5
 50
 50.5
 51
 51.5
 52
0.0 0.2 0.4 0.6 0.8 1.0
% w/ different weight
Fig. 3. Classifier accuracy at different ? values
be
ne
fic
ia
ry
a g
e n
t
ca
u
se
co
n
ta
in
er
co
n
te
nt
de
sti
na
tio
n
eq
ua
tiv
e
in
str
um
en
t
lo
ca
te
d
lo
ca
tio
n
m
at
er
ia
l
o
bje
ct
po
ss
es
so
r
pr
od
uc
t
pr
op
er
ty
re
su
lt
pu
rp
os
e
so
u
rc
e
tim
e
to
pi
c
(accuracy %)
(relation) 0
 20
 40
 60
 80
 100
 0
?wup 5:5
?wup 8:2
?wup 2:8
Fig. 4. Classification accuracy for each semantic relation at different ? values
a sense of modifiers and head nouns having variable impact on the determination of
the overall NC semantic relation. For this test, we used the WUP method based on our
results from above and also because it operates over the scale [0, 1], removing any need
for normalization. In this experiment, modifiers and head nouns were assigned weights
(? in Equations 1 and 2) in the range 0.0, 0.1, ...1.0.
Figure 3 shows the relative contribution of the modifier and head noun in the over-
all NC interpretation process. Interestingly, the head noun seems to be a more reliable
predictor of the overall NC interpretation than the modifier, and yet the best accuracy
is achieved when each noun makes an equal contribution to the overall interpretation
(i.e. ? = 0.5). Thus suggests that, despite any localized biases for individual NC inter-
pretation types, the modifier and head noun have an equal impact on NC interpretation
overall.
Automatic Interpretation of Noun Compounds 953
Bm1 Bm2
Bn1 Bn2
Bn1 Bn2
Bm1 Bm2Ni1 Ni2 Correct
Answer
0.82
0.79
Ni1 Ni2
0.45 Incorrect
Answer
Nj1 Nj2
Nj1 Nj2
0.79
0.45
Correct
Answer
ith step (i+1)th step
Fig. 5. Accumulating correctly tagged data
Figure 4 shows a breakdown of accuracy across the different semantic relation types
for different weights. In Figure 4, we have shown only the weights 0.2, 0.5 and 0.8 (to
show the general effect of variation in ?). The dashed line shows the performance when
the weight of modifiers and head nouns is the same (? = 0.5). The ? symbol shows the
results of modifier-biased interpretation (? = 0.8) and the + symbol shows the results
of head noun-biased interpretation (? = 0.2). From Figure 4, we can see that for rela-
tions such as CAUSE and INSTRUMENT, the modifier plays a more important role in the
determination of the semantic relation of the NC. On the other hand, for the CONTENT
and PROPERTY relations, the head noun contributes more to NC interpretation. Unex-
pectedly, for EQUATIVE, the head noun contributes more than the modifier, although
only 9 examples were tagged with EQUATIVE, such that the result shown may not be
very representative of the general behavior.
8 Discussion
We have presented a method for interpreting the semantic relations of novel NCs using
word similarity. We achieved about 53% interpretation accuracy using a path-based
measure of similarity. Since our system was tested over raw test data from a general
domain, we demonstrated that word similarity has surprising potential for interpreting
the semantic relations of NCs. We also investigated using different weights for the head
noun and modifier to find out how much the modifier and head noun contributes in NC
interpretation and found that, with the exception of some isolated semantic relations,
their relative contribution is equal.
Our method has advantages such its relative simplicity and ability to run over small
amounts of training data, but there are also a few weaknesses. The main bottleneck is
the availability of training data to use in classifying test instances. We suggest that we
could use a bootstrap method to overcome this problem: in each step of classification,
NCs which are highly similar to training instances, as determined by some threshold on
similarity, are added to the training data to use in the next iteration of classification. One
way to arrive at such a threshold is to analyze the relative proportion of correctly- and
incorrectly-classified instances at different similarity levels, through cross-validation
over the training data. We generate such a curve for the test data, as detailed in Fig-
ure 6.
If we were to use the crossover point (similarity ? 0.57), we would clearly ?infect?
the training data with a significant number of misclassified instances, namely 30.69%
of the new training instances; this would have an unpredictable impact on classifica-
tion performance. On the other hand, if we were to select a higher threshold based on
a higher estimated proportion of correctly-classified instances (e.g. 70%), the relative
954 S.N. Kim and T. Baldwin
(accuracy %)
(similarity)
Error
Similarity=0.57
THRESHOLD
(a) error rate with similarity 0.57
 0
 20
 40
 60
 80
 100
 0  0.2  0.4  0.6  0.8  1
?correct.percent?
?incorrect.percent?
Fig. 6. The relative proportion of correctly- and incorrectly-classified NCs at different similarity
values, and the estimated impact of threshold-based bootstrapping
increase in training examples would be slight, and there would be little hope for much
impact on the overall classifier accuracy. Clearly, therefore, there is a trade-off here be-
tween how much training data we wish to acquire automatically and whether this will
impact negatively or positively on classification performance. We leave investigation
of this trade-off as an item for future research. Interestingly, in Figure 6 the propor-
tion of misclassified examples is monotonically decreasing, providing evidence for the
soundness of the proposed similarity-based model.
In the first experiment (where the weight of the modifier and head noun was the
same), we observed that some of the test NCs matched with several training NCs with
high similarity. However, since we chose only the NC with the highest similarity, we
ignored any insight other closely-matching training NCs may have provided into the
semantics of the test NC. One possible workaround here would be to employ a voting
strategy, for example, in taking the k most-similar training instances and determin-
ing the majority class amongst them. Once again, we leave this as an item for future
research.
Acknowledgements
We would like to express our thanks to Bharaneedharan Rathnasabapathy for helping
to tag the noun compound semantic relations, and the anonymous reviewers for their
comments and suggestions.
References
1. Cao, Y., Li, H.: Base noun phrase translation using web data and the em algorithm. In:
COLING2002. (2002)
2. Baldwin, T., Tanaka, T.: Translation by machine of compound nominals: Getting it right. In:
ACL2004-MWE, Barcelona, Spain (2004) 24?31
Automatic Interpretation of Noun Compounds 955
3. Lauer, M.: Designing Statistical Language Learners: Experiments on Noun Compounds.
PhD thesis, Macquarie University (1995)
4. Lapata, M.: The disambiguation of nominalizations. Comput. Linguist. 28 (2002) 357?388
5. Finin, T.W.: The semantic interpretation of compound nominals. PhD thesis, University of
Illinois, Urbana, Illinois, USA (1980)
6. Vanderwende, L.: Algorithm for automatic interpretation of noun sequences. In: Proceedings
of the 15th conference on Computational linguistics. (1994) 782?788
7. Rosario, B., Marti, H.: Classifying the semantic relations in noun compounds via a domain-
specific lexical hierarchy. In: Proceedings of the 2001 Conference on Empirical Methods in
Natural Language Processing. (2001) 82?90
8. Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., Girju, R.: Models for the semantic
classification of noun phrases. HLT-NAACL 2004: Workshop on Computational Lexical
Semantics (2004) 60?67
9. Fan, J., Barker, K., Porter, B.W.: The knowledge required to interpret noun com-
pounds. In: Seventh International Joint Conference on Artificial Intelligence. (2003)
1483?1485
10. Barker, K., Szpakowicz, S.: Semi-automatic recognition of noun modifier relationships. In:
Proceedings of the 17th international conference on Computational linguistics. (1998) 96?
102
11. Artiles, J., Penas, A., Verdejo, F.: Word sense disambiguation based on term to term simi-
larity in a context space. In: Senseval-3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text. (2004) 58?63
12. Patwardhan, S., Banerjee, S., Pedersen, T.: Using measures of semantic relatedness for word
sense disambiguation. In: Proceedings of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics. (2003)
13. Resnik, P.: Disambiguating noun groupings with respect to wordnet senses. In: Proceedings
of the 3rd Workship on Very Large Corpus. (1995) 77?98
14. Wu, Z., Palmer, M.: Verb semantics and lexical selection. In: 32nd. Annual Meeting of the
Association for Computational Linguistics. (1994) 133 ?138
15. Fellbaum, C., ed.: WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA
(1998)
16. Leacock, C., Chodorow, N.: Combining local context and wordnet similarity for word sense
identification. [15]
17. Jiang, J., Conrath, D.: Semantic similarity based on corpus statistics and lexical taxon-
omy. In: Proceedings on International Conference on Research in Computational Linguistics.
(1998) 19?33
18. Lin, D.: An information-theoretic definition of similarity. In: Proceedings of the International
Conference on Machine Learning. (1998)
19. Banerjee, S., Pedersen, T.: Extended gloss overlaps as a measure of semantic relatedness.
In: Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence.
(2003) 805?810
20. Levi, J.: The syntax and semantics of complex nominals. In: New York:Academic Press.
(1979)
21. Downing, P.: On the creation and use of English compound nouns. Language 53 (1977)
810?42
956 S.N. Kim and T. Baldwin
Appendix
Table 5. The Semantic Relations in Noun Compounds (N1 = modifier, N2 = head noun)
Relation Definition Example # of test/training
AGENT N2 is performed by N1 student protest, band concert 10(2)/5
BENEFICIARY N1 benefits from N2 student price, charitable compound 10(1)/7(1)
CAUSE N1 causes N2 printer tray, flood water 54(10)/74(11)
CONTAINER N1 contains N2 exam anxiety 13(6)/19(5)
CONTENT N1 is contained in N2 paper tray, eviction notice 40(5)/34(7)
DESTINATION N1 is destination of N2 game bus, exit route 2(1)/2
EQUATIVE N1 is also head composer arranger, player coach 9/17(3)
INSTRUMENT N1 is used in N2 electron microscope, diesel engine 6/11(2)
LOCATED N1 is located at N2 building site, home town 12(2)/16(4)
LOCATION N1 is the location of N2 lab printer, desert storm 29(10)/24(5)
MATERIAL N2 is made of N1 carbon deposit, gingerbread man 12(1)/15(2)
OBJECT N1 is acted on by N2 engine repair, horse doctor 88(16)/88(21)
POSSESSOR N1 has N2 student loan, company car 32(3)/22(4)
PRODUCT N1 is a product of N2 automobile factory, light bulb 27(1)/32(9)
PROPERTY N2 is N1 elephant seal 76(5)/85(7)
PURPOSE N2 is meant for N1 concert hall, soup pot 160(23)/160(23)
RESULT N1 is a result of N2 storm cloud, cold virus 7(4)/8(1)
SOURCE N1 is the source of N2 chest pain, north wind 86(21)/99(18)
TIME N1 is the time of N2 winter semester, morning class 26(2)/19
TOPIC N2 is concerned with N1 computer expert, safety standard 465(51)/446(60)
The 4thcolumn gives us the number of words tagged with the corresponding relation in the
1stcolumn. The numbers within the parenthesis gives us the number of words that are tagged
with multiple relations( i.e. those that are tagged with the relation in the 1stcolumn and other
relations as well). In the training data, 94 NCs have multiple relations and in test data, 81 NCs
have multiple relations.
Benchmarking Noun Compound Interpretation
Su Nam Kim and Timothy Baldwin
Department of Computer Science and Software Engineering
and
NICTA Victoria Lab
University of Melbourne, VIC 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper we provide benchmark results
for two classes of methods used in inter-
preting noun compounds (NCs): semantic
similarity-based methods and their hybrids.
We evaluate the methods using 7-way and
binary class data from the nominal pair in-
terpretation task of SEMEVAL-2007.1 We
summarize and analyse our results, with
the intention of providing a framework for
benchmarking future research in this area.
1 Introduction
This paper reviews a range of simple and hybrid
approaches to noun compound (NC) interpretation.
The interpretation of NCs such as computer science
and paper submission involves predicting the se-
mantic relation (SR) that underlies a given NC. For
example, student price conventionally expresses the
meaning that a student benefits from the price (SR
= BENEFICIARY), while student protest conven-
tionally means a student undertaking a protest (SR
= AGENT).2
NCs are formed from simplex nouns with high
productivity. The huge number of possible NCs and
potentially large number of SRs makes NC interpre-
tation a very difficult problem. In the past, much NC
interpretation work has been carried out which tar-
gets particular NLP applications such as information
extraction, question-answering and machine trans-
lation. Unfortunately, much of it has not gained
1The 4th International Workshop on Semantic Evaluation
2SRs used in the examples are taken from Barker and Sz-
pakowicz (1998).
traction in real-world applications as the accuracy
of the methods has not been sufficiently high over
open-domain data. Most prior work has been car-
ried out under specific assumptions and with one-
off datasets, which makes it hard to analyze perfor-
mance and to build hybrid methods. Additionally,
disagreement in the inventory of SRs and a lack of
resource sharing has hampered comparative evalua-
tion of different methods.
The first step in NC interpretation is to define a set
of SRs. Levi (1979), for example, proposed a system
of 9 SRs, while others have proposed classifications
with 20-30 SRs (Finin, 1980; Barker and Szpakow-
icz, 1998; Moldovan et al, 2004). Smaller sets tend
to have reduced coverage due to coarse granularity,
whereas larger sets tend to be too fine grained and
suffer from low inter-annotator agreement. Addi-
tionally pragmatic/contextual differentiation leads to
difficulties in defining and interpreting SRs (Down-
ing, 1977; SparckJones, 1983).
Recent attempts in the area of NC interpretation
have taken two basic approaches: analogy-base in-
terpretation (Rosario, 2001; Moldovan et al, 2004;
Kim and Baldwin, 2005; Girju, 2007) and seman-
tic disambiguation relative to an underlying predi-
cate or semantically-unambiguous paraphrase (Van-
derwende, 1994; Lapata, 2002; Kim and Baldwin,
2006; Nakov, 2006). Most methods employ rich on-
tologies and ignore the context of use, supporting
the claim by Fan (2003) that axioms and ontological
distinctions are more important than detailed knowl-
edge of specific nouns for NC interpretation. Addi-
tionally, most approaches use supervised learning,
raising questions about the generality of the test and
569
training data sets and the effectiveness of the algo-
rithms in different domains (coverage of SRs over
the NCs is another issue).
Our aim in this paper is to compare and analyze
existing NC interpretation methods over a common,
publicly available dataset. While recent research
has made significant progress, bringing us one step
closer to practical applicability in NLP applications,
no direct comparison or analysis of the approaches
has been attempted to date. As a result, it is hard to
determine which approach is appropriate in a given
domain or build hybrid methods based on prior ap-
proaches. We also investigate the impact on perfor-
mance of relaxing assumptions made in the origi-
nal research, to compare different approaches in an
identical setting.
In the remainder of the paper, we review the re-
search background and NC interpretation methods
in Section 2, describe the methods and system archi-
tectures in Section 3, detail the datasets used in our
experiments in Section 4, carry out a system evalu-
ation in Section 5 and Section 6, and finally present
a discussion and conclusions in Section 7 and Sec-
tion 8, respectively.
2 Background and Methods
2.1 Research Background
In this study, we selected three semantic similar-
ity based models which had been found to perform
strongly in previous research, and which were easy
to re-implement: SENSE COLLOCATION (Moldovan
et al, 2004), CONSTITUENT SIMILARITY (Kim
and Baldwin, 2005) and CO-TRAINING, e.g. using
SENSE COLLOCATION or CONSTITUENT SIMILAR-
ITY (Kim and Baldwin, 2007). These approaches
were evaluated over a 7-way classification using
open-domain data from the nominal pair interpre-
tation task of SEMEVAL-2007 (Girju et al, 2007).
We test their performance in both 7-way and binary-
class classification settings.
2.2 Sense Collocation Method
The SENSE COLLOCATION method of Moldovan et
al. (2004) is based on the pair of word senses of NC
constituents. The basic idea is that NCs which have
the same or similar sense collocation tend to have
the same SR. As an example, car factory and auto-
mobile factory share the conventional interpretation
of MAKE, which is predicted by car and automo-
bile having the same sense across the two NCs, and
factory being used with the same sense in each in-
stance. This intuition is formulated in Equations 1
and 2 below.
The probability P (r|fifj) (simplified to
P (r|fij)) of a SR r for word senses fi and fj
is calculated based on simple maximum likelihood
estimation:
P (r|fij) = n(r, fij)n(fij) (1)
The preferred SR r? for the given sense combina-
tion is that which maximises the probability:
r? = argmaxr?RP (r|fij)
= argmaxr?RP (fij |r)P (r) (2)
2.3 Constituent Similarity Method
The intuition behind the CONSTITUENT SIMILAR-
ITY method is similar to the SENSE COLLOCATION
method, in that NCs made up of similar words tend
to share the same SR. The principal difference is that
it doesn?t presuppose that we know the word sense
of each constituent word (i.e. the similarity is cal-
culated at the word rather than sense level). The
method takes the form of a 1-nearest neighbour clas-
sifier, with the best-matching training instance for
each test instance predicting its SR. For example,
we may find that test instance chocolate milk most
closely matches apple juice and hence predict that
the SR is MATERIAL.
This idea is formulated in Equation 3 below. For-
mally, SA is the similarity between NCs (Ni,1, Ni,2)
and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1)? ((1? ?)S2 + S2))
2 (3)
where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is the head noun similarity
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
The similarity scores are calculated across the bag
of WordNet senses (without choosing between
570
them) using the method of Wu and Palmer (1994) as
implemented in WordNet::Similarity (Pat-
wardhan et al, 2003). This is done for each pairing
of WordNet senses of the two words in question,
and the overall lexical similarity is calculated as the
average across the pairwise sense similarities.
2.4 Co-Training by Sense Collocation
Co-training by sense collocation (SCOLL CO-
TRAINING) is based on the SENSE COLLOCATION
method and lexical substitution (Kim and Baldwin,
2007). It expands the set of training NCs from
a relatively small number of manually-tagged seed
instances. That is, it makes use of extra train-
ing instances fashioned through a bootstrap process.
For example, assuming automobile factory with the
SR MAKE were a seed instance, NCs generated
from synonyms, hypernyms and sister words of its
constituents would be added as extra training in-
stances, with the same SR of MAKE. That is, we
would add car factory (SYNONYM), vehicle fac-
tory (HYPERNYM) and truck factory (SISTER
WORD), for example. Note that the substitution
takes place only for one constituent at a time to avoid
extreme variation.
2.5 Co-training by Constituent Similarity
Co-training by Constituent Similarity (CS CO-
TRAINING) is also a co-training method, but based
on CONSTITUENT SIMILARITY rather than SENSE
COLLOCATION. The basic idea is that when NCs
are interpreted using the CONSTITUENT SIMILAR-
ITY method, the predictions are more reliable when
the lexical similarity is higher. Hence, we progres-
sively reduce the similarity threshold, and incorpo-
rate higher-similarity instances into our training data
earlier in the bootstrap process. That is, we run
the CONSTITUENT SIMILARITY method and acquire
NCs with similarity equal to or greater than a fixed
threshold. Then in the next iteration, we add the ac-
quired NCs into the training dataset for use in clas-
sifying more instances. As a result, in each step,
the number of training instances increases monoton-
ically. We ?cascade? through a series of decreas-
ing similarity thresholds until we reach a saturation
point. As our threshold, we used a starting value of
0.90, which was decremented down to 0.65 in steps
of 0.05.
Method Description
SCOLL sense collocation
SCOLLCT sense collocation + SCOLL co-training
CSIM constituent similarity
CSIM +SCOLLCT constituent similarity + SCOLL co-training
HYBRID SCOLL + CSIM + SCOLLCT
CSIMCT constituent similarity + CSIM co-training
Table 1: Systems used in our experiments
TEST
untagged test data
untagged test data
untagged test data
untagged test data
tagged data
tagged data
tagged data
tagged data
tagged data
TRAIN
Extension ofTraining databy similar words
? Synonym
? Hypernym
? Sister word
Extended TRAIN
Sense Collcation
Step 1
Similarity
Step 2
Step 3
Step 4
Similarity
Step 5
Sense Collcation
Similarity
Figure 1: Architecture of the HYBRID method
3 Systems and Architectures
We tested the original methods of Moldovan et al
(2004) and Kim and Baldwin (2005), and combined
them with the co-training methods of Kim and Bald-
win (2007) to come up with six different hybrid sys-
tems for evaluation, as detailed in Table 1. To build
the classifiers, we used the TIMBL5.0 memory-
based learner (Daelemans et al, 2004).
The HYBRID method consists of five interpreta-
tion steps. The first step is to use the SENSE COL-
LOCATION method over the original training data.
When the sense collocation of the test and train-
ing instances is the same, we judge the predicted
SR to be correct. The second step is to apply the
CONSTITUENT SIMILARITY method over the origi-
nal training data. In order to confirm that the pre-
dicted SR is correct, we use a threshold of 0.8 to
interpret the test instances. The third step is to ap-
ply SENSE COLLOCATION over the expanded train-
571
TRAIN
#of Tagged
>= 10% of testThreshold
Tagged
finalize current
tags and end
reduce Threshold
TEST
get Similarity
Sim >= TN Y
Y
N
if T == 0.6 &(#of Tagged <
10% of test)
N
Y
Figure 2: Architecture of the CSIMCT system
ing data through the advent of hypernyms and sis-
ter words, using the SCOLL CO-TRAINING method.
This step benefits from a larger amount of training
data (17,613 vs. 937). The fourth step is to apply
the CONSTITUENT SIMILARITY method (EXTCS)
over the consolidated training data, with the thresh-
old unchanged at 0.8. The final step is to apply the
CONSTITUENT SIMILARITY (CSTT) method over
the combined training data without any restriction
on the threshold (to guarantee a SR prediction for
every test instance). We select SRs from the training
instances whose similarity is higher than the origi-
nal training data and expanded training data. How-
ever, since the generated training instances are more
likely to contain errors, we apply a linear weight of
0.8 to the similarity values for the expanded train-
ing instances. This gives preferential treatment to
predictions based on the original training instances.
Note that this weight was based on analysis of the
error rate in the expanded training instances. In pre-
vious work (Kim and Baldwin, 2007), we found the
overall classification accuracy rate after the first it-
eration to be 70-80%. Hence, we settled on a weight
of 0.8.
The CSIMCT system is based solely on the CON-
STITUENT SIMILARITY method with cascading. We
perform iterative CS co-training as described in Sec-
tion 2.5, with the slight variation that we hold off
Binary 7-way
SR Test Train Train* Test Train Train*
CE 80 136 2,588 36 71 1,854
IA 78 135 1,400 36 68 1,001
PP 93 126 2,591 55 78 2,089
OE 81 136 3,085 35 52 1,560
TT 71 129 2,994 27 50 1,718
PW 72 138 2,577 28 64 1,510
CC 74 137 2,378 37 63 1,934
Total 549 937 17,613 254 446 11,664
Table 3: Number of instances associated with each
SR (Train* is the number of expanded train in-
stances)
on reducing the threshold if less than 10% of the
test instances are tagged on a given iteration, giving
other test instances a chance to be tagged at a higher
threshold level relative to newly generated training
instances. The residue of test instances on comple-
tion of the final iteration (threshold = 0.6) are tagged
according to the best-matching training instance, ir-
respective of the magnitude of the similarity.
4 Data
We used the dataset from the SEMEVAL-2007
nominal pair interpretation task, which is based
on 7 SRs: CAUSE-EFFECT (CE), INSTRUMENT-
AGENCY (IA), PRODUCT-PRODUCER (PP),
ORIGIN-ENTITY (OE), THEME-TOOL (TT),
PART-WHOLE (PW), CONTENT-CONTAINER
(CC). The task in SEMEVAL-2007 was to identify
the compatibility of a given SR for each test
instances using word senses retrieved from WORD-
NET 3.0 (Fellbaum, 1998) and queries. Table 2
shows the definition of the SRs.
In our research, we interpret the dataset in two
ways: (1) as a binary classification task for each SR
based on the original data; and (2) as a 7-way clas-
sification task, combining together all positive test
and training instances for each of the 7 SR datasets
into a single dataset. Hence, the size of the dataset
for 7-way classification is much smaller than that of
the original dataset. We also expand the training in-
stances using SCOLL CO-TRAINING. Table 3 de-
scribes the number of test and train instances for NC
interpretation for the binary and 7-way classification
tasks.
Our analysis shows that only 5 NCs are repeated
572
Semantic relation Definition Examples
Cause-Effect (CE) N1 is the cause of N2 virus flu, hormone growth
Instrument-Agency (IA) N1 is the instrument of N2; N2 uses N1 laser printer, axe murderer
Product-Producer (PP) N1 is a product of N2; N2 produces N1 honey bee, music clock
Origin-Entity (OE) N1 is the origin of N2 bacon grease, desert storm
Theme-Tool (TT) N2 is intended for N1 reorganization process, copyright law
Part-Whole (PW) N1 is part of N2 table leg, daisy flower
Content-Container (CC) N1 is stored or carried inside N2 apple basket, wine bottle
Table 2: The set of 7 semantic relations, where N1 is the modifier and N2 is the head noun
across multiple SR datasets (i.e. occur as an instance
in more than one of the 7 datasets), none of which
occur as positive instances for multiple SRs. As
such, no NC instances in the 7-way classification
task end up with a multiclass classification. Also
note that some of NCs are contained within ternary
or higher-order NCs: 40 test NCs and 81 training
NCs for the binary classification task, and 24 test
NCs and 42 training NCs for the 7-way classification
task. For these NCs, we extracted a ?base? binary
NC based on the provided bracketing. The follow-
ing are examples of extraction of binary NCs from
ternary or higher-order NCs.
((billiard table) room) ? table room
(body (bath towel)) ? body towel
In order to extract a binary NC, we take the head
noun of each embedded NC and combine this with
the corresponding head noun or modifier. E.g., table
is the head noun of billiard table, which combines
with the head noun of the complex NC room to form
table room.
5 Experiment 1: 7-way classification
Our first experiment was carried out over the 7-way
classification task?i.e. all 7 SRs in a single classifi-
cation task?using the 6 systems from Section 3. In
our results in Table 4, we use the system categories
from SEMEVAL-2007 of A4 and B4, where A4 sys-
tems use none of the provided word senses, and B4
systems use the word senses.3 We categorized our
systems into these two groups in order to evaluate
them separately within the bounds of the original
SEMEVAL-2007 task. In each case, the baseline is
a majority class classifier.
3In the original SEMEVAL-2007 task, there were two fur-
ther categories, which incorporated the ?query? with or without
the sense information.
Class Method P R F1 A
? Majority .217
A4 CSIM .518 .522 .449 .528
CSIMCT .517 .511 .426 .522
B4 SCOLL .705 .444 .477 .496
SCOLLCT .646 .466 .498 .508
CSIM +SCOLLCT .523 .520 .454 .528
HYBRID .500 .505 .416 .516
Table 4: Experiment 1: Results (P=precision,
R=recall, F1=F-score, A=accuracy)
Step Method Tagged Ai Untagged
1 SCOLL 12 1.000 242
2 CSIM 57 .719 185
3 extSCOLL 0 .000 185
4 extCSIM 78 .462 107
5 CSIMREST 107 .393 0
Table 5: Experiment 1: Classifications for each
step of the HYBRID method (CSREST=the final ap-
plication of CS over the remaining test instances,
Ai=accuracy for classifications made at step i)
Tables 5 and 6 show the results at each step for
the HYBRID and CSIMCT methods, respectively. As
each method proceeds, the amount of tagged data in-
creases but the classification accuracy of the system
decreases, due to the inclusion of increasingly noisy
training instances in the previous step. The perfor-
mance of each individual relation is shown in Fig-
ure 3, which largely mirrors the findings of the sys-
tems in the original SEMEVAL-2007 task in terms
of the relative difficulty to predict each of the 7 SRs.
6 Experiment 2: binary classification
In the second experiment, we performed a separate
binary classification task for each of the 7 SRs, in
the manner of the original SEMEVAL-2007 task.
Table 7 shows the three baselines provided by the
SEMEVAL-2007 organisers and performance of our
573
Iteration ? Tagged Ai Untagged
1 .90 29 .897 225
2 .85 12 .750 213
3 .80 31 .613 182
4 .75 43 .535 139
5 .70 63 .540 76
6 .65 26 .346 50
7 <.65 49 .250 1
Table 6: Experiment 1: Classifications at each step
of the CSIMCT method (?=threshold, Ai=accuracy
for classifications made at iteration i)
CE IA OEPP TT PW CCRelations
Accuracy(%)
KE w/ multiple classes
 0
 20
 40
 60
 80
 100 precision
recallFscore
Figure 3: Experiment 1: Performance over each SR
(CSIM +SCOLLCT method)
6 systems. We also present the best-performing sys-
tem within each group from the SEMEVAL-2007
task. The methods for computing the baselines are
described in Girju et al (2007).
As with the first experiment, we analyzed the
number of tagged instances and accuracy for the HY-
BRID and CSIMCT methods, as shown in Tables 8
and 9, respectively. The overall results are similar to
those for the 7-way classification task.
Figures 4 and 5 show the performance for posi-
tive and negative classifications for each individual
SR. The performance when the classifier outputs are
mapped onto the 7-way classification task are simi-
lar to those in Figure 3.
7 Discussion and Conclusion
We compared the performance of the 6 systems in
Tables 4 and 7 over the 7-way and binary clas-
sification tasks, respectively. The performance of
all methods exceeded the baseline. The CON-
STITUENT SIMILARITY (CSIM) system performed
the best in group A4 and CONSTITUENT SIMILAR-
Class Method P R F1 A
? All True .485 1.000 .648 .485
? Probability .485 .485 .485 .517
? Majority .813 .429 .308 .570
A4 Best .661 .667 .648 .660
CSIM .632 .628 .627 .650
CSIMCT .615 .557 .578 .627
B4 Best .797 .698 .724 .763
SCOLL .672 .584 .545 .634
SCOLLCT .602 .571 .554 .619
CSIM +SCOLLCT .660 .657 .654 .669
HYBRID .617 .568 .587 .625
Table 7: Experiment 2: Binary classification results
(P=precision, R=recall, F1=F-score, A=accuracy)
Step Method Tagged Ai Untagged
1 SCOLL 21 .810 526
2 CSIM 106 .689 420
3 extSCOLL 0 .000 420
4 extCSIM 61 .607 359
5 CSIMREST 359 .619 0
Table 8: Experiment 2: Classifications for each
step of the HYBRID method (CSREST=the final ap-
plication of CS over the remaining test instances,
Ai=accuracy for classifications made at step i)
ITY + SCOLLCT (CSIM +SCOLLCT ) system per-
formed the best in group B4 for both classification
tasks. In general, the performance of CONSTITUENT
SIMILARITY is marginally better than that of SENSE
COLLOCATION. Also, the utility of co-training is
confirmed by it outperforming both CONSTITUENT
SIMILARITY and SENSE COLLOCATION.
In order to compare the original methods with
the hybrid methods, we observed that the original
methods, SCOLL and K, and their co-training vari-
ants performed consistently better than the hybrid
methods, HYBRID and CSIMCT . We found that the
combination of the methods lowers overall perfor-
mance. We also found that the number of training
instances contributes to improved performance, pre-
dictably in the sense that the methods are supervised,
but encouraging in the sense that the extra training
data is generated automatically. As expected, the
step-wise performance of HYBRID and CSIMCT de-
grades with each iteration, although there were in-
stances where the performance didn?t drop from one
iteration to the next (e.g. iteration 3 = 59.46% vs. it-
eration 4 = 72.23% in Experiment 2). This confirms
574
Iteration ? Tagged Ai Untagged
1 .90 21 .810 526
2 .85 52 .726 474
3 .80 56 .714 418
4 .75 74 .595 344
5 .70 101 .722 243
6 .65 222 .572 21
7 <.65 21 .996 0
Table 9: Experiment 2: Classifications at each step
of the CSIMCT method (?=threshold, Ai=accuracy
for classifications made at iteration i)
CE IA PP OE TT PW CC
relations
Accuracy(%)
KE w/ binary classes & tagged as "true"
Fscorerecall
precision
 0
 20
 40
 60
 80
 100
Figure 4: TPR for each SR for the binary task (pos-
itive instances, CSIM +SCOLLCT method)
our expectation that: (a) the similarity threshold is
strongly correlated with the quality of the resultant
data; and (b) the method is susceptible to noisy train-
ing data.
Our performance comparison over the binary
classification task from the SEMEVAL-2007 task
shows that our 6 systems performed below the best
performing system in the competition, to varying de-
grees. This is partly because the methods were origi-
nally designed for multi-way (positive) classification
and require adjustment for the binary task reformu-
lation, although their performance is competitive.
Finally, comparing the SCOLL and CSIM meth-
ods, we found that the methods interpret SRs with
100% accuracy when the sense collocations are
found in both the test and training data. However,
the CSIM method is more sensitive than the SCOLL
method to variation in the sense collocations, which
leads to better performance. Also, the CSIM method
interprets NCs with high accuracy when the com-
puted similarity is sufficiently high (e.g. with simi-
larity ? 0.9 the accuracy is 89.7%). Another benefit
CE IA PP OE TT PW CCRelations
Accuracy(%)
KE w/ binary classes & tagged as "false"
 0
 20
 40
 60
 80
 100 precision
recallFscore
Figure 5: TNR for each SR for the binary task (neg-
ative instances, CSIM +SCOLLCT method)
of this method is that it interprets NCs without word
sense information. As a result, we conclude that the
CSIM method is more flexible and robust. One pos-
sible weakness of CSIM is its reliance on the simi-
larity measure.
8 Conclusions and Future Work
In this paper, we have benchmarked and hybridised
existing NC interpretation methods over data from
the SEMEVAL-2007 nominal pair interpretation
task. In this, we have established guidelines for the
use of the different methods, and also for the rein-
terpretation of the SEMEVAL-2007 data as a more
conventional multi-way classification task. We con-
firmed that CONSTITUENT SIMILARITY is the best
method due to its insensitivity to varied sense col-
locations. We also confirmed that co-training im-
proves the performance of the methods by expand-
ing the number of training instances.
Looking to the future, there is room for improve-
ment for all the methods through such factors as
threshold tweaking and expanding the training in-
stances further.
References
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pp. 805?810,
Acapulco, Mexico.
Ken Barker and Stan Szpakowicz. 1998. Semi-
automatic recognition of noun modifier relationships.
In Proceedings of the 17th International Conference
575
on Computational Linguistics, pp. 96?102, Montreal,
Canada.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Mem-
ory Based Learner, version 5.1, Reference Guide. ILK
Technical Report 04-02.
Pamela Downing. 1977. On the Creation and Use of En-
glish Compound Nouns. Language, 53(4):810?842.
James Fan and Ken Barker and Bruce W. Porter. 2003.
The knowledge required to interpret noun compounds.
In In Proceedings of the 7th International Joint Con-
ference on Artificial Intelligence, Acapulco, Mexico,
1483?1485.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois.
Roxana Girju. 2007. Improving the Interpretation of
Noun Phrases with Cross-linguistic Information. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pp. 568?575,
Prague, Czech Republic.
Roxana Girju and Preslav Nakov and Vivi Nastase and
Stan Szpakowicz and Peter Turney and Deniz Yuret.
2007. SemEval-2007 Task 04: Classification of Se-
mantic Relations between Nominals. In Proceedings
of the 4th Semantic Evaluation Workshop (SemEval-
2007), Prague, Czech Republic, pp.13?18.
Su Nam Kim and Timothy Baldwin. 2005. Auto-
matic interpretation of Noun Compounds using Word-
Net similarity. In Proceedings of the 2nd International
Joint Conference On Natural Language Processing,
pp. 945?956, JeJu, Korea.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
Semantic Relations in Noun Compounds via Verb Se-
mantics. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics and
21st International Conference on Computational Lin-
guistics (COLING/ACL-2006). pp. 491?498, Sydney,
Australia.
Su Nam Kim and Timothy Baldwin. 2007. Interpreting
Noun Compound Using Bootstrapping and Sense Col-
location. In Proceedings of the Pacific Association for
Computational Linguistics (PACLING), pp. 129?136,
Melbourne, Australia.
Maria Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
Judith Levi. 1979. The syntax and semantics of complex
nominals. In The Syntax and Semantics of Complex
Nominals. New York:Academic Press.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In Proceedings
of the HLT-NAACL 2004 Workshop on Computational
Lexical Semantics, pp. 60?67, Boston, USA.
Preslav Nakov and Marti Hearst. 2006. Using Verbs to
Characterize Noun-Noun Relations. In Proceedings of
the 12th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications (AIMSA),
Bularia.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proc. of the ACL-2007 Workshop on
A Broader Perspective on Multiword Expressions,
Prague, Czech Republic.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
Barbara Rosario and Hearst Marti. 2001. Classify-
ing the Semantic Relations in Noun Compounds via
a Domain-Specific Lexical Hierarchy. In In Proceed-
ings of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP-2001), 82?90.
Karen Sparck Jones. 1983. Compound noun interpre-
tation problems. Computer Speech Processing, Frank
Fallside and William A. Woods, Prentice-Hall, Engle-
wood Cliffs, NJ.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
the 15th Conference on Computational linguistics, pp.
782?788.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Computational
Linguistics, pp. 133?138, Las Cruces, USA.
576
MRD-based Word Sense Disambiguation: Further#2 Extending#1 Lesk
Timothy Baldwin,? Su Nam Kim,? Francis Bond,? Sanae Fujita,?
David Martinez? and Takaaki Tanaka?
? CSSE
University of Melbourne
VIC 3010 Australia
? NICT
3-5 Hikaridai, Seika-cho
Soraku-gun, Kyoto
619-0289 Japan
? NTT CS Labs
2-4 Hikari-dai, Seika-cho
Soraku-gun, Kyoto
619-0237 Japan
Abstract
This paper reconsiders the task of MRD-
based word sense disambiguation, in extend-
ing the basic Lesk algorithm to investigate
the impact onWSD performance of different
tokenisation schemes, scoring mechanisms,
methods of gloss extension and filtering
methods. In experimentation over the Lex-
eed Sensebank and the Japanese Senseval-
2 dictionary task, we demonstrate that char-
acter bigrams with sense-sensitive gloss ex-
tension over hyponyms and hypernyms en-
hances WSD performance.
1 Introduction
The aim of this work is to develop and extend word
sense disambiguation (WSD) techniques to be ap-
plied to all words in a text. The goal of WSD is
to link occurrences of ambiguous words in specific
contexts to their meanings, usually represented by
a machine readable dictionary (MRD) or a similar
lexical repository. For instance, given the following
Japanese input:
(1) ?????
quiet
?
dog
?
ACC
????
want to keep
?(I) want to keep a quiet dog?
we would hope to identify each component word as
occurring with the sense corresponding to the indi-
cated English glosses.
WSD systems can be classified according to the
knowledge sources they use to build their models. A
top-level distinction is made between supervised and
unsupervised systems. The former rely on training
instances that have been hand-tagged, while the lat-
ter rely on other types of knowledge, such as lexical
databases or untagged corpora. The Senseval evalu-
ation tracks have shown that supervised systems per-
form better when sufficient training data is available,
but they do not scale well to all words in context.
This is known as the knowledge acquisition bottle-
neck, and is the main motivation behind research on
unsupervised techniques (Mihalcea and Chklovski,
2003).
In this paper, we aim to exploit an existing lexical
resource to build an all-words Japanese word-sense
disambiguator. The resource in question is the Lex-
eed Sensebank (Tanaka et al, 2006) and consists of
the 28,000 most familiar words of Japanese, each of
which has one or more basic senses. The senses take
the form of a dictionary definition composed from
the closed vocabulary of the 28,000 words contained
in the dictionary, each of which is further manually
sense annotated according to the Lexeed sense in-
ventory. Lexeed also has a semi-automatically con-
structed ontology.
Through the Lexeed sensebank, we investigate a
number of areas of general interest to theWSD com-
munity. First, we test extensions of the Lesk algo-
rithm (Lesk, 1986) over Japanese, focusing specif-
ically on the impact of the overlap metric and seg-
ment representation on WSD performance. Second,
we propose further extensions of the Lesk algorithm
that make use of disambiguated definitions. In this,
we shed light on the relative benefits we can expect
from hand-tagging dictionary definitions, i.e. in in-
troducing ?semi-supervision? to the disambiguation
task. The proposed method is language independent,
and is equally applicable to the Extended WordNet1
for English, for example.
2 Related work
Our work focuses on unsupervised and semi-
supervised methods that target al words and parts
of speech (POS) in context. We use the term
?unsupervised? to refer to systems that do not
use hand-tagged example sets for each word, in
line with the standard usage in the WSD litera-
ture (Agirre and Edmonds, 2006). We blur the su-
pervised/unsupervised boundary somewhat in com-
bining the basic unsupervised methods with hand-
tagged definitions from Lexeed, in order to measure
the improvement we can expect from sense-tagged
data. We qualify our use of hand-tagged definition
1 http://xwn.hlt.utdallas.edu
775
sentences by claiming that this kind of resource is
less costly to produce than sense-annotated open text
because: (1) the effects of discourse are limited, (2)
syntax is relatively simple, (3) there is significant se-
mantic priming relative to the word being defined,
and (4) there is generally explicit meta-tagging of
the domain in technical definitions. In our experi-
ments, we will make clear when hand-tagged sense
information is being used.
Unsupervised methods rely on different knowl-
edge sources to build their models. Primarily
the following types of lexical resources have been
used for WSD: MRDs, lexical ontologies, and un-
tagged corpora (monolingual corpora, second lan-
guage corpora, and parallel corpora). Although
early approaches focused on exploiting a single re-
source (Lesk, 1986), recent trends show the bene-
fits of combining different knowledge sources, such
as hierarchical relations from an ontology and un-
tagged corpora (McCarthy et al, 2004). In this sum-
mary, we will focus on a few representative systems
that make use of different resources, noting that this
is an area of very active research which we cannot
do true justice to within the confines of this paper.
The Lesk method (Lesk, 1986) is an MRD-based
system that relies on counting the overlap between
the words in the target context and the dictionary
definitions of the senses. In spite of its simplicity,
it has been shown to be a hard baseline for unsu-
pervised methods in Senseval, and it is applicable to
all-words with minimal effort. Banerjee and Peder-
sen (2002) extended the Lesk method for WordNet-
based WSD tasks, to include hierarchical data from
the WordNet ontology (Fellbaum, 1998). They ob-
served that the hierarchical relations significantly
enhance the basic model. Both these methods will
be described extensively in Section 3.1, as our ap-
proach is based on them.
Other notable unsupervised and semi-supervised
approaches are those of McCarthy et al (2004), who
combine ontological relations and untagged corpora
to automatically rank word senses in relation to a
corpus, and Leacock et al (1998) who use untagged
data to build sense-tagged data automatically based
on monosemous words. Parallel corpora have also
been used to avoid the need for hand-tagged data,
e.g. by Chan and Ng (2005).
3 Background
As background to our work, we first describe the ba-
sic and extended Lesk algorithms that form the core
of our approach. Then we present the Lexeed lex-
ical resource we have used in our experiments, and
finally we outline aspects of Japanese relevant for
this work.
3.1 Basic and Extended Lesk
The original Lesk algorithm (Lesk, 1986) performs
WSD by calculating the relative word overlap be-
tween the context of usage of a target word, and the
dictionary definition of each of its senses in a given
MRD. The sense with the highest overlap is then se-
lected as the most plausible hypothesis.
An obvious shortcoming of the original Lesk al-
gorithm is that it requires that the exact words used
in the definitions be included in each usage of the
target word. To redress this shortcoming, Banerjee
and Pedersen (2002) extended the basic algorithm
for WordNet-based WSD tasks to include hierarchi-
cal information, i.e. expanding the definitions to in-
clude definitions of hypernyms and hyponyms of the
synset containing a given sense, and assigning the
same weight to the words sourced from the different
definitions.
Both of these methods can be formalised accord-
ing to the following algorithm, which also forms the
basis of our proposed method:
for each word wi in context w = w1w2...wn do
for each sense si,j and definition di,j of wi do
score(si,j) = overlap(w, di,j)
end for
s?i = arg maxj score(si,j)
end for
3.2 The Lexeed Sensebank
All our experimentation is based on the Lexeed
Sensebank (Tanaka et al, 2006). The Lexeed Sense-
bank consists of all Japanese words above a certain
level of familiarity (as defined by Kasahara et al
(2004)), giving rise to 28,000 words in all, with a to-
tal of 46,000 senses which are similarly filtered for
similarity. The sense granularity is relatively coarse
for most words, with the possible exception of light
verbs, making it well suited to open-domain appli-
cations. Definition sentences for these senses were
rewritten to use only the closed vocabulary of the
28,000 familiar words (and some function words).
Additionally, a single example sentence was man-
ually constructed to exemplify each of the 46,000
senses, once again using the closed vocabulary of the
Lexeed dictionary. Both the definition sentences and
example sentences were then manually sense anno-
tated by 5 native speakers of Japanese, from which a
majority sense was extracted.
776
In addition, an ontology was induced from the
Lexeed dictionary, by parsing the first definition sen-
tence for each sense (Nichols et al, 2005). Hy-
pernyms were determined by identifying the highest
scoping real predicate (i.e. the genus). Other rela-
tion types such as synonymy and domain were also
induced based on trigger patterns in the definition
sentences, although these are too few to be useful
in our research. Because each word is sense tagged,
the relations link senses rather than just words.
3.3 Peculiarities of Japanese
The experiments in this paper focus exclusively
on Japanese WSD. Below, we outline aspects of
Japanese which are relevant to the task.
First, Japanese is a non-segmenting language, i.e.
there is no explicit orthographic representation of
word boundaries. The native rendering of (1), e.g., is???????????. Various packages exist to
automatically segment Japanese strings into words,
and the Lexeed data has been pre-segmented using
ChaSen (Matsumoto et al, 2003).
Second, Japanese is made up of 3 basic alpha-
bets: hiragana, katakana (both syllabic in nature)
and kanji (logographic in nature). The relevance of
these first two observations to WSD is that we can
choose to represent the context of a target word by
way of characters or words.
Third, Japanese has relatively free word order,
or strictly speaking, word order within phrases is
largely fixed but the ordering of phrases governed
by a given predicate is relatively free.
4 Proposed Extensions
We propose extensions to the basic Lesk algorithm
in the orthogonal areas of the scoring mechanism,
tokenisation, extended glosses and filtering.
4.1 Scoring Mechanism
In our algorithm, overlap provides the means to
score a given pairing of context w and definition
di,j . In the original Lesk algorithm, overlap was
simply the sum of words in common between the
two, which Banerjee and Pedersen (2002) modified
by squaring the size of each overlapping sub-string.
While squaring is well motivated in terms of prefer-
ring larger substring matches, it makes the algorithm
computationally expensive. We thus adopt a cheaper
scoring mechanism which normalises relative to the
length of w and di,j , but ignores the length of sub-
string matches. Namely, we use the Dice coefficient.
4.2 Tokenisation
Tokenisation is particularly important in Japanese
because it is a non-segmenting language with a lo-
gographic orthography (kanji). As such, we can
chose to either word tokenise via a word splitter
such as ChaSen, or character tokenise. Charac-
ter and word tokenisation have been compared in
the context of Japanese information retrieval (Fujii
and Croft, 1993) and translation retrieval (Baldwin,
2001), and in both cases, characters have been found
to be the superior representation overall.
Orthogonal to the question of whether to tokenise
into words or characters, we adopt an n-gram seg-
ment representation, in the form of simple unigrams
and simple bigrams. In the case of word tokenisa-
tion and simple bigrams, e.g., example (1) would be
represented as {?????? ,?? ,????? }.
4.3 Extended Glosses
The main direction in which Banerjee and Peder-
sen (2002) successfully extended the Lesk algorithm
was in including hierarchically-adjacent glosses (i.e.
hyponyms and hypernyms). We take this a step
further, in using both the Lexeed ontology and the
sense-disambiguated words in the definition sen-
tences.
The basic form of extended glossing is the simple
Lesk method, where we take the simple definitions
for each sense si,j (i.e. without any gloss extension).
Next, we replicate the Banerjee and Pedersen
(2002) method in extending the glosses to include
words from the definitions for the (immediate) hy-
pernyms and/or hyponyms of each sense si,j .
An extension of the Banerjee and Pedersen (2002)
method which makes use of the sense-annotated def-
initions is to include the words in the definition of
each sense-annotated word dk contained in defini-
tion di,j = d1d2...dm of word sense si,j . That is,
rather than traversing the ontology relative to each
word sense candidate si,j for the target word wi,
we represent each word sense via the original def-
inition plus all definitions of word senses contained
in it (weighting each to give the words in the original
definition greater import than those from definitions
of those word senses). We can then optionally adopt
a similar policy to Banerjee and Pedersen (2002) in
expanding each sense-annotated word dk in the orig-
inal definition relative to the ontology, to include the
immediate hypernyms and/or hyponyms.
We further expand the definitions (+extdef) by
adding the full definition for each sense-tagged word
in the original definition. This can be combined
with the Banerjee and Pedersen (2002) method by
777
also expanding each sense-annotated word dk in the
original definition relative to the ontology, to in-
clude the immediate hypernyms (+hyper) and/or hy-
ponyms (+hypo).
4.4 Filtering
Each word sense in the dictionary is marked with a
word class, and the word splitter similarly POS tags
every definition and input to the system. It is nat-
ural to expect that the POS tag of the target word
should match the word class of the word sense, and
this provides a coarse-grained filter for discriminat-
ing homographs with different word classes.
We also experiment with a stop word-based filter
which ignores a closed set of 18 lexicographic mark-
ers commonly found in definitions (e.g. ? [ryaku]
?an abbreviation for ...?), in line with those used by
Nichols et al (2005) in inducing the ontology.
5 Evaluation
We evaluate our various extensions over two
datasets: (1) the example sentences in the Lexeed
sensebank, and (2) the Senseval-2 Japanese dictio-
nary task (Shirai, 2002).
All results below are reported in terms of sim-
ple precision, following the conventions of Senseval
evaluations. For all experiments, precision and re-
call are identical as our systems have full coverage.
For the two datasets, we use two baselines: a ran-
dom baseline and the first-sense baseline. Note that
the first-sense baseline has been shown to be hard
to beat for unsupervised systems (McCarthy et al,
2004), and it is considered supervised when, as in
this case, the first-sense is the most frequent sense
from hand-tagged corpora.
5.1 Lexeed Example Sentences
The goal of these experiments is to tag all the words
that occur in the example sentences in the Lexeed
Sensebank. The first set of experiments over the
Lexeed Sensebank explores three parameters: the
use of characters vs. words, unigrams vs. bigrams,
and original vs. extended definitions. The results of
the experiments and the baselines are presented in
Table 1.
First, characters are in all cases superior to words
as our segment granularity. The introduction of bi-
grams has a uniformly negative impact for both char-
acters and words, due to the effects of data sparse-
ness. This is somewhat surprising for characters,
given that the median word length is 2 characters,
although the difference between character unigrams
and bigrams is slight.
Extended definitions are also shown to be superior
to simple definitions, although the relative increment
in making use of large amounts of sense annotations
is smaller than that of characters vs. words, suggest-
ing that the considerable effort in sense annotating
the definitions is not commensurate with the final
gain for this simple method.
Note that at this stage, our best-performing
method is roughly equivalent to the unsupervised
(random) baseline, but well below the supervised
(first sense) baseline.
Having found that extended definitions improve
results to a small degree, we turn to our next exper-
iment were we investigate whether the introduction
of ontological relations to expand the original def-
initions further enhances our precision. Here, we
persevere with the use of word and characters (all
unigrams), and experiment with the addition of hy-
pernyms and/or hyponyms, with and without the ex-
tended definitions. We also compare our method
directly with that of Banerjee and Pedersen (2002)
over the Lexeed data, and further test the impact
of the sense annotations, in rerunning our experi-
ments with the ontology in a sense-insensitive man-
ner, i.e. by adding in the union of word-level hyper-
nyms and/or hyponyms. The results are described in
Table 2. The results in brackets are reproduced from
earlier tables.
Adding in the ontology makes a significant dif-
ference to our results, in line with the findings of
Banerjee and Pedersen (2002). Hyponyms are better
discriminators than hypernyms (assuming a given
word sense has a hyponym ? the Lexeed ontology
is relatively flat), partly because while a given word
sense will have (at most) one hypernym, it often has
multiple hyponyms (if any at all). Adding in hyper-
nyms or hyponyms, in fact, has a greater impact on
results than simple extended definitions (+extdef),
especially for words. The best overall results are
produced for the (weighted) combination of all on-
tological relations (i.e. extended definitions, hyper-
nyms and hyponyms), achieving a precision level
above both the unsupervised (random) and super-
vised (first-sense) baselines.
In the interests of getting additional insights into
the import of sense annotations in our method, we
ran both the original Banerjee and Pedersen (2002)
method and a sense-insensitive variant of our pro-
posed method over the same data, the results for
which are also included in Table 2. Simple hy-
ponyms (without extended definitions) and word-
based segments returned the best results out of all
the variants tried, at a precision of 0.656. This com-
pares with a precision of 0.683 achieved for the best
778
UNIGRAMS BIGRAMS
ALL WORDS POLYSEMOUS ALL WORDS POLYSEMOUS
Simple Definitions
CHARACTERS 0.523 0.309 0.486 0.262
WORDS 0.469 0.229 0.444 0.201
Extended Definitions
CHARACTERS 0.526 0.313 0.529 0.323
WORDS 0.489 0.258 0.463 0.227
Table 1: Precision over the Lexeed example sentences using simple/extended definitions and word/character
unigrams and bigrams (best-performing method in boldface)
ALL WORDS POLYSEMOUS
UNSUPERVISED BASELINE: 0.527 0.315
SUPERVISED BASELINE: 0.633 0.460
Banerjee and Pedersen (2002) 0.648 0.492
Ontology expansion (sense-sensitive)
simple (0.469) (0.229)
+extdef (0.489) (0.258)
+hypernyms 0.559 0.363
W +hyponyms 0.655 0.503
+def +hyper 0.577 0.386
+def +hypo 0.649 0.490
+def +hyper +hypo 0.683 0.539
simple (0.523) (0.309)
+extdef (0.526) (0.313)
+hypernyms 0.539 0.334
C +hyponyms 0.641 0.481
+def +hyper 0.563 0.365
+def +hypo 0.671 0.522
+def +hyper +hypo 0.671 0.522
Ontology expansion (sense-insensitive)
+hypernyms 0.548 0.348
+hyponyms 0.656 0.503
W +def +hyper 0.551 0.347
+def +hypo 0.649 0.490
+def + hyper +hypo 0.631 0.464
+hypernyms 0.537 0.332
+hyponyms 0.644 0.485
C +def +hyper 0.542 0.335
+def +hypo 0.644 0.484
+def + hyper +hypo 0.628 0.460
Table 2: Precision over the Lexeed exam-
ple sentences using ontology-based gloss extension
(with/without word sense information) and word
(W) and character (C) unigrams (best-performing
method in boldface)
of the sense-sensitive methods, indicating that sense
information enhances WSD performance. This rein-
forces our expectation that richly annotated lexical
resources improve performance. With richer infor-
mation to work with, character based methods uni-
formly give worse results.
While we don?t present the results here due to rea-
sons of space, POS-based filtering had very little im-
pact on results, due to very few POS-differentiated
homographs in Japanese. Stop word filtering leads
ALL
WORDS
POLYSEMOUS
Baselines
Unsupervised (random) 0.310 0.260
Supervised (first-sense) 0.577 0.555
Ontology expansion (sense-sensitive)
W +def +hyper +hypo 0.624 0.605
C +def +hyper +hypo 0.624 0.605
Ontology expansion (sense-insensitive)
W +def +hyper +hypo 0.602 0.581
C +def +hyper +hypo 0.593 0.572
Table 3: Precision over the Senseval-2 data
to a very slight increment in precision across the
board (of the order of 0.001).
5.2 Senseval-2 Japanese Dictionary Task
In our second set of experiments we apply our pro-
posed method to the Senseval-2 Japanese dictionary
task (Shirai, 2002) in order to calibrate our results
against previously published results for Japanese
WSD. Recall that this is a lexical sample task,
and that our evaluation is relative to Lexeed re-
annotations of the same dataset, although the relative
polysemy for the original data and the re-annotated
version are largely the same (Tanaka et al, 2006).
The first sense baselines (i.e. sense skewing) for the
two sets of annotations differ significantly, however,
with a precision of 0.726 reported for the original
task, and 0.577 for the re-annotated Lexeed vari-
ant. System comparison (Senseval-2 systems vs. our
method) will thus be reported in terms of error rate
reduction relative to the respective first sense base-
lines.
In Table 3, we present the results over the
Senseval-2 data for the best-performing systems
from our earlier experiments. As before, we in-
clude results over both words and characters, and
with sense-sensitive and sense-insensitive ontology
expansion.
Our results largely mirror those of Table 2, al-
though here there is very little to separate words
and characters. All methods surpassed both the ran-
dom and first sense baselines, but the relative impact
779
of sense annotations was if anything even less pro-
nounced than for the example sentence task.
Both sense-sensitiveWSDmethods achieve a pre-
cision of 0.624 over all the target words (with one
target word per sentence), an error reduction rate
of 11.1%. This compares favourably with an error
rate reduction of 21.9% for the best of the WSD
systems in the original Senseval-2 task (Kurohashi
and Shirai, 2001), particularly given that our method
is semi-supervised while the Senseval-2 system is a
conventional supervised word sense disambiguator.
6 Conclusion
In our experiments extending the Lesk algorithm
over Japanese data, we have shown that definition
expansion via an ontology produces a significant
performance gain, confirming results by Banerjee
and Pedersen (2002) for English. We also explored
a new expansion of the Lesk method, by measuring
the contribution of sense-tagged definitions to over-
all disambiguation performance. Using sense infor-
mation doubles the error reduction compared to the
supervised baseline, a constant gain that shows the
importance of precise sense information for error re-
duction.
Our WSD system can be applied to all words in
running text, and is able to improve over the first-
sense baseline for two separate WSD tasks, using
only existing Japanese resources. This full-coverage
system opens the way to explore further enhance-
ments, such as the contribution of extra sense-tagged
examples to the expansion, or the combination of
different WSD algorithms.
For future work, we are also studying the in-
tegration of the WSD tool with other applications
that deal with Japanese text, such as a cross-lingual
glossing tool that aids Japanese learners reading text.
Another application we are working on is the inte-
gration of the WSD system with parse selection for
Japanese grammars.
Acknowledgements
This material is supported by the Research Collaboration be-
tween NTT Communication Science Laboratories, Nippon
Telegraph and Telephone Corporation and the University of
Melbourne. We would like to thank members of the NTT Ma-
chine Translation Group and the three anonymous reviewers for
their valuable input on this research.
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Timothy Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th Annual
Meeting of the ACL and 10th Conference of the EACL (ACL-
EACL 2001), pages 18?25, Toulouse, France.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using WordNet. In
Proc. of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-2002),
pages 136?45, Mexico City, Mexico.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of the 20th
National Conference on Artificial Intelligence (AAAI 2005),
pages 1037?42, Pittsburgh, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Hideo Fujii and W. Bruce Croft. 1993. A comparison of index-
ing techniques for Japanese text retrieval. In Proc. of 16th
International ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?93), pages 237?
46, Pittsburgh, USA.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In Proc. of SIG NLC-159, Tokyo, Japan.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese tasks. In IEICE Technical Report NLC 2001-10,
pages 1?8. (in Japanese).
Claudia Leacock, Martin Chodorow, and George A. Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Michael Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 1986 SIGDOC Confer-
ence, pages 24?6, Ontario, Canada.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Japanese Morphological Analysis System
ChaSen Version 2.3.3 Manual. Technical report, NAIST.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
Rada Mihalcea and Timothy Chklovski. 2003. Open Mind
Word Expert: Creating Large Annotated Data Collections
with Web Users? Help. In Proceedings of the EACL
2003 Workshop on Linguistically Annotated Corpora (LINC
2003), pages 53?61, Budapest, Hungary.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI-2005), pages 1111?6, Ed-
inburgh, UK.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 japanese dictionary task. In Proc.
of the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 605?8, Las Palmas,
Spain.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Proc. of the Workshop on Frontiers
in Linguistically Annotated Corpora 2006, pages 62?9, Syd-
ney, Australia.
780
Prepositions in Applications: A Survey and
Introduction to the Special Issue
Timothy Baldwin
University of Melbourne, Australia
Valia Kordoni
DFKI GmbH and Saarland University,
Germany
Aline Villavicencio
Federal University of Rio Grande do Sul,
Brazil, and University of Bath, UK
1. Introduction
Prepositions1?as well as prepositional phrases (PPs) and markers of various sorts?
have a mixed history in computational linguistics (CL), as well as related fields such as
artificial intelligence, information retrieval (IR), and computational psycholinguistics:
On the one hand they have been championed as being vital to precise language un-
derstanding (e.g., in information extraction), and on the other they have been ignored
on the grounds of being syntactically promiscuous and semantically vacuous, and
relegated to the ignominious rank of ?stop word? (e.g., in text classification and IR).
Although NLP in general has benefitted from advances in those areas where prepo-
sitions have received attention, there are still many issues to be addressed. For example,
in machine translation, generating a preposition (or ?case marker? in languages such
as Japanese) incorrectly in the target language can lead to critical semantic divergences
over the source language string. Equivalently in information retrieval and information
extraction, it would seem desirable to be able to predict that book on NLP and book about
NLPmean largely the same thing, but paranoid about drugs and paranoid on drugs suggest
very different things.
Prepositions are often among the most frequent words in a language. For example,
based on the British National Corpus (BNC; Burnard 2000), four out of the top-ten
most-frequent words in English are prepositions (of, to, in, and for). In terms of both
parsing and generation, therefore, accurate models of preposition usage are essential to
avoid repeatedly making errors. Despite their frequency, however, they are notoriously
difficult to master, even for humans (Chodorow, Tetreault, and Han 2007). For example,
Lindstromberg (2001) estimates that less than 10% of upper-level English as a Second
1 Our discussion will focus primarily on prepositions in English, but many of the comments we make
apply equally to adpositions (prepositions and postpositions) and case markers in various languages. For
definitions and general discussion of prepositions in English and other languages, we refer the reader to
Lindstromberg (1998), Huddleston and Pullum (2002), and Che?liz (2002), inter alia.
Submission received: 9 December 2008; revised submission received: 17 January 2009; accepted for
publication: 17 January 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
Language (ESL) students can use and understand prepositions correctly, and Izumi et al
(2003) reported error rates of English preposition usage by Japanese speakers of up
to 10%.
The purpose of this special issue is to showcase recent research on prepositions
across the spectrum of computational linguistics, focusing on computational syntax and
semantics. More importantly, however, we hope to reignite interest in the systematic
treatment of prepositions in applications. To this end, this article is intended to present
a cross-section view of research on prepositions and their use in NLP applications. We
begin by outlining the syntax of prepositions and its relevance to NLP applications,
focusing on PP attachment and prepositions in multiword expressions (Section 2). Next,
we discuss formal and lexical semantic aspects of prepositions, and again their rele-
vance to NLP applications (Section 3), and describe instances of applied research where
prepositions have featured prominently (Section 4). Finally, we outline the contributions
of the papers included in this special issue (Section 5) and conclude with a discussion of
research areas relevant to prepositions which we believe are ripe for further exploration
(Section 6).
2. Syntax
There has been a tendency for prepositions to be largely ignored in the area of syntactic
research as ?an annoying little surface peculiarity? (Jackendoff 1973, page 345). In com-
putational terms, the two most important syntactic considerations with prepositions
are: (1) selection (Fillmore 1968; Bennett 1975; Tseng 2000; Kracht 2003), and (2) valence
(Huddleston and Pullum 2002).
Selection is the property of a preposition being subcategorized/specified by the
governor (usually a verb) as part of its argument structure. An example of a selected
preposition is with in dispense with introductions, where introductions is the object of the
verb dispense, butmust be realized in a prepositional phrase headed bywith (c.f. *dispense
introductions). Conventionally, selected prepositions are specified uniquely (e.g., dispense
with) or as well-defined clusters (e.g., chuckle over/at), and have bleached semantics.
Selected prepositions contrast with unselected prepositions, which do not form part
of the argument structure of a governor and do have semantic import (e.g., live in
Japan). Unsurprisingly, there is not a strict dichotomy between selected and unselected
prepositions (Tseng 2000). For example, in the case of rely on Kim, on is specified in
the argument structure of rely but preserves its directional semantics; similarly, in put it
down , put requires a locative adjunct (c.f. *put it) but is entirely agnostic as to its identity,
and the preposition is semantically transparent.
Preposition selection is important in any NLP application that operates at the
syntax?semantics interface, that is, that overtly translates surface strings onto semantic
representations, or vice versa (e.g., information extraction or machine translation using
some form of interlingua). It forms a core component of subcategorization learning
(Manning 1993; Briscoe and Carroll 1997; Korhonen 2002), and poses considerable chal-
lenges when developing language resources with syntactico-semantic mark-up (Kipper,
Snyder, and Palmer 2004).
Prepositions can occur with either intransitive or transitive valence. Intransitive
prepositions (often referred to as ?particles?) are valence-saturated, and as such do not
take arguments. They occur most commonly as: (a) components of larger multiword
expressions (e.g., verb particle constructions, such as pick it up), (b) copular predicates
(e.g., the doctor is in), or (c) prenominal modifiers (e.g., an off day). Transitive prepositions,
120
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
on the other hand, select for (usually noun phrase) complements to form PPs (e.g., at
home). The bare term ?preposition? traditionally refers to a transitive preposition, but in
this article is used as a catch-all for prepositions of all valences.
Preposition valence has received relatively little direct exposure in the NLP liter-
ature but has been a latent feature of all work on part of speech (POS) tagging and
parsing over the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), as the
Penn POS tagset distinguishes between transitive prepositions (IN), selected intransitive
prepositions (RP), and unselected intransitive prepositions (RB, along with a range of
other adverbials).2 There have been only isolated cases of research where a dedicated
approach has been used to distinguish between these three sub-usages, in the interests
of optimizing overall POS tagging or parsing performance (Shaked 1993; Toutanova and
Manning 2000; Klein and Manning 2003; MacKinlay and Baldwin 2005).
Two large areas of research on the syntactic aspects of prepositions are (a) PP-
attachment and (b) prepositions in multiword expressions, which are discussed in the
following sections. Although this article will focus largely on the syntax of preposi-
tions in English, prepositions in other languages naturally have their own challenges.
Notable examples which have been the target of research in computational linguistics
are adpositions in Estonian (Muischnek, Mu?u?risep, and Puolakainen 2005), adpositions
in Finnish (Lestrade 2006), short and long prepositions in Polish (Tseng 2004), and the
French a` and de (Abeille? et al 2003).
2.1 PP Attachment
PP attachment is the task of finding the governor for a given PP. For example, in the
following sentence:
(1) Kim eats pizza with chopsticks
there is syntactic ambiguity, with the PP with chopsticks being governed by either the
noun pizza (i.e., as part of the NP pizza with chopsticks, as indicated in Example (2)), or
the verb eats (i.e., as a modifier of the verb, as indicated in Example (3)).
(2) S
NP
N
Kim
VP
V
eats
NP
N
pizza
PP
P
with
NP
N
chopsticks
2 It also includes the enigmatic TO tag, which is exclusively used for occurrences of to, over all infinitive,
transitive preposition, and intransitive preposition usages.
121
Computational Linguistics Volume 35, Number 2
(3) S
NP
N
Kim
VP
V
eats
NP
N
pizza
PP
P
with
NP
N
chopsticks
Of these, the latter case of verb attachment (i.e., Example (3)) is, of course, the correct
analysis.
Naturally the number of PP contexts with attachment ambiguity is theoretically
unbounded. One special case of note is a sequence of PPs such as Kim eats pizza
with chopsticks on Wednesdays at Papa Gino?s, where the number of discrete analyses for a
sequence of n PPs is defined by the corresponding Catalan number Cn (Church and Patil
1982). The bulk of PP attachment research, however, has focused exclusively on the case
of a single PP occurring immediately after anNP, which in turn is immediately preceded
by a verb. As such, wewill focus our discussion predominantly on this syntactic context.
To simplify discussion, we will refer to the verb as v, the head noun of the immediately
proceeding NP as n1, the preposition as p, and the head noun of the NP object of
the preposition as n2. Returning to our earlier example, the corresponding 4-tuple is
?eatv, pizzan1 ,withp, chopsticksn2?.
The high degree of interest in PP attachment stems from it being a common phe-
nomenon when parsing languages such as English, and hence a major cause of parser
errors (Lin 2003). As such, it has implications for any task requiring full syntactic
analysis or a notion of constituency, such as prosodic phrasing (van Herwijnen et al
2003). Languages other than English with PP attachment ambiguity which have been
the target of research include Dutch (van Herwijnen et al 2003), French (Gaussier and
Cancedda 2001; Gala and Lafourcade 2005), German (Hartrumpf 1999; Volk 2001, 2003;
Foth and Menzel 2006), Spanish (Calvo, Gelbukh, and Kilgarriff 2005), and Swedish
(Kokkinakis 2000; Aasa 2004; Volk 2006).
PP attachment research has undergone a number of significant paradigm shifts over
the course of the last three decades, and been the target of interest of theoretical syntax,
AI, psycholinguistics, statistical NLP, and statistical parsing.
Early research on PP attachment focused on the development of heuristics intended
to model human processing strategies, based on analysis of the competing parse trees
independent of lexical or discourse context (Frazier 1979; Schu?tze 1995). For example,
Minimal Attachment was the strategy of choosing the attachment site which ?mini-
mizes? the parse tree, as calculated by its node membership; assuming the parse trees
provided for Example (1), this would be unable to disambiguate between Examples (2)
and (3) as they both contain the same number of nodes. Late Attachment, on the other
hand, was the strategy of attaching ?low? in the parse tree, corresponding to Exam-
ple (2). Ford, Bresnan, and Kaplan (1982) proposed an alternative heuristic strategy,
based on the existence of p in a subcategorization frame for v. In later research, Pereira
(1985) described amethod for incorporating Right Association andMinimal Attachment
122
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
into a shift-reduce parser, and Whittemore and Ferrara (1990) developed a rule-based
algorithm to combine various attachment preferences on the basis of empirical evalua-
tion of the predictive power of each.
Syntactic preferences were of course a blunt instrument in dealing with PP attach-
ment, and largely ineffectual in predicting the difference in PP attachment between
Kim eats pizza with chopsticks (verb attachment) and Kim eats pizza with anchovies (noun
attachment), for example. This led to a shift away from syntactic methods in the 1980s
towards AI-inspired techniques which used world knowledge to resolve PP attachment
ambiguity. In the case of Example (1), for example, the knowledge that chopsticks are
an eating implement would suggest a preference for verb attachment, and similarly the
knowledge that they are not a foodstuff would suggest a dispreference for noun attach-
ment. Wilks, Huang, and Fass (1985) attempted to capture this type of world knowledge
using hand-coded ?preferential semantics? of each of v, n1, p, and n2. Dahlgren and
McDowell (1986) simplified this analysis to use only p and n2, still in the form of
hand-coded rules. Hirst (1987) used his model of ?commonsense semantics? to resolve
PP attachment ambiguity, based on verb-guided preferences (i.e., valence properties
of the verb) and plausibility relative to a knowledge base. In this same vein, Jensen
and Binot (1987) used the dictionary definitions of v and n2 as a ?knowledge base? to
resolve PP attachment based on approximate reasoning, for example, by determining
that chopsticks is an instrument which is compatible with eat.
In parallel, in the area of psycholinguistics, Altmann and Steedman (1988) provided
experimental evidence to suggest that PP attachment resolution interacts dynamically
with discourse context, and that ambiguity is resolved on the basis of the interdepen-
dence between structure and the mental representation of that context. This suggested
that computational research on PP attachment resolution needed to take into account
the discourse context, in addition to syntax and world knowledge. Spivey-Knowlton
and Sedivy (1995) later used psycholinguistic experiments to demonstrate that verb-
specific attachment properties and NP definiteness both play important roles in the
human processing of PP attachment. Importantly, this workmotivated the need for verb
classes to resolve PP attachment ambiguity.
The next significant shift in NLP research on PP attachment was brought about by
Hindle and Rooth (1993), who were the harbingers of statistical NLP and large-scale
empirical evaluation. Hindle and Rooth challenged the prevailing view at the time that
lexical semantics and/or discourse modeling were needed to resolve PP attachment, in
proposing a distributional approach, based simply on estimation of the probability of
p attaching high or low given v and n1 (ignoring n2). The method uses unambiguous
cases of PP attachment (e.g., cases of n1 being a pronoun [high attachment], or the PP
post-modifying n1 in subject position [low attachment]) to derive smoothed estimates
of Prhigh(p|v), Prhigh(NULL|n) (i.e., the probability of n not being post-modified by
a PP), and Prlow(p|n), which then form the basis of Prhigh(p|v, n) and Prlow(p|v, n),
respectively. The proposedmethodwas significant in demonstrating the effectiveness of
simple co-occurrence probabilities, without explicit semantics or discourse processing,
and also in its ability to operate without explicitly annotated training data.
Resnik and Hearst (1993) observed that PP attachment preferences are also condi-
tioned on the semantics of the noun object of the preposition in the PP, as can be seen
in our earlier example of Kim eats pizza with chopsticks/anchovies where chopsticks leads
to verb attachment and anchovies to noun attachment. Although they were unable to
come up with a model which was empirically superior to existing methods which did
not represent the semantics of the noun object, this paved the way for a new wave of
research using the full 4-tuple of ?v, n1, p, n2?.
123
Computational Linguistics Volume 35, Number 2
The first to successfully apply the full 4-tuple were Ratnaparkhi, Reynar, and
Roukos (1994), who in the process established a benchmark PP attachment data set
upon which most of the subsequent research has been based. The data set, colloquially
known as the ?RRR? data set, was automatically extracted from the Wall Street Journal
section of the Penn Treebank, and is made up of 20,801 training and 3,097 test 4-tuples
of type ?v, n1, p, n2?, each of which is annotated with a binary label for verb or noun
attachment.3 Also of significance was the fact that Ratnaparkhi, Reynar, and Roukos
were able to come up with a ?class? representation for words, based on distributional
similarity, that outperformed a simple word-based model.
The general framework established by Ratnaparkhi, Reynar, and Roukos (1994),
and the RRR data set, defined the task of PP attachment for over a decade. It has been
tackled using a variety of smoothing methods andmachine learning algorithms, includ-
ing backed-off estimation (Collins and Brooks 1995), instance-based learning (Zavrel,
Daelemans, and Veenstra 1997; Zhao and Lin 2004), log-linear models (Franz 1996),
maximum entropy learning (Ratnaparkhi, Reynar, and Roukos 1994), decision trees
(Merlo, Crocker, and Berthouzoz 1997), neural networks (Sopena, Lloberas, andMoliner
1998; Alegre, Sopena, and Lloberas 1999), and boosting (Abney, Schapire, and Singer
1999). In addition to the four lexical and class-based features provided by the 4-tuple
?v, n1, p, n2?, researchers have used noun definiteness, distributional similarity, noun
number, subcategorization categories, word proximity in corpus data, and PP semantic
class (Stetina and Nagao 1997; Yeh and Vilain 1998; Pantel and Lin 2000; Volk 2002). The
empirical benchmark for the data set was achieved by Stetina and Nagao (1997), who
used WordNet (Fellbaum 1998) to explicitly model verb and noun semantics.
In a novel approach to the task, Schwartz, Aikawa, and Quirk (2003) observed that
Japanese translations of English verb and noun attachment involve distinct construc-
tions. This allowed them to automatically identify instances of PP attachment ambiguity
in a parallel English?Japanese corpus, complete with attachment information. They
used this data to disambiguate PP attachment in English inputs, and demonstrated that,
the quality of English?Japanese machine translation improved significantly as a result.
More recently, Atterer and Schu?tze (2007) challenged the real-world utility of meth-
ods based on the RRR data set, on the grounds that it is based on the availability of a
gold-standard parse tree for a given input. They proposed that, instead, PP attachment
be evaluated as a means of post-processing over the raw output of an actual parser,
and produced results to indicate: (a) that a state-of-the-art parser (Bikel 2004) does
remarkably well at PP attachment without a dedicated PP attachment module; but
also (b) that post-processing based on a range of methods developed over the RRR
data set (Collins and Brooks 1995; Toutanova, Manning, and Ng 2004; Olteanu and
Moldovan 2005) generally improves parser accuracy. In addition, they developed a
variant of the RRR data set (RRR-sent) which contains full sentential contexts of possible
PP attachment ambiguity. Others who have successfully built PP re-attachment models
for specific parsers are Olteanu (2004) and Foth and Menzel (2006). Agirre, Baldwin,
and Martinez (2008) used the evaluation methodology of Atterer and Schu?tze (2007) to
confirm the finding from the original RRR data set that lexical semantics (in various
guises) can enhance PP attachment accuracy relative to a baseline parser. As part of this
effort, they developed a standardized data set for exploration of the interaction between
lexical semantics and parsing/PP attachment accuracy.
3 Because it was automatically extracted, the RRR data set is notoriously noisy. For instance, Pantel and
Lin (2000) observed that 133 tuples contain the as either n1 or n2.
124
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
One significant variation on the classic binary PP attachment task which attempts
to generate a richer semantic characterisation of the PP is the work of Merlo (2003) and
Merlo and Esteve Ferrer (2006), who included classification of the PP as an argument
or adjunct, making for a four-way classification task. In this context, they found that
PP attachment resolution for argument PPs is considerably easier than is the case for
adjunct PPs.
Returning to our observation that PP attachment can occur in multiple syntactic
configurations, Merlo, Crocker, and Berthouzoz (1997) applied backed-off estimation
to the problem of multiple PP attachment, in the form of 14 discrete syntactic config-
urations. Unsurprisingly, they found the task considerably harder than the basic V NP
PP case, due to increased ambiguity and data sparseness. Mitchell (2004) similarly per-
formed an extensive analysis of the Penn Treebank to investigate the different contexts
PP attachment ambiguities occur in, and the relative ability of different PP attachment
methods to disambiguate each.
There have also been domain-specific methods proposed for PP attachment, for
example, in the area of biomedicine (Hahn, Romacker, and Schulz 2002; Pustejovsky
et al 2002; Leroy, Chen, and Martinez 2003; Schuman and Bergler 2006).
2.2 The Syntax of Prepositional Multiword Expressions
Prepositions are also often found as part of multiword expressions (MWEs), such as
verb-particle constructions (break down), prepositional verbs (rely on), determinerless
PPs (in hospital), complex prepositions (by means of ) and compound nominals (affairs
of state). MWEs are lexical items which are composed of more than one word and are
lexically, syntactically, semantically, pragmatically, and/or statistically idiosyncratic in
some way (Sag et al 2002). In this section, we present a brief overview of the syntax
of the key prepositional MWEs in English, and discuss a cross-section of some of the
recent research done in the area.
Prepositional MWEs span the full spectrum of morphosyntactic variation. Some
complex prepositions undergo no inflection, internal modification, or word order vari-
ation (e.g., in addition to) and are best analyzed as ?words with spaces? (Sag et al
2002). Others optionally allow internal modification (e.g., with [due/particular/special/...]
regard to) or determiner insertion (e.g., on [the] top of ) and are considered to be semi-
fixed expressions (Villada Moiro?n 2005). Compound nominals are similarly semi-fixed
expressions, in that they have rigid constraints on word order and lexical composition,
but allow morphological inflection (e.g., part(s) of speech).
The three prepositional MWE types that cause the greatest syntactic problems in
English, in terms of their relative frequency and tendency for syntactic variation, are:
1. verb-particle constructions (VPCs), where the verb selects for an
intransitive preposition (e.g., chicken out or hand in: Dehe? et al [2002]);
2. prepositional verbs (PVs), where the verb selects for a transitive
preposition (e.g., rely on or refer to: Huddleston and Pullum [2002]);
3. determinerless PPs (PP?Ds), where a PP is made up of a preposition and
singular noun without a determiner (e.g., at school, off screen: Baldwin et al
[2006]).
All three MWE types undergo limited syntactic variation (Sag et al 2002). For exam-
ple, transitive verb particle constructions generally undergo the particle alternation,
125
Computational Linguistics Volume 35, Number 2
whereby the particle may occur either adjacent to the verb (e.g., tear up the letter), or
be separated from the verb by the NP complement (e.g., tear the letter up). Some VPCs
readily occur with both orders (like tear up), while others have a strong preference for
a particular order (e.g., take off?under the interpretation of having the day off?tends
to occur in the particle-final configuration in usages such as take Friday off vs. ?take off
Friday).4 In addition, many VPCs undergo limited internal modification by adverbials,
where the adverb pre-modifies the particle (e.g., come straight over).
The syntactic variability of prepositional verbs is more subtle. PVs occur in two
basic forms: (1) fixed preposition PVs (e.g., come across), where the verb and selected
preposition must be strictly adjacent; and (2) mobile preposition PVs (e.g., refer to),
where the selected preposition is adjacent to the verb in the canonical word order, but
undergoes limited syntactic alternation. For example, mobile preposition PVs allow
limited coordination of PP objects (e.g., refer to the book and to the DVD), and the NP
object of the selected preposition can be passivized (e.g., the book they referred to).
Even subtler are the syntactic effects observed for determinerless PPs. The singular
noun in the PP?D is often strictly countable (e.g., off screen, on break), resulting in
syntactic markedness as, without a determiner, the noun does not constitute a saturated
NP. This in turn dictates the need for a dedicated analysis in a linguistically motivated
grammar in order to be able to avoid parse failures (Baldwin et al 2004; van der Beek
2005). Additionally, there is considerable variation in the internal modifiability of deter-
minerless PPs, with some not permitting any internal modification (e.g., of course) and
others allowing optional internal modification (e.g., at considerable length). There are also,
however, cases of obligatory internal modification (e.g., at considerable/great expense vs.
*at expense) and highly restricted internal modification (e.g., at long last vs. *at great/short
last). Balancing up these different possibilities in terms of over- and undergeneration in
a grammar is far from trivial (Baldwin et al 2006).
Naturally there are other prepositional MWE types in languages other than English
with their own syntactic complexities. Notable examples to have received attention in
the computational linguistics literature are Dutch ?collocational prepositional phrases?
(VilladaMoiro?n 2005), German complex prepositions (Trawin?ski 2003; Trawinski, Sailer,
and Soehn 2006), German particle verbs (Schulte im Walde 2004; Rehbein and van
Genabith 2006), and Polish preposition?pronoun contractions (Trawin?ski 2005, 2006).
2.2.1 Prepositional MWEs in NLP. The syntactic variation of prepositional MWEs leads
to difficulties for NLP applications. To start with, there is the problem of identifying
their token occurrences, for example, for semantic indexing purposes. As with simplex
words, a given MWE may appear with different subcategorization frames (e.g., give up
vs. give up [something]), but added to that, the order of the elements may be flexible
or internally modified (see previous discussion), and some elements may be optional
(e.g., out in make a big thing (out) of [something]). Additionally, they conspire with PP
attachment ambiguity to compound structural ambiguity. For example, hand the paper in
today is ambiguous between a V NP PP analysis ([V hand] [NP the paper] [PP in today]), a
V NP analysis ([V hand] [NP the paper in today]), and a transitive VPC analysis ([V hand]
[NP the paper] [P in] [NP today]); of these, the final analysis is, of course, correct.
4 The question of whether or not a particle can be separated from the verb depends on factors such as the
degree of bonding of the particle with the verb, the size of the NP, and the type of the NP, as discussed by
Wasow (2002).
126
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Related to this is the issue of resource development, especially lexicon development
for linguistically motivated parsers. First, a representation must be arrived at which is
sufficiently expressive to encode the syntactic subtleties of eachMWE instance (Sag et al
2002; Calzolari et al 2002; Copestake et al 2002; Odijk 2004). Second, the lexiconmust be
populated in order to ensure adequate parser coverage over prepositional MWEs. Due
to the high productivity and domain specificity of prepositional MWEs, a number of
lexical acquisition approaches have been developed, usually customized to a particular
prepositional MWE type in a specific language.
For VPCs in English, for instance, Baldwin and Villavicencio (2002) focused on
their automatic extraction from raw text corpora, on the basis of a POS tagger, chunker,
and chunk grammar, and finally the combined output of the three along with various
linguistic and frequency features. Baldwin (2005a) expanded on this work to extract
VPCs complete with valence information, to produce a fully specified lexical item. In
both cases, it was found that 100% recall was extremely hard to achieve due to the
power-law distribution of VPC token frequencies in corpus data. That is, around half
of the VPC lexical items found in pre-existing lexical resources occurred in the BNC at
most 2 times.
Villavicencio (2005, 2006) took a different approach, in observing that VPCs occur
productively in semantically coherent clusters, based on (near-)synonymy of the head
verb (e.g., clear/clean/drain up and break/rip/cut/tear up). As a result, she expanded a
seed set of VPCs based on class-based verb semantic information, and used Web-based
statistics to filter false positives out of the resultant VPC candidate set.
Li et al (2003) looked at the task of VPC identification rather than extraction,
namely, identifying each individual VPC token instance in corpus data, based on a
set of hand-crafted regular expressions. Although they report very high precision and
recall for their method, it has the obvious disadvantage that the regular expressions
must be manually encoded, and it is not clear that the proposed method is superior
to an off-the-shelf parser. Kim and Baldwin (2006, in press) attempted to automate
the process of identification by post-processing the output of the RASP parser, and
demonstrated (a) that the RASP parser (Briscoe, Carroll, and Watson 2006) is highly
effective at VPC identification, and (b) that the incorporation of lexicalized models of
selectional preferences can lead to modest improvements in parser accuracy.
In terms of English PV extraction, Baldwin (2005b) proposed a method based on a
combination of statistical measures and linguistic diagnostics, and demonstrated that
the combination of statistics with linguistic diagnostics achieved the best extraction
performance.
Research on prepositional MWEs in languages other than English includes Krenn
and Evert (2001) and Evert and Krenn (2005) on the extraction of German PP?verb
collocations (which are similar to verbal idioms/verb?noun combinations in English
[Fazly, Cook, and Stevenson 2009]) based on a range of lexical association measures.
Pecina (2008) further extended this work using a much broader set of lexical association
measures and classifier combination. Looking at German, Do?mges et al (2007) analyzed
the productivity of PP?Ds headed by unter, and used their results to motivate a syntactic
analysis of the phenomenon. For Dutch, van der Beek (2005) worked on the extraction
of PP?Ds from the output of a parser, once again using a range of statistical measures.
Sharoff (2004) described a semi-automatic approach to classifying prepositional MWEs
in Russian, based on statistical measures and manual filtering using knowledge about
the structure of Russian prepositional phrases.
A recent development which we expect will further catalyze research on preposi-
tional (and general) MWE extraction was the release of a number of standardized data
127
Computational Linguistics Volume 35, Number 2
sets for MWE extraction evaluation as part of an LREC 2008 Workshop. This includes
data sets for English VPCs (Baldwin 2008) and German PP?verbs (Krenn 2008).
3. Semantics
There are three diametrically opposed views to the semantics of prepositions: (1) prepo-
sitions are semantically vacuous and unworthy of semantic representation (a view
commonly subscribed to in the information retrieval community: Baeza-Yates and
Ribeiro-Neto [1999], Manning, Raghavan, and Schu?tze [2008]); (2) preposition semantics
is a function of the words that select them and they in turn select, such that it is impos-
sible to devise a standalone semantic characterization of preposition semantics (Tseng
2000; Old 2003); and (3) prepositional semantics is complex but can be captured in a
standalone resource (Kipper, Dang, and Palmer 2000; Saint-Dizier and Vazquez 2001;
Litkowski and Hargraves 2005). Kipper, Snyder, and Palmer (2004, page 23) elegantly
summarized this third position as ?it is precisely because the preposition?semantics
relationship is so complex that properly accounting for it will lead to a more robust
natural language resource.? Turning the clock back 16 years, Zelinski-Wibbelt (1993,
page 1) observed that ?we are now witnessing a veritable plethora of investigations
into the semantics of preposition semantics? and speculated that ?the time has come to
see how natural language processing (NLP) can benefit from the insights of theoretical
linguistics.? Although these prognostications were perhaps overly optimistic at the
time, they are now being progressively fulfilled.
In discussing preposition semantics, there is a basic distinction between composi-
tional (or regular/productive) and non-compositional (or irregular/collocational) se-
mantics. Preposition usages with compositional semantics transparently preserve the
standalone semantics of the preposition (e.g., They met on Friday), whereas those with
non-compositional semantics involve some level of semantic specialization or diver-
gence from the standalone semantics (e.g., They got on famously). In terms of formal
semantics, the complement vs. adjunct distinction5 is also relevant for determining the
logical form for a given input.
Subsequently, we review research on the formal and lexical semantics of preposi-
tions, and the semantics of prepositional MWEs.
3.1 Formal Semantic Approaches to Prepositions
Research on the formal semantics of prepositions has focused predominantly on devis-
ing representations for temporal, spatial, and locative usages, the three most productive
and coherent classes of prepositions.
Prepositions can be used in order to convey temporal information relevant to the
duration of a proposition. Normally, three types of information are identified: (1) the
duration of the preposition, (2) its duration relative to the time of reference of the dis-
course, and (3) its absolute location on the time axis. Some prepositions convey all three
types of information, and others convey only one. Within each information type, the
5 In talking about the complement/adjunct distinction, we put aside the well-known issue of borderline
cases (Rauh 1993), and ignore language-specific phenomena such as Funktionsverbgefu?ge, where
German PPs can also occur as invariant syntagmas in light verb constructions (e.g., in Beschlag nehmen ?to
occupy?) and the complement?adjunct distinction does not apply.
128
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
semantic input of temporal prepositions can vary considerably: They can distinguish
between existential and universal quantification over time, indicate whether or not the
extremes of the period are to be included, and mark the motion through time.
Themain challenge for formal approaches to the semantics of temporal prepositions
is coming upwith a representation which can adequately encode the different semantics
of temporal prepositions (Bennett 1975; Miller and Johnson-Laird 1976; Ro?hrer 1977;
Kamp 1981; Allen 1984; Richards et al 1989; Bre?e and Smit 1986; Durrell and Bre?e 1993).
Another concern has been the ability to support compositional semantic interpretation
of temporal PPs, with emphasis on their quantificational role. For English, Pratt and
Francez (1997), for instance, proposed generalized temporal quantifiers to represent
temporal NPs, temporal PPs, and sentences.
With respect to spatial semantics, the focus of research has been on devising formal
semantic representations that capture both the functional relationships between the
objects, and human interaction with those objects. Along these lines, Kelleher and
Costello (2009) propose computational models of spatial preposition semantics for use
in visually situated dialogue systems.
Finally, with locative prepositions, formal semantic approaches have proposed that
they should decompose into three semantic elements: a locative relation, a reference
entity, and a place value. Each locative expression (e.g., a PP orNP) is generally assumed
to have a unique locative relation and place value. Thus, combinations of pure locative
relationmarkers and prepositions which are specified for place values are ruled out, and
no embedded locative relations are allowed in the semantic structure of a single locative
expression. Some locative prepositions are underspecified for a place value, however,
and thus are able to co-occur with a second preposition that specifies a place value
(e.g., out from under the table). The specific locative relation denoted by a preposition
is generally considered not to be specified in the lexicon, but instead to vary with
eventuality types.
J?rgensen and L?nning (2009) used the Minimal Recursion Semantics (MRS)
framework (Copestake et al 2005) as the basis for a language-independent represen-
tation of semantics of locative prepositions. They applied the proposed representa-
tion to English and Norwegian locative prepositions, and demonstrated its utility for
Norwegian?English machine translation. Hellan and Beermann (2005) similarly used
the MRS framework to capture the locative and spatial semantics of Norwegian prepo-
sitions. Ramsay (2005) proposed a unified theory of preposition semantics, where the
semantics of temporal usages of prepositions are predicted naturally from abstract
relational definitions. Arsenijevic? (2005) developed a formal semantic analysis of prepo-
sitions in terms of event structure and applied it to a natural language generation
task.
Denis, Kuhn, and Wechsler (2003) analyzed the syntactico-semantics of V-PP goal
motion complexes in English (e.g., Kim ran to the library), and developed an analysis
based on the conclusion that the preposition is often semantically rather than syn-
tactically determined. This lends support to arguments for a systematic account of
preposition semantics.
Kordoni (2003b, 2003a, 2006) focused instead on the role of prepositions in diathesis
alternations, and proposed an MRS analysis of indirect prepositional arguments based
on English, German, and Modern Greek data. Specifically, she showed that a robust
formal semantic framework like MRS provides an appropriate theoretical basis for
a linguistically motivated account of indirect prepositional arguments, and also the
necessary formal generalizations for the analysis of such arguments in a multilingual
context (as a result of MRS structures being easily comparable across languages).
129
Computational Linguistics Volume 35, Number 2
3.2 Lexical Semantic Resources for Prepositions
A number of lexical semantic resources have been developed specifically for prepo-
sitions, four of which are outlined here: (1) the English LCS Lexicon, (2) the Preposition
Project, (3) PrepNet, and (4) VerbNet.
The English LCS Lexicon (Dorr 1993, 1997) uses the formalism of Lexical Conceptual
Structure (based on work by Jackendoff [1983, 1990]6) to encode lexical knowledge
using a typed directed graph of semantic primitives and fields. In addition to a large
lexicon of verbs, it includes 165 English prepositions classified into 122 intransitive and
375 transitive senses. For example, the LCS representation for the directional sense of
up (as in up the stairs) is:
(4) (toward Loc (nil 2) (UP Loc (nil 2) (? Thing 6)))
where the numbers indicate the logical arguments of the predicates. This representation
indicates that the logical subject of the PP (indexed by ?2?; e.g., the piano in move the
piano up the stairs) is relocated up in the direction of the logical argument (indexed by
?6?; e.g., the stairs in our example), which is in turn a concrete thing. The LCS Lexicon
was developed from a theoretical point of view and isn?t directly tied to corpus usage.
The Preposition Project (Litkowski 2002; Litkowski and Hargraves 2005, 2006) is
an attempt to develop a comprehensive semantic database for English prepositions,
intended for NLP applications. The project took the New Oxford Dictionary of English
(Pearsall 1998) as its source of preposition sense definitions, which it then fine-tuned
based on cross-comparison with both functionally tagged prepositions in FrameNet
(Baker, Fillmore, and Lowe 1998) and the account of preposition semantics in a de-
scriptive grammar of English (Quirk et al 1985); it also draws partially on Dorr?s
LCS definitions of prepositions. Importantly, the Preposition Project is building up a
significant number of tagged preposition instances through analysis of the preposition
data in FrameNet, which it then uses to characterize the noun selectional preferences of
each preposition sense and also the attachment properties to verbs of different types. At
the time of writing, 673 preposition senses for 334 prepositions (mostly phrasal prepo-
sitions) have been annotated. In tandemwith developing type-level sense definitions of
prepositions, the Preposition Project has sense annotated over 27,000 occurrences of the
56 most common prepositions, based on functionally tagged prepositions in FrameNet.
To date, the primary application of the Preposition Project has been in a SemEval 2007
task on the word sense disambiguation of prepositions (see Section 3.3).
PrepNet (Saint-Dizier and Vazquez 2001; Cannesson and Saint-Dizier 2002; Saint-
Dizier 2005, 2008) is an attempt to develop a compositional account of preposition
semantics which interfaces with the semantics of the predicate (e.g., verb or predicative
noun). Similarly to the English LCS Lexicon, it uses LCS as the descriptive language,
in conjunction with typed ?-calculus and underspecified representations. Noteworthy
elements of PrepNet are that it attempts to capture selectional constraints, metaphor-
ical sense extension, and complex arguments. PrepNet was originally developed over
French prepositions, but has since been applied to the analysis of instrumentals across
a range of languages (Saint-Dizier 2006b).
VerbNet (Kipper, Dang, and Palmer 2000; Kipper Schuler 2005) contains a shallow
hierarchy of 50 spatial prepositions, classified into five categories. The hierarchy is
6 See Levin and Rappaport-Hovav (in press) for a recent review of this style of semantics.
130
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
derived from pioneering work by Spa?rck Jones and Boguraev (1987), which is in turn
derived from Wood (1979). The preposition sense inventory was used as the basis for
extending VerbNet and led to significant redevelopment of the verb class set (Kipper,
Snyder, and Palmer 2004), in a poignant illustration of how preposition semantics
impinges on verb semantics.
In other work, Sablayrolles (1995) classified 199 simple and complex spatial prepo-
sitions into 16 classes. Lersundi and Agirre (2003) applied a similar methodology to
Dorr and Habash (2002) in developing a multilingual sense inventory for Basque post-
positions and English and Spanish prepositions. Fort and Guillaume (2007) developed
a syntactico-semantic lexicon of French prepositions, partly based on PrepNet; their
particular interest was in enhancing parsing performance. Old (2003) analyzed Roget?s
Thesaurus and arrived at the conclusion that it was not a good source of standalone
preposition semantics. Beavers (2003) analyzed the aspectual and path properties of
goal-marking postpositions in Japanese, and proposed an analysis based on predicate
and event restrictions. Boonthum, Toida, and Levinstein (2005, 2006) defined a general-
purpose sense inventory of seven prepositions (but purportedly applicable to all prepo-
sitions), taking the LCS lexicon as a starting point and expanding the sense inventory
by consulting Quirk et al (1985) and Barker (1996). Finally, as part of a more general
attempt to capture lexical semantics in the formalism of Multilayered Extended Seman-
tic Networks (MultiNet), Helbig (2006) developed a semantic treatment of prepositions,
focusing primarily on German.
There has historically been a strong interest in preposition semantics in the field of
cognitive linguistics (Talmy 1988), in the form of ?schemas.? Schemas are an attempt to
visualize the relation between the object of the preposition (the Trajector, or TR) and its
cognitive context (the Landmark, or LM). They provide a language-independent repre-
sentation, and have been used to analyze crosslinguistic correspondences in preposition
semantics. For example, Tyler and Evans (2003)used schemas to capture the semantics of
English prepositions, arguing that all meanings are grounded in human spatio-physical
experience. Schemas have also been used as the basis of crosslinguistic analysis of
preposition semantics, for example, by Brala (2000) to describe the senses of on and in
in English and Croatian, Knas? (2006) to motivate a sense inventory for at in English and
Polish, and Cosme and Gilquin (2008) to contrast with and avec in English and French.
Trujillo (1995) also developed a language-independent classification of spatial
prepositions for machine translation purposes, in a lexicalist framework.
3.3 Automatic Classification of Preposition Sense
Only a modest amount of research has been carried out on the sense disambiguation
of prepositions, largely because until recently, there haven?t been lexical semantic re-
sources and sense-tagged corpora for prepositions that could be used for this purpose.
Classification of preposition sense has been motivated as a standalone task in appli-
cations such as machine translation, and also as a means of improving the general
performance of semantic tasks such as semantic role labeling.
O?Hara and Wiebe (2003) were the first to perform a standalone preposition word
sense disambiguation (WSD) task, based on the semantic roles in the Penn Treebank.
They collapsed the semantic roles into seven basic semantic classes, and built a decision
tree classifier based on a set of contextual features similar to those used inWSD systems.
O?Hara andWiebe (2009) is an updated version of this original research, using a broader
range of resources. Ye and Baldwin (2006) also built on the earlier research, in attempt-
ing to enhance the accuracy of semantic role labelingwith dedicated PP disambiguation.
131
Computational Linguistics Volume 35, Number 2
They demonstrated the potential for accurate preposition labeling to contribute to large-
scale improvements in overall semantic role labeling performance.
As mentioned in Section 3.2, Litkowski and Hargraves (2007) ran a task on the
WSD of prepositions at SemEval 2007, as a spinoff of the Preposition Project. The task
focused on 34 prepositions, with a combined total of 332 senses. Similarly to a lexical
sample WSD task, participants were required to disambiguate token instances of each
preposition relative to the provided discrete sense inventory. Three teams participated
in the task (Popescu, Tonelli, and Pianta 2007; Ye and Baldwin 2007; Yuret 2007), with all
systems outperforming two baselines over both fine- and coarse-grained sense invento-
ries, through various combinations of lexical, syntactic, and semantic features. The best-
performing system achieved F-scores of 0.818 and 0.861 over fine- and coarse-grained
senses, respectively (Ye and Baldwin 2007).
In other papers dedicated to prepositional WSD, Boonthum, Toida, and Levinstein
(2005, 2006) proposed a semantic collocation-based approach to preposition interpre-
tation, and demonstrated the import of the method in a paraphrase recognition task.
Alam (2003, 2004) used decision trees to disambiguate 12 senses of over, distinguishing
between senses which are determined by their governor and those which are deter-
mined by their NP complement.
Baldwin (2006) explored the interaction between preposition valence and the dis-
tributional hypothesis, based on latent semantic analysis (LSA; Deerwester et al 1990).
He derived gold-standard preposition-to-preposition similarities using each of Dorr?s
LCS lexicon (based on the method of Resnik and Diab [2000]) and Roget?s Thesaurus,
and compared them to the similarities predicted by LSA, in each case using either
valence specification (considering intransitive and transitive prepositions separately) or
valence underspecification (considering both preposition valences together). His results
indicated higher correlation when valence specification is used, suggesting not only
that there is a significant difference in semantics between transitive and intransitive
usages of a given preposition, but that it is sufficiently marked in the context of use that
distributional methods are able to pick up on it.
Cook and Stevenson (2006) developed a four-way classification of the semantics
of up in VPCs based on cognitive grammar, and classified token instances based on a
combination of linguistic and word co-occurrence features.
In a machine translation context, Trujillo (1995) used selectional preferences and
unification to perform target language disambiguation over his classification of spatial
prepositions.
Srihari, Niu, and Li (2000) used manual rules to disambiguate prepositions in
named entities.
3.4 The Semantics of Prepositional MWEs
Prepositional MWEs?focusing primarily on the English MWE types of VPCs, PVs, PP?
Ds, and compound nominals?populate the spectrum from fully compositional to fully
non-compositional (Dixon 1982; McCarthy, Keller, and Carroll 2003). For instance, put
up (as in put the picture up) is fully compositional, whereas make out (as in Kim and Sandy
made out) is fully non-compositional. Compositionality can be viewed relative to each
component word (Bannard 2005) or holistically for the MWE as a single unit (McCarthy,
Keller, and Carroll 2003). With play out (as in see how the match plays out), for example,
we might claim that play is non-compositional but out is (semi-)compositional, and that
as a whole the VPC is non-compositional. The question of compositionality is confused
132
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
somewhat by productive constructions such as the resultative up (e.g., eat/beat/finish/...
up) and the manner by (e.g., by train/car/broomstick/...), which have specialized semantics
relative to their simplex usages but occur relatively freely with this semantics within a
given construction (VPC and PP?D, respectively, in our examples).
There have been a number of attempts to model the compositionality of VPCs. Ban-
nard (2005, 2006) considered VPC compositionality at the component word level, and
proposed a distributional approach that assumes there is a positive correlation between
compositionality and the distributional similarity of each component to simplex usages
of that same word. McCarthy, Keller, and Carroll (2003) opted for a holistic notion
of compositionality and used a range of approaches based on the distributional ?the-
saurus? of Lin (1998) to model compositionality, for example, in calculating the overlap
in the top-N similar words for a given VPC and its head verb. They also examined
the use of statistical tests such as mutual information in modeling compositionality,
and found the similarity-based methods to correlate more highly with the human
judgments. Baldwin et al (2003) used LSA to analyze the compositionality of VPCs
(and compound nouns), and once again demonstrated that there is a positive correlation
between compositionality and the distributional similarity between a given VPC and
its head word. Kim and Baldwin (2007) predicted the compositionality of VPCs based
on a combination of the McCarthy, Keller, and Carroll (2003) and Bannard (2006) data
sets, and analysis of verb?particle co-occurrence patterns. Taking a different approach to
the task, Patrick and Fletcher (2005) classified token instances of verb?preposition pairs
according to the three classes of decomposable (syntactic dependence between the P and
V, with compositional semantics), non-decomposable (syntactic dependence between
the P and V, with idiomatic semantics), and independent (no syntactic dependence
between the P and V).
Levi (1978) pioneered the use of prepositions as a means of interpreting compound
nouns, through the notion of compatibility with paraphrases incorporating preposi-
tions. For example, baby chair can be paraphrased as chair for (a) baby, indicating com-
patibility with the FOR class. In her original research, Levi used four prepositions, in
combination with a set of verbs (for relative clause paraphrases) and semantic roles
(for nominalizations). Lauer (1995) extended this research in developing an exclusively
preposition-based set of seven semantic classes. For example, Levi would interpret truck
driver as PATIENT, in the sense that truck is the patient of the underlying verb of the
head noun to drive, whereas Lauer would interpret it as OF (c.f., driver of (the) truck).
Girju (2007, 2009) leveraged translation data to improve the accuracy of compound
noun interpretation, based on the observation that the choice of preposition in Ro-
mance languages is often indicative of the semantics of the compound noun. Con-
versely, Johnston and Busa (1996) used Qualia structure from the Generative Lexicon
(Pustejovsky 1995) to interpret the prepositions in Italian complex nominals, such as
macchina da corsa ?race car?. Jensen and Nilsson (2003) used (Danish) prepositions as
the basis of a finite set of role relations with which to describe the meaning content
of nominals, and demonstrated the utility of the resultant ontology in disambiguating
nominal phrases.
4. Applications
Prepositions have tended to be overlooked in NLP applications, but there have been
isolated examples of prepositions being shown to be worthy of dedicated treatment. In
particular, applications requiring some level of syntactic abstraction tend to benefit from
133
Computational Linguistics Volume 35, Number 2
the inclusion of prepositions. Similarly, applications which incorporate an element of
natural language generation or realization need to preserve prepositions in the interests
of producing well-formed outputs.
Riloff (1995) challenged the validity of the stop-word philosophy for text classi-
fication, and demonstrated that dependency tuples incorporating prepositions are a
more effective document representation than simple words. In a direct challenge to
the prevalent ?stop word? perception of prepositions in information retrieval, Hansen
(2005) and Lassen (2006) placed emphasis on not only prepositions but preposi-
tion semantics in a music retrieval system and ontology-based text search system,
respectively.
Information extraction is one application where prepositions are uncontrover-
sially crucial to system accuracy, in terms of the role they play in named entities
(Cucchiarelli and Velardi 2001; Toral 2005; Kozareva 2006) and in IE patterns, in linking
the elements in a text (Appelt et al 1993; Muslea 1999; Ono et al 2001; Leroy and Chen
2002).
Benamara (2005) used preposition semantics in a cooperative question answering
system. In the context of cross-language question answering (CLQA), Hartrumpf,
Helbig, and Osswald (2006) used MultiNet to interpret the semantics of German
prepositions, and demonstrated that in instances where the answer passage contained
a different preposition to that included in the original question, preposition semantics
boosted the performance of their CLQA system.
Boonthum, Toida, and Levinstein (2006) successfully applied their prepositionWSD
method in a paraphrase recognition task, namely, predicting that Kim covered the baby in
blankets and Kim covered the baby with blankets have essentially the same semantics. They
proposed seven general senses of prepositions (e.g., PARTICIPANT, INSTRUMENT, and
QUALITY), and annotated prepositions occurring in 120 sentences for each of 10 prepo-
sitions. They evaluated aWSDmethod over this data, and sketched how the preposition
sense information could then be used for paraphrase recognition.
Prepositions have deservedly received a moderate amount of attention in appli-
cations which require explicit representation of 3D space, such as robotics, animated
agents, and virtual reality, in the context of interpreting the spatial information of
prepositions. For example, Xu and Badler (2000) developed a geometric definition of
the motion trajectories of prepositions, whereas Tokunaga, Koyama, and Saito (2005)
use potential functions to estimate the spatial extent of Japanese spatial nouns (which
combine with postpositions to have a similar syntactic and semantic profile to Eng-
lish spatial prepositions). Kelleher and van Genabith (2003) proposed a method for
interpreting in front of and behind in a virtual reality environment based on different
frames of reference. Hying (2007) carried out an analysis of preposition semantics
in the HRCR Map Task corpus, and used it to evaluate two models of projective
prepositions. Kelleher and Kruijff (2005) developed a model for grounding spatial
expressions in visual perception and also for modeling proximity, and Reichelt and
Verleih (2005) developed the B3D system for generating a computational representation
of prepositions in geospatial applications. Furlan, Baldwin, and Klippel (2007) used
preposition occurrence in Web data as a means of classifying landmarks for use in
route directions. Finally, Kelleher and Costello (2009) proposed computational models
of topological and projective spatial prepositions for use in a visually situated dialogue
system.
In the field of machine translation (MT), spatial and temporal prepositions have
received a moderate amount of attention, particularly in the context of interlingua-
and transfer-based MT. Dorr and Voss (1993) mapped prepositions onto 5-place spatial
134
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
predicates in the context of interlingua-based MT, building on the work of Talmy (1985)
and Jackendoff (1983, 1990). Nu?bel (1996) developed a set of interlingual predicates for
adjunct PPs in English?German MT, incorporating a dialogue component which can
be used to disambiguate prepositions relative to the discourse context. Bond, Ogura,
and Uchino (1997) developed a dedicated type hierarchy for the generation of preposi-
tions associated with temporal expressions in Japanese?English machine translation.
Kumar Naskar and Bandyopadhyay (2006) proposed a transfer-based method for trans-
lating English prepositions into Bengali postpositions/inflectional markers, focusing
on spatial, temporal, and also idiomatic usages. Bond (1998) proposed an algorithm
for translating Japanese spatial nouns into English prepositions based on semantic
fields. Trujillo (1995) used his classification of spatial prepositions as the basis of an
English?Spanish translation system using bilingual lexical rules. Hajic? et al (2002)
proposed a method for inserting prepositions into tectogrammatical representations in
Czech?English MT, although they did not manage to integrate the predictions into
their final MT system. Husain, Sharma, and Reddy (2007) achieved promising results
using an explicit model of preposition semantics as the basis for preposition selection in
Hindi/Telugu?English machine translation.
In the context of statistical machine translation, Toutanova and Suzuki (2007) iden-
tified that case marker (= postposition) generation poses a significant challenge for stan-
dard phrase-based methods in English?Japanese translation, identifying case marker
errors in 16% of outputs. They proposed an n-best reranking method to improve case
marker generation performance, whereby they expand the n-best list to include extra
case marker variations, and perform case marker prediction for each bunsetsu.7 In eval-
uation, they demonstrated that their proposed method significantly outperforms both
the baseline SMT system (Quirk, Menezes, and Cherry 2005) and a comparable n-best
rerankingmethodwithout dedicated casemarker candidate expansion. The significance
of this research is that it demonstrates that dedicated handling of postpositions can
enhance SMT performance, a result which has promise for adpositions and markers in
other languages.
As stated in Section 1, prepositions are notoriously hard for non-native speakers to
master, and are a frequent source of errors in English as a Second Language (ESL) prose.
Errors can take the form of incorrect preposition selection (e.g., *Kim stayed in home),
erroneous preposition insertion (e.g., *Kim played at outside), or erroneous preposition
omission (e.g., *Kim went ? the conference) (Tetreault and Chodorow 2008b). Tetreault
and Chodorow (2008b) proposed a combined detection/correction method based on a
supervised model. For each preposition in a text, they predict the most likely candidate
from 34 candidates, based on local word context. If the most likely candidate differs
from the original preposition selection, an error is predicted and correction proposed.
In evaluation over ESL texts, they found that their method performs at high precision
but low recall. Separately, they found that the same method performs considerably
better when applied to preposition selection over native English text, and also that it
is important to have multiple annotators correct ESL text in order to avoid skewing the
data (Tetreault and Chodorow 2008a). In other research, Gamon et al (2008) performed
preposition selection in terms of both selection and insertion, but over a smaller
set of prepositions. De Felice and Pulman (2007) similarly proposed a method for
7 Roughly speaking, a bunsetsu is a case-marked chunk.
135
Computational Linguistics Volume 35, Number 2
preposition correction, but only evaluated their model over five prepositions and native
English text.
5. Introduction to the Articles in This Special Issue
For this special issue we invited submissions that brought a theoretical basis to research
on prepositions in lexical resources and NLP tasks. The number of submissions re-
ceived reflects the interest in prepositions at this time, with a total of 16 submissions.
On the basis of a rigorous review process, we selected four articles for inclusion in
the special issue, covering: the use of semantic resources to disambiguate preposition
semantics, for use in lexical acquisition (O?Hara and Wiebe 2009); a crosslingual lexical
semantic analysis of prepositions to interpret nominal compounds (Girju 2009); a formal
semantic analysis of preposition semantics, and its possible application in Norwegian?
English machine translation (J?rgensen and L?nning 2009); and a computational model
for preposition semantics for use in a dialogue system (Kelleher and Costello 2009). We
now outline each of these articles.
The first two articles look at the semantic interpretation of prepositions, as they tend
to be highly polysemous and have a number of closely related senses.
O?Hara and Wiebe look at the disambiguation of preposition semantics for use
in lexical acquisition, in semi-automatically extending lexical resources. They investi-
gate the utility of information learned from resources such as the Penn Treebank and
FrameNet, to perform semantic role disambiguation of PPs. The proposedmethodology
is evaluated in a series of experiments, and different sense granularities are contrasted
in task-based evaluation.
Prepositions are highly frequent in many languages, but also highly idiomatic in
usage in a given language (hence the difficulty of non-native speakers in learning to use
prepositions correctly). However, there are instances of cross-linguistic regularities in
their linguistic realizations. For example, when translating English nominal compounds
of the type Noun Preposition Noun (N P N) and Noun Noun (N N) into Romance
languages, these are often translated into N PN compounds, where the choice of prepo-
sition is (semi-)predictable from the semantics of the compound. Girju investigates
the role of the syntactic and semantic properties of prepositions in English and Romance
languages in the automatic semantic interpretation of English nominal compounds.
On the basis of an extensive corpus analysis of the distribution of semantic relations
in nominal compounds in English and five Romance languages, she attempts to dis-
ambiguate the semantic relations in English nominal compounds. She focuses on non-
equative compositional nominal compounds, and empirically tests the contribution of
prepositions to the task of semantic interpretation, using a supervised, knowledge-
intensive model.
Identification of crosslinguistic and possibly universal properties of prepositions (or
equivalent constructions) could potentially impact on a number of NLP applications. In
machine translation, for example, a formal description of the crosslinguistic semantic
properties of prepositions could provide the basis for an interlingua. This is the topic of
the third article in this special issue: J?rgensen and L?nning propose a unification-based
grammar implementation of a formal semantic analysis of preposition semantics, and
demonstrate its application in Norwegian?English MT.
A correct handling of prepositions, particularly spatial prepositions, is also impor-
tant in dialogue systems, as prepositions are often used to refer to entities in the physical
environment of system interaction. The last article in this special issue addresses this
136
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
topic: Kelleher and Costello present computational models of spatial preposition
semantics for use in visually situated dialogue systems. The proposed models of
topological and projective spatial prepositions can be used for both interpretation and
generation of prepositional expressions in complex visual environments containing
multiple objects, and are able to account for the contextual effect which other distractor
objects can have on the region described by a preposition. The evaluation is done in
terms of psycholinguistic tests evaluating the approach to distractor interference on
prepositional semantics. The models are employed in a human?robot dialogue system
to interpret locative expressions containing a topological preposition and to generate
spatial references in visually situated contexts.
6. Discussion and Conclusion
As we hope to have demonstrated in this introduction, prepositions have led a mixed
existence in computational linguistics and related fields, particularly in the context
of applications. In applications requiring spatial interpretation (e.g., situated dialogue
systems), they have been the focus of dedicated research, and in applications which
incorporate a language generation component such as MT, there have been isolated
instances exemplifying the need for dedicated handling of prepositions/case markers.
In general, however, they tend to have been relegated to the sidelines in applied NLP
research.
Research on prepositions has tended to concentrate on a specific subset of preposi-
tions in a particular language, often as a one-off research task which has failed to make
broader impact in the field of computational linguistics. PP attachment?especially in
English?has been an exception, in the sense that the RRR data set has given rise to an
active strand of research centered around prepositions.We hope that variants of the RRR
data set from Atterer and Schu?tze (2007) and Agirre, Baldwin, and Martinez (2008) will
reinvigorate interest in PP attachment, in a situated parsing context. Similarly, the emer-
gence of resources such as the Preposition Project, and the data set made available for
the SemEval 2007 task on the word sense disambiguation of prepositions (Litkowski
and Hargraves 2007), provide the means for more detailed analysis of preposition
semantics. In addition to standalone word sense disambiguation tasks, however, there
needs to be more research on the interaction of preposition semantics with other
semantic tasks, such as semantic role labeling and the word sense disambiguation of
content words. The increasing availability of large-scale parallel corpora paves the way
for crosslinguistic research on preposition syntax. Areas of particular promise in this
regard are methods for generating prepositions inMT, and automatic error correction of
preposition usage in non-native speaker text. Following the lead of Saint-Dizier (2006b),
J?rgensen and L?nning (2009), and others, we also hope to see more crosslinguistic
and typological research on the lexical semantics of prepositions. Although there has
been a steady proliferation of WordNets for different languages, linked variously to
English WordNet (e.g., EuroWordNet for several European languages [Vossen 1998],
BALKANET for Balkan languages [Stamou et al 2002], HowNet for Chinese [Dong and
Dong 2006], and Japanese WordNet for Japanese [Isahara et al 2008]), they have tended
to follow the lead of English WordNet and focus exclusively on content words. Given
the increasing maturity of resources such as the Preposition Project and PrepNet, the
time seems right to develop preposition sense inventories for more languages, linked
back to English. On the basis of currently available resources and future efforts such
as these, we believe there will be a steady lowering of the barrier to including a more
systematic handling of prepositions in NLP applications.
137
Computational Linguistics Volume 35, Number 2
The purpose of this article has been to highlight the theoretical and applied re-
search that has been done on prepositions in computational linguistics, focusing on
computational syntax and semantics. In particular, we have aimed to highlight applied
research which has focused specifically on prepositions. It is our hope that through this
special issue, we will rekindle interest in prepositions and motivate greater awareness
of prepositions in various applications.
Acknowledgments
Thanks to Ken Litkowski, Paola Merlo,
Patrick Saint-Dizier, and Martin Volk for
feedback on an earlier draft of this paper,
and Robert Dale and Mary Gardiner for their
unfailing support and encouragement
throughout the editorial process. We also
acknowledge the efforts of the considerable
number of reviewers who devoted time to
reviewing the original submissions to the
special issue, and fine-tuning the accepted
articles. The first author of this article was
supported in part by the Australian Research
Council through Discovery Project grant
DP0663879.
References
Aasa, Jo?rgen. 2004. Unsupervised resolution
of PP attachment ambiguities in Swedish.
Master?s thesis, Stockholm University.
Abeille?, Anne, Olivier Bonami, Danie`le
Godard, and Jesse Tseng. 2003. The
syntax of a` and de: an HPSG analysis. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 133?144,
Toulouse.
Abney, Steven, Robert E. Schapire, and
Yoram Singer. 1999. Boosting applied to
tagging and PP attachment. In Proceedings
of the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-99),
pages 38?45, College Park, MD.
Agirre, Eneko, Timothy Baldwin, and
David Martinez. 2008. Improving parsing
and PP attachment performance with
sense information. In Proceedings of the
46th Annual Meeting of the ACL: Human
Language Technologies, pages 317?325,
Columbus, OH.
Alam, Yukiko Sasaki. 2003. For
computational treatment of the polysemy
of prepositional uses of Over. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 66?76,
Toulouse.
Alam, Yukiko Sasaki. 2004. Decision trees
for sense disambiguation of prepositions:
Case of over. In Proceedings of the
HLT-NAACL 2004 Workshop on
Computational Lexical Semantics,
pages 52?59, Boston, MA.
Alegre, Martha A., Josep M. Sopena, and
Agusti Lloberas. 1999. PP-attachment: A
committee machine approach. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC-99), pages 231?238,
College Park, MD.
Allen, James F. 1984. Towards a general
theory of action and time. Artificial
Intelligence, 23:123?154.
Altmann, Gerry and Mark Steedman. 1988.
Interaction with context during human
sentence processing. Cognition,
30(3):191?238.
Appelt, Douglas E., Jerry R. Hobbs, John
Bear, David Israel, Megumi Kameyama,
and Mabry Tyson. 1993. FASTUS: A
finite-state processor for information
extraction from real-world text. In
Proceedings of the 13th International Joint
Conference on Artificial Intelligence
(IJCAI-93), pages 1172?1181, Chambery.
Arsenijevic?, Boban. 2005. Subevents and
prepositions. In Proceedings of the Second
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 39?46, Colchester.
Atterer, Michaela and Hinrich Schu?tze. 2007.
Prepositional phrase attachment without
oracles. Computational Linguistics,
33(4):469?476.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999.Modern Information
Retrieval. Addison Wesley/ACM press,
Harlow, UK.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the ACL and 17th
International Conference on Computational
138
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Linguistics: COLING/ACL-98, pages 86?90,
Montreal.
Baldwin, Timothy. 2005a. Deep lexical
acquisition of verb-particle constructions.
Computer Speech and Language,
19(4):398?414.
Baldwin, Timothy. 2005b. Looking for
prepositional verbs in corpus data. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 180?189, Colchester.
Baldwin, Timothy. 2006. Distributional
similarity and preposition semantics. In
Saint-Dizier (Saint-Dizier 2006a).
Baldwin, Timothy. 2008. A resource for
evaluating the deep lexical acquisition of
English verb-particle constructions. In
Proceedings of the LREC 2008 Workshop:
Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 1?2,
Marrakech.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003.
An empirical model of multiword
expression decomposability. In
Proceedings of the ACL-2003 Workshop
on Multiword Expressions: Analysis,
Acquisition and Treatment, pages 89?96,
Sapporo.
Baldwin, Timothy, John Beavers, Leonoor
van der Beek, Francis Bond, Dan
Flickinger, and Ivan A. Sag. 2006. In search
of a systematic treatment of determinerless
PPs. In Saint-Dizier (Saint-Dizier 2006a).
Baldwin, Timothy, Emily M. Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon.
Baldwin, Timothy and Aline Villavicencio.
2002. Extracting the unextractable: A case
study on verb-particles. In Proceedings of
the 6th Conference on Natural Language
Learning (CoNLL-2002), pages 98?104,
Taipei.
Bannard, Colin. 2005. Learning about the
meaning of verb-particle constructions
from corpora. Computer Speech and
Language, 19(4):467?478.
Bannard, Colin. 2006. Acquiring Phrasal
Lexicons from Corpora. Ph.D. thesis,
University of Edinburgh.
Barker, Ken. 1996. The assessment of
semantic cases using English positional,
prepositional and adverbial case markers.
Technical Report TR-96-08, Computer
Science, University of Ottawa.
Beavers, John. 2003. The semantics and
polysemy of goal marking prepositions in
Japanese. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 205?216, Toulouse.
Benamara, Farah. 2005. Reasoning with
prepositions within a cooperative
question-answering framework. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 145?152, Colchester.
Bennett, David C. 1975. Spatial and Temporal
uses of English Prepositions: As Essay in
Stratified Semantics. Longman, London.
Bikel, Daniel M. 2004. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Bond, Francis. 1998. Interpretation of
Japanese ?spatial? nouns in
Japanese-to-English machine translation.
Presentation at the 1998 Annual General
Meeting of the Australian Linguistics
Society: ALS-98.
Bond, Francis, Kentaro Ogura, and Hajime
Uchino. 1997. Temporal expressions in
Japanese-to-English machine translation.
In Proceedings of the 7th International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-97),
pages 55?62, Santa Fe, NM.
Boonthum, Chutima, Shunichi Toida, and
Irwin Levinstein. 2005. Sense
disambiguation for preposition ?with?. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 153?162, Colchester.
Boonthum, Chutima, Shunichi Toida, and
Irwin Levinstein. 2006. Preposition senses:
Generalized disambiguation model. In
Proceedings of the 7th International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2006),
pages 196?207, Mexico City.
Brala, Marija M. 2000. Understanding and
translating (spatial) prepositions: An
exercise in cognitive semantics for
lexicographic purposes.Working Papers of
the University of Cambridge Research Centre
for English and Applied Linguistics, 7.
University of Cambridge.
139
Computational Linguistics Volume 35, Number 2
Bre?e, D. S. and R. A. Smit. 1986. Temporal
relations. Journal of Semantics, 5(4):345?384.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the 5th
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Briscoe, Ted, John Carroll, and Rebecca
Watson. 2006. The second release of the
RASP system. In Proceedings of the Poster
Session of the 21st International Conference on
Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics (COLING/ACL 2006),
pages 77?80, Sydney.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus. Oxford University
Computing Services, Oxford, UK.
Calvo, Hiram, Alexander Gelbukh, and
Adam Kilgarriff. 2005. Distributional
thesaurus versus WordNet: A comparison
of backoff techniques for unsupervised PP
attachment. In Proceedings of the 6th
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing-2005), pages 177?188,
Mexico City.
Calzolari, Nicoletta, Charles Fillmore,
Ralph Grishman, Nancy Ide, Alessandro
Lenci, Catherine MacLeod, and
Antonio Zampolli. 2002. Towards
best practice for multiword expressions
in computational lexicons. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC 2002), pages 1934?1940,
Las Palmas.
Cannesson, Emmanuelle and Patrick
Saint-Dizier. 2002. Defining and
representing preposition senses: A
preliminary analysis. In Proceedings
of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes
and Future Directions, pages 25?31,
Philadelphia, PA.
Che?liz, Mar??a del Carmen Horno. 2002. Lo
que la Preposicio?n Esconde: Estudio Sobre la
Argumentalidad Preposicional en el Predicado
Verbal. University of Zaragoza Press,
Zaragoza, Spain. (In Spanish).
Chodorow, Martin, Joel Tetreault, and
Na-Rae Han. 2007. Detection of
grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 25?30,
Prague.
Church, Kenneth and Ramesh Patil. 1982.
Coping with syntactic ambiguity or how
to put the block in the box on the table.
American Journal of Computational
Linguistics, 8(3?4):139?149.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through a
backed-off model. In Proceedings of the 3rd
Annual Workshop on Very Large Corpora,
pages 27?38, Cambridge, MA.
Cook, Paul and Suzanne Stevenson. 2006.
Classifying particle semantics in English
verb-particle constructions. In Proceedings
of the COLING/ACL 2006 Workshop on
Multiword Expressions: Identifying and
Exploiting Underlying Properties,
pages 45?53, Sydney.
Copestake, Ann, Dan Flickinger, Ivan A. Sag,
and Carl Pollard. 2005. Minimal recursion
semantics: An introduction. Journal of
Research on Language and Computation,
3(2?3):281?332.
Copestake, Ann, Fabre Lambeau, Aline
Villavicencio, Francis Bond, Timothy
Baldwin, Ivan Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic
precision and reusability. In Proceedings of
the 3rd International Conference on Language
Resources and Evaluation (LREC 2002),
pages 1941?1947, Las Palmas.
Cosme, Christelle and Gae?tanelle Gilquin.
2008. Free and bound prepositions
in a contrastive perspective: The
case of with and avec. In Sylviane
Granger and Fanny Meunier, editors,
Phraseology: An Interdisciplinary
Perspective. John Benjamins, Amsterdam,
pages 259?274.
Cucchiarelli, Alessandro and Paola Velardi.
2001. Unsupervised named entity
recognition using syntactic and semantic
contextual evidence. Computational
Linguistics, 27(1):123?131.
Dahlgren, Kathleen and Joyce McDowell.
1986. Using commonsense knowledge
to disambiguate prepositional
phrase modifiers. In Proceedings of
the 6th Conference on Artificial
Intelligence (AAAI-86), pages 589?593,
Philadelphia, PA.
De Felice, Rachele and Stephen G. Pulman.
2007. Automatically acquiring models of
preposition use. In Proceedings of the 4th
ACL-SIGSEMWorkshop on Prepositions,
pages 45?50, Prague.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
140
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Dehe?, Nicole, Ray Jackendoff, Andrew
McIntyre, and Silke Urban, editors. 2002.
Verb-particle explorations. Mouton de
Gruyter, Berlin/New York.
Denis, Pascal, Jonas Kuhn, and Stephen
Wechsler. 2003. V-PP goal motion
complexes in English: An HPSG account.
In Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 121?132,
Toulouse.
Dixon, Robert M. W. 1982. The grammar of
English phrasal verbs. Australian Journal of
Linguistics, 2:149?247.
Do?mges, Florian, Tibor Kiss, Antje Mu?ller,
and Claudia Roch. 2007. Measuring the
productivity of determinerless PPs. In
Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 31?37,
Prague.
Dong, Zhendong and Qiang Dong. 2006.
HowNet And the Computation of Meaning.
World Scientific Publishing, River
Edge, NJ.
Dorr, Bonnie. 1993.Machine Translation: A
View from the Lexicon. MIT Press,
Cambridge, MA.
Dorr, Bonnie and Nizar Habash. 2002.
Interlingua approximation: A
generation-heavy approach. In Proceedings
of the AMTA-2002 Interlingua Reliability
Workshop, Tiburon, CA.
Dorr, Bonnie J. 1997. Large-scale dictionary
construction for foreign language tutoring
and interlingual machine translation.
Machine Translation, 12(4):271?322.
Dorr, Bonnie J. and Clare R. Voss. 1993.
Machine translation of spatial expressions:
Defining the relation between an
interlingua and a knowledge
representation system. In Proceedings of the
11th Annual Conference on Artificial
Intelligence (AAAI-93), pages 374?379,
Washington, DC.
Durrell, Martin and David Bre?e. 1993.
German temporal prepositions from an
English perspective. In Zelinski-Wibbelt
(Zelinski-Wibbelt 1993), pages 295?326.
Evert, Stefan and Brigitte Krenn. 2005. Using
small random samples for the manual
evaluation of statistical association
measures. Computer Speech and Language,
19(4):450?466.
Fazly, Afsaneh, Paul Cook, and Suzanne
Stevenson. 2009. Unsupervised type and
token identification of idiomatic
expressions. Computational Linguistics,
35(1):61?103.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fillmore, Charles J. 1968. The case for case. In
Emmon Bach and Robert T. Harms,
editors, Universals in Linguistic Theory.
Holt, Rinehart, and Winston, New York,
pages 1?88.
Ford, Marilyn, Joan Bresnan, and Ronald
Kaplan. 1982. A competence-based theory
of syntactic closure. In Joan Bresnan,
editor, The Mental Representation of
Grammatical Relations. MIT Press,
Cambridge, MA, pages 727?796.
Fort, Kare?n and Bruno Guillaume. 2007.
PrepLex: A lexicon of French prepositions
for parsing. In Proceedings of the 4th
ACL-SIGSEMWorkshop on Prepositions,
pages 17?24, Prague.
Foth, Kilian A. and Wolfgang Menzel. 2006.
The benefit of stochastic PP attachment to
a rule-based parser. In Proceedings of the
Poster Session of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006), pages 223?230, Sydney.
Franz, Alexander. 1996. Learning PP
attachment from corpus statistics. In Stefan
Wermter, Ellen Riloff, and Gabriele
Scheler, editors, Connectionist, Statistical,
and Symbolic Approaches to Learning for
Natural Language Processing. Springer,
Berlin, Germany, pages 188?202.
Frazier, Lyn. 1979. On Comprehending
Sentences: Syntactic Parsing Strategies.
Ph.D. thesis, University of Connecticut.
Furlan, Aidan, Timothy Baldwin, and Alex
Klippel. 2007. Landmark classification for
route description generation. In Proceedings
of the 4th ACL-SIGSEMWorkshop on
Prepositions, pages 9?16, Prague.
Gala, Nuria and Mathieu Lafourcade.
2005. Combining corpus-based pattern
distributions with lexical signatures
for PP attachment ambiguity resolution.
In Proceedings of the 6th Symposium on
Natural Language Processing (SNLP-05),
Chiang Rai.
Gamon, Michael, Jianfeng Gao, Chris
Brockett, Alexandre Klementiev,
William B. Dolan, Dmitriy Belenko, and
Lucy Vanderwende. 2008. Using
contextual speller techniques and
language modeling for ESL error
correction. In Proceedings of the 3rd
International Joint Conference on Natural
Language Processing (IJCNLP-08),
pages 449?456, Hyderabad.
141
Computational Linguistics Volume 35, Number 2
Gaussier, Eric and Nicola Cancedda. 2001.
Probabilistic models for PP-attachment
resolution and NP analysis. In Proceedings
of the ACL/EACL-2001 Workshop on
Computational Natural Language Learning
(CoNLL-2001), pages 1?8, Toulouse.
Girju, Roxana. 2007. Improving the
interpretation of noun phrases with
cross-linguistic information. In Proceedings
of the 45th Annual Meeting of the ACL,
pages 568?575, Prague.
Girju, Roxana. 2009. The syntax and
semantics of prepositions in the task of
automatic interpretation of nominal
phrases and compounds: A cross-linguistic
study. Computational Linguistics,
35(2):185?228.
Hahn, Udo, Martin Romacker, and Stefan
Schulz. 2002. Creating knowledge
repositories from biomedical reports: The
MEDSYNDIKATE system. In Proceedings of
the Pacific Symposium on Biocomputing 2002,
pages 338?349, Lihue, HI.
Hajic?, Jan, Martin C?mejrek, Jason Eisner,
Gerald Penn, Owen Rambow, Drago
Radev, Yuan Ding, Terry Koo, and Kristen
Parton. 2002. Natural language generation
in the context of machine translation.
Technical report, CSLP, Johns Hopkins
University. Available at www.clsp.jhu.
edu/ws02/groups/mt/?MT final rpt.pdf.
Hansen, Steffen Leo. 2005. Concept-based
representation of prepositions. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 138?144, Colchester.
Hartrumpf, Sven. 1999. Hybrid
disambiguation of prepositional phrase
attachment and interpretation. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC-99), pages 111?120,
College Park, MD.
Hartrumpf, Sven, Hermann Helbig, and
Rainer Osswald. 2006. Semantic
interpretation of prepositions for NLP
applications. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 29?36, Trento.
Helbig, Hermann. 2006. Knowledge
Representation and the Semantics of
Natural Language. Springer, Dordrecht,
Netherlands.
Hellan, Lars and Dorothee Beermann. 2005.
Classification of prepositional senses for
deep grammar applications. In Proceedings
of the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 74?83,
Colchester.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hirst, Graeme. 1987. Semantic Interpretation
and the Resolution of Ambiguity. Cambridge
University Press, Cambridge, UK.
Huddleston, Rodney and Geoffrey K.
Pullum. 2002. The Cambridge Grammar of
the English Language. Cambridge
University Press, Cambridge, UK.
Husain, Samar, Dipti Misra Sharma, and
Manohar Reddy. 2007. Simple preposition
correspondence: A problem in English to
Indian language machine translation. In
Proceedings of the 4th ACL-SIGSEM
Workshop on Prepositions, pages 51?58,
Prague.
Hying, Christian. 2007. A corpus-based
analysis of geometric constraints on
projective prepositions. In Proceedings
of the 4th ACL-SIGSEMWorkshop on
Prepositions, pages 1?8, Prague.
Isahara, Hitoshi, Francis Bond, Kiyotaka
Uchimoto, Masao Utiyama, and Kyoko
Kanzaki. 2008. Development of the
Japanese WordNet. In Proceedings of the
6th International Conference on Language
Resources and Evaluation (LREC 2008),
Marrakech.
Izumi, Emi, Kiyotaka Uchimoto, Toyomi
Saiga, Thepchai Supnithi, and Hitoshi
Isahara. 2003. Automatic error detection
in the Japanese learners? English spoken
data. In Proceedings of the 41st Annual
Meeting of the ACL, pages 145?148,
Sapporo.
Jackendoff, Ray. 1973. The base rules for
prepositional phrases. In Stephen R.
Anderson and Paul Kiparsky, editors, A
Festschrift for Morris Halle. Holt, Rinehart,
and Winston Inc., New York,
pages 345?356.
Jackendoff, Ray. 1983. Semantics and
Cognition. MIT Press, Cambridge, MA.
Jackendoff, Ray. 1990. Semantic Structures.
MIT Press, Cambridge, MA.
Jensen, Karen and Jean-Louis Binot. 1987.
Disambiguating prepositional phrase
attachments by using on-line dictionary
definitions. Computational Linguistics,
13(3?4):251?260.
Jensen, Per Anker and J. Fischer Nilsson.
2003. Ontology-based semantics for
prepositions. In Proceedings of the
142
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 193?204, Toulouse.
Johnston, Michael and Frederica Busa. 1996.
Qualia structure and the compositional
interpretation of compounds. In
Proceedings of the ACL SIGLEX Workshop on
Breadth and Depth of Semantic Lexicons,
pages 77?88, Santa Cruz, CA.
J?rgensen, Fredrik and Jan Tore L?nning.
2009. A minimal recursion semantic
analysis of locatives. Computational
Linguistics, 35(2):229?270.
Kamp, Hans. 1981. Theory of truth and
semantic representation. In B. J. Janssen
and T. M. V. Stokhof, editors, Formal
Methods in the Study of Language.
Mathematisch Centrum, Amsterdam,
pages 277?322.
Kelleher, John and Josef van Genabith. 2003.
A computational model of the referential
semantics of projective prepositions. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 181?192,
Toulouse.
Kelleher, John D. and Fintan J. Costello.
2009. Applying computational models of
spatial prepositions to visually situated
dialog. Computational Linguistics,
35(2):271?306.
Kelleher, John D. and Geert-Jan M. Kruijff.
2005. A context-dependent model of
proximity in physically situated
environments. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 128?137,
Colchester.
Kim, Su Nam and Timothy Baldwin. 2006.
Automatic identification of English verb
particle constructions using linguistic
features. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 65?72, Trento.
Kim, Su Nam and Timothy Baldwin. 2007.
Detecting compositionality of English
verb-particle constructions using semantic
similarity. In Proceedings of the 7th Meeting
of the Pacific Association for Computational
Linguistics (PACLING 2007), pages 40?48,
Melbourne.
Kim, Su Nam and Timothy Baldwin (in
press). How to pick out token instances of
English verb-particle constructions.
Language Resources and Evaluation.
Kipper, Karin, Hoa Trang Dang, and Martha
Palmer. 2000. Class-based construction of a
verb lexicon. In Proceedings of the 18th
Annual Conference on Artificial Intelligence
(AAAI-2000), pages 691?696, Austin, TX.
Kipper, Karin, Benjamin Snyder, and Martha
Palmer. 2004. Using prepositions to extend
a verb lexicon. In Proceedings of the
HLT-NAACL 2004 Workshop on
Computational Lexical Semantics,
pages 23?29, Boston, MA.
Kipper Schuler, Karin. 2005. VerbNet: A
Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. thesis, University of Pennsylvania.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
ACL, pages 423?430, Sapporo.
Knas?, Iwona. 2006. Polish equivalents of
spatial at. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 9?16, Trento.
Kokkinakis, Dimitris. 2000. Supervised
PP-attachment for Swedish: Combining
unsupervised and supervised training
data. Nordic Journal of Linguistics,
3(2):191?213.
Kordoni, Valia. 2003a. The key role of
semantics in the development of
large-scale grammars of natural language.
In Proceedings of the 10th Conference of the
EACL (EACL 2003), pages 111?14,
Budapest.
Kordoni, Valia. 2003b. A robust deep analysis
of indirect prepositional arguments. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 112?120,
Toulouse.
Kordoni, Valia. 2006. Prepositional
arguments in a multilingual context. In
Saint-Dizier (Saint-Dizier 2006a).
Korhonen, Anna. 2002. Subcategorization
Acquisition. Ph.D. thesis, University of
Cambridge.
Kozareva, Zornitsa. 2006. Bootstrapping
named entity recognition with
automatically generated gazetteer lists. In
Proceedings of the EACL 2006 Student
Research Workshop, pages 15?21, Trento.
Kracht, Marcus. 2003. Directionality
selection. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 89?100, Toulouse.
Krenn, Brigitte. 2008. Description of
evaluation resource?German PP-verb
143
Computational Linguistics Volume 35, Number 2
data. In Proceedings of the LREC 2008
Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008),
pages 7?10, Marrakech.
Krenn, Brigitte and Stefan Evert. 2001. Can
we do better than frequency? A case study
on extracting PP-verb collocations. In
Proceedings of the ACL/EACL 2001 Workshop
on the Computational Extraction, Analysis
and Exploitation of Collocations, pages 39?46,
Toulouse.
Kumar Naskar, Sudip and Sivaji
Bandyopadhyay. 2006. Handling of
prepositions in English to Bengali machine
translation. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 89?94, Trento.
Lassen, Tine. 2006. An ontology-based view
of prepositional senses. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 45?50, Trento.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Noun
Compounds. Ph.D. thesis, Macquarie
University.
Leroy, Gondy and Hsinchun Chen. 2002.
Filling preposition-based templates to
capture information from medical
abstracts. In Proceedings of the Pacific
Symposium on Biocomputing 2002,
pages 350?61, Lihue, HI.
Leroy, Gondy, Hsinchun Chen, and Jesse D.
Martinez. 2003. A shallow parser based on
closed-class words to capture relations in
biomedical text. Journal of Biomedical
Informatics, 36:145?158.
Lersundi, Mikel and Eneko Agirre. 2003.
Semantic interpretations of postpositions
and prepositions: A multilingual inventory
for Basque, English and Spanish. In
Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and Their Use in Computational Linguistics
Formalisms and Applications, pages 56?65,
Toulouse.
Lestrade, S. A. M. 2006. Marked adpositions.
In Proceedings of the Third ACL-SIGSEM
Workshop on Prepositions, pages 23?28,
Trento.
Levi, Judith N. 1978. The Syntax and Semantics
of Complex Nominals. Academic Press,
New York.
Levin, Beth and Malka Rappaport-Hovav (in
press). Lexical conceptual structure. In
Claudia Maienborn, Klaus von Heusinger,
and Paul Portner, editors, Semantics: An
International Handbook of Natural Language
Meaning. Mouton de Gruyter, Berlin,
Germany.
Li, Wei, Xiuhong Zhang, Cheng Niu, Yuankai
Jiang, and Rohini K. Srihari. 2003. An
expert lexicon approach to identifying
English phrasal verbs. In Proceedings of the
41st Annual Meeting of the ACL,
pages 513?520, Sapporo.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the ACL and 17th International
Conference on Computational Linguistics
(COLING/ACL-98), pages 768?774,
Montreal.
Lin, Dekang. 2003. Dependency-based
evaluation of MINIPAR. In Anne Abeille?,
editor, Building and Using Syntactically
Annotated Corpora. Kluwer, Dordrecht,
Netherlands, pages 317?330.
Lindstromberg, Seth. 1998. English
Prepositions Explained. John Benjamins,
Amsterdam, Netherlands.
Lindstromberg, Seth. 2001. Preposition
entries in UK monolingual learner?s
dictionaries: Problems and possible
solutions. Applied Linguistics, 22(1):79?103.
Litkowski, Ken and Orin Hargraves. 2005.
The Preposition Project. In Proceedings of
the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 171?179,
Colchester.
Litkowski, Kenneth C. 2002. Digraph
analysis of dictionary preposition
definitions. In Proceedings of the ACL-02
Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions,
pages 9?16, Philadelphia, PA.
Litkowski, Kenneth C. and Orin Hargraves.
2006. Coverage and inheritance in the
preposition project. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 37?44, Trento.
Litkowski, Kenneth C. and Orin Hargraves.
2007. SemEval-2007 task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 24?29,
Prague.
MacKinlay, Andrew and Timothy Baldwin.
2005. POS tagging with a more informative
tagset. In Proceedings of the Australasian
Language Technology Workshop 2005,
pages 40?48, Sydney.
Manning, Christopher D. 1993. Automatic
acquisition of a large subcategorization
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the ACL,
pages 235?242, Columbus, OH.
144
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Manning, Christopher D., Prabhakar
Raghavan, and Hinrich Schu?tze. 2008.
Introduction to Information Retrieval.
Cambridge University Press, Cambridge,
UK.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: the
Penn treebank. Computational Linguistics,
19(2):313?330.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum of
compositionality in phrasal verbs. In
Proceedings of the ACL-2003 Workshop on
Multiword Expressions: Analysis,
Acquisition and Treatment, pages 73?80,
Sapporo.
Merlo, Paola. 2003. Generalised
PP-attachment disambiguation using
corpus-based linguistic diagnostics. In
Proceedings of the 10th Conference of the
EACL (EACL 2003), pages 251?258,
Budapest.
Merlo, Paola, Matthew W. Crocker, and
Cathy Berthouzoz. 1997. Attaching
multiple prepositional phrases:
Generalized backed-off estimation. In
Proceedings of the 2nd Conference on
Empirical Methods in Natural Language
Processing (EMNLP-97), pages 149?155,
Providence, RI.
Merlo, Paola and Eva Esteve Ferrer. 2006.
The notion of argument in prepositional
phrase attachment. Computational
Linguistics, 32(3):341?377.
Miller, George A. and Philip N.
Johnson-Laird. 1976. Language and
Perception. Cambridge University Press,
Cambridge, UK.
Mitchell, Brian. 2004. Prepositional Phrase
Attachment Using Machine Learning
Algorithms. Ph.D. thesis, University of
Sheffield.
Muischnek, Kadri, Kaili Mu?u?risep, and Tiina
Puolakainen. 2005. Adpositions in
Estonian computational syntax. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 2?10, Colchester.
Muslea, Ion. 1999. Extraction patterns for
information extraction tasks: A survey. In
Proceedings of the AAAI-99 Workshop on
Machine Learning for Information Extraction,
pages 1?6, Orlando, FL.
Nu?bel, Rita. 1996. Knowledge sources for the
disambiguation of prepositions in machine
translation. In Proceedings of the PRICAI-96
Workshop on Future Issues for Multi-lingual
Text Processing, Cairns. Available at
ftp://ftp.mpce.mq.edu.au/pub/comp/
mri/nlp/fimtp/nuebel.ps.gz.
Odijk, Jan. 2004. Reusable lexical
representations for idioms. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation (LREC
2004), pages 903?906, Lisbon.
O?Hara, Tom and Janyce Wiebe. 2003.
Preposition semantic classification via
Treebank and FrameNet. In Proceedings of
the 7th Conference on Natural Language
Learning (CoNLL-2003), pages 79?86,
Edmonton.
O?Hara, Tom and Janyce Wiebe. 2009.
Exploiting semantic role resources for
preposition disambiguation. Computational
Linguistics, 35(2):151?184.
Old, L. John. 2003. An analysis of semantic
overlap among English prepositions
from the Roget?s Thesaurus. In Proceedings
of the ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 13?19,
Toulouse.
Olteanu, Marian and Dan Moldovan. 2005.
PP-attachment disambiguation using large
context. In Proceedings of the 2005
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005),
pages 273?280, Vancouver.
Olteanu, Marian G. 2004. Prepositional
phrase attachment ambiguity resolution
through a rich syntactic, lexical and
semantic set of features applied in support
vector machines learner. Master?s thesis,
University of Texas, Dallas.
Ono, Toshihide, Haretsugu Hishigaki, Akira
Tanigami, and Toshihisa Takagi. 2001.
Automated extraction of information on
protein-protein interactions from the
biological literature. Bioinformatics,
17(2):155?161.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of the 38th
Annual Meeting of the ACL, pages 101?108,
Hong Kong.
Patrick, Jon and Jeremy Fletcher. 2005.
Classifying verb particle constructions by
verb arguments. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 200?209,
Colchester.
145
Computational Linguistics Volume 35, Number 2
Pearsall, Judy, editor. 1998. The New Oxford
Dictionary of English. Clarendon Press,
Oxford, UK.
Pecina, Pavel. 2008. A machine learning
approach to multiword expression
extraction. In Proceedings of the LREC 2008
Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008),
pages 54?57, Marrakech.
Pereira, Fernando C. N. 1985. A new
characterization of attachment preferences.
In David R. Dowty, Lauri Karttunen, and
Arnold M. Zwicky, editors, Natural
Language Parsing: Psychological,
Computational and Theoretical Perspectives.
Cambridge University Press, Cambridge,
UK, pages 307?319.
Popescu, Octavian, Sara Tonelli, and
Emanuele Pianta. 2007. IRST-BP:
Preposition disambiguation based on
chain clarifying relationships contexts. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 191?194,
Prague.
Pratt, Ian and Nissim Francez. 1997. On the
semantics of temporal prepositions and
preposition phrases. Technical Report
LCL9701, Computer Science Department,
Technion, Haifa, Israel.
Pustejovsky, J., J. Castano, R. Saur??,
A. Rumshinsky, J. Zhang, and W. Luo.
2002. Medstract: Creating large-scale
information servers for biomedical
libraries. In Proceedings of the ACL 2002
Workshop on Natural Language Processing in
the Biomedical Domain, pages 85?92,
Philadelphia, PA.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed phrasal
SMT. In Proceedings of the 43rd Annual
Meeting of the ACL, pages 271?279, Ann
Arbor, MI.
Quirk, Randolph, Sidney Greenbaum,
Geoffrey Leech, and Jan Svartvik. 1985. A
Comprehensive Grammar of the English
Language. Longman, London.
Ramsay, Allan. 2005. Prepositions as
abstract relations. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 30?38,
Colchester.
Ratnaparkhi, Adwait, Jeff Reynar, and Salim
Roukos. 1994. A maximum entropy model
for prepositional phrase attachment. In
Proceedings of the ARPA Human Language
Technology Workshop, pages 250?255,
Princeton, NJ.
Rauh, Gisa. 1993. On the grammar of lexical
and non-lexical prepositions in English. In
Zelinski-Wibbelt (Zelinski-Wibbelt 1993),
pages 99?150.
Rehbein, Ines and Josef van Genabith. 2006.
German particle verbs and pleonastic
prepositions. In Proceedings of the Third
ACL-SIGSEMWorkshop on Prepositions,
pages 57?64, Trento.
Reichelt, Thorsten and Etienne Verleih. 2005.
B3D?a system for the description and
calculation of spatial prepositions. In
Proceedings of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of
Prepositions and their Use in Computational
Linguistics Formalisms and Applications,
pages 101?109, Colchester.
Resnik, Philip and Mona Diab. 2000.
Measuring verb similarity. In Proceedings of
the 22nd Annual Meeting of the Cognitive
Science Society (CogSci 2000),
pages 399?404, Philadelphia, PA.
Resnik, Philip and Marti A. Hearst. 1993.
Structural ambiguity and conceptual
relations. In Proceedings of the Workshop on
Very Large Corpora: Academic and Industrial
Perspectives, pages 58?64, Columbus, OH.
Richards, Barry, Inge Bethke, Jaap van der
Does, and Jon Oberlander. 1989. Temporal
Representation and Inference. Academic
Press, London.
Riloff, Ellen. 1995. Little words can make a
big difference for text classification. In
Proceedings of 18th International ACM-SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR?95),
pages 130?136, Seattle, WA.
Ro?hrer, Christian. 1977. How to define
temporal conjunctions. Linguistische
Berichte Braunschweig, 51:1?11.
Sablayrolles, Pierre. 1995. The semantics of
motion. In Proceedings of the 7th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL?95),
pages 281?283, Dublin.
Sag, Ivan, Timothy Baldwin, Francis Bond,
Ann Copestake, and Dan Flickinger.
2002. Multiword expressions: A pain
in the neck for NLP. In Proceedings of the
3rd International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing-2002), pages 1?15,
Mexico City.
Saint-Dizier, Patrick. 2005. An overview of
PrepNet: abstract notions, frames and
inferential patterns. In Proceedings of the
146
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 155?169,
Colchester.
Saint-Dizier, Patrick, editor. 2006a.
Computational Linguistics Dimensions of
Syntax and Semantics of Prepositions.
Kluwer Academic, Dordrecht,
Netherlands.
Saint-Dizier, Patrick. 2006b. PrepNet: a
multilingual lexical description of
prepositions. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation (LREC 2006),
pages 877?885, Genoa.
Saint-Dizier, Patrick. 2008. Syntactic and
semantic frames in PrepNet. In Proceedings
of the 3rd International Joint Conference on
Natural Language Processing (IJCNLP-08),
pages 763?768, Hyderabad.
Saint-Dizier, Patrick and Gloria Vazquez.
2001. A compositional framework for
prepositions. In Proceedings of the Fourth
International Workshop on Computational
Semantics (IWCS-4), pages 165?179,
Tilburg.
Schulte im Walde, Sabine. 2004.
Identification, quantitative description,
and preliminary distributional analysis of
German particle verbs. In Proceedings of the
COLING Workshop on Enhancing and Using
Electronic Dictionaries, pages 85?88, Geneva.
Schuman, Jonathan and Sabine Bergler. 2006.
Postnominal prepositional phrase
attachment in proteomics. In Proceedings of
the HLT-NAACL 06 BioNLP Workshop on
Linking Natural Language Processing and
Biology, pages 82?89, New York.
Schu?tze, Carson. 1995. PP attachment and
argumenthood. In Carson T. Schu?tze,
Jennifer B. Ganger, and Kevin Broihier,
editors, Papers on Language Processing and
Acquisition, volume 26. MIT Working
Papers in Linguistics, Cambridge, MA,
pages 95?152.
Schwartz, Lee, Takako Aikawa, and Chris
Quirk. 2003. Disambiguation of English PP
attachment using multilingual aligned
data. In Proceedings of the Ninth Machine
Translation Summit (MT Summit IX), New
Orleans, LA. Available at www.amtaweb.
org/summit/MTSummit/FinalPapers/
39-Aikawa-final.pdf.
Shaked, Nava A. 1993. How do we count?
The problem of tagging phrasal verbs in
parts. In Proceedings of the 31st Annual
Meeting of the ACL, pages 289?291,
Columbus, OH.
Sharoff, Serge. 2004. What is at stake: A case
study of Russian expressions starting with
a preposition. In Proceedings of the ACL
2004 Workshop on Multiword Expressions:
Integrating Processing, pages 17?23,
Barcelona.
Sopena, Josep M., Agusti Lloberas, and
Joan L. Moliner. 1998. A connectionist
approach to prepositional phrase
attachment for real world texts. In
Proceedings of the 36th Annual Meeting of the
ACL and 17th International Conference on
Computational Linguistics
(COLING/ACL-98), pages 1233?1237,
Montreal.
Spa?rck Jones, Karen and Branimir Boguraev.
1987. A note on a study of cases.
Computational Linguistics, 13(1?2):65?8.
Spivey-Knowlton, Michael and Julie C.
Sedivy. 1995. Resolving attachment
ambiguities with multiple constraints.
Cognition, 55:227?267.
Srihari, Rohini, Cheng Niu, and Wei Li. 2000.
A hybrid approach for named entity and
sub-type tagging. In Proceedings of the 6th
Conference on Applied Natural Language
Processing (ANLP), pages 247?254, Seattle,
WA.
Stamou, Sofia, Kemal Oflazer, Karel Pala,
Dimitris Christoudoulakis, Dan Cristea,
Dan Tufi Svetla Koeva, George Totkov,
Dominique Dutoit, and Maria
Grigoriadou. 2002. BALKANET: A
multilingual semantic network for the
Balkan languages. In Proceedings of the
International Wordnet Conference,
pages 12?14, Mysore.
Stetina, Jiri and Makoto Nagao. 1997.
Corpus based PP attachment ambiguity
resolution with a semantic dictionary.
In Proceedings of the 5th Annual Workshop
on Very Large Corpora, pages 66?80,
Hong Kong.
Talmy, Len. 1985. Lexicalization patterns:
Semantic structure in lexical forms. In
Timothy Shopen, editor, Grammatical
Categories and the Lexicon. Cambridge
University Press, Cambridge, UK,
pages 57?149.
Talmy, Leonard. 1988. Force dynamics in
language and cognition. Cognitive Science,
12:49?100.
Tetreault, Joel R. and Martin Chodorow.
2008a. Native judgments of non-native
usage: Experiments in preposition error
detection. In Coling 2008: Proceedings of the
Workshop on Human Judgements in
Computational Linguistics, pages 24?32,
Manchester.
147
Computational Linguistics Volume 35, Number 2
Tetreault, Joel R. and Martin Chodorow.
2008b. The ups and downs of preposition
error detection in ESL writing. In
Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING 2008), pages 865?872,
Manchester.
Tokunaga, Takenobu, Tomofumi Koyama,
and Suguru Saito. 2005. Meaning of
Japanese spatial nouns. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 93?100,
Colchester.
Toral, Antonio. 2005. DRAMNERI: A free
knowledge based tool to named entity
recognition. In Proceedings of the First Free
Software Technologies Conference,
pages 27?32, Corunna.
Toutanova, Kristina and Christoper D.
Manning. 2000. Enriching the knowledge
sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the
Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-2000),
pages 63?70, Hong Kong.
Toutanova, Kristina, Christopher D.
Manning, and Andrew Y. Ng. 2004.
Learning random walk models for
inducing word dependency distributions.
In Proceedings of the 21st International
Conference on Machine Learning,
pages 815?822, Banff.
Toutanova, Kristina and Hisami Suzuki.
2007. Generating case markers in
machine translation. In Proceedings of
Human Language Technologies 2007: The
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 49?56, Rochester, NY.
Trawin?ski, Beata. 2003. Combinatorial
aspects of PPs headed by raising
prepositions. In Proceedings of the
ACL-SIGSEMWorkshop on the Linguistic
Dimensions of Prepositions and their Use in
Computational Linguistics Formalisms and
Applications, pages 157?168, Toulouse.
Trawin?ski, Beata. 2005. Preposition?pronoun
contraction in Polish. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 20?29,
Colchester.
Trawin?ski, Beata. 2006. A quantitative
approach to preposition-pronoun
contraction in Polish. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 17?22, Trento.
Trawinski, Beata, Manfred Sailer, and
Jan-Philipp Soehn. 2006. Combinatorial
aspects of collocational prepositional
phrases. In Saint-Dizier (Saint-Dizier
2006a).
Trujillo, Arturo. 1995. Lexicalist Machine
Translation of Spatial Prepositions. Ph.D.
thesis, University of Cambridge.
Tseng, Jesse. 2004. Prepositions and
complement selection. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 11?19,
Colchester.
Tseng, Jesse L. 2000. The Representation and
Selection of Prepositions. Ph.D. thesis,
University of Edinburgh.
Tyler, Andrea and Vyvyan Evans. 2003. The
Semantics of English Prepositions: Spatial
Scenes, Embodied Meaning, and Cognition.
Cambridge University Press, Cambridge,
UK.
van der Beek, Leonoor. 2005. The extraction
of determinerless PPs. In Proceedings of the
Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 190?199,
Colchester.
van Herwijnen, Olga, Jacques Terken, Antal
van den Bosch, and Erwin Marsi. 2003.
Learning PP attachment for filtering
prosodic phrasing. In Proceedings of the 10th
Conference of the EACL (EACL 2003),
pages 139?146, Budapest.
Villada Moiro?n, Begon?a. 2005. Data-driven
identification of fixed expressions and their
modifiability. Ph.D. thesis, Alfa-Informatica,
University of Groningen.
Villavicencio, Aline. 2005. The availability of
verb-particle constructions in lexical
resources: How much is enough? Computer
Speech and Language, 19(4):415?432.
Villavicencio, Aline. 2006. Verb-particle
constructions in the World Wide Web. In
Saint-Dizier (2006a).
Volk, Martin. 2001. Exploiting the WWW as a
corpus to resolve PP attachment
ambiguities. In Proceedings of Corpus
Linguistics 2001, pages 601?606, Lancaster.
Volk, Martin. 2002. Combining unsupervised
and supervised methods for PP
attachment disambiguation. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
pages 1?7, Taipei.
148
Baldwin, Kordoni, and Villavicencio A Survey of Prepositions in Applications
Volk, Martin. 2003. German prepositions and
their kin. A survey with respect to the
resolution of PP attachment ambiguities.
In Proceedings of the ACL-SIGSEMWorkshop
on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics
Formalisms and Applications, pages 77?85,
Toulouse.
Volk, Martin. 2006. How bad is the problem
of PP-attachment? A comparison of English,
German, and Swedish. In Proceedings of the
Third ACL-SIGSEMWorkshop on
Prepositions, pages 81?88, Trento.
Vossen, Piek, editor. 1998. EuroWordNet: A
Multilingual Database with Lexical Semantic
Networks. Kluwer Academic, Dordrecht,
Netherlands.
Wasow, Thomas. 2002. Postverbal Behavior.
CSLI Publications, Stanford, CA.
Whittemore, Greg and Kathleen Ferrara.
1990. Empirical study of predictive powers
of simple attachment schemes for
post-modifier prepositional phrases. In
Proceedings of the 28th Annual Meeting of the
ACL, pages 23?30, Pittsburgh, PA.
Wilks, Yorick, Xiuming Huang, and Dan
Fass. 1985. Syntax, preference and right
attachment. In Proceedings of the 9th
International Joint Conference on Artificial
Intelligence (IJCAI-85), pages 779?784, Los
Angeles, CA.
Wood, Frederick T. 1979. English Prepositional
Idioms. Macmillan, London.
Xu, Yilun Dianna and Norman I. Badler.
2000. Algorithms for generating motion
trajectories described by prepositions. In
Proceedings of Computer Animation 2000
(CA?00), pages 30?35, Washington, DC.
Ye, Patrick and Timothy Baldwin. 2006.
Semantic role labeling of prepositional
phrases. ACM Transactions on Asian
Language Information Processing,
5(3):228?244.
Ye, Patrick and Timothy Baldwin. 2007.
MELB-YB: Preposition sense
disambiguation using rich semantic
features. In Proceedings of the 4th
International Workshop on Semantic
Evaluations, pages 241?244, Prague.
Yeh, Alexander S. and Marc B. Vilain.
1998. Some properties of preposition
and subordinate conjunction
attachments. In Proceedings of the 36th
Annual Meeting of the ACL and 17th
International Conference on Computational
Linguistics: COLING/ACL-98,
pages 1436?1442, Montreal.
Yuret, Deniz. 2007. KU: Word sense
disambiguation by substitution. In
Proceedings of the 4th International Workshop
on Semantic Evaluations, pages 207?214,
Prague.
Zavrel, Jakub, Walter Daelemans, and
Jorn Veenstra. 1997. Resolving PP
attachment ambiguities with
memory-based learning. In Proceedings
of the Conference on Computational
Natural Language Learning (CoNLL-97),
pages 136?144, Madrid.
Zelinski-Wibbelt, Cornelia, editor. 1993.
The Semantics of Prepositions: From
Mental Processing to Natural Language
Processing. Mouton de Gruyter, Berlin,
Germany.
Zhao, Shaojun and Dekang Lin. 2004. A
nearest-neighbor method for resolving
PP-attachment ambiguity. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 545?554, Hainan Island.
149

Proceedings of NAACL HLT 2009: Short Papers, pages 69?72,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Web and Corpus Methods for Malay Count Classifier Prediction
Jeremy Nicholson and Timothy Baldwin
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{jeremymn,tim}@csse.unimelb.edu.au
Abstract
We examine the capacity of Web and corpus
frequency methods to predict preferred count
classifiers for nouns in Malay. The observed
F-score for the Web model of 0.671 consid-
erably outperformed corpus-based frequency
and machine learning models. We expect that
this is a fruitful extension for Web?as?corpus
approaches to lexicons in languages other than
English, but further research is required in
other South-East and East Asian languages.
1 Introduction
The objective of this paper is to extend a Malay
lexicon with count classifier information for nomi-
nal types. This is done under the umbrella of deep
lexical acquisition: the process of automatically or
semi-automatically learning linguistic structures for
use in linguistically rich language resources such as
precision grammars or wordnets (Baldwin, 2007).
One might call Malay a ?medium-density? lan-
guage: some NLP resources exist, but substantially
fewer than those for English, and they tend to be
of low complexity. Resources like the Web seem
promising for bootstrapping further resources, aided
in part by simple syntax and a Romanised ortho-
graphic system. The vast size of the Web has been
demonstrated to combat the data sparseness prob-
lem, for example, in Lapata and Keller (2004).
We examine using a similar ?first gloss? strategy
to Lapata and Keller (akin to ?first sense? in WSD,
in this case, identifying the most basic surface form
that a speaker would use to disambiguate between
possible classes), where the Web is used a corpus to
query a set of candidate surface forms, and the fre-
quencies are used to disambiguate the lexical prop-
erty. Due to the heterogeneity of the Web, we expect
to observe a significant amount of blocking from In-
donesian, a language with which Malay is some-
what mutually intelligible (Gordon, 2005). Hence,
we contrast this approach with observing the cues
directly from a corpus strictly of Malay, as well as a
corpus-based supervised machine learning approach
which does not rely on a presupplied gloss.
2 Background
2.1 Count Classifiers
A count classifier (CL) is a noun that occurs in a
specifier phrase with one of a set of (usually nu-
meric) specifiers; the specifier phrase typically oc-
curs in apposition or as a genitive modifier (GEN) to
the head noun. In many languages, including many
South-East Asian, East Asian, and African families,
almost all nouns are uncountable and can only be
counted through specifier phrases. A Malay exam-
ple, where biji is the count classifier (CL) for fruit, is
given in (1).
(1) tiga
three
biji
CL
pisang
banana
?three bananas?
Semantically, a lexical entry for a noun will in-
clude a default (sortal) count classifier which se-
lects for a particular semantic property of the lemma.
Usually this is a conceptual class (e.g. HUMAN or
ANIMAL) or a description of some relative dimen-
sional property (e.g. FLAT or LONG-AND-THIN).
Since each count classifier has a precise seman-
tics, using a classifier other than the default can co-
erce a given lemma into different semantics. For ex-
ample, raja ?king? typically takes orang ?person?
as a classifier, as in 2 orang raja ?2 kings?, but can
take on an animal reading with ekor ?animal? in 2
ekor raja ?2 kingfishers?. An unintended classifier
69
can lead to highly marked or infelicitious readings,
such as #2 biji raja ?2 (chess) kings?.
Most research on count classifiers tends to discuss
generating a hierarchy or taxonomy of the classi-
fiers available in a given language (e.g. Bond and
Paik (1997) for Japanese and Korean, or Shirai et
al. (2008) cross-linguistically) or using language-
specific knowledge to predict tokens (e.g. Bond and
Paik (2000)) or both (e.g. Sornlertlamvanich et al
(1994)).
2.2 Malay Data
Little work has been done on NLP for Malay, how-
ever, a stemmer (Adriani et al, 2007) and a prob-
abilistic parser for Indonesian (Gusmita and Manu-
rung, 2008) have been developed. The mutually in-
telligibility suggests that Malay resources could pre-
sumably be extended from these.
In our experiments, we make use of a Malay?
English translation dictionary, KAMI (Quah et al,
2001), which annotates about 19K nominal lexical
entries for count classifiers. To limit very low fre-
quency entries, we cross-reference these with a cor-
pus of 1.2M tokens of Malay text, described in Bald-
win and Awab (2006). We further exclude the two
non-sortal count classifiers that are attested as de-
fault classifiers in the lexicon, as their distribution is
heavily skewed and not lexicalised.
In all, 2764 simplex common nouns are attested
at least once in the corpus data. We observe 2984
unique noun?to?default classifier assignments. Pol-
ysemy leads to an average of 1.08 count classifiers
assigned to a given wordform. The most difficult
exemplars to classify, and consequently the most in-
teresting ones, correspond to the dispreferred count
classifiers of the multi-class wordforms: direct as-
signment and frequency thresholding was observed
to perform poorly. Since this task is functionally
equivalent to the subcat learning problem, strategies
from that field might prove helpful (e.g. Korhonen
(2002)).
The final distribution of the most frequent classes
is as follows:
CL: orang buah batang ekor OTHER
Freq: 0.389 0.292 0.092 0.078 0.149
Of the 49 classes, only four have a relative frequency
greater than 3% of the types: orang for people,
batang for long, thin objects, ekor for animals, and
buah, the semantically empty classifier, for when no
other classifiers are suitable (e.g. for abstract nouns);
orang and buah account for almost 70% of the types.
3 Experiment
3.1 Methodology
Lapata and Keller (2004) look at a set of generation
and analysis tasks in English, identify simple surface
cues, and query a Web search engine to approximate
those frequencies. They then use maximum likeli-
hood estimation or a variety of normalisation meth-
ods to choose an output.
For a given Malay noun, we attempt to select the
default count classifier, which is a generation task
under their framework, and semantically most simi-
lar to noun countability detection. Specifier phrases
almost always premodify nouns in Malay, so the set
of surface cues we chose was satu CL NOUN ?one/a
NOUN?.1 This was observed to have greater cov-
erage than dua ?two? and other non-numeral spec-
ifiers. 49 queries were performed for each head-
word, and maximum likelihood estimation was used
to select the predicted classifier (i.e. taking most fre-
quently observed cue, with a threshold of 0). Fre-
quencies from the same cues were also obtained
from the corpus of Baldwin and Awab (2006).
We contrasted this with a machine learning model
for Malay classifiers, designed to be language-
independent (Nicholson and Baldwin, 2008). A fea-
ture vector is constructed for each headword by con-
catenating context windows of four tokens to the left
and right of each instance of the headword in the cor-
pus (for eight word unigram features per instance).
These are then passed into two kinds of maximum
entropy model: one conditioned on all 49 classes,
and one cascaded into a suite of 49 separate binary
classifiers designed to predict each class separately.
Evaluation is via 10-fold stratified cross-validation.
A majority class baseline was also examined, where
every headword was assigned the orang class.
For the corpus-based methods, if the frequency of
every cue is 0, no prediction of classifier is made.
Similarly, the suite can predict a negative assign-
1satu becomes cliticised to se- in this construction, so that
instead of cues like satu buah raja, satu orang raja, ..., we have
cues like sebuah raja, seorang raja, ....
70
Method Web Corpus Suite Entire Base
Prec. .736 .908 .652 .570 .420
Rec. .616 .119 .379 .548 .389
F? = 1 .671 .210 .479 .559 .404
Table 1: Performance of the five systems.
Back-off Web Suite Entire orang buah
Prec. .736 .671 .586 .476 .389
Rec. .616 .421 .561 .441 .360
F? = 1 .671 .517 .573 .458 .374
Table 2: Performance of corpus frequency assignment
(Corpus in Table 1), backed-off to the other systems.
ment for each of the 49 classes. Consequently, pre-
cision is calculated as the fraction of correctly pre-
dicted instances to the number of examplars where
a prediction was made. Only the suite of classifiers
could natively handle multi-assignment of classes:
recall was calculated as the fraction of correctly pre-
dicted instances to all 2984 possible headword?class
assignments, despite the fact that four of the systems
could not make 220 of the classifications.
3.2 Results
The observed precision, recall, and F-scores of the
various systems are shown in Table 1. The best
F-score is observed for the Web frequency system,
which also had the highest recall. The best precision
was observed for the corpus frequency system, but
with very low recall ? about 85% of the wordforms
could not be assigned to a class (the corresponding
figure for the Web system was about 9%). Conse-
quently, we attempted a number of back-off strate-
gies so as to improve the recall of this system.
The results for backing off the corpus frequency
system to the Web model, the two maximum entropy
models, and two baselines (the majority class, and
the semantically empty classifier) are shown in Ta-
ble 2. Using a Web back-off was nearly identical to
the basic Web system: most of the correct assign-
ments being made by the corpus frequency system
were also being captured through Web frequencies,
which indicates that these are the easier, high fre-
quency entries. Backing off to the machine learn-
ing models performed the same or slightly better
than using the machine learning model by itself. It
therefore seems that the most balanced corpus-based
model should take this approach.
The fact that the Web frequency system had the
best performance belies the ?noisiness? of the Web,
in that one expects to observe errors caused by
carelessness, laziness (e.g. using buah despite a
more specific classifier being available), or noise
(e.g. Indonesian count classifier attestation; more on
this below). While the corpus of ?clean?, hand-
constructed data did have a precision improvement
over the Web system, the back-off demonstrates that
it was not substantially better over those entries that
could be classified from the corpus data.
4 Discussion
As with many classification tasks, the Web-based
model notably outperformed the corpus-based mod-
els when used to predict count classifiers of Malay
noun types, particularly in recall. In a type-wise lex-
icon, precision is probably the more salient evalua-
tion metric, as recall is more meaningful on tokens,
and a low-precision lexicon is often of little utility;
the Web system had at least comparable precision
for the entries able to be classified by the corpus-
based systems.
We expected that the heterogeneity of the Web,
particularly confusion caused by a preponderance of
Indonesian, would cause performance to drop, but
this was not the case. The Ethnologue estimates that
there are more speakers of Indonesian than Malay
(Gordon, 2005), and one would expect the Web dis-
tribution to reflect this. Also, there are systematic
differences in the way count classifiers are used in
the two languages, despite the intelligibility; com-
pare ?five photographs?: lima keping foto in Malay
and lima lembar foto, lima foto in Indonesian.
While the use of count classifiers is obligatory in
Malay, it is optional in Indonesian for lower reg-
isters. Also, many classifiers that are available in
Malay are not used in Indonesian, and the small set
of Indonesian count classifiers that are not used in
Malay do not form part of the query set, so no confu-
sion results. Consequently, it seems that greater dif-
ficulty would arise when attempting to predict count
classifiers for Indonesian nouns, as their optional-
ity and blocking from Malay cognates would intro-
duce noise in cases where language identification
has not been used to generate the corpus (like the
71
Web) ? hand-constructed corpora might be neces-
sary in that case. Furthermore, the Web system ben-
efits from a very simple surface form, namely se-
CL NOUN: languages that permit floating quantifica-
tion, like Japanese, or require classifiers for stative
verb modification, like Thai, would need many more
queries or lower-precision queries to capture most of
the cues available from the corpus. We intend to ex-
amine these phenomena in future work.
An important contrast is noted between the ?un-
supervised? methods of the corpus-frequency sys-
tems and the ?supervised? machine learning meth-
ods. One presumed advantage of unsupervised sys-
tems is the lack of pre-annotated training data re-
quired. In this case, a comparable time investment
by a lexicographer would be required to generate the
set of surface forms for the corpus-frequency mod-
els. The performance dictates that the glosses for the
Web system give the most value for lexicographer
input; however, for other languages or other lexical
properties, generating a set of high-precision, high-
recall glosses is often non-trivial. If the Web is not
used, having both training data and high-precision,
low-recall glosses is valuable.
5 Conclusion
We examine an approach for using Web and cor-
pus data to predict the preferred generation form for
counting nouns in Malay, and observed greater pre-
cision than machine learning methods that do not
require a presupplied gloss. Most Web?as?corpus
research tends to focus on English; as the Web in-
creases in multilinguality, it becomes an important
resource for medium- and low-density languages.
This task was quite simple, with glosses amenable to
Web approaches, and is promising for automatically
extending the coverage of a Malay lexicon. How-
ever, we expect that the Malay glosses will block
readings of Indonesian classifiers, and classifiers in
other languages will require different strategies; we
intend to examine this in future work.
Acknowledgements
We would like to thank Francis Bond for his valuable in-
put on this research. NICTA is funded by the Australian
government as represented by Department of Broadband,
Communication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excellence
programme.
References
M. Adriani, J. Asian, B. Nazief, S.M.M. Tahaghoghi,
and H.E. Williams. 2007. Stemming Indonesian:
A confix-stripping approach. ACM Transactions on
Asian Language Information Processing, 6.
T. Baldwin and S. Awab. 2006. Open source corpus anal-
ysis tools for Malay. In Proc. of the 5th International
Conference on Language Resources and Evaluation,
pages 2212?5, Genoa, Italy.
T. Baldwin. 2007. Scalable deep linguistic processing:
Mind the lexical gap. In Proc. of the 21st Pacific Asia
Conference on Language, Information and Computa-
tion, pages 3?12, Seoul, Korea.
F. Bond and K. Paik. 1997. Classifying correspondence
in Japanese and Korean. In Proc. of the 3rd Confer-
ence of the Pacific Association for Computational Lin-
guistics, pages 58?67, Tokyo, Japan.
F. Bond and K. Paik. 2000. Reusing an ontology to
generate numeral classifiers. In Proc. of the 19th In-
ternational Conference on Computational Linguistics,
pages 90?96, Saarbru?cken, Germany.
R.G. Gordon, Jr, editor. 2005. Ethnologue: Languages
of the World, Fifteenth Edition. SIL International.
R.H. Gusmita and Ruli Manurung. 2008. Some initial
experiments with Indonesian probabilistic parsing. In
Proc. of the 2nd International MALINDO Workshop,
Cyberjaya, Malaysia.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, Cambridge,
UK.
M. Lapata and F. Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised
web-based models for a range of NLP tasks. In Proc.
of the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meeting
of the NAACL, pages 121?128, Boston, USA.
J. Nicholson and T. Baldwin. 2008. Learning count
classifier preferences of Malay nouns. In Proc. of the
Australasian Language Technology Association Work-
shop, pages 115?123, Hobart, Australia.
C.K. Quah, F. Bond, and T. Yamazaki. 2001. De-
sign and construction of a machine-tractable Malay-
English lexicon. In Proc. of the 2nd Biennial Confer-
ence of ASIALEX, pages 200?205, Seoul, Korea.
K. Shirai, T. Tokunaga, C-R. Huang, S-K. Hsieh, T-
Y. Kuo, V. Sornlertlamvanich, and T. Charoenporn.
2008. Constructing taxonomy of numerative classi-
fiers for Asian languages. In Proc. of the Third Inter-
national Joint Conference on Natural Language Pro-
cessing, Hyderabad, India.
V. Sornlertlamvanich, W. Pantachat, and S. Meknavin.
1994. Classifier assignment by corpus-based ap-
proach. In Proc. of the 15th International Conference
on Computational Linguistics, pages 556?561, Kyoto,
Japan.
72
Proceedings of NAACL HLT 2009: Short Papers, pages 257?260,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Recognising the Predicate?argument Structure of Tagalog
Meladel Mistica
Australian National University ? Linguistics
The University of Melbourne ? CSSE
The University of Sydney ? Linguistics
mmistica@csse.unimelb.edu.au
Timothy Baldwin
CSSE
The University of Melbourne
tim@csse.unimelb.edu.au
Abstract
This paper describes research on parsing
Tagalog text for predicate?argument structure
(PAS). We first outline the linguistic phe-
nomenon and corpus annotation process, then
detail a series of PAS parsing experiments.
1 Introduction
Predicate?argument structure (PAS) has been
shown to be highly valuable in tasks such as infor-
mation extraction (Surdeanu et al, 2003; Miyao et
al., 2009). In this research, we develop a resource for
analysing the predicate?argument structure of Taga-
log, a free word order language native to the Philip-
pines, and carry out preliminary empirical investiga-
tion of PAS parsing methods over Tagalog.
The motivation for this research is the investiga-
tion of the interaction between information structure
and word order in Tagalog. That is, we wish to de-
termine the utility of discourse-based contextual in-
formation in predicting word order in Tagalog, in a
natural language generation context. We see PAS as
the natural representation for this exploration. This
research clearly has implications beyond our imme-
diate interests, however, in terms of resource cre-
ation for an NLP resource-poor language, and the
facilitation of research on parsing and parsing-based
applications in Tagalog. It is also one of the first in-
stances of research on PAS parsing over a genuinely
free word order language.
2 Background
Tagalog is an Austronesian language of the Malayo-
Polynesian branch, which forms the basis of the na-
tional language of the Philippines, Filipino (a.k.a.
Pilipino) (Gordon, 2005). It is a verb-initial lan-
guage, with relatively free word order of verbal
arguments (Kroeger, 1993), as exemplified in the
word-order variants provided with (1). There are
no discernible meaning differences between the pro-
vided variants, but there are various soft constraints
on free word order, as discussed by Kroeger (1993)
and Sells (2000).
(1) Nagbigay
gave
ng
GEN
libro
book
sa
DAT
babae
woman
ang
NOM
lalaki
man
?The man gave the woman a book?
Nagbigay ng libro ang lalaki sa babae
Nagbigay sa babae ng libro ang lalaki
Nagbigay sa babae ang lalaki ng libro
Nagbigay ang lalaki sa babae ng libro
Nagbigay ang lalaki ng librosa babae
In addition to these free word order possibilities,
Tagalog exhibits voice marking, a morpho-syntactic
phenomenon which is common in Austronesian lan-
guages and gives prominence to an element in a sen-
tence (Schachter and Otanes, 1972; Kroeger, 1993).
This poses considerable challenges to generation,
because of the combinatorial explosion in the pos-
sible ways of expressing what is seemingly the same
proposition. Below, we provide a brief introduction
to Tagalog syntax, with particular attention to voice
marking.
2.1 Constituency
There are three case markers in Tagalog: ang, ng
and sa, which are by convention written as separate
preposing words, as in (1). These markers normally
prepose phrasal arguments of a given verb.
The sa marker is predominantly used for goals,
recipients, locations and definite objects, while ng
marks possessors, actors, instruments and indefinite
objects (Kroeger, 1993). Ang is best explained in
terms of Tagalog?s voice-marking system.
257
2.2 Tagalog Voice Marking
Tagalog has rich verbal morphology which gives
prominence to a particular dependent via voice
marking (Schachter and Otanes, 1972); this special
dependent in the sentence is the ang-marked argu-
ment.
There are 5 major voice types in Tagalog: Ac-
tor Voice (AV); Patient/Object Voice (OV); Da-
tive/Locative Voice (DV); Instrumental Voice (IV);
and Benefactive Voice (BV) (Kroeger, 1993). This
voice marking, manifested on the verb, reflects the
semantic role of the ang-marked constituent, as seen
in the sentences below from Kroeger (1993), illus-
trating the 3 voice types of AV, OV, and BV.
(2) Actor Voice (AV)
Bumili
buy
ang
NOM
lalake
man
ng
GEN
isda
fish
sa
DAT
tindahan
store
?The man bought fish at the store?
(3) Object Voice (OV)
Binili
buy
ng
GEN
lalake
man
ang
NOM
isda
fish
sa
DAT
tindahan.
store
?The man bought fish at the store?
(4) Benefactive Voice (BV)
Ibinili
buy
ng
GEN
lalake
man
ng
GEN
isda
fish
ang
NOM
bata.
child
?The man bought fish for the child?
In each case, the morphological marking on the verb
(which indicates the voice type) is presented in bold,
along with the focused ang argument.
In addition to displaying free word order, there-
fore, Tagalog presents the further choice of which
voice to encode the proposition with.
3 Data and Resources
For this research, we annotated our own corpus of
Tagalog text for PAS. This is the first such resource
to be created for the Tagalog language. To date,
we have marked up two chapters (about 2500 to-
kens) from a narrative obtained from the Guten-
berg Project1 called Hiwaga ng Pagibig (?The Mys-
tery of Love?); we intend to expand the amount of
1http://www.gutenberg.org/etext/
18955
annotated data in the future. The annotated data
is available from www.csse.unimelb.edu.au/
research/lt/resources/tagalog-pas.
3.1 Part-of-speech Mark-up
First, we developed a set of 5 high-level part-of-
speech (POS) tags for the task, with an additional
tag for sundries such as punctuation. The tags are as
follows:
Description Example(s)
proper name names of people/cities
pronoun personal pronouns
open-class word nouns, verbs, adjectives
closed-class word conjunctions
function word case markers
other punctuation
These tags are aimed at assisting the identification
of constituent boundaries, focusing primarily on dif-
ferentiating words that have semantic content from
those that perform a grammatical function, with the
idea that function words, such as case markers, gen-
erally mark the start of an argument, while open-
class words generally occur within a predicate or ar-
gument. Closed-class words, on the other hand (e.g.
sentence conjuncts) tend not to be found inside pred-
icates and arguments.
The advantage of having a coarse-grained set of
tags is that there is less margin for error and dis-
agreement on how a word can be tagged. For future
work, we would like to compare a finer-grained set
of tags, such as that employed by dela Vega et al
(2002), with our tags to see if a more detailed dis-
tinction results in significant benefits.
In Section 4, we investigate the impact of the in-
clusion of this extra annotation on PAS recogni-
tion, to gauge whether the annotation effort was war-
ranted.
3.2 Predicate and Argument Mark-up
Next, we marked up predicates and their (core) argu-
ments, employing the standard IOB tag scheme. We
mark up two types of predicates: PRD and PRD-SUB.
The former refers to predicates that belong to main
clauses, whilst the latter refers to predicates that oc-
cur in subordinate or dependent clauses.
We mark up 4 types of arguments: ANG, NG,
SA and NG-COMP. The first three mark nominal
258
phrases, while the last marks sentential comple-
ments (e.g. the object of quotative verbs).
We follow the multi-column format used in
the CoNLL 2004 semantic role labelling (SRL)
task (Carreras and Ma`rquez, 2004), with as many
columns as there are predicates in a sentence, and
one predicate and its associated arguments per col-
umn.
3.3 Annotation
Our corpus consists of 259 predicates (47 of which
are subordinate, i.e. PRD-SUB), and 435 arguments.
The following is a breakdown of the arguments:
Argument type: SA ANG NG NG-CMP
Count: 83 193 147 12
3.4 Morphological Processing
In tandem with the corpus annotation, we developed
a finite-state morphological analyser using XFST and
LEXC (Beesley and Karttunen, 2003), that extracts
morphological features for individual words in the
form of a binary feature vector.2 While LEXC is or-
dinarily used to define a lexicon of word stems, we
opted instead to list permissible syllables, based on
the work of French (1988). This decision was based
purely on resource availability: we did not have an
extensive list of stems in Tagalog, or the means to
generate such a list.
4 Experiments
In this section, we report on preliminary results for
PAS recognition over our annotated data. The ap-
proach we adopt is similar to the conventional ap-
proach adopted in CoNLL-style semantic role la-
belling: a two-phase approach of first identifying the
predicates, then identifying arguments and attaching
them to predicates, in a pipeline architecture. Pri-
mary areas of investigation in our experiments are:
(1) the impact of POS tags on predicate prediction;
and (2) the impact of morphological processing on
overall performance.
In addition to experimenting with the finite state
morphological processing (see Section 3.4), we ex-
periment with a character n-gram method, where we
simply take the first and last n characters of a word
2Thanks to Steven Bird for help with infixation and defining
permissible syllables for the morphological analyser
as features. In our experiments, we set n to 3 and 2
characters for prefix and suffixes, respectively.
We treat each step in the pipeline as a structured
learning task, which we model with conditional ran-
dom fields (Lafferty et al, 2001) using CRF++.3
All of the results were arrived at via leave-one-out
cross-validation, defined at the sentence level, and
the evaluation was carried out in terms of precision
(P), recall (R) and F-score (F) using the evaluation
software from the CoNLL 2004 SRL task.
4.1 Predicate identification
First, we attempt to identify the predicate(s) in a
given sentence. Here, we experiment with word
context windows of varying width (1?6 words),
and also POS features in the given context win-
dow. Three different strategies are used to derive
the POS tags: (1) from CRF++, with a word bi-
gram context window of width 3 (AUTO1); (2) again
from CRF++, with a word bigram context window
of width 1 (AUTO2); and (3) from gold-standard
POS tags, sourced from the corpus (GOLD). AUTO1
and AUTO2 were the two best-performing POS tag-
ging methods amongst a selection of configurations
tested, both achieving a word accuracy of 0.914.
We compare these three POS tagging options with
a method which uses no POS tag information (NO
POS). The results for the different POS taggers with
each word context width size are presented in Ta-
ble 1.
Our results indicate that the optimal window size
for the predicate identification is 5 words. We also
see that POS contributes to the task, and that the rel-
ative difference between the gold-standard POS tags
and the best of the automatic POS taggers (AUTO2)
is small. Of the two POS taggers, the best per-
formance for AUTO2 is clearly superior to that for
AUTO1.
4.2 Argument Identification and Attachment
We next turn to argument identification and attach-
ment, i.e. determining the word extent of arguments
which attach to each predicate identified in the first
step of the pipeline. Here, we build three predicate
recognisers from Section 4.1: NO POS, AUTO2 and
3http://sourceforge.net/projects/
crfpp/
259
Window NO POS AUTO1 AUTO2 GOLD
size P R F P R F P R F P R F
1 .255 .086 .129 .406 .140 .208 .421 .143 .214 .426 .144 .215
2 .436 .158 .232 .487 .272 .349 .487 .262 .340 .529 .325 .403
3 .500 .190 .275 .477 .255 .332 .500 .262 .344 .571 .335 .422
4 .478 .190 .272 .509 .290 .370 .542 .280 .369 .523 .325 .401
5 .491 .204 .278 .494 .274 .351 .558 .349 .429 .571 .360 .442
6 .478 .190 .272 .484 .269 .346 .490 .262 .341 .547 .338 .418
Table 1: Results for predicate identification (best score in each column in bold)
Morphological NO POS AUTO2 GOLD
analysis P R F P R F P R F
FINITE STATE .362 .137 .199 .407 .201 .269 .420 .207 .278
CHAR n-GRAMS .624 .298 .404 .643 .357 .459 .623 .377 .470
COMBINED .620 .307 .410 .599 .362 .451 .623 .386 .477
Table 2: Results for argument identification and attachment (best score in each column in bold)
GOLD, all based on a window size of 5. We com-
bine these with morphological features from: (1) the
finite-state morphological analyser, (2) character n-
grams, and (3) the combination of the two. The re-
sults of the different combinations are shown in Ta-
ble 2, all based on a word context window of 3, as
this was found to be superior for the task in all cases.
The results with character n-grams were in all
cases superior to those for the morphological anal-
yser, although slight gains were seen when the two
were combined in most cases (most notably in re-
call). There was surprisingly little difference be-
tween the GOLD results (using gold-standard POS
tags) and the AUTO2 results.
5 Conclusion
In this paper, we have presented a system that recog-
nises PAS in Tagalog text. As part of this, we cre-
ated the first corpus of PAS for Tagalog, and pro-
duced preliminary results for predicate identification
and argument identification and attachment.
In future work, we would like to experiment with
larger datasets, include semantic features, and trial
other learners amenable to structured learning tasks.
References
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications, Stanford, USA.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. of CoNLL-2004, pages 89?97, Boston,
USA.
Ester D. dela Vega, Melvin Co, and Rowena Cristina
Guevara. 2002. Language model for predicting parts
of speech of Filipino sentences. In Proceedings of the
3rd National ECE Conference.
Koleen Matsuda French. 1988. Insights into Tagalog.
Summer Institute of Linguistics, Dallas, USA.
Raymond Gordon, Jr. 2005. Ethnologue: Languages
of the World. SIL International, Dallas, USA, 15th
edition.
Paul Kroeger. 1993. Phrase Structure and Grammati-
cal Relations in Tagalog. CSLI Publications, Stanford,
USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML 2001, pages 282?289, Williamstown, USA.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein?protein
interaction extraction. Bioinformatics, 25(3):394?400.
Paul Schachter and Fe T. Otanes. 1972. Tagalog Refer-
ence Grammar. University of California Press, Berke-
ley.
Peter Sells. 2000. Raising and the order of clausal
constituents in the Philippine languages. In Ileana
Paul, Vivianne Phillips, and Lisa Travis, editors, For-
mal Issues in Austronesian Linguistics, pages 117?
143. Kluwer Academic Publishers, Dordrecht, Ger-
many.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proc. of ACL 2003,
pages 8?15, Sapporo, Japan.
260
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 491?498,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interpreting Semantic Relations in Noun Compounds via Verb Semantics
Su Nam Kim? and Timothy Baldwin??
? Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
and
? NICTA Victoria Research Lab
University of Melbourne, Victoria 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
We propose a novel method for automat-
ically interpreting compound nouns based
on a predefined set of semantic relations.
First we map verb tokens in sentential con-
texts to a fixed set of seed verbs using
WordNet::Similarity and Moby?s
Thesaurus. We then match the sentences
with semantic relations based on the se-
mantics of the seed verbs and grammatical
roles of the head noun and modifier. Based
on the semantics of the matched sentences,
we then build a classifier using TiMBL.
The performance of our final system at in-
terpreting NCs is 52.6%.
1 Introduction
The interpretation of noun compounds (hereafter,
NCs) such as apple pie or family car is a well-
established sub-task of language understanding.
Conventionally, the NC interpretation task is de-
fined in terms of unearthing the underspecified se-
mantic relation between the head noun and modi-
fier(s), e.g. pie and apple respectively in the case
of apple pie.
NC interpretation has been studied in the con-
text of applications including question-answering
and machine translation (Moldovan et al, 2004;
Cao and Li, 2002; Baldwin and Tanaka, 2004;
Lauer, 1995). Recent work on the automatic/semi-
automatic interpretation of NCs (e.g., Lapata
(2002), Rosario and Marti (2001), Moldovan et al
(2004) and Kim and Baldwin (2005)) has made as-
sumptions about the scope of semantic relations or
restricted the domain of interpretation. This makes
it difficult to gauge the general-purpose utility of
the different methods. Our method avoids any
such assumptions while outperforming previous
methods.
In seminal work on NC interpretation, Finin
(1980) tried to interpret NCs based on hand-coded
rules. Vanderwende (1994) attempted the auto-
matic interpretation of NCs using hand-written
rules, with the obvious cost of manual interven-
tion. Fan et al (2003) estimated the knowledge
required to interpret NCs and claimed that perfor-
mance was closely tied to the volume of data ac-
quired.
In more recent work, Barker and Szpakowicz
(1998) used a semi-automatic method for NC in-
terpretation in a fixed domain. Lapata (2002)
developed a fully automatic method but focused
on nominalizations, a proper subclass of NCs.1
Rosario and Marti (2001) classified the nouns in
medical texts by tagging hierarchical information
using neural networks. Moldovan et al (2004)
used the word senses of nouns based on the do-
main or range of interpretation of an NC, leading
to questions of scalability and portability to novel
domains/NC types. Kim and Baldwin (2005) pro-
posed a simplistic general-purpose method based
on the lexical similarity of unseen NCs with train-
ing instances.
The aim of this paper is to develop an automatic
method for interpreting NCs based on semantic re-
lations. We interpret semantic relations relative to
a fixed set of constructions involving the modifier
and head noun and a set of seed verbs for each
semantic relation: e.g. (the) family owns (a) car
is taken as evidence for family car being an in-
stance of the POSSESSOR relation. We then at-
tempt to map all instances of the modifier and head
noun as the heads of NPs in a transitive senten-
tial context onto our set of constructions via lex-
ical similarity over the verb, to arrive at an inter-
pretation: e.g. we would hope to predict that pos-
sess is sufficiently similar to own that (the) family
possesses (a) car would be recognised as support-
1With nominalizations, the head noun is deverbal, and in
the case of Lapata (2002), nominalisations are assumed to
be interpretable as the modifier being either the subject (e.g.
child behavior) or object (e.g. car lover) of the base verb of
the head noun.
491
ing evidence for the POSSESSOR relation. We use
a supervised classifier to combine together the evi-
dence contributed by individual sentential contexts
of a given modifier?head noun combination, and
arrive at a final interpretation for a given NC.
Mapping the actual verbs in sentences to ap-
propriate seed verbs is obviously crucial to the
success of our method. This is particularly im-
portant as there is no guarantee that we will find
large numbers of modifier?head noun pairings in
the sorts of sentential contexts required by our
method, nor that we will find attested instances
based on the seed verbs. Thus an error in map-
ping an attested verb to the seed verbs could result
in a wrong interpretation or no classification at all.
In this paper, we experiment with the use of Word-
Net (Fellbaum, 1998) and word clusters (based on
Moby?s Thesaurus) in mapping attested verbs to
the seed verbs. We also make use of CoreLex in
dealing with the semantic relation TIME and the
RASP parser (Briscoe and Carroll, 2002) to de-
termine the dependency structure of corpus data.
The data source for our set of NCs is binary
NCs (i.e. NCs with a single modifier) from the
Wall Street Journal component of the Penn Tree-
bank. We deliberately choose to ignore NCs with
multiple modifiers on the grounds that: (a) 88.4%
of NC types in the Wall Street Journal component
of the Penn Treebank and 90.6% of NC types in
the British National Corpus are binary; and (b) we
expect to be able to interpret NCs with multiple
modifiers by decomposing them into binary NCs.
Another simplifying assumption we make is to re-
move NCs incorporating proper nouns since: (a)
the lexical resources we employ in this research
do not contain them in large numbers; and (b)
there is some doubt as to whether the set of seman-
tic relations required to interpret NCs incorporat-
ing proper nouns is that same as that for common
nouns.
The paper is structured as follows. Section 2
takes a brief look at the semantics of NCs and the
basic idea behind the work. Section 3 details the
set of NC semantic relations that is used in our
research, Section 4 presents an extended discus-
sion of our approach, Section 5 briefly explains the
tools we use, Section 6.1 describes how we gather
and process the data, Section 6.2 explains how we
map the verbs to seed verbs, and Section 7 and
Section 8 present the results and analysis of our
approach. Finally we conclude our work in Sec-
tion 9.
2 Motivation
The semantic relation in NCs is the relation be-
tween the head noun (denoted ?H?) and the mod-
ifier(s) (denoted ?M?). We can find this semantic
relation expressed in certain sentential construc-
tions involving the head noun and modifier.
(1) family car
CASE: family owns the car.
FORM: H own M
RELATION: POSSESSOR
(2) student protest
CASE: protest is performed by student.
FORM: M is performed by H
RELATION: AGENT
In the examples above, the semantic relation
(e.g. POSSESSOR) provides an interpretation of
how the head noun and modifiers relate to each
other, and the seed verb (e.g. own) provides a para-
phrase evidencing that relation. For example, in
the case of family car, the family is the POSSES-
SOR of the car, and in student protest, student(s)
are the AGENT of the protest. Note that voice is im-
portant in aligning sentential contexts with seman-
tic relations. For instance, family car can be repre-
sented as car is owned by family (passive) and stu-
dent protest as student performs protest (active).
The exact nature of the sentential context varies
with different synonyms of the seed verbs.
(3) family car
CASE: Synonym=have/possess/belong to
FORM: H own M
RELATION: POSSESSOR
(4) student protest
CASE: Synonym=act/execute/do
FORM: M is performed by H
RELATION: AGENT
The verb own in the POSSESSOR relation has
the synonyms have, possess and belong to. In
the context of have and possess, the form of re-
lation would be same as the form with verb, own.
However, in the context of belong to, family car
492
would mean that the car belongs to family. Thus,
even when the voice of the verb is the same
(voice=active), the grammatical role of the head
noun and modifier can change.
In our approach we map the actual verbs in sen-
tences containing the head noun and modifiers to
seed verbs corresponding to the relation forms.
The set of seed verbs contains verbs representa-
tive of each semantic relation form. We chose two
sets of seed verbs of size 57 and 84, to examine
how the coverage of actual verbs by seed verbs af-
fects the performance of our method. Initially, we
manually chose a set of 60 seed verbs. We then
added synonyms from Moby?s thesaurus for some
of the 60 verbs. Finally, we filtered verbs from the
two expanded sets, since these verbs occur very
frequently in the corpus (as this might skew the
results). The set of seed verbs {have, own, pos-
sess, belong to} are in the set of 57 seed verbs,
and {acquire, grab, occupy} are added to the set
of 84 seed verbs; all correspond to the POSSES-
SOR relation.
For each relation, we generate a set of con-
structional templates associating a subset of seed
verbs with appropriate grammatical relations for
the head noun and modifier. Examples for POS-
SESSOR are:
S({have, own, possess}V,M SUBJ,H OBJ) (5)
S({belong to}V,H SUBJ,M OBJ) (6)
where V is the set of seed verbs, M is the modifier
and H is the head noun.
Two relations which do not map readily onto
seed verbs are TIME (e.g. winter semester) and
EQUATIVE (e.g. composer arranger). Here, we
rely on an independent set of contextual evidence,
as outlined in Section 6.1.
Through matching actual verbs attested in cor-
pus data onto seed verbs, we can match sentences
with relations (see Section 6.2). Using this method
we can identify the matching relation forms of se-
mantic relations to decide the semantic relation for
NCs.
3 Semantic Relations in Compound
Nouns
While there has been wide recognition of the need
for a system of semantic relations with which to
classify NCs, there is still active debate as to what
the composition of that set should be, or indeed
RASP parser
Raw Sentences
Modified Sentences
Final Sentences
Classifier
Semantic Relation
Pre?processing
Collect Subj, Obj, PP, PPN, V, T
Filter sentences
Get sentences with H,M
Verb?Mapping
map verbs onto seed verbs
Match modified sentences
wrt relation forms
Moby?s Thesaurus
WordNet::Similarity
Classifier:Timbl
Noun Compound
Figure 1: System Architecture
whether it is reasonable to expect that all NCs
should be interpretable with a fixed set of semantic
relations.
Based on the pioneering work on Levi (1979)
and Finin (1980), there have been efforts in com-
putational linguistics to arrive at largely task-
specific sets of semantic relations, driven by the
annotation of a representative sample of NCs from
a given corpus type (Vanderwende, 1994; Barker
and Szpakowicz, 1998; Rosario and Marti, 2001;
Moldovan et al, 2004). In this paper, we use the
set of 20 semantic relations defined by Barker and
Szpakowicz (1998), rather than defining a new set
of relations. The main reasons we chose this set
are: (a) that it clearly distinguishes between the
head noun and modifiers, and (b) there is clear
documentation of each relation, which is vital for
NC annotation effort. The one change we make
to the original set of 20 semantic relations is to ex-
clude the PROPERTY relation since it is too general
and a more general form of several other relations
including MATERIAL (e.g. apple pie).
4 Method
Figure 1 outlines the system architecture of our
approach. We used three corpora: the Brown
corpus (as contained in the Penn Treebank), the
Wall Street Journal corpus (also taken from the
Penn treebank), and the written component of the
British National Corpus (BNC). We first parsed
each of these corpora using RASP (Briscoe and
Carroll, 2002), and identified for each verb to-
ken the voice, head nouns of the subject and
object, and also, for each PP attached to that
verb, the head preposition and head noun of the
493
NP (hereafter, PPN). Next, for our test NCs, we
identified all verbs for which the modifier and
head noun co-occur as subject, object, or PPN.
We then mapped these verbs to seed verbs us-
ing WordNet::Similarity and Moby?s The-
saurus (see Section 5 for details). Finally, we iden-
tified the corresponding relation for each seed verb
and selected the best-fitting semantic relation us-
ing a classifier. To evaluate our approach, we built
a classifier using TiMBL (Daelemans et al, 2004).
5 Resources
In this section, we outline the tools and resources
employed in our method.
As our parser, we used RASP, generating a
dependency representation for the most probable
parse for each sentence. Note that RASP also lem-
matises all words in a POS-sensitive manner.
To map actual verbs onto seed verbs,
we experimented with two resources:
WordNet::Similarity and Moby?s the-
saurus. WordNet::Similarity2 is an open
source software package that allows the user
to measure the semantic similarity or related-
ness between two words (Patwardhan et al,
2003). Of the many methods implemented in
WordNet::Similarity, we report on results
for one path-based method (WUP, Wu and Palmer
(1994)), one content-information based method
(JCN, Jiang and Conrath (1998)) and two semantic
relatedness methods (LESK, Banerjee and Peder-
sen (2003), and VECTOR, (Patwardhan, 2003)).
We also used a random similarity-generating
method as a baseline (RANDOM).
The second semantic resource we use for verb-
mapping method is Moby?s thesaurus. Moby?s
thesaurus is based on Roget?s thesaurus, and con-
tains 30K root words, and 2.5M synonyms and re-
lated words. Since the direct synonyms of seed
verbs have limited coverage over the set of sen-
tences used in our experiment, we also experi-
mented with using second-level indirect synonyms
of seed verbs.
In order to deal with the TIME relation, we used
CoreLex (Buitelaar, 1998). CoreLex is based on a
unified approach to systematic polysemy and the
semantic underspecification of nouns, and derives
from WordNet 1.5. It contains 45 basic CoreLex
types, systematic polysemous classes and 39,937
nouns with tags.
2www.d.umn.edu/ tpederse/similarity.html
As mentioned earlier, we built our supervised
classifier using TiMBL.
6 Data Collection
6.1 Data Processing
To test our method, we extracted 2,166 NC types
from the Wall Street Journal (WSJ) component of
the Penn Treebank. We additionally extracted sen-
tences containing the head noun and modifier in
pre-defined constructional contexts from the amal-
gam of: (1) the Brown Corpus subset contained
in the Penn Treebank, (2) the WSJ portion of the
Penn Treebank, and (3) the British National Cor-
pus (BNC). Note that while these pre-defined con-
structional contexts are based on the contexts in
which our seed verbs are predicted to correlate
with a given semantic relation, we instances of all
verbs occurring in those contexts. For example,
based on the construction in Equation 5, we ex-
tract all instances of S(Vi,M SUBJj ,H OBJj ) for all
verbs Vi and all instances of NCj = (Mj ,Hj) in
our dataset.
Two annotators tagged the 2,166 NC types in-
dependently at 52.3% inter-annotator agreement,
and then met to discus all contentious annotations
and arrive at a mutually-acceptable gold-standard
annotation for each NC. The Brown, WSJ and
BNC data was pre-parsed with RASP, and sen-
tences were extracted which contained the head
noun and modifier of one of our 2,166 NCs in sub-
ject or object position, or as (head) noun within the
NP of an PP. After extracting these sentences, we
counted the frequencies of the different modifier?
head noun pairs, and filtered out: (a) all construc-
tional contexts not involving a verb contained in
WordNet 2.0, and (b) all NCs for which the modi-
fier and head noun did not co-occur in at least five
sentential contexts. This left us with a total of 453
NCs for training and testing. The combined total
number of sentential contexts for our 453 NCs was
7,714, containing 1,165 distinct main verbs.
We next randomly split the NC data into 80%
training data and 20% test data. The final number
of test NCs is 88; the final number of training NCs
varies depending on the verb-mapping method.
As noted in Section 2, the relations TIME and
EQUATIVE are not associated with seed verbs. For
TIME, rather than using contextual evidence, we
simply flag the possibility of a TIME if the modifier
is found to occur in the TIME class of CoreLex. In
the case of TIME, we consider coordinated occur-
494
ACTBENEFITHAVE
USE
PLAYPERFORM
...
...
Seed verbs
accept
act
agree HOLD
.
.
.
.
.
Verb?MappingMethods
AGENTBENEFICIARYCONTAINER
OBJECTPOSSESSOR
INSTRUMENT...
...
...
Semantic RelationsOriginal verbs
accommodate
Figure 2: Verb mapping
rences of the modifier and head noun (e.g. coach
and player for player coach) as evidence for the
relation.3 We thus separately collate statistics
from coordinated NPs for each NC, and from this
compute a weight for each NC based on mutual
information:
TIME(NCi) = ?log2
freq(coord(Mi, Hi))
freqMi ? freq(Hi)
(7)
where Mi and Hi are the modifier and head of
NCi, respectively, and freq(coord(Mi,Hi)) is the
frequency of occurrence of Mi and Hi in coordi-
nated NPs.
Finally, we calculate a normalised weight for
each seed verb by determining the proportion of
head verbs each seed verb occurs with.
6.2 Verb Mapping
The sentential contexts gathered from corpus
data contain a wide range of verbs, not just
the seed verbs. To map the verbs onto seed
verbs, and hence estimate which semantic rela-
tion(s) each is a predictor of, we experimented
with two different methods. First we used the
WordNet::Similarity package to calculate
the similarity between a given verb and each
of the seed verbs, experimenting with the 5
methods mentioned in Section 5. Second, we
used Moby?s thesaurus to extract both direct syn-
onyms (D-SYNONYM) and a combination of direct
and second-level indirect synonyms of verbs (I-
SYNONYM), and from this, calculate the closest-
matching seed verb(s) for a given verb.
Figure 2 depicts the procedure for mapping
verbs in constructional contexts onto the seed
verbs. Verbs found in the various contexts in the
3Note the order of the modifier and head in coordinated
NPs is considered to be irrelevant, i.e. player and coach and
coach and player are equally evidence for an EQUATIVE inter-
pretation for player coach (and coach player).
accomplish achieve behave conduct ...
ACT
act conduct deadl with function perform play
LEVEL=1
LEVEL=2
synonym in level1 synonym in level2 not found in level1
Figure 3: Expanding synonyms
# of SeedVB D-Synonyms D/I-Synonyms
57 6,755(87.6%) 7,388(95.8%)
84 6,987(90.6%) 7,389(95.8%)
Table 1: Coverage of D and D/I-Synonyms
corpus (on the left side of the figure) map onto one
or more seed verbs, which in turn map onto one
or more semantic relations.4 We replace all non-
seed verbs in the corpus data with the seed verb(s)
they map onto, potentially increasing the number
of corpus instances.
Since direct (i.e. level 1) synonyms from
Moby?s thesaurus are not sufficient to map all
verbs onto seed verbs, we also include second-
level (i.e. level 2) synonyms, expanding from di-
rect synonyms. Table 1 shows the coverage of
sentences for test NCs, in which D indicates direct
synonyms and I indicates indirect synonyms.
7 Evaluation
We evaluated our method over both 17 semantic
relations (without EQUATIVE and TIME) and the full
19 semantic relations, due to the low frequency
and lack of verb-based constructional contexts for
EQUATIVE and TIME, as indicated in Table 2. Note
that the test data set is the same for both sets of
semantic relations, but that the training data in
the case of 17 semantic relations will not con-
tain any instances for the EQUATIVE and TIME re-
lations, meaning that all such test instances will
be misclassified. The baseline for all verb map-
ping methods is a simple majority-class classifier,
which leads to an accuracy of 42.4% for the TOPIC
relation. In evaluation, we use two different val-
ues for our method: Count and Weight. Count
is based on the raw number of corpus instances,
while Weight employs the seed verb weight de-
scribed in Section 6.1.
4There is only one instance of a seed verb mapping to
multiple semantic relations, namely perform which corre-
sponds to the two relations AGENT and OBJECT.
495
# of SR # SeedV Method WUP JCN RANDOM LESK VECTOR D-SYNONYM I-SYNONYM
17 Baseline .423 .423 .423 .423 .423 .423 .423
57 Count .324 .408 .379 .416 .466 .337 .337
Weight .320 .408 .371 .416 .466 .337 .342
84 Count .406 .470 .184 .430 .413 .317 .333
Weight .424 .426 .259 .457 .526 .341 .406
19 Baseline .409 .409 .409 .409 .409 .409 .409
57 Count .315 .420 .384 .440 .466 .350 .337
Weight .311 .420 .376 .440 .466 .350 .342
84 Count .413 .470 .200 .414 .413 .321 .333
Weight .439 .446 .280 .486 .526 .356 .393
Table 2: Results with 17 relations and with 19 relations
#of SR # SeedVB Method WUP JCN RANDOM LESK VECTOR
17 Baseline .423 .423 .423 .423 .423
57 Count .423 .385 .379 .413 .466
Weight .423 .385 .379 .413 .466
84 Count .325 .439 .420 .484 .466
Weight .281 .393 .317 .476 .466
19 Baseline .409 .409 .409 .409 .409
57 Count .423 .397 .392 .413 .413
Weight .423 .397 .392 .413 .500
84 Count .333 .439 .425 .484 .413
Weight .290 .410 .317 .484 .413
Table 3: Results of combining the proposed method and with the method of Kim and Baldwin (2005)
As noted above, we excluded all NCs for which
we were unable to find at least 5 instances of the
modifier and head noun in an appropriate senten-
tial context. This exclusion reduced the original
set of 2,166 NCs to only 453, meaning that the
proposed method is unable to classify up to 80% of
NCs. For real-world applications, a method which
is only able to arrive at a classification for 20% of
instances is clearly of limited utility, and we need
some way of expanding the coverage of the pro-
posed method. This is achieved by adapting the
similarity method proposed by Kim and Baldwin
(2005) to our task, wherein we use lexical simi-
larity to identify the nearest-neighbour NC for a
given NC, and classify the given NC according to
the classification for the nearest-neighbour. The
results for the combined method are presented in
Table 3.
8 Discussion
For the basic method, as presented in Table 2, the
classifier produced similar results over the 17 se-
mantic relations to the 19 semantic relations. Us-
ing data from Weight and Count for both 17 and
19 semantic relations, the classifier achieved the
best performance with VECTOR (context vector-
based distributional similarity), followed by JCN
and LESK. The main reason is that VECTOR is
more conservative than the other methods at map-
ping (original) verbs onto seed verbs, i.e. the aver-
age number of seed verbs a given verb maps onto
is small. For the other methods, the semantics of
the original sentences are often not preserved un-
der verb mapping, introducing noise to the classi-
fication task.
Comparing the two sets of semantic relations
(17 vs. 19), the set with more semantic rela-
tions achieved slightly better performance in most
cases. A detailed breakdown of the results re-
vealed that TIME both has an above-average clas-
sification accuracy and is associated with a rela-
tively large number of test NCs, while EQUATIVE
has a below-average classification accuracy but is
associated with relatively few instances.
While an increased number of seed verbs gener-
ates more training instances under verb mapping,
it is imperative that the choice of seed verbs be
made carefully so that they not introduce noise
into the classifier and reducing overall perfor-
mance. Figure 4 is an alternate representation of
the numbers from Table 2, with results for each in-
dividual method over 57 and 84 seed verbs juxta-
posed for each of Count andWeight. From this, we
get the intriguing result that Count generally per-
forms better over fewer seed verbs, while Weight
performs better over more seed verbs.
496
WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,I
Result with Count
Verb?mapping method
Accuracy(%)
WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,IVerb?mapping method
Accuracy(%)
Result with Weight
 0
 20
 40
 60
 80
 100
w/ 57 seed verbs
w/ 84 seed verbs
 0
 20
 40
 60
 80
 100
w/ 57 seed verbs
w/ 84 seed verbs
Figure 4: Performance with 57 vs. 84 seed verbs
#of SR # SeedVB WUP LCH JCN LIN RANDOM LESK VECTOR
17 Baseline .433 .433 .441 .441 .433 .477 .428
57 .449 .421 .415 .337 .409 .469 .344
Baseline .433 .433 .433 .433 .428 .438 .444
84 .476 .416 .409 .349 .226 .465 .333
19 Baseline .418 .418 .430 .430 .418 .477 .413
57 .465 .418 .417 .341 .232 .462 .344
Baseline .413 .413 .418 .418 .413 .438 .426
84 .471 .413 .407 .348 .218 .465 .320
Table 4: Results for the method of Kim and Baldwin (2005) over the test set used in this research
For the experiment where we combine our
method with that of Kim and Baldwin (2005), as
presented in Table 3, we find a similar pattern of
results to the proposed method. Namely, VECTOR
and LESK achieve the best performance, with mi-
nor variations in the absolute performance relative
to the original method but the best results for each
relation set actually dropping marginally over the
original method. This drop is not surprising when
we consider that we use an imperfect method to
identify the nearest neighbour for an NC for which
we are unable to find corpus instances in sufficient
numbers, and then a second imperfect method to
classify the instance.
Compared to previous work, our method pro-
duces reasonably stable performance when op-
erated over the open-domain data with small
amounts of training data. Rosario and Marti
(2001) achieved about 60% using a neural net-
work in a closed domain, Moldovan et al (2004)
achieved 43% using word sense disambiguation
of the head noun and modifier over open domain
data, and Kim and Baldwin (2005) produced 53%
using lexical similarities of the head noun and
modifier (using the same relation set, but evaluated
over a different dataset). The best result achieved
by our system was 52.6% over open-domain data,
using a general-purpose relation set.
To get a better understanding of how our
method compares with that of Kim and Baldwin
(2005), we evaluated the method of Kim and Bald-
win (2005) over the same data set as used in this
research, the results of which are presented in Ta-
ble 4. The relative results for the different sim-
ilarity metrics mirror those reported in Kim and
Baldwin (2005). WUP produced the best perfor-
mance at 47-48% for the two relation sets, sig-
nificantly below the accuracy of our method at
53.3%. Perhaps more encouraging is the result
that the combined method?where we classify at-
tested instances according to the proposed method,
and classify unattested instances according to the
nearest-neighbour method of Kim and Baldwin
(2005) and the classifications from the proposed
method?outperforms the method of Kim and
Baldwin (2005). That is, the combined method
has the coverage of the method of Kim and Bald-
win (2005), but inherits the higher accuracy of the
method proposed herein. Having said this, the per-
formance of the Kim and Baldwin (2005) method
over PRODUCT, TOPIC, LOCATION and SOURCE is
superior to that of our method. In this sense,
we believe that alternate methods of hybridisation
may lead to even better results.
Finally, we wish to point out that the method
as presented here is still relatively immature, with
considerable scope for improvement. In its current
form, we do not weight the different seed verbs
497
based on their relative similarity to the original
verb. We also use the same weight and frequency
for each seed verb relative to a given relation, de-
spite seed verbs being more indicative of a given
relation and also potentially occurring more often
in the corpus. For instance, possess is more related
to POSSESSOR than occupy. Also possess occurs
more often in the corpus than belong to. As future
work, we intend to investigate whether allowances
for these considerations can improve the perfor-
mance of our method.
9 Conclusion
In this paper, we proposed a method for au-
tomatically interpreting noun compounds based
on seed verbs indicative of each semantic re-
lation. For a given modifier and head noun,
our method extracted corpus instances of the
two nouns in a range of constructional contexts,
and then mapped the original verbs onto seed
verbs based on lexical similarity derived from
WordNet::Similarity, and Moby?s The-
saurus. These instances were then fed into the
TiMBL learner to build a classifier. The best-
performing method was VECTOR, which is a con-
text vector distributional similarity method. We
also experimented with varying numbers of seed
verbs, and found that generally the more seed
verbs, the better the performance. Overall, the
best-performing system achieved an accuracy of
52.6% with 84 seed verbs and the VECTOR verb-
mapping method.
Acknowledgements
Wewould like to thank the members of the Univer-
sity of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this re-
search.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it right.
In In Proceedings of the ACL 2004 Workshop on Multi-
word Expressions: Integrating Processing, pages 24?31,
Barcelona, Spain.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco, Mex-
ico.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics, pages 96?102, Quebec, Canada.
Ted Briscoe and John Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of the 3rd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-
lands.
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy and
Underspecification. Ph.D. thesis, Brandeis University,
USA.
Yunbo Cao and Hang Li. 2002. Base noun phrase translation
using web data and the EM algorithm. In 19th Interna-
tional Conference on Computational Linguistics, Taipei,
Taiwan.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. ILK Technical Re-
port 04-02.
James Fan, Ken Barker, and Bruce W. Porter. 2003. The
knowledge required to interpret noun compounds. In 7th
International Joint Conference on Artificial Intelligence,
pages 1483?1485, Acapulco, Mexico.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Timothy W. Finin. 1980. The semantic interpretation of
compound nominals. Ph.D. thesis, University of Illinois,
Urbana, Illinois, USA.
Jay Jiang and David Conrath. 1998. Semantic similar-
ity based on corpus statistics and lexical taxonomy. In
Proceedings on International Conference on Research in
Computational Linguistics, pages 19?33.
Su Nam Kim and Timothy Baldwin. 2005. Automatic in-
terpretation of noun compounds using WordNet similar-
ity. In Second International Joint Conference On Natural
Language Processing, pages 945?956, JeJu, Korea.
Maria Lapata. 2002. The disambiguation of nominalizations.
Computational Linguistics, 28(3):357?388.
Mark Lauer. 1995. Designing Statistical Language Learn-
ers: Experiments on Noun Compounds. Ph.D. thesis,
Macquarie University.
Judith Levi. 1979. The Syntax and Semantics of Complex
Nominals. New York: Academic Press.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel An-
tohe, and Roxana Girju. 2004. Models for the seman-
tic classification of noun phrases. In HLT-NAACL 2004:
Workshop on Computational Lexical Semantics, pages 60?
67, Boston, USA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. 2003. Using measures of semantic relatedness for
word sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Processing
and Computational Linguistics.
Siddharth Patwardhan. 2003. Incorporating dictionary and
corpus information into a context vector measure of se-
mantic relatedness. Master?s thesis, University of Min-
nesota, USA.
Barbara Rosario and Hearst Marti. 2001. Classifying the se-
mantic relations in noun compounds via a domain-specific
lexical hierarchy. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Processing,
pages 82?90.
Lucy Vanderwende. 1994. Algorithm for automatic inter-
pretation of noun sequences. In Proceedings of the 15th
Conference on Computational linguistics, pages 782?788.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics and
lexical selection. In 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 133 ?138.
498
Proceedings of ACL-08: HLT, pages 317?325,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Parsing and PP attachment Performance with Sense Information
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
tim@csse.unimelb.edu.au
David Martinez
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
davidm@csse.unimelb.edu.au
Abstract
To date, parsers have made limited use of se-
mantic information, but there is evidence to
suggest that semantic features can enhance
parse disambiguation. This paper shows that
semantic classes help to obtain significant im-
provement in both parsing and PP attachment
tasks. We devise a gold-standard sense- and
parse tree-annotated dataset based on the in-
tersection of the Penn Treebank and SemCor,
and experiment with different approaches to
both semantic representation and disambigua-
tion. For the Bikel parser, we achieved a
maximal error reduction rate over the base-
line parser of 6.9% and 20.5%, for parsing and
PP-attachment respectively, using an unsuper-
vised WSD strategy. This demonstrates that
word sense information can indeed enhance
the performance of syntactic disambiguation.
1 Introduction
Traditionally, parse disambiguation has relied on
structural features extracted from syntactic parse
trees, and made only limited use of semantic in-
formation. There is both empirical evidence and
linguistic intuition to indicate that semantic fea-
tures can enhance parse disambiguation perfor-
mance, however. For example, a number of different
parsers have been shown to benefit from lexicalisa-
tion, that is, the conditioning of structural features
on the lexical head of the given constituent (Mager-
man, 1995; Collins, 1996; Charniak, 1997; Char-
niak, 2000; Collins, 2003). As an example of lexi-
calisation, we may observe in our training data that
knife often occurs as the manner adjunct of open in
prepositional phrases headed by with (c.f. open with
a knife), which would provide strong evidence for
with (a) knife attaching to open and not box in open
the box with a knife. It would not, however, pro-
vide any insight into the correct attachment of with
scissors in open the box with scissors, as the disam-
biguation model would not be able to predict that
knife and scissors are semantically similar and thus
likely to have the same attachment preferences.
In order to deal with this limitation, we propose to
integrate directly the semantic classes of words into
the process of training the parser. This is done by
substituting the original words with semantic codes
that reflect semantic classes. For example, in the
above example we could substitute both knife and
scissors with the semantic class TOOL, thus relating
the training and test instances directly. We explore
several models for semantic representation, based
around WordNet (Fellbaum, 1998).
Our approach to exploring the impact of lexical
semantics on parsing performance is to take two
state-of-the-art statistical treebank parsers and pre-
process the inputs variously. This simple method
allows us to incorporate semantic information into
the parser without having to reimplement a full sta-
tistical parser, and also allows for maximum compa-
rability with existing results in the treebank parsing
community. We test the parsers over both a PP at-
tachment and full parsing task.
In experimenting with different semantic repre-
sentations, we require some strategy to disambiguate
the semantic class of polysemous words in context
(e.g. determining for each instance of crane whether
it refers to an animal or a lifting device). We explore
a number of disambiguation strategies, including the
use of hand-annotated (gold-standard) senses, the
317
use of the most frequent sense, and an unsupervised
word sense disambiguation (WSD) system.
This paper shows that semantic classes help to
obtain significant improvements for both PP attach-
ment and parsing. We attain a 20.5% error reduction
for PP attachment, and 6.9% for parsing. These re-
sults are achieved using most frequent sense infor-
mation, which surprisingly outperforms both gold-
standard senses and automatic WSD.
The results are notable in demonstrating that very
simple preprocessing of the parser input facilitates
significant improvements in parser performance. We
provide the first definitive results that word sense
information can enhance Penn Treebank parser per-
formance, building on earlier results of Bikel (2000)
and Xiong et al (2005). Given our simple procedure
for incorporating lexical semantics into the parsing
process, our hope is that this research will open the
door to further gains using more sophisticated pars-
ing models and richer semantic options.
2 Background
This research is focused on applying lexical seman-
tics in parsing and PP attachment tasks. Below, we
outline these tasks.
Parsing
As our baseline parsers, we use two state-of-the-
art lexicalised parsing models, namely the Bikel
parser (Bikel, 2004) and Charniak parser (Charniak,
2000). While a detailed description of the respective
parsing models is beyond the scope of this paper, it
is worth noting that both parsers induce a context
free grammar as well as a generative parsing model
from a training set of parse trees, and use a devel-
opment set to tune internal parameters. Tradition-
ally, the two parsers have been trained and evaluated
over the WSJ portion of the Penn Treebank (PTB:
Marcus et al (1993)). We diverge from this norm in
focusing exclusively on a sense-annotated subset of
the Brown Corpus portion of the Penn Treebank, in
order to investigate the upper bound performance of
the models given gold-standard sense information.
PP attachment in a parsing context
Prepositional phrase attachment (PP attachment)
is the problem of determining the correct attachment
site for a PP, conventionally in the form of the noun
or verb in a V NP PP structure (Ratnaparkhi et al,
1994; Mitchell, 2004). For instance, in I ate a pizza
with anchovies, the PP with anchovies could attach
either to the verb (c.f. ate with anchovies) or to the
noun (c.f. pizza with anchovies), of which the noun
is the correct attachment site. With I ate a pizza with
friends, on the other hand, the verb is the correct at-
tachment site. PP attachment is a structural ambigu-
ity problem, and as such, a subproblem of parsing.
Traditionally the so-called RRR data (Ratna-
parkhi et al, 1994) has been used to evaluate PP
attachment algorithms. RRR consists of 20,081
training and 3,097 test quadruples of the form
(v,n1,p,n2), where the attachment decision is
either v or n1. The best published results over RRR
are those of Stetina and Nagao (1997), who em-
ploy WordNet sense predictions from an unsuper-
vised WSD method within a decision tree classifier.
Their work is particularly inspiring in that it signifi-
cantly outperformed the plethora of lexicalised prob-
abilistic models that had been proposed to that point,
and has not been beaten in later attempts.
In a recent paper, Atterer and Schu?tze (2007) crit-
icised the RRR dataset because it assumes that an
oracle parser provides the two hypothesised struc-
tures to choose between. This is needed to derive the
fact that there are two possible attachment sites, as
well as information about the lexical phrases, which
are typically extracted heuristically from gold stan-
dard parses. Atterer and Schu?tze argue that the only
meaningful setting for PP attachment is within a
parser, and go on to demonstrate that in a parser set-
ting, the Bikel parser is competitive with the best-
performing dedicated PP attachment methods. Any
improvement in PP attachment performance over the
baseline Bikel parser thus represents an advance-
ment in state-of-the-art performance.
That we specifically present results for PP attach-
ment in a parsing context is a combination of us sup-
porting the new research direction for PP attachment
established by Atterer and Schu?tze, and us wishing
to reinforce the findings of Stetina and Nagao that
word sense information significantly enhances PP
attachment performance in this new setting.
Lexical semantics in parsing
There have been a number of attempts to incorpo-
rate word sense information into parsing tasks. The
318
most closely related research is that of Bikel (2000),
who merged the Brown portion of the Penn Tree-
bank with SemCor (similarly to our approach in Sec-
tion 4.1), and used this as the basis for evaluation of
a generative bilexical model for joint WSD and pars-
ing. He evaluated his proposed model in a parsing
context both with and without WordNet-based sense
information, and found that the introduction of sense
information either had no impact or degraded parse
performance.
The only successful applications of word sense in-
formation to parsing that we are aware of are Xiong
et al (2005) and Fujita et al (2007). Xiong et al
(2005) experimented with first-sense and hypernym
features from HowNet and CiLin (both WordNets
for Chinese) in a generative parse model applied
to the Chinese Penn Treebank. The combination
of word sense and first-level hypernyms produced
a significant improvement over their basic model.
Fujita et al (2007) extended this work in imple-
menting a discriminative parse selection model in-
corporating word sense information mapped onto
upper-level ontologies of differing depths. Based
on gold-standard sense information, they achieved
large-scale improvements over a basic parse selec-
tion model in the context of the Hinoki treebank.
Other notable examples of the successful incorpo-
ration of lexical semantics into parsing, not through
word sense information but indirectly via selectional
preferences, are Dowding et al (1994) and Hektoen
(1997). For a broader review of WSD in NLP appli-
cations, see Resnik (2006).
3 Integrating Semantics into Parsing
Our approach to providing the parsers with sense
information is to make available the semantic de-
notation of each word in the form of a semantic
class. This is done simply by substituting the origi-
nal words with semantic codes. For example, in the
earlier example of open with a knife we could sub-
stitute both knife and scissors with the class TOOL,
and thus directly facilitate semantic generalisation
within the parser. There are three main aspects that
we have to consider in this process: (i) the seman-
tic representation, (ii) semantic disambiguation, and
(iii) morphology.
There are many ways to represent semantic re-
lationships between words. In this research we
opt for a class-based representation that will map
semantically-related words into a common semantic
category. Our choice for this work was the WordNet
2.1 lexical database, in which synonyms are grouped
into synsets, which are then linked via an IS-A hi-
erarchy. WordNet contains other types of relations
such as meronymy, but we did not use them in this
research. With any lexical semantic resource, we
have to be careful to choose the appropriate level of
granularity for a given task: if we limit ourselves to
synsets we will not be able to capture broader gen-
eralisations, such as the one between knife and scis-
sors;1 on the other hand by grouping words related at
a higher level in the hierarchy we could find that we
make overly coarse groupings (e.g. mallet, square
and steel-wool pad are also descendants of TOOL in
WordNet, none of which would conventionally be
used as the manner adjunct of cut). We will test dif-
ferent levels of granularity in this work.
The second problem we face is semantic disam-
biguation. The more fine-grained our semantic rep-
resentation, the higher the average polysemy and the
greater the need to distinguish between these senses.
For instance, if we find the word crane in a con-
text such as demolish a house with the crane, the
ability to discern that this corresponds to the DE-
VICE and not ANIMAL sense of word will allow us
to avoid erroneous generalisations. This problem of
identifying the correct sense of a word in context is
known as word sense disambiguation (WSD: Agirre
and Edmonds (2006)). Disambiguating each word
relative to its context of use becomes increasingly
difficult for fine-grained representations (Palmer et
al., 2006). We experiment with different ways of
tackling WSD, using both gold-standard data and
automatic methods.
Finally, when substituting words with semantic
tags we have to decide how to treat different word
forms of a given lemma. In the case of English, this
pertains most notably to verb inflection and noun
number, a distinction which we lose if we opt to
map all word forms onto semantic classes. For our
current purposes we choose to substitute all word
1In WordNet 2.1, knife and scissors are sister synsets, both
of which have TOOL as their 4th hypernym. Only by mapping
them onto their 1st hypernym or higher would we be able to
capture the semantic generalisation alluded to above.
319
forms, but we plan to look at alternative represen-
tations in the future.
4 Experimental setting
We evaluate the performance of our approach in two
settings: (1) full parsing, and (2) PP attachment
within a full parsing context. Below, we outline the
dataset used in this research and the parser evalu-
ation methodology, explain the methodology used
to perform PP attachment, present the different op-
tions for semantic representation, and finally detail
the disambiguation methods.
4.1 Dataset and parser evaluation
One of the main requirements for our dataset is the
availability of gold-standard sense and parse tree an-
notations. The gold-standard sense annotations al-
low us to perform upper bound evaluation of the rel-
ative impact of a given semantic representation on
parsing and PP attachment performance, to contrast
with the performance in more realistic semantic dis-
ambiguation settings. The gold-standard parse tree
annotations are required in order to carry out evalu-
ation of parser and PP attachment performance.
The only publicly-available resource with these
two characteristics at the time of this work was the
subset of the Brown Corpus that is included in both
SemCor (Landes et al, 1998) and the Penn Tree-
bank (PTB).2 This provided the basis of our dataset.
After sentence- and word-aligning the SemCor and
PTB data (discarding sentences where there was a
difference in tokenisation), we were left with a total
of 8,669 sentences containing 151,928 words. Note
that this dataset is smaller than the one described by
Bikel (2000) in a similar exercise, the reason being
our simple and conservative approach taken when
merging the resources.
We relied on this dataset alne for all the exper-
iments in this paper. In order to maximise repro-
ducibility and encourage further experimentation in
the direction pioneered in this research, we parti-
tioned the data into 3 sets: 80% training, 10% devel-
opment and 10% test data. This dataset is available
on request to the research community.
2OntoNotes (Hovy et al, 2006) includes large-scale tree-
bank and (selective) sense data, which we plan to use for future
experiments when it becomes fully available.
We evaluate the parsers via labelled bracketing re-
call (R), precision (P) and F-score (F1). We use
Bikel?s randomized parsing evaluation comparator3
(with p < 0.05 throughout) to test the statistical sig-
nificance of the results using word sense informa-
tion, relative to the respective baseline parser using
only lexical features.
4.2 PP attachment task
Following Atterer and Schu?tze (2007), we wrote
a script that, given a parse tree, identifies in-
stances of PP attachment ambiguity and outputs the
(v,n1,p,n2) quadruple involved and the attach-
ment decision. This extraction system uses Collins?
rules (based on TREEP (Chiang and Bikel, 2002))
to locate the heads of phrases. Over the combined
gold-standard parsing dataset, our script extracted a
total of 2,541 PP attachment quadruples. As with
the parsing data, we partitioned the data into 3 sets:
80% training, 10% development and 10% test data.
Once again, this dataset and the script used to ex-
tract the quadruples are available on request to the
research community.
In order to evaluate the PP attachment perfor-
mance of a parser, we run our extraction script over
the parser output in the same manner as for the gold-
standard data, and compare the extracted quadru-
ples to the gold-standard ones. Note that there is
no guarantee of agreement in the quadruple mem-
bership between the extraction script and the gold
standard, as the parser may have produced a parse
which is incompatible with either attachment possi-
bility. A quadruple is deemed correct if: (1) it exists
in the gold standard, and (2) the attachment deci-
sion is correct. Conversely, it is deemed incorrect if:
(1) it exists in the gold standard, and (2) the attach-
ment decision is incorrect. Quadruples not found in
the gold standard are discarded. Precision was mea-
sured as the number of correct quadruples divided by
the total number of correct and incorrect quadruples
(i.e. all quadruples which are not discarded), and re-
call as the number of correct quadruples divided by
the total number of gold-standard quadruples in the
test set. This evaluation methodology coincides with
that of Atterer and Schu?tze (2007).
Statistical significance was calculated based on
3www.cis.upenn.edu/?dbikel/software.html
320
a modified version of the Bikel comparator (see
above), once again with p < 0.05.
4.3 Semantic representation
We experimented with a range of semantic represen-
tations, all of which are based on WordNet 2.1. As
mentioned above, words in WordNet are organised
into sets of synonyms, called synsets. Each synset
in turn belongs to a unique semantic file (SF). There
are a total of 45 SFs (1 for adverbs, 3 for adjectives,
15 for verbs, and 26 for nouns), based on syntactic
and semantic categories. A selection of SFs is pre-
sented in Table 1 for illustration purposes.
We experiment with both full synsets and SFs as
instances of fine-grained and coarse-grained seman-
tic representation, respectively. As an example of
the difference in these two representations, knife in
its tool sense is in the EDGE TOOL USED AS A CUT-
TING INSTRUMENT singleton synset, and also in the
ARTIFACT SF along with thousands of other words
including cutter. Note that these are the two ex-
tremes of semantic granularity in WordNet, and we
plan to experiment with intermediate representation
levels in future research (c.f. Li and Abe (1998), Mc-
Carthy and Carroll (2003), Xiong et al (2005), Fu-
jita et al (2007)).
As a hybrid representation, we tested the effect
of merging words with their corresponding SF (e.g.
knife+ARTIFACT ). This is a form of semantic spe-
cialisation rather than generalisation, and allows the
parser to discriminate between the different senses
of each word, but not generalise across words.
For each of these three semantic representations,
we experimented with substituting each of: (1) all
open-class POSs (nouns, verbs, adjectives and ad-
verbs), (2) nouns only, and (3) verbs only. There are
thus a total of 9 combinations of representation type
and target POS.
4.4 Disambiguation methods
For a given semantic representation, we need some
form of WSD to determine the semantics of each
token occurrence of a target word. We experimented
with three options:
1. Gold-standard: Gold-standard annotations
from SemCor. This gives us the upper bound
performance of the semantic representation.
SF ID DEFINITION
adj.all all adjective clusters
adj.pert relational adjectives (pertainyms)
adj.ppl participial adjectives
adv.all all adverbs
noun.act nouns denoting acts or actions
noun.animal nouns denoting animals
noun.artifact nouns denoting man-made objects
...
verb.consumption verbs of eating and drinking
verb.emotion verbs of feeling
verb.perception verbs of seeing, hearing, feeling
...
Table 1: A selection of WordNet SFs
2. First Sense (1ST): All token instances of a
given word are tagged with their most fre-
quent sense in WordNet.4 Note that the first
sense predictions are based largely on the same
dataset as we use in our evaluation, such that
the predictions are tuned to our dataset and not
fully unsupervised.
3. Automatic Sense Ranking (ASR): First sense
tagging as for First Sense above, except that an
unsupervised system is used to automatically
predict the most frequent sense for each word
based on an independent corpus. The method
we use to predict the first sense is that of Mc-
Carthy et al (2004), which was obtained us-
ing a thesaurus automatically created from the
British National Corpus (BNC) applying the
method of Lin (1998), coupled with WordNet-
based similarity measures. This method is fully
unsupervised and completely unreliant on any
annotations from our dataset.
In the case of SFs, we perform full synset WSD
based on one of the above options, and then map the
prediction onto the corresponding (unique) SF.
5 Results
We present the results for each disambiguation ap-
proach in turn, analysing the results for parsing and
PP attachment separately.
4There are some differences with the most frequent sense in
SemCor, due to extra corpora used in WordNet development,
and also changes in WordNet from the original version used for
the SemCor tagging.
321
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .808 .832 .837 .845 .841
SF .855 .809 .831 .847? .854? .850?
SFn .860 .808 .833 .847? .853? .850?
SFv .861 .811 .835 .847? .856? .851?
word + SF .865? .814? .839? .837 .846 .842
word + SFn .862 .809 .835 .841? .850? .846?
word + SFv .862 .810 .835 .840 .851 .845
Syn .863? .812 .837 .845? .853? .849?
Synn .860 .807 .832 .841 .849 .845
Synv .863? .813? .837? .843? .851? .847?
Table 2: Parsing results with gold-standard senses (? in-
dicates that the recall or precision is significantly better
than baseline; the best performing method in each col-
umn is shown in bold)
5.1 Gold standard
We disambiguated each token instance in our cor-
pus according to the gold-standard sense data, and
trained both the Charniak and Bikel parsers over
each semantic representation. We evaluated the
parsers in full parsing and PP attachment contexts.
The results for parsing are given in Table 2. The
rows represent the three semantic representations
(including whether we substitute only nouns, only
verbs or all POS). We can see that in almost all
cases the semantically-enriched representations im-
prove over the baseline parsers. These results are
statistically significant in some cases (as indicated
by ?). The SFv representation produces the best re-
sults for Bikel (F-score 0.010 above baseline), while
for Charniak the best performance is obtained with
word+SF (F-score 0.007 above baseline). Compar-
ing the two baseline parsers, Bikel achieves better
precision and Charniak better recall. Overall, Bikel
obtains a superior F-score in all configurations.
The results for the PP attachment experiments us-
ing gold-standard senses are given in Table 3, both
for the Charniak and Bikel parsers. Again, the F-
score for the semantic representations is better than
the baseline in all cases. We see that the improve-
ment is significant for recall in most cases (particu-
larly when using verbs), but not for precision (only
Charniak over Synv and word+SFv for Bikel). For
both parsers the best results are achieved with SFv,
which was also the best configuration for parsing
with Bikel. The performance gain obtained here is
larger than in parsing, which is in accordance with
the findings of Stetina and Nagao that lexical se-
mantics has a considerable effect on PP attachment
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .714? .809 .758
SFn .671 .792 .726 .706 .818 .758
SFv .729? .823 .773? .733? .827 .778?
word + SF .710? .801 .753 .706? .837 .766?
word + SFn .698? .813 .751 .706? .829 .763?
word + SFv .714? .805 .757? .706? .837? .766?
Syn .722? .814 .765? .702? .825 .758
Synn .678 .805 .736 .690 .822 .751
Synv .702? .817? .755? .690? .834 .755?
Table 3: PP attachment results with gold-standard senses
(? indicates that the recall or precision is significantly bet-
ter than baseline; the best performing method in each col-
umn is shown in bold)
performance. As in full-parsing, Bikel outperforms
Charniak, but in this case the difference in the base-
lines is not statistically significant.
5.2 First sense (1ST)
For this experiment, we use the first sense data from
WordNet for disambiguation. The results for full
parsing are given in Table 4. Again, the perfor-
mance is significantly better than baseline in most
cases, and surprisingly the results are even better
than gold-standard in some cases. We hypothesise
that this is due to the avoidance of excessive frag-
mentation, as occurs with fine-grained senses. The
results are significantly better for nouns, with SFn
performing best. Verbs seem to suffer from lack of
disambiguation precision, especially for Bikel. Here
again, Charniak trails behind Bikel.
The results for the PP attachment task are shown
in Table 5. The behaviour is slightly different here,
with Charniak obtaining better results than Bikel in
most cases. As was the case for parsing, the per-
formance with 1ST reaches and in many instances
surpasses gold-standard levels, achieving statistical
significance over the baseline in places. Compar-
ing the semantic representations, the best results are
achieved with SFv, as we saw in the gold-standard
PP-attachment case.
5.3 Automatic sense ranking (ASR)
The final option for WSD is automatic sense rank-
ing, which indicates how well our method performs
in a completely unsupervised setting.
The parsing results are given in Table 6. We can
see that the scores are very similar to those from
322
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .851 .804 .827 .843 .850 .846
SFn .863? .813 .837? .850? .854? .852?
SFv .857 .808 .832 .843 .853? .848
word + SF .859 .810 .834 .833 .841 .837
word + SFn .862? .811 .836 .844? .851? .848?
word + SFv .857 .808 .832 .831 .839 .835
Syn .857 .810 .833 .837 .844 .840
Synn .863? .812 .837? .844? .851? .848?
Synv .860 .810 .834 .836 .844 .840
Table 4: Parsing results with 1ST (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .710 .808 .756 .702 .806 .751
SFn .671 .781 .722 .702 .829 .760
SFv .737? .836? .783? .718? .821 .766?
word + SF .706 .811 .755 .694 .823 .753
word + SFn .690 .815 .747 .667 .810 .731
word + SFv .714? .805 .757? .710? .819 .761?
Syn .725? .833? .776? .698 .828 .757
Synn .698 .828? .757? .667 .817 .734
Synv .722? .811 .763? .706? .818 .758?
Table 5: PP attachment results with 1ST (? indicates that
the recall or precision is significantly better than baseline;
the best performing method in each column is shown in
bold)
1ST, with improvements in some cases, particularly
for Charniak. Again, the results are better for nouns,
except for the case of SFv with Bikel. Bikel outper-
forms Charniak in terms of F-score in all cases.
The PP attachment results are given in Table 7.
The results are similar to 1ST, with significant im-
provements for verbs. In this case, synsets slightly
outperform SF. Charniak performs better than Bikel,
and the results for Synv are higher than the best ob-
tained using gold-standard senses.
6 Discussion
The results of the previous section show that the im-
provements in parsing results are small but signifi-
cant, for all three word sense disambiguation strate-
gies (gold-standard, 1ST and ASR). Table 8 sum-
marises the results, showing that the error reduction
rate (ERR) over the parsing F-score is up to 6.9%,
which is remarkable given the relatively superficial
strategy for incorporating sense information into the
parser. Note also that our baseline results for the
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .857 .807 .832 .837 .845 .841
SF .863 .815? .838 .845? .852 .849
SFn .862 .810 .835 .845? .850 .847?
SFv .859 .810 .833 .846? .856? .851?
word + SF .859 .810 .834 .836 .844 .840
word + SFn .865? .813? .838? .844? .852? .848?
word + SFv .856 .806 .830 .832 .839 .836
Syn .856 .807 .831 .840 .847 .843
Synn .864? .813? .838? .844? .851? .847?
Synv .857 .806 .831 .837 .845 .841
Table 6: Parsing results with ASR (? indicates that the
recall or precision is significantly better than baseline; the
best performing method in each column is shown in bold)
CHARNIAK BIKELSYSTEM R P F1 R P F1
Baseline .667 .798 .727 .659 .820 .730
SF .733? .824 .776? .698 .805 .748
SFn .682 .791 .733 .671 .807 .732
SFv .733? .813 .771? .710? .812 .757?
word + SF .714? .798 .754 .675 .800 .732
word + SFn .690 .807 .744 .659 .804 .724
word + SFv .706? .800 .750 .702? .814 .754?
Syn .733? .827 .778? .694 .805 .745
Synn .686 .810 .743 .667 .806 .730
Synv .714? .816 .762? .714? .816 .762?
Table 7: PP attachment results with ASR (? indicates that
the recall or precision is significantly better than baseline;
the best performance in each column is shown in bold)
dataset are almost the same as previous work pars-
ing the Brown corpus with similar models (Gildea,
2001), which suggests that our dataset is representa-
tive of this corpus.
The improvement in PP attachment was larger
(20.5% ERR), and also statistically significant. The
results for PP attachment are especially important,
as we demonstrate that the sense information has
high utility when embedded within a parser, where
the parser needs to first identify the ambiguity and
heads correctly. Note that Atterer and Schu?tze
(2007) have shown that the Bikel parser performs as
well as the state-of-the-art in PP attachment, which
suggests our method improves over the current state-
of-the-art. The fact that the improvement is larger
for PP attachment than for full parsing is suggestive
of PP attachment being a parsing subtask where lex-
ical semantic information is particularly important,
supporting the findings of Stetina and Nagao (1997)
over a standalone PP attachment task. We also ob-
served that while better PP-attachment usually im-
proves parsing, there is some small variation. This
323
WSD TASK PAR BASE SEM ERR BEST
Pars.
C .832 .839? 4.2% word+SF
Gold- B .841 .851? 6.3% SFv
standard
PP
C .727 .773? 16.9% SFv
B .730 .778? 17.8% SFv
Pars.
C .832 .837? 3.0% SFn, Synn
1ST
B .841 .852? 6.9% SFn
PP
C .727 .783? 20.5% SFv
B .730 .766? 13.3% SFv
Pars.
C .832 .838? 3.6% SF, word+SFn, Synn
ASR
B .841 .851? 6.3% SFv
PP
C .727 .778? 18.7% Syn
B .730 .762? 11.9% Synv
Table 8: Summary of F-score results with error reduc-
tion rates and the best semantic representation(s) for each
setting (C = Charniak, B = Bikel)
means that the best configuration for PP-attachment
does not always produce the best results for parsing
One surprising finding was the strong perfor-
mance of the automatic WSD systems, actually
outperforming the gold-standard annotation overall.
Our interpretation of this result is that the approach
of annotating all occurrences of the same word with
the same sense allows the model to avoid the data
sparseness associated with the gold-standard distinc-
tions, as well as supporting the merging of differ-
ent words into single semantic classes. While the
results for gold-standard senses were intended as
an upper bound for WordNet-based sense informa-
tion, in practice there was very little difference be-
tween gold-standard senses and automatic WSD in
all cases barring the Bikel parser and PP attachment.
Comparing the two parsers, Charniak performs
better than Bikel on PP attachment when automatic
WSD is used, while Bikel performs better on parsing
overall. Regarding the choice of WSD system, the
results for both approaches are very similar, show-
ing that ASR performs well, even if it does not re-
quire sense frequency information.
The analysis of performance according to the se-
mantic representation is not so clear cut. Gener-
alising only verbs to semantic files (SFv) was the
best option in most of the experiments, particularly
for PP-attachment. This could indicate that seman-
tic generalisation is particularly important for verbs,
more so than nouns.
Our hope is that this paper serves as the bridge-
head for a new line of research into the impact of
lexical semantics on parsing. Notably, more could
be done to fine-tune the semantic representation be-
tween the two extremes of full synsets and SFs.
One could also imagine that the appropriate level of
generalisation differs across POS and even the rel-
ative syntactic role, e.g. finer-grained semantics are
needed for the objects than subjects of verbs.
On the other hand, the parsing strategy is very
simple, as we just substitute words by their semantic
class and then train statistical parsers on the trans-
formed input. The semantic class should be an in-
formation source that the parsers take into account in
addition to analysing the actual words used. Tighter
integration of semantics into the parsing models,
possibly in the form of discriminative reranking
models (Collins and Koo, 2005; Charniak and John-
son, 2005; McClosky et al, 2006), is a promising
way forward in this regard.
7 Conclusions
In this work we have trained two state-of-the-art
statistical parsers on semantically-enriched input,
where content words have been substituted with
their semantic classes. This simple method allows
us to incorporate lexical semantic information into
the parser, without having to reimplement a full sta-
tistical parser. We tested the two parsers in both a
full parsing and a PP attachment context.
This paper shows that semantic classes achieve
significant improvement both on full parsing and
PP attachment tasks relative to the baseline parsers.
PP attachment achieves a 20.5% ERR, and parsing
6.9% without requiring hand-tagged data.
The results are highly significant in demonstrating
that a simplistic approach to incorporating lexical
semantics into a parser significantly improves parser
performance. As far as we know, these are the first
results over both WordNet and the Penn Treebank to
show that semantic processing helps parsing.
Acknowledgements
We wish to thank Diana McCarthy for providing us
with the sense rank for the target words. This work
was partially funded by the Education Ministry (project
KNOW TIN2006-15049), the Basque Government (IT-
397-07), and the Australian Research Council (grant no.
DP0663879). Eneko Agirre participated in this research
while visiting the University of Melbourne, based on joint
funding from the Basque Government and HCSNet.
324
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Michaela Atterer and Hinrich Schu?tze. 2007. Prepositional
phrase attachment without oracles. Computational Linguis-
tics, 33(4):469?476.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proc. of the Joint SIGDAT
Conference on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora (EMNLP/VLC-2000), pages
155?63, Hong Kong, China.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Proc.
of the 43rd Annual Meeting of the ACL, pages 173?80, Ann
Arbor, USA.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In Proc. of the 15th Annual
Conference on Artificial Intelligence (AAAI-97), pages 598?
603, Stanford, USA.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proc. of the 1st Annual Meeting of the North Ameri-
can Chapter of Association for Computational Linguistics
(NAACL2000), Seattle, USA.
David Chiang and David M. Bikel. 2002. Recovering latent
information in treebanks. In Proc. of the 19th International
Conference on Computational Linguistics (COLING 2002),
pages 183?9, Taipei, Taiwan.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Michael J. Collins. 1996. A new statistical parser based on
lexical dependencies. In Proc. of the 34th Annual Meeting
of the ACL, pages 184?91, Santa Cruz, USA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589?637.
John Dowding, Robert Moore, Franc?ois Andry, and Douglas
Moran. 1994. Interleaving syntax and semantics in an effi-
cient bottom-up parser. In Proc. of the 32nd Annual Meeting
of the ACL, pages 110?6, Las Cruces, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In Proc. of the ACL 2007 Workshop on Deep
Linguistic Processing, pages 25?32, Prague, Czech Repub-
lic.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proc. of the 6th Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2001), pages 167?202,
Pittsburgh, USA.
Erik Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies (IWPT-1997),
pages 113?122, Boston, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume: Short
Papers, pages 57?60, New York City, USA.
Shari Landes, Claudia Leacock, and Randee I. Tengi. 1998.
Building semantic concordances. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database. MIT
Press, Cambridge, USA.
Hang Li and Naoki Abe. 1998. Generalising case frames using
a thesaurus and the MDL principle. Computational Linguis-
tics, 24(2):217?44.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics: COLING/ACL-98, pages 768?774, Montreal,
Canada.
David M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proc. of the 33rd Annual Meeting of the ACL,
pages 276?83, Cambridge, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
David McClosky, Eugene Charniak, and Mark Johnson. 2006.
Effective self-training for parsing. In Proc. of the Hu-
man Language Technology Conference of the NAACL
(NAACL2006), pages 152?159, New York City, USA.
Brian Mitchell. 2004. Prepositional Phrase Attachment using
Machine Learning Algorithms. Ph.D. thesis, University of
Sheffield.
Martha Palmer, Hoa Dang, and Christiane Fellbaum. 2006.
Making fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language Engi-
neering, 13(2):137?63.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994.
A maximum entropy model for prepositional phrase attach-
ment. In HLT ?94: Proceedings of the Workshop on Human
Language Technology, pages 250?255, Plainsboro, USA.
Philip Resnik. 2006. WSD in NLP applications. In Eneko
Agirre and Philip Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, chapter 11, pages
303?40. Springer, Dordrecht, Netherlands.
Jiri Stetina and Makoto Nagao. 1997. Corpus based PP attach-
ment ambiguity resolution with a semantic dictionary. In
Proc. of the 5th Annual Workshop on Very Large Corpora,
pages 66?80, Hong Kong, China.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with semantic knowledge. In Proc. of the 2nd Inter-
national Joint Conference on Natural Language Processing
(IJCNLP-05), pages 70?81, Jeju Island, Korea.
325
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 161?164,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Automatic Satire Detection: Are You Having a Laugh?
Clint Burfoot
CSSE
University of Melbourne
VIC 3010 Australia
cburfoot@csse.unimelb.edu.au
Timothy Baldwin
CSSE
University of Melbourne
VIC 3010 Australia
tim@csse.unimelb.edu.au
Abstract
We introduce the novel task of determin-
ing whether a newswire article is ?true?
or satirical. We experiment with SVMs,
feature scaling, and a number of lexical
and semantic feature types, and achieve
promising results over the task.
1 Introduction
This paper describes a method for filtering satirical
news articles from true newswire documents. We
define a satirical article as one which deliberately
exposes real-world individuals, organisations and
events to ridicule.
Satirical news articles tend to mimic true
newswire articles, incorporating irony and non se-
quitur in an attempt to provide humorous insight.
An example excerpt is:
Bank Of England Governor Mervyn King is a
Queen, Says Fed Chairman Ben Bernanke
During last night?s appearance on the Amer-
ican David Letterman Show, Fed Chairman
Ben Bernanke let slip that Bank of England
(BOE) Governor, Mervyn King, enjoys wearing
women?s clothing.
Contrast this with a snippet of a true newswire ar-
ticle:
Delegates prepare for Cairo conference amid
tight security
Delegates from 156 countries began preparatory
talks here Saturday ahead of the official opening
of the UN World Population Conference amid
tight security.
The basis for our claim that the first document is
satirical is surprisingly subtle in nature, and relates
to the absurdity of the suggestion that a prominent
figure would expose another prominent figure as
a cross dresser, the implausibility of this story ap-
pearing in a reputable news source, and the pun on
the name (King being a Queen).
Satire classification is a novel task to compu-
tational linguistics. It is somewhat similar to the
more widely-researched text classification tasks of
spam filtering (Androutsopoulos et al, 2000) and
sentiment classification (Pang and Lee, 2008), in
that: (a) it is a binary classification task, and (b)
it is an intrinsically semantic task, i.e. satire news
articles are recognisable as such through interpre-
tation and cross-comparison to world knowledge
about the entities involved. Similarly to spam fil-
tering and sentiment classification, a key ques-
tion asked in this research is whether it is possi-
ble to perform the task on the basis of simple lex-
ical features of various types. That is, is it pos-
sible to automatically detect satire without access
to the complex inferencing and real-world knowl-
edge that humans make use of.
The primary contributions of this research are as
follows: (1) we introduce a novel task to the arena
of computational linguistics and machine learning,
and make available a standardised dataset for re-
search on satire detection; and (2) we develop a
method which is adept at identifying satire based
on simple bag-of-words features, and further ex-
tend it to include richer features.
2 Corpus
Our satire corpus consists of a total of 4000
newswire documents and 233 satire news articles,
split into fixed training and test sets as detailed in
Table 1. The newswire documents were randomly
sampled from the English Gigaword Corpus. The
satire documents were selected to relate closely
to at least one of the newswire documents by:
(1) randomly selecting a newswire document; (2)
hand-picking a key individual, institution or event
from the selected document, and using it to for-
mulate a phrasal query (e.g. Bill Clinton); (3) us-
ing the query to issue a site-restricted query to the
161
Training Test Total
TRUE 2505 1495 4000
SATIRE 133 100 233
Table 1: Corpus statistics
Google search engine;
1
and (4) manually filtering
out ?non-newsy?, irrelevant and overly-offensive
documents from the top-10 returned documents
(i.e. documents not containing satire news articles,
or containing satire articles which were not rel-
evant to the original query). All newswire and
satire documents were then converted to plain text
of consistent format using lynx, and all content
other than the title and body of the article was
manually removed (including web page menus,
and header and footer data). Finally, all documents
were manually post-edited to remove references to
the source (e.g. AP or Onion), formatting quirks
specific to a particular source (e.g. all caps in the
title), and any textual metadata which was indica-
tive of the document source (e.g. editorial notes,
dates and locations). This was all in an effort to
prevent classifiers from accessing superficial fea-
tures which are reliable indicators of the document
source and hence trivialise the satire detection pro-
cess.
It is important to note that the number of satiri-
cal news articles in the corpus is significantly less
than the number of true newswire articles. This
reflects an impressionistic view of the web: there
is far more true news content than satirical news
content.
The corpus is novel to this research,
and is publicly available for download at
http://www.csse.unimelb.edu.au/
research/lt/resources/satire/.
3 Method
3.1 Standard text classification approach
We take our starting point from topic-based text
classification (Dumais et al, 1998; Joachims,
1998) and sentiment classification (Turney, 2002;
Pang and Lee, 2008). State-of-the-art results in
both fields have been achieved using support vec-
1
The sites queried were satirewire.com,
theonion.com, newsgroper.com, thespoof.
com, brokennewz.com, thetoque.com,
bbspot.com, neowhig.org, humorfeed.com,
satiricalmuslim.com, yunews.com,
newsbiscuit.com.
tor machines (SVMs) and bag-of-words features.
We supplement the bag-of-words model with fea-
ture weighting, using the two methods described
below.
Binary feature weights: Under this scheme
all features are given the same weight, regard-
less of how many times they appear in each arti-
cle. The topic and sentiment classification exam-
ples cited found binary features gave better perfor-
mance than other alternatives.
Bi-normal separation feature scaling: BNS
(Forman, 2008) has been shown to outperform
other established feature representation schemes
on a wide range of text classification tasks. This
superiority is especially pronounced for collec-
tions with a low proportion of positive class in-
stances. Under BNS, features are allocated a
weight according to the formula:
|F
?1
(tpr)? F
?1
(fpr)|
where F
?1
is the inverse normal cumulative dis-
tribution function, tpr is the true positive rate
(P(feature|positive class)) and fpr is the false pos-
itive rate (P(feature|negative class)).
BNS produces the highest weights for features
that are strongly correlated with either the nega-
tive or positive class. Features that occur evenly
across the training instances are given the lowest
weight. This behaviour is particularly helpful for
features that correlate with the negative class in
a negatively-skewed classification task, so in our
case BNS should assist the classifier in making use
of features that identify true articles.
SVM classification is performed with SVM
light
(Joachims, 1999) using a linear kernel and the de-
fault parameter settings. Tokens are case folded;
currency amounts (e.g. $2.50), abbreviations (e.g.
U.S.A.), and punctuation sequences (e.g. a
comma, or a closing quote mark followed by a pe-
riod) are treated as separate features.
3.2 Targeted lexical features
This section describe three types of features in-
tended to embody characteristics of satire news
documents.
Headline features: Most of the articles in the
corpus have a headline as their first line. To a hu-
man reader, the vast majority of the satire docu-
ments in our corpus are immediately recognisable
as such from the headline alone, suggesting that
our classifiers may get something out of having the
162
headline contents explicitly identified in the fea-
ture vector. To this end, we add an additional fea-
ture for each unigram appearing on the first line
of an article. In this way the heading tokens are
represented twice: once in the overall set of uni-
grams in the article, and once in the set of heading
unigrams.
Profanity: true news articles very occasionally
include a verbal quote which contains offensive
language, but in practically all other cases it is in-
cumbent on journalists and editors to keep their
language ?clean?. A review of the corpus shows
that this is not the case with satirical news, which
occasionally uses profanity as a humorous device.
Let P be a binary feature indicating whether
or not an article contains profanity, as determined
by the Regexp::Common::profanity Perl
module.
2
Slang: As with profanity, it is intuitively true
that true news articles tend to avoid slang. An im-
pressionistic review of the corpus suggests that in-
formal language is much more common to satirical
articles. We measure the informality of an article
as:
i
def
=
1
|T |
?
t?T
s(t)
where T is the set of unigram tokens in the article
and s is a function taking the value 1 if the token
has a dictionary definition marked as slang and 0
if it does not.
It is important to note that this measure of ?in-
formality? is approximate at best. We do not at-
tempt, e.g., to disambiguate the sense of individ-
ual word terms to tell whether the slang sense of
a word is the one intended. Rather, we simply
check to see if each word has a slang usage inWik-
tionary.
3
A continuous feature is set to the value of i for
each article. Discrete features highi and lowi are
set as:
highi
def
=
{
1 v >
?
i + 2?;
0
lowi
def
=
{
1 v <
?
i? 2?;
0
where
?
i and ? are, respectively, the mean and stan-
dard deviation of i across all articles.
2
http://search.cpan.org/perldoc?
Regexp::Common::profanity
3
http://www.wiktionary.org
3.3 Semantic validity
Lexical approaches are clearly inadequate if we
assume that good satirical news articles tend to
emulate real news in tone, style, and content.
What is needed is an approach that captures the
document semantics.
One common device in satire news articles is
absurdity, in terms of describing well-known indi-
viduals in unfamiliar settings which parody their
viewpoints or public profile. We attempt to cap-
ture this via validity, in the form of the relative fre-
quency of the particular combination of key partic-
ipants reported in the story. Our method identifies
the named entities in a given document and queries
the web for the conjunction of those entities. Our
expectation is that true news stories will have been
reported in various forums, and hence the number
of web documents which include the same com-
bination of entities will be higher than with satire
documents.
To implement this method, we first use the
Stanford Named Entity Recognizer
4
(Finkel et al,
2005) to identify the set of person and organisation
entities, E, from each article in the corpus.
From this, we estimate the validity of the com-
bination of entities in the article as:
v(E)
def
= |g(E)|
where g is the set of matching documents returned
by Google using a conjunctive query. We antici-
pate that v will have two potentially useful prop-
erties: (1) it will be relatively lower when E in-
cludes made-up entity names such as Hitler Com-
memoration Institute, found in one satirical corpus
article; and (2) it will be relatively lower when E
contains unusual combinations of entities such as,
for example, those in the satirical article beginning
Missing Brazilian balloonist Padre spotted strad-
dling Pink Floyd flying pig.
We include both a continuous representation of
v for each article, in the form of log(v(E)), and
discrete variants of the feature, based on the same
methodology as for highi and lowi.
4 Results
The results for our classifiers over the satire cor-
pus are shown in Table 2. The baseline is a naive
classifier that assigns all instances to the positive
4
http://nlp.stanford.edu/software/
CRF-NER.shtml
163
(?article?SATIRE??) P R F
all-positive baseline 0.063 1.000 0.118
BIN 0.943 0.500 0.654
BIN+lex 0.945 0.520 0.671
BIN+val 0.943 0.500 0.654
BIN+all 0.945 0.520 0.671
BNS 0.944 0.670 0.784
BNS+lex 0.957 0.660 0.781
BNS+val 0.945 0.690 0.798
BNS+all 0.958 0.680 0.795
Table 2: Results for satire detection (P = preci-
sion, R = recall, and F = F-score) for binary un-
igram features (BIN) and BNS unigram features
(BNS), optionally using lexical (lex), validity (val)
or combined lexical and validity (all) features
class (i.e. SATIRE). An SVM classifier with simple
binary unigram word features provides a standard
text classification benchmark.
All of the classifiers easily outperform the base-
line. This is to be expected given the low pro-
portion of positive instances in the corpus. The
benchmark classifier has very good precision, but
recall of only 0.500. Adding the heading, slang,
and profanity features provides a small improve-
ment in both precision and recall.
Moving to BNS feature scaling keeps the very
high precision and increases the recall to 0.670.
Adding in the heading, slang and profanity lexical
features (?+lex?) actually decreases the F-score
slightly, but adding the validity features (?+val?)
provides a near 2 point F-score increase, resulting
in the best overall F-score of 0.798.
All of the BNS scores achieve statistically
significant improvements over the benchmark in
terms of F-score (using approximate randomisa-
tion, p < 0.05). The 1-2% gains given by adding
in the various feature types are not statistically sig-
nificant due to the small number of satire instances
concerned.
All of the classifiers achieve very high precision
and considerably lower recall. Error analysis sug-
gests that the reason for the lower recall is subtler
satire articles, which require detailed knowledge
of the individuals to be fully appreciated as satire.
While they are not perfect, however, the classi-
fiers achieve remarkably high performance given
the superficiality of the features used.
5 Conclusions and future work
This paper has introduced a novel task to computa-
tional linguistics and machine learning: determin-
ing whether a newswire article is ?true? or satiri-
cal. We found that the combination of SVMs with
BNS feature scaling achieves high precision and
lower recall, and that the inclusion of the notion of
?validity? achieves the best overall F-score.
References
Ion Androutsopoulos, John Koutsias, Konstantinos V.
Chandrinos, George Paliouras, and Constantine D.
Spyropoulos. 2000. An evaluation of Naive
Bayesian anti-spam filtering. In Proceedings of the
11th European Conference on Machine Learning,
pages 9?17, Barcelona, Spain.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algo-
rithms and representations for text categorization.
In Proceedings of the Seventh International Confer-
ence on Information and Knowledge Management,
pages 148?155, New York, USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 363?370, Ann Arbor, USA.
George Forman. 2008. BNS scaling: An improved
representation over TF-IDF for SVM text classifi-
cation. In Proceedings of the 17th International
Conference on Information and Knowledge Man-
agement, pages 263?270, Napa Valley, USA.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many rele-
vant features. In Proceedings of the 10th European
Conference on Machine Learning, pages 137?142,
Chemnitz, Germany.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Sch?olkopf, Christopher J. C. Burges, and Alexan-
der J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 169?184. MIT
Press, Cambridge, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 417?424, Philadelphia, USA.
164
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 67?76,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Bootstrapping Deep Lexical Resources: Resources for Courses
Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
tim@csse.unimelb.edu.au
Abstract
We propose a range of deep lexical acqui-
sition methods which make use of mor-
phological, syntactic and ontological lan-
guage resources to model word similarity
and bootstrap from a seed lexicon. The
different methods are deployed in learn-
ing lexical items for a precision gram-
mar, and shown to each have strengths and
weaknesses over different word classes. A
particular focus of this paper is the rela-
tive accessibility of different language re-
source types, and predicted ?bang for the
buck? associated with each in deep lexical
acquisition applications.
1 Introduction
Over recent years, computational linguistics has
benefitted considerably from advances in statisti-
cal modelling and machine learning, culminating
in methods capable of deeper, more accurate au-
tomatic analysis, over a wider range of languages.
Implicit in much of this work, however, has been
the existence of deep language resources (DLR
hereafter) of ever-increasing linguistic complexity,
including lexical semantic resources (e.g. Word-
Net and FrameNet), precision grammars (e.g. the
English Resource Grammar and the various Par-
Gram grammars) and richly-annotated treebanks
(e.g. PropBank and CCGbank).
Due to their linguistic complexity, DLRs are in-
variably constructed by hand and thus restricted in
size and coverage. Our aim in this paper is to de-
velop general-purpose automatic methods which can
be used to automatically expand the coverage of an
existing DLR, through the process of deep lexical
acquisition (DLA hereafter).
The development of DLRs can be broken down
into two basic tasks: (1) design of a data represen-
tation to systematically capture the generalisations
and idiosyncracies of the dataset of interest (system
design); and (2) classification of data items accord-
ing to the predefined data representation (data clas-
sification). In the case of a deep grammar, for exam-
ple, system design encompasses the construction of
the system of lexical types, templates, and/or phrase
structure rules, and data classification corresponds
to the determination of the lexical type(s) each in-
dividual lexeme conforms to. DLA pertains to the
second of these tasks, in automatically mapping a
given lexeme onto a pre-existing system of lexical
types associated with a DLR.
We propose to carry out DLA through a boot-
strap process, that is by employing some notion of
word similarity, and learning the lexical types for a
novel lexeme through analogy with maximally sim-
ilar word(s) for which we know the lexical types. In
this, we are interested in exploring the impact of dif-
ferent secondary language resources (LRs) on DLA,
and estimating how successfully we can expect to
learn new lexical items from a range of LR types.
That is, we estimate the expected DLA ?bang for the
buck? from a range of secondary LR types of vary-
ing size and complexity. As part of this, we look
at the relative impact of different LRs on DLA for
different open word classes, namely nouns, verbs,
adjectives and adverbs.
We demonstrate the proposed DLA methods rel-
ative to the English Resource Grammar (see Sec-
tion 2.1), and in doing so assume the lexical types
of the target DLR to be syntactico-semantic in na-
ture. For example, we may predict that the word
dog has a usage as an intransitive countable noun
(n intr le,1 cf. The dog barked), and also as a
transitive verb (v np trans le, cf. It dogged my
every step).
A secondary interest of this paper is the consid-
eration of how well we could expect to perform
DLA for languages of differing density, from ?low-
1All example lexical types given in this paper are taken di-
rectly from the English Resource Grammar ? see Section 2.1.
67
density? languages (such as Walpiri or Uighur) for
which we have limited LRs, to ?high-density? lan-
guages (such as English or Japanese) for which we
have a wide variety of LRs. To this end, while we ex-
clusively target English in this paper, we experiment
with a range of LRs of varying complexity and type,
including morphological, syntactic and ontological
LRs. Note that we attempt to maintain consistency
across the feature sets associated with each, to make
evaluation as equitable as possible.
The remainder of this paper is structured as fol-
lows. Section 2 outlines the process of DLA and re-
views relevant resources and literature. Sections 3,
4 and 5 propose a range of DLA methods based on
morphology, syntax and ontological semantics, re-
spectively. Section 6 evaluates the proposed meth-
ods relative to the English Resource Grammar.
2 Task Outline
This research aims to develop methods for DLA
which can be run automatically given: (a) a pre-
existing DLR which we wish to expand the cover-
age of, and (b) a set of secondary LRs/preprocessors
for that language. The basic requirements to achieve
this are the discrete inventory of lexical types in the
DLR, and a pre-classification of each secondary LR
(e.g. as a corpus or wordnet, to determine what set of
features to employ). Beyond this, we avoid making
any assumptions about the language family or DLR
type.
The DLA strategy we propose in this research is
to use secondary LR(s) to arrive at a feature sig-
nature for each lexeme, and map this onto the sys-
tem of choice indirectly via supervised learning, i.e.
observation of the correlation between the feature
signature and classification of bootstrap data. This
methodology can be applied to unannotated corpus
data, for example, making it possible to tune a lex-
icon to a particular domain or register as exempli-
fied in a particular repository of text. As it does not
make any assumptions about the nature of the sys-
tem of lexical types, we can apply it fully automat-
ically to any DLR and feed the output directly into
the lexicon without manual intervention or worry of
misalignment. This is a distinct advantage when the
inventory of lexical types is continually undergoing
refinement, as is the case with the English Resource
Grammar (see below).
A key point of interest in this paper is the investi-
gation of the relative ?bang for the buck? when dif-
ferent types of LR are used for DLA. Crucially, we
investigate only LRs which we believe to be plausi-
bly available for languages of varying density, and
aim to minimise assumptions as to the pre-existence
of particular preprocessing tools. The basic types of
resources and tools we experiment with in this paper
are detailed in Table 1.
Past research on DLA falls into two basic cat-
egories: expert system-style DLA customised to
learning particular linguistic properties, and DLA
via resource translation. In the first instance, a spe-
cialised methodology is proposed to (automatically)
learn a particular linguistic property such as verb
subcategorisation (e.g. Korhonen (2002)) or noun
countability (e.g. Baldwin and Bond (2003a)), and
little consideration is given to the applicability of
that method to more general linguistic properties. In
the second instance, we take one DLR and map it
onto another to arrive at the lexical information in
the desired format. This can take the form of a one-
step process, in mining lexical items directly from
a DLR (e.g. a machine-readable dictionary (Sanfil-
ippo and Poznan?ski, 1992)), or two-step process in
reusing an existing system to learn lexical properties
in one format and then mapping this onto the DLR
of choice (e.g. Carroll and Fang (2004) for verb sub-
categorisation learning).
There have also been instances of more gen-
eral methods for DLA, aligned more closely with
this research. Fouvry (2003) proposed a method
of token-based DLA for unification-based precision
grammars, whereby partially-specified lexical fea-
tures generated via the constraints of syntactically-
interacting words in a given sentence context, are
combined to form a consolidated lexical entry for
that word. That is, rather than relying on indi-
rect feature signatures to perform lexical acquisition,
the DLR itself drives the incremental learning pro-
cess. Also somewhat related to this research is the
general-purpose verb feature set proposed by Joanis
and Stevenson (2003), which is shown to be appli-
cable in a range of DLA tasks relating to English
verbs.
2.1 English Resource Grammar
All experiments in this paper are targeted at the
English Resource Grammar (ERG; Flickinger
(2002), Copestake and Flickinger (2000)). The ERG
is an implemented open-source broad-coverage
precision Head-driven Phrase Structure Grammar
68
Secondary LR type Description Preprocessor(s)
Word list??? List of words with basic POS ?
Morphological lexicon? Derivational and inflectional word relations ?
Compiled corpus??? Unannotated text corpus POS tagger??
Chunk parser?
Dependency parser?
WordNet-style ontology? Lexical semantic word linkages ?
Table 1: Secondary LR and tool types targeted in this research (??? = high expectation of availability for a
given language; ?? = medium expectation of availability; ? = low expectation of availability)
(HPSG) developed for both parsing and generation.
It contains roughly 10,500 lexical items, which,
when combined with 59 lexical rules, compile out
to around 20,500 distinct word forms.2 Each lex-
ical item consists of a unique identifier, a lexical
type (one of roughly 600 leaf types organized into
a type hierarchy with a total of around 4,000 types),
an orthography, and a semantic relation. The gram-
mar also contains 77 phrase structure rules which
serve to combine words and phrases into larger con-
stituents. Of the 10,500 lexical items, roughly 3,000
are multiword expressions.
To get a basic sense of the syntactico-semantic
granularity of the ERG, the noun hierarchy, for ex-
ample, is essentially a cross-classification of count-
ability/determiner co-occurrence, noun valence and
preposition selection properties. For example, lex-
ical entries of n mass count ppof le type can
be either countable or uncountable, and optionally
select for a PP headed by of (example lexical items
are choice and administration).
As our target lexical type inventory for DLA, we
identified all open-class lexical types with at least
10 lexical entries, under the assumption that: (a)
the ERG has near-complete coverage of closed-class
lexical entries, and (b) the bulk of new lexical entries
will correspond to higher-frequency lexical types.
This resulted in the following breakdown:3
2All statistics and analysis relating to the ERG in this paper
are based on the version of 11 June, 2004.
3Note that all results are over simplex lexemes only, and that
we choose to ignore multiword expressions in this research.
Word class Lexical types Lexical items
Noun 28 3,032
Verb 39 1,334
Adjective 17 1,448
Adverb 26 721
Total 110 5,675
Note that it is relatively common for a lexeme to
occur with more than one lexical type in the ERG:
22.6% of lexemes have more than one lexical type,
and the average number of lexical types per lexeme
is 1.12.
In evaluation, we assume we have prior knowl-
edge of the basic word classes each lexeme belongs
to (i.e. noun, verb, adjective and/or adverb), infor-
mation which could be derived trivially from pre-
existing shallow lexicons and/or the output of a tag-
ger.
Recent development of the ERG has been tightly
coupled with treebank annotation, and all major ver-
sions of the grammar are deployed over a common
set of treebank data to help empirically trace the
evolution of the grammar and retrain parse selection
models (Oepen et al, 2002). We treat this as a held-
out dataset for use in analysis of the token frequency
of each lexical item, to complement analysis of type-
level learning performance (see Section 6).
2.2 Classifier design
The proposed procedure for DLA is to generate a
feature signature for each word contained in a given
secondary LR, take the subset of lexemes contained
in the original DLR as training data, and learn lex-
ical items for the remainder of the lexemes through
supervised learning. In order to maximise compara-
bility between the results for the different DLRs, we
employ a common classifier design wherever possi-
ble (in all cases other than ontology-based DLA),
69
using TiMBL 5.0 (Daelemans et al, 2003); we
used the IB1 k-NN learner implementation within
TiMBL, with k = 9 throughout.4 We additionally
employ the feature selection method of Baldwin and
Bond (2003b), which generates a combined ranking
of all features in descending order of ?informative-
ness? and skims off the top-N features for use in
classification; N was set to 100 in all experiments.
As observed above, a significant number of lex-
emes in the ERG occur in multiple lexical items. If
we were to take all lexical type combinations ob-
served for a single lexeme, the total number of lex-
ical ?super?-types would be 451, of which 284 are
singleton classes. Based on the sparseness of this
data and also the findings of Baldwin and Bond
(2003b) over a countability learning task, we choose
to carry out DLA via a suite of 110 binary classifiers,
one for each lexical type.
We deliberately avoid carrying out extensive fea-
ture engineering over a given secondary LR, choos-
ing instead to take a varied but simplistic set of fea-
tures which is parallelled as much as possible be-
tween LRs (see Sections 3?5 for details). We addi-
tionally tightly constrain the feature space to a max-
imum of 3,900 features, and a maximum of 50 fea-
ture instances for each feature type; in each case,
the 50 feature instances are selected by taking the
features with highest saturation (i.e. the highest ra-
tio of non-zero values) across the full lexicon. This
is in an attempt to make evaluation across the differ-
ent secondary LRs as equitable as possible, and get
a sense of the intrinsic potential of each secondary
LR in DLA. Each feature instance is further trans-
lated into two feature values: the raw count of the
feature instance for the target word in question, and
the relative occurrence of the feature instance over
all target word token instances.
One potential shortcoming of our classifier archi-
tecture is that a given word can be negatively clas-
sified by all unit binary classifiers and thus not as-
signed any lexical items. In this case, we fall back
on the majority-class lexical type for each word class
the word has been pre-identified as belonging to.
4We also experimented with bsvm and SVMLight, and a
maxent toolkit, but found TiMBL to be superior overall, we hy-
pothesise due to the tight integration of continuous features in
TiMBL.
3 Morphology-based Deep Lexical
Acquisition
We first perform DLA based on the following mor-
phological LRs: (1) word lists, and (2) morphologi-
cal lexicons with a description of derivational word
correspondences. Note that in evaluation, we pre-
suppose that we have access to word lemmas al-
though in the first instance, it would be equally pos-
sible to run the method over non-lemmatised data.5
3.1 Character n-grams
In line with our desire to produce DLA methods
which can be deployed over both low- and high-
density languages, our first feature representation
takes a simple word list and converts each lexeme
into a character n-gram representation.6 In the case
of English, we generated all 1- to 6-grams for each
lexeme, and applied a series of filters to: (1) filter out
all n-grams which occurred less than 3 times in the
lexicon data; and (2) filter out all n-grams which oc-
cur with the same frequency as larger n-grams they
are proper substrings of. We then select the 3,900
character n-grams with highest saturation across the
lexicon data (see Section 2.2).
The character n-gram-based classifier is the sim-
plest of all classifiers employed in this research, and
can be deployed on any language for which we have
a word list (ideally lemmatised).
3.2 Derviational morphology
The second morphology-based DLA method makes
use of derivational morphology and analysis of the
process of word formation. As an example of how
derivational information could assist DLA, know-
ing that the noun achievement is deverbal and in-
corporates the -ment suffix is a strong predictor of
it being optionally uncountable and optionally se-
lecting for a PP argument (i.e. being of lexical type
n mass count ppof le).
We generate derivational morphological features
for a given lexeme by determining its word clus-
ter in CATVAR7 (Habash and Dorr, 2003) and then
for each sister lexeme (i.e. lexeme occurring in the
5Although this would inevitably lose lexical generalisations
among the different word forms of a given lemma.
6We also experimented with syllabification, but found the
character n-grams to produce superior results.
7In the case that the a given lemma is not in CATVAR, we
attempt to dehyphenate and then deprefix the word to find a
match, failing which we look for the lexeme of smallest edit
distance.
70
same cluster as the original lexeme with the same
word stem), determine if there is a series of edit
operations over suffixes and prefixes which maps
the lexemes onto one another. For each sister lex-
eme where such a correspondence is found to ex-
ist, we output the nature of the character transforma-
tion and the word classes of the lexemes involved.
E.g., the sister lexemes for achievementN in CAT-
VAR are achieveV, achieverN, achievableAdj and
achievabilityN; the mapping between achievementN
and achieverN, e.g., would be analysed as:
N ?ment$ ? N +r$
Each such transformation is treated as a single fea-
ture.
We exhaustively generate all such transformations
for each lexeme, and filter the feature space as for
character n-grams above.
Clearly, LRs which document derivational mor-
phology are typically only available for high-density
languages. Also, it is worth bearing in mind that
derivational morphology exists in only a limited
form for certain language families, e.g. agglutinative
languages.
4 Syntax-based Deep Lexical Acquisition
Syntax-based DLA takes a raw text corpus and pre-
processes it with either a tagger, chunker or depen-
dency parser. It then extracts a set of 39 feature types
based on analysis of the token occurrences of a given
lexeme, and filters over each feature type to produce
a maximum of 50 feature instances of highest satura-
tion (e.g. if the feature type is the word immediately
proceeding the target word, the feature instances are
the 50 words which proceed the most words in our
lexicon). The feature signature associated with a
word for a given preprocessor type will thus have
a maximum of 3,900 items (39? 50? 2).8
4.1 Tagging
The first and most basic form of syntactic pre-
processing is part-of-speech (POS) tagging. For
our purposes, we use a Penn treebank-style tagger
custom-built using fnTBL 1.0 (Ngai and Florian,
2001), and further lemmatise the output of the tagger
using morph (Minnen et al, 2000).
8Note that we will have less than 50 feature instances for
some feature types, e.g. the POS tag of the target word, given
that the combined size of the Penn POS tagset is 36 elements
(not including punctuation).
The feature types used with the tagger are detailed
in Table 2, where the position indices are relative to
the target word (e.g. the word at position ?2 is two
words to the left of the target word, and the POS
tag at position 0 is the POS of the target word). All
features are relative to the POS tags and words in the
immediate context of each token occurrence of the
target word. ?Bi-words? are word bigrams (e.g. bi-
word (1, 3) is the bigram made up of the words one
and three positions to the right of the target word);
?bi-tags? are, similarly, POS tag bigrams.
4.2 Chunking
The second form of syntactic preprocessing, which
builds directly on the output of the POS tagger, is
CoNLL 2000-style full text chunking (Tjong Kim
Sang and Buchholz, 2000). The particular chun-
ker we use was custom-built using fnTBL 1.0 once
again, and operates over the lemmatised output of
the POS tagger.
The feature set for the chunker output includes a
subset of the POS tagger features, but also makes
use of the local syntactic structure in the chunker in-
put in incorporating both intra-chunk features (such
as modifiers of the target word if it is the head of a
chunk, or the head if it is a modifier) and inter-chunk
features (such as surrounding chunk types when the
target word is chunk head). See Table 2 for full de-
tails.
Note that while chunk parsers are theoretically
easier to develop than full phrase-structure or tree-
bank parsers, only high-density languages such as
English and Japanese have publicly available chunk
parsers.
4.3 Dependency parsing
The third and final form of syntactic preprocessing
is dependency parsing, which represents the pinna-
cle of both robust syntactic sophistication and inac-
cessibility for any other than the highest-density lan-
guages.
The particular dependency parser we use is
RASP9 (Briscoe and Carroll, 2002), which outputs
head?modifier dependency tuples and further classi-
fies each tuple according to a total of 14 relations;
RASP also outputs the POS tag of each word to-
ken. As our features, we use both local word and
POS features, for comparability with the POS tagger
9RASP is, strictly speaking, a full syntactic parser, but we
use it in dependency parser mode
71
Feature type Positions/description Total
TAGGER 39
POS tag (?4,?3,?2,?1, 0, 1, 2, 3, 4) 9
Word (?4,?3,?2,?1, 1, 2, 3, 4) 8
POS bi-tag ( (?4,?1), (?4, 0), (?3,?2), (?3,?1), (?3, 0), (?2,?1), (?2, 0),
(?1, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3) ) 16
Bi-word ((?3,?2), (?3,?1), (?2,?1), (1, 2), (1, 3), (2, 3)) 6
CHUNKER 39
Modifierhead Chunk heads when target word is modifier 1
Modifierchunk Chunk types when target word is modifier 1
Modifieeword Modifiers when target word is chunk head 1
ModifieePOS POS tag of modifiers when target word is chunk head 1
Modifieeword+POS Word + POS tag of modifiers when target word is chunk head 1
POS tag (?3,?2,?1, 0, 1, 2, 3) 7
Word (?3,?2,?1, 1, 2, 3) 6
Chunk (?4,?3,?2,?1, 0, 1, 2, 3, 4) 9
Chunk head (?3,?2,?1, 1, 2, 3) 6
Bi-chunk ((?2,?1), (?2, 0), (?1, 0), (0, 1), (0, 2), (1, 2)) 6
DEPENDENCY PARSER 39
POS tag (?2,?1, 0, 1, 2) 5
Word (?2,?1, 1, 2) 4
Conjword Words the target word coordinates with 1
ConjPOS POS of words the target word coordinates with 1
Head Head word when target word modifier in dependency relation (? 14) 14
Modifier Modifier when target word head of dependency relation (? 14) 14
Table 2: Feature types used in syntax-based DLA for the different preprocessors
and chunker, and also dependency-derived features,
namely the modifier of all dependency tuples the tar-
get word occurs as head of, and conversely, the head
of all dependency tuples the target word occurs as
modifier in, along with the dependency relation in
each case. See Table 2 for full details.
4.4 Corpora
We ran the three syntactic preprocessors over a to-
tal of three corpora, of varying size: the Brown cor-
pus (?460K tokens) and Wall Street Journal corpus
(?1.2M tokens), both derived from the Penn Tree-
bank (Marcus et al, 1993), and the written compo-
nent of the British National Corpus (?98M tokens:
Burnard (2000)). This selection is intended to model
the effects of variation in corpus size, to investigate
how well we could expect syntax-based DLA meth-
ods to perform over both smaller and larger corpora.
Note that the only corpus annotation we make use
of is sentence tokenisation, and that all preproces-
sors are run automatically over the raw corpus data.
This is in an attempt to make the methods maximally
applicable to lower-density languages where anno-
tated corpora tend not to exist but there is at least the
possibility of accessing raw text collections.
5 Ontology-based Deep Lexical
Acquisition
The final DLA method we explore is based on the
hypothesis that there is a strong correlation between
the semantic and syntactic similarity of words, a
claim which is best exemplified in the work of Levin
(1993) on diathesis alternations. In our case, we
take word similarity as given and learn the syntactic
behaviour of novel words relative to semantically-
similar words for which we know the lexical types.
We use WordNet 2.0 (Fellbaum, 1998) to determine
word similarity, and for each sense of the target
word in WordNet: (1) construct the set of ?seman-
tic neighbours? of that word sense, comprised of all
synonyms, direct hyponyms and direct hypernyms;
and (2) take a majority vote across the lexical types
of the semantic neighbours which occur in the train-
ing data. Note that this diverges from the learning
paradigm adopted for the morphology- and syntax-
based DLA methods in that we use a simple voting
strategy rather than relying on an external learner to
carry out the classification. The full set of lexical
entries for the target word is generated by taking the
union of the majority votes across all senses of the
word, such that a polysemous lexeme can potentially
give rise to multiple lexical entries. This learning
72
procedure is based on the method used by van der
Beek and Baldwin (2004) to learn Dutch countabil-
ity.
As for the suite of binary classifiers, we fall back
on the majority class lexical type as the default in
the instance that a given lexeme is not contained in
WordNet 2.0 or no classification emerges from the
set of semantic neighbours. It is important to re-
alise that WordNet-style ontologies exist only for the
highest-density languages, and that this method will
thus have very limited language applicability.
6 Evaluation
We evaluate the component methods over the 5,675
open-class lexical items of the ERG described in
Section 2.1 using 10-fold stratified cross-validation.
In each case, we calculate the type precision (the
proportion of correct hypothesised lexical entries)
and type recall (the proportion of gold-standard lex-
ical entries for which we get a correct hit), which
we roll together into the type F-score (the harmonic
mean of the two) relative to the gold-standard ERG
lexicon. We also measure the token accuracy for
the lexicon derived from each method, relative to
the Redwoods treebank of Verbmobil data associ-
ated with the ERG (see Section 2.1).10 The token ac-
curacy represents a weighted version of type preci-
sion, relative to the distribution of each lexical item
in a representative text sample, and provides a crude
approximation of the impact of each DLA method
on parser coverage. That is, it gives more credit for a
method having correctly hypothesised a commonly-
occurring lexical item than a low-frequency lexical
item, and no credit for having correctly identified a
lexical item not occurring in the corpus.
The overall results are presented in Figure 1,
which are then broken down into the four open
word classes in Figures 2?5. The baseline method
(Base) in each case is a simple majority-class classi-
fier, which generates a unique lexical item for each
lexeme pre-identified as belonging to a given word
class of the following type:
Word class Majority-class lexical type
Noun n intr le
Verb v np trans le
Adjective adj intrans le
Adverb adv int vp le
10Note that the token accuracy is calculated only over the
open-class lexical items, not the full ERG lexicon.
In each graph, we present the type F-score and to-
ken accuracy for each method, and mark the best-
performing method in terms of each of these evalua-
tion measures with a star (?). The results for syntax-
based DLA (SPOS, SCHUNK and SPARSE) are based
on the BNC in each case. We return to investigate
the impact of corpus size on the performance of the
syntax-based methods below.
Looking first at the combined results over all lex-
ical types (Figure 1), the most successful method
in terms of type F-score is syntax-based DLA,
with chunker-based preprocessing marginally out-
performing tagger- and parser-based preprocessing
(type F-score = 0.641). The most successful method
in terms of token accuracy is ontology-based DLA
(token accuracy = 0.544).
The figures for token accuracy require some qual-
ification: ontology-based DLA tends to be liberal
in its generation of lexical items, giving rise to
over 20% more lexical items than the other meth-
ods (7,307 vs. 5-6000 for the other methods) and
proportionately low type precision. This correlates
with an inherent advantage in terms of token ac-
curacy, which we have no way of balancing up in
our token-based evaluation, as the treebank data of-
fers no insight into the true worth of false nega-
tive lexical items (i.e. have no way of distinguishing
between unobserved lexical items which are plain
wrong from those which are intuitively correct and
could be expected to occur in alternate sets of tree-
bank data). We leave investigation of the impact of
these extra lexical items on the overall parser perfor-
mance (in terms of chart complexity and parse se-
lection) as an item for future research.
The morphology-based DLA methods were
around baseline performance overall, with charac-
ter n-grams marginally more successful than deriva-
tional morphology in terms of both type F-score and
token accuracy.
Turning next to the results for the proposed meth-
ods over nouns, verbs, adjectives and adverbs (Fig-
ures 2?5, respectively), we observe some interest-
ing effects. First, morphology-based DLA hovers
around baseline performance for all word classes
except adjectives, where character n-grams produce
the highest F-score of all methods, and nouns, where
derivational morphology seems to aid DLA slightly
(providing weak support for our original hypothesis
in Section 3.2 relating to deverbal nouns and affixa-
tion).
73
 0
 0.2
 0.4
 0.6
 0.8
 1
OntBase  0
 0.2
 0.4
 0.6
 0.8
 1
Typ
e F-s
core
Tok
en a
ccur
acy
Method S    PARSES    CHUNKS    POSM    DERIVM    CHAR
* *
Figure 1: Results for the proposed deep lexical ac-
quisition methods over ALL lexical types
 0
 0.2
 0.4
 0.6
 0.8
 1
OntBase  0
 0.2
 0.4
 0.6
 0.8
 1
Tok
en a
ccur
acy
MethodM    DERIVM    CHAR S    PARSES    CHUNKS    POS
* *
Typ
e F-s
core
Figure 2: Results for the proposed deep lexical ac-
quisition methods over NOUN lexical types
 0
 0.2
 0.4
 0.6
 0.8
 1
OntBase  0
 0.2
 0.4
 0.6
 0.8
 1
Tok
en a
ccur
acy
MethodM    DERIVM    CHAR S    PARSES    CHUNKS    POS
*
*
Typ
e F-s
core
Figure 3: Results for the proposed deep lexical ac-
quisition methods over VERB lexical types
 0
 0.2
 0.4
 0.6
 0.8
 1
OntBase  0
 0.2
 0.4
 0.6
 0.8
 1
Tok
en a
ccur
acy
MethodM    DERIVM    CHAR S    PARSES    CHUNKS    POS
**
Typ
e F-s
core
Figure 4: Results for the proposed deep lexical ac-
quisition methods over ADJECTIVE lexical types
 0
 0.2
 0.4
 0.6
 0.8
 1
OntBase  0
 0.2
 0.4
 0.6
 0.8
 1
Tok
en a
ccur
acy
MethodM    DERIVM    CHAR S    PARSES    CHUNKS    POS
*
*Type
 F-sc
ore
Figure 5: Results for the proposed deep lexical ac-
quisition methods over ADVERB lexical types
 0
 0.2
 0.4
 0.6
 0.8
 1
BNCWSJBrown  0
 0.2
 0.4
 0.6
 0.8
 1
Typ
e F-
sco
re (
  )
Tok
en a
ccu
rac
y (  
)
Corpus
Noun
Verb
Adj
Adv
o
erb
Adj
F-sc
ore
Tok
en a
cc
Figure 6: Results for the syntax-based deep lexical
acquisition methods over corpora of differing size
Note: Base = baseline, MCHAR = morphology-based DLA with character n-grams, MDERIV = derivational
morphology-based DLA, SPOS = syntax-based DLA with POS tagging, SCHUNK = syntax-based DLA with
chunking, SPARSE = syntax-based DLA with dependency parsing, and Ont = ontology-based DLA
74
Syntax-based DLA leads to the highest type F-
score for nouns, verbs and adverbs, and the highest
token accuracy for adjectives and adverbs. The dif-
ferential in results between syntax-based DLA and
the other methods is particularly striking for ad-
verbs, with a maximum type F-score of 0.544 (for
chunker-based preprocessing) and token accuracy of
0.340 (for tagger-based preprocessing), as compared
to baseline figures of 0.471 and 0.017 respectively.
There is relatively little separating the three styles
of preprocessing in syntax-based DLA, although
chunker-based preprocessing tends to have a slight
edge in terms of type F-score, and tagger-based pre-
processing generally produces the highest token ac-
curacy.11 This suggests that access to a POS tagger
for a given language is sufficient to make syntax-
based DLA work, and that syntax-based DLA thus
has moderately high applicability across languages
of different densities.
Ontology-based DLA is below baseline in terms
of type F-score for all word classes, but results in
the highest token accuracy of all methods for nouns
and verbs (although this finding must be taken with
a grain of salt, as noted above).
Another noteworthy feature of Figures 2?5 is the
huge variation in absolute performance across the
word classes: adjectives are very predictable, with a
majority class-based baseline type F-score of 0.832
and token accuracy of 0.847; adverbs, on the other
hand, are similar to verbs and nouns in terms of their
baseline type F-score (at 0.471), but the adverbs that
occur commonly in corpus data appear to belong to
less-populated lexical types (as seen in the baseline
token accuracy of a miniscule 0.017). Nouns appear
the hardest to learn in terms of the relative incre-
ment in token accuracy over the baseline. Verbs are
extremely difficult to get right at the type level, but it
appears that ontology-based DLA is highly adept at
getting the commonly-occurring lexical items right.
To summarise these findings, adverbs seem to
benefit the most from syntax-based DLA. Adjec-
tives, on the other hand, can be learned most effec-
tively from simple character n-grams, i.e. similarly-
spelled adjectives tend to have similar syntax, a
somewhat surprising finding. Nouns are surpris-
ingly hard to learn, but seem to benefit to some de-
gree from corpus data and also ontological similar-
ity. Lastly, verbs pose a challenge to all methods
11This trend was observed across all three corpora, although
we do no present the full results here.
at the type level, but ontology-based DLA seems to
be able to correctly predict the commonly-occurring
lexical entries.
Finally, we examine the impact of corpus size on
the performance of syntax-based DLA with tagger-
based preprocessing.12 In Figure 6, we examine
the relative change in type F-score and token ac-
curacy across the four word classes as we increase
the corpus size (from 0.5m words to 1m and fi-
nally 100m words, in the form of the Brown cor-
pus, WSJ corpus and BNC, respectively). For verbs
and adjectives, there is almost no change in either
type F-score or token accuracy when we increase
the corpus size, whereas for nouns, the token ac-
curacy actually drops slightly. For adverbs, on the
other hand, the token accuracy jumps up from 0.020
to 0.381 when we increase the corpus size from
1m words to 100m words, while the type F-score
rises only slightly. It thus seems to be the case that
large corpora have a considerable impact on DLA
for commonly-occurring adverbs, but that for the
remaining word classes, it makes little difference
whether we have 0.5m or 100m words. This can
be interpreted either as evidence that modestly-sized
corpora are good enough to perform syntax-based
DLA over (which would be excellent news for low-
density languages!), or alternatively that for the sim-
plistic syntax-based DLA methods proposed here,
more corpus data is not the solution to achieving
higher performance.
Returning to our original question of the ?bang
for the buck? associated with individual LRs, there
seems to be no simple answer: simple word lists are
useful in learning the syntax of adjectives in particu-
lar, but offer little in terms of learning the other three
word classes. Morphological lexicons with deriva-
tional information are moderately advantageous in
learning the syntax of nouns but little else. A POS
tagger seems sufficient to carry out syntax-based
DLA, and the word class which benefits the most
from larger amounts of corpus data is adverbs, other-
wise the proposed syntax-based DLA methods don?t
seem to benefit from larger-sized corpora. Ontolo-
gies have the greatest impact on verbs and, to a lesser
degree, nouns. Ultimately, this seems to lend weight
to a ?horses for courses?, or perhaps ?resources for
courses? approach to DLA.
12The results for chunker- and parser-based preprocessing are
almost identical, and this omitted from the paper.
75
7 Conclusion
We have proposed three basic paradigms for deep
lexical acquisition, based on morphological, syntac-
tic and ontological language resources, and demon-
strated the effectiveness of each strategy at learn-
ing lexical items for the lexicon of a precision En-
glish grammar. We discovered surprising variation
in the results for the different DLA methods, with
each learning method performing particularly well
for at least one basic word class, but the best overall
methods being syntax- and ontology-based DLA.
The results presented in this paper are based on
one particular language (English) and a very spe-
cific style of DLR (a precision grammar, namely the
English Resource Grammar), so some caution must
be exercised in extrapolating the results too liberally
over new languages/DLA tasks. In future research,
we are interested in carrying out experiments over
other languages and alternate DLRs to determine
how well these results generalise and formulate al-
ternate strategies for DLA.
Acknowledgements
This material is based upon work supported in part by NTT
Communication Science Laboratories, Nippon Telegraph and
Telephone Corporation. We would like to thank the members
of the University of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this research.
References
Timothy Baldwin and Francis Bond. 2003a. Learning the
countability of English nouns from corpus data. In Proc. of
the 41st Annual Meeting of the ACL, pages 463?70, Sapporo,
Japan.
Timothy Baldwin and Francis Bond. 2003b. A plethora of
methods for learning English countability. In Proc. of the
2003 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2003), pages 73?80, Sapporo,
Japan.
Ted Briscoe and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd International
Conference on Language Resources and Evaluation (LREC
2002), pages 1499?1504, Las Palmas, Canary Islands.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
John Carroll and Alex Fang. 2004. The automatic acquisi-
tion of verb subcategorisations and their impact on the per-
formance of an HPSG parser. In Proc. of the First Inter-
national Joint Conference on Natural Language Processing
(IJCNLP-04), pages 107?14, Sanya City, China.
Ann Copestake and Dan Flickinger. 2000. An open-source
grammar development environment and broad-coverage En-
glish grammar using HPSG. In Proc. of the 2nd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2000), Athens, Greece.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2003. TiMBL: Tilburg Memory Based
Learner, version 5.0, Reference Guide. ILK Technical Re-
port 03-10.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Dan Flickinger. 2002. On building a more efficient grammar by
exploiting types. In Stephan Oepen, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors, Collaborative Language
Engineering. CSLI Publications, Stanford, USA.
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of Es-
sex.
Nizar Habash and Bonnie Dorr. 2003. CATVAR: A database
of categorial variations for English. In Proc. of the Ninth
Machine Translation Summit (MT Summit IX), pages 471?4,
New Orleans, USA.
Eric Joanis and Suzanne Stevenson. 2003. A general feature
space for automatic verb classification. In Proc. of the 10th
Conference of the EACL (EACL 2003), pages 163?70, Bu-
dapest, Hungary.
Anna Korhonen. 2002. Subcategorization Acquisition. Ph.D.
thesis, University of Cambridge.
Beth Levin. 1993. English Verb Classes and Alterations. Uni-
versity of Chicago Press, Chicago, USA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Ro-
bust, applied morphological generation. In Proceedings of
the first International Natural Language Genration Confer-
ence, Mitzpe Ramon, Israel.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2002. LinGO Redwoods: A rich
and dynamic treebank for HPSG. In Proc. of The First Work-
shop on Treebanks and Linguistic Theories (TLT2002), So-
zopol, Bulgaria.
Antonio Sanfilippo and Victor Poznan?ski. 1992. The acquisi-
tion of lexical knowledge from combined machine-readable
dictionary sources. In Proc. of the 3rd Conference on Ap-
plied Natural Language Processing (ANLP), pages 80?7,
Trento, Italy.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proc.
of the 4th Conference on Computational Natural Language
Learning (CoNLL-2000), Lisbon, Portugal.
Leonoor van der Beek and Timothy Baldwin. 2004. Crosslin-
gual countability classification with EuroWordNet. In Pa-
pers from the 14th Meeting of Computational Linguistics in
the Netherlands, pages 141?55, Antwerp, Belgium. Antwerp
Papers in Linguistics.
76
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, page 1,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Compositionality and Multiword Expressions: Six of One, Half a
Dozen of the Other?
Timothy Baldwin
Melbourne University
In this talk, I will investigate the relationship between compositionality and multiword expressions,
as part of which I will outline different approaches for formalising the notion of compositionality. I will
then briefly review computational methods that have been proposed for modelling compositionality, and
applications thereof. Finally, I will discuss possible future directions for modelling compositionality, and
present some preliminary results.
1
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54?61,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interpretation of Compound Nominalisations using Corpus and Web
Statistics
Jeremy Nicholson and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
and
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{jeremymn,tim}@csse.unimelb.edu.au
Abstract
We present two novel paraphrase tests for
automatically predicting the inherent se-
mantic relation of a given compound nom-
inalisation as one of subject, direct object,
or prepositional object. We compare these
to the usual verb?argument paraphrase test
using corpus statistics, and frequencies ob-
tained by scraping the Google search en-
gine interface. We also implemented a
more robust statistical measure than max-
imum likelihood estimation ? the con-
fidence interval. A significant reduction
in data sparseness was achieved, but this
alone is insufficient to provide a substan-
tial performance improvement.
1 Introduction
Compound nouns are a class of multiword expres-
sion (MWE) that have been of interest in recent
computational linguistic work, as any task with a
lexical semantic dimension (like machine transla-
tion or information extraction) must take into ac-
count their semantic markedness. A compound
noun is a sequence of two or more nouns compris-
ing an N? , for example, polystyrene garden-gnome.
The productivity of compound nouns makes their
treatment equally desirable and difficult. They ap-
pear frequently: more than 1% of the words in the
British National Corpus (BNC: Burnard (2000))
participate in noun compounds (Tanaka and Bald-
win, 2003). However, unestablished compounds
are common: almost 70% of compounds identi-
fied in the BNC co-occur with a frequency of only
one (Lapata and Lascarides, 2003).
Analysis of the entire space of compound nouns
has been hampered to some degree as the space de-
fies some regular set of predicates to define the im-
plicit semantics between a modifier and its head.
This semantic underspecification led early analy-
sis to be primarily of a semantic nature, but more
recent work has advanced into using syntax to pre-
dict the semantics, in the spirit of the study by
Levin (1993) on diathesis alternations.
In this work, we examine compound nominal-
isations, a subset of compound nouns where the
head has a morphologically?related verb. For
example, product replacement has an underlying
verbal head replace, whereas garden-gnome has
no such form. While compound nouns in gen-
eral have a set of semantic relationships between
the head and modifier that is potentially non-finite,
compound nominalisations are better defined, in
that the modifier fills a syntactic argument rela-
tion with respect to the head. For example, prod-
uct might fill the direct object slot of the verb
to replace for the compound above. Compound
nominalisations comprise a substantial minority of
compound nouns, with figures of about 35% being
observed (Grover et al, 2005; Nicholson, 2005).
We propose two novel paraphrases for a corpus
statistical approach to predicting the relationship
for a set of compound nominalisations, and inves-
tigate how using the World Wide Web as a cor-
pus alleviates the common phenomenon of data
sparseness, and how the volume of data impacts
on the classification results. We also examine
a more robust statistical approach to interpreta-
tion of the statistics than maximum likelihood es-
timates, called the confidence interval.
The rest of the paper is structured as follows: in
Section 2, we present a brief background for our
work, with a listing of our resources in Section 3.
We detail our proposed method in Section 4, the
corresponding results in Section 5, with a discus-
54
sion in Section 6 and a brief conclusion in Sec-
tion 7.
2 Background
2.1 Compound Noun Interpretation
Compound nouns were seminally and thoroughly
analysed by Levi (1978), who hand?constructs a
nine?way set of semantic relations that she identi-
fies as broadly defining the observed relationships
between the compound head and modifier. War-
ren (1978) also inspects the syntax of compound
nouns, to create a somewhat different set of twelve
conceptual categories.
Early attempts to automatically classify com-
pound nouns have taken a semantic approach:
Finin (1980) and Isabelle (1984) use ?role nomi-
nals? derived from the head of the compound to
fill a slot with the modifier. Vanderwende (1994)
uses a rule?based technique that scores a com-
pound on possible semantic interpretations, while
Jones (1995) implements a graph?based unifica-
tion procedure over semantic feature structures for
the head. Finally, Rosario and Hearst (2001) make
use of a domain?specific lexical resource to clas-
sify according to neural networks and decision
trees.
Syntactic classification, using paraphrasing,
was first used by Leonard (1984), who uses a pri-
oritised rule?based approach across a number of
possible readings. Lauer (1995) employs a cor-
pus statistical model over a similar paraphrase
set based on prepositions. Lapata (2002) and
Grover et al (2005) again use a corpus statis-
tical paraphrase?based approach, but with verb?
argument relations for compound nominalisations
? attempting to define the relation as one of sub-
ject, direct object, or a number of prepositional ob-
jects in the latter.
2.2 Web?as?Corpus Approaches
Using the World Wide Web for corpus statistics
is a relatively recent phenomenon; we present a
few notable examples. Grefenstette (1998) anal-
yses the plausibility of candidate translations in
a machine translation task through Web statistics,
and avoids some data sparseness within that con-
text. Zhu and Rosenfeld (2001) train a language
model from a large corpus, and use the Web to
estimate low?density trigram frequencies. Keller
and Lapata (2003) show that Web counts can
obviate data sparseness for syntactic predicate?
argument bigrams. They also observe that the
noisiness of the Web, while unexplored in detail,
does not greatly reduce the reliability of their re-
sults. Nakov and Hearst (2005) demonstrate that
Web counts can aid in identifying the bracketing in
higher?arity noun compounds. Finally, Lapata and
Keller (2005) evaluate the performance of Web
counts on a wide range of natural language pro-
cessing tasks, including compound noun bracket-
ing and compound noun interpretation.
2.3 Confidence Intervals
Maximum likelihood statistics are not robust when
many sparse vectors are under consideration, i.e.
naively ?choosing the largest number? may not be
accurate in contexts when the relative value across
samplings may be relevant, for example, in ma-
chine learning. As such, we apply a statistical
test with confidence intervals (Kenney and Keep-
ing, 1962), where we compare sample z-scores in
a pairwise manner, instead of frequencies globally.
The confidence interval P , for z-score n, is:
P = 2?pi
? n/
?
2
0
e?t2dt (1)
t is chosen to normalise the curve, and P is strictly
increasing on n, so we are only required to find the
largest z-score.
Calculating the z-score exactly can be quite
costly, so we instead use the binomial approxi-
mation to the normal distribution with equal prior
probabilities and find that a given z-score Z is:
Z = f ? ?? (2)
where f is the frequency count, ? is the mean in
a pairwise test, and ? is the standard deviation of
the test. A more complete derivation appears in
Nicholson (2005).
3 Resources
We make use of a number of lexical resources
in our implementation and evaluation. For cor-
pus statistics, we use the written component of
the BNC, a balanced 90M token corpus. To find
verb?argument frequencies, we parse this using
RASP (Briscoe and Carroll, 2002), a tag sequence
grammar?based statistical parser. We contrast
the corpus statistics with ones collected from the
55
Web, using an implementation of a freely avail-
able Google ?scraper? from CPAN.1
For a given compound nominalisation, we wish
to determine all possible verbal forms of the head.
We do so using the combination of the morpho-
logical component of CELEX (Burnage, 1990), a
lexical database, NOMLEX (Macleod et al, 1998),
a nominalisation database, and CATVAR (Habash
and Dorr, 2003), an automatically?constructed
database of clusters of inflected words based on
the Porter stemmer (Porter, 1997).
Once the verbal forms have been identified, we
construct canonical forms of the present partici-
ple (+ing) and the past participle (+ed), using the
morph lemmatiser (Minnen et al, 2001). We con-
struct canonical forms of the plural head and plural
modifier (+s) in the same manner.
For evaluation, we have the two?way classified
data set used by Lapata (2002), and a three?way
classified data set constructed from open text.
Lapata automatically extracts candidates from
the British National Corpus, and hand?curates
a set of 796 compound nominalisations which
were interpreted as either a subjective relation
SUBJ (e.g. wood appearance ?wood appears?),
or a (direct) objective relation OBJ (e.g. stress
avoidance ?[SO] avoids stress?. We automatically
validated this data set for consistency, removing:
1. items that did not occur in the same chunk,
according to a chunker based on fnTBL 1.0
(Ngai and Florian, 2001),
2. items whose head did not have a verbal form
according to our lexical resources, and
3. items which consisted in part of proper
nouns,
to end up with 695 consistent compounds. We
used the method of Nicholson and Baldwin (2005)
to derive a small data set of 129 compound
nominalisations, also from the BNC, which we
instructed three unskilled annotators to identify
each as one of subjective (SUB), direct object
(DOB), or prepositional object (POB, e.g. side
show ?[SO] show [ST] on the side?). The an-
notators identified nine prepositional relations:
{about,against,for,from,in,into,on,to,with}.
1www.cpan.org: We limit our usage to examining the
?Estimated Results Returned?, so that our usage is identi-
cal to running the queries manually from the website. The
Google API (www.google.com/apis) gives a method
for examining the actual text of the returned documents.
4 Proposed Method
4.1 Paraphrase Tests
To derive preferences for the SUB, DOB, and var-
ious POB interpretations for a given compound
nominalisation, the most obvious approach is to
examine a parsed corpus for instances of the verbal
form of the head and the modifier occurring in the
corresponding verb?argument relation. There are
other constructions that can be informative, how-
ever.
We examine two novel paraphrase tests: one
prepositional and one participial. The preposi-
tional test is based in part on the work by Leonard
(1984) and Lauer (1995): for a given compound,
we search for instances of the head and modifier
nouns separated by a preposition. For example,
for the compound nominalisation leg operation,
we might search for operation on the leg, corre-
sponding to the POB relation on. Special cases are
by, corresponding to a subjective reading akin to a
passive construction (e.g. investor hesitancy, hesi-
tancy by the investor ? ?the investor hesitates?),
and of, corresponding to a direct object reading
(e.g. language speaker, speaker of the language
? ?[SO] speaks the language?).
The participial test is based on the paraphras-
ing equivalence of using the present participle of
the verbal head as an adjective before the modifier,
for the SUB relation (e.g. the hesitating investor ?
?the investor hesitates?), compared to the past par-
ticiple for the DOB relation (the spoken language
? ?[SO] speaks the language?). The correspond-
ing prepositional object construction is unusual in
English, but still possible: compare ?the operated-
on leg and the lived-in village.
4.2 The Algorithm
Given a compound nominalisation, we perform a
number of steps to arrive at an interpretation. First,
we derive a set of verbal forms for the head from
the combination of CELEX, NOMLEX, and CAT-
VAR. We find the participial forms of each of the
verbal heads, and plurals for the nominal head and
modifier, using the morph lemmatiser.
Next, we examine the BNC for instances of the
modifier and one of the verbal head forms oc-
curring in a verb?argument relation, with the aid
of the RASP parse. Using these frequencies, we
calculate the pairwise z-scores between SUB and
DOB, and between SUB and POB: the score given
to the SUB interpretation is the greater of the two.
56
We further examine the RASP parsed data for in-
stances of the prepositional and participial tests for
the compound, and calculate the z-scores for these
as well.
We then collect our Google counts. Because the
Web data is unparsed, we cannot look for syntactic
structures explictly. Instead, we query a number of
collocations which we expect to be representative
of the desired structure.
For the prepositional test, the head can be sin-
gular or plural, the modifier can be singular or plu-
ral, and there may or may not be an article be-
tween the preposition and the modifier. For exam-
ple, for the compound nominalisation product re-
placement and preposition of we search for all of
the following: (and similarly for the other prepo-
sitions)
replacement of product
replacement of the product
replacement of products
replacement of the products
replacements of product
replacements of the product
replacements of products
replacements of the products
For the participial test, the modifier can be sin-
gular or plural, and if we are examining a prepo-
sitional relation, the head can be either a present
or past participle. For product replacement, we
search for, as well as other prepositions:
the replacing product
the replacing products
the replaced product
the replaced products
the replacing?about product
the replacing?about products
the replaced?about product
the replaced?about products
We comment briefly on these tests in Section 6.
We choose to use the as our canonical article be-
cause it is a reliable marker of the left boundary of
an NP and number-neutral; using a/an represents
a needless complication.
We then calculate the z-scores using the method
described in Section 2, where the individual fre-
quency counts are the maximum of the results ob-
tained across the query set.
Once the z-scores have been obtained, we
choose a classification based on the greatest-
valued observed test. We contrast the confidence
interval?based approach with the maximum like-
lihood method of choosing the largest of the raw
frequencies. We also experiment with a machine
learning package, to examine the mutual predic-
tiveness of the separate tests.
5 Observed Results
First, we found majority-class baselines for each
of the data sets. The two?way data set had
258 SUBJ?classified items, and 437 OBJ?classified
items, so choosing OBJ each time gives a baseline
of 62.9%. The three?way set had 22 SUB items,
63 of DOB, and 44 of POB, giving a baseline of
48.8%.
Contrasting this with human performance on
the data set, Lapata recorded a raw inter-annotator
agreement of 89.7% on her test set, which cor-
responds to a Kappa value ? = 0.78. On the
three?way data set, three annotators had a agree-
ment of 98.4% for identification and classification
of observed compound nominalisations in open
text, and ? = 0.83. For the three-way data set,
the annotators were asked to both identify and
classify compound nominalisations in free text,
and agreement is thus calculated over all words
in the test. The high agreement figure is due to
the fact that most words could be trivially disre-
garded (e.g. were not nouns). Kappa corrects this
for chance agreement, so we conclude that this
task was still better-defined than the one posed
by Lapata. One possible reason for this was the
number of poorly?behaved compounds that we re-
moved due to chunk inconsistencies, lack of a ver-
bal form, or proper nouns: it would be difficult for
the annotators to agree over compounds where an
obvious well?defined interpretation was not avail-
able.
5.1 Comparison Classification
Results for classification over the Lapata two?way
data set are given in Table 1, and results over
the open data three?way set are given in Table 2.
For these, we selected the greatest raw frequency
count for a given test as the intended relation
(Raw), or the greatest confidence interval accord-
ing to the z-score (Z-Score). If a relation could not
be selected due to ties (e.g., the scores were all 0),
we selected the majority baseline. To deal with the
nature of the two?way data set with respect to our
three?way selection, we mapped compounds that
we would prefer to be POB to OBJ, as there are
57
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb?Argument 62.9 67.9 68.3 ? ?
Prepositional 62.9 62.1 62.4 62.6 63.0
Participial 62.9 63.0 63.2 61.4 58.8
Table 1: Classification Results over the two?way data set, in %. Comparison of raw frequency counts
vs. confidence?based z-scores, for BNC data and Google scrapings shown.
Paraphrase Default Corpus Counts Web Counts
Raw Z-Score Raw Z-Score
Verb?Argument 48.8 54.3 55.0 ? ?
Prepositional 48.8 48.4 50.0 59.7 58.9
Participial 48.8 43.2 45.4 43.4 38.0
Table 2: Classification results over the three-way data set, in %. Comparison of raw frequency counts
vs. confidence-based z-scores, for BNC data and Google scrapings shown.
compounds in the set (e.g. adult provision) that
have a prepositional object reading (?provide for
adults?) but have been classified as a direct object
OBJ.
The verb?argument counts obtained from the
parsed BNC are significantly better than the base-
line for the Lapata data set (?2 = 4.12, p ? 0.05),
but not significantly better for the open data set
(?2 = 0.99, p ? 1). Similar results were reported
by Lapata (2002) over her data set using backed?
off smoothing, the most closely related method.
Neither the prepositional nor participial para-
phrases were significantly better than the baseline
for either the two?way (?2 = 0.00, p ? 1), or
the three?way data set (?2 = 3.52, p ? 0.10), al-
though the prepositional test did slightly improve
on the verb?argument results.
5.2 Machine Learning Classification
Although the results were not impressive, we still
believed that there was complementing informa-
tion within the data, which could be extracted with
the aid of a machine learner. For this, we made
use of TiMBL (Daelemans et al, 2003), a nearest-
neighbour classifier which stores the entire train-
ing set and extrapolates further samples, as a prin-
cipled method for combination of the data. We use
TiMBL?s in-built cross-validation method: 90% of
the data set is used as training data to test the other
10%, for each stratified tenth of the set. The results
it achieves are assumed to be able to generalise to
new samples if they are compared to the current
training data set.
The results observed using TiMBL are shown
Corpus Counts Web Counts
Two?way Set 72.4 74.2
Three?way Set 51.1 50.4
Table 3: TiMBL results for the combination of
paraphrase tests over the two?way and three?way
data sets for corpus and Web frequencies
in Table 3. This was from the combination
of all of the available paraphrase tests: verb?
argument, prepositional, and participial for the
corpus counts, and just prepositional and particip-
ial for the Web counts. The results for the two?
way data set derived from Lapata?s data set were a
good improvement over the simple classification
results, significantly so for the Web frequencies
(?2 = 20.3, p ? 0.01). However, we also no-
tice a corresponding decrease in the results for the
three?way open data set, which make these im-
provements immaterial.
Examining the other possible combinations for
the tests did indeed lead to varying results, but not
in a consistent manner. For example, the best com-
bination for the open data set was using the par-
ticipial raw scores and z-scores (58.1%), which
performed particularly badly in simple compar-
isons, and comparatively poorly (70.2%) for the
two?way set.
6 Discussion
Although the observed results failed to match, or
even approach, various benchmarks set by La-
pata (2002) (87.3% accuracy) and Grover et al
(2005) (77%) for the subject?object and subject?
58
direct object?prepositional objects classification
tasks respectively, the presented approach is not
without merit. Indeed, these results relied on
machine learning approaches incorporating many
features independent of corpus counts: namely,
context, suffix information, and semantic similar-
ity resources. Our results were an examination
of the possible contribution of lexical information
available from high?volume unparsed text.
One important concept used in the above bench-
marks was that of statistical smoothing, both
class?based and distance?based. The reason for
this is the inherent data sparseness within the
corpus statistics for these paraphrase tests. La-
pata (2002) observes that almost half (47%) of
the verb?noun pairs constructed are not attested
within the BNC. Grover et al (2005) also note the
sparseness of observed relations. Using the im-
mense data source of the Web allows one to cir-
cumvent this problem: only one compound (an-
archist prohibition) has no instances of the para-
phrases from the scraping,2 from more than 900
compounds between the two data sets. This ex-
tra information, we surmise, would be beneficial
for the smoothing procedures, as the comparative
accuracy between the two methods is similar.
On the other hand, we also observe that sim-
ply alleviating the data sparseness is insufficient
to provide a reliable interpretation. These results
reinforce the contribution made by the statistical
and semantic resources used in arriving at these
benchmarks.
The approach suggested by Keller and Lapata
(2003) for obtaining bigram information from the
Web could provide an approach for estimating the
syntactic verb?argument counts for a given com-
pound (dashes in Tables 1 and 2). In spite of
the inherent unreliability of approximating long?
range dependencies with n-gram information, re-
sults look promising. An examination of the effec-
tiveness of this approach is left as further research.
Similarly, various methods of combining corpus
counts with the Web counts, including smooth-
ing, backing?off, and machine learning, could also
lead to interesting performance impacts.
Another item of interest is the comparative dif-
ficulty of the task presented by the three?way data
set extracted from open data, and the two?way
data set hand?curated by Lapata. The baseline
2Interestingly, Google only lists 3 occurrences of this
compound anyway, so token relevance is low ? further in-
spection shows that those 3 are not well-formed in any case.
of this set is much lower, even compared that
of the similar task (albeit domain?specific) from
Grover et al (2005) of 58.6%. We posit that the
hand?filtering of the data set in these works con-
tributes to a biased sample. For example, remov-
ing prepositional objects for a two?way classifica-
tion, which make up about a third of the open data
set, renders the task somewhat artificial.
Comparison of the results between the maxi-
mum likelihood estimates used in earlier work,
and the more statistically robust confidence inter-
vals were inconclusive as to performance improve-
ment, and were most effective as a feature expan-
sion algorithm. The only obvious result is an aes-
thetic one, in using ?robust statistics?.
Finally, the paraphrase tests which we propose
are not without drawbacks. In the prepositional
test, a paraphrase with of does not strictly con-
tribute to a direct object reading: consider school
aim ?school aims?, for which instances of aim by
the school are overwhelmed by aim of the school.
We experimented with permutations of the avail-
able queries (e.g. requiring the head and modifier
to be of different number, to reflect the pluralis-
ability of the head in such compounds, e.g. aims
of the school), without observing substantially dif-
ferent results.
Another observation is the inherent bias of the
prepositional test to the prepositional object re-
lation. Apparent prepositional relations can oc-
cur in spite of the available verb frames: con-
sider cash limitation, where the most populous in-
stance is limitation on cash, despite the impossi-
bility of *to limit on cash (for to place a limit on
cash). Another example, is bank agreement: find-
ing instances of agreement with bank does not lead
to the pragmatically absurd [SO] agrees with the
bank.
Correspondingly, the participial relation has the
opposite bias: constructions of the form the lived-
in at ?[SO] lived in the flat? are usually lexi-
calised in English. As such, only 17% of com-
pounds in the two?way data set and 34% of the
three-way data set display non-zero values in the
prepositional object relation for the participial test.
We hoped that the inherent biases of the two tests
might balance each other, but there is little evi-
dence of that from the results.
59
7 Conclusion
We presented two novel paraphrase tests for au-
tomatically predicting the inherent semantic rela-
tion of a given compound nominalisation as one of
subject, direct object, or prepositional object. We
compared these to the usual verb?argument para-
phrase test, using corpus statistics, and frequen-
cies obtained by scraping the Google search en-
gine. We also implemented a more robust statisti-
cal measure than the insipid maximum likelihood
estimates ? the confidence interval. A significant
reduction in data sparseness was achieved, but this
alone is insufficient to provide a substantial per-
formance improvement.
Acknowledgements
We would like to thank the members of the Univer-
sity of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this re-
search, as well as Mirella Lapata for allowing use
of the data.
References
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation, pages 1499?1504, Las
Palmas, Canary Islands.
Gavin Burnage. 1990. CELEX: A guide for users.
Technical report, University of Nijmegen.
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2003. TiMBL: Tilburg Mem-
ory Based Learner, version 5.0, Reference Guide.
ILK Technical Report 03-10.
Tim Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings of the First Na-
tional Conference on Artificial Intelligence, pages
310?315, Stanford, USA. AAAI Press.
Gregory Grefenstette. 1998. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the ASLIB Conference on
Translating and the Computer, London, UK.
Claire Grover, Mirella Lapata, and Alex Lascarides.
2005. A comparison of parsing technologies for the
biomedical domain. Journal of Natural Language
Engineering, 11(01):27?65.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for English. In Proceedings of
the 2003 Human Language Technology Conference
of the North American Chapter of the ACL, pages
17?23, Edmonton, Canada.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceeedings of the 10th International
Conference on Computational Linguistics and 22nd
Annual Meeting of the ACL, pages 509?516, Stan-
ford, USA.
Bernard Jones. 1995. Predicating nominal com-
pounds. In Proceedings of the 17th International
Conference of the Cognitive Science Society, pages
130?5, Pittsburgh, USA.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional Linguistics, 29(3):459?484.
John F. Kenney and E. S. Keeping, 1962. Mathematics
of Statistics, Pt. 1, chapter 11.4, pages 167?9. Van
Nostrand, Princeton, USA, 3rd edition.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM
Transactions on Speech and Language Processing,
2(1).
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional
evidence. In Proceedings of the 10th Conference
of the European Chapter of the Association for
Computional Linguistics, pages 235?242, Budapest,
Hungary.
Maria Lapata. 2002. The disambiguation of nomi-
nalizations. Computational Linguistics, 28(3):357?
388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University, Sydney, Australia.
Rosemary Leonard. 1984. The Interpretation of En-
glish Noun Sequences on the Computer. Elsevier
Science, Amsterdam, the Netherlands.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, USA.
Beth Levin. 1993. English Verb Classes and Alter-
nations. The University of Chicago Press, Chicago,
USA.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A lexicon of nominalizations. In Proceedings of the
8th International Congress of the European Associ-
ation for Lexicography, pages 187?193, Liege, Bel-
gium.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?23.
60
Preslov Nakov and Marti Hearst. 2005. Search en-
gine statistics beyond the n-gram: Application to
noun compound bracketing. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning, pages 17?24, Ann Arbor, USA.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of the
2nd Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 40?7, Pittsburgh, USA.
Jeremy Nicholson and Timothy Baldwin. 2005. Sta-
tistical interpretation of compound nominalisations.
In Proceeding of the Australasian Langugae Tech-
nology Workshop 2005, Sydney, Australia.
Jeremy Nicholson. 2005. Statistical interpretation of
compound nouns. Honours Thesis, University of
Melbourne, Melbourne, Australia.
Martin Porter. 1997. An algorithm for suffix strip-
ping. In Karen Sparck Jones and Peter Willett,
editors, Readings in information retrieval. Morgan
Kaufmann, San Francisco, USA.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 6th Conference on Empirical Methods in Natural
Language Processing, Pittsburgh, USA.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL 2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17?24,
Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 782?788, Kyoto, Japan.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds. Acta Universitatis Gothoburgen-
sis, Go?teborg, Sweden.
Xiaojin Zhu and Ronald Rosenfeld. 2001. Improv-
ing trigram language modeling with the World Wide
Web. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 533?6, Salt Lake City, USA.
61
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164?171,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Deep Lexical Acquisition for HPSGs via Supertagging
Phil Blunsom and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
{pcbl,tim}@csse.unimelb.edu.au
Abstract
We propose a conditional random field-
based method for supertagging, and ap-
ply it to the task of learning new lexi-
cal items for HPSG-based precision gram-
mars of English and Japanese. Us-
ing a pseudo-likelihood approximation we
are able to scale our model to hun-
dreds of supertags and tens-of-thousands
of training sentences. We show that
it is possible to achieve start-of-the-art
results for both languages using maxi-
mally language-independent lexical fea-
tures. Further, we explore the performance
of the models at the type- and token-level,
demonstrating their superior performance
when compared to a unigram-based base-
line and a transformation-based learning
approach.
1 Introduction
Over recent years, there has been a resurgence of
interest in the use of precision grammars in NLP
tasks, due to advances in parsing algorithm de-
velopment, grammar development tools and raw
computational power (Oepen et al, 2002b). Pre-
cision grammars are defined as implemented
grammars of natural language which capture fine-
grained linguistic distinctions, and are generative
in the sense of distinguishing between grammat-
ical and ungrammatical inputs (or at least have
some in-built notion of linguistic ?markedness?).
Additional characteristics of precision grammars
are that they are frequently bidirectional, and out-
put a rich semantic abstraction for each span-
ning parse of the input string. Examples include
DELPH-IN grammars such as the English Resource
Grammar (Flickinger, 2002; Uszkoreit, 2002), the
various PARGRAM grammars (Butt et al, 1999),
and the Edinburgh CCG parser (Bos et al, 2004).
Due to their linguistic complexity, precision
grammars are generally hand-constructed and thus
restricted in size and coverage. Attempts to
(semi-)automate the process of expanding the cov-
erage of precision grammars have focused on ei-
ther: (a) constructional coverage, e.g. in the form
of error mining for constructional expansion (van
Noord, 2004; Zhang and Kordoni, 2006), or relax-
ation of lexico-grammatical constraints to support
partial and/or robust parsing (Riezler et al, 2002);
or (b) lexical coverage, e.g. in bootstrapping from
a pre-existing grammar and lexicon to learn new
lexical items (Baldwin, 2005a). Our particular in-
terest in this paper is in the latter of these two,
that is the development of methods for automati-
cally expanding the lexical coverage of an existing
precision grammar, or more broadly deep lexical
acquisition (DLA hereafter). In this, we follow
Baldwin (2005a) in assuming a semi-mature pre-
cision grammar with a fixed inventory of lexical
types, based on which we learn new lexical items.
For the purposes of this paper, we focus specif-
ically on supertagging as the mechanism for hy-
pothesising new lexical items.
Supertagging can be defined as the process of
applying a sequential tagger to the task of predict-
ing the lexical type(s) associated with each word
in an input string, relative to a given grammar. It
was first introduced as a means of reducing parser
ambiguity by Bangalore and Joshi (1999) in the
context of the LTAG formalism, and has since been
applied in a similar context within the CCG for-
malism (Clark and Curran, 2004). In both of these
cases, supertagging provides the means to perform
a beam search over the plausible lexical items for
a given string context, and ideally reduces pars-
ing complexity without sacrificing parser accu-
racy. An alternate application of supertagging is
in DLA, in postulating novel lexical items with
which to populate the lexicon of a given gram-
mar to boost parser coverage. This can take place
164
either: (a) off-line for the purposes of rounding
out the coverage of a static lexicon, in which case
we are generally interested in globally maximising
precision over a given corpus and hence predict-
ing the single most plausible lexical type for each
word token (off-line DLA: Baldwin (2005b)); or
(b) on the fly for a given input string to temporar-
ily expand lexical coverage and achieve a spanning
parse, in which case we are interested in maximis-
ing recall by producing a (possibly weighted) list
of lexical item hypotheses to run past the grammar
(on-line DLA: Zhang and Kordoni (2005)). Our
immediate interest in this paper is in the first of
these tasks, although we would ideally like to de-
velop an off-line method which is trivially portable
to the second task of on-line DLA.
In this research, we focus particularly on
the Grammar Matrix-based DELPH-IN family of
grammars (Bender et al, 2002), which includes
grammars of English, Japanese, Norwegian, Mod-
ern Greek, Portuguese and Korean. The Gram-
mar Matrix is a framework for streamlining and
standardising HPSG-based multilingual grammar
development. One property of Grammar Matrix-
based grammars is that they are strongly lexical-
ist and adhere to a highly constrained lexicon-
grammar interface via a unique (terminal) lexi-
cal type for each lexical item. As such, lexical
item creation in any of the Grammar Matrix-based
grammars, irrespective of language, consists pre-
dominantly of predicting the appropriate lexical
type for each lexical item, relative to the lexical
hierarchy for the corresponding grammar. In this
same spirit of standardisation and multilingual-
ity, the aim of this research is to develop max-
imally language-independent supertagging meth-
ods which can be applied to any Grammar Matrix-
based grammar with the minimum of effort. Es-
sentially, we hope to provide the grammar engi-
neer with the means to semi-automatically popu-
late the lexicon of a semi-mature grammar, hence
accelerating the pace of lexicon development and
producing a resource of sufficient coverage to be
practically useful in NLP tasks.
The contributions of this paper are the devel-
opment of a pseudo-likelihood conditional ran-
dom field-based method of supertagging, which
we then apply to the task of off-line DLA for
grammars of both English and Japanese with only
minor language-specific adaptation. We show the
supertagger to outperform previously-proposed
supertagger-based DLA methods.
The remainder of this paper is structured as
follows. Section 2 outlines past work relative
to this research, and Section 3 reviews the re-
sources used in our supertagging experiments.
Section 4 outlines the proposed supertagger model
and reviews previous research on supertagger-
based DLA. Section 5 then outlines the set-up and
results of our evaluation.
2 Past Research
According to Baldwin (2005b), research on DLA
falls into the two categories of in vitro methods,
where we leverage a secondary language resource
to generate an abstraction of the words we hope to
learn lexical items for, and in vivo methods, where
the target resource that we are hoping to perform
DLA relative to is used directly to perform DLA.
Supertagging is an instance of in vivo DLA, as it
operates directly over data tagged with the lexical
type system for the precision grammar of interest.
Research on supertagging which is relevant to
this paper includes the work of Baldwin (2005b) in
training a transformation-based learner over data
tagged with ERG lexical types. We discuss this
method in detail in Section 5.2 and replicate this
method over our English data set for direct com-
parability with this previous research.
As mentioned above, other work on supertag-
ging has tended to view it as a means of driving
a beam search to prune the parser search space
(Bangalore and Joshi, 1999; Clark and Curran,
2004). In supertagging, token-level annotations
(gold-standard, automatically-generated or other-
wise) for a given DLR are used to train a se-
quential tagger, akin to training a POS tagger over
POS-tagged data taken from the Penn Treebank.
One related in vivo approach to DLA targeted
specifically at precision grammars is that of Fou-
vry (2003). Fouvry uses the grammar to guide
the process of learning lexical items for unknown
words, by generating underspecified lexical items
for all unknown words and parsing with them.
Syntactico-semantic interaction between unknown
words and pre-existing lexical items during pars-
ing provides insight into the nature of each un-
known word. By combining such fragments of in-
formation, it is possible to incrementally arrive at
a consolidated lexical entry for that word. That is,
the precision grammar itself drives the incremen-
tal learning process within a parsing context.
165
An alternate approach is to compile out a set of
word templates for each lexical type (with the im-
portant qualification that they do not rely on pre-
processing of any form), and check for corpus oc-
currences of an unknown word in such contexts.
That is, the morphological, syntactic and/or se-
mantic predictions implicit in each lexical type are
made explicit in the form of templates which rep-
resent distinguishing lexical contexts of that lexi-
cal type. This approach has been shown to be par-
ticularly effective over web data, where the sheer
size of the data precludes the possibility of linguis-
tic preprocessing but at the same time ameliorates
the effects of data sparseness inherent in any lexi-
calised DLA approach (Lapata and Keller, 2004).
Other work on DLA (e.g. Korhonen (2002),
Joanis and Stevenson (2003), Baldwin (2005a))
has tended to take an in vitro DLA approach, in
extrapolating away from a DLR to corpus or web
data, and analysing occurrences of words through
the conduit of an external resource (e.g. a sec-
ondary parser or POS tagger). In vitro DLA can
also take the form of resource translation, in map-
ping one DLR onto another to arrive at the lexical
information in the desired format.
3 Task and Resources
In this section, we outline the resources targeted
in this research, namely the English Resource
Grammar (ERG: Flickinger (2002), Copestake
and Flickinger (2000)) and the JACY grammar of
Japanese (Siegel and Bender, 2002). Note that our
choice of the ERG and JACY as testbeds for exper-
imentation in this paper is somewhat arbitrary, and
that we could equally run experiments over any
Grammar Matrix-based grammar for which there
is treebank data.
Both the ERG and JACY are implemented
open-source broad-coverage precision Head-
driven Phrase Structure Grammars (HPSGs:
Pollard and Sag (1994)). A lexical item in each
of the grammars consists of a unique identifier,
a lexical type (a leaf type of a type hierarchy),
an orthography, and a semantic relation. For
example, in the English grammar, the lexical item
for the noun dog is simply:
dog_n1 := n_-_c_le &
[ STEM < "dog" >,
SYNSEM [ LKEYS.KEYREL.PRED "_dog_n_1_rel" ] ].
in which the lexical type of n - c le encodes
the fact that dog is a noun which does not sub-
categorise for any other constituents and which is
countable, "dog" specifies the lexical stem, and
" dog n 1 rel" introduces an ad hoc predicate
name for the lexical item to use in constructing a
semantic representation. In the context of the ERG
and JACY, DLA equates to learning the range of
lexical types a given lexeme occurs with, and gen-
erating a single lexical item for each.
Recent development of the ERG and JACY has
been tightly coupled with treebank annotation, and
all major versions of both grammars are deployed
over a common set of dynamically-updateable
treebank data to help empirically trace the evo-
lution of the grammar and retrain parse selection
models (Oepen et al, 2002a; Bond et al, 2004).
This serves as a source of training and test data for
building our supertaggers, as detailed in Table 1.
In translating our treebank data into a form that
can be understood by a supertagger, multiword ex-
pressions (MWEs) pose a slight problem. Both the
ERG and JACY include multiword lexical items,
which can either be strictly continuous (e.g. hot
line) or optionally discontinuous (e.g. transitive
English verb particle constructions, such as pick
up as in Kim picked the book up).
Strictly continuous lexical items are described
by way of a single whitespace-delimited lexical
stem (e.g. STEM < "hot line" >). When
faced with instances of this lexical item, the su-
pertagger must perform two roles: (1) predict that
the words hot and line combine together to form
a single lexeme, and (2) predict the lexical type
associated with the lexeme. This is performed
in a single step through the introduction of the
ditto lexical type, which indicates that the cur-
rent word combines (possibly recursively) with the
left-adjacent word to form a single lexeme, and
shares the same lexical type. This tagging conven-
tion is based on that used, e.g., in the CLAWS7
part-of-speech tagset.
Optionally discontinuous lexical items are less
of a concern, as selection of each of the discontin-
uous ?components? is done via lexical types. E.g.
in the case of pick up, the lexical entry looks as
follows:
pick_up_v1 := v_p-np_le &
[ STEM < "pick" >,
SYNSEM [ LKEYS [ --COMPKEY _up_p_sel_rel,
KEYREL.PRED "_pick_v_up_rel" ] ] ].
in which "pick" selects for the up p sel rel
predicate, which in turn is associated with the stem
"up" and lexical type p prtcl le. In terms of
lexical tag mark-up, we can treat these as separate
166
ERG JACY
GRAMMAR
Language English Japanese
Lexemes 16,498 41,559
Lexical items 26,297 47,997
Lexical types 915 484
Strictly continuous MWEs 2,581 422
Optionally discontinuous MWEs 699 0
Proportion of lexemes with more than one lexical item 0.29 0.14
Average lexical items per lexeme 1.59 1.16
TREEBANK
Training sentences 20,000 40,000
Training words 215,015 393,668
Test sentences 1,013 1,095
Test words 10,781 10,669
Table 1. Make-up of the English Resource Grammar (ERG) and JACY grammars and treebanks
tags and leave the supertagger to model the mutual
inter-dependence between these lexical types.
For detailed statistics of the composition of the
two grammars, see Table 1.
For morphological processing (including to-
kenisation and lemmatisation), we use the pre-
existing machinery provided with each of the
grammars. In the case of the ERG, this consists
of a finite state machine which feeds into lexical
rules; in the case of JACY, segmentation and lem-
matisation is based on a combination of ChaSen
(Matsumoto et al, 2003) and lexical rules. That
is, we are able to assume that the Japanese data
has been pre-segmented in a form compatible with
JACY, as we are able to replicate the automatic
pre-processing that it uses.
4 Suppertagging
The DLA strategy we adopt in this research is
based on supertagging, which is a simple in-
stance of sequential tagging with a larger, more
linguistically-diverse tag set than is conventionally
the case, e.g., with part-of-speech tagging. Below,
we describe the pseudo-likelihood CRF model we
base our supertagger on and outline the feature
space for the two grammars.
4.1 Pseudo-likelihood CRF-based
Supertagging
CRFs are undirected graphical models which de-
fine a conditional distribution over a label se-
quence given an observation sequence. Here we
use CRFs to model sequences of lexical types,
where each input word in a sentence is assigned
a single tag.
The joint probability density of a sequence la-
belling,   (a vector of lexical types), given the in-
put sentence,  , is given by:

 

	
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65?72,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Automatic Identification of English Verb Particle Constructions
using Linguistic Features
Su Nam Kim and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
This paper presents a method for identify-
ing token instances of verb particle con-
structions (VPCs) automatically, based on
the output of the RASP parser. The pro-
posed method pools together instances of
VPCs and verb-PPs from the parser out-
put and uses the sentential context of each
such instance to differentiate VPCs from
verb-PPs. We show our technique to per-
form at an F-score of 97.4% at identifying
VPCs in Wall Street Journal and Brown
Corpus data taken from the Penn Tree-
bank.
1 Introduction
Multiword expressions (hereafter MWEs) are
lexical items that can be decomposed into multi-
ple simplex words and display lexical, syntactic
and/or semantic idiosyncracies (Sag et al, 2002;
Calzolari et al, 2002). In the case of English,
MWEs are conventionally categorised syntactico-
semantically into classes such as compound nom-
inals (e.g. New York, apple juice, GM car), verb
particle constructions (e.g. hand in, battle on),
non-decomposable idioms (e.g. a piece of cake,
kick the bucket) and light-verb constructions (e.g.
make a mistake). MWE research has focussed
largely on their implications in language under-
standing, fluency and robustness (Pearce, 2001;
Sag et al, 2002; Copestake and Lascarides, 1997;
Bannard et al, 2003; McCarthy et al, 2003; Wid-
dows and Dorow, 2005). In this paper, our goal
is to identify individual token instances of En-
glish verb particle constructions (VPCs hereafter)
in running text.
For the purposes of this paper, we follow Bald-
win (2005) in adopting the simplifying assump-
tion that VPCs: (a) consist of a head verb and a
unique prepositional particle (e.g. hand in, walk
off); and (b) are either transitive (e.g. hand in, put
on) or intransitive (e.g. battle on). A defining char-
acteristic of transitive VPCs is that they can gen-
erally occur with either joined (e.g. He put on the
sweater) or split (e.g. He put the sweater on) word
order. In the case that the object is pronominal,
however, the VPC must occur in split word order
(c.f. *He handed in it) (Huddleston and Pullum,
2002; Villavicencio, 2003).
The semantics of the VPC can either derive
transparently from the semantics of the head verb
and particle (e.g. walk off ) or be significantly re-
moved from the semantics of the head verb and/or
particle (e.g. look up); analogously, the selectional
preferences of VPCs can mirror those of their head
verbs or alternatively diverge markedly. The syn-
tax of the VPC can also coincide with that of the
head verb (e.g. walk off ) or alternatively diverge
(e.g. lift off ).
In the following, we review relevant past
research on VPCs, focusing on the extrac-
tion/identification of VPCs and the prediction of
the compositionality/productivity of VPCs.
There is a modest body of research on the iden-
tification and extraction of VPCs. Note that in
the case of VPC identification we seek to detect
individual VPC token instances in corpus data,
whereas in the case of VPC extraction we seek
to arrive at an inventory of VPC types/lexical
items based on analysis of token instances in cor-
pus data. Li et al (2003) identify English VPCs
(or ?phrasal verbs? in their parlance) using hand-
coded regular expressions. Baldwin and Villavi-
cencio (2002) extract a simple list of VPCs from
corpus data, while Baldwin (2005) extracts VPCs
with valence information under the umbrella of
deep lexical acquisition.1 The method of Baldwin
(2005) is aimed at VPC extraction and takes into
account only the syntactic features of verbs. In this
paper, our interest is in VPC identification, and we
make use of deeper semantic information.
In Fraser (1976) and Villavicencio (2006) it is
argued that the semantic properties of verbs can
determine the likelihood of their occurrence with
1The learning of lexical items in a form that can be fed
directly into a deep grammar or other richly-annotated lexical
resource
65
particles. Bannard et al (2003) and McCarthy et
al. (2003) investigate methods for estimating the
compositionality of VPCs based largely on dis-
tributional similarity of the head verb and VPC.
O?Hara and Wiebe (2003) propose a method for
disambiguating the verb sense of verb-PPs. While
our interest is in VPC identification?a fundamen-
tally syntactic task?we draw on the shallow se-
mantic processing employed in these methods in
modelling the semantics of VPCs relative to their
base verbs.
The contribution of this paper is to combine
syntactic and semantic features in the task of VPC
identification. The basic intuition behind the pro-
posed method is that the selectional preferences of
VPCs over predefined argument positions,2 should
provide insight into whether a verb and preposi-
tion in a given sentential context combine to form
a VPC (e.g. Kim handed in the paper) or alter-
natively constitute a verb-PP (e.g. Kim walked in
the room). That is, we seek to identify individual
preposition token instances as intransitive preposi-
tions (i.e. prepositional particles) or transitive par-
ticles based on analysis of the governing verb.
The remainder of the paper is structured as fol-
lows. Section 2 outlines the linguistic features of
verbs and their co-occuring nouns. Section 3 pro-
vides a detailed description of our technique. Sec-
tion 4 describes the data properties and the identi-
fication method. Section 5 contains detailed evalu-
ation of the proposed method. Section 6 discusses
the effectiveness of our approach. Finally, Sec-
tion 7 summarizes the paper and outlines future
work.
2 Linguistic Features
When verbs co-occur with particles to form VPCs,
their meaning can be significantly different from
the semantics of the head verb in isolation. Ac-
cording to Baldwin et al (2003), divergences in
VPC and head verb semantics are often reflected
in differing selectional preferences, as manifested
in patterns of noun co-occurrence. In one example
cited in the paper, the cosine similarity between
cut and cut out, based on word co-occurrence vec-
tors, was found to be greater than that between cut
and cut off, mirroring the intuitive compositional-
ity of these VPCs.
(1) and (2) illustrate the difference in the selec-
tional preferences of the verb put in isolation as
compared with the VPC put on.3
2Focusing exclusively on the subject and object argument
positions.
3All sense definitions are derived from WordNet 2.1.
(1) put = place
EX: Put the book on the table.
ARGS: bookOBJ = book, publication, object
ANALYSIS: verb-PP
(2) put on = wear
EX: Put on the sweater .
ARGS: sweaterOBJ = garment, clothing
ANALYSIS: verb particle construction
While put on is generally used in the context of
wearing something, it usually occurs with clothing-
type nouns such as sweater and coat, whereas the
simplex put has less sharply defined selectional re-
strictions and can occur with any noun. In terms
of the word senses of the head nouns of the ob-
ject NPs, the VPC put on will tend to co-occur
with objects which have the semantics of clothes
or garment. On the other hand, the simplex verb
put in isolation tends to be used with objects with
the semantics of object and prepositional phrases
containing NPs with the semantics of place.
Also, as observed above, the valence of a VPC
can differ from that of the head verb. (3) and (4)
illustrate two different senses of take off with in-
transitive and transitive syntax, respectively. Note
that take cannot occur as a simplex intransitive
verb.
(3) take off = lift off
EX: The airplane takes off.
ARGS: airplaneSUBJ = airplane, aeroplane
ANALYSIS: verb particle construction
(4) take off = remove
EX: They take off the cape .
ARGS: theySUBJ = person, individual
capeOBJ = garment, clothing
ANALYSIS: verb particle construction
Note that in (3), take off = lift off co-occurs with
a subject of the class airplane, aeroplane. In (4), on
the other hand, take off = remove and the corre-
sponding object noun is of class garment or cloth-
ing. From the above, we can see that head nouns
in the subject and object argument positions can
be used to distinguish VPCs from simplex verbs
with prepositional phrases (i.e. verb-PPs).
66
3 Approach
Our goal is to distinguish VPCs from verb-PPs in
corpus data, i.e. to take individual inputs such as
Kim handed the paper in today and tag each as
either a VPC or a verb-PP. Our basic approach is
to parse each sentence with RASP (Briscoe and
Carroll, 2002) to obtain a first-gloss estimate of
the VPC and verb-PP token instances, and also
identify the head nouns of the arguments of each
VPC and simplex verb. For the head noun of each
subject and object, as identified by RASP, we use
WordNet 2.1 (Fellbaum, 1998) to obtain the word
sense. Finally we build a supervised classifier us-
ing TiMBL 5.1 (Daelemans et al, 2004).
3.1 Method
Compared to the method proposed by Baldwin
(2005), our approach (a) tackles the task of VPC
identification rather than VPC extraction, and (b)
uses both syntactic and semantic features, employ-
ing the WordNet 2.1 senses of the subject and/or
object(s) of the verb. In the sentence He put the
coat on the table, e.g., to distinguish the VPC put
on from the verb put occurring with the preposi-
tional phrase on the table, we identify the senses
of the head nouns of the subject and object(s) of
the verb put (i.e. he and coat, respectively).
First, we parse all sentences in the given corpus
using RASP, and identify verbs and prepositions
in the RASP output. This is a simple process of
checking the POS tags in the most-probable parse,
and for both particles (tagged RP) and transitive
prepositions (tagged II) reading off the governing
verb from the dependency tuple output (see Sec-
tion 3.2 for details). We also retrieved the head
nouns of the subject and object(s) of each head
verb directly from the dependency tuples. Using
WordNet 2.1, we then obtain the word sense of the
head nouns.
The VPCs or verb-PPs are represented with cor-
responding information as given below:
P (type|v, p,wsSUBJ,wsDOBJ,ws IOBJ)
where type denotes either a VPC or verb-PP, v is
the head verb, p is the preposition, and ws* is the
word sense of the subject, direct object or indirect
object.
Once all the data was gathered, we separated it
into test and training data. We then used TiMBL
5.1 to learn a classifier from the training data,
which was then run and evaluated over the test
data. See Section 5 for full details of the results.
Figure 1 depicts the complete process used to
distinguish VPCs from verb-PPs.
text
raw
Particles Objects
Senses
corpus
Subjects
WordNet
Word
v+p with Semantics
Verbs
TiMBL Classifier
look_after := [..
put_on := [..
take_off := [..
e.g.
Preprocessing RASPparser
Figure 1: System Architecture
3.2 On the use of RASP, WordNet and
TiMBL
RASP is used to identify the syntactic structure
of each sentence, including the head nouns of ar-
guments and first-gloss determination of whether
a given preposition is incorporated in a VPC or
verb-PP. The RASP output contains dependency
tuples derived from the most probable parse, each
of which includes a label identifying the nature
of the dependency (e.g. SUBJ, DOBJ), the head
word of the modifying constituent, and the head of
the modified constituent. In addition, each word
is tagged with a POS tag from which it is possi-
ble to determine the valence of any prepositions.
McCarthy et al (2003) evaluate the precision of
RASP at identifying VPCs to be 87.6% and the re-
call to be 49.4%. However the paper does not eval-
uate the parser?s ability to distinguish sentences
containing VPCs and sentences with verb-PPs.
To better understand the baseline performance
of RASP, we counted the number of false-positive
examples tagged with RP and false-negative ex-
amples tagged with II, relative to gold-standard
data. See Section 5 for details.
We use WordNet to obtain the first-sense word
sense of the head nouns of subject and object
phrases, according to the default word sense rank-
ing provided within WordNet. McCarthy et al
(2004) found that 54% of word tokens are used
with their first (or default) sense. With the per-
formance of current word sense disambiguation
(WSD) systems hovering around 60-70%, a sim-
ple first-sense WSD system has room for improve-
ment, but is sufficient for our immediate purposes
67
in this paper.
To evaluate our approach, we built a super-
vised classifier using the TiMBL 5.1 memory-
based learner and training data extracted from the
Brown and WSJ corpora.
4 Data Collection
We evaluated out method by running RASP over
Brown Corpus and Wall Street Journal, as con-
tained in the Penn Treebank (Marcus et al, 1993).
4.1 Data Classification
The data we consider is sentences containing
prepositions tagged as either RP or II. Based on
the output of RASP, we divide the data into four
groups:
Group A Group BGroup C
RP & II tagged dataRP tagged data II tagged data
Group D
Group A contains the verb?preposition token
instances tagged tagged exclusively as VPCs (i.e.
the preposition is never tagged as II in combi-
nation with the given head verb). Group B con-
tains the verb?preposition token instances iden-
tified as VPCs by RASP where there were also
instances of that same combination identified as
verb-PPs. Group C contains the verb?preposition
token instances identified as verb-PPs by RASP
where there were also instances of that same com-
bination identified as VPCs. Finally, group D
contains the verb-preposition combinations which
were tagged exclusively as verb-PPs by RASP.
We focus particularly on disambiguating verb?
preposition token instances falling into groups B
and C, where RASP has identified an ambiguity
for that particular combination. We do not further
classify token instances in group D, on the grounds
that (a) for high-frequency verb?preposition com-
binations, RASP was unable to find a single in-
stance warranting a VPC analysis, suggesting it
had high confidence in its ability to correctly iden-
tify instances of this lexical type, and (b) for low-
frequency verb?preposition combinations where
the confidence of there definitively no being a
VPC usage is low, the token sample is too small
to disambiguate effectively and the overall impact
would be negligible even if we tried. We do, how-
ever, return to considered data in group D in com-
puting the precision and recall of RASP.
Naturally, the output of RASP parser is not
error-free, i.e. VPCs may be parsed as verb-PPs
FPR FNR Agreement
Group A 4.08% ? 95.24%
Group B 3.96% ? 99.61%
Group C ? 10.15% 93.27%
Group D ? 3.4% 99.20%
Table 1: False positive rate (FPR), false negative
rate (FNR) and inter-annotator agreement across
the four groups of token instances
f ? 1 f ? 5
VPC V-PP VPC V-PP
Group A 5,223 0 3,787 0
Group B 1,312 0 1,108 0
Group C 0 995 0 217
Total 6,535 995 4,895 217
Table 2: The number of VPC and verb-PP token
instances occurring in groups A, B and C at vary-
ing frequency cut-offs
and vice versa. In particular, other than the re-
ported results of McCarthy et al (2003) targeting
VPCs vs. all other analyses, we had no a priori
sense of RASP?s ability to distinguish VPCs and
verb-PPs. Therefore, we manually checked the
false-positive and false-negative rates in all four
groups and obtained the performance of parser
with respect to VPCs. The verb-PPs in group A
and B are false-positives while the VPCs in group
C and D are false-negatives (we consider the VPCs
to be positive examples).
To calculate the number of incorrect examples,
two human annotators independently checked
each verb?preposition instance. Table 1 details the
rate of false-positives and false-negative examples
in each data group, as well as the inter-annotator
agreement (calculated over the entire group).
4.2 Collection
We combined together the 6,535 (putative) VPCs
and 995 (putative) verb-PPs from groups A, B and
C, as identified by RASP over the corpus data. Ta-
ble 2 shows the number of VPCs in groups A and
B and the number of verb-PPs in group C. The
first number is the number of examples occuring
at least once and the second number that of exam-
ples occurring five or more times.
From the sentences containing VPCs and verb-
PPs, we retrieved a total of 8,165 nouns, including
68
Type Groups A&B Group C
common noun 7,116 1,239
personal pronoun 629 79
demonstrative pronoun 127 1
proper noun 156 18
who 94 6
which 32 0
No sense (what) 11 0
Table 3: Breakdown of subject and object head
nouns in group A&B, and group C
pronouns (e.g. I, he, she), proper nouns (e.g. CITI,
Canada, Ford) and demonstrative pronouns (e.g.
one, some, this), which occurred as the head noun
of a subject or object of a VPC in group A or B.
We similarly retrieved 1,343 nouns for verb-PPs in
group C. Table 3 shows the distribution of different
noun types in these two sets.
We found that about 10% of the nouns are pro-
nouns (personal or demonstrative), proper nouns
or WH words. For pronouns, we manually re-
solved the antecedent and took this as the head
noun. When which is used as a relative pronoun,
we identified if it was coindexed with an argument
position of a VPC or verb-PP, and if so, manually
identified the antecedent, as illustrated in (5).
(5) EX: Tom likes the books which he sold off.
ARGS: heSUBJ = person
whichOBJ = book
With what, on the other hand, we were gener-
ally not able to identify an antecedent, in which
case the argument position was left without a word
sense (we come back to this in Section 6).
(6) Tom didn?t look up what to do.
What went on?
We also replaced all proper nouns with cor-
responding common noun hypernyms based on
manual disambiguation, as the coverage of proper
nouns in WordNet is (intentionally) poor. The fol-
lowing are examples of proper nouns and their
common noun hypernyms:
Proper noun Common noun hypernym
CITI bank
Canada country
Ford company
Smith human
produce, green goods, ...
food(3rd)
...
reproductive structure
...
pome, false fruit
reproductive structure
fruit
fruit(2nd)
citrus, citrus fruit, citrous fruit
edible fruit(2nd)
edible fruit(1st)apple
Sense 1
Sense 1
orange
produce, green goods, ...
food(4th)
...
..
fruit(3rd)
Figure 2: Senses of apple and orange
When we retrieved the first word sense of nouns
from WordNet, we selected the first sense and the
associated hypernyms (up to) three levels up the
WordNet hierarchy. This is intended as a crude
form of smoothing for closely-related word senses
which occur in the same basic region of the Word-
Net hierarchy. As an illustration of this process,
in Figure 2, apple and orange are used as edi-
ble fruit, fruit or food, and the semantic overlap is
picked up on by the fact that edible fruit is a hy-
pernym of both apple and orange. On the other
hand, food is the fourth hypernym for orange so it
is ignored by our method. However, because we
use the four senses, the common senses of nouns
are extracted properly. This approach works rea-
sonably well for retrieving common word senses
of nouns which are in the immediate vicinity of
each other in the WordNet hierarchy, as was the
case with apple and orange. In terms of feature
representation, we generate an individual instance
for each noun sense generated based on the above
method, and in the case that we have multiple ar-
guments for a given VPC or verb-PP (e.g. both a
subject and a direct object), we generate an indi-
vidual instance for the cross product of all sense
combinations between the arguments.
We use 80% of the data for training and 20%
for testing. The following is the total number of
training instances, before and after performing hy-
pernym expansion:
Training Instances
Before expansion After expansion
Group A 5,223 24,602
Group B 1,312 4,158
Group C 995 5,985
69
Group Frequency of VPCs Size
B (f?1 ) test:272
(f?5 ) train:1,040
BA (f?1 & f?1 ) test:1,327
(f?5 & f?5 ) train:4,163
BC (f?1 & f?1 ) test:498
(f?5 & f?1 ) train:1,809
BAC (f?1 & f?1 & f?1 ) test:1,598
(f?5 & f?5 & f?1 ) train:5,932
Table 4: Data set sizes at different frequency cut-
offs
5 Evaluation
We selected 20% of the test data from different
combinations of the four groups and over the two
frequency thresholds, leading to a total of 8 test
data sets. The first data set contains examples from
group B only, the second set is from groups B and
A, the third set is from groups B and C, and the
fourth set is from groups B, A and C. Addition-
ally, each data set is divided into: (1) f ? 1, i.e.
verb?preposition combinations occurring at least
once, and (2) f ? 5, i.e. verb?preposition com-
binations occurring at least five times (hereafter,
f ? 1 is labelled f?1 and f ? 5 is labelled f?5 ).
In the group C data, there are 217 verb-PPs with
f?5 , which is slightly more than 20% of the data
so we use verb-PPs with f?1 for experiments in-
stead of verb-PP with f?5 . The first and second
data sets do not contain negative examples while
the third and fourth data sets contain both positive
and negative examples. As a result, the precision
for the first two data sets is 1.0.
Table 5 shows the precision, recall and F-score
of our method over each data set, relative to the
identification of VPCs only. A,B,C are groups and
f# is the frequency of examples.
Table 6 compares the performance of VPC iden-
tification and verb-PP identification.
Table 7 indicates the result using four word
senses (i.e. with hypernym expansion) and only
one word sense (i.e. the first sense only).
6 Discussion
The performance of RASP as shown in Tables 5
and 6 is based on human judgement. Note that
we only consider the ability of the parser to distin-
guish sentences with prepositions as either VPCs
or verb-PPs (i.e. we judge the parse to be correct if
the preposition is classified correctly, irrespective
of whether there are other errors in the output).
Data Freq P R F
RASP f?1 .959 .955 .957
B f?1 1.0 .819 .901
f?5 1.0 .919 .957
BA f?1 f?1 1.0 .959 .979
f?5 f?5 1.0 .962 .980
BC f?1 f?1 .809 .845 .827
f?5 f?1 .836 .922 .877
BAC f?1 f?1 f?1 .962 .962 .962
f?5 f?5 f?1 .964 .983 .974
Table 5: Results for VPC identification only (P =
precision, R = recall, F = F-score)
Data Freq Type P R F
RASP f?1 P+V .933 ? ?
BC f?1 f?1 P+V .8068 .8033 .8051
f?5 f?1 P+V .8653 .8529 .8591
BAC f?1 f?1 P+V .8660 .8660 .8660
f?5 f?1 P+V .9272 .8836 .9054
Table 6: Results for VPC (=V) and verb-PP (=P)
identification (P = precision, R = recall, F = F-
score)
Also, we ignore the ambiguity between particles
and adverbs, which is the principal reason for our
evaluation being much higher than that reported
by McCarthy et al (2003). In Table 5, the preci-
sion (P) and recall (R) for VPCs are computed as
follows:
P = Data Correctly Tagged as VPCs
Data Retrieved as VPCs
R = Data Correctly Tagged as VPCs
All VPCs in Data Set
The performance of RASP in Table 6 shows
how well it distinguishes between VPCs and verb-
PPs for ambiguous verb?preposition combina-
tions. Since Table 6 shows the comparative per-
formance of our method between VPCs and verb-
PPs, the performance of RASP with examples
which are misrecognized as each other should be
the guideline. Note, the baseline RASP accuracy,
based on assigning the majority class to instances
in each of groups A, B and C, is 83.04%.
In Table 5, the performance over high-
frequency data identified from groups B, A and
C is the highest (F-score = .974). In general, we
would expect the data set containing the high fre-
quency and both positive and negative examples
70
Freq Type # P R F
f?1 V 4WS .962 .962 .962
1WS .958 .969 .963
f?1 P 4WS .769 .769 .769
1WS .800 .743 .770
f?5 V 4WS .964 .983 .974
1WS .950 .973 .962
f?5 P 4WS .889 .783 .832
1WS .813 .614 .749
Table 7: Results with hypernym expansion (4WS)
and only the first sense (1WS), in terms of preci-
sion (P), recall (R) and F-score (F)
to give us the best performance at VPC identifi-
cation. We achieved a slightly better result than
the 95.8%-97.5% performance reported by Li et
al. (2003). However, considering that Li et al
(2003) need considerable time and human labour
to generate hand-coded rules, our method has ad-
vantages in terms of both raw performance and
labour efficiency.
Combining the results for Table 5 and Table 6,
we see that our method performs better for VPC
identification than verb-PP identification. Since
we do not take into account the data from group
D with our method, the performance of verb-PP
identification is low compared to that for RASP,
which in turn leads to a decrement in the overall
performance.
Since we ignored the data from group D con-
taining unambiguous verb-PPs, the number of pos-
itive training instances for verb-PP identification
was relatively small. As for the different number
of word senses in Table 7, we conclude that the
more word senses the better the performance, par-
ticularly for higher-frequency data items.
In order to get a clearer sense of the impact of
selectional preferences on the results, we investi-
gated the relative performance over VPCs of vary-
ing semantic compositionality, based on 117 VPCs
(f?1 ) attested in the data set of McCarthy et al
(2003). According to our hypothesis from above,
we would expect VPCs with low composition-
ality to have markedly different selectional pref-
erences to the corresponding simplex verb, and
VPCs with high compositionality to have similar
selectional preferences to the simplex verb. In
terms of the performance of our method, therefore,
we would expect the degree of compositionality
to be inversely proportional to the system perfor-
mance. We test this hypothesis in Figure 3, where
we calculate the error rate reduction (in F-score)
 0
 20
 40
 60
 80
 100
 0  1  2  3  4  5  6  7  8  9  10 0
 20
 40
 60
 80
 100
Err
or R
ate
 Re
duc
tion
 (%
)
Typ
es
Compositionality
Figure 3: Error rate reduction for VPCs of varying
compositionality
for the proposed method relative to the majority-
class baseline, at various degrees of composition-
ality. McCarthy et al (2003) provides compo-
sitionality judgements from three human judges,
which we take the average of and bin into 11 cate-
gories (with 0 = non-compositional and 10 = fully
compositional). In Figure 3, we plot both the er-
ror rate reduction in each bin (both the raw num-
bers and a smoothed curve), and also the number
of attested VPC types found in each bin. From
the graph, we see our hypothesis born out that,
with perfect performance over non-compositional
VPCs and near-baseline performance over fully
compositional VPCs. Combining this result with
the overall results from above, we conclude that
our method is highly successful at distinguishing
non-compositional VPCs from verb-PPs, and fur-
ther that there is a direct correlation between the
degree of compositionality and the similarity of
the selectional preferences of VPCs and their verb
counterparts.
Several factors are considered to have influ-
enced performance. Some data instances are miss-
ing head nouns which would assist us in determin-
ing the semantics of the verb?preposition combi-
nation. Particular examples of this are imperative
and abbreviated sentences:
(7) a. Come in.
b. (How is your cold?) Broiled out.
Another confounding factor is the lack of word
sense data, particularly in WH questions:
(8) a. What do I hand in?
b. You can add up anything .
71
7 Conclusion
In this paper, we have proposed a method for iden-
tifying VPCs automatically from raw corpus data.
We first used the RASP parser to identify VPC
and verb-PP candidates. Then, we used analysis of
the head nouns of the arguments of the head verbs
to model selectional preferences, and in doing so,
distinguish between VPCs and verb-PPs. Using
TiMBL 5.1, we built a classifier which achieved
an F-score of 97.4% at identifying frequent VPC
examples. We also investigated the comparative
performance of RASP at VPC identification.
The principal drawback of our method is that it
relies on the performance of RASP and we assume
a pronoun resolution oracle to access the word
senses of pronouns. Since the performance of such
systems is improving, however, we consider our
approach to be a promising, stable method of iden-
tifying VPCs.
Acknowledgements
This material is based upon work supported in part by the
Australian Research Council under Discovery Grant No.
DP0663879 and NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation. We would
like to thank the three anonymous reviewers for their valu-
able input on this research.
References
Timothy Baldwin and Aline Villavicencio. 2002. Extract-
ing the unextractable: A case study on verb-particles. In
Proc. of the 6th Conference on Natural Language Learn-
ing (CoNLL-2002), pages 98?104, Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Do-
minic Widdows. 2003. An empirical model of multiword
expression decomposability. In Proc. of the ACL-2003
Workshop on Multiword Expressions: Analysis, Acquisi-
tion and Treatment, pages 89?96, Sapporo, Japan.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expressions,
19(4):398?414.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the ACL-2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treatment,
pages 65?72, Sapporo, Japan.
Ted Briscoe and John Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of the 3rd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-
lands.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod, and
Antonio Zampolli. 2002. Towards best practice for mul-
tiword expressions in computational lexicons. In Proc. of
the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 1934?40, Las Pal-
mas, Canary Islands.
Ann Copestake and Alex Lascarides. 1997. Integrating sym-
bolic and statistical representations: The lexicon pragmat-
ics interface. In Proc. of the 35th Annual Meeting of the
ACL and 8th Conference of the EACL (ACL-EACL?97),
pages 136?43, Madrid, Spain.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. ILK Technical Re-
port 04-02.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
B. Fraser. 1976. The Verb-Particle Combination in English.
The Hague: Mouton.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, and Ro-
hini K. Srihari. 2003. An expert lexicon approach to iden-
tifying English phrasal verbs. In Proc. of the 41st Annual
Meeting of the ACL, pages 513?20, Sapporo, Japan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?30.
Diana McCarthy, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proc. of the 42nd Annual Meeting of the ACL, pages
280?7, Barcelona, Spain.
Tom O?Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the
7th Conference on Natural Language Learning (CoNLL-
2003), pages 79?86, Edmonton, Canada.
Darren Pearce. 2001. Synonymy in collocation extraction.
In Proceedings of the NAACL 2001 Workshop on WordNet
and Other Lexical Resources: Applications, Extensions
and Customizations, Pittsburgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proc. of the 3rd Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 1?15, Mex-
ico City, Mexico.
Aline Villavicencio. 2003. Verb-particle constructions and
lexical resources. In Proc. of the ACL-2003 Workshop on
Multiword Expressions: Analysis, Acquisition and Treat-
ment, pages 57?64, Sapporo, Japan.
Aline Villavicencio. 2006. Verb-particle constructions in the
world wide web. In Patrick Saint-Dizier, editor, Compu-
tational Linguistics Dimensions of Syntax and Semantics
of Prepositions. Springer, Dordrecht, Netherlands.
Dominic Widdows and Beate Dorow. 2005. Automatic ex-
traction of idioms using graph analysis and asymmetric
lexicosyntactic patterns. In Proc. of the ACL-SIGLEX
2005 Workshop on Deep Lexical Acquisition, pages 48?
56, Ann Arbor, USA.
72
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 49?56,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Dynamic Path Prediction and Recommendation in a Museum Environment
Karl Grieser??, Timothy Baldwin? and Steven Bird?
? CSSE
University of Melbourne
VIC 3010, Australia
? DIS
University of Melbourne
VIC 3010, Australia
{kgrieser,tim,sb}@csse.unimelb.edu.au
Abstract
This research is concerned with making
recommendations to museum visitors
based on their history within the physical
environment, and textual information
associated with each item in their history.
We investigate a method of providing
such recommendations to users through
a combination of language modelling
techniques, geospatial modelling of
the physical space, and observation of
sequences of locations visited by other
users in the past. This study compares
and analyses different methods of path
prediction including an adapted naive
Bayes method, document similarity, visitor
feedback and measures of lexical similarity.
1 Introduction
Visitors to an information rich environment such as
a museum, are invariably there for a reason, be it
entertainment or education. The visitor has paid
their admission fee, and we can assume they intend
to get the most out of their visit. As with other
information rich environments and systems, first-
time visitors to the museum are at a disadvantage as
they are not familiar with every aspect of the collec-
tion. Conversely, the museum is severely restricted
in the amount of information it can convey to the
visitor in the physical space.
The use of a dynamic, intuitive interface can over-
come some of these issues (Filippini, 2003; Benford
et al, 2001). Such an interface would convention-
ally take the form of a tour guide, audio tour, or a
curator stationed at points throughout the museum.
This research is built around the assumption that the
museum visitor has access to a digital device such
as a PDA and that it is possible for automatic sys-
tems to interact with the user via this device. In this
way we aim to be able to deliver relevant content
to the museum visitor based on observation of their
movements within the physical museum space, as
well as make recommendations of what exhibits they
might like to visit next and why. At present, we are
focusing exclusively on the task of recommendation.
Recommendations can be used to convey predic-
tions about what theme or topic a given visitor is
interested in. They can also help to communicate
unexpected connections between exhibits (Hitzeman
et al, 1997), or explicitly introduce variety into the
visit. For the purposes of this research, we focus
on this first task of providing recommendations con-
sistent with the visitor?s observed behaviour to that
point. We investigate different factors which we
hypothesise impact on the determination of what
exhibits a given visitor will visit, namely: the phys-
ical proximity of exhibits, the conceptual similarity
of exhibits, and the relative sequence in which other
visitors have visited exhibits.
Recommendation systems in physical environ-
ments are notoriously hard to evaluate, as the
recommendation system is only one of many stimuli
which go to determine the actual behaviour of the
visitor. In order to evaluate the relative impact
of different factors in determining actual visitor
behaviour, we separate the stimuli present into
a range of predictive methods. In this paper we
target the task of user prediction, that is prediction
of what exhibit a visitor will visit next based on
49
their previous history. Language based models are
intended to simulate a potentially unobservable
source of information: the visitor?s thought process.
In order to identify the reason for the visitor?s
interest in the multiple part exhibits we parallel this
problem with the task of word sense disambiguation
(WSD). Determining the visitor?s reason for visiting
an exhibit allows a predictive system to more
accurately model the visitor?s future path.
This study aims to arrive at accurate methods of
predicting how a user will act in an information-rich
museum. The space focused on in this research is
the Australia Gallery Collection of the Melbourne
Museum, at Carlton Gardens in Melbourne,
Australia. The predictions take the form of which
exhibits a visitor will visit given a history of
previously visited exhibits. This study analyses
and compares the effectiveness of supervised and
unsupervised learning methods in the museum
domain, drawing on a range of linguistic and
geospatial features. A core contribution of
this study is its focus on the relative import of
heterogeneous information sources a user makes
use of in selecting the next exhibit to visit.
2 Problem Description
In order to recommend exhibits to visitors while they
are going through a museum, the recommendations
need to be accurate/pertinent to the goals that the
visitor has in mind. Without accurate recomme nda-
tions, recommendations given to a visitor are essen-
tially useless, and might as well not have been rec-
ommended at all.
Building a recommender system based on contex-
tual information (Resnick and Varian, 1997) is the
ultimate goal of this research. However the envi-
ronment in this circumstance is physical, and the
actions of visitors are expected to vary within such
a space, as opposed to the usual online or digital
domain of recommender systems. Studies such as
HIPS (Benelli et al, 1999) and the Equator project1
have analysed the importance and difficulty of inte-
grating the virtual environment into the physical, as
well as identifying how non-physical navigation sys-
tems can relate to similar physical systems. For the
purpose of this study, it is sufficient to acknowledge
1http://www.equator.ac.uk
the effect of the physical environment by scaling all
recommendations against their distances from one
another.
The common information that museum exhibits
contain is key in determining how each individual
relates to each other exhibit in the collection. At
the most basic level, the exhibits are simply isolated
elements that share no relationship with one another,
their only similarity being that they occur together
in visitor paths. This interpretation disregards any
meaning or content that each exhibit contains. But
museum exhibits are created with the goal of pro-
viding information, and to disregard the content of
an exhibit is to disregard its purpose.
An exhibit in a museum may be many kinds of
things, and hence most exhibits will differ in presen-
tation and content. The target audience of a museum
is one indicator of the type of content that can be
expected within each exhibit. An art gallery is com-
prised of mainly paintings and sculptures: single
component exhibits with brief descriptions. A chil-
dren?s museum will contain a high proportion of
interactive exhibits, and much audio and visual con-
tent. In these two cases the reason for visiting the
exhibit differs greatly.
Given the diversity of information contained
within each exhibit and the greater diversity of a
museum collection, it can be difficult to see why
visitors only examine certain exhibits during their
tours. It is very difficult to perceive what a visitor?s
intention is without constant feedback, making the
problem of providing relevant recommendations a
question of predicting what a visitor is interested in
based on characteristics of exhibits the visitor has
already seen. The use of both physical attributes and
exhibit information content are used in conjunction
in an effort to account for multiple possible reasons
for visiting as exhibit. Connections between
physical attributes of an exhibit are easier to identify
than connections based on information content.
This is due to the large quantity of information
associated with each exhibit, and the difficulty in
determining what the visitor liked (or disliked)
about the exhibit.
In order to make prediction based on a visitor?s
history, the importance of the exhibits in the visi-
tors path must be known. This is difficult to obtain
directly without the aid of real-time feedback from
50
the user themselves. In an effort to emulate the
difficulty of observing mental processes adopted by
each visitor, language based predictive models are
employed.
3 Resources
The domain in which all experimentation takes place
is the Australia Gallery of the Melbourne Museum.
This exhibition provides a history of the city of Mel-
bourne Melbourne, from its settlement up to the
present day, and includes such exhibits as the taxi-
dermised coat of Phar Lap (Australia?s most famous
race horse) and CSIRAC (Australia?s first, and the
world?s fourth, computer). The Gallery contains
enough variation so that not all exhibits can be clas-
sified into a single category, but is sufficiently spe-
cialised to offer much interaction and commonality
between the exhibits.
The exhibits within the Australia Gallery take
a wide variety of forms, from single items with
a description plaque, to multiple component dis-
plays with interactivity and audio-visual enhance-
ment; note, for our purposes in experimentation,
we do not differentiate between exhibit types or
modalities. The movement of visitors within an
exhibition can be restricted if the positioning of the
exhibits require visitors to take a set path (Peponis
et al, 2004), which can alter how a visitor chooses
between exhibits to view. In the case of the Australia
Gallery, however, the collection is spread out over
a sizeable area, and has an open plan design such
that visitor movement is not restricted or funnelled
through certain areas and there is no predetermined
sequence or selection of exhibits that a given visitor
can be expected to spend time at.
We used several techniques to represent the dif-
ferent aspects of each exhibit. We categorised each
exhibit by way of its physical attributes (e.g. size)
and taxonomic information about the exhibit con-
tent (e.g. clothing or animal). We also described
each exhibit by way of its physical location within
the Australia Gallery, relative to a floorplan of the
Gallery.
The Melbourne Museum also has a sizable
web-site2 which contains much detailed information
about the exhibits within the Australia Gallery. This
2http://www.museum.vic.gov.au/
data is extremely useful in that it provides a rich
vocabulary of information based on the content
of each exhibit. Each exhibit identified within the
Australia Gallery has a corresponding web-page
describing it. The information content of an exhibit
is made up of the text in its corresponding web-page
combined with its attributes. By having a large
source of natural language information associated
with the exhibit, linguistic based predictive methods
can more accurately identify the associations made
by visitors.
The dataset that forms that basis of this research
is a database of 60 visitor paths through the Aus-
tralia Gallery, which was collected by Melbourne
Museum staff over a period of four months towards
the end of 2001. The Australia Gallery contains a
total of fifty-three exhibits. This data is used to eval-
uate both physical and conceptual predictive meth-
ods. If predictive methods are able to accurately
describe how a visitor travels in a museum, then
the predictive method creates an accurate model of
visitor behaviour.
Exhibit components can be combined to form a
description for each exhibit. For this purpose, the
Natural Language Toolkit 3 (Bird, 2005) was used
to analyse and compare the lexical content associ-
ated with each exhibit, so that relationships between
exhibits can be identified.
4 Methodology
Analysis of user history as a method of prediction
(or recommendation) has been examined in
Chalmers et al (1998). Also discussed is the
role that user history plays in anticipating user
goals. This approach can be adapted to a physical
environment by simply substituting in locations
visited in place of web pages visited. Data gathered
from the paths of previous visitors also forms a valid
means of predicting other visitors? paths (Zukerman
and Albrecht, 2001). This approach operates under
the assumption that all visitors behave in a similar
fashion when visiting a museum. However visitors?
goals in visiting a museum can differ widely. For
example, the goals of a student researching a project
will differ to those of a family with young children
on a weekend outing.
3http://nltk.sourceforge.net/
51
A conceptual model of the exhibition space is cre-
ated by visitors with a specific task in mind. Inter-
pretation of this conceptual model is key to creating
accurate recommendations. The building of such a
conceptual model takes place from the moment a
visitor enters an exhibition, until the time they leave,
and skews the visitor towards groups of conceptual
locations and categories.
The representation of these intrinsically dynamic
models is directly related to the task the visitor has
in mind. Students will form a conceptual model
based around their course requirements, children
around the most visually attractive exhibits, and
so forth. This necessitates the need for multiple
exhibit similarity measures, however in the absence
of express knowledge of the ?type? of each visitor in
the sample data, a broad-coverage recommendation
system that functions best in all circumstances is the
desired goal. It is hoped that in future, reevaluation
of the data to classify visitors into broad categories
(e.g. information seeking, entertainment seeking)
will allow for the development of specialised
models tailored to visitor types.
The models of exhibit representation we exam-
ine in this research are exhibit proximity, text-based
exhibit information content, and exhibit popularity
(based on the previous visitor data provided by the
Melbourne Museum), as well as combinations of the
three. Exhibit information content is a two part rep-
resentation: primarily each exhibit has a large body
of text describing the exhibit drawn from the Mel-
bourne Museum website. It is fortunate that this
information is curated, and managed from a cen-
tral source, so that inconsistencies between exhibit
information are extremely rare. The authors were
unable to find any contradictory information in the
web-pages used for experimentation, as may be the
case with larger non-curated document bodies. The
second component of the information content is a
small set of key terms describing the attributes of
the exhibit. Textual content as a means of deter-
mining exhibit similarity has been analysed previ-
ously (Green et al, 1999), both in terms of keyword
attributes and bodies of explanatory text.
In order to form a prediction about which exhibit
a visitor will next visit, the probability of the tran-
sition of the visitor from their current location to
every other exhibit in the collection must be known.
Prediction of the next exhibit by proximity simply
means choosing the closest not-yet-visited exhibit to
the visitor?s current location. In terms of information
content, each exhibit is related to all other exhibits to
a certain degree. To express this we use the attribute
keywords as a query to find the exhibit most simi-
lar. We use the attribute keywords associated with
each document to search the document space of the
exhibits to find the exhibit that is most similar to the
exhibit the visitor is currently located at. To do this
we use a simple tf?idf scheme, using the attribute
keywords as the queries, and the exhibit associated
web pages as the document space. The score of each
query over each document is normalised into a tran-
sitional probability array such that
?
j P (q|dj) = 1
for a query (q) over the j exhibit documents (dj).
In order to determine the popularity of an
exhibit, the visitor paths provided by the Melbourne
Museum were used to form another matrix of
transitional probabilities based on the likelihood
that a visitor will travel to an exhibit from the
exhibit they are currently at. I.e. for each exhibit e
an array of transitional probabilities is formed such
that
?
j P (e|cj) = 1 where cj ? C ? = C/{e}, i.e.
all exhibits other than e. In both cases Laplacian
smoothing was used to remove zero probabilities.
The methods of exhibit popularity and physical
proximity are superficial in scope and do not extend
into the conceptual space adopted by the visitors.
They do however give insight into how a physical
space affects a visitors? mental representation of the
conceptual areas associated with specific exhibit col-
lections, and are more easily observable. Visitor
reaction to exhibit information content is harder to
observe and more problematic to predict. Any accu-
rate recommender systems produced in this fashion
will need to take into account the limitations these
two methods place on the thought processes of visi-
tors.
Connections that visitors make between exhibits
are more fluid, and are harder to represent in terms
of similarity measures. Specifically it is difficult to
see why visitors make connections between exhibits
as there can be multiple similarities between two
exhibits. To this end we have equated this prob-
lem with the task of Word Sense Disambiguation
(WSD). The path that a visitor takes can be seen
as a sentence of exhibits, and each exhibit in the
52
sentence has an associated meaning. WSD is used
to determine the meaning of the next exhibit based
on the meanings of previous exhibits in the path. For
each word in the keyword set of each exhibit, the
WordNet (Fellbaum, 1998) similarity is calculated
against each word in another exhibit. The similar-
ity is the sum of the WordNet similarities between
all attribute keywords in the two exhibits (K1, K2),
normalised over the length of both keyword sets:
?
k1?K1
?
k2?K2 WNsim(k1, k2)
|K1||K2|
For the purposes of this experiment we have
chosen to use three WordNet similarity/relatedness
measures to simulate the conceptual connections
that visitors make between exhibits. The Lin (Lin,
1998) and Leacock-Chodorow (Leacock et al,
1998) similarity measures and the Banerjee-
Pedersen (Patwardhan and Pedersen, 2003)
relatedness measures were used. The similarities
were normalised and transformed into probability
matrices such that
?
j PWNsim(e|cj) = 1 for each
next exhibit ci. The use of WordNet measures is
intended to simulate the mental connections that
visitors make between exhibit content, given that
each visit can interpret content in a number of
different ways.
The history of the visitor at any given time is
essential in keeping the visitor?s conceptual model
of the exhibit space current. The recency of a given
exhibit within a visitor?s history is inversely propor-
tional to how long ago the exhibit was encountered.
To take into account the visitor history, the col-
laborative data, proximity, document vectors, and
conceptual WordNet similarity, we adapt the naive
Bayes approach. The conditional probabilities of
each method are combined along with the temporal
recency of an exhibit to produce a predictive exhibit
recommender. The resultant recommendation to a
visitor can be described as follows:
c? = arg max
ci
P (ci)
t
?
j=1
P (Aj |ci) ? 2?(t?j+1) +
2?t
t
where t is the length of the visitor?s history, Aj ? C
is an exhibit at time j in the visitor history (and C
is the full set of exhibits), and ci ? C ? = C/{Aj}
is each unvisited exhibit. The most probable next
exhibit (c?) is selected from all possible next exhibits
(ci). Any selections made must be compared against
the visitor?s history. In this, we assume that a pre-
viously visited exhibit has already been seen, and
hence should not be recommended again.
The effectiveness of these methods was tested in
multiple combinations, both with history modeling
and without (only the exhibit the visitor is currently
at is considered). Testing was carried out using
the sixty visitor paths supplied by the Melbourne
Museum. For each method two tests were carried
out:
? Predict the next exhibit in the visitor?s path.
? Only make a prediction if the probability of the
prediction is above a given threshold.
Each path was analysed independently of the oth-
ers, and the resulting recommendations evaluated as
a whole. The measures of precision and recall in
the evaluation of recommender systems has been
applied effectively in previous studies (Raskutti et
al., 1997; Basu et al, 1998). In the second test
precision is the measure we are primarily concerned
with: it is not the aim of this recommender system to
predict all elements of a visitor?s path in the correct
order. The correctness of the exhibits predicted is
more important than the quantity of the predictions
the visitor visits, hence only exhibits predicted with
a (relatively) high probability are included in the
final list of predicted exhibits for that visitor.
The thresholds are designed to increase the cor-
rectness of the predictions, by only making a pre-
diction if there is a high probability of the visitor
travelling to the exhibit. As all predictive methods
choose the most probable transition from all possible
transitions, the transition with the highest probabil-
ity is always selected. The threshold values simply
cut off all probabilities below a certain value.
5 Results and Evaluation
The first tests carried out were done only using the
simple probability matrices described in Section 4,
and hence only use the information associated with
the visitor?s current location and not the entirety of
their history. The baseline method being used in all
testing is the naive method of moving to the closest
not-yet-visited exhibit.
53
Method BOE Accuracy
Proximity (baseline) 0.270 0.192
Popularity 0.406 0.313
Tf?Idf 0.130 0.018
Lin 0.129 0.039
Leacock-Chodorow 0.116 0.024
Banerjee-Pedersen 0.181 0.072
Popularity - Tf?Idf 0.196 0.093
Popularity - Lin 0.225 0.114
Popularity - Leacock-Chodorow 0.242 0.130
Popularity - Banerjee-Pedersen 0.163 0.064
Proximity - Tf?Idf 0.205 0.084
Proximity - Lin 0.180 0.114
Proximity - Leacock-Chodorow 0.220 0.151
Proximity - Banerjee-Pedersen 0.205 0.105
Proximity - Popularity 0.232 0.129
Table 1: Single exhibit history using individual and
combined transitional probabilities
In order to prevent specialisation of the methods
over the training data (the aforementioned 60 visitor
paths), 60 fold cross-validation was used. With the
path being used as the test case removed from the
training data at each iteration.
The results of prediction using only the current
exhibit as information can be seen in Table 1. Com-
binations of predictive methods are also included to
add physical environment factors to conceptual sim-
ilarity methods. For example, if two exhibits may
be highly related conceptually but on opposite sides
of the exhibit space, a visitor may forgo the distant
exhibit in favour of a closer exhibit that is slightly
less relevant.
Due to the lengths of the recommendation sets
made for each visitor (a recommendation is made
for each exhibit visited), precision and recall are
identical. The measure of Bag Of Exhibits (BOE)
describes the percentage of exhibits that were visited
by the visitor, but not necessarily in the same order
as they were recommended. The BOE measure is
the same as measuring precision and recall for the
purposes of this evaluation. With the introduction of
thresholds to improve precision, precision and recall
are measured as separate entities.
As seen in Table 1 the performance of the
conceptual or information similarity methods
(the tf?idf method, Lin, Leacock-Chodorow and
Banerjee-Pedersen) is worse than that of the
methods based on static features of the exhibits,
and all perform worse than the baseline. In
order to produce a higher percentage of correct
recommendations, thresholds were introduced.
Using thresholds, a recommendation is only made
if the probability of a visitor visiting an exhibit next
is above a given percentage. The thresholds used
in Table 2 are arbitrary, and were arrived at after
experimentation.
It is worth noting that in both tests, with and
without thresholds, the method of exhibit popularity
based on visitor paths is the most successful. One
expects this trend to continue with the introduction
of the history based model described in Section 4.
Each transitional probability matrix was used in con-
junction with the history model, the results of this
experimentation can be seen in Table 3.
Only single transitional probability matrices are
used in conjunction with the history model. The
physical distance to an exhibit is only relevant to the
current prediction, the distance travelled in the past
from exhibit to exhibit is irrelevant, and so physical
conceptual combinations are not necessary. A model
such as this describes the evolution of a thought pro-
cess, or is able to identify the common conceptual
thread linking the exhibits in a visitor?s path. This
is only true if the visitor has a conceptual model in
mind when touring the museum. Without the aid of
a common information thread, conceptual predictive
methods based on exhibit information content will
always perform poorly.
6 Discussion
The visitor paths supplied by the Melbourne
Museum represent sequential lists of exhibits, and
each visitor is a black box travelling from exhibit
to exhibit. It is this token vs. type problem that
does not allow us to select an appropriate predictive
method with which to make recommendations.
Instead a broad coverage method is necessary. Use
of history models to analyse entire visitor paths are
less successful than analysis of solely the current
location of the visitor. This can be attributed to the
fact that a majority of the visitors tracked may not
have had preconceived tasks in mind when they
entered the museum space, and just moved from
one visually impressive exhibit to the next. The
visitors do not consider their entire history as being
relevant, and only take into account their current
54
Method Threshold Precision Recall F-score
Proximity 0.03 0.271 0.270 0.270
Popularity 0.06 0.521 0.090 0.153
Tf?Idf 0.06 0.133 0.122 0.128
Lin 0.01 0.129 0.129 0.129
Leacock-Chodorow 0.01 0.117 0.117 0.117
Banerjee-Pedersen 0.01 0.182 0.180 0.181
Popularity - Tf?Idf 0.001 0.176 0.154 0.164
Popularity - Lin 0.0005 0.383 0.316 0.348
Popularity - Leacock-Chodorow 0.0005 0.430 0.349 0.385
Popularity - Banerjee-Pedersen 0.001 0.236 0.151 0.184
Proximity - Tf?Idf 0.001 0.189 0.174 0.181
Proximity - Lin 0.0005 0.239 0.237 0.238
Proximity - Leacock-Chodorow 0.0005 0.252 0.250 0.251
Proximity - Banerjee-Pedersen 0.0005 0.182 0.180 0.181
Proximity - Popularity 0.001 0.262 0.144 0.186
Table 2: Single exhibit history predictive methods using thresholds
Method BOE Accuracy
Proximity 0.066 0.0
Popularity 0.016 0.0
Tf?Idf 0.033 0.0
Lin 0.064 0.0
Leacock-Chodorow 0.036 0.0
Banerjee-Pedersen 0.036 0.0
Table 3: Entire visitor history predictive methods.
context. This also explains the relative success of
the predictive method built from analysis of the
visitor paths, presenting a marked improvement
over the baseline of nearest exhibit. In the best case
(as seen in Table 2) the exhibit popularity predictive
method was able to give relevant recommendations
52% of the time.
The interaction between predictive methods here
is highly simplified. The assumption made is that all
aspects of the visitor?s conceptual model are inde-
pendent, or only interact on a superficial level (see
the lower halves of Tables 1?2). More complex
methods of prediction need to be explored fully
take into account the interaction between predictive
methods.
Representations based on physical proximity take
into account little of how a visitor conceptualises a
museum space. They do however describe the fact
that closer exhibits are more visible to visitors, and
are hence more likely to be visited. Proximity can
be used as an augmentation to a conceptual model
designed to be used within a physical space.
Any exhibit is best described by the information it
contains. Visitors with a specific task in mind when
entering an exhibition already have a pre-initialised
conceptual model, relating to a theme. The visitors
seek out content related to their conceptual model,
and separate the bulk of the collection content from
the information they require. The representation of
the content within each exhibit as a vocabulary of
terms allows us to find similarity between exhibits.
The data available at the time of this testing does not
make the distinction between user types, and so only
broad coverage methods result in a improvements.
With the introduction of user types to the data sup-
plied by the museum, specific predictive methods
can be applied to each individual user. This addi-
tional information can be significantly beneficial as
the specialisation of predictive types to visitors is
expected to produce much more accurate predictions
and recommendations. Currently the only method
available to discern the user type is to analyse the
length of time the visitor spends at each each exhibit.
This data is yet to be adapted and annotated from the
raw data supplied by the Melbourne Museum.
7 Conclusion
The above methods are intended to represent base-
line components of possible conceptual models that
represent how a visitor is able to selectively assess
the dynamic context of museum visits. The model
that a visitor generates for themselves is unique, and
is difficult to represent in terms of physical attributes
of exhibits.
55
Being able to predict future actions of a user
within a given environment allows a recommender
system to influence a user?s choices. Key to the pre-
diction of future actions, is the idea that a user has
a conceptual model of how they see content within
the environment in relation to a task. With respect to
a museum environment, the majority of users have
no preconceived conceptual model upon entering an
exhibition and must build one as they explore the
environment. Users with a preconceived task will
more often than not stick to exhibits surrounding
a particular theme. Use of a language-based con-
ceptual model based on the information contained
within an exhibit can be combined with conceptual
models based on geospatial attributes of the exhibit
to create a representation of how a user will react
to an exhibit. The use of heterogeneous information
contained within the exhibit space is only relevant
when the visitor has an information-centric task in
mind.
7.1 Future Work
The methods dealing with a language-based concep-
tual model given here are very basic, and the overall
accuracy and precision of the recommender system
components require improvement. Additional anno-
tation of the paths of visitors to the museum will
enable proper evaluation of conceptual information
based predictive methods. On-site testing of predic-
tive methods at the Melbourne Museum is the ulti-
mate goal of this project, and testing the effects of
visitor feedback on recommendations will also be
analysed. In order to gain more insight into vis-
itor behaviour, the current small-scale set of visi-
tors needs to be expanded to include multiple visitor
types, as well as tasks.
Acknowledgments
This research was supported by Australian Research Council
DP grant no. DP0770931. The authors wish to thank the staff
of the Melbourne Museum for their help in this study. Special
thanks goes to Carolyn Meehan and Alexa Reynolds for their
gathering of data, and helpful suggestions throughout this study.
Thanks also goes to Ingrid Zukerman and Liz Sonenberg for
their input on this research.
References
Chumki Basu, Haym Hirsh, and William Cohen. 1998. Rec-
ommendations as classification: Using social and content-
based information in recommendation. In Proceedings of the
National Conference of Artificial Intelligence, pages 714?
720, Madison, United States.
Giuliano Benelli, Alberto Bianchi, Patrizia Marti, David Sen-
nati, and Elena Not. 1999. HIPS: Hyper-Interaction within
Physical Space. In ICMCS ?99: Proceedings of the IEEE
International Conference on Multimedia Computing and
Systems, volume 2, page 1075. IEEE Computer Society.
Steve Benford, John Bowers, Paul Chandler, Luigina Ciolfi,
Martin Flintham, Mike Fraser, Chris Greenhalgh, Tony Hall,
Sten-Olof Hellstrom, Shahram Izadi, Tom Rodden, Holger
Schnadelbach, and Ian Taylor. 2001. Unearthing virtual
history: using diverse interfaces to reveal hidden worlds. In
Proc Ubicomp, pages 1?6. ACM.
Steven Bird. 2005. NLTK-Lite: Efficient scripting for natural
language processing. In Proceedings of the 4th International
Conference on Natural Language Processing (ICON), pages
11?18, Kanpur, India.
Matthew Chalmers, Kerry Rodden, and Dominique Brodbeck.
1998. The Order of Things: Activity-Centred Information
Access. Computer Networks and ISDN Systems, 30:1?7.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Silvia Filippini. 2003. Personalisation through IT in museums:
Does it really work? Presentation at ICHIM 2003.
Stephen J. Green, Maria Milosavljevic, Robert Dale, and Cecile
Paris. 1999. When virtual documents meet the real world.
In Proc. of WWW8 Workshop: Virtual Documents, Hypertext
Functionality and the Web.
Janet Hitzeman, Chris Mellish, and Jon Oberlander. 1997.
Dynamic generation of museum web pages: The intelli-
gent labelling explorer. Archives and Museum Informatics,
11(2):117?115.
Claudia Leacock, Martin Chodorow, and George A Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In (CoLING)-(ACL), pages 768?774, Montreal,
Canada.
Siddharth Patwardhan and Ted Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In Interna-
tional Joint Conference on Artificial Intelligence, pages 805?
810, Acapulco, Mexico.
John Peponis, Ruth Conroy Dalton, Jean Wineman, and Nick
Dalton. 2004. Measuring the effect of layout on visitors?
spatial behaviors in open plan exhibition settings. Environ-
ment and Planning B: Planning and Design, 31:453?473.
Bhavani Raskutti, Anthony Beitz, and Belinda Ward. 1997. A
feature-based approach to recommending selections based
on past preferences. User Modelling and User Adaption,
7(3):179?218.
Paul Resnick and Hal R Varian. 1997. Recommender systems.
Commun. ACM, 40(3):56?58.
Ingrid Zukerman and David W Albrecht. 2001. Predictive
statistical models for user modeling. User Modeling and
User-Adapted Interaction, 11(1?2):5?18.
56
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 152?159,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Corpus and the Lexicon: Standardising Deep Lexical Acquisition
Evaluation
Yi Zhang? and Timothy Baldwin? and Valia Kordoni?
? Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
{yzhang,kordoni}@coli.uni-sb.de
tim@csse.unimelb.edu.au
Abstract
This paper is concerned with the standard-
isation of evaluation metrics for lexical ac-
quisition over precision grammars, which
are attuned to actual parser performance.
Specifically, we investigate the impact that
lexicons at varying levels of lexical item
precision and recall have on the perfor-
mance of pre-existing broad-coverage pre-
cision grammars in parsing, i.e., on their
coverage and accuracy. The grammars used
for the experiments reported here are the
LinGO English Resource Grammar (ERG;
Flickinger (2000)) and JACY (Siegel and
Bender, 2002), precision grammars of En-
glish and Japanese, respectively. Our re-
sults show convincingly that traditional F-
score-based evaluation of lexical acquisition
does not correlate with actual parsing per-
formance. What we argue for, therefore, is a
recall-heavy interpretation of F-score in de-
signing and optimising automated lexical ac-
quisition algorithms.
1 Introduction
Deep processing is the process of applying rich lin-
guistic resources within NLP tasks, to arrive at a
detailed (=deep) syntactic and semantic analysis of
the data. It is conventionally driven by deep gram-
mars, which encode linguistically-motivated predic-
tions of language behaviour, are usually capable of
both parsing and generation, and generate a high-
level semantic abstraction of the input data. While
enjoying a resurgence of interest due to advances
in parsing algorithms and stochastic parse prun-
ing/ranking, deep grammars remain an underutilised
resource predominantly because of their lack of cov-
erage/robustness in parsing tasks. As noted in previ-
ous work (Baldwin et al, 2004), a significant cause
of diminished coverage is the lack of lexical cover-
age.
Various attempts have been made to ameliorate
the deficiencies of hand-crafted lexicons. More
recently, there has been an explosion of interest
in deep lexical acquisition (DLA; (Baldwin, 2005;
Zhang and Kordoni, 2006; van de Cruys, 2006))
for broad-coverage deep grammars, either by ex-
ploiting the linguistic information encoded in the
grammar itself (in vivo), or by using secondary lan-
guage resources (in vitro). Such approaches provide
(semi-)automatic ways of extending the lexicon with
minimal (or no) human interference.
One stumbling block in DLA research has been
the lack of standardisation in evaluation, with
commonly-used evaluation metrics including:
? Type precision: the proportion of correctly hy-
pothesised lexical entries
? Type recall: the proportion of gold-standard
lexical entries that are correctly hypothesised
? Type F-measure: the harmonic mean of the
type precision and type recall
? Token Accuracy: the accuracy of the lexical en-
tries evaluated against their token occurrences
in gold-standard corpus data
It is often the case that the different measures lead
to significantly different assessments of the quality
of DLA, even for a given DLA approach. Addi-
tionally, it is far from clear how the numbers gen-
erated by these evaluation metrics correlate with ac-
tual parsing performance when the output of a given
DLA method is used. This makes standardised com-
parison among the various different approaches to
DLA very difficult, if not impossible. It is far from
clear which evaluation metrics are more indicative of
the true ?goodness? of the lexicon. The aim of this
research, therefore, is to analyse how the different
evaluation metrics correlate with actual parsing per-
formance using a given lexicon, and to work towards
152
a standardised evaluation framework for future DLA
research to ground itself in.
In this paper, we explore the utility of different
evaluation metrics at predicting parse performance
through a series of experiments over two broad cov-
erage grammars: the English Resource Grammar
(ERG; Flickinger (2000)) and JACY (Siegel and
Bender, 2002). We simulate the results of DLA
by generating lexicons at different levels of preci-
sion and recall, and test the impact of such lexicons
on grammar coverage and accuracy related to gold-
standard treebank data. The final outcome of this
analysis is a proposed evaluation framework for fu-
ture DLA research.
The remainder of the paper is organised as fol-
lows: Section 2 reviews previous work on DLA for
the robust parsing task; Section 3 describes the ex-
perimental setup; Section 4 presents the experiment
results; Section 5 analyses the experiment results;
Section 6 concludes the paper.
2 Lexical Acquisition in Deep Parsing
Hand-crafted large-scale grammars are error-prone.
An error can be roughly classified as undergenerat-
ing (if it prevents a grammatical sentence from be-
ing generated/parsed) or overgenerating (if it allows
an ungrammatical sentence to be generated/parsed).
Hence, errors in deep grammar lexicons can be clas-
sified into two categories: i) a lexical entry is miss-
ing for a specific lexeme; and ii) an erroneous lexical
entry enters the lexicon. The former error type will
cause the grammar to fail to parse/generate certain
sentences (i.e. undergenerate), leading to a loss in
coverage. The latter error type will allow the gram-
mar to parse/generate inappropriate sentences (i.e.
overgenerate), potentially leading to a loss in ac-
curacy. In the first instance, we will be unable to
parse sentences involving a given lexical item if it is
missing from our lexicon, i.e. coverage will be af-
fected assuming the lexical item of interest occurs
in a given corpus. In the second instance, the im-
pact is indeterminate, as certain lexical items may
violate constraints in the grammar and never be li-
cenced, whereas others may be licenced more lib-
erally, generating competing (incorrect) parses for a
given input and reducing parse accuracy. It is these
two competing concerns that we seek to quantify in
this research.
Traditionally, errors in the grammar are detected
manually by the grammar developers. This is usu-
ally done by running the grammar over a carefully
designed test suite and inspecting the outputs. This
procedure becomes less reliable as the grammar gets
larger. Also we can never expect to attain complete
lexical coverage, due to language evolution and the
effects of domain/genre. A static, manually com-
piled lexicon, therefore, becomes inevitably insuffi-
cient when faced with open domain text.
In recent years, some approaches have been de-
veloped to (semi-)automatically detect and/or repair
the lexical errors in linguistic grammars. Such ap-
proaches can be broadly categorised as either sym-
bolic or statistical.
Erbach (1990), Barg and Walther (1998) and
Fouvry (2003) followed a unification-based sym-
bolic approach to unknown word processing for
constraint-based grammars. The basic idea is to
use underspecified lexical entries, namely entries
with fewer constraints, to parse whole sentences,
and generate the ?real? lexical entries afterwards by
collecting information from the full parses. How-
ever, lexical entries generated in this way may be ei-
ther too general or too specific. Underspecified lex-
ical entries with fewer constraints allow more gram-
mar rules to be applied while parsing, and fully-
underspecified lexical entries are computationally
intractable. The whole procedure gets even more
complicated when two unknown words occur next
to each other, potentially allowing almost any con-
stituent to be constructed. The evaluation of these
proposals has tended to be small-scale and some-
what brittle. No concrete results have been pre-
sented relating to the improvement in grammar per-
formance, either for parsing or for generation.
Baldwin (2005) took a statistical approach to au-
tomated lexical acquisition for deep grammars. Fo-
cused on generalising the method of deriving DLA
models on various secondary language resources,
Baldwin used a large set of binary classifiers to pre-
dict whether a given unknown word is of a particular
lexical type. This data-driven approach is grammar
independent and can be scaled up for large gram-
mars. Evaluation was via type precision, type recall,
type F-measure and token accuracy, resulting in dif-
ferent interpretations of the data depending on the
evaluation metric used.
Zhang and Kordoni (2006) tackled the robustness
problem of deep processing from two aspects. They
employed error mining techniques in order to semi-
automatically detect errors in deep grammars. They
then proposed a maximum entropy model based lex-
153
ical type predictor, to generate new lexical entries
on the fly. Evaluation focused on the accuracy of
the lexical type predictor over unknown words, not
the overall goodness of the resulting lexicon. Simi-
larly to Baldwin (2005), the methods are applicable
to other constraint-based lexicalist grammars, but no
direct measurement of the impact on grammar per-
formance was attempted.
van de Cruys (2006) took a similar approach over
the Dutch Alpino grammar (cf. Bouma et al (2001)).
Specifically, he proposed a method for lexical ac-
quisition as an extension to automatic parser error
detection, based on large amounts of raw text (cf.
van Noord (2004)). The method was evaluated us-
ing type precision, type recall and type F-measure.
Once again, however, these numbers fail to give us
any insight into the impact of lexical acquisition on
parser performance.
Ideally, we hope the result of DLA to be both ac-
curate and complete. However, in reality, there will
always be a trade-off between coverage and parser
accuracy. Exactly how these two concerns should be
balanced up depends largely on what task the gram-
mar is applied to (i.e. parsing or generation). In this
paper, we focus exclusively on the parsing task.1
3 Experimental Setup
In this research, we wish to evaluate the impact
of different lexicons on grammar performance. By
grammar performance, we principally mean cov-
erage and accuracy. However, it should be noted
that the efficiency of the grammar?e.g. the aver-
age number of edges in the parse chart, the average
time to parse a sentence and/or the average number
of analyses per sentence?is also an important per-
formance measurement which we expect the quality
of the lexicon to impinge on. Here, however, we
expect to be able to call on external processing opti-
misations2 to dampen any loss in efficiency, in a way
which we cannot with coverage and accuracy.
3.1 Resources
In order to get as representative a set of results as
possible, we choose to run the experiment over two
1In generation, we tend to have a semantic representation
as input, which is linked to pre-existing lexical entries. Hence,
lexical acquisition has no direct impact on generation.
2For example, (van Noord, 2006) shows that a HMM POS
tagger trained on the parser outputs can greatly reduce the lexi-
cal ambiguity and enhance the parser efficiency, without an ob-
servable decrease in parsing accuracy.
large-scale HPSGs (Pollard and Sag, 1994), based
on two distinct languages.
The LinGO English Resource Grammar (ERG;
Flickinger (2000)) is a broad-coverage, linguis-
tically precise HPSG-based grammar of English,
which represents the culmination of more than 10
person years of (largely) manual effort. We use the
jan-06 version of the grammar, which contains about
23K lexical entries and more than 800 leaf lexical
types.
JACY (Siegel and Bender, 2002) is a broad-
coverage linguistically precise HPSG-based gram-
mar of Japanese. In our experiment, we use the
November 2005 version of the grammar, which con-
tains about 48K lexical entries and more than 300
leaf lexical types.
It should be noted in HPSGs, the grammar is
made up of two basic components: the grammar
rules/type hierarchy, and the lexicon (which inter-
faces with the type hierarchy via leaf lexical types).
This is different to strictly lexicalised formalisms
like LTAG and CCG, where essentially all linguistic
description resides in individual lexical entries in the
lexicon. The manually compiled grammars in our
experiment are also intrinsically different to gram-
mars automatically induced from treebanks (e.g. that
used in the Charniak parser (Charniak, 2000) or the
various CCG parsers (Hockenmaier, 2006)). These
differences sharply differentiate our work from pre-
vious research on the interaction between lexical ac-
quisition and parse performance.
Furthermore, to test the grammar precision and
accuracy, we use two treebanks: Redwoods (Oepen
et al, 2002) for English and Hinoki (Bond et al,
2004) for Japanese. These treebanks are so-called
dynamic treebanks, meaning that they can be (semi-
)automatically updated when the grammar is up-
dated. This feature is especially useful when we
want to evaluate the grammar performance with dif-
ferent lexicon configurations. With conventional
treebanks, our experiment is difficult (if not impos-
sible) to perform as the static trees in the treebank
cannot be easily synchronised to the evolution of the
grammar, meaning that we cannot regenerate gold-
standard parse trees relative to a given lexicon (es-
pecially when for reduced recall where there is no
guarantee we will be able to produce all of the parses
in the 100% recall gold-standard). As a result, it is
extremely difficult to faithfully update the statistical
models.
The Redwoods treebank we use is the 6th growth,
154
which is synchronised with the jan-06 version of the
ERG. It contains about 41K test items in total.
The Hinoki treebank we use is updated for the
November 2005 version of the JACY grammar. The
?Rei? sections we use in our experiment contains
45K test items in total.
3.2 Lexicon Generation
To simulate the DLA results at various levels of pre-
cision and recall, a random lexicon generator is used.
In order to generate a new lexicon with specific pre-
cision and recall, the generator randomly retains a
portion of the gold-standard lexicon, and generates a
pre-determined number of erroneous lexical entries.
More specifically, for each grammar we first ex-
tract a subset of the lexical entries from the lexicon,
each of which has at least one occurrence in the tree-
bank. This subset of lexical entries is considered to
be the gold-standard lexicon (7,156 entries for the
ERG, 27,308 entries for JACY).
Given the gold-standard lexicon L, the target pre-
cision P and recall R, a new lexicon L? is created,
which is composed of two disjoint subsets: the re-
tained part of the gold-standard lexicon G, and the
erroneous entries E. According to the definitions of
precision and recall:
P = |G||L?| (1) R =
|G|
|L| (2)
and the fact that:
|L?| = |G| + |E| (3)
we get:
|G| = |L| ? R (4)
|E| = |L| ? R ? ( 1P ? 1) (5)
To retain a specific number of entries from the
gold-standard lexicon, we randomly select |G| en-
tries based on the combined probabilistic distribu-
tion of the corresponding lexeme and lexical types.3
We obtain the probabilistic distribution of lexemes
from large corpora (BNC for English and Mainichi
Shimbun [1991-2000] for Japanese), and the distri-
bution of lexical types from the corresponding tree-
banks. For each lexical entry e(l, t) in the gold-
standard lexicon with lexeme l and lexical type t,
3For simplicity, we assume mutual independence of the lex-
emes and lexical types.
the combined probability is:
p(e(l, t)) = CL(l) ? CT (t)?
e?(l?,t?)?L CL(l?) ? CT (t?)
(6)
The erroneous entries are generated in the same
way among all possible combinations of lexemes
and lexical types. The difference is that only open
category types and less frequent lexemes are used
for generating new entries (e.g. we wouldn?t expect
to learn a new lexical item for the lexeme the or the
lexical type d - the le in English). In our ex-
periment, we consider lexical types with more than
a predefined number of lexical entries (20 for the
ERG, 50 for JACY) in the gold-standard lexicon to
be open-class lexical types; the upper-bound thresh-
old on token frequency is set to 1000 for English and
537 for Japanese, i.e. lexemes which occur more fre-
quently than this are excluded from lexical acquisi-
tion under the assumption that the grammar develop-
ers will have attained full coverage of lexical items
for them.
For each grammar, we then generate 9 differ-
ent lexicons at varying precision and recall levels,
namely 60%, 80%, and 100%.
3.3 Parser Coverage
Coverage is an important grammar performance
measurement, and indicates the proportion of inputs
for which a correct parse was obtained (adjudged
relative to the gold-standard parse data in the tree-
banks). In our experiment, we adopt a weak defini-
tion of coverage as ?obtaining at least one spanning
tree?. The reason for this is that we want to obtain
an estimate for novel data (for which we do not have
gold-standard parse data) of the relative number of
strings for which we can expect to be able to produce
at least one spanning parse. This weak definition of
coverage actually provides an upper bound estimate
of coverage in the strict sense, and saves the effort to
manually evaluate the correctness of the parses. Past
evaluations (e.g. Baldwin et al (2004)) have shown
that the grammars we are dealing with are relatively
precise. Based on this, we claim that our results for
parse coverage provide a reasonable estimate indica-
tion of parse coverage in the strict sense of the word.
In principle, coverage will only decrease when
the lexicon recall goes down, as adding erroneous
entries should not invalidate the existing analy-
ses. However, in practice, the introduction of er-
roneous entries increases lexical ambiguity dramati-
155
0.6 0.8 1.0P \ R C E A C E A C E A
0.6 4294 2862 7156 5725 3817 9542 7156 4771 11927
0.8 4294 1073 5367 5725 1431 7156 7156 1789 8945
1.0 4294 0 4294 5725 0 5725 7156 0 7156
Table 1: Different lexicon configurations for the ERG with the number of correct (C), erroneous (E) and
combined (A) entries at each level of precision (P) and recall (R)
0.6 0.8 1.0P \ R C E A C E A C E A
0.6 16385 10923 27308 21846 14564 36410 27308 18205 45513
0.8 16385 4096 20481 21846 5462 27308 27308 6827 34135
1.0 16385 0 16385 21846 0 21846 27308 0 27308
Table 2: Different lexicon configurations for JACY with the number of correct (C), erroneous (E) and
combined (A) entries at each level of precision (P) and recall (R)
cally, readily causing the parser to run out of mem-
ory. Moreover, some grammars use recursive unary
rules which are triggered by specific lexical types.
Here again, erroneous lexical entries can lead to ?fail
to parse? errors.
Given this, we run the coverage tests for the two
grammars over the corresponding treebanks: Red-
woods and Hinoki. The maximum number of pas-
sive edges is set to 10K for the parser. We used
[incr tsdb()] (Oepen, 2001) to handle the dif-
ferent lexicon configurations and data sets, and PET
(Callmeier, 2000) for parsing.
3.4 Parser Accuracy
Another important measurement of grammar perfor-
mance is accuracy. Deep grammars often generate
hundreds of analyses for an input, suggesting the
need for some means of selecting the most probable
analysis from among them. This is done with the
parse disambiguation model proposed in Toutanova
et al (2002), with accuracy indicating the proportion
of inputs for which we are able to accurately select
the correct parse.
The disambiguation model is essentially a maxi-
mum entropy (ME) based ranking model. Given an
input sentence s with possible analyses t1 . . . tk, the
conditional probability for analysis ti is given by:
P (ti|s) =
exp ?mj=1 fj(ti)?j
?k
i?=1 exp
?m
j=1 fj(ti?)?j
(7)
where f1 . . . fm are the features and ?1 . . . ?m
are the corresponding parameters. When ranking
parses,
?m
j=1 fj(ti)?j is the indicator of ?good-
ness?. Drawing on the discriminative nature of the
ME models, various feature types can be incor-
porated into the model. In combination with the
dynamic treebanks where the analyses are (semi-
)automatically disambiguated, the models can be
easily re-trained when the grammar is modified.
For each lexicon configuration, after the cover-
age test, we do an automatic treebank update. Dur-
ing the automatic treebank update, only those new
parse trees which are comparable to the active trees
in the gold-standard treebank are marked as cor-
rect readings. All other trees are marked as in-
active and deemed as overgeneration of the gram-
mar. The ME-based parse disambiguation models
are trained/evaluated using these updated treebanks
with 5-fold cross validation. Since we are only in-
terested in the difference between different lexicon
configurations, we use the simple PCFG-S model
from (Toutanova et al, 2002), which incorporates
PCFG-style features from the derivation tree of the
parse. The accuracy of the disambiguation model
is calculated by top analysis exact matching (i.e. a
ranking is only considered correct if the top ranked
analysis matches the gold standard prefered reading
in the treebank).
All the Hinoki Rei noun sections (about 25K
items) were used in the accuracy evaluation for
JACY. However, due to technical limitations, only
the jh sections (about 6K items) of the Redwoods
Treebank were used for training/testing the disam-
biguation models for the ERG.
4 Experiment Results
The experiment consumes a considerable amount of
computational resources. For each lexicon config-
156
P \ R 0.6 0.8 1.0
0.6 44.56% 66.88% 75.51%
0.8 42.18% 65.82% 75.86%
1.0 40.45% 66.19% 76.15%
Table 3: Parser coverage of JACY with different lex-
icons
P \ R 0.6 0.8 1.0
0.6 27.86% 39.17% 79.66%
0.8 27.06% 37.42% 79.57%
1.0 26.34% 37.18% 79.33%
Table 4: Parser coverage of the ERG with different
lexicons
uration of a given grammar, we need to i) process
(parse) all the items in the treebank, ii) compare the
resulting trees with the gold-standard trees and up-
date the treebank, and iii) retrain the disambiguation
models over 5 folds of cross validation. Given the
two grammars with 9 configurations each, the en-
tire experiment takes over 1 CPU month and about
120GB of disk space.
The coverage results are shown in Table 3 and
Table 4 for JACY and the ERG, respectively.4 As
expected, we see a significant increase in grammar
coverage when the lexicon recall goes up. This in-
crease is more significant for the ERG than JACY,
mainly because the JACY lexicon is about twice as
large as the ERG lexicon; thus, the most frequent
entries are still in the lexicons even with low recall.
When the lexicon recall is fixed, the grammar cov-
erage does not change significantly at different lev-
els of lexicon precision. Recall that we are not eval-
uating the correctness of such parses at this stage.
It is clear that the increase in lexicon recall boosts
the grammar coverage, as we would expect. The
precision of the lexicon does not have a large in-
fluence on coverage. This result confirms that with
DLA (where we hope to enhance lexical coverage
relative to a given corpus/domain), the coverage of
the grammar can be enhanced significantly.
The accuracy results are obtained with 5-fold
cross validation, as shown in Table 5 and Table 6
4Note that even with the lexicons at 100% precision and re-
call level, there is no guarantee of 100% coverage. As the con-
tents of the Redwoods and Hinoki treebanks were determined
independently of the respective grammars, rather than the gram-
mars being induced from the treebanks e.g., they both still con-
tain significant numbers of strings for which the grammar can-
not produce a correct analysis.
P-R #ptree Avg. ?
060-060 13269 62.65% 0.89%
060-080 19800 60.57% 0.83%
060-100 22361 59.61% 0.63%
080-060 14701 63.27% 0.62%
080-080 23184 60.97% 0.48%
080-100 27111 60.04% 0.56%
100-060 15696 63.91% 0.64%
100-080 26859 61.47% 0.68%
100-100 31870 60.48% 0.71%
Table 5: Accuracy of disambiguation models for
JACY with different lexicons
P-R #ptree Avg. ?
060-060 737 71.11% 3.55%
060-080 1093 63.94% 2.75%
060-100 3416 60.92% 1.23%
080-060 742 70.07% 1.50%
080-080 1282 61.81% 3.60%
080-100 3842 59.05% 1.30%
100-060 778 69.76% 4.62%
100-080 1440 60.59% 2.64%
100-100 4689 57.03% 1.36%
Table 6: Accuracy of disambiguation models for the
ERG with different lexicons
for JACY and the ERG, respectively. When the lex-
icon recall goes up, we observe a small but steady
decrease in the accuracy of the disambiguation mod-
els, for both JACY and ERG. This is generally a side
effect of change in coverage: as the grammar cover-
age goes up, the parse trees become more diverse,
and are hence harder to discriminate.
When the recall is fixed and the precision of the
lexicon goes up, we observe a very small accuracy
gain for JACY (around 0.5% for each 20% increase
in precision). This shows that the grammar accu-
racy gain is limited as the precision of the lexicon
increases, i.e. that the disambiguation model is re-
markably robust to the effects of noise.
It should be noted that for the ERG we failed to
observe any accuracy gain at all with a more pre-
cise lexicon. This is partly due to the limited size
of the updated treebanks. For the lexicon config-
uration 060 ? 060, we obtained only 737 preferred
readings/trees to train/test the disambiguation model
over. The 5-fold cross validation results vary within
a margin of 10%, which means that the models are
still not converging. However, the result does con-
firm that there is no significant gain in grammar ac-
curacy with a higher precision lexicon.
Finally, we combine the coverage and accuracy
scores into a single F-measure (? = 1) value. The
results are shown in Figure 1. Again we see that
157
the difference in lexicon recall has a more signif-
icant impact on the overall grammar performance
than precision.
0.4
0.5
0.6
0.7
F-
sc
or
e 
(Ja
CY
)
R=0.6
R=0.8
R=1.0
P=0.6
P=0.8
P=1.0
0.4
0.5
0.6
0.7
0.6 0.8 1.0
F-
sc
or
e 
(E
RG
)
Lex. Precision
R=0.6
R=0.8
R=1.0
0.6 0.8 1.0
Lex. Recall
P=0.6
P=0.8
P=1.0
Figure 1: Grammar performance (F-score) with dif-
ferent lexicons
5 Discussion
5.1 Is F-measure a good metric for DLA
evaluation?
As mentioned in Section 2, a number of relevant ear-
lier works have evaluated DLA results via the un-
weighted F-score (relative to type precision and re-
call). This implicitly assumes that the precision and
recall of the lexicon are equally important. How-
ever, this is clearly not the case as we can see in the
results of the grammar performance. For example,
the lexicon configurations 060 ? 100 and 100 ? 060
of JACY (i.e. 60% precision, 100% recall vs. 100%
precision, 60% recall, respectively) have the same
unweighted F-scores, but their corresponding over-
all grammar performance (parser F-score) differs by
up to 17%.
5.2 Does precision matter?
The most interesting finding in our experiment is
that the precision of the deep lexicon does not ap-
pear to have a significant impact on grammar accu-
racy. This is contrary to the earlier predominant be-
lief that deep lexicons should be as accurate as pos-
sible. This belief is derived mainly from observa-
tion of grammars with relatively small lexicons. In
such small lexicons, the closed-class lexical entries
and frequent entries (which comprise the ?core? of
the lexicon) make up a large proportion of lexical
entries. Hence, any loss in precision means a signif-
icant degradation of the ?core? lexicon, which leads
to performance loss of the grammar. For example,
we find that the inclusion of one or two erroneous
entries for frequent closed-class lexical type words
(such as the, or of in English, for instance) may eas-
ily ?break? the parser.
However, in state-of-the-art broad-coverage deep
grammars such as JACY and ERG, the lexicons are
much larger. They usually have more or less similar
?cores? to the smaller lexicons, but with many more
open-class lexical entries and less frequent entries,
which compose the ?peripheral? parts of the lexi-
cons. In our experiment, we found that more than
95% of the lexical entries belong to the top 5% of
the open-class lexical types. The bigger the lexicon
is, the larger the proportion of lexical entries that be-
long to the ?peripheral? lexicon.
In our experiment, we only change the ?periph-
eral? lexicon by creating/removing lexical entries
for less frequent lexemes and open-class lexical
types, leaving the ?core? lexicon intact. Therefore, a
more accurate interpretation of the experimental re-
sults is that the precision of the open type and less
frequent lexical entries does not have a large impact
on the grammar performance, but their recall has a
crucial effect on grammar coverage.
The consequence of this finding is that the bal-
ance between precision and recall in the deep lexi-
con should be decided by their impact on the task to
which the grammar is applied. In research on auto-
mated DLA, the motivation is to enhance the robust-
ness/coverage of the grammars. This work shows
that grammar performance is very robust over the
inevitable errors introduced by the DLA, and that
more emphasis should be placed on recall.
Again, caution should be exercised here. We
do not mean that by blindly adding lexical entries
without worrying about their correctness, the per-
formance of the grammar will be monotonically en-
hanced ? there will almost certainly be a point at
which noise in the lexicon swamps the parse chart
and/or leads to unacceptable levels of spurious am-
biguity. Also, the balance between precision and re-
call of the lexicon will depend on various expecta-
tions of the grammarians/lexicographers, i.e. the lin-
guistic precision and generality, which is beyond the
scope of this paper.
As a final word of warning, the absolute gram-
mar performance change that a given level of lexi-
158
con type precision and recall brings about will obvi-
ously depend on the grammar. In looking across two
grammars from two very different languages, we are
confident of the robustness of our results (at least for
grammars of the same ilk) and the conclusions that
we have drawn from them. For any novel grammar
and/or formalism, however, the performance change
should ideally be quantified through a set of exper-
iments with different lexicon configurations, based
on the procedure outlined here. Based on this, it
should be possible to find the optimal balance be-
tween the different lexicon evaluation metrics.
6 Conclusion
In this paper, we have investigated the relationship
between evaluation metrics for deep lexical acquisi-
tion and grammar performance in parsing tasks. The
results show that traditional DLA evaluation based
on F-measure is not reflective of grammar perfor-
mance. The precision of the lexicon appears to have
minimal impact on grammar accuracy, and therefore
recall should be emphasised more greatly in the de-
sign of deep lexical acquisition techniques.
References
Timothy Baldwin, Emily Bender, Dan Flickinger, Ara Kim, and
Stephan Oepen. 2004. Road-testing the English Resource
Grammar over the British National Corpus. In Proc. of the
fourth international conference on language resources and
evaluation (LREC 2004), pages 2047?2050, Lisbon, Portu-
gal.
Timothy Baldwin. 2005. Bootstrapping deep lexical resources:
Resources for courses. In Proc. of the ACL-SIGLEX 2005
workshop on deep lexical acquisition, pages 67?76, Ann Ar-
bor, USA.
Petra Barg and Markus Walther. 1998. Processing unknown
words in HPSG. In Proc. of the 36th Conference of the
ACL and the 17th International Conference on Computa-
tional Linguistics, pages 91?95, Montreal, Canada.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: a treebank for text understanding. In Proc. of the
first international joint conference on natural language pro-
cessing (IJCNLP04), pages 554?562, Hainan, China.
Gosse Bouma, Gertjan van Noord, and Robert Malouf. 2001.
Alpino: wide-coverage computational analysis of Dutch. In
Computational linguistics in the Netherlands 2000, pages
45?59, Tilburg, the Netherlands.
Ulrich Callmeier. 2000. PET ? a platform for experimentation
with efficient HPSG processing techniques. Natural Lan-
guage Engineering, 6(1):99?107.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proc. of the 1st Annual Meeting of the North Ameri-
can Chapter of Association for Computational Linguistics
(NAACL2000), Seattle, USA.
Gregor Erbach. 1990. Syntactic processing of unknown words.
IWBS Report 131, IBM, Stuttgart, Germany.
Dan Flickinger. 2000. On building a more efficient grammar by
exploiting types. Natural Language Engineering, 6(1):15?
28.
Frederik Fouvry. 2003. Lexicon acquisition with a large-
coverage unification-based grammar. In Proc. of the 10th
Conference of the European Chapter of the Association for
Computational Linguistics (EACL 2003), pages 87?90, Bu-
dapest, Hungary.
Julia Hockenmaier. 2006. Creating a CCGbank and a wide-
coverage CCG lexicon for German. In Proc. of the 21st
International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational
Linguistics, pages 505?512, Sydney, Australia.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher
Manning, Dan Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebank: Motivation and preliminary ap-
plications. In Proc. of the 17th international conference on
computational linguistics (COLING 2002), Taipei, Taiwan.
Stephan Oepen. 2001. [incr tsdb()] ? competence and perfor-
mance laboratory. User manual. Technical report, Compu-
tational Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago, USA.
Melanie Siegel and Emily Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proc. of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion, Taipei, Taiwan.
Kristina Toutanova, Christoper Manning, Stuart Shieber, Dan
Flickinger, and Stephan Oepen. 2002. Parse ranking for
a rich HPSG grammar. In Proc. of the First Workshop on
Treebanks and Linguistic Theories (TLT2002), pages 253?
263, Sozopol, Bulgaria.
Tim van de Cruys. 2006. Automatically extending the lexicon
for parsing. In Proc. of the eleventh ESSLLI student session,
pages 180?191, Malaga, Spain.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In Proc. of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04), Main
Volume, pages 446?453, Barcelona, Spain.
Gertjan van Noord. 2006. At Last Parsing Is Now Operational.
In Actes de la 13e conference sur le traitement automatique
des langues naturelles (TALN06), pages 20?42, Leuven, Bel-
gium.
Fei Xia, Chung-Hye Han, Martha Palmer, and Aravind Joshi.
2001. Automatically extracting and comparing lexicalized
grammars for different languages. In Proc. of the 17th Inter-
national Joint Conference on Artificial Intelligence (IJCAI-
2001), pages 1321?1330, Seattle, USA.
Yi Zhang and Valia Kordoni. 2006. Automated deep lexical
acquisition for robust open texts processing. In Proc. of
the fifth international conference on language resources and
evaluation (LREC 2006), pages 275?280, Genoa, Italy.
159
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 9?16,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Landmark Classification for Route Directions
Aidan Furlan?, Timothy Baldwin? and Alex Klippel?
? CSSE
University of Melbourne
VIC 3010, Australia
{afurlan,tim}@csse.unimelb.edu.au
? Department of Geography
Penn State University
University Park, PA 16802, USA
klippel@psu.edu
Abstract
In order for automated navigation systems
to operate effectively, the route instructions
they produce must be clear, concise and eas-
ily understood by users. In order to incorpo-
rate a landmark within a coherent sentence,
it is necessary to first understand how that
landmark is conceptualised by travellers ?
whether it is perceived as point-like, line-
like or area-like. This paper investigates
the viability of automatically classifying the
conceptualisation of landmarks relative to a
given city context. We use web data to learn
the default conceptualisation of those land-
marks, crucially analysing preposition and
verb collocations in the classification.
1 Introduction
At present, many navigation systems produce badly-
worded and difficult to follow route instructions,
which do not closely correspond with the way
people give one another directions (Dale et al,
2005). Typically, automated navigation systems
give turning instructions with street names as refer-
ence points, eg turn right at Smith St. By contrast,
human-generated route instructions tend to use land-
marks in preference to street names as navigational
reference points (Michon and Denis, 2001).
According to Allen (1997), landmarks are typi-
cally used in route directions in one of two ways?
as descriptives, providing a static picture of a spa-
tial scene so that the traveller can verify his or her
location along a route, eg the City Library is on
your left, or to specify or clarify a point on a route
at which the traveller must make a choice between
multiple pathways, termed choice points or decision
points. Route instructions which identify decision
points with respect to landmarks have been found to
be significantly easier to follow than standard street-
based or turn-based route instructions (Michon and
Denis, 2001).
This paper goes beyond classical approaches to
landmarks that focus on salient point-like objects.
Instead, we aim to find appropriate ways of classify-
ing landmarks automatically, based on the way those
landmarks are used in spatial sentences on the web:
as point-like, linear-like, and area-like objects that
structure movement pattern in urban spaces. In par-
ticular, we analyse how different prepositions and
verbs with pre-classified semantics co-occur with
mentions of the landmarks. A preposition such as
through can be used with reference to a landmark
we are conceptualising as an area, but not one we are
conceptualising as a point. Landau and Jackendoff
(1993) presented an analysis of the spatial proper-
ties of commonly used English spatial prepositions,
such as at, in and to. This classification used as the
basis of a list of prepositions for the present study,
grouped according to whether the preposition indi-
cates a point-like, line-like or area-like landmark. In
addition, a short list of verbs was compiled based
on the verb classes of Levin (1993) and similarly di-
vided into the three conceptual classes.
Each of the verbs and prepositions was combined
in turn with a list of landmarks in Melbourne, Aus-
tralia, to produce a series of spatial phrases such
as at Flinders St Station. These phrases were then
9
sent to the Google search engine, which determined
the approximate number of documents on the web
containing that exact phrase. The document counts
were then summed over the conceptual categories
the prepositions and verbs appeared in ? point, line
and area. The result of this was a probabilistic cat-
egorisation of each landmark, according to its usage
in spatial contexts on the web.
Evaluation of the baseline was performed based
on annotators? independent judgements of the con-
ceptual class of each of the landmarks, gathered
from a web-based annotation interface. It was found
that the baseline classification agreed with the gold
standard classification 63.8% of the time. A slight
improvement on the baseline was achieved via a su-
pervised neural network classifier, which took the
web counts as inputs. This classifier agreed with
the gold standard 68.5% of the time. As a result
of this analysis, a set of systematically ambiguous
landmarks was identified, with implications for fu-
ture landmark classification models.
In the remainder of this paper, we describe back-
ground research (Section 2) and then outline our re-
search methodology (Section 3). We then present
the results of a series of landmark classification
experiments (Section 4), and finally discuss the
broader implications of the experiments (Section 5).
2 Background
2.1 Spatial Cognition
Route directions should be designed in such a way as
to be quickly and easily comprehended by the trav-
eller (Lovelace et al, 1999). Optimally, route di-
rections should exhibit cognitive adequacy ? char-
acterising an external representation of a route (as
with a map or route directions) in a way supportive
of human spatial cognitive processes and knowledge
representation (Klippel, 2003). For this reason, the
improvement of route directions requires an investi-
gation into human spatial cognition.
Route instructions which reference landmarks are
able to achieve a number of worthwhile goals: they
have the effect of increasing the spatial awareness
of the recipient by informing them about their sur-
roundings; landmark-referencing route instructions
can decrease the cognitive load on the recipient; and
it is more natural-sounding to receive route instruc-
tions in terms of landmarks.
2.2 Landmark Conceptualisation
In order to provide appropriate landmark-referenced
route instructions, it is necessary to understand how
landmarks can be used in spatial sentences to locate
a trajector. On a geometric level, all landmarks can
be considered areas when projected onto a top-down
map. However, on a conceptual level, landmarks can
be used in a point-like, line-like or area-like manner,
depending on their spatial relationship with a route
(Hansen et al, 2006).
One possible approach to determining a land-
mark?s conceptual class is to make use of the land-
mark?s geometric context, including its size relative
to the route and the number of decision points with
which it coincides. However, this approach may
have little ecological validity, as people may not in
fact conceptualise landmarks as point, line or area
based purely on geometry, but also based on prag-
matic considerations. For instance, it may be the
case that people don?t tend to conceptualise Flinders
St Station as an area, even though it satisfies the ge-
ometric criteria.
2.3 Past Research on Landmark Interpretation
The only research we are aware of which has ad-
dressed this same topic of landmark interpretation
is that of Tezuka and Tanaka (2005). In an investi-
gation of the spatial use of landmarks in sentences,
Tezuka and Tanaka (2005) modified existing web
mining methods to include spatial context in order
to obtain landmark information.
It is natural to question the appropriateness of web
data for research purposes, because web data is in-
evitably noisy and search engines themselves can in-
troduce certain idiosyncracies which can distort re-
sults (Kilgarriff and Grefenstette, 2003). However,
the vast amount of data available can nevertheless
give better results than more theoretically motivated
techniques (Lapata and Keller, 2004). And impor-
tantly, the data that can be gleaned from the web
does not mirror the view of a single person or a se-
lect group, but of the entire global community (or at
least the best available representation of it).
10
3 Methodology
The prepositions and verbs which accompany a
landmark in spatial sentences capture that land-
mark?s implicit conceptualisation. We use this im-
plicit conceptualisation, as represented on the web,
to develop two automated classification schemes: a
simple voting classifier and a neural network clas-
sifier. We compile a set of gold standard classifi-
cations in order to evaluate the performance of the
classifiers.
3.1 Landmarks
A list of 58 landmarks was generated for Mel-
bourne, Australia. The landmarks were chosen to be
uniquely identifiable and recognisable by most in-
habitants of Melbourne.
3.2 Gold Standard
We had annotators use a web interface to uniquely
classify each landmark as either point-, line- or area-
like. Each landmark?s gold standard category was
taken to be the category with the greatest number
of annotator votes. Where the annotations were
split equally between classes, the maximal geomet-
ric class was chosen, which is to say, line was cho-
sen in preference to point, and area was chosen in
preference to line. The rationale for this is that, for
example, a point-like representation is always recov-
erable from a landmark nominally classified as an
area, but not the other way around. Hence the classi-
fication which maintains both pieces of information,
that this landmark may be treated as an area or a
point, was assigned preferentially to the alternative,
that this landmark may only be treated as a point.
Since landmark conceptualisations can depend on
the mode of transport involved, annotators were in-
structed to consider themselves a cyclist who never-
theless behaves like a car by always staying on the
street network. The intention was to elicit conceptu-
alisations based on a modality which is intermediate
between a car and a pedestrian. Annotators were
also asked to indicate their confidence in each anno-
tation.
3.3 Web Mining
We identified a set of prepositions and verbs as in-
dicating a point-like, line-like or area-like repre-
sentation. The number of documents on the web
which were found to contain a particular landmark
in point-like, line-like or area-like spatial sentences
provided the raw data for our automated classifi-
cation schemes. The web data thus obtained can
be considered an implicit representation of a gen-
eralised cognitive model of the landmarks.
Prepositions
Landau and Jackendoff (1993) investigated the
use of English spatial prepositions and the require-
ments they place on the geometric properties of ref-
erence objects. This analysis was projected onto the
conceptual classes of point, line and area, to form
a list of conceptually grouped spatial prepositions.
Hence prepositions which require the reference ob-
ject to be (or contain) a bounded enclosure, such
as inside, were classified as denoting an area-like
landmark; prepositions which require the reference
to have an elongated principal axis, such as along,
were classified as denoting a line-like landmark; and
prepositions which place no geometric constraints
on the reference object, such as at, were classified
as denoting a point-like landmark.
The prepositions used were restricted to those
which pertain to a horizontal planar geometry com-
patible with route directions; for example, preposi-
tions which make use of a reference object?s ver-
tical axis such as on top of and under were ig-
nored, as were prepositions denoting contact such
as against. The preposition out was also excluded
from the study as it is typically used in non-spatial
contexts, and in spatial contexts the reference object
is usually covert (eg he took his wallet out) (Tyler
and Evans, 2003). Conversely, out of is frequently
spatial and the reference object is overt, so this com-
pound preposition was retained. The complete list
of prepositions used in the study is given in Table 1.
Verbs
In addition to the list of prepositions, a list of
verbs was created based on the verb classes of Levin
(1993), restricted to verbs of inherently directed mo-
tion which can be used in a phrase immediately pre-
ceding a landmark, such as the verb pass in the
phrase pass the MCG; in other words, the chosen
verbs can be used in a way which parallels the use
of spatial prepositions, as opposed to verbs such as
11
Point-like Line-like Area-like
across from along around
at alongside across
after in
away from inside (of)
before into
behind out of
beside outside (of)
in front of through
near within
next to without
opposite
past
to
to the left of
to the right of
to the side of
toward
Table 1: Prepositions used in this research (based on
Landau and Jackendoff (1993))
Point-like Line-like Area-like
hit follow cross
pass enter
reach leave
Table 2: Verbs used in this research
proceed, which specify a motion but require a prepo-
sition for clarification. This second type of verb is of
no interest to the study as they tell us nothing about
the conceptualisation of landmarks.
As with the prepositions, the verbs were grouped
into the conceptual classes of point, line and area ac-
cording to the requirements they place on reference
objects, including enter for an area-like object, fol-
low for a line-like object and pass for a point-like
object. The complete list of verbs used in the study
is given in Table 2.
Document Counts
Each of the prepositions and verbs was com-
bined with each of the landmarks to create a cross-
product of linguistic chunks, such as at Queen Victo-
ria Market, through Queen Victoria Market, and so
on. Alternative names and common misspellings of
the landmark names were taken into account, such
as Flinders St Station, Flinders Street Station and
Flinder?s Street Station. Additionally, three con-
jugations of each verb were used?present tense
non-3rd person singular (eg reach), present tense
3rd person singular (eg reaches), and past tense (eg
reached).
Each linguistic chunk was sent in turn to the
Google search engine, which determined the ap-
proximate number of documents on the web contain-
ing that exact phrase. The counts were then summed
over the conceptual categories in which each prepo-
sition and verb appeared. The result of this was
a probabilistic categorisation of each landmark as
point, line or area, according to its usage in spatial
sentences on the web.
It is difficult to determine the context of sentences
using a search engine. It is uncertain whether the
documents found by Google use the searched-for
linguistic chunks in a spatial context or in some
other context. For this reason, each preposition and
verb was assigned a weight based on the proportion
of occurrences of that word in the Penn Treebank
(Marcus et al, 1993) which are labelled with a spa-
tial meaning. This weighting should give an approx-
imation to the proportion of spatial usages of that
word on the web.
Automated Classification
As a naive automated classification of the land-
marks, the document counts were used to place each
landmark in one of the three conceptual classes.
Each landmark was placed in the class in which it
was found to appear most frequently, based on the
classes of the prepositions and verbs with which it
appeared on the web. Hence landmarks which ap-
peared more often with a point-like preposition or
verb, such as at or pass, were placed in the point cat-
egory; landmarks which appeared more often with
a line-like preposition or verb, such as follow, were
placed in the line category; and landmarks which ap-
peared more often with an area-like preposition or
verb, such as around, were placed in the area cate-
gory.
As a more sophisticated classification scheme,
we developed a supervised artificial neural network
classifier. The neural network we developed con-
sisted of a three node input layer, a two node hid-
den layer and a two node output layer, with learning
12
taking place via the backpropagation algorithm. For
each landmark, the percentage of web counts in each
of the three conceptual classes was used as the initial
activation value of the three nodes in the input layer.
The activation of the output nodes was rounded to 1
or 0. The output node activations were used to indi-
cate whether a landmark falls into the point, line or
area category ? 01 for point, 10 for line and 11 for
area. An output of 00 was taken to indicate a fail-
ure to classify. The neural network was trained and
tested using fourfold cross-validation, with the gold
standard classification as the desired output in each
case.
4 Results
Five experiments were conducted on the simple
voting classifier and the neural network classifier.
These experiments used increasingly sophisticated
inputs and gold standard measures to try to im-
prove the performance of the classifiers, as measured
against the gold standard. The neural network clas-
sifier outperformed the voting classifier in all exper-
iments but the final one.
Of the 58 Melbourne landmarks, 27 were clas-
sified as points by the majority of annotators, 2 as
lines, and 29 as areas. These majority classifications
were used as the gold standard. For these classifica-
tions, we calculated a kappa statistic of 0.528 (Car-
letta, 1996). This suggests that the annotation classi-
fication task itself was only moderately well-formed,
and that the assumption that multiple annotators will
classify landmarks in a similar manner does not nec-
essarily hold true.
To determine whether the classifiers were per-
forming at an acceptable level, we established a
majority-class baseline: 29 of the 58 landmarks
were areas, and hence the majority class classifier
has an accuracy of 50%.
The maximum meaningful accuracy that can be
achieved by a classifier is limited by the accuracy
of the annotations themselves, creating an upper
bound for classifier performance. The upper bound
was calculated as the mean pairwise inter-annotator
agreement, which was determined to be 74.4%.
Accuracy (%) E.R.R. (%)
Baseline 50.0
Voting Classifier 63.8 56.6
Neural Net Classifier 70.0 82.0
Agreement 74.4
Table 3: Results with simple web counts (Experi-
ment 1)
4.1 Experiment 1
Experiment 1 involved using only the raw web count
data as input into the classifiers. The accuracy and
error rate reduction (E.R.R.) of the classifiers are
given in Table 3.
The neural network classifier produced results
slightly better than the simple voting classifier, but
with 18 landmarks incorrectly classified by the neu-
ral network, there is still plently of room for im-
provement. The raw web count data used in this ex-
periment was likely to be biased in favour of certain
prepositions and verbs, because some of these words
(such as at and in, which each occur in over 7 bil-
lion documents) are much more common than others
(such as beside, which occurs in just over 50 million
documents). This may result in the web counts be-
ing unfairly weighted towards one class or another,
creating classifier bias.
The simple voting classifier showed a tendency
towards point classifications over line or area classi-
fications. The neural network classifier reversed the
bias shown by the simple voting classifier, with the
area class showing high recall but low precision, re-
sulting in a low recall for the point class. Neither of
the two line landmarks were classified correctly; in
fact, none of the landmarks were classified as lines.
4.2 Experiment 2
To adjust for the potential bias in preposition and
verb use, the web counts were normalised against
the prior probabilities of the relevant preposition or
verb, by calculating the ratio of the count of each lin-
guistic chunk to the count of its preposition or verb
in isolation. The accuracy and error rate reduction
of the classifiers are given in Table 4.
Normalising the web counts by the prior probabil-
ities of the prepositions and verbs did not improve
the accuracy of the classifiers as expected. The sim-
13
Accuracy (%) E.R.R. (%)
Baseline 50.0
Voting Classifier 55.2 21.3
Neural Net Classifier 70.0 82.0
Upper 74.4
Table 4: Results with normalised web counts (Ex-
periment 2)
ple voting classifier reduced in accuracy, while the
accuracy of the neural net classifier remained un-
changed.
4.3 Experiment 3
As explained in Section 3.2, the annotators who gen-
erated the gold standard were required to choose one
of point, line or area for each landmark, even if they
were unfamiliar with the landmark. Some of these
annotators may have been forced to guess the ap-
propriate class. As a result, these annotations may
cause the gold standard to lack validity, which could
be one of the barriers to classifier improvement.
In this experiment, a more sound gold standard
was generated by weighting annotators? classifica-
tions by their familiarity with the landmark. The
effect of this is that the judgement of an annota-
tor who is very familiar with a landmark outweighs
the judgement of an annotator who is less familiar.
Experiments 1 and 2 were conducted again based
on this new gold standard. These repeated exper-
iments are dubbed Experiments 1? and 2? respec-
tively. The results of each of the repeated experi-
ments are shown in Table 5.
The simple voting classifier showed improvement
using the weighted gold standard, with the accura-
cies under Experiments 1? and 2? each exceeding
the accuracy of the equivalent experiment using the
original gold standard. Experiment 1? showed the
most improvement for the simple voting classifier,
giving an accuracy of 67.2% (only one landmark shy
of the 70% accuracy achieved by the neural network
classifier in experiment 1).
While landmarks well-known to all are likely
to produce consistently valid classifications, and
landmarks poorly known to all are likely to pro-
duce consistently invalid classifications, regardless
of whether a weighting scheme is used, it is the land-
marks which are well-known to some and poorly
known to others which should have gained the great-
est benefit from annotations weighted by familiarity.
However, the majority of such landmarks were al-
ready being classified correctly by the neural net-
work in both Experiments 1 and 2, which explains
why the neural network showed no improvement.
5 Discussion
Surprisingly, the naive conditions in Experiment 1
produced the best overall result, which was a 70%
accuracy for the neural network classifier. Although
the voting classifier and the neural network classi-
fier produced similar levels of accuracy for many of
the experiments, there was very little overlap in the
landmarks that were correctly assigned by each clas-
sifier. Of the 40 landmarks correctly assigned by the
neural network, 18 were incorrectly classified by the
voting classifier. Conversely, of the 37 landmarks
correctly assigned by the voting classifier, 15 were
incorrectly assigned by the neural network. This in-
dicates that the neural net is doing something more
sophisticated than simply assigning each landmark
to its maximum category.
A rather large subset of the landmarks was found
to be consistently misclassified by the neural net,
under various training conditions. For a number of
these landmarks, the annotators showed strong dis-
agreement and indicated that the landmark is am-
biguous, suggesting that there is indeed an inherent
ambiguity in the way these landmarks are concep-
tualised, both between annotators and on the web.
Interestingly, all of the hospitals in the landmark list
were consistently misclassified. A number of anno-
tators expressed confusion with regard to these land-
marks, as to whether the hospital itself or the sur-
rounding gardens should be taken into account. As a
result, annotations of the hospitals tended to be split
between point and area.
However, some of the landmarks that were mis-
classified by the neural net were classified consis-
tently by the annotators ? for example, GPO was
classified as a point by all of the Melbourne an-
notators. The ambiguity here presumably lies in
the web counts, which were not able to detect the
same conceptualisation generated by the annotators.
One complication with using web counts is the fact
14
Voting Classifier Neural Network Classifier
Experiment Accuracy (%) E.R.R. (%) Accuracy (%) E.R.R. (%)
1? 67.2 70.5 65.5 63.5
2? 58.6 35.2 65.5 63.5
Table 5: Results weighted according to landmark familiarity (Experiments 1? and 2?)
that the data is global in scope, and with a simple
abbreviation like GPO, there may well be interfer-
ence from documents which do not refer to the Mel-
bourne landmark, and in fact may not refer to a land-
mark or spatial object at all.
One of the underlying assumptions of the study
was that all landmarks can be represented as falling
into exactly one of the three conceptual classes ?
point, line or area. This may be an oversimplifica-
tion. Some landmarks may in fact be more proto-
typical or ambiguous than others. Certainly, a num-
ber of the landmark annotations were split almost
equally between point, line and area. It may be that
annotators did not or could not take upon themselves
the mentality of a cyclist as requested in the annota-
tion instructions, and instead simply conceptualised
the landmarks as they usually would, whether that
entails a pedestrian or car modality, or some alterna-
tive such as a train or tram-like modality. It may also
be the case that there are individual differences in
the way people conceptualise certain types of land-
marks, or indeed space in general, regardless of the
modality involved. If this is true, then the low inter-
annotator agreement may be a product of these indi-
vidual differences and not merely an artifact of the
experiment design.
In summary, we have proposed a method for clas-
sifying landmarks according to whether they are
most point-like, line-like or area-like, for use in the
generation of route descriptions. Our method re-
lies crucially on analysis of what prepositions and
verbs the landmarks co-occur with in web data. In a
series of experiments, we showed that we are able
to achieve accuracy levels nearing inter-annotator
agreement levels for the task.
One simplification made during the course of this
study was the treatment of parks and districts as be-
ing comparable entities (i.e. area-like landmarks). In
fact, a distinction may be made between open areas
such as districts, with which the preposition through
may be used, and closed areas such as parks, for
which through does not apply for car navigation (al-
though obviously does apply for pedestrian naviga-
tion). We hope to take this into account in future
work.
Acknowledgments
This research was supported by Australian Research Council
DP grant no. DP0770931. The authors wish to thank Lars Kulik
for his input into this research.
References
Gary L. Allen. 1997. From knowledge to words to wayfinding:
Issues in the production and comprehension of route direc-
tions. In Spatial Information Theory: Cognitive and Com-
putational Foundations of Geographic Information Science
(COSIT 1997), pages 363?372.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
Robert Dale, Sabine Geldof, and Jean-Philippe Prost. 2005.
Using natural language generation in automatic route de-
scription. Journal of Research and Practice in Information
Technology, 37(1):89?105.
Stefan Hansen, Kai-Florian Richter, and Alexander Klippel.
2006. Landmarks in OpenLS ? a data structure for cog-
nitive ergonomic route directions. In Geographic Informa-
tion Science ? Fourth International Conference, GIScience
2006, pages 128?144.
Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computational
Linguistics, 29(3):333?347.
Alexander Klippel. 2003. Wayfinding Choremes: Conceptual-
izing Wayfinding and Route Direction Elements. Ph.D. the-
sis, Universitt Bremen.
Barbara Landau and Ray Jackendoff. 1993. ?what? and
?where? in spatial cognition. Behavioral and Brain Sci-
ences, 16:217?65.
Mirella Lapata and Frank Keller. 2004. The web as a base-
line: Evaluating the performance of unsupervised web-based
models for a range of nlp tasks. In Proceedings of the Hu-
man Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics, pages 121?128.
15
Beth Levin. 1993. English Verb Classes and Alternations.
A Preliminary Investigation. University of Chicago Press,
Chicago.
Kirstin Lovelace, Mary Hegarty, and Daniel R. Montello. 1999.
Elements of good route directions in familiar and unfamil-
iar environments. In Spatial Information Theory: Cognitive
and Computational Foundations of Geographic Information
Science (COSIT 1999), pages 65?82, Stade, Germany.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Pierre-Emmanuel Michon and Michel Denis. 2001. When and
why are visual landmarks used in giving directions? In
Spatial Information Theory: Cognitive and Computational
Foundations of Geographic Information Science (COSIT
2001), pages 292?305, Morro Bay, USA.
Taro Tezuka and Katsumi Tanaka. 2005. Landmark extraction:
A web mining approach. In Spatial Information Theory:
Cognitive and Computational Foundations of Geographic
Information Science (COSIT 2005), pages 379?396, Elli-
cottville, USA.
Andrea Tyler and Vyvyan Evans. 2003. Lexical meaning and
experience: the semantics of English prepositions. Cam-
bridge University Press, Cambridge, U.K.
16
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 231?236,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-KB: Nominal Classification as Noun Compound Interpretation
Su Nam Kim and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper, we outline our approach to
interpreting semantic relations in nominal
pairs in SemEval-2007 task #4: Classifica-
tion of Semantic Relations between Nomi-
nals. We build on two baseline approaches
to interpreting noun compounds: sense col-
location, and constituent similarity. These
are consolidated into an overall system in
combination with co-training, to expand the
training data. Our two systems attained an
average F-score over the test data of 58.7%
and 57.8%, respectively.
1 Introduction
This paper describes two systems entered in
SemEval-2007 task #4: Classification of Semantic
Relations between Nominals. A key contribution of
this research is that we examine the compatibility of
noun compound (NC) interpretation methods over
the extended task of nominal classification, to gain
empirical insight into the relative complexity of the
two tasks.
The goal of the nominal classification task is to
identify the compatibility of a given semantic re-
lation with each of a set of test nominal pairs,
e.g. between climate and forest in the fragment the
climate in the forest with respect to the CONTENT-
CONTAINER relation. Semantic relations (or SRs)
in nominals represent the underlying interpretation
of the nominal, in the form of the directed relation
between the two nominals.
The proposed task is a generalisation of the more
conventional task of interpreting noun compounds
(NCs), in which we take a NC such as cookie jar and
interpret it according to a pre-defined inventory of
semantic relations (Levi, 1979; Vanderwende, 1994;
Barker and Szpakowicz, 1998). Examples of seman-
tic relations are MAKE,1, as exemplified in apple pie
where the pie is made from apple(s), and POSSES-
SOR, as exemplified in family car where the car is
possessed by a family.
In the SemEval-2007 task, SR interpretation
takes the form of a binary decision for a
given nominal pair in context and a given SR,
in judging whether that nominal pair conforms
to the SR. Seven relations were used in the
task: CAUSE-EFFECT, INSTRUMENT-AGENCY,
PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME-
TOOL, PART-WHOLE and CONTENT-CONTAINER.
Our approach to the task was to: (1) naively treat
all nominal pairs as NCs (e.g. the climate in the for-
est is treated as an instance of climate forest); and
(2) translate the individual binary classification tasks
into a single multiclass classification task, in the in-
terests of benchmarking existing SR interpretation
methods over a common dataset. That is, we take
all positive training instances for each SR and pool
them together into a single training dataset. For each
test instance, we make a prediction according to one
of the seven relations in the task, which we then
map onto a binary classification for final evaluation
purposes. This mapping is achieved by determining
which binary SR classification the test instance was
sourced from, and returning a positive classification
if the predicted SR coincides with the target SR, and
a negative classification if not.
We make three (deliberately naive) assumptions
in our approach to the nominal interpretation task.
First, we assume that all the positive training in-
1For direct comparability with our earlier research, seman-
tic relations used in our examples are taken from (Barker and
Szpakowicz, 1998), and differ slightly from those used in the
SemEval-2007 task.
231
stances correspond uniquely to the SR in question,
despite the task organisers making it plain that there
is semantic overlap between the SRs. As a machine
learning task, this makes the task considerably more
difficult, as the performance for the standard base-
lines drops considerably from that for the binary
tasks. Second, we assume that each nominal pair
maps onto a NC. This is clearly a misconstrual of the
task, and intended to empirically validate whether
such an approach is viable. In line with this assump-
tion, we will refer to nominal pairs as NCs for the
remainder of the paper. Third and finally, we assume
that the SR annotation of each training and test in-
stance is insensitive to the original context, and use
only the constituent words in the NC to make our
prediction. This is for direct comparability with ear-
lier research, and we acknowledge that the context
(and word sense) is a strong determinant of the SR
in practice.
Our aim in this paper is to demonstrate the effec-
tiveness of general-purpose SR interpretation over
the nominal classification task, and establish a new
baseline for the task.
The remainder of this paper is structured as fol-
lows. We present our methods in Section 2 and de-
pict the system architectures in Section 4. We then
describe and discuss the performance of our meth-
ods in Section 5 and conclude the paper in Section 6.
2 Approach
We used two basic NC interpretation methods. The
first method uses sense collocations as proposed by
Moldovan et al (2004), and the second method uses
the lexical similarity of the component words in the
NC as proposed by Kim and Baldwin (2005). Note
that neither method uses the context of usage of the
NC, i.e. the only features are the words contained in
the NC.
2.1 Sense Collocation Method
Moldovan et al (2004) proposed a method called se-
mantic scattering for interpreting NCs. The intuition
behind this method is that when the sense colloca-
tion of NCs is the same, their SR is most likely the
same. For example, the sense collocation of auto-
mobile factory is the same as that of car factory, be-
cause the senses of automobile and car, and factory
in the two instances, are identical. As a result, the
two NCs have the semantic relation MAKE.
The semantic scattering model is outlined below.
The probability P (r|fifj) (simplified to
P (r|fij)) of a semantic relation r for word
senses fi and fj is calculated based on simple
maximum likelihood estimation:
P (r|fij) =
n(r, fij)
n(fij)
(1)
and the preferred SR r? for the given word sense
combination is that which maximises the probabil-
ity:
r? = argmaxr?RP (r|fij)
= argmaxr?RP (fij |r)P (r) (2)
Note that in limited cases, the same sense collo-
cation can lead to multiple SRs. However, since we
do not take context into account in our method, we
make the simplifying assumption that a given sense
collocation leads to a unique SR.
2.2 Constituent Similarity Method
In earlier work (Kim and Baldwin, 2005), we pro-
posed a simplistic general-purpose method based on
the lexical similarity of unseen NCs with training
instances. That is, the semantic relation of a test
instance is derived from the train instance which
has the highest similarity with the test instance, in
the form of a 1-nearest neighbour classifier. For
example, assuming the test instance chocolate milk
and training instances apple juice and morning milk,
we would calculate the similarity between modifier
chocolate and each of apple and morning, and head
noun milk and each of juice and milk, and find, e.g.,
the similarities .71 and .27, and .83 and 1.00 respec-
tively. We would then add these up to derive the
overall similarity for a given NC and find that apple
juice is a better match. From this, we would assign
the SR of MAKE from apple juice to chocolate milk.
Formally, SA is the similarity between NCs
(Ni,1, Ni,2) and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1)? ((1? ?)S2 + S2))
2
(3)
where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is head noun similarity
232
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
The similarity scores are calculated using the
method of Wu and Palmer (1994) as implemented
in WordNet::Similarity (Patwardhan et al,
2003). This is done for each pairing of WordNet
senses of each of the two words in question, and the
overall lexical similarity is calculated as the average
across the pairwise sense similarities.
The final classification is derived from the training
instance which has the highest lexical similarity with
the test instance in question.
3 Co-Training
As with many semantic annotation tasks, SR tag-
ging is a time-consuming and expensive process. At
the same time, due to the inherent complexity of the
SR interpretation task, we require large amounts of
training data in order for our methods to perform
well. In order to generate additional training data to
train our methods over, we experiment with different
co-training methodologies for each of our two basic
methods.
3.1 Co-Training for the Sense Collocation
Method
For the sense collocation method, we experiment
with a substitution method whereby we replace one
constituent in a training NC instance by a similar
word, and annotate the new instance with the same
SR as the original NC. For example, car in car fac-
tory (SR = MAKE) has similar words automobile,
vehicle, truck from the synonym, hypernym and sis-
ter word taxonomic relations, respectively. When
car is replaced by a similar word, the new noun
compound(s) (i.e. automobile/vehicle/truck factory)
share the same SR as the original car factory. Note
that each constituent in our original example is
tagged for word sense, which we use both in ac-
cessing sense-specific substitution candidates (via
WordNet), and sense-annotating the newly gener-
ated NCs.
Substitution is restricted to one constituent at a
time in order to avoid extreme semantic variation.
This procedure can be repeated to generate more
training data. However, as the procedure goes fur-
ther, we introduce increasingly more noise.
In our experiments, we use this co-training
method with the sense collocation method to expand
the size and variation of training data, using syn-
onym, hypernym and sister word relations. For our
experiment, we ran the expansion procedure for only
one iteration in order to avoid generating excessive
amounts of incorrectly-tagged NCs.
3.2 Co-Training for the Constituent Similarity
Method
Our experiments with the constituent similarity
method over the trial data showed, encouragingly,
that there is a strong correlation between the strength
of overall similarity with the best-matching training
NC, and the accuracy of the prediction. From this,
we experimented with implementing the constituent
similarity method in a cascading architecture. That
is, we batch evaluate all test instances on each it-
eration, and tag those test instances for which the
best match with a training instance is above a pre-
set threshold, which we decrease on each iteration.
In subsequent iterations, all tagged test instances are
included in the training data. Hence, on each itera-
tion, the number of training instances is increasing.
As our threshold, we used a starting value of 0.85,
which was decreased down to 0.65 in increments of
0.05.
4 Architectures
In Section 4.1 and Section 4.2, we describe the ar-
chitecture of our two systems.
4.1 Architecture (I)
Figure 1 presents the architecture of our first system,
which interleaves sense collocation and constituent
similarity, and includes co-training for each. There
are five steps in this system.
First, we apply the basic sense collocation method
relative to the original training data. If the sense col-
location between the test and training instances is
the same, we judge the predicted SR to be correct.
Second, we apply the similarity method described
in Section 2.2 over the original training data. How-
ever, we only classify test instances where the final
similarity is above a threshold of 0.8.
Third, we apply the sense collocation co-training
method and re-run the sense collocation method
over the expanded training data from the first two
steps. Since the sense collocations in the expanded
233
TEST
untagged test data
untagged test data
untagged test data
untagged test data
tagged data
tagged data
tagged data
tagged data
tagged data
TRAIN
Extension of
Training databy similar words
? Synonym
? Hypernym
? Sister word
Extended TRAIN
Sense Collcation
Step 1
Similarity
Step 2
Step 3
Step 4
Similarity
Step 5
Sense Collcation
Similarity
Figure 1: System Architecture (I)
training data have been varied through the advent of
hypernyms and sister words, the number of sense
collocations in the expanded training data is much
greater than that of the original training data (937
vs. 16,676).
Fourth, we apply the constituent similarity co-
training method over the consolidated training data
(from both sense collocation and constituent simi-
larity co-training) with the threshold unchanged at
0.8.
Finally, we apply the constituent similarity
method over the combined training data, without any
threshold (to guarantee a SR prediction for every
test instance). However, since the generated train-
ing instances are more likely to contain errors, we
decrement the similarity values for generated train-
ing instances by 0.2, to prefer predictions based on
the original training instances.
4.2 Architecture (II)
Figure 2 depicts our second system, which is based
solely on the constituent similarity method, with co-
training.
We perform iterative co-training as described in
TRAIN
#of Tagged
>= 10% of testThreshold
Tagged
finalize current
tags and end
reduce Threshold
TEST
get Similarity
Sim >= TN Y
Y
N
if T == 0.6 &(#of Tagged <
10% of test)
N
Y
Figure 2: System Architecture (II)
Section 3.2, with the slight variation that we hold
off reducing the threshold if more than 10% of the
test instances are tagged on a given iteration, giving
other test instances a chance to be tagged at a higher
threshold level relative to newly generated training
instances. The residue of test instances on comple-
tion of the final iteration (threshold = 0.6) are tagged
according to the best-matching training instance, ir-
respective of the magnitude of the similarity.
5 Evaluation
We group our evaluation into two categories: (A)
doesn?t use WordNet 2.1 or the query context;
and (B) uses WordNet 2.1 only (again with-
out the query context). Of our two basic meth-
ods the sense collocation method and co-training
method are based on WordNet 2.1 only, while
the constituent similarity method is based indirectly
on WordNet 2.1, but doesn?t preserve WordNet
2.1 sense information. Hence, our first system is
category B while our second system is (arguably)
category A.
Table 1 presents the three baselines for the task,
and the results for our two systems (System I and
System II). The performance for both systems ex-
ceeded all three baselines in terms of accuracy, and
all but the All True baseline (i.e. every instance is
judged to be compatible with the given SR) in terms
234
Method P R F A
All True 48.5 100.0 64.8 48.5
Probability 48.5 48.5 48.5 51.7
Majority 81.3 42.9 30.8 57.0
System I 61.7 56.8 58.7 62.5
System II 61.5 55.7 57.8 62.7
Table 1: System results (P = precision, R = recall, F
= F-score, and A = accuracy)
Team P R F A
759 66.1 66.7 64.8 66.0
281 60.5 69.5 63.8 63.5
633 62.7 63.0 62.7 65.4
220 61.5 55.7 57.8 62.7
161 56.1 57.1 55.9 58.8
538 48.2 40.3 43.1 49.9
Table 2: Results of category A systems
of F-score and recall.
Tables 2 and 3 show the performance of the teams
which performed in the task, in categories A and B.
Team 220 in Table 2 is our second system, and team
220 in Table 3 is our first system.
In Figures 3 and 4, we present a breakdown of
the performance our first and second system, re-
spectively, over the individual semantic relations.
Our approaches performed best for the PRODUCT-
PRODUCER SR, and worst for the PART-WHOLE
SR. In general, our systems achieved similar perfor-
mance on most SRs, with only PART-WHOLE be-
ing notably worse. The lower performance of PART-
WHOLE pulls down our overall performance consid-
erably.
Tables 4 and 5 show the number of tagged and un-
tagged instances for each step of System I and Sys-
tem II, respectively. The first system tagged more
than half of the data in the fifth (and final) step,
where it weighs up predictions from the original and
expanded training data. Hence, the performance of
this approach relies heavily on the similarity method
and expanded training data. Additionally, the differ-
ence in quality between the original and expanded
training data will influence the performance of the
approach appreciably. On the other hand, the num-
ber of instances tagged by the second system is well
distributed across each iteration. However, since
we accumulate generated training instances on each
step, the relative noise level in the training data will
Team P R F A
901 79.7 69.8 72.4 76.3
777 70.9 73.4 71.8 72.9
281 72.8 70.6 71.5 73.2
129 69.9 64.6 66.8 71.4
333 62.0 71.7 65.4 67.0
538 66.7 62.8 64.3 67.2
571 55.7 66.7 60.4 59.1
759 66.4 58.1 60.3 63.6
220 61.7 56.8 58.7 62.5
371 56.8 56.3 56.1 57.7
495 55.9 57.8 51.4 53.7
Table 3: Results of category B systems
CE IA PP OE TT PW CC
relations
(%) F?scorerecallprecision accuracy
 0
 20
 40
 60
 80
 100
Figure 3: System I performance for each rela-
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
increase across iterations, impacting on the final per-
formance of the system.
Over the trial data, we noticed that the system pre-
dictions are appreciably worse when the similarity
value is low. In future work, we intend to analyse
what is happening in terms of the overall system
performance at each step. This analysis is key to
improving the performance of our systems.
Recall that we are generalising from the set of
binary classification tasks in the original task, to a
multiclass classification task. As such, a direct com-
parison with the binary classification baselines is
perhaps unfair (particularly All True, which has no
correlate in a multiclass setting), and it is if anything
remarkable that our system compares favourably
compared to the baselines. Similarly, while we
clearly lag behind other systems participating in the
235
(%)
CE IA PP OE TT PW CC
relations
F?scorerecallprecision accuracy
 0
 20
 40
 60
 80
 100
Figure 4: System II performance for each rela-
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
step method tagged accumulated untagged
s1 SC 21 3.8% 528
s2 Sim 106 23.1% 422
s3 extSC 0 23.1% 422
s4 extSim 61 34.2% 361
s5 SvsExtS 359 99.6% 2
Table 4: System I: Tagged data from each step
(SC= sense collocation; Sim = the similarity method;
extSC = SC over the expanded training data; extSim
= similarity over the expanded training data; SvsExtS
= the final step over both the original and expanded
training data)
task, we believe we have demonstrated that NC in-
terpretation methods can be successfully deployed
over the more general task of nominal pair classifi-
cation.
6 Conclusion
In this paper, we presented two systems entered in
the SemEval-2007 Classification of Semantic Re-
lations between Nominals task. Both systems are
based on baseline NC interpretation methods, and
the naive assumption that the nominal classification
task is analogous to a conventional multiclass NC
interpretation task. Our results compare favourably
with the established baselines, and demonstrate that
NC interpretation methods are compatible with the
more general task of nominal classification.
I T tagged accumulated untagged
i1 .85 73 13.3% 476
i2 .80 56 23.5% 420
i3 .75 74 37.0% 346
i4 .70 101 55.4% 245
i5 .65 222 95.8% 23
? <.65 21 99.6% 2
Table 5: System II: data tagged on each iteration (T
= the threshold; iX = the iteration number)
Acknowledgments
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of the
17th International Conference on Computational Linguis-
tics, pages 96?102, Montreal, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Timothy W. Finin. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D. thesis, University of Illinois.
Su Nam Kim and Timothy Baldwin. 2005. Automatic inter-
pretation of Noun Compounds using WordNet similarity. In
Proc. of the 2nd International Joint Conference On Natural
Language Processing, pages 945?956, JeJu, Korea.
Judith Levi. 1979. The syntax and semantics of complex nom-
inals. In The Syntax and Semantics of Complex Nominals.
New York:Academic Press.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe,
and Roxana Girju. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of the HLT-NAACL 2004
Workshop on Computational Lexical Semantics, pages 60?
67, Boston, USA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the Fourth International
Conference on Intelligent Text Processing and Computa-
tional Linguistics, pages 241?57, Mexico City, Mexico.
Lucy Vanderwende. 1994. Algorithm for automatic interpreta-
tion of noun sequences. In Proc. of the 15th conference on
Computational linguistics, pages 782?788, Kyoto, Japan.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics and
lexical selection. In Proc. of the 32nd Annual Meeting of the
Association for Computational Linguistics, pages 133?138,
Las Cruces, USA.
236
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 237?240,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-MKB: Lexical Substitution System based on Relatives in Context
David Martinez, Su Nam Kim and Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper we describe the MELB-MKB
system, as entered in the SemEval-2007 lex-
ical substitution task. The core of our sys-
tem was the ?Relatives in Context? unsuper-
vised approach, which ranked the candidate
substitutes by web-lookup of the word se-
quences built combining the target context
and each substitute. Our system ranked third
in the final evaluation, performing close to
the top-ranked system.
1 Introduction
This paper describes the system we developed for
the SemEval lexical substitution task, a new task in
SemEval-2007. Although we tested different con-
figurations on the trial data, our basic system relied
on WordNet relatives (Fellbaum, 1998) and Google
queries in order to identify the most plausible sub-
stitutes in the context.
The main goal when building our system was to
study the following factors: (i) substitution candi-
date set, (ii) settings of the relative-based algorithm,
and (iii) syntactic filtering. We analysed these fac-
tors over the trial data provided by the organisation,
and used the BEST metric to tune our system. This
metric accepts multiple answers, and averages the
score across the answers. We did not experiment
with the OOT (top 10 answers) and MULTIWORD
metrics.
In the remainder of this paper we briefly intro-
duce the basic Relatives in Context algorithm in Sec-
tion 2. Next we describe our experiments on the trial
data in Section 3. Our final system and its results are
described in Section 4. Finally, our conclusions are
outlined in Section 5.
2 Algorithm
Our basic algorithm is an unsupervised method pre-
sented in Martinez et al (2006). This technique
makes use of the WordNet relatives of the target
word for disambiguation, by way of the following
steps: (i) obtain a set of close relatives from Word-
Net for each sense of the target word; (ii) for each
test instance define all possible word sequences that
include the target word; (iii) for each word sequence,
substitute the target word with each relative, and
then query Google; (iv) rank queries according to
the following factors: length of the query, distance
of the relative to the target word, and number of hits;
and (v) select the relative from the highest ranked
query.1
For the querying step, first we tokenise each tar-
get sentence, and then we apply sliding windows of
different sizes (up to 6 tokens) that include the tar-
get word. For each window and each relative in the
pool, we substitute the target word for the relative,
and query Google. The algorithm stops augment-
ing the window for the relative when one of its sub-
strings returns zero hits. The length of the query is
measured as the number of words, and the distance
of the relative to the target words gives preference
to synonyms over hypernyms, and immediate hyper-
nyms over further ones.
One important parameter in this method is the
candidate set. We performed different experiments
to measure the expected score we could achieve
1In the case of WSD we would use the relative to chose the
sense it relates to.
237
from WordNet relatives, and the contribution of dif-
ferent types of filters (syntactic, frequency-based,
etc.) to the overall result. We also explored other
settings of the algorithm, such as the ranking crite-
ria, and the number of answers to return. These ex-
periments and some other modifications of the basic
algorithm are covered in Section 3.
3 Development on Trial data
In this section we analyse the coverage of WordNet
over the data, the basic parameter exploration pro-
cess, a syntactic filter, and finally the extra experi-
ments we carried out before submission. The trial
data consisted on 300 instances of 34 words with
gold-standard annotations.
3.1 WordNet coverage
The most obvious resource for selecting substitution
candidates was WordNet, due to its size and avail-
ability. We used version 2.0 throughout this work.
In our first experiment, we tried to determine which
kind of relationships to use, and the coverage of the
gold-standard annotations that we could expect from
WordNet relations only. As a basic set of relations,
we used the following: SYNONYMY, SIMILAR-TO,
ENTAILMENT, CAUSE, ALSO-SEE, and INSTANCE.
We created two extended candidate sets using im-
mediate and 2-step hypernyms (hype and hype2, re-
spectively, in Table 1).
Given that we are committed to using Word-
Net, we set out to measure the percentage of gold-
standard substitutes that were ?reachable? using dif-
ferent WordNet relations. Table 1 shows the cov-
erage for the three sets of candidates. Instance-
coverage indicates the percentage of instances that
have at least one of the gold-standard instances cov-
ered from the candidate set. We can see that the per-
centage is surprisingly low.
Any shortcoming in coverage will have a direct
impact on performance, suggesting the need for al-
ternate means to obtain substitution candidates. One
possibility is to extend the candidates from Word-
Net by following links from the relatives (e.g. col-
lect all synonyms of the synonymous words), but
this could add many noisy candidates. We can also
use other lexical repositories built by hand or auto-
matically, such as the distributional theusauri built
Candidate Set Subs. Cov. Inst. Cov.
basic 344/1152 (30%) 197 / 300 (66%)
hype 404/1152 (35%) 229/300 (76%)
hype2 419/1152 (36%) 229/300 (76%)
Table 1: WordNet coverage for different candidate
sets, based on substitute (Subs.) and instance (Inst.)
coverage.
in Lin (1998). A different approach that we are test-
ing for future work is to adapt the algorithm to work
with wildcards instead of explicit candidates. Due to
time constraints, we only relied on WordNet for our
submission.
3.2 Parameter Tuning
In this experiment we tuned different parameters of
the basic algorithm. First, we observed the data in
order to identify the most relevant variables for this
task. We tried to avoid including too many parame-
ters and overfitting the system to the trial dataset. At
this point, we separated the instances by PoS, and
studied the following parameters:
Candidate set: From WordNet, we tested four
possible datasets for each target word: basic-set, 1st-
sense (basic relations from the first sense only), hype
(basic set and immediate hypernyms), and hype2
(basic set and up to two-step hypernyms).
Semcor-based filters: Semcor provides frequency
information for WordNet senses, and can be used
to identify rare senses. As each candidate is ob-
tained via WordNet semantic relations with the tar-
get word, we can filter out those candidates that are
related with unfrequent senses in Semcor. We tested
three configurations: (1) no filter, (2) filter out candi-
dates when the candidate-sense in the relation does
not occur in Semcor, (3) and filter out candidates
when the target-sense in the relation does not oc-
cur in Semcor. The filters can potentially lead to the
removal of all candidates, in which case a back-off
is applied (see below).
Relative-ranking criteria: Our algorithm ranks
relatives according to the length in words of their
context-match. In the case of ties, the number of re-
turned hits from Google is applied. The length can
be different depending on whether we count punc-
tuation marks as separate tokens, and whether the
word-length of substitute multiwords is included.
238
We tested three options: including the target word,
not including the target word (multiwords count as a
single word), and not counting punctuation marks.
Back-off: We need a back-off method in case the
basic algorithm does not find any matches. We
tested the following: sense-ordered synonyms from
WordNet (highest sense first, randomly breaking
ties), and most frequent synonyms from the first sys-
tem (using two corpora: Semcor and BNC).
Number of answers: We also measured the per-
formance for different numbers of system outputs
(1, 2, or 3).
All in all, we performed 324 (4x3x3x3x3) runs
for each PoS, based on the different combinations.
The best scores for each PoS are shown in Table 2,
together with the baselines. We can see that the pre-
cision is above the official WordNet baseline, but is
still very low. The results illustrate the difficulty of
the task. In error analysis, we observed that the per-
formance and settings varied greatly depending on
the PoS of the target word. Adverbs produced the
best performance, followed by nouns. The scores
were very low for adjectives and verbs (the baseline
score for verbs was only 2%).
We will now explain the main conclusions ex-
tracted from the parameter analysis. Regarding the
candidate set, we observed that using synonyms only
was the best approach for all PoS, except for verbs,
where hypernyms helped. The option of limiting the
candidates to the first sense only helped for adjec-
tives, but not for other PoS.
For the Semcor-based filter, our results showed
that the target-sense filter improved the performance
for verbs and adverbs. For nouns and adjectives, the
candidate-sense filter worked best. All in all, apply-
ing the Semcor filters was effective in removing rare
senses and improving performance.
The length criteria did not affect the results signif-
icantly, and only made a difference in some extreme
cases. Not counting the length of the target word
helped slightly for nouns and adverbs, and removing
punctuation improved results for adjectives. Regard-
ing the back-off method, we observed that the count
of frequencies in Semcor was the best approach for
all PoS except verbs, which reached their best per-
formance with BNC frequencies.
PoS Relatives in Context WordNet Baseline
Nouns 18.4 14.9
Verbs 6.7 2.0
Adjectives 9.6 7.5
Adverbs 31.1 29.9
Overall 14.4 10.4
Table 2: Experiments to tune parameters on the trial
data, based on the BEST metric. Scores correspond
to precision (which is the same as recall).
Finally, we observed that the performance for the
BEST score decreased significantly when more than
one answer was returned, probably due to the diffi-
culty of the task.
3.3 Syntactic Filter
After the basic parameter analysis, we studied the
contribution of a syntactic filter to remove those can-
didates that, when substituted, generate an ungram-
matical sentence. Intuitively, we would expect this
to have a high impact for verbs, which vary consid-
erably in their subcategorisation properties. For ex-
ample, in the case of the (reduced) target If we order
our lives well ..., the syntactic filter should ideally
disallow candidates such as If we range our lives
well ...
In order to apply this filter, we require a parser
which has an explicit notion of grammaticality, rul-
ing out the standard treebank parsers. We experi-
mented briefly with RASP, but found that the En-
glish Resource Grammar (ERG: Flickinger (2002)),
combined with the PET run-time engine, was the
best fit for out needs. Unfortunately we could not get
unknown word handling working within the ERG
for our submission, such that we get a meaningful
output for a given input string only in the case that
the ERG has full lexical coverage over that string
(we will never get a spanning parse for an input
where we are missing lexical entries). As such, the
syntactic filter is limited in coverage only to strings
where the ERG has lexical coverage.
Ideally, we would have tested this filter on trial
data, but unfortunately we ran out of time. Thus, we
simply eyeballed a sample of examples, and we de-
cided to include this filter in our final submission. As
we will see in Section 4, its effect was minimal. We
plan to perform a complete evaluation of this module
in the near future.
239
3.4 Extra experiments
One of the limitations of the ?Relatives in Context?
algorithm is that it only relies on the local con-
text. We wanted to explore the contribution of other
words in the context for the task, and we performed
an experiment including the Topical Signatures re-
source (Agirre and Lopez de Lacalle, 2004). We
simply counted the overlapping of words shared be-
tween the context and the different candidates. We
only tested this for nouns, for which the results were
below baseline. We then tried to integrate the topic-
signature scores with the ?Relatives in Context? al-
gorithm, but we did not improve our basic system?s
results on the trial data. Thus, this approach was not
included in our final submission.
Another problem we observed in error analysis
was that the Semcor-based filters were too strict in
some cases, and it was desirable to have a way of
penalising low frequency senses without removing
them completely. Thus, we weighted senses by the
inverse of their sense-rank. As we did not have time
to test this intuition properly, we opted for applying
the sense-weighting only when the candidates had
the same context-match length, instead of using the
number of hits. We will see the effect of this method
in the next section.
4 Final system
The test data consisted of 1,710 instances. For our
final system we applied the best configuration for
each PoS as observed in the development experi-
ments, and the syntactic filter. We also incorpo-
rated the sense-weighting to solve ties. The results
of our system, the best competing system, and the
best baseline (WordNet) are shown in Table 3 for the
BEST metric. Precision and recall are provided for
all the instances, and also for the ?Mode? instances
(those that have a single preferred candidate).
Our method outperforms the baseline in all cases,
and performs very close to the top system, ranking
third out of eight systems. This result is consistent
in the ?further analysis? tables provided by the task
organisers for subsets of data, where our system al-
ways performs close to the top score. The overall
scores are below 13% recall for all systems when
targeting all instances. This illustrates the difficulty
of the task, and the similarity of the top-3 scores sug-
All instances Mode
System P R P R
Best 12.90 12.90 20.65 20.65
Relat. in Context 12.68 12.68 20.41 20.41
WordNet baseline 9.95 9.95 15.28 15.28
Table 3: Official results based on the BEST metric.
gests that similar resources (i.e. WordNet) have been
used in the development of the systems.
After the release of the gold-standard data, we
tested two extra settings to measure the effect of the
syntactic filter and the sense-weighting in the final
score. We observed that our application of the syn-
tactic filter had almost no effect in the performance,
but sense-weighting increased the overall recall by
0.4% (from 12.3% to 12.7%).
5 Conclusions
Although the task was difficult and the scores were
low, we showed that by using WordNet and the lo-
cal context we are able to outperform the baselines
and achieve close to top performance. For future
work, we would like to integrate a parser with un-
known word handling in our system. We also aim to
adapt the algorithm to match the target context with
wildcards, in order to avoid explicitly defining the
candidate set.
Acknowledgments
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly avail-
able topic signatures for all WordNet nominal senses. In
Proc. of the 4rd International Conference on Languages Re-
sources and Evaluations (LREC 2004), pages 1123?6, Lis-
bon, Portugal.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Dan Flickinger. 2002. On building a more efficient grammar by
exploiting types. In Stephan Oepen, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors, Collaborative Language
Engineering. CSLI Publications, Stanford, USA.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, pages 768?74,
Montreal, Canada.
David Martinez, Eneko Agirre, and Xinglong Wang. 2006.
Word relatives in context for word sense disambiguation. In
Proc. of the 2006 Australasian Language Technology Work-
shop, pages 42?50, Sydney, Australia.
240
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 241?244,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-YB: Preposition Sense Disambiguation Using Rich Semantic
Features
Patrick Ye and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Australia
{jingy,tim}@csse.unimelb.edu.au
Abstract
This paper describes a maxent-based prepo-
sition sense disambiguation system entry to
the preposition sense disambiguation task
of the SemEval 2007. This system uses a
wide variety of semantic and syntactic fea-
tures to perform the disambiguation task and
achieves a precision of 69.3% over the test
data.
1 Introduction
Prepositional phrases (PPs) are both common and
semantically varied in open English text. While the
conventional view on prepositions from the com-
putational linguistics community has been that they
are semantically transient at best, and semantically-
vacuous at worst, a robust account of the semantics
of prepositions and disambiguation method can be
helpful in a range of NLP tasks including machine
translation, parsing (prepositional phrase attach-
ment) and semantic role labelling (Durand, 1993;
O?Hara and Wiebe, 2003; Ye and Baldwin, 2006a).
The SemEval 2007 preposition sense disambigua-
tion task provides a common test bed for the evalua-
tion of preposition sense disambiguation systems.
Our proposed method is maximum entropy based,
and combines features developed in the context of
preposition sense disambiguation for semantic role
labelling (Ye and Baldwin, 2006a), and verb sense
disambiguation (Ye and Baldwin, 2006b).
The remainder of this paper is structured as fol-
lows. We first discuss the pre-processing steps
used in our system (Section 2), and outline the fea-
tures our preposition disambiguation method uses
(Section 3) and our parameter tuning method (Sec-
tion 4). We then discuss and analyse the results of
our method (Section 5) and conclude the paper (Sec-
tion 6).
2 Pre-processing
The following list shows the pre-processing steps
that our system goes through and the tools used:
Part of speech tagging SVMTool version 1.2
(Gime?nez and Ma`rquez, 2004).
Chunking An in-house chunker implemented
with fnTBL, a transformation based learner (Ngai
and Florian, 2001), and trained on the British Na-
tional Corpus (BNC).1
Parsing Charniak?s re-ranking parser, version Au-
gust, 2006 (Charniak and Johnson, 2005).
Named entity extraction A statistical NER sys-
tem described in Cohn et al (2005).
Supersense tagging A WordNet-based super-
sense tagger (Ciaramita and Altun, 2006).
Semantic role labeling ASSERT version 1.4
(Pradhan et al, 2004).
3 Features
The disambiguation features used by our system can
be divided into three categories: collocation fea-
tures, syntactic features and semantic-role based fea-
tures. We discuss each in turn below.
3.1 Collocation Features
The collocation features were inspired by the
one-sense-per-collocation heuristic proposed by
Yarowsky (1995). These features were designed to
capture open class words that exhibit strong colloca-
tion properties with respect to the different senses of
the target preposition. Details of the features in this
category are listed below.
1This chunker is not exactly the same as Ngai and Florian?s
system, however it does use the default transformation tem-
plates supplied by fnTBL.
241
Bag of open class words The part-of-speech
(POS) tags and lemmas of all the open class words
that occur in the same sentence as the target prepo-
sition.
Bag of WordNet synsets The WordNet (Miller,
1993) synonym sets and their hypernyms of all the
open class words that occur in the same sentence as
the target preposition.
Bag of named entities Each named entity in the
same sentence as the target preposition is treated as
a separate feature.
Surrounding words These features are the com-
binations of the lemma, POS tag and relative posi-
tion of the words surrounding the target preposition
within a window of 7 words.
Surrounding super senses These features are the
combinations of super-sense tag, POS tag and rel-
ative position of the words surrounding the target
preposition within a window of 7 words.
3.2 Syntactic Features
The syntactic features were designed to capture both
the flat and recursive syntactic properties of the tar-
get preposition. The flat syntactic features were de-
rived from the surrounding POS tags and chunk tags
of the target preposition; the recursive syntactic fea-
tures were derived from the parse trees. The details
of these feature are given below.
Surrounding POS tags These features are the
combination of POS tag and relative position of the
words surrounding the target preposition within a
window of 7 words.
Surrounding chunk tags These features are the
combination of IOB style chunk tag and relative po-
sition of the words surrounding the target preposi-
tion within a window of 5 words.
Surrounding chunk types Instead of using only
the chunk tags themselves, we also extracted the ac-
tual chunk types (NP, VP, ADJP, etc) of the words
surrounding the target preposition within a window
of 5 words. Each chunk type is also combined with
its relative position to the target preposition as a sep-
arate feature.
S
I
NP VP
live
in 
PP
Melbourne
S_NP S_VP
live VP_PP
in PP_NP
Melbourne
I
S
NP
Figure 1: Parse tree examples
Parse tree features Given the position of the tar-
get preposition p in the parse tree, the basic form of
the corresponding parse tree feature is just the list of
nodes of p?s siblings in the tree (the POS tags are
treated as part of the terminal). For example, sup-
pose the original parse tree for the sentence I live in
Melbourne is the left tree in Figure 1, for the target
preposition in, the basic form of the parse tree fea-
ture would be (1, NP). In order to gain more syn-
tactic information, we further annotated each non-
terminal of the parse tree with its parent node, and
used the new non-terminals as our features. The
right tree in Figure 1 shows the result of applying
this annotation once to the original parse tree. Two
levels of additional annotation were performed on
the original parse trees in our feature extraction.
3.3 Semantic-Role Based Features
Finally, since prepositional phrases can often func-
tion as the temporal, location, and manner modifiers
for verbs, we designed semantic-role-based features
to specifically capture this type of verb-preposition
semantic information. The details of these features
are as follows:
Surrounding semantic role tags The semantic
role tags of the words surrounding the target preposi-
tion within a window of 5 words are combined with
their relative positions to the target preposition and
treated as separate features. For example, consider
the preposition on in the sentence The man who
stole my car on Sunday has apologised to me, the
semantic roles for the two verbs (stole and apolo-
gised ) are shown in Table 1. The semantic roles for
stole would generate the following features: (-5, I-
A0), (-4, R-A0), (-3, TARGET), (-2, B-A1), (-1,
I-A1), (0, B-AM-TMP), (1, I-AM-TMP), (2, O), (3,
O), (4, O and (5, O).
Attached verbs This feature was designed to
capture the verb-particle and verb-preposition-
242
The man who stole my car on Sunday has apologised to me
stole B-A0 I-A0 R-A0 TARGET B-A1 I-A1 B-AM-TMP I-AM-TMP O O O O
apologised B-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 I-A0 O TARGET B-A2 I-A2
Table 1: Example semantic-role-labelled sentence
attachment relationships between verbs and prepo-
sitions. There are two situations in which a preposi-
tion p is deemed to be attached to a verb v: (1) p has
a semantic role tag relative to v and this tag is a ?B?
tag, (2) p has no semantic role tag relative to v, but
the first token to the right of p has a ?B? tag relative
to v. In the sentence shown in Table 1, stole would
be considered as the governor of on.
Verb?s relative position The lemma of each verb
in the same sentence as the target preposition is com-
bined with its relative position to the target preposi-
tion and treated as a separate feature. For example,
the sentence shown in Table 1 would generate the
two features: (-1, steal) and (1, apologize).
More detailed descriptions and examples for these
features may be found in Ye and Baldwin (2006b).
4 Parameter Tuning
We used the ranking-based feature selection method
from Ye and Baldwin (2006b) to select the most rele-
vant feature based on our training data. This method
works in two steps. Firstly, we calculated the infor-
mation gain, gain ratio and Chi-squared statistics for
each feature, and used these values to generate 3 sets
of rankings for the features. We then summed up the
individual ranks, and used the sums to create a set of
final rankings for the features.
The feature selection process is based on 10-fold
cross validation: we divided our training data into
10 pairs of training-test datasets; then for each fold,
we extracted the top N% ranked features using our
feature selection heuristic from the cv-training set
(where N was set to values 5, 10, .., 100), and used
these features to test the held-out test set. The best
N as determined by the cross validation was then
applied to the entire training data set.
Additionally, since we used a maximum entropy-
based machine learning package,2 it was important
to determine the best Gaussian smoothing parameter
g for the probability distribution. The tuning of g
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
was incorporated into the cross validation process of
feature selection.
Given the possible combinations of parameter
tuning, we trained the following three classifiers for
the preposition sense disambiguation task:
Non-tuned Using all the original features and
10.0 for the Gaussian smoothing parameter.
Smoothing-tuned Using all the original features
but automatically tuned Gaussian smoothing param-
eter.
Fully-tuned Using both automatically tuned fea-
tures and Gaussian smoothing parameter.
5 Results and Analysis
The overall precision (%) obtained by the three clas-
sifiers for the fine-grained senses are as follows:
Non-tuned Smoothing-tuned Fully tuned
67.9 68.0 69.3
The best overall results were achieved when both
the features and the Gaussian smoothing parameters
were automatically tuned, achieving a 1.4% absolute
precision gain over the non-tuned system. However,
such parameter tuning may not always be useful: the
same tuning process was found to be detrimental in a
Senseval-2 verb sense disambiguation task (Ye and
Baldwin, 2006b). Consistent with the findings of
Ye and Baldwin (2006b), the improvement caused
by the tuning of the Gaussian smoothing parame-
ter is only marginal compared with the improvement
caused by the tuning of the features.
We also evaluated our features based on their cate-
gories and types. Collocation features performed the
best among the three feature categories. Without any
parameter tuning, the collocation-feature-only clas-
sifier achieved an overall precision of 67.4% on the
test set; the semantic-role-feature-only classifier and
the syntactic-feature-only classifier achieved preci-
sion of 46.9% and 50.5% respectively.
The best-performing individual features are the
bag-of-words features and bag-of-synsets features.
243
Feature type % in Overall
Feature type top N% features % of the
10 20 30 feature type
Bag of Words 13.46 13.43 12.94 13.37
Bag of Synsets 57.83 58.38 59.53 58.29
Verb?s rel. positions 3.97 3.95 3.76 4.02
Surrounding POS tags 1.36 1.33 1.43 1.27
Table 2: Percentages of top-performing feature
types in the top N% ranked features
On the test set, the bag-of-words-only classifier and
the bag-of-synsets-only classifier achieved overall
precision of 63.2% and 61.9% respectively.
We also analysed the top ranking features as cal-
culated by our feature selection algorithm, as pre-
sented in Table 2. The results show the percentages
of the top-performing feature types of each feature
category in the top N% ranked features. It can be
observed that none of the top-performing features
seem to have a significantly disproportional repre-
sentation in the top-ranked features. This indicates
that the disambiguation power of a particular type
of features is determined mostly by the number of
features of that type.
On the other hand, the bag-of-words features ap-
pear to be the most effective, considering that they
account for only 13.4% of the total features, but
out-performed the bag-of-synsets features which ac-
count for nearly 60% of the total features.
It is also disappointing to see that the syntactic
and semantic-role based features had little positive
influence in the disambiguation process. However,
this is perhaps caused by the sparseness of these fea-
tures since they together only account for less than
10% of all the extracted features.
The overall finding from all this is that, similar
to nouns and verbs, preposition sense is determined
primarily by word context, and that syntactic and se-
mantic role-based features play only a minor role.
6 Conclusions
In this paper, we have described a maximum entropy
based preposition sense disambiguation system that
uses a rich set of features. We have shown that
this system performed well above the majority class
baseline of 39.6% precision. Our analysis showed
that the most important disambiguation features are
collocation-based features. This indicates that the
semantics of prepositions can be learnt mostly from
their surrounding context, and not syntactic proper-
ties or verb-preposition semantics.
Acknowledgements
The research in this paper has been supported by the Aus-
tralian Research Council through Discovery Project grant num-
ber DP0663879.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 173?180, Ann
Arbor, USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia.
Trevor Cohn, Andrew Smith, and Miles Osborne. 2005. Scal-
ing conditional random fields using error-correcting codes.
In Proceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages 10?17,
Ann Arbor, USA.
Jacques Durand. 1993. On the translation of prepositions in
multilingual MT. In Frank Van Eynde, editor, Linguistic Is-
sues in Machine Translation, pages 138?159. Pinter Publish-
ers, London, UK.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector machines.
In Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, pages 43?46, Lisbon, Por-
tugal.
George A. Miller. 1993. Wordnet: a lexical database for en-
glish. In HLT ?93: Proceedings of the workshop on Human
Language Technology, pages 409?409, Princeton, USA.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the 7th
Conference on Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004. Support
vector learning for semantic argument classification. Ma-
chine Learning, 60(1?3):11?39.
David Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Meeting of the Associ-
ation for Computational Linguistics, pages 189?196, Cam-
bridge, USA.
Patrick Ye and Timothy Baldwin. 2006a. Semantic role label-
ing of prepositional phrases. ACM Transactions on Asian
Language Information Processing (TALIP), 5(3):228?244.
Patrick Ye and Timothy Baldwin. 2006b. Verb sense dis-
ambiguation using selectional preferences extracted with a
state-of-the-art semantic role labeler. In Proceedings of the
Australasian Language Technology Workshop, pages 141?
148, Sydney, Australia.
244
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350?353,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-UMB: Combining unsupervised and supervised systems for all-words
WSD
David Martinez,Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,tim}@csse.unimelb.edu.au
Eneko Agirre, Oier Lopez de Lacalle
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
{e.agirre,jibloleo}@ehu.es
Abstract
This paper describes the joint submission
of two systems to the all-words WSD sub-
task of SemEval-2007 task 17. The main
goal of this work was to build a competitive
unsupervised system by combining hetero-
geneous algorithms. As a secondary goal,
we explored the integration of unsupervised
predictions into a supervised system by dif-
ferent means.
1 Introduction
This paper describes the joint submission of two sys-
tems to the all-words WSD subtask of SemEval-
2007 task 17. The systems were developed by the
University of the Basque Country (UBC), and the
University of Melbourne (UMB). The main goal of
this work was to build a competitive unsupervised
system by combining heterogeneous algorithms. As
a secondary goal, we explored the integration of
this method into a supervised system by different
means. Thus, this paper describes both the unsu-
pervised system (UBC-UMB-1), and the combined
supervised system (UBC-UMB-2) submitted to the
all-words task.
Our motivation in building unsupervised systems
comes from the difficulty of creating hand-tagged
data for all words and all languages, which is col-
loquially known as the knowledge acquisition bot-
tleneck. There have also been promising results in
recent work on the combination of unsupervised ap-
proaches that suggest the gap with respect to super-
vised systems is narrowing (Brody et al, 2006).
The remainder of the paper is organized as fol-
lows. First we describe the disambiguation algo-
rithms in Section 2. Next, the development exper-
iments are presented in Section 3, and our final sub-
missions and results in Section 4. Finally, we sum-
marize our conclusions in Section 5.
2 Algorithms
In this section, we will describe the standalone algo-
rithms (three unsupervised and one supervised) and
the combination schemes we explored. The unsu-
pervised methods are based on different intuitions
for disambiguation (topical features, local context,
and WordNet relations), which is a desirable charac-
teristic for combining algorithms.
2.1 Topic Signatures (TS)
Topic signatures (Agirre and de Lacalle, 2004) are
lists of words related to a particular sense. They can
be built from a variety of sources, and be used di-
rectly to perform WSD. Cuadros and Rigau (2006)
present a detailed evaluation of topic signatures built
from a variety of knowledge sources. In this work
we built those coming from the following:
? the relations in the Multilingual Central Repos-
itory (TS-MCR)
? the relations in the Extended WordNet (TS-
XWN)
In order to apply this resource for WSD, we sim-
ply measured the word-overlap between the target
context and each of the senses of the target word.
The sense with highest overlap is chosen as the cor-
rect sense.
350
2.2 Relatives in Context (RIC)
This is an unsupervised method presented in Mar-
tinez et al (2006). This algorithm makes use of
the WordNet relatives of the target word for disam-
biguation. The process is carried out in these steps:
(i) obtain a set of close relatives from WordNet for
each sense (the relatives can be polysemous); (ii) for
each test instance define all possible word sequences
that include the target word; (iii) for each word se-
quence, substitute the target word with each relative
and query a web search engine; (iv) rank queries ac-
cording to the following factors: length of the query,
distance of the relative to the target word, and num-
ber of hits; and (v) select the sense associated with
the highest ranked query.
The intuition behind this system is that we can
find related words that can be substituted for the tar-
get word in a given context, which are indicative of
its sense. The close relatives that can form more
common phrases from the target context determine
the target sense.
2.3 Relative Number (RNB)
This heuristic has been motivated as a way of identi-
fying rare senses of a word. An important disadvan-
tage of unsupervised systems is that rare senses can
be over-represented in the models, while supervised
systems are able to discard them because they have
access to token-level word sense distributions.
This simple algorithm relies on the number of
close relatives found in WordNet for each sense of
the word. The senses are ranked according to the
number of synonyms, direct hypernyms, and di-
rect hyponyms they have in WordNet. The highest
ranked sense is taken to be the most important for the
target word, and all occurrences of the target word
are tagged with that sense.
2.4 k-Nearest Neighbours (kNN)
As our supervised system, we relied on kNN. This is
a memory-based learning method where the neigh-
bours are the k most similar contexts, represented by
feature vectors (~ci) of the test vector (~f ). The sim-
ilarity among instances is measured by the cosine
of their vectors. The test instance is labeled with the
sense that obtains the maximum sum of the weighted
votes of the k most similar contexts. Each vote is
weighted depending on its (neighbour) position in
the ordered rank, with the closest being first. Equa-
tion 1 formalizes kNN, where Ci corresponds to the
sense label of the i-th closest neighbour.
arg max
Sj
=
k
?
i=1
{
1
i if Ci = Sj
0 otherwise (1)
The UBC group used a combination of kNN clas-
sifiers trained over a large set of features, and en-
hanced this method using Singular Value Decompo-
sition (SVD) for their supervised submission (UBC-
ALM) to the lexical-sample and all-words subtasks
(Agirre and Lopez de Lacalle, 2007). However, we
only used the basic implementation in this work, due
to time constraints.
2.5 Combination of systems
We explored two approaches to combine the stan-
dalone systems. The first consisted simply of adding
up the normalized weights that each system would
give to each sense. We tested this voting approach
both for the unsupervised and supervised settings.
The second method could only be applied in com-
bination with the supervised kNN system. The
idea was to include the unsupervised predictions as
weighted features for the supervised system. We re-
fer to this method as ?stacking?, and it has been pre-
viously used to integrate heterogeneous knowledge
sources for WSD (Stevenson and Wilks, 2001).
3 Development experiments
We tested the single algorithms and their combina-
tion over both Semcor and the training distribution
of the SemEval-2007 lexical-sample subtask of task
17 (S07LS for short). The goal of these experiments
was to obtain an estimate of the expected perfor-
mance, and submit the most promising configura-
tion. We present first the tests on the unsupervised
setting, and then the supervised setting. It is im-
portant to note that the hand-tagged corpora was not
used to fine-tune the parameters of the unsupervised
algorithms.
3.1 Unsupervised systems
For the first evaluation of our unsupervised systems,
we relied on Semcor, and tagged 43,063 instances
of the 329 word types occurring in SemEval-2007
351
System Recall
RNB 30.6
TS-MCR 57.5
TS-XWN 47.0
TS-MCR & TS-XWN 57.3
RBN & TS-MCR & TS-XWN 53.6
Table 1: Evaluation of standalone and combined
unsupervised systems over 43,063 instances from
Semcor
System Recall
TS-MCR 60.1
TS-XWN 54.3
TS-MCR & TS-XWN 61.1
TS-MCR & TS-XWN & RIC* 61.2
Table 2: Evaluation of standalone and combined
unsupervised systems over 8,518 instances from
S07LS training
all-words. Due to time constraints, we were not able
to test the RIC algorithm on this dataset. The re-
sults are shown in Table 1. We can see that the RNB
heuristic performs poorly, and that the best configu-
ration consists of applying the single TS-MCR algo-
rithm. From this experiment, we decided to remove
the RNB heuristic and focus on the topic signatures
and RIC.
We also used S07LS for extra experiments in
the unsupervised setting. From the training part of
the S07LS dataset, we extracted 8,518 instances of
words also occurring in SemEval-2007 all-words.
As S07LS used senses from OntoNotes, we relied
on the mapping provided by the task organisers to
link them to WordNet senses. We left RNB out of
this experiment due to its low performance in Sem-
cor, and regarding RIC, we only evaluated a sample
of 68 instances. Results are shown in Table 2. The
best scores are achieved when combining both sets
of topic signatures. The few cases that have been
disambiguated with RIC improve the overall perfor-
mance slightly.
3.2 Combined system
We could not rely on Semcor in the supervised set-
ting (we used it for training), and therefore tried to
use as much data as possible from the training com-
ponent of S07LS, wherein all the instances avail-
able (22,281) were disambiguated. We tested first
System Recall
kNN 87.4
kNN & TS-MCR 86.8
kNN & TS-XWN 86.4
kNN & TS-MCR & TS-XWN 86.0
Table 3: Evaluation of voting supervised systems in
22,281 instances from S07LS training
System Recall
kNN 71.7
kNN & TS-MCR & TS-XWN 71.8
Table 4: Evaluation of ?stacking? the unsupervised
systems on kNN over 8,518 instances from S07LS
training
the voting combination by adding the normalized
weights from the output of each system. Due to
time constraints we only evaluated the combination
of kNN with TS-MCR and TS-XWN. Results are
shown in Table 3, where we can see that combin-
ing the unsupervised systems with voting hurts the
performance of the kNN method.
Finally, we applied the second combination ap-
proach, consisting of including the predictions of the
unsupervised systems as features for kNN (?stack-
ing?). We performed this experiment on the training
part of S07LS, but only for the 8,518 instances of
the words occurring on the all-words dataset. The
results of this experiment are given in Table 4. We
observed a slight improvement in this case.
4 Final systems
For our final submissions, we chose the combination
?TS-MCR& TS-XWN&RIC? for the unsupervised
system (UBC-UMB-1), and the combination ?kNN
& TS-MCR & TS-XWN? via ?stacking? for our su-
pervised system (UBC-UMB-2). The results of all
the systems are given in Table 5.
We can see that our unsupervised system ranked
10th. Unfortunately, we do not know at the time of
writing which other systems are unsupervised, and
therefore are unable to compare to other unsuper-
vised systems.
Our ?stacking? supervised system performs
slightly lower than the kNN supervised systems by
UBC-ALM (which ranks 7th), showing that our sys-
tem was not able to profit from information from
352
System Precision Recall
1. 0.537 0.537
2. 0.527 0.527
3. 0.524 0.524
4. 0.522 0.486
5. 0.518 0.518
6. 0.514 0.514
7. 0.493 0.492
8. UBC-UMB-2 0.485 0.484
9. 0.420 0.420
10. UBC-UMB-1 0.362 0.362
11. 0.355 0.355
12. 0.337 0.337
13. 0.298 0.298
14. 0.120 0.118
Table 5: Official results for all systems in task #17
of SemEval-2007. Our systems are shown in bold.
UBC-UMB-1 stands for TS-MCR & TS-XWN &
RIC, and UBC-UMB-2 for kNN & TS-MCR & TS-
XWN.
System Precision Recall
TS-MCR 36.7 36.5
TS-XWN 33.1 32.9
RIC 30.6 30.4
TS-MCR & TS-XWN 37.5 37.3
TS-MCR & TS-XWN & RIC 36.2 36.2
Table 6: Our unsupervised systems in the SemEval-
2007 all words test data
the unsupervised systems. However, we cannot at-
tribute the decrease only to the unsupervised fea-
tures, as the kNN implementations were different
(UBC-ALM relied on SVD).
After the gold-standard data was released, we
were able to test the contribution of each of the un-
supervised systems in the ensemble, as well as two
additional combinations. The results are given in
Table 6. We can see that TS-MCR is the best per-
forming method, confirming our development ex-
periments (cf. Tables 1 and 2). In contrast, in-
cluding RIC decreased the performance by 0.7 per-
cent points, and had we used only TS-MCR and TS-
XWN our results would have been better.
5 Conclusions
In this submission we combined heterogeneous un-
supervised algorithms to obtain competitive perfor-
mance without relying on training data. However,
due to time constraints, we were only able to submit
a preliminary system, and some of the unsupervised
methods were not properly developed and tested.
For future work we plan to properly test these
methods, and deploy other unsupervised algorithms.
We also plan to explore more sophisticated combina-
tion strategies, using meta-learning to try to predict
which features of each word make a certain WSD
system succeed (or fail).
Acknowledgements
The first and second authors were supported by Aus-
tralian Research Council grant no. DP0663879. We
want to thank German Rigau from the University of
the Basque Country for kindly providing access to
the MCR.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4rd International
Conference on Language Resources and Evaluations
(LREC), pages 1123?6, Lisbon, Portugal.
Eneko Agirre and Oier Lopez de Lacalle. 2007. UBC-
ALM: Lexical-Sample and All-Words tasks. In
Proceedings of SemEval-2007 (forthcoming), Prague,
Czech Republic.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised WSD. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 97?104, Sydney, Australia.
Montse Cuadros and German Rigau. 2006. Quality as-
sessment of large scale knowledge resources. In Pro-
ceedings of the International Conference on Empirical
Methods in Natural Language Processing (EMNLP-
06), pages 534?41, Sydney, Australia.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42?50, Syd-
ney, Australia.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?49.
353
Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Impact of Deep Linguistic Processing on Parsing Technology
Timothy Baldwin
University of Melbourne
tim@csse.unimelb.edu.au
Mark Dras
Macquarie University
madras@ics.mq.edu.au
Julia Hockenmaier
University of Pennsylvania
juliahr@cis.upenn.edu
Tracy Holloway King
PARC
thking@parc.com
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
As the organizers of the ACL 2007 Deep
Linguistic Processing workshop (Baldwin et
al., 2007), we were asked to discuss our per-
spectives on the role of current trends in
deep linguistic processing for parsing tech-
nology. We are particularly interested in
the ways in which efficient, broad coverage
parsing systems for linguistically expressive
grammars can be built and integrated into
applications which require richer syntactic
structures than shallow approaches can pro-
vide. This often requires hybrid technolo-
gies which use shallow or statistical methods
for pre- or post-processing, to extend cover-
age, or to disambiguate the output.
1 Introduction
Our talk will provide a view on the relevance of deep
linguistic processing for parsing technologies from
the perspective of the organizers of the ACL 2007
Workshop on Deep Linguistic Processing (Baldwin
et al, 2007). The workshop was conceived with the
broader aim of bringing together the different com-
putational linguistic sub-communities which model
language predominantly by way of theoretical syn-
tax, either in the form of a particular theory (e.g.
CCG, HPSG, LFG, TAG, the Prague School) or a
more general framework which draws on theoretical
and descriptive linguistics. These ?deep linguistic
processing? approaches differ from shallower meth-
ods in that they yield richer, more expressive, struc-
tural representations which capture long-distance
dependencies or the underlying predicate-argument
structure directly.
Aspects of this research have often had their own
separate fora, such as the ACL 2005 workshop on
deep lexical acquisition (Baldwin et al, 2005), as
well as the TAG+ (Kallmeyer and Becker, 2006),
Alpino (van der Beek et al, 2005), ParGram (Butt
et al, 2002) and DELPH-IN (Oepen et al, 2002)
projects and meetings. However, the fundamental
approaches to building a linguistically-founded sys-
tem and many of the techniques used to engineer
efficient systems are common across these projects
and independent of the specific grammar formal-
ism chosen. As such, we felt the need for a com-
mon meeting in which experiences could be shared
among a wider community, similar to the role played
by recent meetings on grammar engineering (Wint-
ner, 2006; Bender and King, 2007).
2 The promise of deep parsing
Deep linguistic processing has traditionally been
concerned with grammar development (for use in
both parsing and generation). However, the linguis-
tic precision and complexity of the grammars meant
that they had to be manually developed and main-
tained, and were computationally expensive to run.
In recent years, machine learning approaches
have fundamentally altered the field of natural lan-
guage processing. The availability of large, manu-
ally annotated, treebanks (which typically take years
of prior linguistic groundwork to produce) enabled
the rapid creation of robust, wide-coverage parsers.
However, the standard evaluation metrics for which
such parsers have been optimized generally ignore
36
much of the rich linguistic information in the orig-
inal treebanks. It is therefore perhaps only natural
that deep processing methods, which often require
substantial amounts of manual labor, have received
considerably less attention during this period.
But even if further work is required for deep
processing techniques to fully mature, we believe
that applications that require natural language under-
standing or inference, among others, will ultimately
need detailed syntactic representations (capturing,
e.g., bounded and unbounded long-range dependen-
cies) from which semantic interpretations can eas-
ily be built. There is already some evidence that
our current deep techniques can, in some cases, out-
perform shallow approaches. There has been work
demonstrating this in question answering, targeted
information extraction and the recent textual entail-
ment recognition task, and perhaps most notably in
machine translation: in this latter field, after a period
of little use of linguistic knowledge, deeper tech-
niques are beginning to lead to better performance,
e.g. by redefining phrases by syntactic ?treelets?
rather than contiguous word sequences, or by explic-
itly including a syntactic component in the probabil-
ity model, or by syntactic preprocessing of the data.
3 Closing the divide
In the past few years, the divide between ?deep?,
rule-based, methods and ?shallow?, statistical, ap-
proaches, has begun to close from both sides. Re-
cent advances in using the same treebanks that have
advanced shallow techniques to extract more expres-
sive grammars or to train statistical disambiguators
for them, and in developing framework-specific tree-
banks, have made it possible to obtain similar cov-
erage, robustness, and disambiguation accuracy for
parsers that use richer structural representations. As
witnessed by many of the papers in our workshop
(Baldwin et al, 2007), a large proportion of current
deep systems have statistical components to them,
e.g., as pre- or post-processing to control ambigu-
ity, as means of acquiring and extending lexical re-
sources, or even use machine learning techniques
to acquire deep grammars automatically. From the
other side of the divide, many of the purely statistical
approaches are using progressively richer linguistic
features and are taking advantage of these more ex-
pressive features to tackle problems that were tradi-
tionally thought to require deep systems, such as the
recovery of traces or semantic roles.
4 The continued need for research on deep
processing
Although statistical techniques are becoming com-
monplace even for systems built around hand-
written grammars, there is still a need for further
linguistic research and manual grammar develop-
ment. For example, supervised machine-learning
approaches rely on large amounts of manually anno-
tated data. Where such data are available, develop-
ers of deep parsers and grammars can exploit them
to determine frequency of certain constructions, to
bootstrap gold standards for their systems, and to
provide training data for the statistical components
of their systems such as parse disambiguators. But
for the majority of the world?s languages, and even
for many languages with large numbers of speakers,
such corpora are unavailable. Under these circum-
stances, manual grammar development is unavoid-
able, and recent progress has allowed the underlying
systems to become increasingly better engineered,
allowing for more rapid development of any given
grammar, as well as for overlay grammars that adapt
to particular domains and applications and for port-
ing of grammars from one language to another.
Despite recent work on (mostly dependency
grammar-based) multilingual parsing, it is still the
case that most research on statistical parsing is done
on English, a fixed word-order language where sim-
ple context-free approximations are often sufficient.
It is unclear whether our current models and al-
gorithms carry over to morphologically richer lan-
guages with more flexible word order, and it is possi-
ble that the more complex structural representations
allowed by expressive formalisms will cease to re-
main a luxury.
Further research is required on all aspects of
deep linguistic processing, including novel linguis-
tic analyses and implementations for different lan-
guages, formal comparisons of different frame-
works, efficient parse and learning algorithms, better
statistical models, innovative uses of existing data
resources, and new evaluation tools and methodolo-
gies. We were fortunate to receive so many high-
37
quality submissions on all of these topics for our
workshop.
5 Conclusion and outlook
Deep linguistic processing brings together a range of
perspectives. It covers current approaches to gram-
mar development and issues of theoretical linguis-
tic and algorithmic properties, as well as the appli-
cation of deep linguistic techniques to large-scale
applications such as question answering and dialog
systems. Having industrial-scale, efficient parsers
and generators opens up new application domains
for natural language processing, as well as inter-
esting new ways in which to approach existing ap-
plications, e.g., by combining statistical and deep
processing techniques in a triage process to pro-
cess massive data quickly and accurately at a fine
level of detail. Notably, several of the papers ad-
dressed the relationship of deep linguistic process-
ing to topical statistical approaches, in particular in
the area of parsing. There is an increasing inter-
est in deep linguistic processing, an interest which
is buoyed by the realization that new, often hybrid,
techniques combined with highly engineered parsers
and generators and state-of-the-art machines opens
the way towards practical, real-world application of
this research. We look forward to further opportu-
nities for the different computational linguistic sub-
communities who took part in this workshop, and
others, to continue to come together in the future.
References
Timothy Baldwin, Anna Korhonen, and Aline Villavicen-
cio, editors. 2005. Proceedings of the ACL-SIGLEX
Workshop on Deep Lexical Acquisition. Ann Arbor,
USA.
Timothy Baldwin, Mark Dras, Julia Hockenmaier,
Tracy Holloway King, and Gertjan van Noord, editors.
2007. Proceedings of the ACL Workshop on Deep Lin-
guistic Processing, Prague, Czech Republic.
Emily Bender and Tracy Holloway King, editors. 2007.
Grammar Engineering Across Frameworks, Stanford
University. CSLI On-line Publications. to appear.
Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,
and Christian Rohrer. 2002. The parallel grammar
project. In COLING Workshop on Grammar Engi-
neering and Evaluation, Taipei, Taiwan.
Laura Kallmeyer and Tilman Becker, editors. 2006. Pro-
ceedings of the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms (TAG+),
Sydney, Australia.
Stephan Oepen, Dan Flickinger, J. Tsujii, and Hand
Uszkoreit, editors. 2002. Collaborative Language En-
gineering: A Case Study in Efficient Grammar-based
Processing. CSLI Publications.
Leonoor van der Beek, Gosse Bouma, Jan Daciuk, Tanja
Gaustad, Robert Malouf, Mark-Jan Nederhof, Gert-
jan van Noord, Robbert Prins, and Bego na Vil-
lada Moiro?n. 2005. Algorithms for linguistic pro-
cessing. NWO Pionier final report. Technical report,
University of Groningen.
Shuly Wintner. 2006. Large-scale grammar development
and grammar engineering. Research workshop of the
Israel Science Foundation.
38
Proceedings of the Workshop on BioNLP: Shared Task, pages 77?85,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Biomedical Event Annotation with CRFs and Precision Grammars
Andrew MacKinlay, David Martinez and Timothy Baldwin
NICTA Victoria Research Laboratories
University of Melbourne, VIC 3010, Australia
{amack,davidm,tim}@csse.unimelb.edu.au
Abstract
This work describes a system for the tasks
of identifying events in biomedical text and
marking those that are speculative or negated.
The architecture of the system relies on
both Machine Learning (ML) approaches and
hand-coded precision grammars. We submit-
ted the output of our approach to the event ex-
traction shared task at BioNLP 2009, where
our methods suffered from low recall, al-
though we were one of the few teams to pro-
vide answers for task 3.
1 Introduction
We present in this paper our techniques for the tasks
1 and 3 of the event extraction shared task at BioNLP
2009. We make use of both Machine Learning (ML)
approaches and hand-coded precision grammars in
an architecture that combines multiple dedicated
modules. In the third task on negation/speculation,
we extract extract rich linguistic features resulting
from our HPSG high-precision grammar to train an
ML classifier.
2 Methodology
2.1 Task 1: Shallow Features and CRFs
Our system consists of two main modules, the first
of which is devoted to the detection of event trigger
words, and the second to event?theme analysis.
2.1.1 Trigger-word detection
We developed two separate systems to perform
trigger word detection, and also a hybrid system
which combines their outputs. The first system is
a simple dictionary-based look-up tagger; the sec-
ond system learns a structured model from the train-
ing data using conditional random fields (CRFs).
For pre-processing, we relied on the domain-specific
token and sentence splitter from the JULIE Lab
(Tomanek et al, 2007) and the GENIA tagger for
lemmatisation, POS tagging, chunking, and protein
detection (Tsuruoka et al, 2005).
The look-up tagger operates by counting the oc-
currences in the training data of different event tags
for a given term. Over the development and test data,
each occurrence of a given term is assigned the event
class with the highest prior in the training data. We
experimented with a frequency cut-off that allows us
to explore the precision/recall trade-off.
Our second system relies on CRFs, as imple-
mented in the CRF++ toolkit (Lafferty et al, 2001).
CRFs provide a discriminative framework for build-
ing structured models to segment and label sequence
data. CRFs have the well-known advantage that they
both model sequential effects and support the use of
large numbers of features. In our experiments we
used the following feature types: word-forms, lem-
mas, POS, chunk tags, protein annotation, and gram-
matical dependencies. For dependency annotation,
we used the Bikel parser and GDep as provided by
the organisers. This information was provided as a
feature that expresses the grammatical function of
the token. We explored window sizes of?3 and?4.
Finally, we tested combining the outputs of the
look-up tagger and CRF, by selecting all trigger
words from both outputs.
77
2.1.2 Event-theme construction
We constructed the output for task 1 by differenti-
ating among three types of events, according to their
expected themes: basic events, binding events, and
regulation events. We applied a simple strategy, as-
signing the closest events or proteins within a given
sentence as themes.
For the basic events, we simply assigned the clos-
est protein, an approach that we found to perform
well over the training and development data. For
binding events, we estimated the maximum dis-
tance away from the event word(s) for themes, and
the maximum number of themes. For regulation
events, we had to choose between proteins or events
as themes, and the CAUSE field was also required.
Again, we relied on a maximum distance threshold,
and gave priority to events over proteins as themes.
We removed regulation events as theme candidates,
since our basic approach could not indicate the di-
rection of the regulation. We also tested predicting
the CAUSE by relying on the protein closest to the
regulation event.
2.2 Task 3: Deep Parsing and Maximum
Entropy classification
For task 3 we ran a syntactic parser over the abstracts
and used the outputs to construct feature vectors for
a machine learning algorithm. We built two classi-
fiers (possibly with overlapping sets of feature vec-
tors) for each training run: one to identify specula-
tion and one for negation. We deliberately built a
separate binary classifier for each task instead of a
single four-class classifier, since the problem natu-
rally decomposes this way. Speculation and nega-
tion are independent of one another (informally, not
statistically) and it enables us to focus on feature en-
gineering for each subtask.
2.2.1 Deep Parsing with the ERG
It seemed likely that syntactico-semantic analysis
would be useful for task 3. To identify negation or
speculation with relatively high precision, it is prob-
able that knowledge of the relationships of possibly
distant elements (such as the negation particle not)
to a particular target word would provide valuable
information for classification.
Further to this, it was our intention to evaluate
the utility of deep parsing in such an approach,
rather than a shallower annotation such as the out-
put of a dependency parser. With this in mind,
we selected the English Resource Grammar1 (ERG:
Copestake and Flickinger (2000)), an open-source,
broad-coverage high-precision grammar of English
in the HPSG framework.
While the ERG is relatively robust across dif-
ferent domains, it is a general-purpose resource,
and there are some aspects of the language used in
the biomedical abstracts that cause difficulties; un-
known word handling is especially important given
the nature of terms in the domain. Fortunately we
can make some optimisations to mitigate this. The
GENIA tagger mentioned in Section 2.1.1 provides
both POS and named entity annotations, which we
used to constrain the input to the ERG in two ways:
? Biological named entities identified by the GE-
NIA tagger are flagged as such, and the parser
does not attempt to decompose them.
? POS tags are appended to each input token to
constrain the token to an appropriate category
if it is absent from the ERG lexicon.
With these modifications to the parser, as well as
preprocessing to handle differences in the tokenisa-
tion expected by the ERG to the output of the tagger,
we were able to obtain a spanning parse for 72% of
the training sentences. This still leaves 28% of the
sentences inaccessible ? the need for a fallback strat-
egy is discussed further in Section 4.2.
2.2.2 Feature Extraction from RMRSs
Rather than outputting syntactic parse trees, the
ERG can also produce output in particular semantic
formalisms: Minimal Recursion Semantics (MRS:
Copestake et al (2005)) and the closely related Ro-
bust Minimal Recursion Semantics (RMRS: Copes-
take (2004)). For our feature generation here we
make use of the latter.
Figure 1 shows an example RMRS obtained from
one of the training documents. While there is in-
sufficient space to give a complete treatment here,
we highlight several aspects for expository purposes.
1Specifically the July 2008 version, downloadable from
http://lingo.stanford.edu/ftp/test/
78
l1,
{ l3: thus a 1?62:67?(e5, ARG1: h4),
l16: generic unk nom rel?68:78?(x11, CARG: ?nf- kappa b?),
l6: udef q rel?68:89?(x9, RSTR: h8, BODY: h7),
l10: compound rel?68:89?(e12, ARG1: x9, ARG2: x11),
l13: udef q rel?68:89?(x11, RSTR: h15, BODY: h14),
l101: activation n 1?79:89?(x9),
l17: neg rel?94:97?(e19, ARG1: h18),
l20: require v 1?98:106?(e2, ARG1: u21, ARG2: x9),
l102: parg d rel?98:106?(e22, ARG1: e2, ARG2: x9),
l103: for p?107:110?(e24, ARG1: e2, ARG2: x23),
l34: generic unk nom rel?111:129?(x29,
CARG: ?neuroblastoma cell?),
l25: udef q rel?111:146?(x23, RSTR: h27, BODY: h26),
l28: compound rel?111:146?(e30, ARG1: x23, ARG2: x29),
l31: udef q rel?111:146?(x29, RSTR: h33, BODY: h32),
l104: differentiation n of?130:146?(x23, ARG1: u35) },
{ h4 qeq l17, h8 qeq l10, h15 qeq l16, h18 qeq l20, h27 qeq l28,
h33 qeq l34 },
{ l10 in-g l101, l20 in-g l102, l20 in-g l103, l28 in-g l104 }
Figure 1: RMRS representation of the sentence Thus NF-
kappa B activation requires neuroblastoma cell differ-
entiation showing, in order, elementary predicates, qeq-
constraints, and in-g constraints
The primary component of an RMRS is bag of ele-
mentary predicates, or EPs. Each EP shown has: (a)
a label, such as ?l104?; (b) a predicate name, such as
? differentiation n 1? (where ?n? indicates the part-
of-speech); (c) character indices to the source sen-
tence; and (d) a set of arguments. The first argu-
ment is always ARG0 and is afforded special sta-
tus, generally referring to the variable introduced by
the predicate. Subsequent arguments are labelled ac-
cording to the relation of the argument to the pred-
icate. Arguments can be variables such as ?e30? or
?x23? (where the first letter indicates the nature of
the variable ? ?e? referring to events and ?x? to enti-
ties), or handles such as ?h33?.
These handles are generally used in the qeq con-
straints, which relate a handle to a label, indicating
a particular kind of outscoping relationship between
the handle and the label ? either that the handle and
label are equal or that the handle is equal to the label
except that one or more quantifiers occur between
the two (the name is derived from ?equality mod-
ule quantifiers?). Finally there are in-g constraints
which indicate that labels can be treated as equal.
For our purposes this simply affects which qeq con-
straints they participate in ? for example from the
in-g constraint ?l28 in-g l104? and the qeq constraint
?h27 qeq l28?, we can also infer that ?h27 qeq l104?.
In constructing features, we make use of:
? The outscopes relationship (specifically qeq-
outscopes) ? if EP A has a handle argument
which qeq-outscopes the label of EP B, A is
said to immediately outscope B ; outscopes is
the transitive closure of this.
? The shared-argument relationship, where EPs
C and D refer to the same variable in one
or more of their argument positions. We also
in some cases make further restrictions on the
types of arguments (ARG0 , RSTR , etc) that
may be shared on either end of the relationship.
2.2.3 Feature Sets and Classification
Feature vectors for a given event are constructed
on the basis of the trigger word for the particular
event, which we assume has already been identified;
a natural consequence is that all events with the same
trigger words have identical feature vectors. We use
the term trigger EPs to describe the EP(s) which cor-
respond to that trigger word ? i.e. those whose char-
acter span encompasses the trigger word. We have
a potentially large set of related EPs (with the kinds
of relationships described above), which we filter to
create the various feature sets, as outlined below.
We have several feature sets targeted at identify-
ing negation:
? NEGOUTSCOPE2: If any EPs in the RMRS
have predicate names in { no q, but+not c,
nor c, only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, and that
EP outscopes a trigger EP, set a general feature
as well as a specific one for the particle.
? NEGCONJINDEX: If any EPs in the RMRS
have predicate names in { not c, but+not c,
nor c}, the R-INDEX (RHS of a conjunction)
of that EP is the ARG0 a trigger EP, set a gen-
eral feature as well as a specific one for the par-
ticle ? capturing the notion that these conjunc-
tions are semantically negative for the particle
on the right. This also had a corresponding fea-
ture for the L-INDEX of nor c, corresponding
to the LHS of the neither...nor construction.
79
? ARG0NEGOUTSCOPEESA: For any EPs
which have an argument that matches the
ARG0 of a trigger EP, if they are outscoped
by an EP whose predicate name is in
the list { only a, never a, not+as+yet a,
not+as+yet a, unable a, neg rel}, set a gen-
eral feature to true, as well as features for the
name of the outscoping and outscoped EPs.
This is designed to catch trigger EP which are
nouns, where the verb of which they are subject
or object (or indeed an adjective/preposition to
which they are linked) is semantically negated.
And several targeted at identifying speculation:
? SPECVOBJ2: if a verb is a member of
the set { investigate, study, examine, test,
evaluate, observe} and its ARG2 (which cor-
responds to the verb object) is the ARG0 of a
trigger EP. This has a general feature for if any
of the verbs match, and a feature which is spe-
cific to each verb in the target list.
? SPECVOBJ2+WN: as above, but augment the
list of seed verbs with a list of WordNet sisters
(i.e. any lemmas from any synsets for the verb),
and add a feature which is set for the seed verbs
which gave rise to other sister verbs.
? MODALOUTSCOPE: modal verbs (can, should,
etc) may be strong indicators of specula-
tion; this sets a value when the trigger EP is
outscoped by any predicate corresponding to a
modal, both as a general feature and a specific
feature for the particular modal.
? ANALYSISSA: the ARG0 of the trigger EP is
also an argument of an EP with the predicate
name analysis n. Such constructions involv-
ing the word analysis are relatively frequent in
speculative events in the data.
And some general features, aiming to see if the
learning algorithm could pick up other patterns we
had missed:
? TRIGPREDPROPS: Set a feature value for the
predicate name of each trigger EP, as well as
the POS of each trigger EP.
? TRIGOUTSCOPES: Set a feature value for the
predicate name and POS of each EP that is
outscoped by the trigger EP.
? MODADJ: Set a feature value for any EPs
which have an ARG1 which matches the ARG0
of the trigger EP if their POS is marked as ad-
jective or adverb.
? +CONJ: This is actually a variant on the feature
extraction method, which attempts to abstract
away the effect of conjunctions. If the trigger
EP is a member of a conjunction (i.e. shares
an ARG0 with the L-INDEX or R-INDEX of a
conjunction), also treat the EPs which are con-
junction parents (and their conjunctive parents
if they exist) as trigger EPs in the feature con-
struction.
2.2.4 Implementation
To produce training data to feed into a classifier,
we parsed as many sentences as possible using the
ERG, and used the output RMRSs to create train-
ing data using various combinations of the feature
sets described above. The construction of features,
however, presupposes annotations for the events and
trigger words. For producing training data, we used
the provided trigger annotations. For the test phase,
we simply use the outputs of the classifier we built
in phase 1, selecting the combination with the best
performance over the development set. This pipeline
architecture places limits on annotation performance
? in particular, the recall in task 1 is an upper bound
on task 3 recall. We used a maximum entropy clas-
sification algorithm for the ML component here ? it
has a low level of parameterization and is a solid per-
former in NLP tasks. The implementation we used
was Zhang Le?s Maxent Toolkit.2
3 Development experiments
3.1 Task 1
We devised a set of experiments over the trial, train-
ing, and development data in order to estimate the
parameters for our final submission. Using the trial
data, we performed manual error analysis on the
rules used to construct events. With the training
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
80
data, we performed our own evaluation based on
cross-validation to detect trigger words and con-
struct events. For the experiments over the devel-
opment data, we relied on the evaluation interface
provided by the organisation. We focused on testing
the following modules: look-up tagger, CRF, com-
bined system, and event construction.
First, we tuned the parameters of our look-up tag-
ger over the training data. We used a threshold on
the minimum number of term occurrences required
to use the class information for that term from the
training data. We evaluated thresholding on raw fre-
quencies, and also on the percentage of occurrences
of the term that were linked to the majority event. In
cross-validation over the training data, we found that
the raw-frequency threshold worked best, achiev-
ing a maximum F-score of 38.86%, as compared to
30.81% for the percentage approach (the results are
shown in the bottom part of Table 1). We also es-
timated the frequency threshold as ? 25, and ob-
served that most of the terms identified consisted of
a single word, due to data sparseness in the training
set.
Our next experiments are devoted to the CRF sys-
tem, focusing on feature engineering. The results
over the training data for: (a) the full feature set, and
(b) removing one feature type at a time, are shown
in Table 1, for windows of size ?3 and ?4. We can
see that the best F-score is achieved by the?3 word-
window system when removing the syntactic depen-
dencies from Bikel?s parser. These results improved
over the look-up system.
As a final experiment on feature combinations
and window size, we used the development evalu-
ation interface. We submitted the best combinations
shown in the above experiment, and also syntactic
dependencies extracted with GDep. We observed
the same behaviour as in training data, with the ?3
word window obtaining the best F-score, and syn-
tactic dependencies harming performance. These re-
sults are shown in the upper part of Table 2. Our
final CRF system used this configuration (?3 word
window and all feature types except syntactic depen-
dencies).
Our next step was to test the integration of the
look-up tagger and CRF into a single system. We
observed that by combining the outputs directly we
W. size Feats. Rec. Prec. FSc.
?3 All 30.28 64.44 41.20
?3 ?synt. dep. 30.20 65.01 41.24
?3 ?protein NER 28.04 65.73 39.31
?3 ?chunking 30.13 65.16 41.20
?3 ?POS 29.68 65.25 40.80
?3 ?lemma 27.96 62.60 38.66
?3 ?word form 29.98 63.81 40.79
?4 All 28.86 66.15 40.19
?4 ?synt. dep. 29.75 67.06 41.22
?4 ?protein NER 28.11 66.73 39.56
?4 ?chunking 28.56 66.61 39.98
?4 ?POS 28.19 66.67 39.62
?4 ?lemma 26.55 65.20 37.73
?4 ?word form 28.19 65.28 39.38
Look-up (freq.) 52.14 30.97 38.86
Look-up (perc.) 38.20 25.82 30.81
Table 1: Trigger-word detection performance over train-
ing data. Results for the look-up tagger and CRFs with
the full feature set and when removing one feature type
at a time, for 3 and 4 word windows. The best results per
column are shown in bold.
W. size Feats. Rec. Prec. FSc.
?3 All - synt. 17.55 56.17 26.75
?4 All - synt. 17.38 56.75 26.62
?3 All (GDep) 15.48 58.69 24.50
Combined (All) 26.94 27.83 27.38
Combined (Best) 21.24 39.92 27.73
Table 2: Performance of selected feature and window-
size combinations over development data. Best results
per column are given in bold.
could improve over the recall of CRF, and achieve
higher F-score. This approach is referred to as
?Combined (All)? in Table 2. We also tested the
results when choosing either the look-up tagger or
CRF depending on their performance over each
event in the training data. The results of this sys-
tem (?Combined (Best)?) show a slight improve-
ment over the basic combination.
Finally, we analysed the results of the event con-
struction step. We used the gold-standard trigger an-
notation over the trial data and analysed the errors of
our rules. We found out that there were three main
types of error: (1) incorrect assignation of regulation
themes; (2) trigger words having multiple themes;
and (3) themes crossing sentence boundaries. We
plan to address these problems in future work. We
also observed that predicting CAUSE for the regula-
tory events caused the F-score to drop, resulting in
us removing this functionality from the system.
81
N1: NEGOUTSCOPE2+CONJ, NEGCONJINDEX
N2: N1, TRIGPREDPROPS
N3: N1, ARG0NEGOUTSCOPEESA
N4: N3, TRIGPREDPROPS, NEGVOUTSCOPE
N5: N3, NEGVOUTSCOPE
S1: SPECVOBJ2+WN+CONJ, ANALYSISSA
S2: S1, TRIGPREDPROPS
S3: S1, MODADJ, MODALOUTSCOPE
S4: S3, TRIGOUTSCOPES
S5: SPECVOBJ2+WN+CONJ, MODADJ,
MODALOUTSCOPE,TRIGOUTSCOPES
B+y?x: Context window of lemmatized tokens: x preceding and yfollowing.
Table 3: Task 3 feature sets
3.2 Task 3
We evaluated the classification performance of vari-
ous feature sets (including some not described here)
using 10-fold cross-validation over the training data
in the initial stages. We ran various combinations of
the most promising features over the development
data and evaluated their relative performance in an
attempt to avoid overfitting.
To evaluate the performance boost we got in task
3 relative to more naive methods, we also experi-
mented with feature sets based on a bag-of-words
approach with a sliding context window of lemma-
tised tokens on either side. We evaluated all com-
binations of preceding and following context win-
dow sizes from 0 to 3. There are features for tokens
that precede the trigger, follow the trigger, or lie any-
where within the context window, as well as for the
trigger itself. A ?token? here may also be a named
biological entity (protein etc) produced by GENIA
tagger in our preprocessing phase, which would not
be lemmatised. For comparability we only evaluate
these features for sentences which we were able to
parse. For the best performing baseline and RMRS-
based feature sets, we also tested them in combina-
tion to see whether the features produced were com-
plementary.
In Table 4 we present the results over the develop-
ment data, using the provided gold-standard annota-
tions of trigger words, as well as some selected re-
sults for our other task 1 outputs. The gold-standard
figures are unrealistically high compared to what
we would expect to achieve against the test data,
but they are indicative at least of what we could
achieve with a perfect event classifier. Similar to
Task 1 Mod Feats. Rec. Prec. FSc.
Gold Spec B+2?2 23.2 40.0 29.3
Gold Spec B+3?3 22.1 47.7 30.2
Gold Spec S2 15.8 83.3 26.5
Gold Spec S3 18.9 78.3 30.5
Gold Spec S3,B+2?2 21.1 58.8 31.0
Gold Spec S3,B+3?3 23.2 57.9 33.1
Comb(best) Spec S3 4.2 21.0 7.0
Gold Spec S4 17.9 94.4 30.1
Gold Spec S5 17.9 100.0 30.4
Gold Neg B+0?2 14.0 33.3 19.7
Gold Neg B+1?3 15.0 30.2 20.0
Gold Neg N2 19.6 61.8 29.8
Comb(best) Neg N2 0.9 7.7 1.7
Gold Neg N3 15.9 68.0 25.8
Gold Neg N4 19.6 67.7 30.4
Gold Neg N4,B+1?3 22.4 52.2 31.4
Gold Neg N4,B+0?2 24.3 68.4 35.9
Gold Neg N5 16.8 69.2 30.1
Table 4: Results (exact match) over development data
for task 3 using gold-standard event/trigger annotations
and selected other annotations for task 1. Feature sets
described in Table 3
task 1, our system shows reasonable precision but
suffers badly in recall. The substantially poorer per-
formance when using our own annotations for the in-
put events is discussed in more detail in Section 4.2
One area where we could improve is to go after
the 30% of sentences for which we do not have a
spanning parse and resultant RMRS. To reuse ex-
isting infrastructure, we could produce RMRS out-
put from an alternative processing component with
broader coverage but less precision. Several meth-
ods exist to do this ? e.g. producing RMRS out-
put from RASP (Briscoe et al, 2006) is described in
Frank (2004). However there is clearly room for im-
provement in the remaining 70% of sentences which
we can parse ? our results in Table 4 are still well
below the limit of roughly 70% recall.3
Additional lexical resources beyond WordNet,
particularly domain-specific ones, are likely to be
useful in boosting performance since they will help
maximally utilise the training data. Additionally,
we have not yet made use of other event annota-
tions apart from the trigger words ? features based
on characteristics such as the event class or proper-
ties of the event arguments could also be useful.
3We have not performed any analysis to verify whether the
number of events per sentence differs between parseable and
unparseable sentences.
82
System Rec. Prec. FSc.
Combined (Best) 17.44 39.99 24.29
Combined (All) 24.36 30.87 27.23
CRF 12.23 62.24 20.44
CRF (+ synt feats) 12.01 61.91 20.11
Look-Up 22.88 29.67 25.84
Look-Up (freq >= 20) 23.26 26.74 24.88
Look-Up (freq >= 30) 21.37 30.50 25.13
Table 5: Task 1 results with approximate span matching,
recursive evaluation (our final submission is in bold)
4 Results
4.1 Task 1
Our experiments on the training and development set
showed that our CRF++ was biased towards preci-
sion at the cost of recall, and for the look-up system
the best F-score was obtained when aiming for high
recall at the cost of lower precision. The best results
were obtained when combining both approaches,
and this was the composition of the system we sub-
mitted.
For our final submission, the CRF++ approach
had a ?3 word window, and all the features ex-
cept for syntactic dependencies, which were found
to harm performance. Our final look-up system re-
lied on raw frequencies to choose candidate terms,
and those above 24 occurrences in training data were
included in the dictionary. For the combination, we
observed that for most events the look-up system
performed better (although the overall F-score was
lower), and we decided to use the CRF++ output
only for the events that showed better performance
than the look-up system (TRANSCRIPTION, GENE
EXPRESSION, and POSITIVE REGULATION).
The results over the test data for our final submis-
sion and the main variants we explored are shown in
Table 5. We can see that the CRF performed poorly,
with very low recall over the test set, in contrast with
the development results, where the higher recall re-
sulted in a higher F-score than the look-up approach.
The best of our systems was the full combination of
CRF and the look-up tagger, with a 27.23% F-score.
The results for each event separately are given in
Table 6. The system performs much worse on regu-
lation events, due to the difficulty of having to cor-
Event Class Rec. Prec. FSc.
Localization 25.86 65.22 37.04
Binding 17.00 28.92 21.42
Gene-expression 45.71 69.18 55.05
Transcription 34.31 26.26 29.75
Protein-catabolism 42.86 85.71 57.14
Phosphorylation 45.19 64.21 53.04
EVT-TOTAL 35.84 53.15 42.81
Regulation 15.46 13.24 14.26
Positive-regulation 13.84 14.82 14.31
Negative-regulation 12.14 20.44 15.23
REG-TOTAL 13.73 15.31 14.48
ALL-TOTAL 24.36 30.87 27.23
Table 6: Results for the different events from our com-
bined system. Averaged scores for single events, regula-
tions, and all.
rectly identify other events in the near context.
4.2 Task 3
For testing, we repurposed all of the development
data as training data and retrained our classifiers.
The results in Table 7 were somewhat disappointing,
but a drop in recall versus the equivalent run over
the development data using oracle task 1 annotations
was unsurprising and the ratio of this drop is within
the bounds of what we would expect. The substan-
tial drop in precision can similarly be explained by
flow-on effects from our task 1 classification, a nat-
ural consequence of our pipeline architecture. It is
quite possible for our system to identify false pos-
itive events as being modified; in the online eval-
uation system, these classifications of non-existent
events reduce our precision in task 3.
In the feature engineering stage, we primarily
used the oracle data for task 1 to maximise the
amount of training data available. We felt that if we
were to use our task 1 classifications for events and
trigger words, the effectively lower number of train-
ing instances would only hurt performance. How-
ever this possibly led to bias towards features which
were more useful for classifying events that we
couldn?t successfully classify in task 1. The devel-
opment set shows similar performance drops under
these conditions in Table 4.
It is also possible that our features work reason-
ably but that our classification engine trained over
the oracle data simply learnt the wrong parameters
83
Task 1 Mod Fts. Rec. Prec. FSc.
Comb(Best) Spc B+3?3 2.88 12.24 4.67
Comb(Best) Spc S2 4.33 37.50 7.76
Comb(Best) Spc S3 4.81 30.30 8.30
Comb(All) Spc S3 5.29 26.19 8.80
Comb(Best) Spc S3,B+3?3 4.81 14.08 7.17
Comb(Best) Spc S4 3.85 27.59 6.75
Comb(Best) Spe S5 3.85 27.59 6.75
Comb(Best) Neg B+3?1 3.96 25.00 6.84Comb(Best) Neg N2 5.29 34.48 9.17
Comb(All) Neg N2 5.73 30.00 9.62
Comb(Best) Neg N3 5.29 27.78 8.88
Comb(Best) Neg N4 5.29 34.48 9.17
Comb(Best) Neg N4,B+0?2 4.85 28.12 8.27
Comb(All) Neg N4 5.73 27.27 9.47
Comb(Best) Neg N5 5.29 29.41 8.96
Table 7: Results over test data for task 3 using gold-
standard event annotations (approx recursive matching),
showing which set of trigger word classifications from
task 1 was used as input (submitted results in bold). Fea-
ture sets described in table 3
for the events we had identified correctly in task 1.
We could check this by training a classifier using
our task 1 event classifications combined with the
gold-standard trigger annotations. However com-
bining the gold-standard annotations for task 3 with
the classifier outputs of task 1 is non-trivial and was
not attempted due to time constraints. It also would
have been instructive to calculate a ceiling on our
task 3 performance given our performance in task 1
? i.e. how many modifications we could have cor-
rectly identified with a perfect task 3 classifier, but
we were not able to show this for similar reasons.
5 Conclusions
Our analysis of task 1 seemed to indicate that the
scarcity of training instances was the main reason
for the low recall of CRFs. The look-up system con-
tributed to increase the recall, but at the cost of lower
precision. In order to improve this module we plan
to find ways to extend the training data automatically
in a bootstrapping process.
Another limitation of our system is the event-
construction module, which follows simple rules
and performs poorly on regulation events. For this
subtask we plan to extend the rule set and apply op-
timisation techniques, following the lessons learned
in error analysis.
In task 3 we investigated the application of a pre-
cise, general-purpose grammar over this domain,
and were relatively successful. However, while the
parse coverage for task 3 is very respectable for a
precision grammar on comparatively difficult mate-
rial, it is clearly unwise to throw away 30% of sen-
tences, so a method to extract features from these is
desirable. Further sources of data would also be use-
ful, such as data from the event annotations them-
selves, and additional lexical resources tailored to
the biomedical domain.
We have also shown the syntactico-semantic out-
put of a deep parser, in the form of an RMRS, can
be beneficial in such a task compared with a more
naive approach based on bags of words within a
sliding context window. From Table 4, for nega-
tion, the syntactic features provided substantial per-
formance gains over the best set of baseline param-
eters we could find. For speculation the evidence
here is less compelling, with similar scores from
both approaches. Over test data in Table 7, the the
deep methods showed superior performance, albeit
over a smaller number of instances. Regardless, the
RMRS still has some advantages, giving (unsurpris-
ingly) higher precision than the baseline methods.
Combining naive and deep features does tend to give
slightly higher performance than either of the inputs
over the development data (although not over the test
data, perhaps due to the poorer performance of naive
methods), suggesting that the two approaches iden-
tify slightly different kinds of modification.
Our system suffered from the pipeline approach ?
there was no way to recover from an incorrect classi-
fication in task 1, resulting in greatly reduced preci-
sion and recall in task 3. It is possible that a carefully
constructed integrated system could annotate events
for trigger words and argument at the same time as
modification, with features shared between the two,
which may avoid some of these issues.
Acknowledgements
We wish to thank Rebecca Dridan, Dan Flickinger
and Lawrence Cavedon for their advice. NICTA
is funded by the Australian Government as repre-
sented by the Department of Broadband, Communi-
cations and the Digital Economy and the Australian
Research Council through the ICT Centre of Excel-
lence program.
84
References
Edward Briscoe, John Carroll, and Rebecca Watson.
2006. The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Poster
System, pages 77?80, Sydney, Australia.
Ann Copestake and Dan Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Interna-
tional Conference on Language Resources and Evalu-
ation.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
pages 281?332.
Ann Copestake. 2004. Report on the design of RMRS.
Technical Report D1.1a, University of Cambridge,
Cambridge, UK.
Anette Frank. 2004. Constraint-based RMRS construc-
tion from shallow grammars. In COLING ?04: Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, page 1269, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning, pages 282?289.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49?57, Melbourne, Australia.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics - 10th Panhellenic Conference on Infor-
matics, pages 382?392.
85
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 572?580,
Beijing, August 2010
Evaluating N-gram based Evaluation Metrics for
Automatic Keyphrase Extraction
Su Nam Kim, Timothy Baldwin
CSSE
University of Melbourne
sunamkim@gmail.com, tb@ldwin.net
Min-Yen Kan
School of Computing
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
This paper describes a feasibility study
of n-gram-based evaluation metrics for
automatic keyphrase extraction. To ac-
count for near-misses currently ignored
by standard evaluation metrics, we adapt
various evaluation metrics developed for
machine translation and summarization,
and also the R-precision evaluation metric
from keyphrase evaluation. In evaluation,
the R-precision metric is found to achieve
the highest correlation with human anno-
tations. We also provide evidence that
the degree of semantic similarity varies
with the location of the partially-matching
component words.
1 Introduction
Keyphrases are noun phrases (NPs) that are repre-
sentative of the main content of documents. Since
they represent the key topics in documents, ex-
tracting good keyphrases benefits various natu-
ral language processing (NLP) applications such
as summarization, information retrieval (IR) and
question-answering (QA). Keyphrases can also be
used in text summarization as semantic metadata
(Barzilay and Elhadad, 1997; Lawrie et al, 2001;
D?Avanzo and Magnini, 2005). In search engines,
keyphrases supplement full-text indexing and as-
sist users in creating good queries.
In the past, a large body of work on keyphrases
has been carried out as an extraction task, uti-
lizing three types of cohesion: (1) document
cohesion, i.e. cohesion between documents and
keyphrases (Frank et al, 1999; Witten et al, 1999;
Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007; Wan and
Xiao, 2008); (2) keyphrase cohesion, i.e. cohe-
sion among keyphrases (Turney, 2003); and (3)
term cohesion, i.e. cohesion among terms in a
keyphrase (Park et al, 2004).
Despite recent successes in keyphrase extrac-
tion (Frank et al, 1999; Turney, 2003; Park et al,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007), current work is hampered by the in-
flexibility of standard metrics in evaluating differ-
ent approaches. As seen in other fields, e.g. ma-
chine translation (MT) and multi-document sum-
marization, the advent of standardized automatic
evaluation metrics, combined with standardized
datasets, has enabled easy comparison of sys-
tems and catalyzed the respective research ar-
eas. Traditionally, the evaluation of automatic
keyphrase extraction has relied on the number
of exact matches in author-assigned keyphrases
and reader-assigned keyphrases. The main prob-
lem with this approach is that even small vari-
ants in the keyphrases are not given any credit.
For example, given the gold-standard keyphrase
effective grid computing algorithm, grid com-
puting algorithm is a plausible keyphrase candi-
date and should be scored appropriately, rather
than being naively evaluated as wrong. Addition-
ally, author-assigned keyphrases and even reader-
assigned keyphrases often have their own prob-
lems in this type of evaluation (Medelyan andWit-
ten, 2006). For example, some keyphrases are
often partly or wholly subsumed by other can-
didates or may not even occur in the document.
Therefore, counting the exactly-matching candi-
dates has been shown to be suboptimal (Jarmasz
572
and Barriere, 2004).
Our goal in this paper is to evaluate the relia-
bility of automatic evaluation metrics that better
account for near-misses. Prior research based on
semantic similarity (Jarmasz and Barriere, 2004;
Mihalcea and Tarau, 2004; Medelyan and Wit-
ten, 2006) has taken the approach of using ex-
ternal resources such as large corpora, Wikipedia
or manually-curated index words. While we ac-
knowledge that these methods can help address
the near-miss problem, they are impractical due
to the effort required to compile the requisite re-
sources for each individual evaluation exercise,
and furthermore, the resources tend to be domain-
specific. In order to design a cheap, practical and
stable keyphrase evaluation metric, our aim is to
properly account for these near-misses without re-
liance on costly external resources.
According to our analysis, the degree of se-
mantic similarity of keyphrase candidates varies
relative to the location of overlap. For exam-
ple, the candidate grid computing algorithm has
higher semantic similarity than computing algo-
rithm with the gold-standard keyphrase effective
grid computing algorithm. Also, computing algo-
rithm is closer than effective grid to the same gold-
standard keyphrase. From these observations, we
infer that n-gram-based evaluation metrics can
be applied to evaluating keyphrase extraction, but
also that candidates with the same relative n-gram
overlap are not necessarily equally good.
Our primary goal is to test the utility of n-gram
based evaluation metrics to the task of keyphrase
extraction evaluation. We test the following eval-
uation metrics: (1) evaluation metrics from MT
and multi-document summarization (BLEU, NIST,
METEOR and ROUGE); and (2) R-precision (Zesch
and Gurevych, 2009), an n-gram-based evalua-
tion metric developed specifically for keyphrase
extraction evaluation which has yet to be evalu-
ated against humans at the extraction task. Sec-
ondarily, we attempt to shed light on the bigger
question of whether it is feasible to expect that
n-gram-based metrics without access to external
resources should be able to capture subtle seman-
tic differences in keyphrase candidates. To this
end, we experimentally verify the impact of lex-
ical overlap of different types on keyphrase sim-
ilarity, and use this as the basis for proposing a
variant of R-precision.
In the next section, we present a brief primer on
keyphrases. We then describe the MT and sum-
marization evaluation metrics trialled in this re-
search, along with R-precision, modified R-precision
and a semantic similarity-based evaluation metric
for keyphrase evaluation (Section 3). In Section 4,
we discuss our gold-standard and candidate ex-
traction method. We compare the evaluation met-
rics with human assigned scores for suitability in
Section 5, before concluding the paper.
2 A Primer on Keyphrases
Keyphrases can be either simplex words (e.g.
query, discovery, or context-awareness)1 or larger
N-bars/noun phrases (e.g. intrusion detection,
mobile ad-hoc network, or quality of service).
The majority of keyphrases are 1?4 words long
(Paukkeri et al, 2008).
Keyphrases are normally composed of nouns
and adjectives, but may occasionally contain ad-
verbs (e.g. dynamically allocated task, or partially
observable Markov decision process) or other
parts of speech. They may also contain hyphens
(e.g. sensor-grouping or multi-agent system) and
apostrophes for possessives (e.g. Bayes? theorem
or agent?s goal).
Keyphrases can optionally incorporate PPs (e.g.
service quality vs. quality of service). A variety of
prepositions can be used (e.g. incentive for coop-
eration, inequality in welfare, agent security via
approximate policy), although the genetive of is
the most common.
Keyphrases can also be coordinated, either as
simple nouns at the top level (e.g. performance
and scalability or group and partition) or within
more complex NPs or between N-bars (e.g. his-
tory of past encounter and transitivity or task and
resource allocation in agent system).
When candidate phrases get too long, abbre-
viations also help to form valid keyphrases (e.g.
computer support collaborative work vs. CSCW,
or partially observable Markov decision process
vs. POMDP).
1All examples in this section are taken from the data set
outlined in Section 4.
573
3 Evaluation Metrics
There have been various evaluation metrics de-
veloped and validated for reliability in fields such
as MT and summarization (Callison-Burch et al,
2009). While n-gram-based metrics don?t cap-
ture systematic alternations in keyphrases, they do
support partial match between keyphrase candi-
dates and the reference keyphrases.
In this section, we first introduce a range of
popular n-gram-based evaluation metrics from
the MT and automatic summarization literature,
which we naively apply to the task of keyphrase
evaluation. We then present R-precision, an n-
gram-based evaluation metric developed specif-
ically for keyphrase evaluation, and propose a
modified version of R-precision which weights n-
grams according to their relative position in the
keyphrase. Finally, we present a semantic similar-
ity method.
3.1 Machine Translation and Summarization
Evaluation Metrics
In this research, we experiment with four popu-
lar n-gram-based metrics from the MT and au-
tomatic summarization fields ? BLEU, METEOR,
NIST and ROUGE. The basic task performed by the
respective evaluation metrics is empirical determi-
nation of how good an approximation is string1 of
string2?, which is not far removed from the re-
quirements of keyphrase evaluation. We briefly
outline each of the methods below.
One subtle property of keyphrase evaluation is
that there is no a priori preference for shorter
keyphrases over longer keyphrases, unlike MT
where shorter strings tend to be preferred. Hence,
we use the longer NP as reference and the shorter
NP as a translation, to avoid the length penalty in
most MT metrics.2
BLEU (Papineni et al, 2002) is an evaluation
metric for measuring the relative similarity be-
tween a candidate translation and a set of ref-
erence translations, based on n-gram composi-
tion. It calculates the number of overlapping n-
grams between the candidate translation and the
2While we don?t present the numbers in this paper, the
results were lower for the MT evaluation metrics without this
reordering of the reference and candidate keyphrases.
set of reference translations. In order to avoid hav-
ing very short translations receive artificially high
scores, BLEU adds a brevity penalty to the scoring
equation.
METEOR (Agarwal and Lavie, 2008) is similar
to BLEU, in that it measures string-level similarity
between the reference and candidate translations.
The difference is that it allows for more match
flexibility, including stem variation and WordNet
synonymy. The basic metric is based on the num-
ber of mapped unigrams found between the two
strings, the total number of unigrams in the trans-
lation, and the total number of unigrams in the ref-
erence.
NIST (Martin and Przybocki, 1999) is once
again similar to BLEU, but integrates a propor-
tional difference in the co-occurrences for all n-
grams while weighting more heavily n-grams that
occur less frequently, according to their informa-
tion value.
ROUGE (Lin and Hovy, 2003) ? and its vari-
ants including ROUGE-N and ROUGE-L ? is simi-
larly based on n-gram overlap between the can-
didate and reference summaries. For example,
ROUGE-N is based on co-occurrence statistics,
using higher-order n-grams (n > 1) to esti-
mate the fluency of summaries. ROUGE-L uses
longest common subsequence (LCS)-based statis-
tics, based on the assumption that the longer the
substring overlap between the two strings, the
greater the similar Saggion et al (2002). ROUGE-
W is a weighted LCS-based statistic that priori-
tizes consecutive LCSes. In this research, we ex-
periment exclusively with the basic ROUGE met-
ric, and unigrams (i.e. ROUGE-1).
3.2 R-precision
In order to analyze near-misses in keyphrase ex-
traction evaluation, Zesch and Gurevych (2009)
proposed R-precision, an n-gram-based evalua-
tion metric for keyphrase evaluation.3 R-precision
contrasts with the majority of previous work on
keyphrase extraction evaluation, which has used
semantic similarity based on external resources
3Zesch and Gurevych?s R-precision has nothing to do with
the information retrieval evaluation metric of the same name,
where P@N is calculated forN equal to the number of rele-
vant documents.
574
(Jarmasz and Barriere, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006). As our inter-
est is in fully automated evaluation metrics which
don?t require external resources and are domain
independent (for maximal reproducibility of re-
sults), we experiment only with R-precision in this
paper.
R-precision is based on the number of overlap-
ping words between a keyphrase and a candi-
date, as well as the length of each. The met-
ric differentiates three types of near-misses: IN-
CLUDE, PARTOF and MORPH. The first two
types are based on an n-gram approach, while
the third relies on lexical variation. As we use
stemming, in line with the majority of previous
work on keyphrase extraction evaluation, we fo-
cus exclusively on the first two cases, namely IN-
CLUDE, and PARTOF. The final score returned
by R-precision is:
number of overlapping word(s)
length of keyphrase/candidate
where the denominator is the longer of the
keyphrase and candidate.
Zesch and Gurevych (2009) evaluated R-
precision over three corpora (Inspec, DUC and SP)
based on 566 non-exact matching candidates. In
order to evaluate the human agreement, they hired
4 human annotators to rate the near-miss candi-
dates, and reported agreements of 80% and 44%
for the INCLUDE and PARTOF types, respec-
tively. They did not, however, perform holistic
evaluation with human scores to verify its relia-
bility in full system evaluation. This is one of our
contributions in this paper.
3.3 Modified R-precision
In this section, we describe a modification to
R-precision which assigns different weights for
component words based on their position in the
keyphrase (unlike R-precision which assigns the
same score for each matching component word).
The head noun generally encodes the core seman-
tics of the keyphrase, and as a very rough heuris-
tic, the further a word is from the head noun,
the less semantic import on the keyphrase it has.
As such, modified R-precision assigns a score to
each component word relative to its position as
CW = 1N?i+1 where N is the number of com-ponent words in the keyphrase and i is the posi-
tion of the component word in the keyphrase (1 =
leftmost word).
For example, AB and BC from ABC would be
scored as 13+ 121
3+
1
2+
1
1
= 511 and
1
2+
1
1
1
3+
1
2+
1
1
= 911 , re-
spectively. Thus, with the keyphrase effective
grid computing algorithm and candidates effec-
tive grid, grid computing and computing algo-
rithm, modified R-precision assigns different scores
for each candidate (computing algorithm > grid
computing > effective grid). In contrast, the orig-
inal R-precision assigns the same score to all can-
didates.
3.4 Semantic Similarity
In Jarmasz and Barriere (2004) and Mihalcea and
Tarau (2004), the authors used a large data set
to compute the semantic similarity of two NPs
to assign partial credits for semantically similar
candidate keyphrases. To simulate these meth-
ods, we adopted the distributional semantic simi-
larity using web documents. That is, we computed
the similarity between a keyphrase and its sub-
string by cosine measure over collected the snip-
pets from Yahoo! BOSS.4 We use the computed
similarity as our score for near-misses.
4 Data
4.1 Data Collection
We constructed a keyphrase extraction dataset us-
ing papers across 4 different categories5 of the
ACM Digital Library.6 In addition to author-
assigned keyphrases provided as part of the ACM
Digital Library, we generated reader-assigned
keyphrases by assigning 250 students 5 papers
each, a list of candidate keyphrases (see below for
details), and standardized instructions on how to
assign keyphrases. It took them an average of 15
minutes to annotate each paper. This is the same
4http://developer.yahoo.com/search/
boss/
5C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial Intelligence ?
Multiagent Systems) and J4 (Social and Behavioral Sciences
? Economics).
6http://portal.acm.org/dl.cfm
575
Author Reader Total
Total 1298/1305 3110/3221 3816/3962
NPs 937 2537 3027
Average 3.85/4.01 12.44/12.88 15.26/15.85
Found 769 2509 2864
Table 1: Details of the keyphrase dataset
(Rule1) NBAR = (NN*|JJ*)?(NN*)
e.g. complexity, effective algorithm,
distributed web-service discovery architecture
(Rule2) NBAR IN NBAR
e.g. quality of service, sensitivity of VOIP traffic,
simplified instantiation of zebroid
Table 2: Regular expressions for candidate selec-
tion
document collection and set of keyphrase annota-
tions as was used in the SemEval 2010 keyphrase
extraction task (Kim et al, 2010).
Table 1 shows the details of the final dataset.
The numbers after the slashes indicate the number
of keyphrases after including alternate keyphrases
based on of -PPs. Despite the reliability of author-
assigned keyphrases discussed in Medelyan and
Witten (2006), many author-assigned keyphrases
and some reader-assigned keyphrases are not
found verbatim in the source documents because:
(1) many of them are substrings of the candidates
or vice versa (about 75% of the total keyphrases
are found in the documents); and (2) our candi-
date selection method does not extract keyphrases
in forms such as coordinated NPs or adverbial
phrases.
4.2 Candidate Selection
During preprocessing, we first converted the
PDF versions of the papers into text using
pdftotext. We then lemmatized and POS
tagged all words using morpha and the Lingua
POS tagger. Next, we applied the regular expres-
sions in Table 2 to extract candidates, based on
Nguyen and Kan (2007). Finally, we selected can-
didates in terms of their frequency: simplex words
with frequency ? 2 and NPs with frequency ? 1.
We observed that for reader-assigned keyphrases,
NPs were often selected regardless of their fre-
quency in the source document. In addition, we
allowed variation in the possessive form, noun
number and abbreviations.
Rule1 detects simplex nouns or N-bars as candi-
dates. Rule2 extracts N-bars with post-modifying
PPs. In Nguyen and Kan (2007), Rule2 was not
used to additionally extract N-bars inside modify-
ing PPs. For example, our rules extract not only
performance of grid computing as a candidate, but
also grid computing. However, we did not extend
the candidate selection rules to cover NPs includ-
ing adverbs (e.g. partially-observable Markov de-
cision process) or conjunctions (e.g. behavioral
evolution and extrapolation), as they are rare.
4.3 Human Assigned Score
We hired four graduate students working in NLP
to assign human scores to substrings in the gold-
standard data. The scores are between 0 and 4
(0 means no semantic overlap between a NP and
its substring, while 4 means semantically indistin-
guishable).
We broke down the candidate?keyphrases pairs
into subtypes, based on where the overlap oc-
curs relative to the keyphrase (e.g. ABCD): (1)
Head: the candidate contains the head noun of
the keyphrase (e.g. CD); (2) First: the candi-
date contains the first word of the keyphrase (e.g.
AB); and (3) Middle: the candidate overlaps with
the keyphrase, but contains neither its first word
nor its head word (e.g. BC). The average human
scores are 1.94 and 2.11 for First and Head, re-
spectively, when the candidate is shorter, while
they are 2.00, 1.89 and 2.15 for First, Middle, and
Head, respectively when the candidate is longer.
Note that we did not have Middle instances with
candidates as the shorter string. The scores are
slightly higher for the keyphrases as substrings
than for the candidates as substrings.
5 Correlation
To check the feasibility of metrics for keyphrase
evaluation, we checked the Spearman rank corre-
lation between the machine-generated score and
the human-assigned score for each keyphrase?
candidate pairing.
As the percentage of annotators who agree on
the exact score is low (i.e. 2 subjects agree ex-
576
Human R-precision BLEU METEOR NIST ROUGE SemanticOrig Mod Similarity
Average
All .4506 .4763 .2840 .3250 .3246 .3366 .3246 .2116
L ? 4 .4510 .5264 .2806 .3242 .3238 .3369 .3240 .2050
L ? 3 .4551 .4834 .2893 .3439 .3437 .3584 .3437 .1980
Majority
All .4603 .4763 .3438 .3407 .3403 .3514 .3404 .2224
L ? 4 .4604 .5264 .3434 .3423 .3421 .3547 .3422 .2168
L ? 3 .4638 .4838 .3547 .3679 .3675 .3820 .3676 .2123
Table 3: Rank correlation between humans and the different evaluation metrics, based on the human
average (top half) and majority (bottom half)
Human R-precision BLEU METEOR NIST ROUGEOrig Mod
LOCATION
First .5508 .5032 .5033 .3844 .3844 .4057 .3844
Middle .5329 .5741 .5988 .4669 .4669 .4055 .4669
Head .3783 .4838 .4838 .3865 .3860 .3780 .3864
COMPLEXITY
Simple .4452 .4715 .2790 .3653 .3445 .3527 .3445
PP .4771 .4814 .1484 .3367 .3122 .3443 .3123
CC .3645 .3810 .3140 .3748 .3446 .3384 .3748
POS AdjN .4616 .4844 .3507 .3147 .3132 .3115 .3133NN .4467 .4586 .2581 .3321 .3321 .3488 .3322
Table 4: Rank correlation between human average judgments and n-gram-based metrics
actly on 55%-70% of instances, 3 subjects agree
exactly on 25%-35% of instances), we require a
method for combining the annotations. We ex-
periment with two combination methods: major-
ity and average. The majority is simply the label
with the majority of annotations associated with
it; in the case of a tie, we break the tie by select-
ing that annotation which is closest to the median.
The average is simply the average score across all
annotators.
5.1 Overall Correlation with Human Scores
Table 3 presents the correlations between the hu-
man scores (acting as an upper bound for the
task), as well as those between human scores
with machine-generated scores. We first present
the overall results, then results over the subset of
keyphrases of length 4 words or less, and also 3
words or less. We present the results for the anno-
tator average and majority in top and bottom half,
respectively, of the table.
To compute the correlation between the hu-
man annotators, we used leave-one-out cross-
validation, holding out one annotator, and com-
paring them to the combination of the remaining
annotators (using either the majority or average
method to combine the remaining annotations).
This was repeated across all annotators, and the
Spearman?s ? was averaged across the annotators.
Overall, we found that R-precision achieved the
highest correlation with humans, above the inter-
annotator correlation in all instances. That is,
based on the evaluation methodology employed,
it is performing slightly above the average level
of a single annotator. The relatively low inter-
annotator correlation is, no doubt, due to the dif-
ficulty of the task, as all of our near-misses have
2 or more terms, and the annotators have to make
very fine-grained, and ultimately subjective, deci-
sions about the true quality of the candidate.
Comparing the n-gram-based methods with the
semantic similarity-based method, the n-gram-
based metrics achieved higher correlations across
the board, with BLEU, METEOR, NIST and ROUGE
all performing remarkably consistently, but well
577
Human R-precision BLEU METEOR NIST ROUGEOrig Mod
LOCATION
First .5642 .5162 .5163 .4032 .4032 .4297 .4032
Middle .5510 .4991 .5320 .4175 .4175 .3653 .4175
Head .4147 .5073 .5074 .4156 .4153 .4042 .4156
COMPLEXITY
Simple .4580 .4869 .3394 .3653 .3651 .3715 .3651
PP .4715 .5068 .3724 .3367 .3367 .3652 .3367
CC .5777 .5513 .3841 .5745 .5571 .5600 .5745
POS AdjN .4501 .4861 .3968 .3266 .3251 .3246 .3252NN .4631 .4733 .3244 .3499 .3499 .3648 .3500
Table 5: Rank correlation between human majority and n-gram-based metrics
below the level of R-precision. Due to the markedly
lower performance of the semantic similarity-
based method, we do not consider it for the re-
mainder of our experiments. A general finding
was that as the length of the keyphrase (L) got
longer, the correlation tended to be higher across
all n-gram-based metrics.
One disappointment at this stage is that the re-
sults for modified R-precision are well below those
of the original, especially over the average of the
human annotators.
5.2 Correlation with Different NP Subtypes
To get a clearer sense of how the different eval-
uation metrics are performing, we broke down
the keyphrases according to three syntactic sub-
classifications: (1) the location of overlap (see
Section 4.3); (2) the complexity of the NP (does
the keyphrase contain a preposition [PP], a con-
junction [CC] or neither a preposition nor a con-
junction [Simple]?); and (3) the word class se-
quence of the keyphrase (is the keyphrase an NN
[NN] or an AdjN sequence [AdjN]?). We present
the results in Tables 4 and Table 4 for the human
average and majority, respectively, presenting re-
sults in boldface when the correlation for a given
method is higher than for that same method in
our holistic evaluation in Table 3 (i.e. .4506 and
.4603, for the average and majority human scores,
respectively).
All methods, including inter-annotator correla-
tion, improve in raw numbers over the subsets
of the data based on overlap location, indicating
that the data was partitioned into more internally-
consistent subsets. Encouragingly, modified R-
precision equalled or bettered the performance of
the original R-precision over each subset of the
data based on overlap location. Where modified
R-precision appears to fall down most noticeably
is over keyphrases including prepositions, as our
assumption about the semantic import based on
linear ordering clearly breaks down in the face of
post-modifying PPs. It is also telling that it does
worse over noun?noun sequences than adjective?
noun sequences. In being agnostic to the effects
of syntax, the original R-precision appears to bene-
fit overall. Another interesting effect is that the
performance of BLEU, METEOR and ROUGE is
notably better over candidates which match with
non-initial and non-final words in the keyphrase.
We conclude from this analysis that keyphrase
scoring should be sensitive to overlap location.
Furthermore, our study also shows that n-gram-
based MT and summarization metrics are sur-
prisingly adept at capturing partial matches in
keyphrases, despite them being much shorter than
the strings they are standardly applied to. More
compellingly, we found that R-precision is the best
overall performer, and that it matches the perfor-
mance of our human annotators across the board.
This is the first research to establish this fact. Our
findings for modified R-precision were more sober-
ing, but its location sensitivity was shown to im-
prove over R-precision for instances of overlap in
the middle or with the head of the keyphrase.
578
6 Conclusion
In this work, we have shown that preexisting n-
gram-based evaluation metrics from MT, summa-
rization and keyphrase extraction evaluation are
able to handle the effects of near-misses, and that
R-precision performs at or above the average level
of a human annotator. We have also shown that
a semantic similarity-based method which uses
web data to model distributional similarity per-
formed below the level of all of the n-gram-based
methods, despite them requiring no external re-
sources (web or otherwise). We proposed a mod-
ification to R-precision based on the location of
match, but found that while it could achieve better
performance over certain classes of keyphrases,
its net effect was to drag the performance of R-
precision down. Other methods were found to be
remarkably consistent across different subtypes of
keyphrase.
Acknowledgements
Many thanks to the anonymous reviewers for their
insightful comments. We wish to acknowledge
the generous funding from National Research
Foundation grant R 252-000-279-325 in support-
ing Min-Yen Kan?s work.
References
Abhaya Agrwal and Alon Lavie. METEOR, M-
BLEU and M-TER: Evaluation Metrics for High-
Correlation with Human Rankings of Machine
Translation Output. In Proceedings of ACL Work-
shop on Statistical Machine Translation. 2008.
Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases. In
Proceedings of BCCSCSI : Advances in Artificial
Intelligence. 2000, pp.96?103.
Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10?17.
Chris Callison-Burch, Philipp Koehn, Christof Monz
and Josh Schroeder. Proceedings of 4th Workshop
on Statistical Machine Translation. 2009.
Ernesto D?Avanzo and Bernado Magnini. A Key-
phrase-Based Approach to Summarization: the
LAKE System at DUC-2005. In Proceedings of
DUC. 2005.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin and Craig G. Nevill-Manning. Domain
Specific Keyphrase Extraction. In Proceedings of
IJCAI. 1999, pp.668?673.
Mario Jarmasz and Caroline Barriere. Using semantic
similarity over Tera-byte corpus, compute the per-
formance of keyphrase extraction. In Proceedings
of CLINE. 2004.
Su Nam Kim, Olena Medelyan, Min-Yen Kan and
Timothy Baldwin. SemEval-2010 Task 5: Auto-
matic Keyphrase Extraction from Scientific Arti-
cles. In Proceedings of SemEval-2: Evaluation Ex-
ercises on Semantic Evaluation. to appear.
Dawn Lawrie, W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summariza-
tion. In Proceedings of SIGIR. 2001, pp. 349?357.
Chin-Yew Lin and Edward H. Hovy. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In In Proceedings of HLT-NAACL. 2003.
Alvin Martin and Mark Przybocki. The 1999 NIST
Speaker Recognition Evaluation, Using Summed
Two-Channel Telephone Data for Speaker Detec-
tion and Speaker Tracking. In Proceedings of Eu-
roSpeech. 1999.
Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
occurrence Statistical Information. International
Journal on Artificial Intelligence Tools. 2004,
13(1), pp. 157?169.
Olena Medelyan and Ian Witten. Thesaurus based
automatic keyphrase indexing. In Proceedings of
ACM/IEED-CS JCDL. 2006, pp. 296?297.
Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP 2004.
2004, pp. 404?411.
Guido Minnen, John Carroll and Darren Pearce. Ap-
plied morphological processing of English. NLE.
2001, 7(3), pp. 207?223.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientific Publications. In Proceeding
of ICADL. 2007, pp. 317?326.
Sebastian Pado?, Michel Galley, Dan Jurafsky and
Christopher D. Manning. Textual Entailment Fea-
tures for Machine Translation Evaluation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation. 2009, pp. 37?41.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
2001, pp. 311?318.
579
Youngja Park, Roy J. Byrd and Branimir Boguraev.
Automatic Glossary Extraction Beyond Terminol-
ogy Identification. In Proceedings of COLING.
2004, pp. 48?55.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Polla
and Timo Honkela. A Language-Independent Ap-
proach to Keyphrase Extraction and Evaluation. In
Proceedings of COLING. 2008, pp. 83?86.
Horacio Saggion, Dragomir Radev, Simon Teufel,
Wai Lam and Stephanie Strassel. Meta-evaluation
of Summaries in a Cross-lingual Environment us-
ing Content-based Metrics. In Proceedings of COL-
ING. 2002, pp. 1?7.
Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434?
439.
Xiaojun Wan and Jianguo Xiao. CollabRank: to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969?976.
Ian Witten, Gordon Paynter, Eibe Frank, Car Gutwin
and Craig Nevill-Manning. KEA:Practical Auto-
matic Key phrase Extraction. In Proceedings of
ACM conference on Digital libraries. 1999, pp.
254?256.
Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In-
ternational Conference on Recent Advances in Nat-
ural Language Processing. 2009.
580
Coling 2010: Poster Volume, pages 605?613,
Beijing, August 2010
Best Topic Word Selection for Topic Labelling
Jey Han Lau,?? David Newman,?? Sarvnaz Karimi? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California
jhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.net
Abstract
This paper presents the novel task of best
topic word selection, that is the selection
of the topic word that is the best label for
a given topic, as a means of enhancing the
interpretation and visualisation of topic
models. We propose a number of features
intended to capture the best topic word,
and show that, in combination as inputs to
a reranking model, we are able to consis-
tently achieve results above the baseline of
simply selecting the highest-ranked topic
word. This is the case both when training
in-domain over other labelled topics for
that topic model, and cross-domain, us-
ing only labellings from independent topic
models learned over document collections
from different domains and genres.
1 Introduction
In the short time since its inception, topic mod-
elling (Blei et al, 2003) has become a main-
stream technique for tasks as diverse as multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008) and information retrieval (Wei
and Croft, 2006). For many of these tasks, the
multinomial topics learned by the topic model can
be interpreted natively as probabilities, or mapped
onto a pre-defined discrete class set. However,
for tasks where the learned topics are provided
to humans as a first-order output, e.g. for use in
document collection analysis/navigation, it can be
difficult for the end-user to interpret the rich sta-
tistical information encoded in the topics. This
research is concerned with making topics more
readily human interpretable, by selecting a single
term with which to label the topic.
Although topics are formally a multinomial dis-
tribution over terms, with every term having finite
probability in every topic, topics are usually dis-
played by printing the top-10 terms (i.e. the 10
most probable terms) in the topic. These top-10
terms typically account for about 30% of the topic
mass for reasonable setting of number of topics,
and usually provide sufficient information to de-
termine the subject area and interpretation of a
topic, and distinguish one topic from another.
Our research task can be illustrated via the top-
10 terms in the following topic, learned from a
book collection. Terms wi are presented in de-
scending order of P (wi|tj) for the topic tj :
trout fish fly fishing water angler stream rod
flies salmon
Clearly the topic relates to fishing, and indeed,
the fourth term fishing is an excellent label for the
topic. The task is thus termed best word or most
representative word selection, as we are selecting
the label from the closed set of the top-N topic
words in that topic.
Naturally, not all topics are equally coherent,
however, and the lower the topic coherence, the
more difficult the label selection task becomes.
For example:
oct sept nov aug dec july sun lite adv globe
appears to conflate months with newspaper
names, and no one of these topic words is able to
capture the topic accurately. As such, our method-
ology presupposes an automatic means of rating
topics for coherence. Fortunately, recent research
by Newman et al (2010) has shown that this is
achievable at levels approaching human perfor-
mance, meaning that this is not an unreasonable
assumption.
Labelling topics has applications across a di-
verse range of tasks. Our original interest in the
605
problem stems from work in document collection
visualisation/navigation, and the realisation that
presenting users with topics natively (e.g. as rep-
resented by the top-N terms) is ineffective, and
would be significantly enhanced if we could au-
tomatically predict succinct labels for each topic.
Another application area where labelling has been
shown to enhance the utility of topic models is se-
lectional preference learning via topic modelling
(Ritter et al, to appear). Here, topic labelling via
taxonomic classes (e.g. WordNet synsets) can lead
to better topic generalisation, in addition to better
human readability.
This paper is based around the assumption that
an appropriate label for a topic can be found
among the high-ranking (high probability) terms
in that topic. We assess the suitability of each term
by way of comparison with other high-ranking
terms in that same topic, using simple pointwise
mutual information and conditional probabilities.
We first experiment with a simple ranking method
based on the component scores, and then move
on to using those scores, along with features from
WordNet and from the original topic model, in a
ranking support vector regression (SVR) frame-
work. Our experiments demonstrate that we are
able to perform the task significantly better than
the baseline of selecting the topic word of high-
est marginal probability, including when training
the ranking model on labelled topics from other
document collections.
2 Related Work
Predictably, there has been significant work on in-
terpreting topics in the context of topic modelling.
Topic are conventionally interpreted via the top-
N words in each topic (Blei et al, 2003; Grif-
fiths and Steyvers, 2004), or alternatively by post-
hoc manual labelling of each topic based on do-
main knowledge and subjective interpretation of
each topic (Wang and McCallum, 2006; Mei et
al., 2006).
Mei et al (2007) proposed various approaches
for automatically suggesting phrasal labels for
topics, based on first extracting phrases from the
document collection, and subsequently ranking
the phrases based on KL divergence with a given
topic.
Magatti et al (2009) proposed a method for la-
belling topics induced by hierarchical topic mod-
elling, based on ontological alignment with the
Google Directory (gDir) hierarchy, and optionally
expanding topics based on a thesaurus or Word-
Net. Preliminary experiments suggest the method
has promise, but the method crucially relies on
both a hierarchical topic model and a pre-existing
ontology, so has limited applicability.
Over the general task of labelling a learned se-
mantic class, Pantel and Ravichandran (2004) pro-
posed the use of lexico-semantic patterns involv-
ing each member of that class to learn a (usu-
ally hypernym) label. The proposed method was
shown to perform well over the semantically ho-
mogeneous, fine-grained clusters learned by CBC
(Pantel and Lin, 2002), but for the coarse-grained,
heterogeneous topics learned by topic modelling,
it is questionable whether it would work as well.
The first works to report on human scoring of
topics were Chang et al (2009) and Newman et
al. (2010). The first study used a novel but syn-
thetic intruder detection task where humans eval-
uate both topics (that had an intruder word), and
assignment of topics to documents (that had an in-
truder topic). The second study had humans di-
rectly score topics learned by a topic model. This
latter work introduced the pointwise mutual infor-
mation (PMI) score to model human scoring. Fol-
lowing this work, we use PMI as features in the
ranking SVR model.
3 Methodology
Our task is to predict which words annotators
tend to select as most representative or best words
when presented with a list of ten words. Since
annotators are not generally unanimous in their
choice of best word, we formulate this as a rank-
ing task, and treat the top-1, 2 and 3 system-
ranked items as the best words, and compare that
to the top-1, 2 and 3 words chosen most frequently
by annotators. In this section, we describe the fea-
tures that may be useful for this ranking task. We
start with features motivated by word association.
An obvious idea is that the most representative
word should be readily evoked by other words
in the topic. For example, given a list of words
?space, earth, moon, nasa, mission?, which is a
606
Space Exploration topic, space could arguably be
the most representative word. This is because
it is natural to think about the word space after
seeing the words earth, moon and nasa individ-
ually. A good candidate for best word could be
the word that has high average conditional proba-
bility given each of the other words. To calculate
conditional probability, we use word counts from
the entire collection of English Wikipedia articles.
Conditional probability is defined as:
P (wi|wj) =
P (wi, wj)
P (wj)
,
where i 6= j and P (wi, wj) is the probability of
observing both wi and wj in the same sliding win-
dow, and P (wi) is the overall probability of word
wi in the corpus. In the above example, evoked by
means that space would fill the slot of wi. The av-
erage conditional probability for word wi is given
by:
avg-CP1(wi) = 19
?
j
P (wi|wj),
for j = 1 . . . 10, j 6= i (this range of indices ap-
plies to all following average quantities).
In other cases, we have the flip situation, where
the most representative word may evoke (rather
than be evoked by) other words in the list of ten
words. Imagine a NASCAR Racing topic, which
has a list of words ?race, car, nascar, driver, rac-
ing?. Given the word nascar, words from the list
such as race, car, racing and driver might come
to mind because nascar is heavily associated with
these words. Therefore, a good candidate, wi,
might also correlate with high P (wj |wi). As be-
fore, the average conditional probability (here de-
noted with CP2) for word wi is given by:
avg-CP2(wi) = 19
?
j
P (wj |wi).
Another approach to measuring word associa-
tion is by calculating pointwise mutual informa-
tion (PMI) between word pairs. Unlike condi-
tional probability, PMI is symmetric and thus the
order of words in a pair does not matter. We
calculate PMI using word counts from English
Wikipedia as follows:
PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .
The average PMI for word wi is given by:
avg-PMI(wi) = 19
?
j
PMI(wi, wj).
The topic model produces an ordered list of
words for each topic, and the ordering is given by
the marginal probability of each word given that
topic, P (wi|tj). The ranking of words based on
these probabilities indicates the importance of a
word in a topic, and it is also a feature that we use
for predicting the most representative word.
We also observe that sometimes the most repre-
sentative words are generalized concepts of other
words. As such, hypernym relations could be an-
other feature that may be relevant to predicting the
best word. To this end, we use WordNet to find
hypernym relations between pairs of words in a
topic and obtain a set of boolean-valued relation-
ships for each topic word.
Our last feature is the distributional similar-
ity scores of Pantel et al (2009), as trained over
Wikipedia.1 This takes the form of representing
the distributional similarity between each pairing
of terms sim(wi|wj); if wi is not in the top-200
most similar terms for a given wj , we assume it to
have a similarity of 0.
While the above features can be used alone
to get a ranking on the ten topic words, we can
also use various combinations of features in a
reranking model such as support vector regres-
sion (SVMrank: Joachims (2006)). Applying the
features described above ? conditional probabil-
ities, PMI, WordNet hypernym relations, the topic
model word rank, and Pantel?s distributional simi-
larity score ? as features for SVMrank, a ranking
of words is produced and candidates for the most
representative word are selected by choosing the
top-ranked words.
607
NEWS stock market investor fund trading investment firm exchange ...
police gun officer crime shooting death killed street victim ...
food restaurant chef recipe cooking meat meal kitchen eat...
patient doctor medical cancer hospital heart blood surgery ...
BOOKS loom cloth thread warp weaving machine wool cotton yarn ...
god worship religion sacred ancient image temple sun earth ...
crop land wheat corn cattle acre grain farmer manure plough ...
sentence verb noun adjective grammar speech pronoun ...
Figure 1: Selected topics from the two collections
(each line is one topic, with fewer than ten topic
words displayed because of limited space)
4 Datasets
We used two collections of text documents from
different genres for our experiments. The first col-
lection (NEWS) was created by selecting 55,000
news articles from the LDC Gigaword corpus.
The second collection (BOOKS) was 12,000 En-
glish language books selected from the Inter-
net Archive American Libraries collection. The
NEWS and BOOKS collections provide a diverse
range of content for topic modeling. In the first
case ? news articles from the past decade written
by journalists ? each article usually attempts to
clearly and concisely convey information to the
reader, and hence the learned topics tend to be
fairly interpretable. For BOOKS (with publication
dates spanning more than a century), the writing
style often uses lengthy and descriptive prose, so
one sees a different style to the learned topics.
The input to the topic model is a bag-of-words
representation of the collection of text documents,
where word counts are preserved, but word order
is lost. After performing fairly standard tokeniza-
tion and limited lemmatisation, and creating a vo-
cabulary of terms that occurred at least ten times,
each corpus was converted into its bag-of-words
representation. We learned topic models for the
two collections, choosing a setting of T = 200
topics for NEWS and T = 400 topics for BOOKS.
After computing the PMI-score for each topic (ac-
cording to Newman et al (2010)), we selected 60
topics with high PMI-score, and 60 topics with
low PMI-score, from both corpora, resulting in a
total of 240 topics for human evaluation.
The 240 topics selected for human scoring were
1Accessed from http://demo.patrickpantel.
com/Content/LexSem/thesaurus.htm.
Features Description
PMI Pointwise mutual information
CP1 Conditional probability P (wi|?)
CP2 Conditional probability P (?|wi)
TM Rank Original topic model word rank
Hypernym WordNet hypernym relationships
PDS Pantel distributional similarity score
Table 1: Description of feature sets
each evaluated by between 10 and 20 users. For
the two topic models, we used the conventional
approach of displaying each topic with its top-10
terms. In a typical survey, a user was asked to
evaluate anywhere from 60 to 120 topics. The in-
structions asked the user to perform the following
tasks, for each topic in the survey: (a) score the
topic for ?usefulness? or ?coherence? on a scale
of 1 to 3; and (b) select the single best word that
exemplifies the topic (when score=3).
From both NEWS and BOOKS, the 40 topics
with the highest average human scores had rela-
tively complete data for the ?best word? selection
task (i.e. every time a user gave a topics score=3,
they also selected a ?best word?). The remain-
der of this paper is concerned with the 40 NEWS
topics and 40 BOOKS topics where we had ?best
word? data from the annotators. Sample topics
from these two sets are given in Figure 1.
To measure presentational bias (i.e. the extent
to which annotators tend to choose a word seen
earlier rather than later, particularly when armed
with the knowledge that words are presented in or-
der of probability), we reissued a survey using the
40 NEWS topics to ten additional annotators, but
this time the top-10 topic words were presented
in random order. Again, these ten new annotators
were asked to select the best word.
5 Experiments
We used average PMI and conditional probabili-
ties, CP1 and CP2, to rank the ten words in each
topic. Candidates for the best words were selected
by choosing the top-1, 2 and 3 ranked words.
We used the following weighted scoring func-
tion for evaluation:
Best-N score =
?N
i=1 n(wrevi)?N
i=1 n(wi)
608
Features Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
PMI 0.25 0.38 0.49
CP1 0.30 0.42 0.51
CP2 0.15 0.27 0.45
Upper bound 0.48 ? ?
Table 2: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for NEWS
Features Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
PMI 0.25 0.38 0.49
CP1 0.30 0.38 0.47
CP2 0.15 0.30 0.49
Upper bound 0.64 ? ?
Table 3: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for BOOKS
where wrevi is the ith term ranked by the system
and wi is the ith most popular term selected by
annotators; revi gives the index of the word wi
in the annotator?s list; and n(w) is the number of
votes given by annotators for word w.
The baseline is obtained using the original word
rank produced by the topic model based on topic
word probabilities P (wi|tj). An upperbound is
calculated by evaluating the decision of an annota-
tor against others for each topic. This upperbound
signifies the maximum accuracy for human anno-
tators on average; since the annotators were asked
to pick a single best word in the survey, only the
Best-1 upperbound can be obtained.
The Best-1/2/3 results are summarized in Ta-
ble 2 for NEWS and Table 3 for BOOKS. These
Best-N scores are computed just using the single
feature of PMI, CP1 and CP2 (each in turn) to rank
the words in each topic. None of these features
alone produces a result that exceeds baseline per-
formance.
To make better use of all the features described
in Section 3, namely the PMI score, conditional
probabilities (both directions), topic model word
rank, WordNet Hypernym relationships and Pan-
tel?s distributional similarity score, we build a
ranking classifier using SVMrank and evaluating
Feature Set Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
All Features 0.43 0.56 0.62
?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)
?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)
?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)
?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)
?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)
?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)
Upper bound 0.48 ? ?
Table 4: SVR-based best topic word results for
NEWS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
Feature Set Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
All Features 0.40 0.51 0.62
?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)
?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)
?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)
?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)
?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)
?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)
Upper bound 0.64 ? ?
Table 5: SVR-based best topic word results for
BOOKS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
using 10-fold cross validation. Our first approach
is to use the entire set of features to train the clas-
sifier. Following this, we also measure the effect
of each feature by ablating (removing) one fea-
ture at a time. The drop in Best-N score indicates
which features are the strongest predictors of the
best words (a larger drop in score indicates that
feature is more important). The results for Best-1,
Best-2 and Best-3 scores are summarized in Ta-
ble 4 for NEWS, and Table 5 for BOOKS (averaged
across the 10 iterations of cross validation).
We then produced a condensed set of features,
consisting of the conditional probabilities, the
original topic model word rank and the WordNet
hypernym relationships. This ?best? set of fea-
tures is used to make predictions of best words.
Results are improved in most cases, and are sum-
marized in Table 6 for both NEWS and BOOKS.
609
Dataset Best-1 Best-2 Best-3
NEWS
Baseline 0.35 0.50 0.59
Best Feat. Set 0.45 0.50 0.65
Upper bound 0.48 ? ?
BOOKS
Baseline 0.38 0.48 0.60
Best Feat. Set 0.48 0.56 0.66
Upper bound 0.64 ? ?
Table 6: Results with the best feature set com-
pared to the baseline
Dataset Best-1 Best-2 Best-3
NEWS baseline 0.35 0.50 0.59
BOOKS ? NEWS 0.38 0.56 0.62
NEWS upper bound 0.48 ? ?
BOOKS baseline 0.38 0.48 0.60
NEWS ? BOOKS 0.48 0.56 0.65
BOOKS upper bound 0.64 ? ?
Table 7: Results for cross-domain learning
We also tested whether the SVM classifier
could be trained using data from one domain, and
run on data from another domain. Using our two
datasets as these different domains, we trained a
model using BOOKS data and made predictions
for NEWS, and then we trained a model using
NEWS data and made predictions for BOOKS.
The results, shown in Table 7, indicate that
we are still able to outperform the baseline, even
when the ranking classifier is trained on a differ-
ent domain. In fact, when we trained a model
using NEWS, we saw almost no drop in perfor-
mance for predicting best words for BOOKS, and
improvement is seen for Best-2 score from NEWS.
This implies that the SVM classifier generalizes
well across domains and suggests the possibility
of having a fixed training model to predict best
words for any data.
In these experiments, topic words are presented
in the original order that the topic model produces,
i.e. in descending order of probability of a word
under a topic P (wi|tj). We noticed that the first
words of the topics are frequently selected as the
best words by annotators, and suspected that this
was introducing a bias towards the first word. As
our baseline scores are derived from this topic
word ordering, such a bias could give rise to an
artificially high baseline.
To investigate this effect, we ran a second anno-
Word Order Best-1 Best-2 Best-3
Original 0.35 0.50 0.59
Randomized 0.23 0.33 0.46
Table 8: Reduction of baseline scores for NEWS
when words are presented in random order to an-
notators.
2 4 6 8 10
0.
0
0.
1
0.
2
0.
3
0.
4
Rank
Fr
ac
tio
n 
of
 h
um
an
 s
el
ec
te
d 
be
st
 w
or
d
ordered
random
Figure 2: Bias for humans selecting the best word,
when the topic words are presented in their origi-
nal ordering (ordered) or randomised (random)
tation exercise over the same set of topics (but dif-
ferent annotators), to obtain a new set of best word
annotations for NEWS, with the topic words pre-
sented in random order. In Figure 2, we plot the
cumulative proportion of words selected as best
word by the annotators across the topics, in the
case of the random topic word order, mapping the
topic words back onto their original ranks in the
topic model. A slight drop can be observed in the
proportion of first- and second-ranked topic words
being selected when we randomise the topic word
order. When we recalculate the baseline accuracy
for NEWS on the basis of the new set of annota-
tions, we observe an appreciable drop in the scores
(see Table 8).
6 Discussion
From the experiments in Section 5, perhaps the
first thing to observe is: (a) the high performance
of the baseline, and (b) the relatively low (Best-
1) upper bound accuracy for the task. The first is
perhaps unsurprising, given that it represents the
610
topic model?s own interpretation of the word(s)
which are most representative of that topic. In this
sense, we have set our sights high in attempting to
better the baseline. The upper bound accuracy is
a reflection of both the inter-annotator agreement,
and the best that we can meaningfully expect to
do for the task. That is, any result higher than this
would paradoxically suggest that we are able to do
better at a task than humans, where we are evalu-
ating ourselves relative to the labellings of those
humans. The upper bound for NEWS was slightly
less than 0.5, indicating that humans agree on the
best topic word only 50% of the time. To better
understand what is happening here, consider the
following topic from Figure 1:
health drug patient medical doctor hospital
care cancer treatment disease
This is clearly a coherent topic, but at least two
topic words suggest themselves as labels: health
and medical. By way of having between 10 and 20
annotators (uniquely) label a given topic, and in-
terpreting the multiple labellings probabilistically,
we are side-stepping the inter-annotator agree-
ment issue, but ultimately, for the Best-1 evalu-
ation, we are forced to select one term only, and
consider any alternative to be wrong. Because an-
notators selected only one best topic word, we un-
fortunately have no way of performing Best-2 or
Best-3 upper bound evaluation and deal with top-
ics such as this, but would expect the numbers to
rise appreciably.
Looking at the original feature rankings in Ta-
bles 2 and 3, no clear picture emerges as to which
of the three methods (PMI, CP1 and CP2) was
most successful, but there were certainly clear dif-
ferences in the relative numbers for each, point-
ing to possible complementarity in the scoring.
This expectation was born out in the results for
the reranking model in Tables 4 and 5, where the
combined feature set surpassed the baseline in all
cases, and feature ablation tended to lead to a drop
in results, with the single most effective feature set
being CP1 (P (wi|?)), followed by CP2 (P (?|wi))
and topic model rank. The lexical semantic fea-
tures of WordNet hypernymy and PDS (Pantel?s
distributional similarity) were the worst perform-
ers, often having no or negative impact on the re-
sults.
Comparing the best results for the SVR-based
reranking model and the upper bound Best-1
score, we approach the upper bound performance
for NEWS, but are still quite a way off with
BOOKS when training in-domain. This is encour-
aging, but a slightly artificial result in terms of the
broader applicability of this research, as what it
means in practical terms is that if we can access
multi-annotator best word labelling for the ma-
jority of topics in a given topic model, we can
use those annotations to predict the best word for
the remainder of the topics with reasonably suc-
cess. When we look to the cross-domain results,
however, we see that we almost perfectly replicate
the best-achieved Best-1, Best-2 and Best-3 in-
domain results for BOOKS by training on NEWS
(making no use of the annotations for BOOKS).
Applying the annotations for BOOKS to NEWS is
less successful in terms of Best-1 accuracy, but we
actually achieve higher Best-2, and largely mir-
ror the Best-3 results as compared to the best of
the in-domain results in Table 6. This leads to
the much more compelling conclusion that we can
take annotations from an independent topic model
(based on a completely unrelated document col-
lection), and apply them to successfully model the
best topic word for a new topic model, without
requiring any additional annotation. As we now
have two sets of topics multiply-annotated for best
words, this result suggests that we can perform the
best topic word selection task with high success
over novel topic models.
We carried out manual analysis of topics where
the model did particularly poorly, to get a sense
for how and where our model is being led astray.
One such example is the topic:
race car nascar driver racing cup winston team
gordon season
where the following topic words were selected by
our annotators: nascar (8 people), race (2 peo-
ple), and racing (2 people). First, we observe the
split between race and racing, where more judi-
cious lemmatisation/stemming would make both
the annotation easier and the evaluation cleaner.
The SVR model tends to select more common,
general terms, so in this case chose race as the
best word, and ranked nascar third. This is one
611
instance were nascar evokes all of the other words
effectively, but not conversely (racing is asso-
ciated with many events/sports beyond nascar,
e.g.).
Another topic where our model had difficulty
was:
window nave aisle transept chapel tower arch
pointed arches roof
where our best model selected nave, while the hu-
man annotators selected chapel (6 people), arch
(2 people), nave, roof , tower and transept (1 per-
son each). Clearly, our annotators struggled to
come up with a best word here, despite the topic
again being coherent. This is an obvious candi-
date for labelling with a hypernym/holonym of
the topic words (e.g. church or church architec-
ture), and points to the limitations of best word la-
belling ? there are certainly many topics where
best word labelling works, as our upper bound
analysis demonstrated, but there are equally many
topics where the most natural label is not found
in the top-ranked topic words. While this points
to slight naivety in the current task set up ? we
are forcing annotators to label words with topic
words, where we know that this is sub-optimal
for a significant number of topics ? we contend
that our numbers suggest that: (a) consistent best
topic word labelling is possible at least 50% of
the time; and (b) we have developed a method
which is highly adept at labelling these topics. As
a way forward, we intend to relax the constraint
on the topic label needing to be based on a topic
word, and explore the possibility of predicting
which topics are best labelled with topic words,
and which require independent labels. For topics
which can be labelled with topic words, we can
use the methodology developed here, and for top-
ics where this is predicted to be sub-optimal, we
intend to build on the work of Mei et al (2007),
Pantel and Ravichandran (2004) and others in se-
lecting phrasal/hypernym labels for topics. We are
also interested in applying the methodology pro-
posed herein to the closely-related task of intruder
word, or worst topic word, detection, as proposed
by Chang et al (2009).
Finally, looking to the question of the impact of
the presentation order of the topic words on best
word selection, it would appear that our baseline
is possibly an over-estimate (based on Table 8).
Having said that, the flipside of the bias is that it
leads to more consistency in the annotations, and
tends to help in tie-breaking of examples such as
race and racing from above, for example. In sup-
port of this claim, the upper bound Best-1 accu-
racy of the randomised annotations, relative to the
original gold-standard is 0.44, slightly below the
original upper bound for NEWS. More work is
needed to determine the real impact of this bias
on the overall task setup and evaluation.
7 Conclusion
This paper has presented the novel task of best
topic word selection, that is the selection of the
topic word that is the best label for a given topic.
We proposed a number of features intended to
capture the best topic word, and demonstrated
that, while they were relatively unsuccessful in
isolation, in combination as inputs to a rerank-
ing model, we were able to consistently achieve
results above the baseline of simply selecting the
highest-ranked topic word, both when training in-
domain over other labelled topics for that topic
model, and cross-domain, using only labellings
from independent topic models learned over docu-
ment collections from different domains and gen-
res.
Acknowledgements
NICTA is funded by the Australian government as repre-
sented by Department of Broadband, Communication and
Digital Economy, and the Australian Research Council
through the ICT centre of Excellence programme. DN has
also been supported by a grant from the Institute of Museum
and Library Services, and a Google Research Award.
References
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Brody, S. and M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the EACL (EACL 2009), pages 103?111, Athens,
Greece.
Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the 23rd
612
Annual Conference on Neural Information Process-
ing Systems (NIPS 2009), pages 288?296, Vancou-
ver, Canada.
Griffiths, T. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101:5228?5235.
Haghighi, A. and L. Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics ?
Human Language Technologies 2009 (NAACL HLT
2009), pages 362?370, Boulder, USA.
Joachims, T. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226, Philadelphia, USA.
Magatti, D., S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In Proceedings of the
International Conference on Intelligent Systems De-
sign and Applications, pages 1227?1232, Pisa, Italy.
Mei, Q., C. Liu, H. Su, and C. Zhai. 2006. A prob-
abilistic approach to spatiotemporal theme pattern
mining on weblogs. In Proceedings of the 15th
International World Wide Web Conference (WWW
2006), pages 533?542.
Mei, Q., X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2007), pages 490?499, San Jose, USA.
Newman, D., J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
Pantel, P. and D. Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 613?619, Ed-
monton, Canada.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 321?
328, Boston, USA.
Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,
and V. Vyas. 2009. Web-scale distributional sim-
ilarity and entity set expansion. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
938?947, Singapore.
Ritter, A, Mausam, and O Etzioni. to appear. A la-
tent Dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the ACL (ACL 2010), Uppsala, Sweden.
Titov, I. and R. McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International World Wide Web
Conference (WWW 2008), pages 111?120, Beijing,
China.
Wang, X. and A. McCallum. 2006. Topics over time:
A non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), pages 424?433,
Philadelphia, USA.
Wei, S. and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of 29th
International ACM-SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2006), pages 178?185, Seattle, USA.
613
Coling 2010: Demonstration Volume, pages 37?40,
Beijing, August 2010
PanLex and LEXTRACT: Translating all Words of all Languages of the
World
Timothy Baldwin,? Jonathan Pool? and Susan M. Colowick?
? CSSE
University of Melbourne
tb@ldwin.net
? Utilika Foundation
{pool,smc}@utilika.org
Abstract
PanLex is a lemmatic translation re-
source which combines a large number of
translation dictionaries and other translin-
gual lexical resources. It currently covers
1353 language varieties and 12M expres-
sions, but aims to cover all languages and
up to 350M expressions. This paper de-
scribes the resource and current applica-
tions of it, as well as lextract, a new
effort to expand the coverage of PanLex
via semi-automatic dictionary scraping.
1 Introduction
Translation dictionaries, multilingual thesauri,
and other translingual lexical (more precisely,
lemmatic) resources answer queries of the form
?Given lemma X in language A, what possible
translations of it into language B exist?? However,
existing resources answer only a small fraction of
the potential queries of this form. For example,
one may find attested translations of the Santiago
del Estero Quichua word unta into German, En-
glish, Spanish, Italian, French, Danish, Aymara,
and several other Quechua languages, but not into
the other (roughly 7 thousand) languages in the
world.
Answers to the vast majority of possible lem-
matic translation queries must be inferred. If unta
can be translated into Spanish as lleno, and lleno
can be translated into Hungarian as tele, e.g., per-
haps Quichua unta can be translated into Hungar-
ian as tele. But such inference is nontrivial, be-
cause lexical ambiguity degenerates the quality of
indirect translations as the paths through interme-
diate languages grow longer.
Current Goal
Resources 766 10K
Language varieties 1353 7000
Expressions 12M 350M
Expression?meaning pairs 27M 1000M
Expression?expression pairs 91M 1000M
Table 1: Current and goal PanLex coverage
Thus, it appears that the quality and range of
lemmatic translation would be supported by an
easily accessible graph combining a large (or, ide-
ally, complete) set of translations reported by the
world?s lexical resources. PanLex (http://
panlex.org) is a project developing a publicly
accessible graph of attested lemmatic translations
among all languages. As of 2010, it provides about
90 million undirected pairwise translations among
about 12 million lemmata in over 1,300 language
varieties, based on the consultation of over 750 re-
sources, as detailed in Table 1. By 2011 it is ex-
pected that the resources consulted will approxi-
mately quadruple.
2 The PanLex Project
PanLex is an attempt to generate as complete as
possible a translation graph, made up of expres-
sion nodes, meaning nodes, and undirected edges,
each of which links an expression node with a
meaning node. Each expression is uniquely de-
fined by a character string and a language. An ex-
pression ei is a translation or synonym of an ex-
pression ej iff there is at least one meaning mk
such that edges v(ei,mk) and v(ej,mk) exist. For
example, frame in English shares a meaning with
bikar in Bahasa Malay, and bikar shares a mean-
ing with beaker in English, but frame shares no
37
meaning with beaker. Whether ei and ej are syn-
onyms or translations depends on whether their
languages are identical. In Table 1, ?expres-
sion?meaning pairs? refers to edges v(e,m) and
?expression?expression pairs? refers to expres-
sions with at least one meaning in common.
2.1 Current Applications of PanLex
While lemmatic translation falls short of senten-
tial and discourse translation, it is not without
practical applications. It is particularly useful
in author?machine collaborative translation, when
authors are in a position to lemmatize expres-
sions. The prototype PanImages application
(http://www.panimages.org), based on Pan-
Dictionary, elicits a lemmatic search query
from the user and expands the query into dozens
of languages for submission to image-search ser-
vices. Hundreds of thousands of visitors have used
it to discover relevant images labeled in languages
they do not know, sometimes selecting particular
target languages for cultural specificity or to craft
less ambiguous queries than their own language
would permit (Christensen et al, 2009).
In lemmatic messaging applications developed
for user studies, users lemmatized sentences to tell
stories or send mail across language boundaries.
Evenwith context-unaware translation of lemmata
producing mostly non-optimal translations, users
were generally able to reconstruct half or more of
the originally intended sentences (Soderland et al,
2009). The PanLex database was also used in a
multilingual extension of the image-labeling game
initiated by Von Ahn and Dabbish (2004).
User and programmatic interfaces to PanLex
are under development. A lemmatic user in-
terface (http://panlex.org/u) communicates
with the user in a potentially unlimited set of
languages, with PanLex dynamically using its
own data for the localization. A primitive API
makes it possible for developers to provide, or
make infrastructural use of, lemmatic transla-
tion via PanLex. Prototype lemmatic transla-
tion services like TeraDict (http://panlex.
org/demo/treng.html), InterVorto (http://
panlex.org/demo/trepo.html), and T?mS?z
(http://panlex.org/demo/trtur.html) ex-
ploit the API.
2.2 Extraction and Normalization
The approach taken by PanLex to populate the
translation graph with nodes and edges is a combi-
nation of: (a) extraction of translation pairs from
as many translingual lexical resources as can be
found on the web and elsewhere; and (b) infer-
ence of new edges between expressions that exist
in PanLex.
To date, extraction has taken the form of hand
writing a series of regular expression-based scripts
for each individual dictionary, to generate normal-
ized PanLex database records. While this is ef-
ficient for families of resources which adhere to
a well-defined format (e.g. Freedict or Star-
dict dictionaries), it does not scale to the long
tail of one-off dictionaries constructed by lexi-
cographers using ad hoc formats, as detailed in
Section 2.2. lextract is an attempt to semi-
automate this process, as detailed in Section 3.
Inference of new translation edges is nontrivial,
because lexical ambiguity degenerates the qual-
ity of indirect translations as the paths through
intermediate languages grow longer. PanDic-
tionary is an attempt to infer a denser translation
graph fromPanLex combining translations from
many resources based on path redundancy, evi-
dence of ambiguity, and other information (Sam-
mer and Soderland, 2007; Mausam et al, 2009;
Mausam et al, 2010).
PanLex is more than a collection, or docbase,
of independent resources. Its value in translation
inference depends on its ability to combine facts
attested by multiple resources into a single graph,
in which lemmata frommultiple resources that are
substantively identical are recognized as identi-
cal. The obstacles to such integration of heteroge-
neous lexical data are substantial. They include:
(1) ad hoc formatting, including format changes
between portions of a resource; (2) erratic spacing,
punctuation, capitalization, and line wrapping; (3)
undocumented and non-standard character encod-
ings; (4) vagueness of the distinction between lem-
matic (e.g. Rana erythraea) and explanatory trans-
lations (e.g. a kind of tree frog); and (5) absence of
consensus for some languages as to the representa-
tion of lemmata, e.g. hyphenation and prefixation
in Bantu languages, and inclusion or exclusion of
tones in tonal languages.
38
#NAME "English-Hindi Dictionary"
#INDEX_LANGUAGE "English"
a
[p]det.[/p]
[m1][trn]??, ?????? ??????? ?? ??? ??? ??? ???; (??? ??) ???? ????? ???? ?? ?????[/trn][/m]
aback
[p]adv.[/p]
[m1][trn]?????, ?????; ????[/trn][/m]
...
?
2
eng-00
hin-00
ex
a
wc
detr
ex
??
...
Figure 1: A snippet of an English?Hindi dictionary, in its source form (left) and as normalizedPanLex
records (right)
3 lextract
lextract is a sub-project of PanLex, aimed
at automating the extraction and normalization
of data from arbitrary lexical resources, focusing
in the first instance on text-based resources, but
ultimately including XML, (X)HTML, PDF and
wiki markup-based resources. The approach taken
in lextract is to emulate the manual work-
flow used by the PanLex developers to scrape
data from dictionary files, namely learning of se-
ries of regular expressions to convert the source
dictionary into structured database records. In
this, we assume that the source dictionary has
been transcoded into utf-8 encoding,1 and fur-
ther that the first fivePanLex translation records
found in the source dictionary have been hand
generated as seed instances to bootstrap the ex-
traction process off, as illustrated in Figure 1.
Briefly, this provides vital data including: specifi-
cation of the source and target languages; manual
disambiguation of expression?expression vs. ex-
pression?meaning structuring; any optional fields
such as part of speech; and (implicitly) where the
records start from in the source file, and what
fields in the original dictionary should not be pre-
served in the PanLex database.
The procedure for learning regular expressions
can be broken down into 3 steps: (1) recordmatch-
ing; (2) match lattice pruning; and (3) regular ex-
pression generalization.
Record matching involves determining the set
of codepoint spans in the original dictionary where
the component strings (minimally the source and
1We have experimented with automatic character encod-
ing detection methods, but the consensus to date has been
that methods developed for web documents, such as the
chardet library, are inaccurate when applied to dictionary
files.
target language expressions, but possibly includ-
ing domain information, word class information or
other metadata) encoded in the five seed records
can be found, to use as the basis for learning
the formatting idiom employed in the dictionary.
For each record, we determine all positions in the
source dictionary file where all component strings
can be found within a fixed window width of one
another. This is returned as a match lattice, repre-
senting the possible sub-extents (?spans?) in the
source dictionary of each record, and the loca-
tion(s) of each component string within each.
Match lattice pruning takes the match lattice
from the record matching step, and prunes it based
on a combination of hard and soft constraints. The
single hard constraint currently used at present is
that the records must occur in the lexicon in se-
quence; any matches in the lattice which violate
this constraint can be pruned. Soft constraints in-
clude: each record should span the same num-
ber of lines; the fields in each record should oc-
cur in the same linear order; and the width of the
inter-field string(s) should be consistent. These
are expectations on dictionary formatting, but can
be violated (e.g. a given dictionary may have some
entries on a single line and others spanning two
lines). To avoid over-pruning the lattice, we de-
termine the coverage of each such soft constraint
in the form of: (a) type-level coverage, i.e. the pro-
portion of records for which a given constraint set-
ting (e.g. record size in terms of the number of
lines it spans) matches with at least one record
span; and (b) token-level coverage, i.e. the pro-
portion of individual spans a given constraint set-
ting matches. We apply soft constraints conser-
vatively, selecting the soft constraint setting with
full type-level coverage (i.e. it matches all records)
39
and maximum token-level coverage (i.e. it prunes
the least edges in the lattice). Soft constraints are
applied iteratively, as indicated in Algorithm 1.
Algorithm 1 Match lattice pruning algorithm
1: Initialize l . initialize record matching match lattice
2: repeat
3: change? False
4: for all hi ? H do . update hard constraint coverage
5: (htypei,htokeni)? coverage(hi, l)
6: if htokeni < 1 then . if pruneable edges
7: l? apply(hi, l) . apply constraint
8: change? True
9: end if
10: end for
11: for all si ? S do . update soft constraint coverage
12: {(stypeij,stokenij)}? coverage(ci, l)
13: end for
14: if s ? argmaxsij(?stypeij = 1.0 ? stoken < 1.0 ?
(?i? 6= i : |stypei?| > 1, ? j? : stokenij < 1.0 : stokenij >
stokeni?j?)) then
15: l? apply(s, l) . apply constraint
16: change? True
17: end if
18: until change = False
The final step is regular expression generaliza-
tion, whereby the disambiguated match lattice is
used to identify the multiline span of all records
in the source dictionary, and inter-field strings not
corresponding to any record field are generalized
across records to form a regular expression, which
is then applied to the remainder of the dictionary to
extract out normalized PanLex records. As part
of this, we build in dictionary-specific heuristics,
such as the common practice of including optional
fields in parentheses.
The lextract code is available from http:
//lextract.googlecode.com.
lextract has been developed over 10 sample
dictionaries, and record matching and match lat-
tice pruning has been found to perform with 100%
precision and recall over the seed records. We are
in the process of carrying out extensive evaluation
of the regular expression generalization over full
dictionary files.
Future plans for lextract to get closer to
true emulation of themanual extraction process in-
clude: dynamic normalization of target language
strings (e.g. normalizing capitalization or correct-
ing inconsistent pluralization) using a combina-
tion of language-specific tools for high-density
target languages such as English, and analysis of
existing PanLex expressions in that language;
elicitation of user feedback for extents of the doc-
ument where extraction has failed, fields where
the correct normalization strategy is unclear (e.g.
normalization of POS tags not seen in the seed
records, as for det.?detr in Figure 1); and extend-
ing lextract to handle (X)HTML and other file
types.
References
Christensen, Janara, Mausam, and Oren Etzioni. 2009.
A rose is a roos is a ruusu: Querying translations for
web image search. In Proc. of the Joint conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Pro-
cessing (ACL-IJCNLP 2009), pages 193?196, Sun-
tec, Singapore.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of the Joint con-
ference of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th
International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), pages
262?270, Suntec, Singapore.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174(9?10):619?637.
Sammer, Marcus and Stephen Soderland. 2007. Build-
ing a sense-distinguished multilingual lexicon from
monolingual corpora and bilingual lexicons. In
Proc. of the Eleventh Machine Translation Summit
(MT Summit XI), pages 399?406, Copenhagen, Den-
mark.
Soderland, Stephen, Christopher Lim, Mausam,
Bo Qin, Oren Etzioni, and Jonathan Pool. 2009.
Lemmatic machine translation. In Proc. of Ma-
chine Translation Summit XII, page 2009, Ottawa,
Canada.
Von Ahn, Luis and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proc. of the SIGCHI
conference on Human factors in computing systems,
pages 319?326, Vienna, Austria.
40
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1624?1635, Dublin, Ireland, August 23-29 2014.
Novel Word-sense Identification
Paul Cook
?
, Jey Han Lau
?
, Diana McCarthy
?
and Timothy Baldwin
?
? Department of Computing and Information Systems, The University of Melbourne
? Department of Philosophy, King?s College London
? University of Cambridge
paulcook@unimelb.edu.au, jeyhan.lau@gmail.com,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
Automatic lexical acquisition has been an active area of research in computational linguistics
for over two decades, but the automatic identification of new word-senses has received attention
only very recently. Previous work on this topic has been limited by the availability of appropriate
evaluation resources. In this paper we present the largest corpus-based dataset of diachronic
sense differences to date, which we believe will encourage further work in this area. We then
describe several extensions to a state-of-the-art topic modelling approach for identifying new
word-senses. This adapted method shows superior performance on our dataset of two different
corpus pairs to that of the original method for both: (a) types having taken on a novel sense over
time; and (b) the token instances of such novel senses.
1 Novel word-senses
The meanings of words change over time with, in particular, established words taking on new senses. For
example, the usages of drop, wall, and blow up in the following sentences correspond to relatively-recent
senses of these words that appear to be quite common in text related to popular culture, but are not listed
in many dictionaries; for example, they are all missing from WordNet 3.0 (Fellbaum, 1998).
1. The reissue album drops March 27 and is an extension of Perry?s huge 2010 Teenage Dream. [drops
= ?comes out?, ?is released? ]
2. On Facebook, you can plainly see much of the data the site has on you, because it?s posted to your
wall. [wall = ?Facebook wall?, ?personal electronic noticeboard? ]
3. Why would I give him my number so he can blow up my phone the way he does my inbox. [blow up
= ?overwhelm with messages? ]
Computational lexicons are an essential component of systems for a variety of natural language process-
ing (NLP) tasks. The success of such systems, therefore, depends on the quality of the lexicons they use,
and (semi-)automatic techniques for identifying new word-senses could benefit applied NLP by helping
to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in
addition to new words themselves; methods which identify new word-senses could therefore also help to
keep dictionaries current.
In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses.
Specifically, we consider the identification of word-senses that are not attested in a reference corpus,
taken to represent standard usage, but that are attested in a focus corpus of newer texts.
Lau et al. (2012) introduced the task of novel sense identification. They presented a method for
identifying novel word-senses ? described here in Section 4 ? and evaluated this method on a very
small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al.
(2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new word-
senses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1624
as follows. After discussing related work in Section 2, we present a substantially-expanded evaluation
dataset in Section 3, that is based on a second corpus pair and consists of many more lemmas with a
novel sense. We describe the models used by Lau et al. and Cook et al., and our new extensions to
them, in Section 4. In Section 5 we analyse the results of novel sense identification, and consider a new
baseline for this task. We demonstrate that the extended methods give an improvement over the original
method of Lau et al. We conclude by discussing some previously-unexplored variations on novel sense
identification, and limitations of the approaches considered.
The primary contributions of this paper are: (1) development of a novel sense detection dataset much
larger than has been used in research to date; (2) development and evaluation of a new baseline for
novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only
the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4)
extension of the novel sense detection method of Cook et al. to automatically acquire information about
the expected domain(s) of novel senses.
2 Related work
Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently
in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to
identify specific types of semantic change ? widening and narrowing, and amelioration and pejoration,
respectively ? based on specific properties of these phenomena. Gulordava and Baroni (2011) identify
diachronic sense change in an n-gram database, but using a model that is not restricted to any particular
type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for
identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able
to identify words that have undergone a change in meaning, but not the token instances which give rise
to these sense differences.
Bamman and Crane (2011) use a parallel Latin?English corpus to induce word senses and build a
WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al.
(2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based
approaches there is a clear connection between (induced) word-senses and tokens, making it possible to
identify usages of a specific (new) sense.
Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010)
consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either
marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant word-
senses in corpora, including differences between domains. However, this approach does not identify new
senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domain-
specific parallel corpus with novel translations.
The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel word-
senses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a
natural account of polysemy and not only identifies word types that have a novel sense, but identifies
the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing
sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further
extensions to this method.
3 Datasets
Evaluating approaches to identifying semantic change is a challenge due to the lack of appropriate evalu-
ation resources (i.e., corpora for the appropriate time periods, known to exhibit particular sense changes);
indeed, most previous approaches have used very small datasets (e.g., Sagi et al., 2009; Cook and Steven-
son, 2010; Bamman and Crane, 2011). In this study we consider two datasets of relatively newly-coined
word-senses: (1) an extended version of the dataset based on the BNC (Burnard, 2000) and ukWaC (Fer-
raresi et al., 2008) used by Lau et al. (2012); and (2) a new dataset based on the SiBol/Port Corpus.
1
This
1
http://www3.lingue.unibo.it/blog/clb/?page_id=8
1625
is the largest dataset for evaluating approaches to identifying diachronic semantic change constructed
from corpus evidence to be presented to date.
3.1 BNC?ukWaC
Lau et al. (2012) take the written portion of the BNC (approximately 87 million words of British English
from the late 20th century) as the reference corpus, and a similarly-sized random sample of documents
from the ukWaC (a Web corpus built from the .uk domain in 2007) as the focus corpus. They used
TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora.
A set of words that has acquired a new sense between the late 20th and early 21st centuries ? the time
periods of the reference and focus corpora ? is required. The Concise Oxford English Dictionary aims
to document contemporary usage, and has been published in numerous editions including Thompson
(1995, COD95) and Soanes and Stevenson (2005, COD08), enabling the identification of new senses
amongst the entries in COD08 relative to COD95. Manually searching these dictionaries for new senses
would be time intensive, but new words often correspond to concepts that are culturally salient (Ayto,
2006), and one can leverage this observation to speed up the process of finding some candidate words
with novel senses.
2
Between the time periods of the reference and focus corpora, computers and the Internet have become
much more mainstream in society. Lau et al. therefore extracted all headwords in COD08 whose entries
contain the word computing. They then carefully annotated these lemmas to identify those that indeed
exhibit the novel sense indicated in the dictionary in the corpora. Here, we expand Lau et al.?s dataset by
extracting all headwords including any of the following words code, computer, internet, network, online,
program, web, and website. We then follow a similar annotation process to Lau et al.
An annotator read the entries for the selected lexical items in COD95 and COD08, and identified those
which have a clear sense related to computers or the Internet in COD08 that is not present in COD95;
such senses are referred to as novel senses. This process, along with all the annotation in this section
(including Section 3.2), is carried out by native English-speaking authors of this paper and graduate
students in computational linguistics.
To ensure that the words identified from the dictionaries do in fact have a new sense in the ukWaC
sample compared to the BNC, we examine word sketches (Kilgarriff et al., 2004)
3
for each of these
lemmas in the BNC and ukWaC for collocates that likely correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel sense in the BNC, or fail to find evidence of the novel
sense in the ukWaC sample.
4
We further examine the usage of these words in the corpora. We extract a random sample of 100
usages of each lemma from the BNC and ukWaC sample, and annotate these usages as to whether they
correspond to the novel sense or not. This binary distinction is easier than fine-grained sense annotation,
and since we do not use these annotations for formal evaluation ? only for selecting items for our dataset
? we do not carry out an inter-annotator agreement study here. We eliminate any lemma for which we
find evidence of the novel sense in the usages from the BNC, or for which we do not find evidence of the
novel sense in the ukWaC sample usages.
5
This process resulted in the identification of two lemmas not in the dataset of Lau et al., with frequency
greater than 1000 in the ukWaC sample, and having a novel sense in the ukWaC compared to the BNC
(feed (n) and visit (v)). Combining these new lemmas with the dataset of Lau et al. gives an expanded
dataset consisting of seven lemmas. For both of the two new lemmas, a second annotator annotated
the sample of 100 usages from the ukWaC. The observed agreement and unweighted Kappa for this
annotation task for all seven lemmas is 97.4% and 0.93, respectively, indicating that this is indeed a
relatively easy annotation task. The annotators discussed the small number of disagreements to reach
2
We access the dictionaries in the same way as Lau et al., namely we use COD08 online via http://oxfordreference.
com, and the paper version of COD95.
3
http://www.sketchengine.co.uk/
4
We examine word sketches for the full ukWaC because this version of the corpus is available through the Sketch Engine.
5
We use the IMS Open Corpus Workbench (http://cwb.sourceforge.net/) to extract usages of our target lemmas
from the corpora. This extraction process fails in a number of cases, and so we also eliminate such items from our dataset.
1626
BNC?ukWaC
Lemma Frequency Novel sense definition
domain (n) 41 Internet domain
export (v) 28 export data
feed (n) 23 data feed
mirror (n) 10 mirror website
poster (n) 4 one who posts online
visit (v) 28 access a website
worm (n) 30 malicious program
SiBol/Port
Lemma Frequency Novel sense definition
cloud (n) 9 Internet-based computational resources
drag (v) 1 move on a computer screen using a mouse
follower (n) 34 Twitter follower
help (n) 1 displayed instructions, e.g., help menu
hit (n) 2 search hit
platform (n) 22 computing platform
poster (n) 5 one who posts online
reader (n) 3 e-reader
rip (v) 1 copy music
site (n) 39 website
text (n) 39 text message
visit (v) 7 access a website
wall (n) 2 Facebook wall
Table 1: Lemmas in the BNC?ukWaC and SiBol/Port datasets. For each lemma, the frequency of its
novel sense in the annotated sample of usages from the focus corpus, and a definition of its novel sense,
are shown.
consensus. The seven lemmas in this dataset are shown in Table 1, along with definitions of their novel
senses, and the frequencies of their novel senses in the focus corpus.
Lau et al. compared the novelty of the lemmas with a novel sense to that of a same-size set of distractor
lemmas not having a novel sense. Here we consider a much larger set of 50 distractors ? 25 nouns and
25 verbs ? randomly sampled from a similar frequency range as the items with a novel sense.
One shortcoming of this dataset (and indeed the subset of it used by Lau et al.) is that text types are
represented to different extents in the BNC and ukWaC, with, for example, texts related to the Internet
being much more common in the ukWaC. Such differences in corpus composition are a noted challenge
for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010). In the
following subsection we therefore consider the creation of a new dataset from more-comparable corpora.
3.2 SiBol/Port
The SiBol/Port Corpus consists of texts from several British newspapers for the years 1993, 2005, and
2010; we use the 1993 and 2010 portions of this corpus ? referred to as SP1993 and SP2010 ? as
our reference and focus corpora, respectively. SP1993 and SP2010 contain approximately 93M and 99M
words, respectively. In contrast to BNC?ukWaC, our reference and focus corpora are now comparable,
in that they both consist of texts from British newspapers but they differ with respect to the specific year.
The novel word-senses in the BNC?ukWaC dataset are all related to computers and the Internet, but
there has been recent lexical semantic change unrelated to technology as well (e.g., sick can be used to
mean ?excellent?). In an effort to include such non-technical novel senses in this new dataset, we obtain
a list of headwords for which a sense was added to the Macmillan English Dictionary for Advanced
1627
Learners (MEDAL)
6
since its first edition (Rundell and Fox, 2002), courtesy of Macmillan Dictionaries.
Beginning with these candidates from MEDAL, and the items extracted from COD from Section 3.1, we
discard any lemma whose frequency is less than 1000 in SP1993 or SP2010.
As for the BNC?ukWaC dataset, an annotator examined word sketches for these lemmas. However, it
is possible that the novel sense for a lemma is present in a corpus, but that we fail to find evidence for it in
that lemma?s word sketch. We therefore also obtain judgements from two annotators as to whether each
novel sense is expected to be very infrequent (or unattested) in SP2010. To reduce subsequent annotation
effort, we discard any lemma for which its novel sense is believed to be infrequent in SP2010 by both
judges, and is not found in the word sketch from SP2010.
Annotators then annotate a random sample of 100 usages of each lemma in the reference and focus
corpora as before, and again eliminate any lemma for which we find evidence of its novel sense in the
reference corpus, or fail to find evidence of that sense in the focus corpus. We identify thirteen lemmas
having a novel sense in SP2010 relative to SP1993. These lemmas are also shown in Table 1.
We obtain a second set of annotations for the usages of these lemmas in the sample from SP2010,
with each lemma being annotated by a different annotator than before. The observed agreement and
unweighted Kappa between the two sets of annotations is 96.2% and 0.81, respectively. In cases of
disagreement, a final annotation is again reached through discussion.
We randomly select 164 lemmas (116 nouns and 48 verbs) from a similar frequency range as the
lemmas having a novel sense, to serve as distractors.
Both the BNC?ukWaC and SiBol/Port datasets have been made available.
7
4 The WSI-based approach to novel word-sense detection
In this section we describe the WSI-based method of Lau et al. (2012) for detecting novel senses, and an
extension of this method from Cook et al. (2013). We then present new extensions of this method.
The Lau et al. (2012) WSI model is based on a Hierarchical Dirichlet Process (HDP, Teh et al., 2006),
which is a non-parametric variant of a topic model that, like the commonly-used Latent Dirichlet Allo-
cation (LDA, Blei et al., 2003), learns topics (in the form of multinomial probability distributions over
words) and per-document topic assignments (in the form of multinomial probability distributions over
topics) for a collection of documents; unlike LDA, however, it also optimises the number of topics in an
unsupervised data-driven manner. In the context of WSI, by creating ?documents? that consist of sen-
tences containing a target word, we can view the topics learnt by topic models as the sense representation
of the target word. Indeed, topic models have been previously applied to WSI (e.g., Brody and Lapata,
2009; Yao and Van Durme, 2011).
To generate the input for the topic model, the documents are tokenised (in this case, a ?document? is
a short context, typically 1?3 sentences, containing a target word) into a bag of words. All words are
lemmatised, and stopwords and low frequency terms are removed. Positional word features ? commonly
used in WSI ? for each of the three words to the left and right of the target word are also included.
To induce the senses of a target word w from a given set of usages of w, HDP is run on those usages
(represented according to the features described above) to induce topics; these topics are then interpreted
as representing the senses of w (one topic per sense). To determine the sense assigned to each instance,
the system aggregates over the topic assignments for each word in the context of w, and selects the topic
with the highest aggregated probability, i.e., argmax
z
P (t = z|d), where d is a document and t is a topic.
Recently, Lau et al. (2013a,b) found this method to give the overall best performance on two WSI
shared tasks (Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013), demonstrating that the method
is competitive with the state-of-the-art in WSI, and appropriate as the basis for a method for identifying
novel word-senses.
6
http://www.macmillandictionary.com/
7
http://www.csse.unimelb.edu.au/
~
tim/etc/novel-sense-dataset.tgz
1628
4.1 Novel Sense Detection
Following Lau et al. (2012), to detect novel senses of a target word using this WSI method, we jointly
topic model two corpora: a reference corpus ? taken to represent standard usage ? and a focus corpus
of newer texts potentially containing novel senses. In other words, we extract usages of a target word w
from both corpora, and then topic model the pooled instances of w. Under this approach, the discovered
topics are applicable to both corpora, so there is no need to reconcile two different sets of topics. For the
experiments in this paper, we extract three sentences of context for each usage, one sentence to either
side of the usage of the target word.
As each usage is given a sense assignment, we can identify novel senses ? senses present in the focus
corpus, but unattested in the reference corpus ? based on differences in the sense distribution for a given
word between the two corpora. Lau et al. present a Novelty score which is proportional to the following:
Novelty
Ratio
(s) =
p
f
(s)
p
r
(s)
(1)
where p
f
(s) and p
r
(s) are the proportion of usages of a given word corresponding to sense s in the focus
corpus and reference corpus, respectively, calculated using smoothed maximum likelihood estimates.
The score for a given lemma is the maximum score for any of its induced senses. We refer to the novel
sense for a lemma as the induced sense corresponding to this maximum.
4.2 Alternative Formulations of Novelty
The WSI system underlying the approach of Lau et al. labels each usage of a target lemma with an
induced sense. Therefore, any approach to identifying keywords ? words that are substantially more
frequent in one corpus than another ? can potentially be applied to identify novel senses, by viewing
?words? as (word,sense) tuples. We consider a version of Novelty based on the difference in relative
frequency of an induced sense in the focus and reference corpora, as below:
Novelty
Diff
(s) = p
f
(s)? p
r
(s) (2)
We consider a further new variant of Novelty based on the log-likelihood ratio of an induced sense in the
two corpora, referred to as Novelty
LLR
.
4.3 Incorporating knowledge of expected topics of novel senses
Cook et al. (2013) extended Lau et al.?s method by incorporating the observation that many neologisms
are related to topics that are culturally salient (e.g., Ayto, 2006); nowadays we see many neologisms
related to computing and the Internet. Indeed this observation was used to construct the gold-standard
dataset for this study. Cook et al. identified a set of words, W , related to computing and the Inter-
net, based on manual analysis of keywords for the corpora they considered. They then formulated the
Relevance of an induced sense s for a given word as follows:
Relevance
Manual
(s) =
?
w?W
p(w|s) (3)
For a given lemma, Relevance
Manual
is the maximum of this score for any of its induced senses, similar
to Novelty.
Following Cook et al., we calculate Relevance and Novelty for each induced sense of each lemma,
and then rank all the induced senses by these measures independently. We then compute the rank sum
of each induced sense of each lemma under these two rankings. The final score for a given lemma is
then the rank sum of its highest-ranked sense, and this sense is taken as that lemma?s novel sense. We
refer to this new method as ?Rank Sum?. Cook et al. only considered Novelty and Rank Sum; here we
additionally consider Relevance on its own.
For the keywords, we manually construct a set of words related to computing and the Internet, the
topics for which we expect to observe many novel senses in both of our datasets, in a similar way to
Cook et al. In order to minimize annotation effort, we concentrate on words that are more-frequent in the
1629
focus corpus than the reference corpus. For a given corpus pair, we begin by computing the keywords
for those corpora using Kilgarriff?s (2009) method.
8
Two annotators ? both computational linguists
and not authors of this paper ? independently scanned the top-1000 keywords for the focus corpus, and
selected those that were, based on their intuition, related to computing and the Internet. We then took
the topically-relevant words for a given corpus pair to be those in the intersection of the sets of words
selected by the two annotators. For BNC?ukWaC and SiBol/Port this gives 102 and 30 topically-relevant
words, respectively. This annotation required, on average, 23 minutes per annotator per corpus pair to
complete. Examples of the keywords selected for SiBol/Port include broadband, click, device, online,
and tweet.
4.4 Automatically-extracting keywords
We propose a new fully-automated method for identifying a set of topically-relevant keywords. Because
of the differences in corpus composition, the BNC?ukWaC keywords are often related to computing and
the Internet. To automatically obtain topically-relevant words, we take the top-1000 keywords for the
ukWaC relative to the BNC (i.e., the same keywords annotated for the BNC?ukWaC in Section 4.3).
The keywords for SiBol/Port are less-clearly related to the topics of interest, so we therefore use the
topically-relevant keywords from BNC?ukWaC for both datasets.
5 Results
In the following subsections we consider results at the type and then token level.
5.1 Type-level results
In these experiments we rank all items ? lemmas with a novel sense, and distractors ? by the various
Novelty, Relevance and Rank Sum methods for the BNC?ukWaC and SiBol/Port datasets. When a
lemma takes on a new sense, it might also increase in frequency. We therefore also consider a baseline in
which we rank the lemmas by the ratio of their frequency in the focus corpus and the reference corpus.
This baseline has not been previously considered by Lau et al. (2012) or Cook et al. (2013).
To compare approaches, we examine precision?recall curves in Figures 1 and 2. In an applied setting,
we envision these ranked lists being manually examined; we are therefore primarily interested in the
highly-ranked items, i.e., the left portion of the precision?recall curves.
For BNC?ukWaC (Figure 1), Novelty
Diff
and Novelty
Ratio
perform much better than Novelty
LLR
, but
not better than the frequency ratio baseline, at least for the left-most portion of the precision?recall
curve. Surprisingly, for Relevance, Relevance
Auto
outperforms Relevance
Manual
. This could be because
the focus corpus exhibits a clear topical bias towards computing and the Internet (the expected domain
of many neologisms in the focus corpus), and therefore a larger set of potentially noisy keywords is
more informative than a smaller, hand-selected set. All of the measures including the baseline, except
for Novelty
LLR
, assign higher scores to lemmas with a gold-standard novel sense than the distractors,
according to a one-sided Wilcoxon rank sum test (p < 0.05 in each case).
Turning to SiBol/Port in Figure 2, the frequency ratio baseline is much less effective here; the fre-
quency of the gold-standard novel senses is much lower overall than for BNC?ukWaC. All of the Novelty
and Relevance methods outperform the baseline, and ? with the exception of Novelty
Ratio
? rank the
lemmas with a gold-standard novel sense higher than the distractors (again using a one-sided Wilcoxon
rank sum test and p < 0.05). Furthermore, in this case, Relevance
Manual
outperforms Relevance
Auto
, as
expected.
In terms of the three Novelty measures, only Novelty
Diff
ranked items with a novel sense higher than
the distractors for both datasets. We therefore also show results for the Rank Sum approach combin-
ing Novelty
Diff
and each of Relevance
Manual
and Relevance
Auto
, denoted Rank Sum
Diff,manual
and Rank
Sum
Diff,auto
, respectively, in Figures 1 and 2. For both BNC?ukWaC and SiBol/Port, Rank Sum
Diff,manual
8
Using this method, the keywordness score for a given word is simply the ratio of its frequency per million words, plus a
constant, in two corpora; we set the constant to 100, the value recommended by Kilgarriff.
1630
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 1: Precision?recall curve for the BNC?ukWaC dataset.
gives the best performance, and is a clear improvement over either of the individual methods. As ex-
pected, the performance of Rank Sum
Diff,auto
is not as good, but is nevertheless an improvement over the
frequency ratio baseline for both datasets and provides an alternative to manual scrutiny of the keywords.
To further examine the potential of incorporating knowledge of the expected domains of novel senses
to improve novel sense identification, we consider the case of cloud (n) from the SiBol/Port dataset. The
highest-probability words for the topic with highest Novelty
Diff
are the following: ash, volcanic, flight,
@card@,
9
travel, airline, volcano, airport, air, cloud. This sense appears to be related to the eruption
of the Eyjafjallajo?kull volcano, a major event in 2010 (the year from which the SiBol/Port focus corpus
is taken). Such topical differences, which do not correspond to a novel sense, are a problem for any
approach to identifying lexical semantic differences between two corpora based on differences in the
lexical context of a target word, and indeed observations such as this motivated our use of the methods
incorporating Relevance. The highest probability words for the topic with highest Relevance
Auto
are
the following: cloud, @card@, company, service, business, computing, market, security, datum, need.
This topic appears to correspond to the expected novel sense of Internet-based computational resources,
demonstrating the potential to improve a system for identifying novel word-senses by incorporating
knowledge of the expected domains of neologisms. Moreover, incorporating Relevance is particularly
powerful for avoiding false positives. For example, the distractor clause (n) is the lemma with the
sixth-highest Novelty
Diff
for SiBol/Port. The highest probability words for the corresponding topic are
the following: contract, @card@, club, player, million, england, capello, manager, sign, deal. This
induced sense appears to be related to clauses in Fabio Capello?s contract as manager of the England
national football team, and is not a novel sense of clause. However, none of the induced senses of clause
have high Relevance
Auto
or Relevance
Manual
, and so incorporating information from Relevance can avoid
incorrectly identifying this lemma as having a novel sense.
9
A generic token signifying a cardinal number.
1631
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 2: Precision?recall curve for the SiBol/Port dataset.
5.2 Token-level results
In this section, we consider the token-level identification of instances of the gold-standard novel senses.
We compare Novelty, Relevance, and Rank Sum to a baseline that assigns all usages of a lemma to a
single topic which is selected as the novel sense; in this case recall is 1, and precision is proportional to
the frequency of the novel sense. We further consider the theoretical upper-bound of a method which
selects a single topic as the novel sense, based on the output of the HDP-based WSI method; this oracle
selects the best topic in terms of F-score as the novel sense. Results are presented in Table 2.
Each variant of Novelty and Relevance is an improvement over the baseline, although the Relevance
measures don?t perform as well as the Novelty ones, despite this dataset only containing novel senses
related to computing (despite our efforts to include non-technical novel senses). For consistency with
the presentation of the type-level results, we again consider Rank Sum using Novelty
Diff
, even though it
doesn?t perform as well as Novelty
LLR
or Novelty
Ratio
on BNC?ukWaC. Using either automatically- or
manually-obtained keywords, the performance of Rank Sum on BNC?ukWaC is remarkably on par with
the upper-bound, although for SiBol/Port there is little or no improvement over Novelty
Diff
. Neverthe-
less, these findings are further indication that novel sense identification can be improved by incorporating
information about the topics for which we expect to see novel senses. However, this approach is par-
ticularly helpful at the type-level, where information about the expected topics of novel senses prevents
lemmas not having a novel sense (i.e., the distractors) from being assigned high novelty.
6 Discussion and conclusion
The methods considered in this paper could be applied to any corpus pair, and potentially to identify
lexical semantic differences between, for example, domains or language varieties. The focus of this
study is English; sufficiently-large comparable corpora of national varieties of English (e.g., British and
American English), are not readily-available, but could potentially be inexpensively constructed in the
future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports
1632
Method
F-score
BNC?ukWaC SiBol/Port
Novelty
Diff
0.57 0.29
Novelty
LLR
0.67 0.28
Novelty
Ratio
0.66 0.28
Relevance
Auto
0.48 0.24
Relevance
Manual
0.45 0.27
Rank Sum
Diff,auto
0.72 0.30
Rank Sum
Diff,manual
0.72 0.29
Upper-bound 0.72 0.42
Baseline 0.36 0.20
Table 2: Token-level F-score for the BNC?ukWaC and SiBol/Port datasets using variants of Novelty,
Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown.
and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed
very high Novelty
Ratio
for many distractors (selected in a similar way to our other experiments). Unlike
the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to
cooccur with very different words in the corpora, and Novelty
Ratio
will consequently be high. To address
vocabulary differences between corpora, in their experiments on identifying lexical semantic differences
between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word
to those with moderate frequency in each of the two corpora used. We considered a similar restriction in
experiments on SiBol/Port, but did not see an overall improvement in performance.
We demonstrated that the performance of a method for identifying novel word-senses can be improved
by incorporating information ? acquired manually or automatically ? about the expected topics of
novel senses, which tend to be related to culturally-salient concepts. In future work, we intend to consider
improved approaches for automatically identifying topically-relevant words by incorporating information
about the top keywords of a corpus harvested from the Web for the domain of interest (e.g., PVS et al.,
2012). We also believe that topic models could be useful for identifying emerging or changing domains
themselves given the reference and focus corpus, and related work in this area (e.g., Wang andMcCallum,
2006; Blei and Lafferty, 2007).
To conclude, we have presented the largest type- and token-level dataset of diachronic sense differ-
ences to date, drawing on two pairs of corpora, and have made this dataset available. We applied a
recently-proposed WSI-based method to the task of finding sense differences in this data. We demon-
strated that, while the method shows promise, on a type-based task it is comparable to a a simple fre-
quency baseline, which had not been previously considered for this task. We carried out the first empirical
evaluation of a recently-proposed extension of this method that incorporates manually-acquired knowl-
edge of the expected domains of new senses, and found it to have superior performance at both the type
and token level. We further proposed and evaluated an approach that only uses this domain knowledge,
and a method for automating its acquisition.
Acknowledgments
We thank Michael Rundell and Macmillan Dictionaries for providing the list of headwords added to
MEDAL since its first edition, and Charlotte Taylor for providing us with early access to SiBol/Port. We
also thank Richard Fothergill, Karl Grieser, and Andrew Mackinlay for their help in annotation. This
research was supported in part by funding from the Australian Research Council.
References
John Ayto. 2006. Movers and Shakers: A Chronology of Words that Shaped our Age. Oxford University
Press, Oxford.
1633
David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1?10. Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research,
3:993?1022.
David M. Blei and John D. Lafferty. 2007. Latent dirichlet allocation. The Annals of Applied Statistics,
1(1):17?35.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?111. Athens, Greece.
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford
University Computing Services.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages
1435?1445. Sofia, Bulgaria.
Paul Cook and Graeme Hirst. 2011. Automatic identification of words with novel but infrequent
senses. In Proceedings of the 25th Pacific Asia Conference on Language Information and Compu-
tation (PACLIC 25), pages 265?274. Singapore.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties
of English? In Actes des 11es Journ
?
ees Internationales d?Analyse Statistique des Donn
?
ees Textuelles /
Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages 281?293.
Lie`ge, Belgium.
Paul Cook, Jey Han Lau, Michael Rundell, Diana McCarthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for detecting new word-senses. In Electronic lexicography
in the 21st century: thinking outside the paper. Proceedings of the eLex 2013 conference, pages 49?65.
Tallinn, Estonia.
Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh International Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34. Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evalu-
ating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54. Marrakech, Morocco.
Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of
semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics, pages 67?71. Edinburgh, Scotland.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-2013 task 13: Word sense induction for graded
and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation
(SemEval 2013), pages 290?299. Atlanta, USA.
Adam Kilgarriff. 2009. Simple maths for keywords. In Proceedings of the Corpus Linguistics Confer-
ence. Liverpool, UK.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceed-
ings of the Eleventh EURALEX International Congress (EURALEX 2004), pages 105?116. Lorient,
France.
Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predom-
inant sense acquisition. In Proceedings of Human Language Technology Conference and Conference
1634
on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 419?426. Van-
couver, Canada.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a. unimelb: Topic modelling-based word sense
induction. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),
pages 307?311. Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b. unimelb: Topic modelling-based word sense
induction for web snippet clustering. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221. Atlanta, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense
induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter
of the Association for Computational Linguistics (EACL 2012), pages 591?601. Avignon, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics, 33(4):553?590.
Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 task 11: Word sense induction and dis-
ambiguation within an end-user application. In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?201. Atlanta, USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language Engineering, 16(4):469?491.
Avinesh PVS, Diana McCarthy, Dominic Glennon, and Jan Pomika?lek. 2012. Domain specific corpora
from the Web. In Proceedings of the 15th Euralex International Congress, pages 336?342. Oslo,
Norway.
Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies (ACL 2011),
pages 305?310. Portland, USA.
Michael Rundell and Gwyneth Fox, editors. 2002. Macmillan English Dictionary for Advanced Learn-
ers. Macmillan Education, Oxford, UK.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word
meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?111. Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing, pages 44?49. Manchester, UK.
Catherine Soanes and Angus Stevenson, editors. 2008. The Concise Oxford English Dictionary. Oxford
University Press, Oxford, UK, eleventh (revised) edition. Oxford Reference Online.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Association, 101:1566?1581.
Della Thompson, editor. 1995. The Concise Oxford Dictionary of Current English. Oxford University
Press, Oxford, UK, ninth edition.
Xuerei Wang and Andrew McCallum. 2006. Topics over time: A non-Markov continuous-time model of
topical trends. In Proceedings of the Eleventh International Conference on Knowledge Discovery and
Data Mining, pages 424?433. Philadelphia, USA.
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10?14.
Portland, USA.
1635
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694?704,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Unsupervised Parse Selection for HPSG
Rebecca Dridan and Timothy Baldwin
Dept. of Computer Science and Software Engineering
University of Melbourne, Australia
rdridan@csse.unimelb.edu.au, tb@ldwin.net
Abstract
Parser disambiguation with precision gram-
mars generally takes place via statistical rank-
ing of the parse yield of the grammar using
a supervised parse selection model. In the
standard process, the parse selection model is
trained over a hand-disambiguated treebank,
meaning that without a significant investment
of effort to produce the treebank, parse selec-
tion is not possible. Furthermore, as treebank-
ing is generally streamlined with parse selec-
tion models, creating the initial treebank with-
out a model requires more resources than sub-
sequent treebanks. In this work, we show that,
by taking advantage of the constrained nature
of these HPSG grammars, we can learn a dis-
criminative parse selection model from raw
text in a purely unsupervised fashion. This al-
lows us to bootstrap the treebanking process
and provide better parsers faster, and with less
resources.
1 Introduction
Parsing with precision grammars is generally a two-
stage process: (1) the full parse yield of the preci-
sion grammar is calculated for a given item, often
in the form of a packed forest for efficiency (Oepen
and Carroll, 2000; Zhang et al, 2007); and (2) the
individual analyses in the parse forest are ranked us-
ing a statistical model (?parse selection?). In the do-
main of treebank parsing, the Charniak and Johnson
(2005) reranking parser adopts an analogous strat-
egy, except that ranking and pruning are incorpo-
rated into the first stage, and the second stage is
based on only the top-ranked parses from the first
stage. For both styles of parsing, however, parse se-
lection is based on a statistical model learned from a
pre-existing treebank associated with the grammar.
Our interest in this paper is in completely remov-
ing this requirement of parse selection on explicitly
treebanked data, ie the development of fully unsu-
pervised parse selection models.
The particular style of precision grammar we ex-
periment with in this paper is HPSG (Pollard and
Sag, 1994), in the form of the DELPH-IN suite
of grammars (http://www.delph-in.net/).
One of the main focuses of the DELPH-IN collab-
oration effort is multilinguality. To this end, the
Grammar Matrix project (Bender et al, 2002) has
been developed which, through a set of question-
naires, allows grammar engineers to quickly pro-
duce a core grammar for a language of their choice.
Bender (2008) showed that by using and expanding
on this core grammar, she was able to produce a
broad-coverage precision grammar of Wambaya in
a very short amount of time. However, the Gram-
mar Matrix can only help with the first stage of pars-
ing. The statistical model used in the second stage
of parsing (ie parse selection) requires a treebank to
learn the features, but as we explain in Section 2, the
treebanks are created by parsing, preferably with a
statistical model. In this work, we look at methods
for bootstrapping the production of these statistical
models without having an annotated treebank. Since
many of the languages that people are building new
grammars for are under-resourced, we can?t depend
on having any external information or NLP tools,
and so the methods we examine are purely unsuper-
vised, using nothing more than the grammars them-
694
selves and raw text. We find that, not only can we
produce models that are suitable for kick-starting the
treebanking process, but the accuracy of these mod-
els is comparable to parsers trained on gold standard
data (Clark and Curran, 2007b; Miyao and Tsujii,
2008), which have been successfully used in appli-
cations (Miyao et al, 2008).
2 The problem
The current method of training a parse selection
model uses the [incr tsdb()] treebanking mechanism
(Oepen, 2001) and works well for updating models
for mature grammars, although even for these gram-
mars, building a new model for a different domain
requires a time-consuming initial treebanking effort.
The treebanks used with DELPH-IN grammars are
dynamic treebanks (Oepen et al, 2004) created by
parsing text and having an annotator select the cor-
rect analysis (or discard all of them). The annotation
process involves making binary decisions based on
so-called parse discriminants (Carter, 1997). When-
ever the grammar is changed, the treebank can be
quickly updated by re-parsing and re-applying the
old annotation decisions. This treebanking process
not only produces gold standard trees, but also a set
of non-gold trees which provides the negative train-
ing data necessary for a discriminative maximum en-
tropy model.
The standard process for creating a parse selection
model is:
1. parse the training set, recording up to 500
highest-ranking parses for each sentence;
2. treebank the training set;
3. extract features from the gold and non-gold
parses;
4. learn feature weights using the TADM toolkit.1
(Malouf, 2002)
The useful training data from this process is the
parses from those sentences for which: more than
one parse was found; and at least one parse has been
annotated as correct. That is, there needs to be both
gold and non-gold trees for any sentence to be used
in training the discriminative model.
1http://tadm.sourceforge.net/
There are two issues with this process for new
grammars. Firstly, treebanking takes many person-
hours, and is hence both time-consuming and ex-
pensive. Complicating that is the second issue: N -
best parsing requires a statistical model. While it is
possible to parse exhaustively with no model, pars-
ing is much slower, since the unpacking of results
is time-consuming. Selective unpacking (Zhang et
al., 2007) speeds this up a great deal, but requires
a parse selection model. Treebanking is also much
slower when the parser must be run exhaustively,
since there are usually many more analyses to man-
ually discard.
This work hopes to alleviate both problems. By
producing a statistical model without requiring hu-
man treebanking, we can have a working and effi-
cient parser with less human effort. Even if the top-
1 parses this parser produces are not as accurate as
those trained on gold standard data, this model can
be used to produce the N -best analyses for the tree-
banker. Since our models are much better than ran-
dom selection, we can afford to reduce N and still
have a reasonable likelihood that the correct parse
is in that top N , making the job of the treebanker
much faster, and potentially leading to even better
parse selection accuracy based on semi-supervised
or fully-supervised parse selection.
3 Data and evaluation
Our ultimate goal is to use these methods for under-
resourced languages but, since there are no pre-
existing treebanks for these languages, we have no
means to measure which method produces the best
results. Hence, in this work, we experiment with
languages and grammars where we have gold stan-
dard data, in order to be able to evaluate the qual-
ity of the parse selection models. Since we have
gold-standard trained models to compare with, this
enables us to fully explore how these unsupervised
methods work, and show which methods are worth
trying in the more time-consuming and resource-
intensive future experiments on other languages. It
is worth reinforcing that the gold-standard data is
used for evaluation only, except in calculating the
supervised parse selection accuracy as an upper-
bound.
The English Resource Grammar (ERG:
695
Language Sentences Average Averagewords parses
Japanese 6769 10.5 49.6
English 4855 9.0 59.5
Table 1: Initial model training data, showing the average
word length per sentence, and also the ambiguity mea-
sured as the average number of parses found per sentence.
Flickinger (2002)) is an HPSG-based grammar
of English that has been under development for
many person years. In order to examine the
cross-lingual applicability of our methods, we also
use Jacy, an HPSG-based grammar of Japanese
(Siegel and Bender, 2002). In both cases, we use
grammar versions from the ?Barcelona? release,
from mid-2009.
3.1 Training Data
Both of our grammars come with statistical models,
and the parsed data and gold standard annotations
used to create these models are freely available. As
we are trying to simulate a fully unsupervised setup,
we didn?t want any influence from these earlier mod-
els. Hence, in our experiments we used the parsed
data from those sentences that received less than 500
parses and ignored any ranking, thus annulling the
effects of the statistical model. This led to a re-
duced data set, both in the number of sentences, and
in the fact that the more ambiguous sentences were
discarded, but it allows clean comparison between
different methods, without incorporating external in-
formation. The details of our training sets are shown
in Table 1,2 indicating that the sentence lengths are
relatively short, and hence the ambiguity (measured
as average parses per sentence) is low for both our
grammars. The ambiguity figures also suggest that
the Japanese grammar is more constrained (less am-
biguous) than the English grammar, since there are,
on average, more parses per sentence for English,
even with a lower average sentence length.
3.2 Test Data
The test data sets used throughout our experiments
are described in Table 2. The tc-006 data set is from
2Any sentences that do not have both gold and non-gold
analyses (ie, had no correct parse, only one parse, or none) are
not included in these figures.
Test Set Language Sentences Average Averagewords parses
tc-006 Japanese 904 10.7 383.9
jhpstgt English 748 12.8 4115.1
catb English 534 17.6 9427.3
Table 2: Test data, showing the average word length per
sentence, and also the ambiguity measured as the average
number of parses found per sentence. Note that the ambi-
guity figures for the English test sets are under-estimates,
since some of the longer sentences timed out before giv-
ing an analysis count.
the same Tanaka Corpus (Tanaka, 2001) which was
used for the Japanese training data. There is a wider
variety of treebanked data available for the English
grammar than for the Japanese. We use the jhp-
stgt data set, which consists of text from Norwegian
tourism brochures, from the same LOGON corpus
as the English training data (Oepen et al, 2004). In
order to have some idea of domain effects, we also
use the catb data set, the text of an essay on open-
source development.3 We see here that the sentences
are longer, particularly for the English data. Also,
since we are not artificially limiting the parse am-
biguity by ignoring those with 500 or more parses,
the ambiguity is much higher. This ambiguity figure
gives some indication of the difficulty of the parse
selection task. Again we see that the English sen-
tences are more ambiguous, much more in this case,
making the parse selection task difficult. In fact,
the English ambiguity figures are an under-estimate,
since some of the longer sentences timed out before
producing a parse count. This ambiguity can be a
function of the sentence length or the language it-
self, but also of the grammar. A more detailed and
informative grammar makes more distinctions, not
all of which are relevant for every analysis.
3.3 Evaluation
The exact match metric is the most common accu-
racy metric used in work with the DELPH-IN tool
set, and refers to the percentage of sentences for
which the top parse matched the gold parse in every
way. This is akin to the sentence accuracy that is oc-
casionally reported in the parsing literature, except
3The Cathedral and the Bazaar, by Eric Raymond.
Available from: http://catb.org/esr/writings/
cathedral-bazaar/
696
that it also includes fine-grained syntactico-semantic
features that are not often present in other parsing
frameworks. Exact match is a useful metric for parse
selection evaluation, but it is very blunt-edged, and
gives no way of evaluating how close the top parse
was to the gold standard. Since these are very de-
tailed analyses, it is possible to get one detail wrong
and still have a useful analysis. Hence, in addition
to exact match, we also use the EDMNA evalua-
tion defined by Dridan (2009). This is a predicate?
argument style evaluation, based on the semantic
output of the parser (MRS: Minimal Recursion Se-
mantics (Copestake et al, 2005)). This metric is
broadly comparable to the predicate?argument de-
pendencies of CCGBank (Hockenmaier and Steed-
man, 2007) or of the ENJU grammar (Miyao and
Tsujii, 2008), and also somewhat similar to the
grammatical relations (GR) of the Briscoe and Car-
roll (2006) version of DepBank. The EDMNA met-
ric matches triples consisting of predicate names and
the argument type that connects them.4
4 Initial Experiments
All of our experiments are based on the same basic
process: (1) for each sentence in the training data
described in Section 3.1, label a subset of analyses
as correct and the remainder as incorrect; (2) train
a model using the same features and learner as in
the standard process of Section 2; (3) parse the test
data using that model; and (4) evaluate the accuracy
of the top analyses. The differences lay in how the
?correct? analyses are selected each time. Each of
the following sections detail different methods for
nominating which of the (up to 500) analyses from
the training data should be considered pseudo-gold
for training the parse selection model.
4.1 Upperbound and baseline models
As a first step we evaluated each data set using an
upperbound and a baseline model. The upperbound
model in this case is the model trained with gold
standard annotations. These accuracy figures are
slightly lower than others found in the literature for
this data, since, to allow for comparison, we lim-
ited the training data to the sets described in Table 1.
4The full EDM metric also includes features such as tense
and aspect, but this is less comparable to the other metrics men-
tioned.
Test Set Exact EDMMatch Precision Recall F-score
tc-006 72.90 0.961 0.957 0.959
jhpstgt 48.07 0.912 0.908 0.910
catb 22.29 0.838 0.839 0.839
Table 3: Accuracy of the gold standard-based parse se-
lection model.
Test Set Exact EDMMatch Precision Recall F-score
tc-006 17.26 0.779 0.839 0.807
jhpstgt 12.48 0.720 0.748 0.734
catb 8.30 0.646 0.698 0.671
Table 4: Accuracy of the baseline model, trained on ran-
domly selected pseudo-gold analyses.
By throwing out those sentences with more than 500
parses, we exclude much of the data that is used in
the standard model and so our exact match figures
are slightly lower than might be expected.
For the baseline model, we used random selection
to select our gold analyses. For this experiment, we
randomly assigned one parse from each sentence in
the training data to be correct (and the remainder of
analyses as incorrect), and then used that ?gold stan-
dard? to train the model. Results for the upperbound
and baseline models are shown in Tables 3 and 4.
As expected, the results for Japanese are much
higher, since the lower ambiguity makes this an eas-
ier task. The catb test set results suffer, not only
from being longer, more ambiguous sentences, but
also because it is completely out of the domain of
the training data.
The exact match results from the random baseline
are approximately what one might expect, given the
respective ambiguity levels in Table 2. The EDM
figures are perhaps higher than might be expected
given random selection from the entire parse forest.
This results from using a precision grammar, with
an inbuilt notion of grammaticality, hence constrain-
ing the parser to only produce somewhat reasonable
parses, and creating a reasonably high baseline for
our parse selection experiments.
We also tried a separate baseline, eliminating the
parse selection model altogether, and using random
selection directly to select the top analysis. The ex-
act match and EDM precision results were slightly
lower than using random selection to train a model,
697
which may be due to the learner giving weight to
features that are common across the training data,
but the differences weren?t significant. Recall was
significantly lower when using random selection di-
rectly, due to the time outs caused by running with-
out a model. For this reason, we use the random
selection-based model results as our baseline for the
other unsupervised parse selection models, noting
that correctly identifying approximately three quar-
ters of the dependencies in the jhpstgt set, and over
80% when using the Japanese grammar, is a fairly
high baseline.
4.2 First attempts
As a first approach to unsupervised parse selection,
we looked at two heuristics to designate some num-
ber of the analyses as ?gold? for training. Both of
these heuristics looked independently at the parses
of each sentence, rather than calculating any num-
bers across the whole training set.
The first method builds on the observation from
the random selection-based model baseline exper-
iment that just giving weight to common features
could improve parser accuracy. In this case, we
looked at the edges of the parsing chart. For each
sentence, we counted the number of times an edge
was present in an analysis, and used that number
(normalised by the total number of times any edge
was used) as the edge weight. We then calculated
an analysis score by summing the edge weights of
all the edges in that analysis, and dividing by the
number of edges, to give an average edge weight for
an analysis. All analyses that had the best analysis
score for a sentence were designated ?gold?. Since it
was possible for multiple analyses to have the same
score, there could be multiple gold analyses for any
one sentence. If all the analyses had the same score,
this sentence could not be used as part of the train-
ing data. This method has the effect of selecting the
parse(s) most like all the others, by some definitions
the centroid of the parse forest. This has some rela-
tionship to the partial training method described by
Clark and Curran (2006), where the most frequent
dependencies where used to train a model for the
C&C CCG parser. In that case, however, the de-
pendencies were extracted only from analyses that
matched the gold standard supertag sequence, rather
than the whole parse forest.
Test Set Exact Match F-scoreEdges Branching Edges Branching
tc-006 17.48 21.35 0.815 0.822
jhpstgt 15.27 17.53 0.766 0.780
catb 9.36 10.86 0.713 0.712
Table 5: Accuracy for each test set, measured both as per-
centage of sentences that exactly matched the gold stan-
dard, and f-score over elementary dependencies.
The second heuristic we tried is one often used as
a baseline method: degree of right (or left) branch-
ing. In this instance, we calculated the degree of
branching as the number of right branches in a parse
divided by the number of left branches (and vice
versa for Japanese, a predominantly left-branching
language). In the same way as above, we designated
all parses with the best branching score as ?gold?.
Again, this is not fully discriminatory, and it was
common to get multiple gold trees for a given sen-
tence.
Table 5 shows the results for these two methods.
All the results show an improvement over the base-
line, with all but the F-score for the Edges method
of tc-006 being at a level of statistical significance.5
The only statistically significant difference between
the Edges and Branching methods is over the jhp-
stgt data set. While improvement over random is
encouraging, the results were still uninspiring and
so we moved on to slightly more complex methods,
described in the next section.
5 Supertagging Experiments
The term supertags was first used by Bangalore and
Joshi (1999) to describe fine-grained part of speech
tags which include some structural or dependency
information. In that original work, the supertags
were LTAG (Schabes and Joshi, 1991) elementary
trees, and they were used for the purpose of speed-
ing up parsing by restricting the allowable leaf types.
Subsequent work involving supertags has mostly fo-
cussed on this efficiency goal, but they can also be
used to inform parse selection. Dalrymple (2006)
and Blunsom (2007) both look at how discrimina-
tory a tag sequence is in filtering a parse forest. This
5All statistical significance tests in these experiments use the
computationally-intensive randomisation test described in Yeh
(2000), with p < 0.05.
698
work has shown that tag sequences can be success-
fully used to restrict the set of parses produced, but
generally are not discriminatory enough to distin-
guish a single best parse. Toutanova et al (2002)
present a similar exploration but also go on to in-
clude probabilities from a HMM model into the
parse selection model as features. There has also
been some work on using lexical probabilities for
domain adaptation of a model (Hara et al, 2007;
Rimell and Clark, 2008). In Dridan (2009), tag se-
quences from a supertagger are used together with
other factors to re-rank the top 500 parses from the
same parser and English grammar we use in this re-
search, and achieve some improvement in the rank-
ing where tagger accuracy is sufficiently high. We
use a similar method, one level removed, in that we
use the tag sequences to select the ?gold? parse(s)
that are then used to train a model, as in the previous
sections.
5.1 Gold Supertags
In order to test the viability of this method, we first
experimented using gold standard tags, extracted
from the gold standard parses. Supertags come in
many forms, depending on both the grammar for-
malism and the implementation. For this work, we
use HPSG lexical types (lextypes), the native word
classes in the grammars. These lextypes encode part
of speech and subcategorisation information, as well
as some more idiosyncratic features of words, such
as restrictions on preposition forms, mass/count dis-
tinctions and comparative versus superlative forms
of adjectives. As a few examples from the En-
glish grammar, v np le represents a basic transi-
tive verb, while n pp c-of le represents a count
noun that optionally takes a prepositional phrase
complement headed by of. The full definition of a
lextype consists of a many-featured AVM (attribute
value matrix), but the type names have been de-
liberately chosen to represent the main features of
each type. In the Dridan (2009) work, parse ranking
showed some improvement when morphological in-
formation was added to the tags. Hence, we also
look at more fine-grained tags constructed by con-
catenating appropriate morphological rules onto the
lextypes, as in v np le:past verb orule (ie a
simple transitive verb with past tense).
We used these tags by extracting the tag sequence
Test Set Exact Match F-scorelextype +morph lextype +morph
tc-006 40.49 41.37 0.903 0.903
jhpstgt 32.93 32.93 0.862 0.858
catb 20.41 19.85 0.798 0.794
Table 6: Accuracy using gold tag sequence compatibility
to select the ?gold? parse(s).
from the leaf types of all the parses in the forest,
marking as ?gold? any parse that had the same se-
quence as the gold standard parse and then training
the models as before. Table 6 shows the results from
parsing with models based on both the basic lextype
and the lextype with morphology. The results are
promising. They still fall well below training purely
on gold standard data (at least for the in-domain
sets), since the tag sequences are not fully discrimi-
natory and hence noise can creep in, but accuracy is
significantly better than the heuristic methods tried
earlier. This suggested that, at least with a reason-
ably accurate tagger, this was a viable strategy for
training a model. With no significant difference be-
tween the basic and +morph versions of the tag set,
we decided to use the basic lextypes as tags, since
a smaller tag set should be easier to tag with. How-
ever, we first had to train a tagger, without using any
gold standard data.
5.2 Unsupervised Supertagging
Research into unsupervised part-of-speech tagging
with a tag dictionary (sometimes called weakly su-
pervised POS tagging) has been going on for many
years (cf Merialdo (1994), Brill (1995)), but gener-
ally using a fairly small tag set. The only work we
know of on unsupervised tagging for the more com-
plex supertags is from Baldridge (2008), and more
recently, Ravi et al (2010a). In this work, the con-
straining nature of the (CCG) grammar is used to
mitigate the problem of having a much more am-
biguous tag set. Our method has a similar under-
lying idea, but the implementation differs both in
the way we extract the word-to-tag mappings, and
also how we extract and use the information from
the grammar to initialise the tagger model.
We chose to use a simple first-order Hidden
Markov Model (HMM) tagger, using the implemen-
699
tation of Dekang Lin,6 which re-estimates probabil-
ities, given an initial model, using the Baum-Welch
variant of the Expectation-Maximisation (EM) algo-
rithm. One possibility for an initial model was to ex-
tract the word-to-lextype mappings from the gram-
mar lexicon as Baldridge does, and make all starting
probabilities uniform. However, our lexicon maps
between lextypes and lemmas, rather than inflected
word forms, which is what we?d be tagging. That
is to say, from the lexicon we could learn that the
lemma walk can be tagged as v pp* dir le, but
we could not directly extract the fact that therefore
walked should also receive that tag.7 For this rea-
son, we decided it would be simplest to initialise
our probability estimates using the output of the
parser, feeding in only those tag sequences which
are compatible with analyses in the parse forest for
that item. This method takes advantage of the fact
that, because the grammars are heavily constrained,
the parse forest only contains viable tag sequences.
Since parsing without a model is slow, we restricted
the training set to those sentences shorter than a
specific word length (12 for English and 15 for
Japanese, since that was the less ambiguous gram-
mar and hence faster).
Table 7 shows how much parsed data this gave us.
From this parsed data we extracted tag-to-word and
tag-to-tag frequency counts from all parses for all
sentences, and used these frequencies to produce the
emission and transition probabilities, respectively.
The emission probabilities were taken directly from
the normalised frequency counts, but for the tran-
sition probabilities we allow for all possible transi-
tions, and add one to all counts before normalising.
This model we call our initial counts model. The
EM trained model is then produced by starting with
this initial model and running the Baum-Welch al-
gorithm using raw text sentences from the training
corpus.
5.3 Supertagging-based parse selection models
We use both the initial counts and EM trained
models to tag the training data from Table 1 and
then compared this with the extracted tag sequences
6Available from http://webdocs.cs.ualberta.
ca/?lindek/hmm.htm
7Morphological processing occurs before lexicon lookup in
the PET parser.
Japanese English
Parsed Sentences 9760 3770
Average Length 9.63 6.36
Average Parses 80.77 96.29
Raw Sentences 13500 9410
Raw Total Words 146053 151906
Table 7: Training data for the HMM tagger (both the
parsed data from which the initial probabilities were de-
rived, and the raw data which was used to estimated the
EM trained models).
Test Set
Exact Match F-score
Initial EM Initial EM
counts trained counts trained
tc-006 32.85 40.38 0.888 0.898
jhpstgt 26.29 24.04 0.831 0.827
catb 14.61 14.42 0.782 0.783
Table 8: Accuracy using tag sequences from a HMM tag-
ger to select the ?correct? parse(s). The initial counts
model was based on using counts from a parse forest
to approximate the emission and transition probabilities.
The EM trained model used the BaumWelch algorithm to
estimate the probabilities, starting from the initial counts
state.
used in the gold tag experiment. Since we could
no longer assume that our tag sequence would be
present within the extracted tag sequences, we used
the percentage of tokens from a parse whose lextype
matched our tagged sequence as the parse score.
Again, we marked as ?gold? any parse that had the
best parse score for each sentence, and trained a new
parse selection model.
Table 8 shows the results of parsing with these
models. The results are impressive, significantly
higher than all our previous unsupervised methods.
Interestingly, we note that there is no significant
difference between the initial count and EM trained
models for the English data. To explore why this
might be so, we looked at the tagger accuracy for
both models over the respective training data sets,
shown in Table 9. The results are not conclusive. For
both languages, the EM trained model is less accu-
rate, though not significantly so for Japanese. How-
ever, this insignificant tagger accuracy decrease for
Japanese produced a significant increase in parser
accuracy, while a more pronounced tagger accuracy
decrease had no significant effect on parser accuracy
in English.
700
Language Initial counts EM trained
Japanese 84.4 83.3
English 71.7 64.6
Table 9: Tagger accuracy over the training data, using
both the initial counts and the EM trained models.
There is much potential for further work in this
direction, experimenting with more training data or
more estimation iterations, or even looking at dif-
ferent estimators as suggested in Johnson (2007)
and Ravi et al (2010b). There is also the issue of
whether tag accuracy is the best measure for indicat-
ing potential parse accuracy. The Japanese parsing
results are already equivalent to those achieved us-
ing gold standard tags. It is possible that parsing ac-
curacy is reasonably insensitive to tagger accuracy,
but it is also possible that there is a better metric to
look at, such as tag accuracy over frequently con-
fused tags.
6 Discussion
The results of Table 8 show that, using no human
annotated data, we can get exact match results that
are almost half way between our random baseline
and our gold-standard-trained upperbound. EDM F-
scores of 90% and 83% over in-domain data com-
pare well with dependency-based scores from other
parsers, although a direct comparison is very diffi-
cult to do (Clark and Curran, 2007a; Miyao et al,
2007). It still remains to see whether this level of ac-
curacy is good enough to be useful. The main aim of
this work is to bootstrap the treebanking process for
new grammars, but to conclusively show the efficacy
of our methods in that situation requires a long-term
experiment that we are now starting, based on the
results we have here. Another possible use for these
methods was alluded to in Section 2: producing a
new model for a new domain.
Results at every stage have been much worse for
the catb data set, compared to the other jhpstgt En-
glish data set. While sentence length plays some
part, the major reason for this discrepancy was do-
main mismatch between the training and test data.
One method that has been successfully used for do-
main adaption in parsing is self-training (McClosky
et al, 2006). In this process, data from the new do-
main is parsed with the parser trained on the old do-
Source of ?Gold? Data Exact Match F-score
Random Selection 8.30 0.671
Supertags (initial counts) 14.61 0.782
Gold Standard 22.29 0.839
Self-training 15.92 0.791
Table 10: Accuracy results over the out-of-domain catb
data set, using the initial counts unsupervised model to
produce in-domain training data in a self-training set up.
The previous results are shown for easy comparison.
main, and then the top analyses of the parsed new
domain data are added to the training data, and the
parser is re-trained. This is generally considered a
semi-supervised method, since the original parser
is trained on gold standard data. In our case, we
wanted to test whether parsing data from the new do-
main using our unsupervised parse selection model
was accurate enough to still get an improvement us-
ing self-training for domain adaptation.
It is not immediately clear what one might con-
sider to be the ?domain? of the catb test set, since do-
main is generally very vaguely defined. In this case,
there was a limited amount of text available from
other essays by the same author.8 While the topics
of these essays vary, they all relate to the social side
of technical communities, and so we used this to rep-
resent in-domain data for the catb test set. It is, how-
ever, a fairly small amount of data for self-training,
being only around 1000 sentences. We added the re-
sults of parsing this data to the training set we used
to create the initial counts model and again retrained
and parsed. Table 10 shows the results. Previous re-
sults for the catb data set are given for comparison.
The results show that the completely unsuper-
vised parse selection method produces a top parse
that is at least accurate enough to be used in self-
training, providing a cheap means of domain adapta-
tion. In future work, we hope to explore this avenue
of research further.
7 Conclusions and Further Work
Comparing Tables 8 and 4, we can see that for both
English and Japanese, we are able to achieve parse
selection accuracy well above our baseline of a ran-
8http://www.catb.org/esr/writings/
homesteading/
701
dom selection-based model using only the informa-
tion available in the grammar and raw text. This
was in part because it is possible to extract a rea-
sonable tagging model from uncorrected parse data,
due to the constrained nature of these grammars.
These models will hopefully allow grammar engi-
neers to more easily build statistical models for new
languages, using nothing more than their new gram-
mar and raw text.
Since fully evaluating the potential for building
models for new languages is a long-term ongoing
experiment, we looked at a more short-term eval-
uation of our unsupervised parse selection meth-
ods: building models for new domains. A pre-
liminary self-training experiment, using our initial
counts tagger trained model as the starting point,
showed promising results for domain adaptation.
There are plenty of directions for further work
arising from these results. The issues surrounding
what makes a good tagger for this purpose, and how
can we best learn one without gold training data,
would be one possibly fruitful avenue for further
exploration. Another interesting slant would be to
investigate domain effects of the tagger. Previous
work has already found that training just a lexical
model on a new domain can improve parsing results.
Since the optimal tagger ?training? we saw here (for
English) was merely to read off frequency counts for
parsed data, it would be easy to retrain the tagger on
different domains. Alternatively, it would be inter-
esting so see how much difference it makes to train
the tagger on one set of data, and use that to tag a
model training set from a different domain. Other
methods of incorporating the tagger output could
also be investigated. Finally, a user study involv-
ing a grammar engineer working on a new language
would be useful to validate the results we found here
and confirm whether they are indeed helpful in boot-
strapping a new grammar.
Acknowledgements
This research was supported by Australian Research
Council grant no. DP0988242 and Microsoft Re-
search Asia.
References
Jason Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), pages 57?64, Manch-
ester, UK.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proceedings of the Workshop on Grammar Engineer-
ing and Evaluation at the 19th International Con-
ference on Computational Linguistics, pages 8?14,
Taipei, Taiwan.
Emily M. Bender. 2008. Evaluating a crosslinguistic
grammar resource: A case study of Wambaya. In Pro-
ceedings of the 46th Annual Meeting of the ACL, pages
977?985, Columbus, USA.
Philip Blunsom. 2007. Structured Classification for
Multilingual Natural Language Processing. Ph.D.
thesis, Department of Computer Science and Software
Engineering, the University of Melbourne.
Eric Brill. 1995. Unsupervised learning of disambigua-
tion rules for part of speech tagging. In Proceedings
of the Third Workshop on Very Large Corpora, pages
1?13, Cambridge, USA.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalised statistical parser on the
PARC DepBank. In Proceedings of the 44th Annual
Meeting of the ACL and the 21st International Confer-
ence on Computational Linguistics, pages 41?48, Syd-
ney, Australia.
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Proceedings of a
Workshop on Computational Environments for Gram-
mar Development and Linguistic Engineering, pages
9?15, Madrid, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor, USA.
Stephen Clark and James R. Curran. 2006. Partial train-
ing for a lexicalized-grammar parser. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the ACL (NAACL), pages
144?151, New York City, USA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the ACL, pages 248?255, Prague, Czech Republic.
702
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
vol 3(no 4):pp 281?332.
Mary Dalrymple. 2006. How much can part-of-speech
tagging help parsing? Natural Language Engineering,
12(4):373?389.
Rebecca Dridan. 2009. Using lexical statistics to im-
prove HPSG parsing. Ph.D. thesis, Saarland Univer-
sity.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. Stanford: CSLI Publications.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of the 10th International Con-
ference on Parsing Technology (IWPT 2007), pages
11?22, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396, September.
Mark Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305,
Prague, Czech Republic.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, Taipei, Taiwan.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, pages 152?159, New York City, USA.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Computa-
tional Linguistics, 34(1):35?80.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of the GEAF 2007
Workshop, Palo Alto, California.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of the 46th Annual Meeting of the ACL,
pages 46?54, Columbus, USA.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing - practical results. In
Proceedings of the 1st Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 162?169, Seattle, USA.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christopher D. Manning. 2004. LinGO redwoods. a
rich and dynamic treebank for HPSG. Journal of Re-
search in Language and Computation, 2(4):575?596.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual, Computational
Linguistics, Saarland University, Saarbru?cken, Ger-
many.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, USA.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a.
Minimized models and grammar-informed initializa-
tion for supertagging with highly ambiguous lexicons.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 495?503,
Uppsala, Sweden.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010b. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 940?948, Beijing, China.
Laura Rimell and Stephen Clark. 2008. Adapting
a lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 475?484, Honolulu, USA.
Yves Schabes and Aravind K. Joshi. 1991. Parsing with
lexicalized tree adjoining grammar. In Masaru Tomita,
editor, Current Issues in Parsing Technology, chap-
ter 3, pages 25?48. Kluwer.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 3rd
Workshop on Asian Language Resources and Interna-
tional Standardization. Coling 2002 Post-Conference
Workshop., Taipei, Taiwan.
Yasuhito Tanaka. 2001. Compilation of a multilingual
parallel corpus. In Proceedings of PACLING 2001,
pages 265?268, Kitakyushu, Japan.
Kristina Toutanova, Chistopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT2002), pages 253?263.
703
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbrcken, Germany.
Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th international conference on pars-
ing technologies (IWPT 2007), pages 48?59, Prague,
Czech Republic.
704
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862?871,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Classifying Dialogue Acts in One-on-one Live Chats
Su Nam Kim,? Lawrence Cavedon? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne
? School of Computer Science and IT, RMIT University
sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net
Abstract
We explore the task of automatically classify-
ing dialogue acts in 1-on-1 online chat forums,
an increasingly popular means of providing
customer service. In particular, we investi-
gate the effectiveness of various features and
machine learners for this task. While a sim-
ple bag-of-words approach provides a solid
baseline, we find that adding information from
dialogue structure and inter-utterance depen-
dency provides some increase in performance;
learners that account for sequential dependen-
cies (CRFs) show the best performance. We
report our results from testing using a corpus
of chat dialogues derived from online shop-
ping customer-feedback data.
1 Introduction
Recently, live chats have received attention due to
the growing popularity of chat services and the in-
creasing body of applications. For example, large
organizations are increasingly providing support or
information services through live chat. One advan-
tage of chat-based customer service over conven-
tional telephone-based customer service is that it
becomes possible to semi-automate aspects of the
interaction (e.g. conventional openings or canned
responses to standard questions) without the cus-
tomer being aware of it taking place, something that
is not possible with speech-based dialogue systems
(as synthesised speech is still easily distinguishable
from natural speech). Potentially huge savings can
be made by organisations providing customer help
services if we can increase the degree of automation
of live chat.
Given the increasing impact of live chat services,
there is surprisingly little published computational
linguistic research on the topic. There has been sub-
stantially more work done on dialogue and dialogue
corpora, mostly in spoken dialogue (e.g. Stolcke et
al. (2000)) but also multimodal dialogue systems in
application areas such as telephone support service
(Bangalore et al, 2006) and tutoring systems (Lit-
man and Silliman, 2004). Spoken dialogue analysis
introduces many complications related to the error
inherent in current speech recognition technologies.
As an instance of written dialogue, an advantage of
live chats is that recognition errors are not such an is-
sue, although the nature of language used in chat is
typically ill-formed and turn-taking is complicated
by the semi-asynchronous nature of the interaction
(e.g. Werry (1996)).
In this paper, we investigate the task of automatic
classification of dialogue acts in 1-on-1 live chats,
focusing on ?information delivery? chats since these
are proving increasingly popular as part of enter-
prise customer-service solutions. Our main chal-
lenge is to develop effective features and classifiers
for classifying aspects of 1-on-1 live chat. Much of
the work on analysing dialogue acts in spoken di-
alogues has relied on non-lexical features, such as
prosody and acoustic features (Stolcke et al, 2000;
Julia and Iftekharuddin, 2008; Sridhar et al, 2009),
which are not available for written dialogues. Pre-
vious dialogue-act detection for chat systems has
used bags-of-words (hereafter, BoW) as features
for dialogue-act detection; this simple approach
has shown some promise (e.g. Bangalore et al
(2006), Louwerse and Crossley (2006) and Ivanovic
(2008)). Other features such as keywords/ontologies
(Purver et al, 2005; Forsyth, 2007) and lexical cues
(Ang et al, 2005) have also been used for dialogue
act classification.
862
In this paper, we first re-examine BoW features
for dialogue act classification. As a baseline, we
use the work of Ivanovic (2008), which explored 1-
grams and 2-grams with Boolean values in 1-on-1
live chats in the MSN Online Shopping domain (this
dataset is described in Section 5). Although this
work achieved reasonably high performance (up to
a micro-averaged F-score of around 80%), we be-
lieve that there is still room for improvement using
BoW only. We extend this work by using ideas from
related research such as text categorization (Debole
and Sebastiani, 2003), and explore variants of BoW
based on analysis of live chats, along with feature
weighting. Finally, our main aim is to explore new
features based on dialogue structure and dependen-
cies between utterances1 that can enhance the use of
BoW for dialogue act classification. Our hypothesis
is that, for task-oriented 1-on-1 live chats, the struc-
ture and interactions among utterances are useful in
predicting future dialogue acts: for example, conver-
sations typically start with a greeting, and questions
and answers typically appear as adjacency pairs in
a conversation. Therefore, we propose new features
based on structural and dependency information de-
rived from utterances (Sections 4.2 and 4.3).
2 Related Work
While there has been significant work on classify-
ing dialogue acts, the bulk of this has been for spo-
ken dialogue. Most such work has considered: (1)
defining taxonomies of dialogue acts; (2) discover-
ing useful features for the classification task; and (3)
experimenting with different machine learning tech-
niques. We focus here on (2) and (3); we return to
(1) in Section 3.
For classifying dialogue acts in spoken dialogue,
various features such as dialogue cues, speech char-
acteristics, and n-grams have been proposed. For
example, Samuel et al (1998) utilized the charac-
teristics of spoken dialogues and examined speaker
direction, punctuation marks, cue phrases and n-
grams for classifying spoken dialogues. Jurafsky et
al. (1998) used prosodic, lexical and syntactic fea-
tures for spoken dialogue classification. More re-
cently, Julia and Iftekharuddin (2008) and Sridhar et
1An utterance is the smallest unit to deliver a participant?s
message(s) in a turn.
al. (2009) achieved high performance using acous-
tic and prosodic features. Louwerse and Cross-
ley (2006), on the other hand, used various n-gram
features?which could be adapted to both spoken
and written dialogue?and tested them using the
Map Task Corpus (Anderson et al, 1991). Extend-
ing the discourse model used in previous work, Ban-
galore et al (2006) used n-grams from the previous
1?3 utterances in order to classify dialogue acts for
the target utterance.
There has been substantially less effort on clas-
sifying dialogue acts in written dialogue: Wu et al
(2002) and Forsyth (2007) have used keyword-based
approaches for classifying online chats; Ivanovic
(2008) tested the use of n-gram features for 1-on-1
live chats with MSN Online Shopping assistants.
Various machine learning techniques have been
investigated for the dialogue classification task.
Samuel et al (1998) used transformation-based
learning to classify spoken dialogues, incorporat-
ing Monte Carlo sampling for training efficiency.
Stolcke et al (2000) used Hidden Markov Mod-
els (HMMs) to account for the structure of spo-
ken dialogues, while Wu et al (2002) also used
transformation- and rule-based approaches plus
HMMs for written dialogues. Other researchers
have used Bayesian based approaches, such as
naive Bayes (e.g. (Grau et al, 2004; Forsyth,
2007; Ivanovic, 2008)) and Bayesian networks (e.g.
(Keizer, 2001; Forsyth, 2007)). Maximum entropy
(e.g. (Ivanovic, 2008)), support vector machines
(e.g. (Ivanovic, 2008)), and hidden Markov models
(e.g. (Bui, 2003)) have also all been applied to auto-
matic dialogue act classification.
3 Dialogue Acts
A number of dialogue act taxonomies have been pro-
posed, designed mainly for spoken dialogue. Many
of these use the Dialogue Act Markup in Several
Layers (DAMSL) scheme (Allen and Core, 1997).
DAMSL was originally applied to the TRAINS cor-
pus of (transcribed) spoken task-oriented dialogues,
but various adaptations of it have since been pro-
posed for specific types of dialogue. The Switch-
board corpus (Godfrey et al, 1992) defines 42 types
of dialogue acts from human-to-human telephone
conversations. The HCRCMap Task corpus (Ander-
863
son et al, 1991) defines a set of 128 dialogue acts to
model task-based spoken conversations.
For casual online chat dialogues, Wu et al (2002)
define 15 dialogue act tags based on previously-
defined dialogue act sets (Samuel et al, 1998;
Shriberg et al, 1998; Jurafsky et al, 1998; Stolcke
et al, 2000). Forsyth (2007) defines 15 dialogue acts
for casual online conversations, based on 16 conver-
sations with 10,567 utterances. Ivanovic (2008) pro-
poses 12 dialogue acts based on DAMSL for 1-on-1
online customer service chats.
Ivanovic?s set of dialogue acts for chat dia-
logues has significant overlap with the dialogue act
sets of Wu et al (2002) and Forsyth (2007) (e.g.
GREETING, EMOTION/EXPRESSION, STATEMENT,
QUESTION). In our work, we re-use the set of dia-
logue acts proposed in Ivanovic (2008), due to our
targeting the same task of 1-on-1 IM chats, and in-
deed experimenting over the same dataset. The def-
initions of the dialogue acts are provided in Table 1,
along with examples.
4 Feature Selection
In this section, we describe our initial dialogue-act
classification experiments using simple BoW fea-
tures, and then introduce two groups of new fea-
tures based on structural information and dependen-
cies between utterances.
4.1 Bag-of-Words
n-gram-based BoW features are simple yet effec-
tive for identifying similarities between two utter-
ances, and have been used widely in previous work
on dialogue act classification for online chat di-
alogues (Louwerse and Crossley, 2006; Ivanovic,
2008). However, chats containing large amounts of
noise such as typos and emoticons pose a greater
challenge for simple BoW approaches. On the other
hand, keyword-based features (Forsyth, 2007) have
achieved high performance; however, keyword-
based approaches are more domain-dependent. In
this work, we chose to start with a BoW approach
based on our observation that commercial live chat
services contain relatively less noise; in particular,
the commercial agent tends to use well-formed, for-
mulaic prose.
Previously, Ivanovic (2008) explored Boolean 1-
gram and 2-gram features to classify MSN Online
Shopping live chats, where a user requests assis-
tance in purchasing an item, in response to which the
commercial agent asks the customer questions and
makes suggestions. Ivanovic (2008) achieved solid
performance over this data (around 80% F-score).
While 1-grams performed well (as live chat utter-
ances are generally shorter than, e.g., sentences in
news articles), we expect 2- and 3-grams are needed
to detect formulaic expressions, such as No problem
and You are welcome. We would also expect a pos-
itive effect from combining n-grams due to increas-
ing the coverage of feature words. We thus test 1-,
2- and 3-grams individually, as well as the combi-
nation of 1- and 2-grams together (i.e. 1+2-grams)
and 1-, 2- and 3-grams (i.e. 1+2+3-grams); this re-
sults in five BoW sets. Also, unlike Ivanovic (2008),
we test both raw words and lemmas; we expect the
use of lemmas to perform better than raw words as
our data is less noisy. As the feature weight, in addi-
tion to simple Boolean, we also experiment with TF,
TF?IDF and Information Gain (IG).
4.2 Structural Information
Our motivation for using structural information as
a feature is that the location of an utterance can be
a strong predictor of the dialogue act. That is, dia-
logues are sequenced, comprising turns (i.e. a given
user is sending text), each of which is made up of
one or more messages (i.e. strings sent by the user).
Structured classification methods which make use of
this sequential information have been applied to re-
lated tasks such as tagging semantic labels of key
sentences in biomedical domains (Chung, 2009) and
post labels in web forums (Kim et al, 2010).
Based on the nature of live chats, we observed that
the utterance position in the chat, as well as in a turn,
plays an important role when identifying its dialogue
act. For example, an utterance such as Hello will oc-
cur at the beginning of a chat while an utterance such
as Have a nice day will typically appear at the end.
The position of utterances in a turn can also help
identify the dialogue act; i.e. when there are several
utterances in a turn, utterances are related to each
other, and thus examining the previous utterances in
the same turn can help correctly predict the target
utterance. For example, the greeting (Welcome to ..)
and question (How may I help you?) could occur in
864
Dialogue Act, Definition and Examples
CONVENTIONAL CLOSING: Various ways of ending a conversation e.g. Bye Bye
CONVENTIONAL OPENING: Greeting and other ways of starting a conversation e.g. Hello Customer
DOWNPLAYER: A backwards-linking label often used after THANKS to down play the contribution
e.g. You are welcome, my pleasure
EXPRESSIVE: An acknowledgement of a previous utterance or an indication of the speaker?s mood.
e.g. haha, : ?) wow
NO ANSWER: A backward-linking label in the form of a negative response to a YESNO-QUESTION e.g. no, nope
OPEN QUESTION: A question that cannot be answered with only a yes or no. The answer is usually
some form of explanation or statement. e.g. how do I use the international version?
REQUEST: Used to express a speaker?s desire that the learner do something ? either performing some action
or simply waiting. e.g. Please let me know how I can assist you on MSN Shopping today.
RESPONSE ACK: A backward-linking acknowledgement of the previous utterance. Used to confirm
that the previous utterance was received/accepted. e.g. Sure
STATEMENT: Used for assertions that may state a belief or commit the speaker to doing something
e.g. I am sending you the page which will pop up in a new window on your screen.
THANKS: Conventional thanks e.g. Thank you for contacting us.
YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g. yes, yeah
YESNO QUESTION: A closed question which can be answered in the affirmative or negative.
e.g. Did you receive the page, Customer?
Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008)
the same turn. We also noticed that identifying the
utterance author can help classify the dialogue act
(previously used in Ivanovic (2008)).
Based on these observations, we tested the follow-
ing four structural features:
? Author information,
? Relative position in the chat,
? Author + Relative position,
? Author + Turn-relative position among utter-
ances in a given turn.
We illustrate our structural features in Table 2,
which shows an example of a 1-on-1 live chat. The
participants are the agent (A) and customer (C); Uxx
indicates an utterance (U) with ID number xx. This
conversation has 42 utterances in total. The relative
position is calculated by dividing the utterance num-
ber by the total number of utterances in the dialogue;
the turn-relative position is calculated by dividing
the utterance position by the number of utterances
in that turn. For example, for utterance 4 (U4), the
relative position is 442 , while its turn-relative position
is 23 since U4 is the second utterance among U3,4,5
that the customer makes in a single turn.
4.3 Utterance Dependency
In recent work, Kim et al (2010) demonstrated the
importance of dependencies between post labels in
web forums. The authors introduced series of fea-
tures based on structural dependencies among posts.
They used relative position, author information and
automatically predicted labels from previous post(s)
as dependency features for assigning a semantic la-
bel to the current target post.
Similarly, by examining our chat corpus, we ob-
served significant dependencies between utterances.
First, 1-on-1 (i.e. agent-to-user) dialogues often con-
tain dependencies between adjacent utterances by
different authors. For example, in Table 2, when the
agent asks Is that correct?, the expected response
from the user is a Yes or No. Another example is
that when the agent makes a greeting, such as Have
a nice day, then the customer will typically respond
with a greeting or closing remark, and not a Yes or
No. Second, the flow of dialogues is in general co-
hesive, unless the topic of utterances changes dra-
matically (e.g. U5: Are you still there?, U22: brb
in 1 min in Table 2). Third, we observed that be-
tween utterances made by the same author (either
agent or user), the target utterance relies on previous
utterances made by the same author, especially when
865
ID Utterance
A:U1 Hello Customer, welcome to MSN Shopping.
A:U2 My name is Krishna and I am your
online Shopping assistant today.
C:U3 Hello!
C:U4 I?m trying to find a sports watch.
C:U5 are you still there?
A:U6 I understand that you are looking for sports
watch.
A:U7 Is that correct?
C:U8 yes, that is correct.
..
C:U22 brb in 1 min
C:U23 Thank you for waiting
..
A:U37 Thank you for allowing us to assist
you regarding wrist watch.
A:U38 I hope you found our session today helpful.
A:U39 If you have any additional questions or
you need additional information,
please log in again to chat with us.
We are available 24 hours a day, 7 days a
week for your help.
A:U40 Thank you for contacting MSN Shopping.
A:U41 Have a nice day! Good Bye and Take Care.
C:U42 You too.
Table 2: An example of a 1-on-1 live chat, with turn and
utterance structure
the agent and user repeatedly question and answer.
With these observations, we checked the likelihood
of dialogue act pairings between two adjacent utter-
ances, as well as between two adjacent utterances
made by the same author. Overall, we found strong
co-occurrence (as measured by number of occur-
rences of labels across adjacency pairs) between cer-
tain pairs of dialogue acts (e.g. (YESNO QUESTION
?YES ANSWER/NO ANSWER) and (REQUEST
?YES ANSWER)). STATEMENT, on the other
hand, can associate with most other dialogue acts.
Based on this, we designed the following five ut-
terance dependency features; by combining these,
we obtain 31 feature sets.
1. Dependency of utterances regardless of author
(a) Dialogue act of previous utterance
(b) Accumulated dialogue act(s) of previous
utterances
(c) Accumulated dialogue acts of previous ut-
terances in a given turn
2. Dependency of utterances made by a single au-
thor
(a) Dialogue act of previous utterance
by same author; a dialogue act can be in
the same turn or in the previous turn
(b) Accumulated dialogue acts of previous
utterances by same author; dialogue acts
can be in the same turn or in the previous
turn
To capture utterance dependency, Bangalore et al
(2006) previously used n-gram BoW features from
the previous 1?3 utterances. In contrast, instead of
using utterances which indirectly encode dialogue
acts, we directly use the dialogue act classifications,
as done in Stolcke et al (2000). The motivation is
that, due to the high performance of simple BoW
features, using dialogue acts directly would cap-
ture the dependency better than indirect information
from utterances, despite introducing some noise. We
do not build a probabilistic model of dialogue tran-
sitions the way Stolcke et al (2000) does, but follow
an approach similar to that used in Kim et al (2010)
in using predicted dialogue act(s) labels learned in
previous step(s) as a feature.
5 Experiment Setup
As stated earlier, we use the data set from Ivanovic
(2008) for our experiments; it contains 1-on-1 live
chats from an information delivery task. This dataset
contains 8 live chats, including 542 manually-
segmented utterances. The maximum and minimum
number of utterances in a dialogue are 84 and 42,
respectively; the maximum number of utterances in
a turn is 14. The live chats were manually tagged
with the 12 dialogue acts described in Section 3.
The utterance distribution over the dialogue acts is
described in Table 3.
For our experiments, we calculated TF, TF?IDF
and IG (Information Gain) over the utterances,
which were optionally lemmatized with the morph
tool (Minnen et al, 2000). We then built a dialogue
act classifier using three different machine learn-
ers: SVM-HMM (Joachims, 1998),2 naive Bayes
2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
866
Dialogue Act Utterance number
CONVENTIONAL CLOSING 15
CONVENTIONAL OPENING 12
DOWNPLAYER 15
EXPRESSIVE 5
NO ANSWER 12
OPEN QUESTION 17
REQUEST 28
RESPONSE ACK 27
STATEMENT 198
THANKS 79
YES ANSWER 35
YESNO QUESTION 99
Table 3: Dialogue act distribution in the corpus
Index Learner Ours Ivanovic
Feature Acc. Feature Acc.
Word SVM 1+2+3/B .790 1/B .751
NB 1/B .673 1/B .673
CRF 1/IG .839 1/B .825
Lemma SVM 1+2+3/IG .777 N/A N/A
NB 1/B .672 N/A N/A
CRF 1/B .862 N/A N/A
Table 4: Best accuracy achieved by the different learn-
ers over different feature sets and weighting methods (1
= 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = in-
formation gain)
from the WEKA machine learning toolkit (Wit-
ten and Frank, 2005), and Conditional Random
Fields (CRF) using CRF++.3 Note that we chose
to test CRF and SVM-HMM as previous work (e.g.
(Samuel et al, 1998; Stolcke et al, 2000; Chung,
2009)) has shown the effectiveness of structured
classification models on sequential dependencies.
Thus, we expect similar effects with CRF and SVM-
HMM. Finally, we ran 8-fold cross-validation using
the feature sets described above (partitioning across
the 8 sessions). All results are presented in terms
of classification accuracy. The accuracy of a zero-R
(i.e. majority vote) baseline is 0.36.
6 Evaluation
6.1 Testing Bag-of-Words Features
Table 4 shows the best accuracy achieved by the dif-
ferent learners, in combination with BoW represen-
3http://crfpp.sourceforge.net/
n-gram Boolean TF TF?IDF IG
1 .731 .511 .517 .766
2 .603 .530 .601 .614
3 .474 .463 .472 .482
1+2 .756 .511 .522 .777
1+2+3 .773 .511 .528 .777
Table 5: Accuracy of different feature representations and
weighting methods for SVM-HMM
tations and feature weighting methods. Note that the
CRF learner ran using 1-grams only, as CRF++ does
not accept large numbers of features. As a bench-
mark, we also tested the method in Ivanovic (2008)
and present the best performance over words (rather
than lemmas). Overall, we found using just 1-grams
produced the best performance for all learners, al-
though SVM achieved the best performance when
using all three n-gram orders (i.e. 1+2+3). Since the
utterances are very short, 2-grams or 3-grams alone
are too sparse to be effective. Among the feature
weighting methods, Boolean and IG achieved higher
accuracy than TF and TF?IDF. Likewise, due to the
short utterances, simple Boolean values were often
the most effective. However, as IG was computed
using the training data, it also achieved high perfor-
mance. When comparing the learners, we found that
CRF produced the best performance, due to its abil-
ity to capture inter-utterance dependencies. Finally,
we confirmed that using lemmas results in higher ac-
curacy.
Table 5 shows the accuracy over all feature sets;
for brevity, we show this for SVM only since the
pattern is similar across all learners.
6.2 Using Structural Information
In this section, we describe experiments using struc-
tural information?i.e. author and/or position?with
BoWs. As with the base BoW technique, we used
1-gram lemmas with Boolean values, based on the
results from Section 6.1. Table 6 shows the results:
Pos indicates the relative position of an utterance in
the whole dialogue, Author means author informa-
tion, and Posturn indicates the relative position of
the utterance in a turn. All methods outperformed
the baseline; methods that surpassed the results for
the simple BoW method (for the given learner) at a
867
Feature Learners
CRF SVM NB
BoW .862 .731 .672
BoW+Author .860 .655 .649
BoW+Pos .862 .721 .655
BoW+Posabsolute .863 .631 .524
BoW+Author+Pos .875 .700 .642
BoW+Author+Posturn .871 .651 .631
Table 6: Accuracy with structural information
level of statistical significance (based on randomised
estimation, p < 0.05) are boldfaced.
Overall, using CRFs with Author and Position in-
formation produced better performance than using
BoW alone. Clearly, the ability of CRFs to natively
optimise over structural dependencies provides an
advantage over other learners.
Relative position cannot of course be measured
directly in an actual online application; hence Ta-
ble 6 also includes the use of ?absolute position? as
a feature. We see that, for CRF, the absolute posi-
tion feature shows an insignificant drop in accuracy
as compared to the use of relative position. (How-
ever, we do see a significant drop in performance
when using this feature with SVM and NB.)
6.3 Using Utterance Dependency
We next combined the inter-utterance dependency
features with the BoW features. Since we use the
dialogue acts directly in utterance dependency, we
first experimented using gold-standard dialogue act
labels. We also tested using the dialogue acts which
were automatically learned in previous steps.
Table 7 shows performance using both the gold-
standard and learned dialogue acts. The differ-
ent features listed are as follows: LabelList/L in-
dicates those corresponding to all utterances in
a dialogue preceding the target utterance; Label-
Prev/P indicates a dialogue act from a previous
utterance; LabelAuthor/A indicates a dialogue act
from a previous utterance by the same author;
and LabelPrevt/LabelAuthort indicates the previ-
ous utterance(s) and previously same-authored ut-
terance(s) in a turn, respectively. Since the accuracy
for SVM and NB using learned labels is similar to
that using gold standard labels, for brevity we report
Features Dialogue Acts
Goldstandard Learned
CRF HMM NB CRF
BoW .862 .731 .672 .862
BoW+LabelList(L) .795 .435 .225 .803
BoW+LabelPrev(P) .875 .661 .364 .876
BoW+LabelAuthor(A) .865 .633 .559 .865
BoW+LabelPrevt(Pt) .873 .603 .557 .873
BoW+LabelAuthort(At) .862 .587 .535 .851
BoW+L+P .804 .428 .227 .808
BoW+L+A .799 .404 .225 .804
BoW+L+Pt .803 .413 .229 .804
BoW+L+At .808 .408 .216 .801
BoW+P+A .873 .631 .517 .869
BoW+P+Pt .878 .579 .539 .875
BoW+P+At .871 .603 .519 .867
BoW+A+Pt .847 .594 .519 .849
BoW+A+At .869 .594 .530 .871
BoW+Pt+At .871 .592 .519 .867
BoW+L+P+A .812 .419 .231 .804
BoW+L+P+Pt .816 .423 .229 .812
BoW+L+P+At .808 .397 .225 .806
BoW+L+A+Pt .810 .388 .225 .810
BoW+L+A+At .812 .415 .216 .801
BoW+L+Pt+At .810 .375 .205 .816
BoW+P+A+Pt .875 .602 .522 .876
BoW+P+A+At .862 .609 .511 .864
BoW+P+Pt+At .873 .594 .515 .867
BoW+A+Pt+At .865 .594 .517 .864
BoW+L+P+A+Pt .817 .410 .231 .810
BoW+L+P+A+At .814 .411 .223 .810
BoW+L+P+Pt+At .816 .382 .205 .806
BoW+L+A+Pt+At .812 .406 .203 .808
BoW+P+A+Pt+At .865 .583 .513 .865
BoW+L+P+A+Pt+At .816 .399 .205 .803
Table 7: Accuracy for the different learners with depen-
dency features
the performance for CRF using learned labels only.
Results that exceed the BoW accuracy at a level of
statistical significance (p < 0.05) are boldfaced.
Utterance dependency features worked well in
combination with CRF only. Individually, Prev and
Prevt (i.e. BoW+P+Pt) helped to achieve higher ac-
curacies, and the Author feature was also benefi-
cial. However, List decreased the performance, as
the flow of dialogues can change, and when a larger
history of dialogue acts is included, it tends to in-
troduce noise. Comparing use of gold-standard and
learned dialogue acts, the reduction in accuracy was
not statistically significant, indicating that we can
868
Feature CRF SVM NB
C+LabelList .9557 .4613 .2565
C+LabelPrev .9649 .6365 .5720
C+LabelAuthor .9686 .6310 .5424
C+LabelPrevt .9686 .5738 .5738
C+LabelAuthort .9561 .6125 .5332
Table 8: Accuracy with Structural and Dependency Infor-
mation: C means lemmatized Unigram+Position+Author
achieve high performance on dialogue act classifi-
cation even with interactively-learned dialogue acts.
We believe this demonstrates the robustness of the
proposed techniques.
Finally, we tested the combination of features
from structural and dependency information. That
is, we used a base feature (unigrams with Boolean
value), relative position, author information, com-
bined with each of the different dependency features
? LabelList, LabelPrev, LabelAuthor, LabelPrevt
and LabelAuthort.
Table 8 shows the performance when using these
combinations, for each dependency feature. As we
would expect, CRFs performed well with the com-
bined features since CRFs can incorporate the struc-
tural and dependency information; the achieved the
highest accuracy of 96.86%.
6.4 Error Analysis and Future Work
Finally, we analyzed the errors of
the best-performing feature set (i.e.
BoW+Position+Author+LabelAuthor). In Ta-
ble 9, we present a confusion matrix of errors,
for CONVENTIONAL CLOSING (Cl), CON-
VENTIONAL OPENING (Op), DOWNPLAYER
(Dp), EXPRESSIVE (Ex), NO ANSWER (No),
OPEN QUESTION (Qu), REQUEST (Rq), RE-
SPONSE ACK (Ack), STATEMENT (St), THANKS
(Ta), YES ANSWER (Yes), and YESNO QUESTION
(YN). Rows indicate the correct dialogue acts and
columns indicate misclassified dialogue acts.
Looking over the data, STATEMENT is a common
source of misclassification, as it is the majority class
in the data. In particularly, a large number of RE-
QUEST and RESPONSE ACK utterances were tagged
as STATEMENT. We did not include punctuation
such as question marks in our feature sets; includ-
ing this would likely improve results further.
In future work, we plan to investigate methods for
automatically cleansing the data to remove typos,
and taking account of temporal gaps that can some-
times arise in online chats (e.g. in Table 2, there is
a time gap between C:U22 brb in 1 min and C:U23
Thank you for waiting).
7 Conclusion
We have explored an automated approach for classi-
fying dialogue acts in 1-on-1 live chats in the shop-
ping domain, using bag-of-words (BoW), structural
information and utterance dependency features. We
found that the BoW features perform remarkably
well, with slight improvements when using lemmas
rather than words. Including structural and inter-
utterance dependency information further improved
performance. Of the learners we experimented with,
CRFs performed best, due to their ability to natively
capture sequential dialogue act dependencies.
Acknowledgements
This research was supported in part by funding from
Microsoft Research Asia.
References
J.Allen and M.Core. Draft of DAMSL: Dialog Act
Markup in Several Layers. The Multiparty Dis-
course Group. University of Rochester, Rochester,
USA. 1997.
A. Anderson, M. Bader, E. Bard, E. Boyle G.M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H.S. Thompson, R. and Weinert.
The HCRC Map Task Corpus. Language and Speech.
1991, 34, pp. 351?366.
J. Ang, Y. Liu and E. Shriberg. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. IEEE International Conference on Acoustics,
Speech, and Signal Processing. 2005, pp, 1061?1064.
S. Bangalore, G. Di Fabbrizio and A. Stent. Learning
the Structure of Task-Driven Human-Human Dialogs.
Proceedings of the 21st COLING and 44th ACL. 2006,
pp. 201?208.
H. H. Bui. A general model for online probabilistic plan
recognition. IJCAI. 2003, pp. 1309?1318.
G.Y Chung. Sentence retrieval for abstracts of random-
ized controlled trials. BMC Medical Informatics and
Decision Making. 2009, 9(10), pp. 1?13.
F. Debole and F. Sebastiani. Supervised term weighting
for automated text categorization. 18th ACM Sympo-
sium on Applied Computing. 2003, pp. 784?788.
869
Cl Op Dp Ex No Qu Rq Ack St Ta Yes YN
Op 0 0 0 0 0 0 0 0 0 0 0 2
Cl 0 0 0 0 0 0 0 0 1 1 0 0
Dp 0 0 0 0 0 0 0 0 0 0 0 0
Ex 0 0 0 0 0 0 0 0 0 0 0 0
No 0 0 0 0 0 0 0 0 0 0 0 0
Qu 0 0 0 0 0 0 0 0 0 0 0 0
Rq 0 0 0 0 0 0 0 0 3 0 0 0
Ack 0 0 1 0 1 0 0 0 5 0 0 0
St 0 0 0 0 0 0 1 0 0 0 0 0
Ta 1 0 0 0 0 0 0 0 0 0 0 0
Yes 0 0 0 0 0 0 0 0 0 0 0 0
YN 0 1 0 0 0 0 0 0 0 0 0 0
Table 9: Confusion matrix for errors from the CRF with BoW+Position+Author+LabelAuthor (rows = correct clas-
sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWN-
PLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK
= Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)
E. N. Forsyth. Improving Automated Lexical and Dis-
course Analysis of Online Chat Dialog. Master?s the-
sis. Naval Postgraduate School, 2007.
J. Godfrey and E. Holliman and J. McDaniel. SWITCH-
BOARD: Telephone speech corpus for research and
development. Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing. 1992, pp. 517?520.
S. Grau, E. Sanchis, M. Jose and D. Vilar. Dialogue act
classification using a Bayesian approach. Proceedings
of the 9th Conference on Speech and Computer. 2004.
P. A. Heeman and J. Allen. The Trains 93 Dialogues.
Trains Technical Note 94-2. Computer Science Dept.,
University of Rochester, March 1995.
T. Joachims. Text categorization with support vector ma-
chines: Learning with many relevant features. Pro-
ceedings of European Conference on Machine Learn-
ing. 1998, pp. 137?142.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker and P. Maloor.
MATCH: An Architecture for Multimodal Dialogue
Systems. Proceedings of 40th ACL. 2002, pp. 376?
383.
F. N. Julia and K. M. Iftekharuddin. Dialog Act clas-
sification using acoustic and discourse information of
MapTask Data. Proceedings of the International Joint
Conference on Neural Networks. 2008, pp. 1472?
1479.
D. Jurafsky, E. Shriberg, B Fox and T. Curl. Lexical,
Prosodic, and Syntactic Cues for Dialog Acts. Pro-
ceedings of ACL/COLING-98 Workshop on Discourse
Relations and Discourse Markers. 1998, pp. 114?120.
E. Ivanovic. Automatic instant messaging dialogue us-
ing statistical models and dialogue acts. Master?s The-
sis. The University of Melbourne. 2008.
S. Keizer. A Bayesian Approach to Dialogue Act Clas-
sification. 5th Workshop on Formal Semantics and
Pragmatics of Dialogue. 2001, pp. 210?218.
S.N. Kim and L. Wang and T. Baldwin. Tagging and
Linking Web Forum Posts. Fourteenth Conference on
Computational Natural Language Learning. 2010.
J. Lafferty, A. McCallum and F. Pereira. Conditional
random fields: Probabilistic models for segmenting
and labeling sequence data. Proceedings of ICML.
2001, pp. 282?289.
D. J. Litman and S. Silliman. ITSPOKE: An Intelligent
Tutoring Spoken Dialogue SYstem. Proceedings of
the HLT/NAACL. 2004.
M. M. Louwerse and S. Crossley. Dialog Act Classifica-
tion Using N -Gram Algorithms. FLAIRS Conference,
2006, pp. 758?763.
G. Minnen, J. Carroll and D. Pearce. Applied morpho-
logical processing of English Natural Language Engi-
neering 2000, 7(3), pp. 77?80.
M. Purver, J. Niekrasz and S. Peters. Ontology-Based
Discourse Understanding for a Persistent Meeting As-
sistant. Proc. CHI 2005 Workshop on The Virtuality
Continuum Revisited. 2005.
K. Samuel, Sandra Carberry and K. Vijay-Shanker. Dia-
logue Act Tagging with Transformation-Based Learn-
ing. Proceedings of COLING/ACL 1998. 1998, pp.
1150-1156.
E. Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,
K. Ries, N. Coccaro, R. Martin, M. Meteer and C. Van
870
Ess-Dykema. Can Prosody Aid the Automatic Clas-
sification of Dialog Acts in Conversational Speech?.
Language and Speech. 1998, 41(3-4), pp. 439?487.
V. R. Sridhar, S. Bangalore and S. Narayanan. Combin-
ing lexical, syntactic and prosodic cues for improved
online dialog act tagging. Computer Speech and Lan-
guage. 2009, 23(4), pp. 407?422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema
and M. Meteer. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics. 2000, 26(3), pp. 339?373.
A. Stolcke and E. Shriberg. Markovian Combination of
Language and Prosodic Models for better Speech Un-
derstanding and Recognition . Invited talk at the IEEE
Workshop on Speech Recognition and Understanding,
Madonna di Campiglio, Italy, December 2001 2001,
C. C. Werry. Linguistic and interactional features of In-
ternet Relay Chat. In S. C. Herring (ed.). Computer-
Mediated Communication. Benjamins, 1996.
I. Witten and E. Frank. Data Mining: Practical Machine
Learning Tools and Techniques. Morgan Kaufmann,
2005.
T. Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M.
Pottenger. Posting act tagging using transformation-
based learning. Proceedings of the Workshop on Foun-
dations of Data Mining and Discovery, IEEE Interna-
tional Conference on Data Mining. 2002.
871
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13?25,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting Thread Discourse Structure over Technical Web Forums
Li Wang,?? Marco Lui,?? Su Nam Kim,?? Joakim Nivre? and Timothy Baldwin??
? Dept. of Computer Science and Software Engineering, University of Melbourne
? NICTA Victoria Research Laboratory
? Dept. of Linguistics and Philology, Uppsala University
li.wang.d@gmail.com, saffsd@gmail.com,
sunamkim@gmail.com, joakim.nivre@lingfil.uu.se, tb@ldwin.net
Abstract
Online discussion forums are a valuable
means for users to resolve specific information
needs, both interactively for the participants
and statically for users who search/browse
over historical thread data. However, the com-
plex structure of forum threads can make it
difficult for users to extract relevant informa-
tion. The discourse structure of web forum
threads, in the form of labelled dependency re-
lationships between posts, has the potential to
greatly improve information access over web
forum archives. In this paper, we present the
task of parsing user forum threads to deter-
mine the labelled dependencies between posts.
Three methods, including a dependency pars-
ing approach, are proposed to jointly clas-
sify the links (relationships) between posts
and the dialogue act (type) of each link. The
proposed methods significantly surpass an in-
formed baseline. We also experiment with ?in
situ? classification of evolving threads, and es-
tablish that our best methods are able to per-
form equivalently well over partial threads as
complete threads.
1 Introduction
Web user forums (or simply ?forums?) are online
platforms for people to discuss information and ob-
tain information via a text-based threaded discourse,
generally in a pre-determined domain (e.g. IT sup-
port or DSLR cameras). With the advent of Web
2.0, there has been an explosion of web authorship in
this area, and forums are now widely used in various
areas such as customer support, community devel-
opment, interactive reporting and online eduction.
In addition to providing the means to interactively
participate in discussions or obtain/provide answers
to questions, the vast volumes of data contained in
forums make them a valuable resource for ?support
sharing?, i.e. looking over records of past user inter-
actions to potentially find an immediately applica-
ble solution to a current problem. On the one hand,
more and more answers to questions over a wide
range of domains are becoming available on forums;
on the other hand, it is becoming harder and harder
to extract and access relevant information due to the
sheer scale and diversity of the data.
This research aims at enhancing information ac-
cess and support sharing, by mining the discourse
structure of troubleshooting-oriented web user fo-
rum threads. Previous research has shown that sim-
ple thread structure information (e.g. reply-to struc-
ture) can enhance tasks such as forum information
retrieval (Seo et al, 2009) and post quality assess-
ment (Lui and Baldwin, 2009). We aim to move be-
yond simple threading, to predict not only the links
between posts, but also show the manner of each
link, in the form of the discourse structure of the
thread. In doing so, we hope to be able to perform
richer visualisation of thread structure (e.g. high-
lighting the key posts which appear to have led to
a successful resolution to a problem), and more fine-
grained weighting of posts in threads for search pur-
poses.
To illustrate the task, we use an example thread,
made up of 5 posts from 4 distinct participants, from
the CNET forum dataset of Kim et al (2010b), as
shown in Figure 1. The discourse structure of the
thread is modelled as a rooted directed acyclic graph
13
HTML Input Code...Please can someone tell me how to create an input box that asks the user to enter their ID, and then allows them to press go. It will then redirect to the page ...
User APost 1
User BPost 2
User CPost 3
Re: html input codePart 1: create a form with a text field. See ... Part 2: give it a Javascript action
asp.net c\# videoI?ve prepared for you video.link click ...
Thank You!Thanks a lot for that ... I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site? ...
A little more help... You would simply do it this way: ... You could also just ... An example of this is ...
User APost 4
User DPost 5
0+Question-Question
2+Answer-Answer
4+Answer-Answer
1+Answer-Answer
1+Answer-Confirmation
3+Question-Add
?
Figure 1: A snippeted and annotated CNET thread
(DAG) with a dialogue act label associated with each
edge of the graph. In this example, UserA initiates
the thread with a question (dialogue act = Question-
Question) in the first post, by asking how to create
an interactive input box on a webpage. In response,
UserB and UserC provide independent answers (di-
alogue act = Answer-Answer). UserA responds to
UserC to confirm the details of the solution (dia-
logue act = Answer-Confirmation), and at the same
time, adds extra information to his/her original ques-
tion (dialogue act = Question-Add); i.e., this one
post has two distinct dependency links associated
with it. Finally, UserD proposes a different solution
again to the original question.
To predict thread discourse structure of this type,
we jointly classify the links and dialogue acts be-
tween posts, experimenting with a variety of su-
pervised classification methods, namely dependency
parsing and linear-chain conditional random fields.
In this, we build on the earlier work of Kim et al
(2010b) who first proposed the task of thread dis-
course analysis, but only carried out experiments on
post linking and post dialogue act classification as
separate tasks. In addition to achieving state-of-the-
art accuracy over the task, we carry out in-depth
analysis of classification effectiveness at different
thread depths, and establish that the accuracy of our
method over partial threads is equivalent to that over
full threads, indicating that the method is applica-
ble to in-situ thread classification. Finally, we in-
vestigate the role of user-level features in discourse
structure analysis.
2 Related Work
This work builds directly on earlier work of a subset
of the authors (Kim et al, 2010b), whereby a novel
post-level dialogue act set was proposed, and used
as the basis for annotation of a set of threads taken
from CNET. In the original work, we proposed a set
of novel features, which we applied to the separate
tasks of post link classification and dialogue act clas-
sification. We later applied the same basic method-
ology to dialogue act classification over one-on-one
live chat data with provided message dependencies
(Kim et al, 2010a), demonstrating the generalisabil-
ity of the original method. In both cases, however,
we tackled only a single task, either link classifica-
tion (optionally given dialogue act tags) or dialogue
act classification, but never the two together. In this
paper, we take the obvious step of exploring joint
classification of post link and dialogue act tags, to
generate full thread discourse structures.
Discourse disentanglement (i.e. link classifica-
tion) and dialogue act tagging have been studied
largely as independent tasks. Discourse disentangle-
ment is the task of dividing a conversation thread
(Elsner and Charniak, 2008; Lemon et al, 2002)
or document thread (Wolf and Gibson, 2005) into
a set of distinct sub-discourses. The disentangled
discourse is sometimes assumed to take the form of
a tree structure (Grosz and Sidner, 1986; Lemon et
al., 2002; Seo et al, 2009), an acyclic graph struc-
ture (Rose? et al, 1995; Schuth et al, 2007; Elsner
and Charniak, 2008; Wang et al, 2008; Lin et al,
2009), or a more general cyclic chain graph struc-
ture (Wolf and Gibson, 2005). Dialogue acts are
used to describe the function or role of an utterance
in a discourse, and have been applied to the anal-
ysis of mediums of communication including con-
versational speech (Stolcke et al, 2000; Shriberg et
al., 2004; Murray et al, 2006), email (Cohen et al,
2004; Carvalho and Cohen, 2005; Lampert et al,
2008), instant messaging (Ivanovic, 2008; Kim et
al., 2010a), edited documents (Soricut and Marcu,
2003; Sagae, 2009) and online forums (Xi et al,
14
2004; Weinberger and Fischer, 2006; Wang et al,
2007; Fortuna et al, 2007; Kim et al, 2010b). For a
more complete review of models for discourse dis-
entanglement and dialogue act tagging, see Kim et
al. (2010b).
Joint classification has been applied in a number
of different contexts, based on the intuition that it
should be possible to harness interactions between
different sub-tasks to the mutual benefit of both.
Warnke et al (1997) jointly performed segmenta-
tion and dialogue act classification over a German
spontaneous speech corpus. In their approach, the
predictions of a multi-layer perceptron classifier on
dialogue act boundaries were fed into an n-gram
language model, which was used for the joint seg-
mentation and classification of dialogue acts. Sut-
ton and McCallum (2005) performed joint parsing
and semantic role labelling (SRL), using the results
of a probabilistic SRL system to improve the accu-
racy of a probabilistic parser. Finkel and Manning
(2009) built a joint, discriminative model for pars-
ing and named entity recognition (NER), address-
ing the problem of inconsistent annotations across
the two tasks, and demonstrating that NER bene-
fited considerably from the interaction with parsing.
Dahlmeier et al (2009) proposed a joint probabilis-
tic model for word sense disambiguation (WSD) of
prepositions and SRL of prepositional phrases (PPs),
and achieved state-of-the-art results over both tasks.
There has been a recent growth in user-level
research over forums. Lui and Baldwin (2009)
explored a range of user-level features, including
replies-to and co-participation graph analysis, for
post quality classification. Lui and Baldwin (2010)
introduced a novel user classification task where
each user is classified against four attributes: clar-
ity, proficiency, positivity and effort. User commu-
nication roles in web forums have also been studied
(Chan and Hayes, 2010; Chan et al, 2010).
Threading information has been shown to en-
hance retrieval effectiveness for post-level retrieval
(Xi et al, 2004; Seo et al, 2009), thread-level
retrieval (Seo et al, 2009; Elsas and Carbonell,
2009), sentence-level shallow information extrac-
tion (Sondhi et al, 2010), and near-duplicate thread
detection (Muthmann et al, 2009). These results
suggest that the thread structural representation used
in this research, which includes both linking struc-
ture and the dialogue act associated with each link,
could potentially provide even greater leverage in
these retrieval tasks.
Another related research area is post-level classi-
fication, such as general post quality classification
(Weimer et al, 2007; Weimer and Gurevych, 2007;
Wanas et al, 2008; Lui and Baldwin, 2009), and
post descriptiveness in particular domains (e.g. med-
ical forums: Leaman et al (2010)). It has been
demonstrated (Wanas et al, 2008; Lui and Bald-
win, 2009) that thread discourse structure can signif-
icantly improve the classification accuracy for post-
level tasks.
Initiation?response pairs (e.g. question?answer,
assessment?agreement, and blame?denial) from on-
line forums have the potential to enhance thread
summarisation or automatically generate knowledge
bases for Community Question Answering (cQA)
services such as Yahoo! Answers. While initiation?
response pair identification has been explored as a
pairwise ranking problem (Wang and Rose?, 2010),
question?answer pair identification has been ap-
proached via the two separate sub-tasks of ques-
tion classification and answer detection (Cong et al,
2008; Ding et al, 2008; Cao et al, 2009). Our
thread discourse structure prediction task includes
joint classification of post roles (i.e. dialogue acts)
and links, and could potentially be performed at the
sub-post sentence level to extract initiation?response
pairs.
3 Task Description and Data Set
The main task performed in this research is joint
classification of inter-post links (Link) and dialogue
acts (DA) within forum threads. In this, we assume
that a post can only link to an earlier post (or a vir-
tual root node), and that dialogue acts are labels on
edges. It is possible for there to be multiple edges
from a given post, e.g. if a post both confirms the va-
lidity of an answer and adds extra information to the
original question (as happens in Post4 in Figure 1).
We experiment with two different approaches to
joint classification: (1) a linear-chain CRF over
combined Link/DA post labels; and (2) a depen-
dency parser. The joint classification task is a nat-
ural fit for dependency parsing, in that the task is
intrinsically one of inferring labelled dependencies
15
between posts, but it has a number of special prop-
erties that distinguish it from standard dependency
parsing:
strict reverse-chronological directionality: the
head always precedes the dependent, in terms
of the chronological sequencing of posts.
non-projective dependencies: threads can contain
non-projective dependencies, e.g. in a 4-post
thread, posts 2 and 3 may be dependent on
post 1, and post 4 dependent on post 2; around
2% of the threads in our dataset contain non-
projective dependencies.
multi-headedness: it is possible for a given post to
have multiple heads, including the possibility
of multiple dependency links to the same post
(e.g. adding extra information to a question
[Question-Add] as well as retracting infor-
mation from the original question [Question-
Correction]); around 6% of the threads in our
dataset contain multi-headed dependencies.
disconnected sub-graphs: it is possible for there to
be disconnected sub-graphs, e.g. in instances
where a user hijacks a thread to ask their
own unrelated question, or submit an unrelated
spam post; around 2% of the threads in our
dataset contain disconnected sub-graphs.
The first constraint potentially simplifies depen-
dency parsing, and non-projective dependencies are
relatively well understood in the dependency parsing
community (Tapanainen and Jarvinen, 1997; Mc-
Donald et al, 2005). Multi-headedness and dis-
connected sub-graphs pose greater challenges to de-
pendency parsing, although there has been research
done on both (McDonald and Pereira, 2006; Sagae
and Tsujii, 2008; Eisner and Smith, 2005). The
combination of non-projectivity, multi-headedness
and disconnected sub-graphs in a single dataset,
however, poses a challenge for dependency parsing.
In addition to performing evaluation in batch
mode over complete threads, we consider the task of
?in situ thread classification?, whereby we predict
the discourse structure of a thread after each post.
This is intended to simulate the more realistic set-
ting of incrementally crawling/updating thread data,
but needing to predict discourse structure for partial
threads. We are interested in determining the rela-
tive degradation in accuracy for in situ classification
vs. batch classification.
As our dataset, we use the CNET forum dataset
of Kim et al (2010b),1 which contains 1332 an-
notated posts spanning 315 threads, collected from
the Operating System, Software, Hardware and Web
Development sub-forums of cnet.2 Each post is la-
belled with one or more links (including the possi-
bility of null-links, where the post doesn?t link to
any other post), and each link is labelled with a di-
alogue act. The dialogue act set is made up of 5
super-categories: Question, Answer, Resolution
(confirmation of the question being resolved), Re-
production (external confirmation of a proposed so-
lution working) and Other. The Question category
contains 4 sub-classes: Question, Add, Confirma-
tion and Correction. Similarly, the Answer cate-
gory contains 5 sub-classes: Answer, Add, Confir-
mation, Correction and Objection. For example,
the label Question-Add signifies the Question su-
perclass and Add subclass, i.e. addition of extra in-
formation to a question. For full details of the dia-
logue act tagset, see Kim et al (2010b).
Dependency links are represented by their relative
position in the chronologically-sorted list of posts,
e.g. 1 indicates a link back to the preceding post,
and 2 indicates a link back two posts.
Unless otherwise noted, evaluation is over the
combined link and dialogue act tag, including the
combination of superclass and subclass for the
Question and Answer dialogue acts. For ex-
ample, 1+Answer-Answer indicates a dependency
link back one post, which is an answer to a question.
The most common label in the dataset is 1+Answer-
answer (28.4%).
4 Learners and Features
4.1 Learners
To predict thread discourse structure, we use a struc-
tured classification approach ? based on the find-
ings of Kim et al (2010b) and Kim et al (2010a)
? and a dependency parser. The structured clas-
sification approach we experiment with is a linear-
1Available from http://www.csse.unimelb.edu.
au/research/lt/resources/conll2010-thread/
2http://forums.cnet.com/
16
chain conditional random field learner (CRF: Laf-
ferty et al (2001)), within which we explore two
simple approaches to joint classification, as is ex-
plained in Section 5.1. Dependency parsing (Ku?bler
et al, 2009) is the task of automatically predicting
the dependency structure of a token sequence, in
the form of binary asymmetric dependency relations
with dependency types.
Standardly, CRFs have been applied to tasks such
as part-of-speech tagging, named entity recognition,
semantic role labelling and supertagging, where the
individual tokens are single words. Similarly, de-
pendency parsing is conventionally applied to sen-
tences, with single-word tokens. In our case, our
tokens are thread posts, with much greater scope for
feature engineering than single words, and techni-
cal challenges in scaling the underlying implemen-
tations to handle potentially much larger feature sets.
As our learners, we deployed CRFSGD (Bot-
tou, 2011) to learn the CRF, and MaltParser (Nivre
et al, 2007) as our dependency parser. CRFSGD
uses stochastic gradient descent to efficiently solve
the convex optimisation problem, and scales well to
large feature sets. We used the default parameter set-
tings for CRFSGD, with feature templates includ-
ing all unigram features of the current token as well
as bigram features combining the previous output to-
ken with the current token.
MaltParser implements transition-based parsing,
where no formal grammar is considered, and a tran-
sition system, or state machine, is learned to map a
sentence onto its dependency graph. One feature of
MaltParser that makes it well suited to our task is
that it is possible to define feature models of arbi-
trary complexity for each token. In presenting the
thread data to MaltParser, we represent the null-
link from the initial post of each thread, as well as
any disconnected posts, as the root.
To the best of our knowledge, there is no past
work on using dependency parsing to learn thread
discourse structure. Based on extensive experimen-
tation, we determined that the MaltParser configu-
ration that obtains the best results for our task is the
Nivre algorithm in arc-standard mode (Nivre, 2003;
Nivre, 2004), using LIBSVM (Chang and Lin, 2011)
with a linear kernel as the learner, and a feature
model with exhaustive combinations of features re-
lating to the features and predictions of the first/top
three tokens from both ?Input? and ?Stack?.3 As
such, MaltParser is actually unable to predict any
non-projective structures, as experiments with algo-
rithms supporting non-projective structures invari-
ably led to lower results. In our choice of parsing al-
gorithm, we are also unable to detect posts with mul-
tiple heads, but can potentially detect disconnected
sub-graphs.
4.2 Features
The features used in our classifiers are as follows:
Structural Features:
Initiator a binary feature indicating whether the
current post?s author is the thread initiator.
Position the relative position of the current post,
as a ratio over the total number of posts in the
thread.
Semantic Features:
TitSim the relative location of the post which has
the most similar title (based on unweighted co-
sine similarity) to the current post.
PostSim the relative location of the post which
has the most similar content (based on un-
weighted cosine similarity) to the current post.
Punct the number of question marks (QuCount),
exclamation marks (ExCount) and URLs
(UrlCount) in the current post.
UserProf the class distribution (in the training
thread) of the author of the current post.
These features are drawn largely from the work
of Kim et al (2010b), with two major differences:
(1) we do not use post context features because our
learners (i.e. CRFSGD and MaltParser) inherently
capture Markov chains; and (2) our UserProf fea-
tures are customised to the class set associated with
the task at hand, e.g. the UserProf features for the
standalone linking task take the form of the link la-
bels (and not dialogue act labels) of the posts by the
relevant author in the training data. Table 1 shows
the feature representation of the third post in a thread
17
Feature Value Explanation
Initiator 1.0 post from the initiator
ExCount 4.0 4 exclamation marks
QuCount 0.0 0 question marks
UrlCount 0.0 0 URLs
Position 0.25 i?1n = 3?18PostSim 2.0 most similar to post 1
TitSim 2.0 most similar to post 1
UserProf ~x counts for posts of each
class from the same author
in the training data
Table 1: The feature presentation of the third post in a
thread of length 8
of length 8. The values of each feature are scaled to
the range [0, 1] before being fed into the learners.
We also experimented with other features,
including raw bag-of-words lexical features,
dimensionality-reduced lexical features (using
principal components analysis), and different post
similarity measures such as longest common subse-
quence (LCS) match. While we were able to obtain
gains in isolation, when combined with the other
features, these features had no impact, and are thus
not included in the results presented in this paper.
5 Classification Methodology
All our experiments were carried out based on strati-
fied 10-fold cross-validation, stratifying at the thread
level to ensure that all posts from a given thread
occur in a single fold. The results are primarily
evaluated using post-level micro-averaged F-score
(F?: ? = 1), and additionally with thread-level F-
score/classification accuracy (i.e. the proportion of
threads where all posts have been correctly classi-
fied4), where space allows. Statistical significance
is tested using randomised estimation (Yeh, 2000)
with p < 0.05. Initial experiments showed it is
hard for learners to discover which posts have multi-
ple links, largely due to the sparsity of multi-headed
posts (which account for less than 5% of the total
posts). Therefore, only the the most recent link for
3http://maltparser.org/userguide.html#
parsingalg
4Classification accuracy = F-score at the thread-level, as
each thread is assigned a single label of correct or incorrect.
each multi-headed post was included in training, but
evaluation still considers all links.
5.1 Joint classification
In our experiments, we test two basic approaches to
joint classification for the CRF: (1) classifying the
Link and DA separately, and composing the predic-
tions to form the joint classification (Composition);
and (2) combining the Link and DA labels into a sin-
gle class, and applying the learner over the posts
with the combined class (Combine). Note that
Composition has the potential for mismatches in
the number of Link and DA predictions it gener-
ates, causing complications in the class composition.
Even if the same number of labels is predicted for
both Link and DA, if multiple tags are predicted in
both cases, we are left with the problem of determin-
ing which link label to combine with which dialogue
act label. As such, we have our reservations about
Composition, but as the CRF performs strict 1-of-
n labelling, these are not issues in the experiments
reported herein.
MaltParser natively handles the combination of
Link and DA in its dependency parsing formulation.
5.2 In Situ Thread Classification
One of the biggest challenges in classifying the dis-
course structure of a forum thread is that threads
evolve over time, as new posts are posted. In or-
der to capture this phenomenon, and compare the
accuracy of different models when applied to partial
thread data (artificially cutting off a thread at post
N ) vs. complete threads.5 This is done in the fol-
lowing way: classification over the first two posts
only ([1, 2]), the first four posts ([1, 4]), the first six
posts ([1, 6]), the first eight posts ([1, 8]), and all
posts ([all]). In each case, we limit the test data
only, meaning that the only variable in play is the
extent of thread context used to learn the thread dis-
course structure for the given set of posts. We break
down the results in each case into the indicated sub-
threads, e.g. we take the predictions for [all], and
break them down into the results for [1, 2], [1, 4],
[1, 6], [1, 8] and [all], for direct comparison with the
predictions over the respective sub-thread data.
5In practice, completeness is defined at a given point in time,
when the crawl was done, and it is highly likely that some of the
?complete? threads had extra posts after the crawl.
18
Method Link DA
Kim et al (2010b) .863 / .676 .751 / .543
CRFSGD .891 / .727 .795 / .609
Table 2: Post/thread-level component-wise classification
F-scores for Link and DA classes
6 Experiments and Analysis
6.1 Joint classification
As our baseline for the task, we first use a sim-
ple majority class classifier in the form of the sin-
gle joint class of 1+Answer-Answer for all posts,
which has a post-level F-score of 0.284. A stronger
baseline is to classify all first posts as 0+Question-
Question and all subsequent posts as 1+Answer-
answer, which achieves a post-level F-score of
0.515 (labelled as Heuristic).
As described in Section 5.1, one approach to joint
classification with CRFSGD is to firstly conduct
component-wise classification over Link and DA
separately, and compose the predictions. The results
for the separate Link and DA classification tasks are
presented in Table 2, along with the best results for
Link and DA classification from Kim et al (2010b).
At the component-wise tasks, our method is superior
to Kim et al (2010b), based on a different learner
and slightly different feature set.
Next, we compose the component-wise clas-
sifications for the CRF into joint classifications
(Composition). We contrast this with the com-
bined class approach for CRFSGD and MaltParser
(jointly presented as Joint in Table 3). With the
combined class results, we additionally ablate each
of the feature types from Section 4.2, and also
present results for a dummy model, where no fea-
tures are provided and the prediction is based simply
on sequential priors (Dummy). The results are pre-
sented in Table 3, along with the Heuristic baseline
result.
Several interesting things can be observed from
the post-level F-score results in Table 3. First, with
no features (Dummy), while CRFSGD performs
slightly worse than the Heuristic baseline, Malt-
Parser significantly surpasses the baseline. This is
due to the richer sequential context model of Malt-
Parser. Second, the single feature with the greatest
impact on results is UserProf, i.e. user profile fea-
Method CRFSGD MaltParser
Heuristic .515?/ .311?
Dummy .508?/ .394? .533?/ .356?
Composition .728?/ .553? ?
Joint +ALL .756 / .578 .738 / .578
?Initiator .745 / .569 .708?/ .534?
?Position .750 / .565 .736 / .568
?PostSim .753 / .578 .737 / .568
?TitSim .760 / .587 .734 / .571
?Punct .745 / .571 .735 / .578
?UserProf .672?/ .527? .701?/ .536?
Table 3: Post/thread-level Link-DA joint classification F-
scores (??? signifies a significantly worse result than that
for the same learner with ALL features)
tures extracted from the training data; CRFSGD in
particular benefits from this feature. We return to ex-
plore this effect in Section 6.4. Third, although the
Initiator feature does not have much effect on CRF-
SGD, it affects the performance of MaltParser sig-
nificantly. Further experiments shown that the com-
bination of Initiator and UserProf is sufficient to
achieve a competitive result (i.e. 0.731). It therefore
seems that MaltParser is more robust than CRF-
SGD, whose performance relies crucially on user-
level features which must be learned from the train-
ing data (i.e. UserProf).
Looking to the thread-level F-scores, we observe
some interesting divergences from the post-level F-
score results. First, with no features (Dummy),
CRFSGD significantly outperforms both the base-
line and MaltParser. This appears to be because
CRFSGD performs particularly well over short
threads (e.g. of length 3 and 4), but worse over
longer threads. Second, the best thread-level F-
scores from CRFSGD (i.e. 0.587) and MaltParser
(i.e. 0.578) are not significantly different, despite the
discrepancy in post-level F-score (where CRFSGD
is markedly superior in this case). With the extra
features, the performance of MaltParser on short
threads appears to pick up noticeably, and the differ-
ence in post-level predictions is over longer threads.
If we evaluate the two models over DA super-
classes only (ignoring mismatches at the subclass
level for Question and Answer), the post-level F-
scores for joint classification with ALL features for
CRFSGD and MaltParser are 0.803 and 0.787, re-
spectively.
19
Approaches Link DA
Component-wise .891 / .727? .795 / .609
CRFSGD decomp .893 / .749 .785 / .603
MaltParser decomp .870?/ .730? .766?/ .571?
Table 4: Post/thread-level Link and DA F-scores from
component-wise classification, and from Link-DA clas-
sification decomposition (??? signifies a significantly
worse result than the best result in that column)
Looking at the performance of CRFSGD (in
Combine mode) and MaltParser on disconnected
sub-graphs, while both models did predict a small
number of non-initial posts with null-links (includ-
ing MaltParser predicting 5 out of 6 posts in a sin-
gle thread as having null-links), none were correct,
and neither model was able to correctly predict any
of the 6 actual non-initial instances of null-links in
the dataset.
Finally, we took the joint classification results
from CRFSGD and MaltParser using ALL fea-
tures, and decomposed the predictions into Link and
DA. The results are presented in Table 4, along with
the results for component-wise classification from
Table 2. Somewhat surprisingly, the decomposed
predictions are mostly slightly worse than the re-
sults for the component-wise classification, despite
achieving higher F-score for the joint classification
task. This is simply due to the combined method
tending to get both labels correct or both labels
wrong, for a given post.
6.2 Post Position-based Result Breakdown
One question in thread discourse structure classifica-
tion is how accurate the predictions are at different
depths in a thread (e.g. the first two posts vs. the sec-
ond two posts). A breakdown of results across posts
at different positions is presented in Figure 2.
The overall trend for both CRFSGD and Malt-
Parser is that it becomes increasingly hard to clas-
sify posts as we continue through a thread, due to
greater variability in discourse structure and greater
sparsity in the data. However, it is interesting to note
that the results for CRFSGD actually improve from
posts 7 and 8 ([7, 8]) to posts 9 and onwards ([9, ]).
To further investigate this effect, we performed class
decomposition over the joint classification predic-
tions, and performed a similar breakdown of posts
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posts
F ?
 
 CRFSGDMaltParser
Figure 2: Breakdown of post-level Link-DA results for
CRFSGD and MaltParser based on post position
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed Link
 
 CRFSGDMaltParser
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed DA
 
 CRFSGDMaltParser
Figure 3: Breakdown of post-level Link and DA F-score
based on the decomposition of CRFSGD and Malt-
Parser classifications
for Link and DA; the results are presented in Fig-
ure 3. It is clear that the anomaly for CRFSGD
comes from the DA component, due to there being
greater predictability in the dialogue for final posts
in a thread (users tend to confirm a successful reso-
lution of the problem, or report on successful exter-
nal reproduction of the solution). MaltParser seems
less adept at identifying that a post is at the end
of a thread, and predicting the dialogue act accord-
ingly. This observation is congruous with the find-
ings of McDonald and Nivre (2007) that errors prop-
agate, due to MaltParser?s greedy inference strat-
egy. The higher results for Link are to be expected,
as throughout the thread, most posts tend to link lo-
cally.
20
XXXXXXXXXTest
B/down [1, 2] [1, 4] [1, 6] [1, 8] [All]
[1, 2] .947/.947 ? ? ? ?
[1, 4] .946/.947 .836/.841 ? ?
[1, 6] .946/.947 .840/.841 .800/.794 ? ?
[1, 8] .946/.947 .840/.841 .800/.794 .780/.769 ?
[All] .946/.946 .840/.838 .800/.791 .776/.767 .756/.738
Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of
different lengths (indicated in the rows), broken down over different post extents (indicated in the columns)
6.3 In Situ Structure Prediction
As described in Section 5.2, we simulate in situ
thread discourse structure prediction by removing
differing numbers of posts from the tail of the thread,
and applying the trained model over the resultant
sub-threads. The results for in situ classification are
presented in Table 5, with the rows indicating the
size of the test sub-thread, and the columns being a
breakdown of results over different portions of the
classified thread. The reason that we do not pro-
vide numbers for all cells in the table is that the size
of the test sub-thread determines the post extents we
can breakdown the results into, e.g. we cannot return
results for posts 1?4 ([1, 4]) when the size of the test
thread was only two posts ([1, 2]).
From the results, we can see that both CRFSGD
and MaltParser are very robust when applied to par-
tial threads, to the extent that we actually achieve
higher results over shortened versions of the thread
than over the complete thread in some instances, al-
though the only difference that is statistically signif-
icant is over [1, 8] for CRFSGD, where the predic-
tion over the partial thread is actually superior to that
over the complete thread. From this, we can con-
clude that it is possible to apply our method to partial
threads without any reduction in effectiveness rela-
tive to classification over complete threads. As such,
our method is shown to be robust when applied to
real-time analysis of dynamically evolving threads.
6.4 User profile feature analysis
In our experiments, we noticed that the user profile
feature (UserProf) is the most effective feature for
both CRFSGD and MaltParser. To gain a deeper
insight into the behaviour of the feature, we binned
the posts according to the number of times the author
had posted in the training data, evaluated based on a
Bin uscore Posts Total Totalper user users posts
High 224.6 251 1 251
Medium 1?41.7 4?48 45 395
Low 0 2?4 157 377
Very Low 0 1 309 309
Table 6: Statistics for the 4 groups of users
user score (uscore) for each user:
uscorei =
?ni
j=1 spi,j
ni
where ni is the number of posts by user i, and spi,j is
the number of posts by user i that occur as training
instances for other posts by the same author. uscore
reflects the average training?test post ratio per user
in cross-validation. Note that as we include all posts
from a given thread in a single partition during cross-
validation, it is possible for an author to have posted
4 times, but have a uscore of 0 due to those posts all
occurring in the same thread.
We ranked the users in the dataset in descending
order of uscore, sub-ranking on ni in cases of a tie
in uscore. The users were binned into 4 groups
of roughly equal post size. The detailed statistics
are shown in Table 6, noting that the high-frequency
bin (?High?) contains posts from a single user. We
present the post-level micro-averaged F-score for
posts in each bin based on CRFSGD, with and with-
out user profile features, in Figure 4.
Contrary to expectation, the UserProf features
have the greatest impact for users with fewer posts.
In fact, a statistically significant difference was ob-
served only for users with no posts in the training
data (uscore = 0), where the F-score jumped over
10% in absolute terms for both the Low and Very
Low bins. Our explanation for this effect is that the
21
High Median Low Very Low0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
User Group
F ?
 
 With UserProfWithout UserProf
Figure 4: Post-level joint classification results for users
binned by uscore, based on CRFSGD with and without
UserProf features)
lack of user profile information is predictive of the
sort of posts we can expect from a user (i.e. they
tend to be newbie users, asking questions).
7 Conclusions and Future Work
In this research, we explored the joint classification
of web user forum thread discourse structure, in the
form of a rooted directed acyclic graph over posts,
with edges labelled with dialogue acts. Three classi-
fication approaches were proposed: separately pre-
dicting Link and DA labels, and composing them
into a joint class; predicting a combined Link-DA
class using a structured classifier; and applying de-
pendency parsing to the problem. We found the
combined approach based on CRFSGD to perform
best over the task, closely followed by dependency
parsing with MaltParser.
We also examined the task of in situ classification
of dialogue structure, in the form of predicting the
discourse structure of partial threads, as contrasted
with classifying only complete threads. We found
that there was no drop in F-score over different sub-
extents of the thread in classifying partial threads,
despite the relative lack of thread context.
In future work, we plan to delve further into de-
pendency parsing, looking specifically at the impli-
cations of multi-headedness and disconnected sub-
graphs on dependency parsing. We also intend to
carry out meta-classification, combining the predic-
tions of CRFSGD and MaltParser.
Our user profile features were found to be the
pick of our features, but counter-intuitively, to bene-
fit users with no posts in the training data, rather than
prolific users. We wish to explore this effect further,
including incorporating unsupervised user-level fea-
tures into our classifiers.
Acknowledgements
The authors wish to acknowledge the development
efforts of Johan Hall in configuring MaltParser to
handle numeric features, and be able to parse thread
structures. NICTA is funded by the Australian gov-
ernment as represented by Department of Broad-
band, Communication and Digital Economy, and the
Australian Research Council through the ICT Centre
of Excellence programme.
References
Le?on Bottou. 2011. CRFSGD software. http://
leon.bottou.org/projects/sgd.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management (CIKM 2009), pages
265?274, Hong Kong, China.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?. In
Proceedings of 28th International ACM-SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR 2005), pages 345?352.
Jeffrey Chan and Conor Hayes. 2010. Decomposing dis-
cussion forums using user roles. In Proceedings of the
WebSci10: Extending the Frontiers of Society On-Line
(WebSci10), pages 1?8, Raleigh, USA.
Jeffrey Chan, Conor Hayes, and Elizabeth M. Daly.
2010. Decomposing discussion forums using user
roles. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media (ICWSM
2010), pages 215?8, Washington, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 309?316, Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
22
pairs from online forums. In Proceedings of 31st Inter-
national ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?08), pages
467?474, Singapore.
Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and seman-
tic roles of prepositional phrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2009), pages 450?458,
Singapore. Association for Computational Linguistics.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
text and answers of questions from online forums. In
Proceedings of the 46th Annual Meeting of the ACL:
HLT (ACL 2008), pages 710?718, Columbus, USA.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 30?41, Vancouver, Canada.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?09), pages
714?715, Boston, USA.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?842,
Columbus, USA.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2009), pages 326?334, Boulder, Col-
orado. Association for Computational Linguistics.
Blaz Fortuna, Eduarda Mendes Rodrigues, and Natasa
Milic-Frayling. 2007. Improving the classification of
newsgroup messages through social network analysis.
In Proceedings of the 16th ACM Conference on In-
formation and Knowledge Management (CIKM 2007),
pages 877?880, Lisbon, Portugal.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010a. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010), pages 862?871, Boston, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Proceedings
of the 14th Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 192?202, Upp-
sala, Sweden.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on Hu-
man Language Technologies, 2(1):1?127.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289, Williamstown, USA.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008.
The nature of requests and commitments in email mes-
sages. In Proceedings of the AAAI 2008 Workshop on
Enhanced Messaging, pages 42?47, Chicago, USA.
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-
nie Skariah, Jian Yang, and Graciela Gonzalez. 2010.
Towards internet-age pharmacovigilance: Extracting
adverse drug reactions from user posts in health-
related social networks. In Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing (ACL 2010), pages 117?125, Uppsala, Sweden.
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL), Special Issue on Dialogue, 43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling semantics
and structure of discussion threads. In Proceedings of
the 18th International Conference on the World Wide
Web (WWW 2009), pages 1103?1104, Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse. In
Proceedings of the 14th Australasian Document Com-
puting Symposium (ADCS 2009), Sydney, Australia.
Marco Lui and Timothy Baldwin. 2010. Classifying
user forum participants: Separating the gurus from the
hacks, and other tales of the internet. In Proceedings
of the 2010 Australasian Language Technology Work-
shop (ALTW 2010), pages 49?57, Melbourne, Aus-
tralia.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 122?131, Prague,
Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
23
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2006), pages 81?88, Trento,
Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, Canada.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings
of the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 367?
374.
Klemens Muthmann, Wojciech M. Barczyn?ski, Falk
Brauer, and Alexander Lo?ser. 2009. Near-duplicate
detection for web-forums. In Proceedings of the 2009
International Database Engineering & Applications
Symposium (IDEAS 2009), pages 142?151, Cetraro,
Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95?135.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop Incremental Parsing: Bringing Engineer-
ing and Cognition Together (ACL-2004), pages 50?57,
Barcelona, Spain.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995. Discourse
processing of dialogues with multiple threads. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 31?38,
Cambridge, USA.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), pages 753?760, Manchester,
UK.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT-09), pages 81?
84, Paris, France.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in comments
on news-articles. In Proceedings of the 9th Annual
ACM International Workshop on Web Information and
Data Management, pages 97?104, Lisboa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, USA.
Parikshit Sondhi, Manish Gupta, ChengXiang Zhai, and
Julia Hockenmaier. 2010. Shallow information ex-
traction from medical forum data. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Posters Volume, pages
1158?1166, Beijing, China.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 149?156, Edmonton,
Canada.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Pail
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 225?228, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 64?71, Washington, USA.
Nayer Wanas, Motaz El-Saban, Heba Ashour, and
Waleed Ammar. 2008. Automatic scoring of online
discussion posts. In Proceeding of the 2nd ACM work-
shop on Information credibility on the web (WICOW
?08), pages 19?26, Napa Valley, USA.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Mak-
ing conversational structure explicit: identification of
initiation-response pairs within online discussions. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 673?676.
24
Yi-Chia Wang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions
(ACL 2007), pages 73?76, Prague, Czech Republic.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages 152?
160, Seattle, USA.
V. Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated dialog act segmentation and classification
using prosodic features and language models. In Proc.
Eurospeech, volume 1, pages 207?210.
Markus Weimer and Iryna Gurevych. 2007. Predicting
the perceived quality of web forum posts. In Proceed-
ings of the 2007 International Conference on Recent
Advances in Natural Language Processing (RANLP
2007), pages 643?648, Borovets, Bulgaria.
Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.
2007. Automatically assessing the post quality in on-
line discussions on software. In Proceedings of the
45th Annual Meeting of the ACL: Interactive Poster
and Demonstration Sessions, pages 125?128, Prague,
Czech Republic.
Armin Weinberger and Frank Fischer. 2006. A
framework to analyze argumentative knowledge con-
struction in computer-supported collaborative learn-
ing. Computers & Education, 46:71?95, January.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search. In
Proceedings of 27th International ACM-SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR 2004), pages 394?401. Sheffield,
UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
25
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 421?432, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Automatically Constructing a Normalisation Dictionary for Microblogs
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
Microblog normalisation methods often utilise
complex models and struggle to differenti-
ate between correctly-spelled unknown words
and lexical variants of known words. In this
paper, we propose a method for construct-
ing a dictionary of lexical variants of known
words that facilitates lexical normalisation via
simple string substitution (e.g. tomorrow for
tmrw). We use context information to generate
possible variant and normalisation pairs and
then rank these by string similarity. Highly-
ranked pairs are selected to populate the dic-
tionary. We show that a dictionary-based ap-
proach achieves state-of-the-art performance
for both F-score and word error rate on a stan-
dard dataset. Compared with other methods,
this approach offers a fast, lightweight and
easy-to-use solution, and is thus suitable for
high-volume microblog pre-processing.
1 Lexical Normalisation
A staggering number of short text ?microblog? mes-
sages are produced every day through social me-
dia such as Twitter (Twitter, 2011). The immense
volume of real-time, user-generated microblogs that
flows through sites has been shown to have utility
in applications such as disaster detection (Sakaki et
al., 2010), sentiment analysis (Jiang et al2011;
Gonza?lez-Iba?n?ez et al2011), and event discovery
(Weng and Lee, 2011; Benson et al2011). How-
ever, due to the spontaneous nature of the posts,
microblogs are notoriously noisy, containing many
non-standard forms ? e.g., tmrw ?tomorrow? and
2day ?today? ? which degrade the performance of
natural language processing (NLP) tools (Ritter et
al., 2010; Han and Baldwin, 2011). To reduce this
effect, attempts have been made to adapt NLP tools
to microblog data (Gimpel et al2011; Foster et al
2011; Liu et al2011b; Ritter et al2011). An al-
ternative approach is to pre-normalise non-standard
lexical variants to their standard orthography (Liu et
al., 2011a; Han and Baldwin, 2011; Xue et al2011;
Gouws et al2011). For example, se u 2morw!!!
would be normalised to see you tomorrow! The nor-
malisation approach is especially attractive as a pre-
processing step for applications which rely on key-
word match or word frequency statistics. For ex-
ample, earthqu, eathquake, and earthquakeee ? all
attested in a Twitter corpus ? have the standard
form earthquake; by normalising these types to their
standard form, better coverage can be achieved for
keyword-based methods, and better word frequency
estimates can be obtained.
In this paper, we focus on the task of lexical nor-
malisation of English Twitter messages, in which
out-of-vocabulary (OOV) tokens are normalised to
their in-vocabulary (IV) standard form, i.e., a stan-
dard form that is in a dictionary. Following other re-
cent work on lexical normalisation (Liu et al2011a;
Han and Baldwin, 2011; Gouws et al2011; Liu et
al., 2012), we specifically focus on one-to-one nor-
malisation in which one OOV token is normalised to
one IV word.
Naturally, not all OOV words in microblogs are
lexical variants of IV words: named entities, e.g.,
are prevalent in microblogs, but not all named en-
tities are included in our dictionary. One chal-
lenge for lexical normalisation is therefore to dis-
421
tinguish those OOV tokens that require normalisa-
tion from those that are well-formed. Recent un-
supervised approaches have not attempted to distin-
guish such tokens from other types of OOV tokens
(Cook and Stevenson, 2009; Liu et al2011a), lim-
iting their applicability to real-world normalisation
tasks. Other approaches (Han and Baldwin, 2011;
Gouws et al2011) have followed a cascaded ap-
proach in which lexical variants are first identified,
and then normalised. However, such two-step ap-
proaches suffer from poor lexical variant identifica-
tion performance, which is propagated to the nor-
malisation step. Motivated by the observation that
most lexical variants have an unambiguous standard
form (especially for longer tokens), and that a lexi-
cal variant and its standard form typically occur in
similar contexts, in this paper we propose methods
for automatically constructing a lexical normalisa-
tion dictionary ? a dictionary whose entries consist
of (lexical variant, standard form) pairs ? that en-
ables type-based normalisation.
Despite the simplicity of this dictionary-based
normalisation method, we show it to outperform
previously-proposed approaches. This very fast,
lightweight solution is suitable for real-time pro-
cessing of the large volume of streaming microblog
data available from Twitter, and offers a simple solu-
tion to the lexical variant detection problem that hin-
ders other normalisation methods. Furthermore, this
dictionary-based method can be easily integrated
with other more-complex normalisation approaches
(Liu et al2011a; Han and Baldwin, 2011; Gouws
et al2011) to produce hybrid systems.
After discussing related work in Section 2, we
present an overview of our dictionary-based ap-
proach to normalisation in Section 3. In Sections 4
and 5 we experimentally select the optimised con-
text similarity parameters and string similarity re-
ranking method. We present experimental results on
the unseen test data in Section 6, and offer some con-
cluding remarks in Section 7.
2 Related Work
Given a token t, lexical normalisation is the task
of finding argmaxP (s|t) ? argmaxP (t|s)P (s),
where s is the standard form, i.e., an IV word. Stan-
dardly in lexical normalisation, t is assumed to be an
OOV token, relative to a fixed dictionary. In prac-
tice, not all OOV tokens should be normalised; i.e.,
only lexical variants (e.g., tmrw ?tomorrow?) should
be normalised and tokens that are OOV but other-
wise not lexical variants (e.g., iPad ?iPad?) should
be unchanged. Most work in this area focuses only
on the normalisation task itself, oftentimes assuming
that the task of lexical variant detection has already
been completed.
Various approaches have been proposed to esti-
mate the error model, P (t|s). For example, in work
on spell-checking, Brill and Moore (2000) improve
on a standard edit-distance approach by consider-
ing multi-character edit operations; Toutanova and
Moore (2002) build on this by incorporating phono-
logical information. Li et al2006) utilise distri-
butional similarity (Lin, 1998) to correct misspelled
search queries.
In text message normalisation, Choudhury et al
(2007) model the letter transformations and emis-
sions using a hidden Markov model (Rabiner, 1989).
Cook and Stevenson (2009) and Xue et al2011)
propose multiple simple error models, each of which
captures a particular way in which lexical variants
are formed, such as phonetic spelling (e.g., epik
?epic?) or clipping (e.g., walkin ?walking?). Never-
theless, optimally weighting the various error mod-
els in these approaches is challenging.
Without pre-categorising lexical variants into dif-
ferent types, Liu et al2011a) collect Google
search snippets from carefully-designed queries
from which they then extract noisy lexical variant?
standard form pairs. These pairs are used to train
a conditional random field (Lafferty et al2001) to
estimate P (t|s) at the character level. One short-
coming of querying a search engine to obtain train-
ing pairs is it tends to be costly in terms of time and
bandwidth. Here we exploit microblog data directly
to derive (lexical variant, standard form) pairs, in-
stead of relying on external resources. In more-
recent work, Liu et al2012) endeavour to improve
the accuracy of top-n normalisation candidates by
integrating human cognitive inference, character-
level transformations and spell checking in their nor-
malisation model. The encouraging results shift the
focus to reranking and promoting the correct nor-
malisation to the top-1 position. However, like much
previous work on lexical normalisation, this work
422
assumes perfect lexical variant detection.
Aw et al2006) and Kaufmann and Kalita (2010)
consider normalisation as a machine translation task
from lexical variants to standard forms using off-the-
shelf tools. These methods do not assume that lexi-
cal variants have been pre-identified; however, these
methods do rely on large quantities of labelled train-
ing data, which is not available for microblogs.
Recently, Han and Baldwin (2011) and Gouws
et al2011) propose two-step unsupervised ap-
proaches to normalisation, in which lexical vari-
ants are first identified, and then normalised. They
approach lexical variant detection by using a con-
text fitness classifier (Han and Baldwin, 2011) or
through dictionary lookup (Gouws et al2011).
However, the lexical variant detection of both meth-
ods is rather unreliable, indicating the challenge
of this aspect of normalisation. Both of these
approaches incorporate a relatively small normal-
isation dictionary to capture frequent lexical vari-
ants with high precision. In particular, Gouws et
al. (2011) produce a small normalisation lexicon
based on distributional similarity and string simi-
larity (Lodhi et al2002). Our method adopts a
similar strategy using distributional/string similarity,
but instead of constructing a small lexicon for pre-
processing, we build a much wider-coverage nor-
malisation dictionary and opt for a fully lexicon-
based end-to-end normalisation approach. In con-
trast to the normalisation dictionaries of Han and
Baldwin (2011) and Gouws et al2011) which fo-
cus on very frequent lexical variants, we focus on
moderate frequency lexical variants of a minimum
character length, which tend to have unambiguous
standard forms; our intention is to produce normali-
sation lexicons that are complementary to those cur-
rently available. Furthermore, we investigate the im-
pact of a variety of contextual and string similarity
measures on the quality of the resulting lexicons.
In summary, our dictionary-based normalisation ap-
proach is a lightweight end-to-end method which
performs both lexical variant detection and normal-
isation, and thus is suitable for practical online pre-
processing, despite its simplicity.
3 A Lexical Normalisation Dictionary
Before discussing our method for creating a normal-
isation dictionary, we first discuss the feasibility of
such an approach.
3.1 Feasibility
Dictionary lookup approaches to normalisation have
been shown to have high precision but low recall
(Han and Baldwin, 2011; Gouws et al2011). Fre-
quent (lexical variant, standard form) pairs such as
(u, you) are typically included in the dictionaries
used by such methods, while less-frequent items
such as (g0tta, gotta) are generally omitted. Be-
cause of the degree of lexical creativity and large
number of non-standard forms observed on Twitter,
a wide-coverage normalisation dictionary would be
expensive to construct manually. Based on the as-
sumption that lexical variants occur in similar con-
texts to their standard forms, however, it should
be possible to automatically construct a normalisa-
tion dictionary with wider coverage than is currently
available.
Dictionary lookup is a type-based approach to
normalisation, i.e., every token instance of a given
type will always be normalised in the same way.
However, lexical variants can be ambiguous, e.g., y
corresponds to ?you? in yeah, y r right! LOL but
?why? in AM CONFUSED!!! y you did that? Nev-
ertheless, the relative occurrence of ambiguous lex-
ical variants is small (Liu et al2011a), and it has
been observed that while shorter variants such as y
are often ambiguous, longer variants tend to be un-
ambiguous. For example bthday and 4eva are un-
likely to have standard forms other than ?birthday?
and ?forever?, respectively. Therefore, the normali-
sation lexicons we produce will only contain entries
for OOVs with character length greater than a spec-
ified threshold, which are likely to have an unam-
biguous standard form.
3.2 Overview of approach
Our method for constructing a normalisation dictio-
nary is as follows:
Input: Tokenised English tweets
1. Extract (OOV, IV) pairs based on distributional
similarity.
423
2. Re-rank the extracted pairs by string similarity.
Output: A list of (OOV, IV) pairs ordered by string
similarity; select the top-n pairs for inclusion in
the normalisation lexicon.
In Step 1, we leverage large volumes of Twitter
data to identify the most distributionally-similar IV
type for each OOV type. The result of this pro-
cess is a set of (OOV, IV) pairs, ranked by dis-
tributional similarity. The extracted pairs will in-
clude (lexical variant, standard form) pairs, such as
(tmrw, tomorrow), but will also contain false posi-
tives such as (Tusday, Sunday) ? Tusday is a lexical
variant, but its standard form is not ?Sunday? ? and
(Youtube,web) ? Youtube is an OOV named en-
tity, not a lexical variant. Nevertheless, lexical vari-
ants are typically formed from their standard forms
through regular processes (Thurlow, 2003) ? e.g.,
the omission of characters ? and from this per-
spective Sunday and web are not plausible standard
forms for Tusday and Youtube, respectively. In Step
2, we therefore capture this intuition to re-rank the
extracted pairs by string similarity. The top-n items
in this re-ranked list then form the normalisation lex-
icon, which is based only on development data.
Although computationally-expensive to build,
this dictionary can be created offline. Once built,
it then offers a very fast approach to normalisation.
We can only reliably compute distributional simi-
larity for types that are moderately frequent in a cor-
pus. Nevertheless, many lexical variants are suffi-
ciently frequent to be able to compute distributional
similarity, and can potentially make their way into
our normalisation lexicon. This approach is not suit-
able for normalising low-frequency lexical variants,
nor is it suitable for shorter lexical variant types
which ? as discussed in Section 3.1 ? are more
likely to have an ambiguous standard form. Never-
theless, previously-proposed normalisation methods
that can handle such phenomena also rely in part on
a normalisation lexicon. The normalisation lexicons
we create can therefore be easily integrated with pre-
vious approaches to form hybrid normalisation sys-
tems.
4 Contextually-similar Pair Generation
Our objective is to extract contextually-similar
(OOV, IV) pairs from a large-scale collection of mi-
croblog data. Fundamentally, the surrounding words
define the primary context, but there are different
ways of representing context and different similar-
ity measures we can use, which may influence the
quality of generated normalisation pairs.
In representing the context, we experimentally ex-
plore the following factors: (1) context window size
(from 1 to 3 tokens on both sides); (2) n-gram or-
der of the context tokens (unigram, bigram, trigram);
(3) whether context words are indexed for relative
position or not; and (4) whether we use all context
tokens, or only IV words. Because high-accuracy
linguistic processing tools for Twitter are still under
exploration (Liu et al2011b; Gimpel et al2011;
Ritter et al2011; Foster et al2011), we do not
consider richer representations of context, for exam-
ple, incorporating information about part-of-speech
tags or syntax. We also experiment with a number
of simple but widely-used geometric and informa-
tion theoretic distance/similarity measures. In par-
ticular, we use Kullback?Leibler (KL) divergence
(Kullback and Leibler, 1951), Jensen?Shannon (JS)
divergence (Lin, 1991), Euclidean distance and Co-
sine distance.
We use a corpus of 10 million English tweets to do
parameter tuning over, and a larger corpus of tweets
in the final candidate ranking. All tweets were col-
lected from September 2010 to January 2011 via
the Twitter API.1 From the raw data we extract
English tweets using a language identification tool
(Lui and Baldwin, 2011), and then apply a simpli-
fied Twitter tokeniser (adapted from O?Connor et al
(2010)). We use the Aspell dictionary (v6.06)2 to
determine whether a word is IV, and only include
in our normalisation dictionary OOV tokens with
at least 64 occurrences in the corpus and character
length ? 4, both of which were determined through
empirical observation. For each OOV word type in
the corpus, we select the most similar IV type to
form (OOV, IV) pairs. To further narrow the search
space, we only consider IV words which are mor-
phophonemically similar to the OOV type, follow-
ing settings in Han and Baldwin (2011).3
1https://dev.twitter.com/docs/
streaming-api/methods
2http://aspell.net/
3We only consider IV words within an edit distance of 2 or a
phonemic edit distance of 1 from the OOV type, and we further
424
In order to evaluate the generated pairs, we ran-
domly selected 1000 OOV words from the 10 mil-
lion tweet corpus. We set up an annotation task
on Amazon Mechanical Turk,4 presenting five in-
dependent annotators with each word type (with no
context) and asking for corrections where appropri-
ate. For instance, given tmrw, the annotators would
likely identify it as a non-standard variant of ?to-
morrow?. For correct OOV words like iPad, on the
other hand, we would expect them to leave the word
unchanged. If 3 or more of the 5 annotators make
the same suggestion (in the form of either a canoni-
cal spelling or leaving the word unchanged), we in-
clude this in our gold standard for evaluation. In
total, this resulted in 351 lexical variants and 282
correct OOV words, accounting for 63.3% of the
1000 OOV words. These 633 OOV words were used
as (OOV, IV) pairs for parameter tuning. The re-
mainder of the 1000 OOV words were ignored on
the grounds that there was not sufficient consensus
amongst the annotators.5
Contextually-similar pair generation aims to in-
clude as many correct normalisation pairs as pos-
sible. We evaluate the quality of the normalisation
pairs using ?Cumulative Gain? (CG):
CG =
N ??
i=1
rel?i
Suppose there are N ? correct generated pairs
(oovi, ivi), each of which is weighted by rel?i, the
frequency of oovi to indicate its relative importance;
for example, (thinkin, thinking) has a higher weight
than (g0tta, gotta) because thinkin is more frequent
than g0tta in our corpus. In this evaluation we don?t
consider the position of normalisation pairs, and nor
do we penalise incorrect pairs. Instead, we push dis-
tinguishing between correct and incorrect pairs into
the downstream re-ranking step in which we incor-
porate string similarity information.
Given the development data and CG, we run an
exhaustive search of parameter combinations over
only consider the top 30% most-frequent of these IV words.
4https://www.mturk.com/mturk/welcome
5Note that the objective of this annotation task is to identify
lexical variants that have agreed-upon standard forms irrespec-
tive of context, as a special case of the more general task of
lexical normalisation (where context may or may not play a sig-
nificant role in the determination of the normalisation).
our development corpus. The five best parameter
combinations are shown in Table 1. We notice the
CG is almost identical for the top combinations. As
a context window size of 3 incurs a heavy process-
ing and memory overhead over a size of 2, we use
the 3rd-best parameter combination for subsequent
experiments, namely: context window of?2 tokens,
token bigrams, positional index, and KL divergence
as our distance measure.
To better understand the sensitivity of the method
to each parameter, we perform a post-hoc parame-
ter analysis relative to a default setting (as under-
lined in Table 2), altering one parameter at a time.
The results in Table 2 show that bigrams outper-
form other n-gram orders by a large margin (note
that the evaluation is based on a log scale), and
information-theoretic measures are superior to the
geometric measures. Furthermore, it also indicates
using the positional indexing better captures context.
However, there is little to distinguish context mod-
elling with just IV words or all tokens. Similarly,
the context window size has relatively little impact
on the overall performance, supporting our earlier
observation from Table 1.
5 Pair Re-ranking by String Similarity
Once the contextually-similar (OOV, IV) pairs are
generated using the selected parameters in Section
4, we further re-rank this set of pairs in an at-
tempt to boost morphophonemically-similar pairs
like (bananaz, bananas), and penalise noisy pairs
like (paninis, beans).
Instead of using the small 10 million tweet cor-
pus, from this step onwards, we use a larger cor-
pus of 80 million English tweets (collected over the
same period as the development corpus) to develop
a larger-scale normalisation dictionary. This is be-
cause once pairs are generated, re-ranking based on
string comparison is much faster. We only include
in the dictionary OOV words with a token frequency
> 15 to include more OOV types than in Section 4,
and again apply a minimum length cutoff of 4 char-
acters.
To measure how well our re-ranking method pro-
motes correct pairs and demotes incorrect pairs (in-
cluding both OOV words that should not be nor-
malised, e.g. (Youtube,web), and incorrect normal-
425
Rank Window size n-gram Positional index? Lex. choice Sim/distance measure log(CG)
1 ?3 2 Yes All KL divergence 19.571
2 ?3 2 No All KL divergence 19.562
3 ?2 2 Yes All KL divergence 19.562
4 ?3 2 Yes IVs KL divergence 19.561
5 ?2 2 Yes IVs JS divergence 19.554
Table 1: The five best parameter combinations in the exhaustive search of parameter combinations
Window size n-gram Positional index? Lexical choice Similarity/distance measure
?1 19.325 1 19.328 Yes 19.328 IVs 19.335 KL divergence 19.328
?2 19.327 2 19.571 No 19.263 All 19.328 Euclidean 19.227
?3 19.328 3 19.324 JS divergence 19.311
Cosine 19.170
Table 2: Parameter sensitivity analysis measured as log(CG) for correctly-generated pairs. We tune one parameter at
a time, using the default (underlined) setting for other parameters; the non-exhaustive best-performing setting in each
case is indicated in bold.
isations for lexical variants, e.g. (bcuz, cause)), we
modify our evaluation metric from Section 4 to
evaluate the ranking at different points, using Dis-
counted Cumulative Gain (DCG@N : Jarvelin and
Kekalainen (2002)):
DCG@N = rel1 +
N?
i=2
reli
log2 (i)
where reli again represents the frequency of the
OOV, but it can be gain (a positive number) or loss
(a negative number), depending on whether the ith
pair is correct or incorrect. Because we also expect
correct pairs to be ranked higher than incorrect pairs,
DCG@N takes both factors into account.
Given the generated pairs and the evaluation met-
ric, we first consider three baselines: no re-ranking
(i.e., the final ranking is that of the contextual simi-
larity scores), and re-rankings of the pairs based on
the frequencies of the OOVs in the Twitter corpus,
and the IV unigram frequencies in the Google Web
1T corpus (Brants and Franz, 2006) to get less-noisy
frequency estimates. We also compared a variety of
re-rankings based on a number of string similarity
measures that have been previously considered in
normalisation work (reviewed in Section 2). We ex-
periment with standard edit distance (Levenshtein,
1966), edit distance over double metaphone codes
(phonetic edit distance: (Philips, 2000)), longest
common subsequence ratio over the consonant edit
distance of the paired words (hereafter, denoted as
consonant edit distance: (Contractor et al2010)),
and a string subsequence kernel (Lodhi et al2002).
In Figure 1, we present the DCG@N results for
each of our ranking methods at different rank cut-
offs. Ranking by OOV frequency is motivated by
the assumption that lexical variants are frequently
used by social media users. This is confirmed
by our findings that lexical pairs like (goin, going)
and (nite, night) are at the top of the ranking.
However, many proper nouns and named entities
are also used frequently and ranked at the top,
mixed with lexical variants like (Facebook, speech)
and (Youtube,web). In ranking by IV word fre-
quency, we assume the lexical variants are usually
derived from frequently-used IV equivalents, e.g.
(abou, about). However, many less-frequent lexical
variant types have high-frequency (IV) normalisa-
tions. For instance, the highest-frequency IV word
the has more than 40 OOV lexical variants, such as
tthe and thhe. These less-frequent types occupy the
top positions, reducing the cumulative gain. Com-
pared with these two baselines, ranking by default
contextual similarity scores delivers promising re-
sults. It successfully ranks many more intuitive nor-
malisation pairs at the top, such as (2day, today)
and (wknd,weekend), but also ranks some incorrect
pairs highly, such as (needa, gotta).
The string similarity-based methods perform bet-
ter than our baselines in general. Through man-
ual analysis, we found that standard edit dis-
426
tance ranking is fairly accurate for lexical vari-
ants with low edit distance to their standard forms,
but fails to identify heavily-altered variants like
(tmrw, tomorrow). Consonant edit distance is simi-
lar to standard edit distance, but places many longer
words at the top of the ranking. Edit distance
over double metaphone codes (phonetic edit dis-
tance) performs particularly well for lexical vari-
ants that include character repetitions ? commonly
used for emphasis on Twitter ? because such rep-
etitions do not typically alter the phonetic codes.
Compared with the other methods, the string subse-
quence kernel delivers encouraging results. It mea-
sures common character subsequences of length n
between (OOV, IV) pairs. Because it is computa-
tionally expensive to calculate similarity for larger
n, we choose n=2, following Gouws et al2011).
As N (the lexicon size cut-off) increases, the per-
formance drops more slowly than the other meth-
ods. Although this method fails to rank heavily-
altered variants such as (4get, forget) highly, it typi-
cally works well for longer words. Given that we fo-
cus on longer OOVs (specifically those longer than
4 characters), this ultimately isn?t a great handicap.
6 Evaluation
Given the re-ranked pairs from Section 5, here we
apply them to a token-level normalisation task us-
ing the normalisation dataset of Han and Baldwin
(2011).
6.1 Metrics
We evaluate using the standard evaluation metrics of
precision (P), recall (R) and F-score (F) as detailed
below. We also consider the false alarm rate (FA)
and word error rate (WER), also as shown below.
FA measures the negative effects of applying nor-
malisation; a good approach to normalisation should
not (incorrectly) normalise tokens that are already
in their standard form and do not require normalisa-
tion.6 WER, like F-score, shows the overall benefits
of normalisation, but unlike F-score, measures how
many token-level edits are required for the output to
be the same as the ground truth data. In general, dic-
tionaries with a high F-score/low WER and low FA
6FA + P ? 1 because some lexical variants might be incor-
rectly normalised.
are preferable.
P =
# correctly normalised tokens
# normalised tokens
R =
# correctly normalised tokens
# tokens requiring normalisation
F =
2PR
P +R
FA =
# incorrectly normalised tokens
# normalised tokens
WER =
# token edits needed after normalisation
# all tokens
6.2 Results
We select the three best re-ranking methods, and
best cut-off N for each method, based on the
highest DCG@N value for a given method over
the development data, as presented in Figure 1.
Namely, they are string subsequence kernel (S-dict,
N=40,000), double metaphone edit distance (DM-
dict, N=10,000) and default contextual similarity
without re-ranking (C-dict, N=10,000).7
We evaluate each of the learned dictionaries in Ta-
ble 3. We also compare each dictionary with the
performance of the manually-constructed Internet
slang dictionary (HB-dict) used by Han and Bald-
win (2011), the small automatically-derived dictio-
nary of Gouws et al2011) (GHM-dict), and com-
binations of the different dictionaries. In addition,
the contribution of these dictionaries in hybrid nor-
malisation approaches is also presented, in which we
first normalise OOVs using a given dictionary (com-
bined or otherwise), and then apply the normalisa-
tion method of Gouws et al2011) based on con-
sonant edit distance (GHM-norm), or the approach
of Han and Baldwin (2011) based on the summation
of many unsupervised approaches (HB-norm), to the
remaining OOVs. Results are shown in Table 3, and
discussed below.
6.2.1 Individual Dictionaries
Overall, the individual dictionaries derived by the
re-ranking methods (DM-dict, S-dict) perform bet-
7We also experimented with combining ranks using Mean
Reciprocal Rank. However, the combined rank didn?t improve
performance on the development data. We plan to explore other
ranking aggregation methods in future work.
427
N
 c
ut
?o
ffs
Discounted Cumulative Gain
10K
30K
50K
70K
90K
110K
130K
150K
170K
190K
?
60
K
?
40
K
?
20
K0
20
K
40
K
W
ith
ou
t r
er
a
n
k
O
OV
 fr
eq
ue
nc
y
IV
 fr
eq
ue
nc
y
Ed
it 
di
st
an
ce
Co
ns
on
an
t e
di
t d
ist
.
Ph
on
et
ic
 e
di
t d
ist
.
St
rin
g 
su
bs
eq
. k
e
rn
e
l
Figure 1: Re-ranking based on different string similarity methods.
ter than that based on contextual similarity (C-dict)
in terms of precision and false alarm rate, indicating
the importance of re-ranking. Even though C-dict
delivers higher recall ? indicating that many lexi-
cal variants are correctly normalised ? this is offset
by its high false alarm rate, which is particularly un-
desirable in normalisation. Because S-dict has better
performance than DM-dict in terms of both F-score
and WER, and a much lower false alarm rate than
C-dict, subsequent results are presented using S-dict
only.
Both HB-dict and GHM-dict achieve better than
90% precision with moderate recall. Compared to
these methods, S-dict is not competitive in terms of
either precision or recall. This result seems rather
discouraging. However, considering that S-dict is an
automatically-constructed dictionary targeting lexi-
cal variants of varying frequency, it is not surprising
that the precision is worse than that of HB-dict ?
which is manually-constructed ? and GHM-dict ?
which includes entries only for more-frequent OOVs
for which distributional similarity is more accurate.
Additionally, the recall of S-dict is hampered by the
restriction on lexical variant token length of 4 char-
acters.
6.2.2 Combined Dictionaries
Next we look to combining HB-dict, GHM-dict
and S-dict. In combining the dictionaries, a given
OOV word can be listed with different standard
forms in different dictionaries. In such cases we use
the following preferences for dictionaries ? moti-
vated by our confidence in the normalisation pairs
of the dictionaries ? to resolve conflicts: HB-dict
> GHM-dict > S-dict.
When we combine dictionaries in the second sec-
tion of Table 3, we find that they contain com-
plementary information: in each case the recall
and F-score are higher for the combined dictio-
nary than any of the individual dictionaries. The
combination of HB-dict+GHM-dict produces only
a small improvement in terms of F-score over HB-
dict (the better-performing dictionary) suggesting
that, as claimed, HB-dict and GHM-dict share many
frequent normalisation pairs. HB-dict+S-dict and
GHM-dict+S-dict, on the other hand, improve sub-
428
Method Precision Recall F-Score False Alarm Word Error Rate
C-dict 0.474 0.218 0.299 0.298 0.103
DM-dict 0.727 0.106 0.185 0.145 0.102
S-dict 0.700 0.179 0.285 0.162 0.097
HB-dict 0.915 0.435 0.590 0.048 0.066
GHM-dict 0.982 0.319 0.482 0.000 0.076
HB-dict+S-dict 0.840 0.601 0.701 0.090 0.052
GHM-dict+S-dict 0.863 0.498 0.632 0.072 0.061
HB-dict+GHM-dict 0.920 0.465 0.618 0.045 0.063
HB-dict+GHM-dict+S-dict 0.847 0.630 0.723 0.086 0.049
GHM-dict+GHM-norm 0.338 0.578 0.427 0.458 0.135
HB-dict+GHM-dict+S-dict+GHM-norm 0.406 0.715 0.518 0.468 0.124
HB-dict+HB-norm 0.515 0.771 0.618 0.332 0.081
HB-dict+GHM-dict+S-dict+HB-norm 0.527 0.789 0.632 0.332 0.079
Table 3: Normalisation results using our derived dictionaries (contextual similarity (C-dict); double metaphone ren-
dering (DM-dict); string subsequence kernel scores (S-dict)), the dictionary of Gouws et al2011) (GHM-dict), the
Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition,
we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined
unsupervised approach of Han and Baldwin (2011) (HB-norm).
stantially over HB-dict and GHM-dict, respectively,
indicating that S-dict contains markedly different
entries to both HB-dict and GHM-dict. The best F-
score and WER are obtained using the combination
of all three dictionaries, HB-dict+GHM-dict+S-dict.
Furthermore, the difference between the results us-
ing HB-dict+GHM-dict+S-dict and HB-dict+GHM-
dict is statistically significant (p < 0.01), based on
the computationally-intensive Monte Carlo method
of Yeh (2000), demonstrating the contribution of S-
dict.
6.2.3 Hybrid Approaches
The methods of Gouws et al2011) (i.e.
GHM-dict+GHM-norm) and Han and Baldwin
(2011) (i.e. HB-dict+HB-norm) have lower preci-
sion and higher false alarm rates than the dictionary-
based approaches; this is largely caused by lex-
ical variant detection errors.8 Using all dic-
tionaries in combination with these methods ?
HB-dict+GHM-dict+S-dict+GHM-norm and HB-
dict+GHM-dict+S-dict+HB-norm ? gives some
improvements, but the false alarm rates remain high.
Despite the limitations of a pure dictionary-based
approach to normalisation ? discussed in Section
3.1 ? the current best practical approach to normal-
8Here we report results that do not assume perfect detection
of lexical variants, unlike the original published results in each
case.
Error type OOV
Standard form
Dict. Gold
(a) plurals playe players player
(b) negation unlike like dislike
(c) possessives anyones anyone anyone?s
(d) correct OOVs iphone phone iphone
(e) test data errors durin during durin
(f) ambiguity siging signing singing
Table 4: Error types in the combined dictionary (HB-
dict+GHM-dict+S-dict)
isation is to use a lexicon, combining hand-built and
automatically-learned normalisation dictionaries.
6.3 Discussion and Error Analysis
We first manually analyse the errors in the combined
dictionary (HB-dict+GHM-dict+S-dict) and give ex-
amples of each error type in Table 4. The most fre-
quent word errors are caused by slight morphologi-
cal variations, including plural forms (a), negations
(b), possessive cases (c), and OOVs that are correct
and do not require normalisation (d). In addition, we
also notice some missing annotations where lexical
variants are skipped by human annotations but cap-
tured by our method (e). Ambiguity (f) definitely
exists in longer OOVs, however, these cases do not
appear to have a strong negative impact on the nor-
malisation performance. An example of a remain-
429
Length cut-off (N ) #Variants Precision Recall (? N ) Recall (all) False Alarm
?4 556 0.700 0.381 0.179 0.162
?5 382 0.814 0.471 0.152 0.122
?6 254 0.804 0.484 0.104 0.131
?7 138 0.793 0.471 0.055 0.122
Table 5: S-dict normalisation results broken down according to OOV token length. Recall is presented both over the
subset of instances of length ? N in the data (?Recall (? N )?), and over the entirety of the dataset (?Recall (all)?);
?#Variants? is the number of token instances of the indicated length in the test dataset.
ing miscellaneous error is bday ?birthday?, which is
mis-normalised as day.
To further study the influence of OOV word
length relative to the normalisation performance, we
conduct a fine-grained analysis of the performance
of the derived dictionary (S-dict) in Table 5, bro-
ken down across different OOV word lengths. The
results generally support our hypothesis that our
method works better for longer OOV words. The
derived dictionary is much more reliable for longer
tokens (length 5, 6, and 7 characters) in terms of pre-
cision and false alarm. Although the recall is rela-
tively modest, in the future we intend to improve re-
call by mining more normalisation pairs from larger
collections of microblog data.
7 Conclusions and Future Work
In this paper, we describe a method for automat-
ically constructing a normalisation dictionary that
supports normalisation of microblog text through di-
rect substitution of lexical variants with their stan-
dard forms. After investigating the impact of dif-
ferent distributional and string similarity methods
on the quality of the dictionary, we present ex-
perimental results on a standard dataset showing
that our proposed methods acquire high quality
(lexical variant, standard form) pairs, with reason-
able coverage, and achieve state-of-the-art end-to-
end lexical normalisation performance on a real-
world token-level task. Furthermore, this dictionary-
lookup method combines the detection and normali-
sation of lexical variants into a simple, lightweight
solution which is suitable for processing of high-
volume microblog feeds.
In the future, we intend to improve our dictionary
by leveraging the constantly-growing volume of mi-
croblog data, and considering alternative ways to
combine distributional and string similarity. In addi-
tion to direct evaluation, we also want to explore the
benefits of applying normalisation for downstream
social media text processing applications, e.g. event
detection.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their insightful comments, and Stephan
Gouws for kindly sharing his data and discussing his
work.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT centre of Excel-
lence programme.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING/ACL 2006, pages
33?40, Sydney, Australia.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT 2011), pages 389?398, Port-
land, Oregon, USA.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 286?293,
Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157?174.
430
Danish Contractor, Tanveer A. Faruquie, and L. Venkata
Subramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
pages 189?196, Beijing, China.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ?09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78, Boulder, USA.
Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,
Joseph L. Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS Tagging and Parsing the Twitterverse.
In Analyzing Microtext: Papers from the 2011 AAAI
Workshop, volume WS-11-05 of AAAI Workshops,
pages 20?25, San Francisco, CA, USA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 42?47,
Portland, Oregon, USA.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in Twitter:
a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-HLT
2011), pages 581?586, Portland, Oregon, USA.
Stephan Gouws, Dirk Hovy, and Donald Metzler. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, pages 82?90, Edinburgh,
Scotland, UK.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 368?378,
Portland, Oregon, USA.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
151?160, Portland, Oregon, USA.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals of Mathematical Statistics, 22:49?
86.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 282?289, San Fran-
cisco, CA, USA.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of COL-
ING/ACL 2006, pages 1025?1032, Sydney, Australia.
Jianhua Lin. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 36th An-
nual Meeting of the ACL and 17th International Con-
ference on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Quebec, Canada.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011a. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), pages
71?76, Portland, Oregon, USA.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011b. Recognizing named entities in tweets.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011), pages 359?367,
Portland, Oregon, USA.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012), Jeju, Republic of Korea.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. J. Mach. Learn. Res., 2:419?
444.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
431
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for Twitter. In Proceedings of the 4th In-
ternational Conference on Weblogs and Social Media
(ICWSM 2010), pages 384?385, Washington, USA.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38?43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-HLT 2010), pages 172?180, Los Angeles,
USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2011), pages 1524?1534, Edinburgh,
Scotland, UK.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International Conference on the World Wide
Web (WWW 2010), pages 851?860, Raleigh, North
Carolina, USA.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting of the
ACL and 3rd Annual Meeting of the NAACL (ACL-02),
pages 144?151, Philadelphia, USA.
Official Blog Twitter. 2011. 200 million tweets per day.
Retrived at August 17th, 2011.
Jianshu Weng and Bu-Sung Lee. 2011. Event detection
in Twitter. In Proceedings of the 5th International
Conference on Weblogs and Social Media (ICWSM
2011), Barcelona, Spain.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison. 2011.
Normalizing microtext. In Proceedings of the AAAI-
11 Workshop on Analyzing Microtext, pages 74?79,
San Francisco, USA.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (COLING 2010), pages 947?953,
Saarbru?cken, Germany.
432
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172?176,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Testing for Significance of Increased Correlation with Human Judgment
Yvette Graham Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
graham.yvette@gmail.com, tb@ldwin.net
Abstract
Automatic metrics are widely used in ma-
chine translation as a substitute for hu-
man assessment. With the introduction
of any new metric comes the question of
just how well that metric mimics human
assessment of translation quality. This is
often measured by correlation with hu-
man judgment. Significance tests are gen-
erally not used to establish whether im-
provements over existing methods such as
BLEU are statistically significant or have
occurred simply by chance, however. In
this paper, we introduce a significance test
for comparing correlations of two metrics,
along with an open-source implementation
of the test. When applied to a range of
metrics across seven language pairs, tests
show that for a high proportion of metrics,
there is insufficient evidence to conclude
significant improvement over BLEU.
1 Introduction
Within machine translation (MT), efforts are on-
going to improve evaluation metrics and find bet-
ter ways to automatically assess translation qual-
ity. The process of validating a new metric in-
volves demonstration that it correlates better with
human judgment than a standard metric such as
BLEU (Papineni et al., 2001). However, although
it is standard practice in MT evaluation to mea-
sure increases in automatic metric scores with sig-
nificance tests (Germann, 2003; Och, 2003; Ku-
mar and Byrne, 2004; Koehn, 2004; Riezler and
Maxwell, 2005; Graham et al., 2014), this has
not been the case in papers proposing new met-
rics. Thus it is possible that some reported im-
provements in correlation with human judgment
are attributable to chance rather than a systematic
improvement.
In this paper, we motivate and introduce a novel
significance test to assess the statistical signifi-
cance of differences in correlation with human
judgment for pairs of automatic metrics. We ap-
ply tests to the WMT-12 shared metrics task to
compare each of the participating methods, and
find that for a high proportion of metrics, there is
not enough evidence to conclude that they signifi-
cantly outperform BLEU.
2 Correlation with Human Judgment
A common means of assessing automatic MT
evaluation metrics is Spearman?s rank correlation
with human judgments (Melamed et al., 2003),
which measures the relative degree of monotonic-
ity between the metric and human scores in the
range [?1, 1]. The standard justification for cal-
culating correlations over ranks rather than raw
scores is to: (a) reduce anomalies due to absolute
score differences; and (b) focus evaluation on what
is generally the primary area of interest, namely
the ranking of systems/translations.
An alternative means of evaluation is Pearson?s
correlation, which measures the linear correlation
between a metric and human scores (Leusch et al.,
2003). Debate on the relative merits of Spear-
man?s and Pearson?s correlation for the evaluation
of automatic metrics is ongoing, but there is an in-
creasing trend towards Pearson?s correlation, e.g.
in the recent WMT-14 shared metrics task.
Figure 1 presents the system-level results for
two evaluation metrics ? AMBER (Chen et al.,
2012) and TERRORCAT (Fishel et al., 2012)
? over the WMT-12 Spanish-to-English metrics
task. These two metrics achieved the joint-highest
rank correlation (? = 0.965) for the task, but dif-
fer greatly in terms of Pearson?s correlation (r =
0.881 vs. 0.971, resp.). The largest contributor to
this artifact is the system with the lowest human
score, represented by the leftmost point in both
plots.
172
ll
ll
l ll
l
ll
l
l
?3 ?2 ?1 0 1 2 3?
3
?
2
?
1
0
1
2
3
Human
AMB
ER
Spearman: 0.965Pearson: 0.881
(a) AMBER
ll
ll
l l
ll
ll
l
l
?3 ?2 ?1 0 1 2 3?
3
?
2
?
1
0
1
2
3
Human
Terro
rCat
Spearman: 0.965Pearson: 0.971
(b) TERRORCAT
Figure 1: Scatter plot of human and automatic scores of WMT-12 Spanish-to-English systems for two
MT evaluation metrics (AMBER and TERRORCAT)
Consistent with the WMT-14 metrics shared
task, we argue that Pearson?s correlation is more
sensitive than Spearman?s correlation. There is
still the question, however, of whether an observed
difference in Pearson?s r is statistically significant,
which we address in the next section.
3 Significance Testing
Evaluation of a new automatic metric, M
new
,
commonly takes the form of quantifying the cor-
relation between the new metric and human judg-
ment, r(M
new
, H), and contrasting it with the cor-
relation for some baseline metric, r(M
base
, H). It
is very rare in the MT literature for significance
testing to be performed in such cases, however.
We introduce a statistical test which can be used
for this purpose, and apply the test to the evalua-
tion of metrics participating in the WMT-12 metric
evaluation task.
At first gloss, it might seem reasonable to per-
form significance testing in the following man-
ner when an increase in correlation with human
assessment is observed: apply a significance test
separately to the correlation of each metric with
human judgment, with the hope that the newly
proposed metric will achieve a significant correla-
tion where the baseline metric does not. However,
besides the fact that the correlation between al-
most any document-level metric and human judg-
ment will generally be significantly greater than
zero, the logic here is flawed: the fact that
one correlation is significantly higher than zero
(r(M
new
, H)) and that of another is not, does not
necessarily mean that the difference between the
two correlations is significant. Instead, a specific
test should be applied to the difference in corre-
lations on the data. For this same reason, con-
fidence intervals for individual correlations with
human judgment are also not particularly mean-
ingful.
In psychological studies, it is often the case that
samples that data are drawn from are independent,
and differences in correlations are computed on in-
dependent data sets. In such cases, the Fisher r
to z transformation is applied to test for signifi-
cant differences in correlations. In the case of au-
tomatic metric evaluation, however, the data sets
used are almost never independent. This means
that if r(M
base
, H) and r(M
new
, H) are both> 0,
the correlation between the metric scores them-
selves, r(M
base
,M
new
), must also be > 0. The
strength of this correlation, directly between pairs
of metrics, should be taken into account using a
significance test of the difference in correlation be-
tween r(M
base
, H) and r(M
new
, H).
3.1 Correlated Correlations
Correlations computed for two separate automatic
metrics on the same data set are not independent,
and for this reason in order to test the difference in
correlation between them, the degree to which the
pair of metrics correlate with each other should be
taken into account. The Williams test (Williams,
173
Terro
rCat
MET
EOR Saga
n
Sem
pos PosF
XEn
ErrC
ats
WBE
rrCa
ts
Amb
er
BErr
Cats
Simp
BLE
U
BLE
U.4c
c TER
TERBLEU?4cc
SimpBLEUBErrCats
AmberWBErrCats
XEnErrCatsPosF
SemposSagan
METEORTerrorCat
(a) Pearson?s correlation
Terro
rCat
MET
EOR Saga
n
Sem
pos PosF
XEn
ErrC
ats
WBE
rrCa
ts
Amb
er
BErr
Cats
Simp
BLE
U
BLE
U.4c
c TER
TERBLEU?4cc
SimpBLEUBErrCats
AmberWBErrCats
XEnErrCatsPosF
SemposSagan
METEORTerrorCat
(b) Statistical significance
Figure 2: (a) Pearson?s correlation between pairs of automatic metrics; and (b) p-value of Williams
significance tests, where a colored cell in row i (named on y-axis), col j indicates that metric i (named
on x-axis) correlates significantly higher with human judgment than metric j; all results are based on the
WMT-12 Spanish-to-English data set.
1959)
1
evaluates significance in a difference in de-
pendent correlations (Steiger, 1980). It is formu-
lated as follows, as a test of whether the population
correlation betweenX
1
andX
3
equals the popula-
tion correlation between X
2
and X
3
:
t(n? 3) =
(r
13
? r
23
)
?
(n? 1)(1 + r
12
)
?
2K
(n?1)
(n?3)
+
(r
23
+r
13
)
2
4
(1? r
12
)
3
,
where r
ij
is the Pearson correlation between X
i
and X
j
, n is the size of the population, and:
K = 1? r
12
2
? r
13
2
? r
23
2
+ 2r
12
r
13
r
23
The Williams test is more powerful than the
equivalent for independent samples (Fisher r to
z), as it takes the correlations between X
1
and
X
2
(metric scores) into account. All else being
equal, the higher the correlation between the met-
ric scores, the greater the statistical power of the
test.
4 Evaluation and Discussion
Figure 2a is a heatmap of the degree to which au-
tomatic metrics correlate with one another when
computed on the same data set, in the form of the
Pearson?s correlation between each pair of met-
rics that participated in the WMT-12 metrics task
for Spanish-to-English evaluation. Metrics are or-
dered in all tables from highest to lowest correla-
tion with human assessment. In addition, for the
1
Also sometimes referred to as the Hotelling?Williams
test.
purposes of significance testing, we take the abso-
lute value of all correlations, in order to compare
error-based metrics with non-error based ones.
In general, the correlation is high amongst all
pairs of metrics, with a high proportion of paired
metrics achieving a correlation in excess of r =
0.9. Two exceptions to this are TERRORCAT
(Fishel et al., 2012) and SAGAN (Castillo and Es-
trella, 2012), as seen in the regions of yellow and
white.
Figure 2b shows the results of Williams sig-
nificance tests for all pairs of metrics. Since we
are interested in not only identifying significant
differences in correlations, but ultimately ranking
competing metrics, we use a one-sided test. Here
again, the metrics are ordered from highest to low-
est (absolute) correlation with human judgment.
For the Spanish-to-English systems, approxi-
mately 60% of WMT-12 metric pairs show a sig-
nificant difference in correlation with human judg-
ment at p < 0.05 (for one of the two metric di-
rections).
2
As expected, the higher the correlation
with human judgment, the more metrics a given
method is superior to at a level of statistical signifi-
cance. Although TERRORCAT (Fishel et al., 2012)
achieves the highest absolute correlation with hu-
man judgment, it is not significantly better (p ?
0.05) than the four next-best metrics (METEOR
(Denkowski and Lavie, 2011), SAGAN (Castillo
and Estrella, 2012), SEMPOS (Mach?a?cek and Bo-
2
Correlation matrices (red) are maximally filled, in con-
trast to one-sided significance test matrices (green), where, at
a maximum, fewer than half of the cells can be filled.
174
BLEU
.4cc
Simp
BLEU Semp
os Ambe
r TER Saga
n
MET
EOR
Terro
rCat
BErr
Cats
XEnE
rrCat
s PosF
WBE
rrCat
s WBErrCats
PosFXEnErrCats
BErrCatsTerrorCat
METEORSagan
TERAmber
SemposSimpBLEU
BLEU?4cc
(a) Czech-to-English
Terro
rCat Semp
os
MET
EOR
Simp
BLEU BLEU
.4cc Amb
er PosF
XEnE
rrCat
s
BErr
Cats
WBE
rrCat
s TER
TERWBErrCats
BErrCatsXEnErrCats
PosFAmber
BLEU?4ccSimpBLEU
METEORSempos
TerrorCat
(b) French-to-English
Semp
os
MET
EOR
Terro
rCat Ambe
r
BErr
Cats PosF
WBE
rrCat
s
XEnE
rrCat
s
Simp
BLEU TER BLEU
.4cc
BLEU?4ccTER
SimpBLEUXEnErrCats
WBErrCatsPosF
BErrCatsAmber
TerrorCatMETEOR
Sempos
(c) German-to-English
Terro
rCat
EnXE
rrCat
s
Amb
er
BErr
Cats
WBE
rrCat
s
BLEU
.4cc PosF
Simp
BLEU TER MET
EOR
METEORTER
SimpBLEUPosF
BLEU?4ccWBErrCats
BErrCatsAmber
EnXErrCatsTerrorCat
(d) English-to-Spanish
EnXE
rrCat
s
BErr
Cats
Simp
BLEU MET
EOR
WBE
rrCat
s
Amb
er
BLEU
.4cc
Terro
rCat PosF TER
TERPosF
TerrorCatBLEU?4cc
AmberWBErrCats
METEORSimpBLEU
BErrCatsEnXErrCats
(e) English-to-French
Terro
rCat
Simp
BLEU PosF BErrC
ats
EnXE
rrCat
s
Amb
er TER
WBE
rrCat
s
BLEU
.4cc
MET
EOR
METEORBLEU?4cc
WBErrCatsTER
AmberEnXErrCats
BErrCatsPosF
SimpBLEUTerrorCat
(f) English-to-German
Figure 3: Significance results for pairs of automatic metrics for each WMT-12 language pair.
jar, 2011) and POSF (Popovic, 2012)). There is
not enough evidence to conclude, therefore, that
this metric is any better at evaluating Spanish-to-
English MT system quality than the next four met-
rics.
Figure 3 shows the results of significance tests
for the six other language pairs used in the WMT-
12 metrics shared task.
3
For no language pair
is there an outright winner amongst the met-
rics, with proportions of significant differences be-
tween metrics for a given language pair ranging
from 3% for Czech-to-English to 82% for English-
to-French (p < 0.05). The number of metrics that
significantly outperform BLEU for a given lan-
guage pair is only 34% (p < 0.05), and no method
significantly outperforms BLEU over all language
pairs ? indeed, even the best methods achieve sta-
tistical significance over BLEU for only a small
minority of language pairs. This underlines the
dangers of assessing metrics based solely on cor-
relation numbers, and emphasizes the importance
of statistical testing.
It is important to note that the number of com-
3
We omit English-to-Czech due to some metric scores be-
ing omitted from the WMT-12 data set.
peting metrics a metric significantly outperforms
should not be used as the criterion for ranking
competing metrics. This is due to the fact that
the power of the Williams test to identify signifi-
cant differences between correlations changes de-
pending on the degree to which the pair of met-
rics correlate with each other. Therefore, a metric
that happens to correlate strongly with many other
metrics would be at an unfair advantage, were
numbers of significant wins to be used to rank met-
rics. For this reason, it is best to interpret pairwise
metric tests in isolation.
As part of this research, we have made avail-
able an open-source implementation of statis-
tical tests tailored to the assessment of MT
metrics available at https://github.com/
ygraham/significance-williams.
5 Conclusions
We have provided an analysis of current method-
ologies for evaluating automatic metrics in ma-
chine translation, and identified an issue with re-
spect to the lack of significance testing. We in-
troduced the Williams test as a means of cal-
culating the statistical significance of differences
175
in correlations for dependent samples. Analysis
of statistical significance in the WMT-12 metrics
shared task showed there is currently insufficient
evidence for a high proportion of metrics to con-
clude that they outperform BLEU.
Acknowledgments
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported by funding from the Australian Research
Council.
References
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 52?58, Montr?eal, Canada.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving AMBER, an MT evaluation metric. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 59?63, Montr?eal,
Canada.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, UK.
Mark Fishel, Rico Sennrich, Maja Popovi?c, and Ond?rej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 64?70, Montr?eal, Canada.
Ulrich Germann. 2003. Greedy decoding for statis-
tical machine translation in almost linear time. In
Proceedings of the 2003 Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics on Human Language Technology-Volume 1,
pages 1?8, Edmonton, Canada.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, pages
266?274, Baltimore, USA.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Empirical Methods in Natural Language Processing
2004 (EMNLP 2004), pages 388?395, Barcelona,
Spain.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the 4th International Con-
ference on Human Language Technology Research
and 5th Annual Meeting of the NAACL (HLT-NAACL
2004), pages 169?176, Boston, USA.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings 9th Machine Translation Summit
(MT Summit IX), pages 240?247, New Orleans,
USA.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approx-
imating a deep-syntactic metric for MT evaluation
and tuning. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 92?98, Edin-
burgh, UK.
Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and recall of machine translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003) ? Short Papers, pages 61?63, Ed-
monton, Canada.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), IBM Research, Thomas J.
Watson Research Center.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for mt. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, USA.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245.
Evan J. Williams. 1959. Regression Analysis, vol-
ume 14. Wiley, New York, USA.
176
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792?1797,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Detecting Non-compositional MWE Components using Wiktionary
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We propose a simple unsupervised ap-
proach to detecting non-compositional
components in multiword expressions
based on Wiktionary. The approach makes
use of the definitions, synonyms and trans-
lations in Wiktionary, and is applicable to
any type of MWE in any language, assum-
ing the MWE is contained in Wiktionary.
Our experiments show that the proposed
approach achieves higher F-score than
state-of-the-art methods.
1 Introduction
A multiword expression (MWE) is a combina-
tion of words with lexical, syntactic or seman-
tic idiosyncrasy (Sag et al., 2002; Baldwin and
Kim, 2009). An MWE is considered (semanti-
cally) ?non-compositional? when its meaning is
not predictable from the meaning of its compo-
nents. Conversely, compositional MWEs are those
whose meaning is predictable from the meaning
of the components. Based on this definition, a
component is compositional within an MWE, if its
meaning is reflected in the meaning of the MWE,
and it is non-compositional otherwise.
Understanding which components are non-
compositional within an MWE is important in
NLP applications in which semantic information
is required. For example, when searching for
spelling bee, we may also be interested in docu-
ments about spelling, but not those which contain
only bee. For research project, on the other hand,
we are likely to be interested in documents which
contain either research or project in isolation, and
for swan song, we are only going to be interested
in documents which contain the phrase swan song,
and not just swan or song.
In this paper, we propose an unsupervised ap-
proach based on Wikitionary for predicting which
components of a given MWE have a composi-
tional usage. Experiments over two widely-used
datasets show that our approach outperforms state-
of-the-art methods.
2 Related Work
Previous studies which have considered MWE
compositionality have focused on either the iden-
tification of non-compositional MWE token in-
stances (Kim and Baldwin, 2007; Fazly et al.,
2009; Forthergill and Baldwin, 2011; Muzny and
Zettlemoyer, 2013), or the prediction of the com-
positionality of MWE types (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). The
identification of non-compositional MWE tokens
is an important task when a word combination
such as kick the bucket or saw logs is ambiguous
between a compositional (generally non-MWE)
and non-compositional MWE usage. Approaches
have ranged from the unsupervised learning of
type-level preferences (Fazly et al., 2009) to su-
pervised methods specific to particular MWE con-
structions (Kim and Baldwin, 2007) or applica-
ble across multiple constructions using features
similar to those used in all-words word sense
disambiguation (Forthergill and Baldwin, 2011;
Muzny and Zettlemoyer, 2013). The prediction
of the compositionality of MWE types has tradi-
tionally been couched as a binary classification
task (compositional or non-compositional: Bald-
win et al. (2003), Bannard (2006)), but more re-
cent work has moved towards a regression setup,
where the degree of the compositionality is pre-
dicted on a continuous scale (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). In ei-
ther case, the modelling has been done either over
the whole MWE (Reddy et al., 2011; Salehi and
Cook, 2013), or relative to each component within
the MWE (Baldwin et al., 2003; Bannard, 2006).
In this paper, we focus on the binary classification
of MWE types relative to each component of the
1792
MWE.
The work that is perhaps most closely related to
this paper is that of Salehi and Cook (2013) and
Salehi et al. (2014), who use translation data to
predict the compositionality of a given MWE rel-
ative to each of its components, and then combine
those scores to derive an overall compositionality
score. In both cases, translations of the MWE and
its components are sourced from PanLex (Bald-
win et al., 2010; Kamholz et al., 2014), and if
there is greater similarity between the translated
components and MWE in a range of languages,
the MWE is predicted to be more compositional.
The basis of the similarity calculation is unsuper-
vised, using either string similarity (Salehi and
Cook, 2013) or distributional similarity (Salehi et
al., 2014). However, the overall method is su-
pervised, as training data is used to select the
languages to aggregate scores across for a given
MWE construction. To benchmark our method,
we use two of the same datasets as these two pa-
pers, and repurpose the best-performing methods
of Salehi and Cook (2013) and Salehi et al. (2014)
for classification of the compositionality of each
MWE component.
3 Methodology
Our basic method relies on analysis of lexical
overlap between the component words and the def-
initions of the MWE in Wiktionary, in the man-
ner of Lesk (1986). That is, if a given component
can be found in the definition, then it is inferred
that the MWE carries the meaning of that compo-
nent. For example, the Wiktionary definition of
swimming pool is ?An artificially constructed pool
of water used for swimming?, suggesting that the
MWE is compositional relative to both swimming
and pool. If the MWE is not found in Wiktionary,
we use Wikipedia as a backoff, and use the first
paragraph of the (top-ranked) Wikipedia article as
a proxy for the definition.
As detailed below, we further extend the basic
method to incorporate three types of information
found in Wiktionary: (1) definitions of each word
in the definitions, (2) synonyms of the words in the
definitions, and (3) translations of the MWEs and
components.
3.1 Definition-based Similarity
The basic method uses Boolean lexical overlap be-
tween the target component of the MWE and a
definition. A given MWE will often have multiple
definitions, however, begging the question of how
to combine across them, for which we propose the
following three methods.
First Definition (FIRSTDEF): Use only the
first-listed Wiktionary definition for the MWE,
based on the assumption that this is the predom-
inant sense.
All Definitions (ALLDEFS): In the case that
there are multiple definitions for the MWE, cal-
culate the lexical overlap for each independently
and take a majority vote; in the case of a tie, label
the component as non-compositional.
Idiom Tag (ITAG): In Wiktionary, there is fa-
cility for users to tag definitions as idiomatic.
1
If,
for a given MWE, there are definitions tagged as
idiomatic, use only those definitions; if there are
no such definitions, use the full set of definitions.
3.2 Synonym-based Definition Expansion
In some cases, a component is not explicitly men-
tioned in a definition, but a synonym does occur,
indicating that the definition is compositional in
that component. In order to capture synonym-
based matches, we optionally look for synonyms
of the component word in the definition,
2
and ex-
pand our notion of lexical overlap to include these
synonyms.
For example, for the MWE china clay, the defi-
nition is kaolin, which includes neither of the com-
ponents. However, we find the component word
clay in the definition for kaolin, as shown below.
A fine clay, rich in kaolinite, used in ce-
ramics, paper-making, etc.
This method is compatible with the three
definition-based similarity methods described
above, and indicated by the +SYN suffix (e.g.
FIRSTDEF+SYN is FIRSTDEF with synonym-
based expansion).
3.3 Translations
A third information source in Wiktionary that can
be used to predict compositionality is sense-level
translation data. Due to the user-generated na-
ture of Wiktionary, the set of languages for which
1
Although the recall of these tags is low (Muzny and
Zettlemoyer, 2013).
2
After removing function words, based on a stopword list.
1793
ENC EVPC
WordNet 91.1% 87.5%
Wiktionary 96.7% 96.2%
Wiktionary+Wikipedia 100.0% 96.2%
Table 1: Lexical coverage of WordNet, Wik-
tionary and Wiktionary+Wikipedia over our two
datasets.
translations are provided varies greatly across lexi-
cal entries. Our approach is to take whatever trans-
lations happen to exist in Wiktionary for a given
MWE, and where there are translations in that lan-
guage for the component of interest, use the LCS-
based method of Salehi and Cook (2013) to mea-
sure the string similarity between the translation
of the MWE and the translation of the compo-
nents. Unlike Salehi and Cook (2013), however,
we do not use development data to select the opti-
mal set of languages in a supervised manner, and
instead simply take the average of the string simi-
larity scores across the available languages. In the
case of more than one translation in a given lan-
guage, we use the maximum string similarity for
each pairing of MWE and component translation.
Unlike the definition and synonym-based ap-
proach, the translation-based approach will pro-
duce real rather than binary values. To combine
the two approaches, we discretise the scores given
by the translation approach. In the case of dis-
agreement between the two approaches, we label
the given MWE as non-compositional. This re-
sults in higher recall and lower precision for the
task of detecting compositionality.
3.4 An Analysis of Wiktionary Coverage
A dictionary-based method is only as good as the
dictionary it is applied to. In the case of MWE
compositionality analysis, our primary concern is
lexical coverage in Wiktionary, i.e., what propor-
tion of a representative set of MWEs is contained
in Wiktionary. We measure lexical coverage rela-
tive to the two datasets used in this research (de-
scribed in detail in Section 4), namely 90 En-
glish noun compounds (ENCs) and 160 English
verb particle constructions (EVPCs). In each case,
we calculated the proportion of the dataset that
is found in Wiktionary, Wiktionary+Wikipedia
(where we back off to a Wikipedia document in the
case that a MWE is not found in Wiktionary) and
WordNet (Fellbaum, 1998). The results are found
in Table 1, and indicate perfect coverage in Wik-
tionary+Wikipedia for the ENCs, and very high
coverage for the EVPCs. In both cases, the cov-
erage of WordNet is substantially lower, although
still respectable, at around 90%.
4 Datasets
As mentioned above, we evaluate our method over
the same two datasets as Salehi and Cook (2013)
(which were later used, in addition to a third
dataset of German noun compounds, in Salehi
et al. (2014)): (1) 90 binary English noun com-
pounds (ENCs, e.g. spelling bee or swimming
pool); and (2) 160 English verb particle construc-
tions (EVPCs, e.g. stand up and give away). Our
results are not directly comparable with those of
Salehi and Cook (2013) and Salehi et al. (2014),
however, who evaluated in terms of a regression
task, modelling the overall compositionality of the
MWE. In our case, the task setup is a binary clas-
sification task relative to each of the two compo-
nents of the MWE.
The ENC dataset was originally constructed by
Reddy et al. (2011), and annotated on a contin-
uous [0, 5] scale for both overall compositional-
ity and the component-wise compositionality of
each of the modifier and head noun. The sampling
was random in an attempt to make the dataset bal-
anced, with 48% of compositional English noun
compounds, of which 51% are compositional in
the first component and 60% are compositional in
the second component. We generate discrete la-
bels by discretising the component-wise composi-
tionality scores based on the partitions [0, 2.5] and
(2.5, 5]. On average, each NC in this dataset has
1.4 senses (definitions) in Wiktionary.
The EVPC dataset was constructed by Ban-
nard (2006), and manually annotated for com-
positionality on a binary scale for each of the
head verb and particle. For the 160 EVPCs,
76% are verb-compositional and 48% are particle-
compositional. On average, each EVPC in this
dataset has 3.0 senses (definitions) in Wiktionary.
5 Experiments
The baseline for each dataset takes the form of
looking for a user-annotated idiom tag in the Wik-
tionary lexical entry for the MWE: if there is an id-
iomatic tag, both components are considered to be
non-compositional; otherwise, both components
are considered to be compositional. We expect
this method to suffer from low precision for two
1794
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 66.7 68.2 67.4 66.7 83.3 74.1
LCS 60.0 77.7 67.7 81.6 68.1 64.6
DS 62.1 88.6 73.0 80.5 86.4 71.2
DS+DSL2 62.5 92.3 74.5 78.4 89.4 70.6
LCS+DS+DSL2 66.3 87.5 75.4 82.1 80.6 70.1
FIRSTDEF 59.4 93.2 72.6 54.2 88.9 67.4
ALLDEFS 59.5 100.0 74.6 52.9 100.0 69.2
ITAG 60.3 100.0 75.2 54.5 100.0 70.6
FIRSTDEF+SYN 64.9 84.1 73.3 63.8 83.3 72.3
ALLDEFS+SYN 64.5 90.9 75.5 60.4 88.9 71.9
ITAG+SYN 64.5 90.9 75.5 61.8 94.4 74.7
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
82.9 85.3 84.1 81.9 80.0 69.8
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
81.2 88.1 84.5 87.3 80.6 73.3
ITAG+SYN
COMB(LCS+DS+DSL2)
81.0 88.1 84.1 88.0 81.1 73.9
Table 2: Compositionality prediction results over the ENC dataset, relative to the first component (the
modifier noun) and the second component (the head noun)
reasons: first, the guidelines given to the annota-
tors of our datasets might be different from what
Wiktionary contributors assume to be an idiom.
Second, the baseline method assumes that for any
non-compositional MWE, all components must be
equally non-compositional, despite the wealth of
MWEs where one or more components are com-
positional (e.g. from the Wiktionary guidelines
for idiom inclusion,
3
computer chess, basketball
player, telephone box).
We also compare our method with: (1) ?LCS?,
the string similarity-based method of Salehi and
Cook (2013), in which 54 languages are used;
(2) ?DS?, the monolingual distributional similarity
method of Salehi et al. (2014); (3) ?DS+DSL2?,
the multilingual distributional similarity method
of Salehi et al. (2014), including supervised lan-
guage selection for a given dataset, based on cross-
validation; and (4) ?LCS+DS+DSL2?, whereby
the first three methods are combined using a su-
pervised support vector regression model. In
each case, the continuous output of the model
is equal-width discretised to generate a binary
classification. We additionally present results for
the combination of each of the six methods pro-
posed in this paper with LCS, DS and DSL2, us-
ing a linear-kernel support vector machine (rep-
resented with the suffix ?
COMB(LCS+DS+DSL2)
? for
a given method). The results are based on cross-
3
http://en.wiktionary.org/wiki/
Wiktionary:Idioms_that_survived_RFD
validation, and for direct comparability, the parti-
tions are exactly the same as Salehi et al. (2014).
Tables 2 and 3 provide the results when our pro-
posed method for detecting non-compositionality
is applied to the ENC and EVPC datasets, respec-
tively. The inclusion of translation data was found
to improve all of precision, recall and F-score
across the board for all of the proposed methods.
For reasons of space, results without translation
data are therefore omitted from the paper.
Overall, the simple unsupervised methods pro-
posed in this paper are comparable with the unsu-
pervised and supervised state-of-the-art methods
of Salehi and Cook (2013) and Salehi et al. (2014),
with ITAG achieving the highest F-score for the
ENC dataset and for the verb components of the
EVPC dataset. The inclusion of synonyms boosts
results in most cases.
When we combine each of our proposed meth-
ods with the string and distributional similar-
ity methods of Salehi and Cook (2013) and
Salehi et al. (2014), we see substantial improve-
ments over the comparable combined method of
?LCS+DS+DSL2? in most cases, demonstrating
both the robustness of the proposed methods and
their complementarity with the earlier methods. It
is important to reinforce that the proposed meth-
ods make no language-specific assumptions and
are therefore applicable to any type of MWE and
any language, with the only requirement being that
the MWE of interest be listed in the Wiktionary for
1795
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 24.6 36.8 29.5 59.6 40.5 48.2
LCS 36.5 49.2 39.3 61.5 63.7 60.3
DS 32.8 34.1 33.5 80.9 19.6 29.7
DS+DSL2 31.8 72.4 44.2 74.8 27.5 36.6
LCS+DS+DSL2 36.1 62.6 45.8 77.9 42.8 49.2
FIRSTDEF 24.8 84.2 38.3 54.5 94.0 69.0
ALLDEFS 25.0 97.4 39.8 53.6 97.6 69.2
ITAG 26.2 89.5 40.5 54.6 91.7 68.4
FIRSTDEF+SYN 32.9 65.8 43.9 60.4 65.5 62.9
ALLDEFS+SYN 28.4 81.6 42.1 62.5 77.4 69.1
ITAG+SYN 30.5 65.8 41.7 57.8 61.9 59.8
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
34.0 65.3 44.7 83.6 67.3 65.4
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
37.4 70.9 48.9 80.4 65.9 63.0
ITAG+SYN
COMB(LCS+DS+DSL2)
35.6 70.9 47.4 83.5 64.9 64.2
Table 3: Compositionality prediction results over the EVPC dataset, relative to the first component (the
head verb) and the second component (the particle)
that language.
6 Error Analysis
We analysed all items in each dataset where the
system score differed from that of the human
annotators. For both datasets, the majority of
incorrectly-labelled items were compositional but
predicted to be non-compositional by our sys-
tem, as can be seen in the relatively low preci-
sion scores in Tables 2 and 3. In many of these
cases, the prediction based on definitions and syn-
onyms was compositional but the prediction based
on translations was non-compositional. In such
cases, we arbitrarily break the tie by labelling the
instance as non-compositional, and in doing so
favour recall over precision.
Some of the incorrectly-labelled ENCs have
a gold-standard annotation of around 2.5, or in
other words are semi-compositional. For exam-
ple, the compositionality score for game in game
plan is 2.82/5, but our system labels it as non-
compositional; a similar thing happens with figure
and the EVPC figure out. Such cases demonstrate
the limitation of approaches to MWE composi-
tionality that treat the problem as a binary clas-
sification task.
On average, the EVPCs have three senses,
which is roughly twice the number for ENCs. This
makes the prediction of compositionality harder,
as there is more information to combine across (an
effect that is compounded with the addition of syn-
onyms and translations). In future work, we hope
to address this problem by first finding the sense
which matches best with the sentences given to the
annotators.
7 Conclusion
We have proposed an unsupervised approach for
predicting the compositionality of an MWE rel-
ative to each of its components, based on lexi-
cal overlap using Wiktionary, optionally incorpo-
rating synonym and translation data. Our experi-
ments showed that the various instantiations of our
approach are superior to previous state-of-the-art
supervised methods. All code to replicate the re-
sults in this paper has been made publicly avail-
able at https://github.com/bsalehi/
wiktionary_MWE_compositionality.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
References
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
1796
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M.
Colowick. 2010. PanLex and LEXTRACT: Trans-
lating all words of all languages of the world. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
37?40, Beijing, China.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Richard Forthergill and Timothy Baldwin. 2011.
Fleshing it out: A supervised approach to MWE-
token and MWE-type classification. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 911?
919, Chiang Mai, Thailand.
David Kamholz, Jonathan Pool, and Susan Colowick.
2014. PanLex: Building a resource for panlingual
lexical translation. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), pages 3145?3150, Reyk-
javik, Iceland.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th Annual International Conference on Systems
Documentation, pages 24?26, Ontario, Canada.
Grace Muzny and Luke Zettlemoyer. 2013. Auto-
matic idiom identification in Wiktionary. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1417?
1421, Seattle, USA.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014.
Using distributional similarity of multi-way transla-
tions to predict multiword expression composition-
ality. In Proceedings of the 14th Conference of the
EACL (EACL 2014), pages 472?481, Gothenburg,
Sweden.
1797
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Word Sense Induction for Novel Sense Detection
Jey Han Lau,?? Paul Cook,? Diana McCarthy, ?
David Newman,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
? Lexical Computing
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
Abstract
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
1 Introduction
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling ? using La-
tent Dirichlet Allocation (LDA: Blei et al(2003))
and derivative approaches ? and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics ? in the form of
multinomial probability distributions over words
? and per-document topic assignments ? in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
2 Methodology
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
591
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document?s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models?s probabilistic assignment of
topics to words for the WSI task.
2.1 Data Representation and Pre-processing
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The ?document? in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pado? and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
2.2 Topic Modelling
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (?instance?
in WSI parlance) as our distribution over senses
for that word.
1Notwithstanding the one sense per discourse heuristic
(Gale et al 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al 2006).
In our initial experiments, we use LDA topic
modelling, which requires us to set T , the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P (t = z|d) then; (2) draw a word w from
the chosen topic P (w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
P (w|d) =
T
?
z=1
P (w|t = z)P (t = z|d).
In standard LDA, the user needs to specify the
number of topics T . In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al(2006)),3 where, for each
document, a distribution of mixture components
P (t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ?
DP (?,H); (2) for each document d, generate dis-
tribution P (t|d) ? DP (?0, G0); (3) draw a la-
tent topic z from the document?s mixture compo-
nent distribution P (t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P (w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, argmaxz P (t = z|d).
3 SemEval Experiments
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/?blei/
topicmodeling.html) in our experiments.
4The two HDP parameters ? and ?0 control the variabil-
ity of senses in the documents. In particular, ? controls the
degree of sharing of topics across documents ? a high ?
value leads to more topics, as topics for different documents
are more dissimilar. ?0, on the other hand, controls the de-
gree of mixing of topics within a document? a high ?0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
592
Soroa, 2007; Manandhar et al 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
3.1 SemEval-2010
3.1.1 Dataset and Methodology
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al 2010), and
are hence omitted from this paper.
In our original experiments with LDA, we set
the number of topics (T ) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
1. LDA+Variable T : LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T : LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T ), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom ? randomly assign each test instance to one
of four senses; (2) Baseline MFS ? most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
3.2 SemEval-2010 Results
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
593
System WSD (80%/20%)All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark
UoY 0.62 0.67 0.59
Table 1: WSD F-score over the SemEval-2010 dataset
Looking first at the results for LDA, we see
that the first LDA approach (variable T ) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn?t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn?t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are ?junk? senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the ?Vari-
able T ? version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the ?genuine? topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly ?split? a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
594
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
3.3 Comparison with other Topic Modelling
Approaches to WSI
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a ?lay-
ered? feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn?t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al(2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
595
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
3.4 Discussion
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD?s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD?s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
4 Identifying Novel Senses
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
1. We all know Frankie is adorable, but does he
have swag? [swag = ?style?]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ?send a mes-
sage on Twitter?]
These senses of swag and tweet are not included
in many dictionaries or computational lexicons ?
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) ? yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin?
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
596
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
4.1 Method
Given two corpora ? a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts ? we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (?Nov?) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
Nov(w) = max
({
ps(ti)? pr(ti)
pr(ti)
: ti ? T
})
(1)
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
4.2 Data
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC ? consisting primar-
ily of British English text from the late 20th cen-
tury ? as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al 2008) ? a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types ? as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
597
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation ? only for selecting items
for our dataset ? we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) ?In-
ternet domain?; export (v) ?export data?; mirror
(n) ?mirror website?; poster (n) ?one who posts
online?; and worm (n) ?malicious program?. For
each of the 5 lemmas with novel senses, a sec-
ond annotator ? also a native English-speaking
author of this paper ? annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
4.3 Results
We compute novelty (?Nov?, Equation 1) for all
10 items in our dataset, based on the output of the
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 ?
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 ?
cinema (n) 9.7 1.30 ?
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 ?
symptom (n) 2.1 1.16 ?
Table 4: Novelty score (?Nov?), ratio of frequency in
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
?Novelty? in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p < .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn ?Freq. ratio? in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p > .05).
Examining the frequency of the novel senses?
shown in column ?Novel sense freq.? in Table 4
? we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n?t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
598
Lemma
Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (?Internet domain?) being split across two
induced senses ? the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma?s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (?Nov?), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense ? in terms of F-score ?
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses ? in
terms of F-score ? as the novel sense could of-
fer further improvements. In this case ? results
shown in the final three columns of Table 5 ?
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
5 Conclusion
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
599
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1?10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103?111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233?237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67?71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-He?le`ne Corre?ard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125?
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355?358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116?126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177?182,
Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161?199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
600
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, Oregon.
601
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 69?72,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Support Platform for Event Detection using Social Intelligence
Timothy Baldwin, Paul Cook, Bo Han, Aaron Harwood,
Shanika Karunasekera and Masud Moshtaghi
Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
Abstract
This paper describes a system designed
to support event detection over Twitter.
The system operates by querying the data
stream with a user-specified set of key-
words, filtering out non-English messages,
and probabilistically geolocating each mes-
sage. The user can dynamically set a proba-
bility threshold over the geolocation predic-
tions, and also the time interval to present
data for.
1 Introduction
Social media and micro-blogs have entered the
mainstream of society as a means for individu-
als to stay in touch with friends, for companies
to market products and services, and for agen-
cies to make official announcements. The attrac-
tions of social media include their reach (either
targeted within a social network or broadly across
a large user base), ability to selectively pub-
lish/filter information (selecting to publish cer-
tain information publicly or privately to certain
groups, and selecting which users to follow),
and real-time nature (information ?push? happens
immediately at a scale unachievable with, e.g.,
email). The serendipitous takeoff in mobile de-
vices and widespread support for social media
across a range of devices, have been significant
contributors to the popularity and utility of social
media.
While much of the content on micro-blogs de-
scribes personal trivialities, there is also a vein of
high-value content ripe for mining. As such, or-
ganisations are increasingly targeting micro-blogs
for monitoring purposes, whether it is to gauge
product acceptance, detect events such as traffic
jams, or track complex unfolding events such as
natural disasters.
In this work, we present a system intended
to support real-time analysis and geolocation of
events based on Twitter. Our system consists of
the following steps: (1) user selection of key-
words for querying Twitter; (2) preprocessing of
the returned queries to rapidly filter out messages
not in a pre-selected set of languages, and option-
ally normalise language content; (3) probabilistic
geolocation of messages; and (4) rendering of the
data on a zoomable map via a purpose-built web
interface, with facility for rich user interaction.
Our starting in the development of this system
was the Ushahidi platform,1 which has high up-
take for social media surveillance and information
dissemination purposes across a range of organ-
isations. The reason for us choosing to imple-
ment our own platform was: (a) ease of integra-
tion of back-end processing modules; (b) extensi-
bility, e.g. to visualise probabilities of geolocation
predictions, and allow for dynamic thresholding;
(c) code maintainability; and (d) greater logging
facility, to better capture user interactions.
2 Example System Usage
A typical user session begins with the user spec-
ifying a disjunctive set of keywords, which are
used as the basis for a query to the Twitter
Streaming API.2 Messages which match the query
are dynamically rendered on an OpenStreetMap
mash-up, indexed based on (grid cell-based) loca-
tion. When the user clicks on a location marker,
they are presented with a pop-up list of messages
matching the location. The user can manipulate a
time slider to alter the time period over which to
present results (e.g. in the last 10 minutes, or over
1http://ushahidi.com/
2https://dev.twitter.com/docs/
streaming-api
69
Figure 1: A screenshot of the system, with a pop-up presentation of the messages at the indicated location.
the last hour), to gain a better sense of report re-
cency. The user can further adjust the threshold of
the prediction accuracy for the probabilistic mes-
sage locations to view a smaller number of mes-
sages with higher-confidence locations, or more
messages that have lower-confidence locations.
A screenshot of the system for the following
query is presented in Figure 1:
study studying exam ?end of semester?
examination test tests school exams uni-
versity pass fail ?end of term? snow
snowy snowdrift storm blizzard flurry
flurries ice icy cold chilly freeze freez-
ing frigid winter
3 System Details
The system is composed of a front-end, which
provides a GUI interface for query parameter in-
put, and a back-end, which computes a result for
each query. The front-end submits the query pa-
rameters to the back-end via a servlet. Since
the result for the query is time-dependent, the
back-end regularly re-evaluates the query, gener-
ating an up-to-date result at regular intervals. The
front-end regularly polls the back-end, via another
servlet, for the latest results that match its submit-
ted query. In this way, the front-end and back-end
are loosely coupled and asynchronous.
Below, we describe details of the various mod-
ules of the system.
3.1 Twitter Querying
When the user inputs a set of keywords, this is is-
sued as a disjunctive query to the Twitter Stream-
ing API, which returns a streamed set of results
in JSON format. The results are parsed, and
piped through to the language filtering, lexical
normalisation, and geolocation modules, and fi-
nally stored in a flat file, which the GUI interacts
with.
3.2 Language Filtering
For language identification, we use langid.py,
a language identification toolkit developed at
The University of Melbourne (Lui and Baldwin,
2011).3 langid.py combines a naive Bayes
classifier with cross-domain feature selection to
provide domain-independent language identifica-
tion. It is available under a FOSS license as
a stand-alone module pre-trained over 97 lan-
guages. langid.py has been developed specif-
ically to be able to keep pace with the speed
of messages through the Twitter ?garden hose?
feed on a single-CPU machine, making it par-
ticularly attractive for this project. Additionally,
in an in-house evaluation over three separate cor-
pora of Twitter data, we have found langid.py
to be overall more accurate than other state-of-
the-art language identification systems such as
3http://www.csse.unimelb.edu.au/
research/lt/resources/langid
70
lang-detect4 and the Compact Language De-
tector (CLD) from the Chrome browser.5
langid.py returns a monolingual prediction
of the language content of a given message, and is
used to filter out all non-English tweets.
3.3 Lexical Normalisation
The prevalence of noisy tokens in microblogs
(e.g. yr ?your? and soooo ?so?) potentially hin-
ders the readability of messages. Approaches
to lexical normalisation?i.e., replacing noisy to-
kens by their standard forms in messages (e.g.
replacing yr with your)?could potentially over-
come this problem. At present, lexical normali-
sation is an optional plug-in for post-processing
messages.
A further issue related to noisy tokens is that
it is possible that a relevant tweet might contain
a variant of a query term, but not that query term
itself. In future versions of the system we there-
fore aim to use query expansion to generate noisy
versions of query terms to retrieve additional rel-
evant tweets. We subsequently intend to perform
lexical normalisation to evaluate the precision of
the returned data.
The present lexical normalisation used by our
system is the dictionary lookup method of Han
and Baldwin (2011) which normalises noisy to-
kens only when the normalised form is known
with high confidence (e.g. you for u). Ultimately,
however, we are interested in performing context-
sensitive lexical normalisation, based on a reim-
plementation of the method of Han and Baldwin
(2011). This method will allow us to target a
wider variety of noisy tokens such as typos (e.g.
earthquak ?earthquake?), abbreviations (e.g. lv
?love?), phonetic substitutions (e.g. b4 ?before?)
and vowel lengthening (e.g. goooood ?good?).
3.4 Geolocation
A vital component of event detection is the de-
termination of where the event is happening, e.g.
to make sense of reports of traffic jams or floods.
While Twitter supports device-based geotagging
of messages, less than 1% of messages have geo-
tags (Cheng et al 2010). One alternative is to re-
turn the user-level registered location as the event
4http://code.google.com/p/
language-detection/
5http://code.google.com/p/
chromium-compact-language-detector/
location, based on the assumption that most users
report on events in their local domicile. However,
only about one quarter of users have registered lo-
cations (Cheng et al 2010), and even when there
is a registered location, there?s no guarantee of
its quality. A better solution would appear to be
the automatic prediction of the geolocation of the
message, along with a probabilistic indication of
the prediction quality.6
Geolocation prediction is based on the terms
used in a given message, based on the assumption
that it will contain explicit mentions of local place
names (e.g. London) or use locally-identifiable
language (e.g. jawn, which is characteristic of the
Philadelphia area). By including a probability
with the prediction, we can give the system user
control over what level of noise they are prepared
to see in the predictions, and hopefully filter out
messages where there is insufficient or conflicting
geolocating evidence.
We formulate the geolocation prediction prob-
lem as a multinomial naive Bayes classification
problem, based on its speed and accuracy over the
task. Given a message m, the task is to output the
most probable location locmax ? {loci}n1 for m.
User-level classification can be performed based
on a similar formulation, by combining the total
set of messages from a given user into a single
combined message.
Given a message m, the task is to find
argmaxi P (loci|m) where each loci is a grid cell
on the map. Based on Bayes? theorem and stan-
dard assumptions in the naive Bayes formulation,
this is transformed into:
argmax
i
P (loci)
v?
j
P (wj |loci)
To avoid zero probabilities, we only consider to-
kens that occur at least twice in the training data,
and ignore unseen words. A probability is calcu-
lated for the most-probable location by normalis-
ing over the scores for each loci.
We employ the method of Ritter et al(2011) to
tokenise messages, and use token unigrams as fea-
tures, including any hashtags, but ignoring twitter
mentions, URLs and purely numeric tokens. We
6Alternatively, we could consider a hybrid approach of
user- and message-level geolocation prediction, especially
for users where we have sufficient training data, which we
plan to incorporate into a future version of the system.
71
ll
l
l
l
l
l l l
l l l
10000 20000 30000 40000
0.15
0.20
0.25
0.30
0.35
0.40
Feature Number
Pre
dict
ion 
Acc
urac
y
Figure 2: Accuracy of geolocation prediction, for
varying numbers of features based on information gain
also experimented with included the named en-
tity predictions of the Ritter et al(2011) method
into our system, but found that it had no impact
on predictive accuracy. Finally, we apply feature
selection to the data, based on information gain
(Yang and Pedersen, 1997).
To evaluate the geolocation prediction mod-
ule, we use the user-level geolocation dataset
of Cheng et al(2010), based on the lower 48
states of the USA. The user-level accuracy of our
method over this dataset, for varying numbers of
features based on information gain, can be seen
in Figure 2. Based on these results, we select the
top 36,000 features in the deployed version of the
system.
In the deployed system, the geolocation pre-
diction model is trained over one million geo-
tagged messages collected over a 4 month pe-
riod from July 2011, resolved to 0.1-degree lat-
itude/longitude grid cells (covering the whole
globe, excepting grid locations where there were
less than 8 messages). For any geotagged mes-
sages in the test data, we preserve the geotag and
simply set the probability of the prediction to 1.0.
3.5 System Interface
The final output of the various pre-processing
modules is a list of tweets that match the query,
in the form of an 8-tuple as follows:
? the Twitter user ID
? the Twitter message ID
? the geo-coordinates of the message (either
provided with the message, or automatically
predicted)
? the probability of the predicated geolocation
? the text of the tweet
In addition to specifying a set of keywords for
a given session, system users can dynamically se-
lect regions on the map, either via the manual
specification of a bounding box, or zooming the
map in and out. They can additionally change
the time scale to display messages over, specify
the refresh interval and also adjust the threshold
on the geolocation predictions, to not render any
messages which have a predictive probability be-
low the threshold. The size of each place marker
on the map is rendered proportional to the num-
ber of messages at that location, and a square is
superimposed over the box to represent the max-
imum predictive probability for a single message
at that location (to provide user feedback on both
the volume of predictions and the relative confi-
dence of the system at a given location).
References
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based ap-
proach to geo-locating twitter users. In Proceedings
of the 19th ACM international conference on In-
formation and knowledge management, CIKM ?10,
pages 759?768, Toronto, ON, Canada. ACM.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011),
pages 368?378, Portland, USA.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
In Proceedings of the 5th International Joint Con-
ference on Natural Language Processing (IJCNLP
2011), pages 553?561, Chiang Mai, Thailand.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An
experimental study. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1524?1534, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
412?420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
72
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 443?451,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Is Machine Translation Getting Better over Time?
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems
The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
Recent human evaluation of machine
translation has focused on relative pref-
erence judgments of translation quality,
making it difficult to track longitudinal im-
provements over time. We carry out a
large-scale crowd-sourcing experiment to
estimate the degree to which state-of-the-
art performance in machine translation has
increased over the past five years. To fa-
cilitate longitudinal evaluation, we move
away from relative preference judgments
and instead ask human judges to provide
direct estimates of the quality of individ-
ual translations in isolation from alternate
outputs. For seven European language
pairs, our evaluation estimates an aver-
age 10-point improvement to state-of-the-
art machine translation between 2007 and
2012, with Czech-to-English translation
standing out as the language pair achiev-
ing most substantial gains. Our method
of human evaluation offers an economi-
cally feasible and robust means of per-
forming ongoing longitudinal evaluation
of machine translation.
1 Introduction
Human evaluation provides the foundation for em-
pirical machine translation (MT), whether human
judges are employed directly to evaluate system
output, or via the use of automatic metrics ?
validated through correlation with human judg-
ments. Achieving consistent human evaluation
is not easy, however. Annual evaluation cam-
paigns conduct large-scale human assessment but
report ever-decreasing levels of judge consistency
? when given the same pair of translations to
repeat-assess, even expert human judges will wor-
ryingly often contradict both the preference judg-
ment of other judges and their own earlier prefer-
ence (Bojar et al., 2013). For this reason, human
evaluation has been targeted within the commu-
nity as an area in need of attention, with increased
efforts to develop more reliable methodologies.
One standard platform for human evaluation is
WMT shared tasks, where assessments have (since
2007) taken the form of ranking five alternate sys-
tem outputs from best to worst (Bojar et al., 2013).
This method has been shown to produce more con-
sistent judgments compared to fluency and ade-
quacy judgments on a five-point scale (Callison-
Burch et al., 2007). However, relative preference
judgments have been criticized for being a sim-
plification of the real differences between trans-
lations, not sufficiently taking into account the
large number of different types of errors of vary-
ing severity that occur in translations (Birch et al.,
2013). Relative preference judgments do not take
into account the degree to which one translation is
better than another ? there is no way of knowing if
a winning system produces far better translations
than all other systems, or if that system would have
ranked lower if the severity of its inferior transla-
tion outputs were taken into account.
Rather than directly aiming to increase human
judge consistency, some methods instead increase
the number of reference translations available to
automatic metrics. HTER (Snover et al., 2006)
employs humans to post-edit each system out-
put, creating individual human-targeted reference
translations which are then used as the basis for
computing the translation error rate. HyTER, on
the other hand, is a tool that facilitates creation
of very large numbers of reference translations
(Dreyer and Marcu, 2012). Although both ap-
proaches increase fairness compared to automatic
metrics that use a single generic reference transla-
tion, even human post-editors will inevitably vary
in the way they post-edit translations, and the pro-
cess of creating even a single new reference trans-
443
lation for each system output is often too resource-
intensive to be used in practice.
With each method of human evaluation, a trade-
off exists between annotation time and the number
of judgments collected. At one end of the spec-
trum, the WMT human evaluation collects large
numbers of quick judgments (approximately 3.5
minutes per screen, or 20 seconds per label) (Bojar
et al., 2013).
1
In contrast, HMEANT (Lo and Wu,
2011) uses a more time-consuming fine-grained
semantic-role labeling analysis at a rate of approx-
imately 10 sentences per hour (Birch et al., 2013).
But even with this detailed evaluation methodol-
ogy, human judges are inconsistent (Birch et al.,
2013).
Although the trend appears to be toward more
fine-grained human evaluation of MT output, it
remains to be shown that this approach leads to
more reliable system rankings ? with a main rea-
son to doubt this being that far fewer judgments
will inevitably be possible. We take a counter-
approach and aim to maintain the speed by which
assessments are collected in shared task evalua-
tions, but modify the evaluation set-up in two main
ways: (1) we structure the judgments as monolin-
gual tasks, reducing the cognitive load involved
in assessing translation quality; and (2) we ap-
ply judge-intrinsic quality control and score stan-
dardization, to minimize noise introduced when
crowd-sourcing is used to leverage numbers of as-
sessments and to allow for the fact that human
judges will vary in the way they assess transla-
tions. Assessors are regarded as reliable as long
as they demonstrate consistent judgments across a
range of different quality translations.
We elicit direct estimates of quality from
judges, as a quantitative estimate of the magni-
tude of each attribute of interest (Steiner and Nor-
man, 1989). Since we no longer look for rela-
tive preference judgments, we revert back to the
original fluency and adequacy criteria last used in
WMT 2007 shared task evaluation. Instead of five-
point fluency/adequacy scales, however, we use
a (100-point) continuous rating scale, as this fa-
cilitates more sophisticated statistical analyses of
score distributions for judges, including worker-
intrinsic quality control for crowd-sourcing. The
latter does not depend on agreement with ex-
perts, and is made possible by the reduction in
1
WMT 2013 reports 361 hours of labor to collect 61,695
labels, with approximately one screen of five pairwise com-
parisons each yielding a set of 10 labels.
information-loss when a continuous scale is used.
In addition, translations are assessed in isolation
from alternate system outputs, so that judgments
collected are no longer relative to a set of five
translations. This has the added advantage of elim-
inating the criticism made of WMT evaluations
that systems sometimes gain advantage from luck-
of-the-draw comparison with low quality output,
and vice-versa (Bojar et al., 2011).
Based on our proposed evaluation methodology,
human judges are able to work quickly, on average
spending 18 and 13 seconds per single segment ad-
equacy and fluency judgment, respectively. Addi-
tionally, when sufficiently large volumes of such
judgments are collected, mean scores reveal sig-
nificant differences between systems. Further-
more, since human evaluation takes the form of di-
rect estimates instead of relative preference judg-
ments, our evaluation introduces the possibility
of large-scale longitudinal human evaluation. We
demonstrate the value of longitudinal evaluation
by investigating the improvement made to state-
of-the-art MT over a five year time period (be-
tween 2007 and 2012) using the best participating
WMT shared task system output. Since it is likely
that the test data used for shared tasks has varied
in difficulty over this time period, we additionally
propose a simple mechanism for scaling system
scores relative to task difficulty.
Using the proposed methodology for measur-
ing longitudinal change in MT, we conclude that,
for the seven European language pairs we evalu-
ate, MT has made an average 10% improvement
over the past 5 years. Our method uses non-expert
monolingual judges via a crowd-sourcing portal,
with fast turnaround and at relatively modest cost.
2 Monolingual Human Evaluation
There are several reasons why the assessment of
MT quality is difficult. Ideally, each judge should
be a native speaker of the target language, while
at the same time being highly competent in the
source language. Genuinely bilingual people are
rare, however. As a result, judges are often peo-
ple with demonstrated skills in the target language,
and a working knowledge ? often self-assessed ?
of the source language. Adding to the complexity
is the discipline that is required: the task is cog-
nitively difficult and time-consuming when done
properly. The judge is, in essence, being asked to
decide if the supplied translations are what they
444
would have generated if they were asked to do the
same translation.
The assessment task itself is typically structured
as follows: the source segment (a sentence or
a phrase), plus five alternative translations and a
?reference? translation are displayed. The judge
is then asked to assign a rank order to the five
translations, from best to worst. A set of pairwise
preferences are then inferred, and used to generate
system rankings, without any explicit formation of
stand-alone system ?scores?.
This structure introduces the risk that judges
will only compare translations against the refer-
ence translation. Certainly, judges will vary in
the degree they rely on the reference translation,
which will in turn impact on inter-judge inconsis-
tency. For instance, even when expert judges do
assessments, it is possible that they use the ref-
erence translation as a substitute for reading the
source input, or do not read the source input at
all. And if crowd-sourcing is used, can we really
expect high proportions of workers to put the ad-
ditional effort into reading and understanding the
source input when a reference translation (proba-
bly in their native language) is displayed? In re-
sponse to this potential variability in how annota-
tors go about the assessment task, we trial assess-
ments of adequacy in which the source input is not
displayed to human judges. We structure assess-
ments as a monolingual task and pose them in such
a way that the focus is on comparing the meaning
of reference translations and system outputs.
2
We therefore ask human judges to assess the de-
gree to which the system output conveys the same
meaning as the reference translation. In this way,
we focus the human judge indirectly on the ques-
tion we wish to answer when assessing MT: does
the translation convey the meaning of the source?
The fundamental assumption of this approach is
that the reference translation accurately captures
the meaning of the source; once that assumption
is made, it is clear that the source is not required
during the evaluation.
Benefits of this change are that the task is both
easier to describe to novice judges, and easier
to answer, and that it requires only monolingual
speakers, opening up the evaluation to a vastly
larger pool of genuinely qualified workers.
With this set-up in place for adequacy, we also
2
This dimension of the assessment is similar but not iden-
tical to the monolingual adequacy assessment in early NIST
evaluation campaigns (NIST, 2002).
re-introduce a fluency assessment. Fluency rat-
ings can be carried out without the presence of a
reference translation, reducing any remnant bias
towards reference translations in the evaluation
setup. That is, we propose a judgment regime in
which each task is presented as a two-item fluency
and adequacy judgment, evaluated separately, and
with adequacy restructured into a monolingual
?similarity of meaning? task.
When fluency and adequacy were originally
used for human evaluation, each rating used a 5-
point adjective scale (Callison-Burch et al., 2007).
However, adjectival scale labels are problematic
and ratings have been shown to be highly depen-
dent on the exact wording of descriptors (Seymour
et al., 1985). Alexandrov (2010) provides a sum-
mary of the extensive problems associated with the
use of adjectival scale labels, including bias result-
ing from positively- and negatively-worded items
not being true opposites of one another, and items
intended to have neutral intensity in fact proving
to have specific conceptual meanings.
It is often the case, however, that the question
could be restructured so that the rating scale no
longer requires adjectival labels, by posing the
question as a statement such as The text is fluent
English and asking the human assessor to specify
how strongly they agree or disagree with that state-
ment. The scale and labels can then be held con-
stant across experimental set-ups for all attributes
evaluated ? meaning that if the scale is still biased
in some way it will be equally so across all set-ups.
3 Assessor Consistency
One way of estimating the quality of a human
evaluation regime is to measure its consistency:
whether or not the same outcome is achieved if
the same question is asked a second time. In
MT, annotator consistency is commonly measured
using Cohen?s kappa coefficient, or some variant
thereof (Artstein and Poesio, 2008). Originally de-
veloped as a means of establishing assessor inde-
pendence, it is now commonly used in the reverse
sense, with high numeric values being used as ev-
idence of agreement. Two different measurements
can be made ? whether a judge is consistent with
other judgments performed by themselves (intra-
annotator agreement), and whether a judge is con-
sistent with other judges (inter-annotator agree-
ment).
Cohen?s kappa is intended for use with categor-
445
ical judgments, but is also commonly used with
five-point adjectival-scale judgments, where the
set of categories has an explicit ordering. One
particular issue with five-point assessments is that
score standardization cannot be applied. As such,
a judge who assigns two neighboring intervals is
awarded the same ?penalty? for being ?different?
as the judge who chooses the extremities. The
kappa coefficient cannot be directly applied to
many-valued interval or continuous data.
This raises the question of how we should eval-
uate assessor consistency when a continuous rat-
ing scale is in place. No judge, when given the
same translation to judge twice on a continuous
rating scale, can be expected to give precisely the
same score for each judgment (where repeat as-
sessments are separated by a considerable number
of intervening ones). A more flexible tool is thus
required. We build such a tool by starting with two
core assumptions:
A: When a consistent assessor is presented with
a set of repeat judgments, the mean of the
initial set of assessments will not be signifi-
cantly different from the mean score of repeat
assessments.
B: When a consistent judge is presented with a
set of judgments for translations from two
systems, one of which is known to produce
better translations than the other, the mean
score for the better system will be signifi-
cantly higher than that of the inferior system.
Assumption B is the basis of our quality-control
mechanism, and allows us to distinguish between
Turkers who are working carefully and those who
are merely going through the motions. We use a
100-judgment HIT structure to control same-judge
repeat items and deliberately-degraded system
outputs (bad reference items) used for worker-
intrinsic quality control (Graham et al., 2013).
bad reference translations for fluency judgments
are created as follows: two words in the translation
are randomly selected and randomly re-inserted
elsewhere in the sentence (but not as the initial or
final words of the sentence).
Since adding duplicate words will not degrade
adequacy in the same way, we use an alternate
method to create bad reference items for adequacy
judgments: we randomly delete a short sub-string
of length proportional to the length of the origi-
nal translation to emulate a missing phrase. Since
total fltrd Assum A total fltrd
wrkrs wrkrs holds segs segs
F 557 321 (58%) 314 (98.8%) 122k 78k (64%)
A 542 283 (52%) 282 (99.6%) 102k 62k (61%)
Table 1: Total quality control filtered workers and
assessments (F = fluency; A = adequacy).
this is effectively a new degradation scheme, we
tested against experts. For low-quality transla-
tions, deleting just two words from a long sentence
often made little difference. The method we even-
tually settled on removes a sequence of k words,
as a function of sentence length n:
2 ? n ? 3 ? k = 1
4 ? n ? 5 ? k = 2
6 ? n ? 8 ? k = 3
9 ? n ? 15 ? k = 4
16 ? n ? 20 ? k = 5
n > 20 ? k =
?
n
5
?
To filter out careless workers, scores for
bad reference pairs are extracted, and a
difference-of-means test is used to calculate
a worker-reliability estimate in the form of a
p-value. Paired tests are then employed using the
raw scores for degraded and corresponding system
outputs, using a reliability significance threshold
of p < 0.05. If a worker does not demonstrate
the ability to reliably distinguish between a bad
system and a better one, the judgments from
that worker are discarded. This methodology
means that careless workers who habitually rate
translations either high or low will be detected,
as well as (with high probability) those that click
(perhaps via robots) randomly. It also has the
advantage of not filtering out workers who are
internally consistent but whose scores happen not
to correspond particularly well to a set of expert
assessments.
Having filtered out users who are unable to reli-
ably distinguish between better and worse sets of
translations (p ? 0.05), we can now examine how
well Assumption A holds for the remaining users,
i.e. the extent to which workers apply consistent
scores to repeated translations. We compute mean
scores for the initial and repeat items and look for
even very small differences in the two distribu-
tions for each worker. Table 1 shows numbers of
workers who passed quality control, and also that
446
Si
S
i+5
1 bad reference its corresponding system output
1 system output a repeat of it
1 reference its corresponding system output
Above in reverse for S
i
and S
i+5
4 system outputs 4 system outputs
Table 2: Control of repeat item pairs. S
i
denotes
the i
th
set of 10 translations assessed within a 100
translation HIT.
the vast majority (around 99%) of reliable work-
ers have no significant difference between mean
scores for repeat items.
4 Five Years of Machine Translation
To estimate the improvement in MT that took
place between 2007 and 2012, we asked work-
ers on Amazon?s Mechanical Turk (MTurk) to rate
the quality of translations produced by the best-
reported participating system for each of WMT
2007 and WMT 2012 (Callison-Burch et al., 2007;
Callison-Burch et al., 2012). Since it is likely that
the test set has changed in difficulty over this time
period, we also include in the evaluation the orig-
inal test data for 2007 and 2012, translated by a
single current MT system. We use the latter to cal-
ibrate the results for test set difficulty, by calcu-
lating the average difference in rating, ?, between
the 2007 and 2012 test sets. This is then added
to the difference in rating for the best-reported
systems in 2012 and 2007, to arrive at an over-
all evaluation of the 5-year gain in MT quality for
a given language pair, separately for fluency and
adequacy.
Experiments were carried out for each of Ger-
man, French and Spanish into and out of English,
and also for Czech-to-English. English-to-Czech
was omitted because of a low response rate on
MTurk. For language pairs where two systems tied
for first place in the shared task, a random selec-
tion of translations from both systems was made.
HIT structure
To facilitate quality control, we construct each
HIT on MTurk as an assessment of 100 trans-
lations. Each individual translation is rated in
isolation from other translations with workers re-
quired to iterate through 100 translations without
the opportunity to revisit earlier assessments. A
100-translation HIT contains the following items:
70 randomly selected system outputs made up of
roughly equal proportions of translations for each
evaluated system, 10 bad reference translations
(each based on one of the 70 system outputs), 10
exact repeats and 10 reference translations. We di-
vide a 100-translation HIT into 10 sets of 10 trans-
lations. Table 2 shows how the content of each set
is determined. Translations are then randomized
only within each set (of 10 translations), with the
original sequence order of the sets preserved. In
this way, the order of quality control items is un-
predictable but controlled so pairs are separated by
a minimum of 40 intervening assessments (4 sets
of translations). The HIT structure results in 80%
of assessed translations corresponding to genuine
outputs of a system (including exact repeat assess-
ments), which is ultimately what we wish to ob-
tain, with 20% of assessments belonging to quality
control items (bad reference or reference transla-
tions).
Assessment set-up
Separate HITs were provided for evaluation of flu-
ency and adequacy. For fluency, a single system
output was displayed per screen, with a worker re-
quired to rate the fluency of a translation on a 100-
point visual analog scale with no displayed point
scores. A similar set-up was used for adequacy but
with the addition of a reference translation (dis-
played in gray font to distinguish it from the sys-
tem output being assessed). The Likert-type state-
ment that framed the judgment was Read the text
below and rate it by how much you agree that:
? [for fluency] the text is fluent English
? [for adequacy] the black text adequately ex-
presses the meaning of the gray text.
In neither case was the source language string pro-
vided to the workers.
Tasks were published on MTurk, with no re-
gion restriction but the stipulation that only na-
tive speakers of the target language should com-
plete HITs, and with a qualification of an MTurk
prior HIT-approval rate of at least 95%. Instruc-
tions were always presented in the target language.
Workers were paid US$0.50 per fluency HIT, and
US$0.60 per adequacy HIT.
3
3
Since insufficient assessments were collected for French
and German evaluations in the initial run, a second and ulti-
mately third set of HITs were needed for these languages with
increased payment per HIT of US$1.0 per 100-judgment ade-
quacy HIT, US$0.65 per 100-judgment fluency HIT and later
again to US$1.00 per 100-judgment fluency HIT.
447
Close to one thousand individual Turkers con-
tributed to this experiment (some did both flu-
ency and adequacy assessments), providing a to-
tal of more than 220,000 translations, of which
140,000 were provided by workers meeting the
quality threshold.
In general, it cost approximately US$30 to as-
sess each system, with low-quality workers ap-
proximately doubling the cost of the annotation.
We rejected HITs where it was clear that random-
clicking had taken place, but did not reject solely
on the basis of having not met the quality control
threshold, to avoid penalizing well-intentioned but
low-quality workers.
Overall change in performance
Table 3 shows the overall gain made in five years,
from WMT 07 to WMT 12. Mean scores for the
two top-performing systems from each shared task
(BEST
07
, BEST
12
) are included, as well as scores
for the benchmark current MT system on the two
test sets (CURR
07
, CURR
12
). For each language
pair, a 100-translation HIT was constructed by
randomly selecting translations from the pool of
(3003+2007)?2 that were available, and this re-
sults in apparently fewer assessments for the 2007
test set. In fact, numbers of evaluated translations
are relative to the size of each test set. Average z
scores for each system are also presented, based on
the mean and standard deviation of all assessments
provided by an individual worker, with positive
values representing deviations above the mean of
workers. In addition, we include mean BLEU (Pa-
pineni et al., 2001) and METEOR (Banerjee and
Lavie, 2005) automatic scores for the same system
outputs.
The CURR benchmark shows fluency scores
that are 5.9 points higher on the 2007 data set than
they are on the 2012 test data, with a larger dif-
ference in adequacy of 8.3 points. As such, the
2012 test data is more challenging than the 2007
test data. Despite this, both fluency and adequacy
scores for the best system in 2012 have increased
by 4.5 and 2.0 points respectively, amounting to
estimated average gains of 10.4 points in fluency
and 10.3 points in adequacy for state-of-the-art
MT across the seven language pairs.
Looking at the standardized scores, it is appar-
ent that the presence of the CURR translations for
the 2007 test set pushes the mean score for the
2007 best systems below zero. The presence in
the HITs of reference translations also shifts stan-
dardized system evaluations below zero, because
they are not attributable to any of the systems be-
ing assessed.
4
Results for automatic metrics lead to similar
conclusions: that the test set has indeed increased
in difficulty; and that, in spite of this, substantial
improvements have been made according to auto-
matic metrics, +13.5 using BLEU, and +7.1 on
average using METEOR.
Language pairs
Table 4 shows mean fluency and adequacy scores
by language pair for translation into English. Rel-
ative gains in both adequacy and fluency for the to-
English language pairs are in agreement with the
estimates generated through the use of the two au-
tomatic metrics. Most notably, Czech-to-English
translation appears to have made substantial gains
across the board, achieving more than double the
gain made by some of the other language pairs; re-
sults for best participating 2007 systems show that
this may in part be caused by the fact that Czech-
to-English translation had a lower 2007 baseline
to begin with (BEST
07
F:40.8; A:41.7) in compar-
ison to, for example, Spanish-to-English transla-
tion (BEST
07
F:56.7; A:59.0).
Another notable result is that although the test
data for each year?s shared task is parallel across
five languages, test set difficulty increases by dif-
ferent degrees according to human judges and au-
tomatic metrics, with BLEU scores showing sub-
stantial divergence across the to-English language
pairs. Comparing BLEU scores achieved by the
benchmark system for Spanish to English and
Czech-to-English, for example, the benchmark
system achieves close scores on the 2007 test data
with a difference of only |52.3 ? 51.2| = 1.1,
compared to the score difference for the bench-
mark scores for translation of the 2012 test data of
|25.0 ? 38.3| = 13.3. This may indicate that the
increase in test set difficulty that has taken place
over the years has made the shared task dispro-
portionately more difficult for some language pairs
than for others. It does seem that some language
pairs are harder to translate than others, and the
differential change may be a consequence of the
fact that increasing test set complexity for all lan-
guages in parallel has a greater impact on transla-
tion difficulty for language pairs that are intrinsi-
cally harder to translate between.
4
Scores for reference translations can optionally be omit-
ted for score standardization.
448
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
fl
u
e
n
c
y
score 64.1 58.2 5.9 53.5 58.0 (+4.5) 10.4
z 0.18 0.00 0.18 ?0.16 0.00 (+0.16) 0.34
n 12,334 18,654 12,513 18,579
a
d
e
q
u
a
c
y
score 65.0 56.7 8.3 54.0 56.0 (+2.0) 10.3
z 0.18 ?0.07 0.25 ?0.16 ?0.09 (+0.07) 0.32
n 10,022 14,870 10,049 14,979
m
e
t
r
i
c
s
BLEU 41.5 30.0 11.4 25.6 27.7 (+2.1) 13.5
METEOR 49.2 41.1 8.1 41.1 40.1 (?1.0) 7.1
Table 3: Average human evaluation results for all language pairs; mean and standardized z scores are
computed in each case for n translations. In this table, and in Tables 4 and 5, all reported fluency and
adequacy values are in points relative to the 100-point assessment scale.
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
D
E
-
E
N
fluency
score 65.3
???
57.9 7.4 52.8 55.0
?
(+2.2) 9.6
n 2,164 3,381 2,242 3,253
adequacy
score 63.8
???
52.8 11.0 46.5 49.8
??
(+3.3) 14.3
n 1,458 2,175 1,454 2,193
metrics
BLEU 38.3 26.5 11.8 21.1 23.8 (+2.7) 14.5
METEOR 40.3 32.7 7.6 33.4 31.7 (?1.7) 5.9
F
R
-
E
N
fluency
score 65.9
???
58.0 7.9 57.8 60.2
??
(+2.4) 10.3
n 2,172 3,267 2,203 3,238
adequacy
score 61.0
???
52.3 8.7 52.7 51.5 (?1.2) 7.5
n 1,754 2,651 1,763 2,712
metrics
BLEU 39.4 32.0 7.4 28.6 31.5 (+2.9) 10.3
METEOR 39.8 34.6 5.2 35.9 34.3 (?1.6) 3.6
E
S
-
E
N
fluency
score 68.4
???
59.2 9.2 56.7 56.7 (+0.0) 9.2
n 1,514 2,234 1,462 2,230
adequacy
score 68.0
???
56.9 11.1 59.0
???
55.7 (?3.3) 7.8
n 1,495 2,193 1,492 2,180
metrics
BLEU 51.2 38.3 12.9 35.1 33.5 (?1.6) 11.3
METEOR 45.4 37.0 8.4 39.9 36.0 (?3.9) 4.5
C
S
-
E
N
fluency
score 62.3
???
49.9 12.4 40.8 50.5
???
(+9.7) 22.1
n 1,873 2,816 1,923 2,828
adequacy
score 62.4
???
47.5 14.9 41.7 47.4
???
(+5.7) 20.6
n 1,218 1,830 1,257 1,855
metrics
BLEU 52.3 25.0 27.3 25.1 22.4 (?2.7) 24.6
METEOR 44.7 31.6 13.1 34.3 30.8 (?3.5) 9.6
Table 4: Human evaluation of WMT 2007 and 2012 best systems for to-English language pairs. Mean
scores are computed in each case for n translations. In this table and in Table 5,
?
denotes significance at
p < 0.05;
??
significance at p < 0.01; and
???
significance at p < 0.001.
Table 5 shows results for translation out-of En-
glish, and once again human evaluation scores are
in agreement with automatic metrics with English-
to-Spanish translation achieving most substantial
gains for the three out-of-English language pairs,
an increase of 12.4 points for fluency, and 11.8
points with respect to adequacy, while English-
to-French translation achieves a gain of 8.8 for
449
CURR
07
CURR
12
?
BEST
07
BEST
12
5-Year Gain
(CURR
07
? CURR
12
) (BEST
12
? BEST
07
+ ?)
E
N
-
E
S
fluency
score 77.2
???
73.4 3.8 63.3 71.9
???
(+8.6) 12.4
n 2,286 3,318 2,336 3,420
adequacy
score 75.2
???
68.1 7.1 62.5 67.2 (+4.7) 11.8
n 1,410 2,039 1,399 2,112
metrics
BLEU 48.2 38.7 9.5 29.1 35.3 (+6.2) 15.7
METEOR 69.9 59.6 10.3 57.0 58.1 (+1.1) 11.4
E
N
-
F
R
fluency
score 57.1 55.2 1.9 49.5 56.4 (+6.9) 8.8
n 1,008 1,645 1,039 1,588
adequacy
score 64.2
?
61.9 2.3 57.2 62.3 (+5.1) 7.4
n 1,234 1,877 1,274 1,775
metrics
BLEU 37.2 30.8 6.4 25.3 29.9 (+4.6) 11.0
METEOR 59.4 52.9 6.5 50.4 52.0 (+1.6) 8.1
E
N
-
D
E
fluency
score 52.3 54.1
?
?1.8 53.7 55.5 (+1.8) 0.0
n 1,317 1,993 1,308 2,022
adequacy
score 60.3
??
57.4 2.9 58.3 58.3 (+0.0) 2.9
n 1,453 2,105 1,410 2,152
metrics
BLEU 23.6 18.7 4.9 14.6 17.2 (+2.6) 7.5
METEOR 44.7 39.1 5.6 36.7 38.0 (+1.3) 6.9
Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs.
Mean scores are computed in each case for n translations.
fluency and 7.4 points for adequacy. English-to-
German translation achieves the lowest gain of
all languages, with apparently no improvement
in fluency, as the human fluency evaluation of
the benchmark system on the supposedly easier
2007 data receives a substantially lower score than
the same system over the 2012 data. This result
demonstrates why fluency, evaluated without a ref-
erence translation, should not be used to evalu-
ate MT systems without an adequacy assessment,
since it is entirely possible for a low-adequacy
translation to achieve a high fluency score.
For all language pairs, Figure 1 plots the net
gain in fluency, adequacy and F
1
against increase
in test data difficulty.
5 Conclusion
We carried out a large-scale human evaluation
of best-performing WMT 2007 and 2012 shared
task systems in order to estimate the improvement
made to state-of-the-art machine translation over
this five year time period. Results show significant
improvements have been made in machine trans-
lation of European language pairs, with Czech-
to-English recording the greatest gains. It is also
clear from our data that the difficulty of the task
has risen over the same period, to varying degrees
0 5 10 15
0
5
10
15
Best12 ? Best 07
? ( C
urr  07
 
?
 
Curr
12 ) de?enfr?en
es?en
cs?en
en?de
en?fr
en?es
F1
Fluency
Adequacy
Figure 1: Mean fluency, adequacy and combined
F
1
scores for language pairs.
for individual language pairs.
Researchers interested in making use of the
dataset are invited to contact the first author.
Acknowledgments This work was supported by
the Australian Research Council.
450
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?
73, Ann Arbor, MI.
A. Birch, B. Haddow, U. Germann, M. Nadejde,
C. Buck, and P. Koehn. 2013. The feasibility of
HMEANT as a human MT evaluation metric. In
Proc. 8th Wkshp. Statistical Machine Translation,
pages 52?61, Sofia, Bulgaria. ACL.
O. Bojar, M. Ercegov?cevic, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evalua-
tion. In Proc. 6th Wkshp. Statistical Machine Trans-
lation, pages 1?11, Edinburgh, Scotland. ACL.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc.
8th Wkshp. Statistical Machine Translation, pages
1?44, Sofia, Bulgaria. ACL.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada. ACL.
M. Dreyer and D. Marcu. 2012. HyTER: Meaning-
equivalent semantics for translation evaluation. In
Proc. 2012 Conf. North American Chapter of the
ACL: Human Language Technologies, pages 162?
171, Montreal, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. & Interoperability with Dis-
course, pages 33?41, Sofia, Bulgaria. ACL.
C. Lo and D. Wu. 2011. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In Proc.
49th Annual Meeting of the ACL: Human Language
Techologies, pages 220?229, Portland, OR. ACL.
NIST. 2002. The 2002 NIST machine translation
evaluation plan. National Institute of Standards and
Technology. http://www.itl.nist.gov/
iad/894.01/tests/mt/2003/doc/mt03_
evalplan.v2.pdf.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2001. BLEU: A method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research, Thomas J. Watson
Research Center.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visual analogue scales in dental pain.
Pain, 21:177?185.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223?231, Boston, MA.
D. L. Steiner and G. R. Norman. 1989. Health Mea-
surement Scales, A Practical Guide to their Devel-
opment and Use. Oxford University Press, Oxford,
UK, fourth edition.
451
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472?481,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Using Distributional Similarity of Multi-way Translations to Predict
Multiword Expression Compositionality
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We predict the compositionality of multi-
word expressions using distributional sim-
ilarity between each component word and
the overall expression, based on transla-
tions into multiple languages. We evaluate
the method over English noun compounds,
English verb particle constructions and
German noun compounds. We show that
the estimation of compositionality is im-
proved when using translations into multi-
ple languages, as compared to simply us-
ing distributional similarity in the source
language. We further find that string sim-
ilarity complements distributional similar-
ity.
1 Compositionality of MWEs
Multiword expressions (hereafter MWEs) are
combinations of words which are lexically, syntac-
tically, semantically or statistically idiosyncratic
(Sag et al., 2002; Baldwin and Kim, 2009). Much
research has been carried out on the extraction and
identification of MWEs
1
in English (Schone and
Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009)
and other languages (Dias, 2003; Evert and Krenn,
2005; Salehi et al., 2012). However, considerably
less work has addressed the task of predicting the
meaning of MWEs, especially in non-English lan-
guages. As a step in this direction, the focus of
this study is on predicting the compositionality of
MWEs.
An MWE is fully compositional if its meaning
is predictable from its component words, and it is
non-compositional (or idiomatic) if not. For ex-
ample, stand up ?rise to one?s feet? is composi-
1
In this paper, we follow Baldwin and Kim (2009) in
considering MWE ?identification? to be a token-level disam-
biguation task, and MWE ?extraction? to be a type-level lex-
icon induction task.
tional, because its meaning is clear from the mean-
ing of the components stand and up. However, the
meaning of strike up ?to start playing? is largely
unpredictable from the component words strike
and up.
In this study, following McCarthy et al. (2003)
and Reddy et al. (2011), we consider composition-
ality to be graded, and aim to predict the degree
of compositionality. For example, in the dataset
of Reddy et al. (2011), climate change is judged
to be 99% compositional, while silver screen is
48% compositional and ivory tower is 9% com-
positional. Formally, we model compositionality
prediction as a regression task.
An explicit handling of MWEs has been shown
to be useful in NLP applications (Ramisch, 2012).
As an example, Carpuat and Diab (2010) proposed
two strategies for integrating MWEs into statisti-
cal machine translation. They show that even a
large scale bilingual corpus cannot capture all the
necessary information to translate MWEs, and that
in adding the facility to model the compositional-
ity of MWEs into their system, they could improve
translation quality. Acosta et al. (2011) showed
that treating non-compositional MWEs as a sin-
gle unit in information retrieval improves retrieval
effectiveness. For example, while searching for
documents related to ivory tower, we are almost
certainly not interested in documents relating to
elephant tusks.
Our approach is to use a large-scale multi-way
translation lexicon to source translations of MWEs
and their component words, and then model the
relative similarity between each of the component
words and the MWE, using distributional similar-
ity based on monolingual corpora for the source
language and each of the target languages. Our
hypothesis is that using distributional similarity
in more than one language will improve the pre-
diction of compositionality. Importantly, in order
to make the method as language-independent and
472
broadly-applicable as possible, we make no use of
corpus preprocessing such as lemmatisation, and
rely only on the availability of a translation dictio-
nary and monolingual corpora.
Our results confirm our hypothesis that distri-
butional similarity over the source language in ad-
dition to multiple target languages improves the
quality of compositionality prediction. We also
show that our method can be complemented with
string similarity (Salehi and Cook, 2013) to further
improve compositionality prediction. We achieve
state-of-the-art results over two datasets.
2 Related Work
Most recent work on predicting the composi-
tionality of MWEs can be divided into two
categories: language/construction-specific and
general-purpose. This can be at either the token-
level (over token occurrences of an MWE in a cor-
pus) or type-level (over the MWE string, indepen-
dent of usage). The bulk of work on composition-
ality has been language/construction-specific and
operated at the token-level, using dedicated meth-
ods to identify instances of a given MWE, and
specific properties of the MWE in that language
to predict compositionality (Lin, 1999; Kim and
Baldwin, 2007; Fazly et al., 2009).
General-purpose token-level approaches such
as distributional similarity have been commonly
applied to infer the semantics of a word/MWE
(Schone and Jurafsky, 2001; Baldwin et al., 2003;
Reddy et al., 2011). These techniques are based
on the assumption that the meaning of a word is
predictable from its context of use, via the neigh-
bouring words of token-level occurrences of the
MWE. In order to predict the compositionality of
a given MWE using distributional similarity, the
different contexts of the MWE are compared with
the contexts of its components, and the MWE is
considered to be compositional if the MWE and
component words occur in similar contexts.
Identifying token instances of MWEs is not al-
ways easy, especially when the component words
do not occur sequentially. For example consider
put on in put your jacket on, and put your jacket
on the chair. In the first example put on is an
MWE while in the second example, put on is a
simple verb with prepositional phrase and not an
instance of an MWE. Moreover, if we adopt a con-
servative identification method, the number of to-
ken occurrences will be limited and the distribu-
tional scores may not be reliable. Additionally,
for morphologically-rich languages, it can be dif-
ficult to predict the different word forms a given
MWE type will occur across, posing a challenge
for our requirement of no language-specific pre-
processing.
Pichotta and DeNero (2013) proposed a token-
based method for identifying English phrasal
verbs based on parallel corpora for 50 languages.
They show that they can identify phrasal verbs bet-
ter when they combine information from multiple
languages, in addition to the information they get
from a monolingual corpus. This finding lends
weight to our hypothesis that using translation data
and distributional similarity from each of a range
of target languages, can improve compositionality
prediction. Having said that, the general applica-
bility of the method is questionable ? there are
many parallel corpora involving English, but for
other languages, this tends not to be the case.
Salehi and Cook (2013) proposed a general-
purpose type-based approach using translation
data from multiple languages, and string similar-
ity between the MWE and each of the compo-
nent words. They use training data to identify the
best-10 languages for a given family of MWEs, on
which to base the string similarity, and once again
find that translation data improves their results
substantially. Among the four string similarity
measures they experimented with, longest com-
mon substring was found to perform best. Their
proposed method is general and applicable to dif-
ferent families of MWEs in different languages. In
this paper, we reimplement the method of Salehi
and Cook (2013) using longest common substring
(LCS), and both benchmark against this method
and combine it with our distributional similarity-
based method.
3 Our Approach
To predict the compositionality of a given MWE,
we first measure the semantic similarity between
the MWE and each of its component words
2
using
distributional similarity based on a monolingual
corpus in the source language. We then repeat the
process for translations of the MWE and its com-
ponent words into each of a range of target lan-
guages, calculating distributional similarity using
2
Note that we will always assume that there are two
component words, but the method is easily generalisable to
MWEs with more than two components.
473
MWE component1 component2 
score1 score2 
Translations 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Translate 
(using Panlex) 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Figure 1: Outline of our approach to computing
the distributional similarity (DS) of translations
of an MWE with each of its component words,
for a given target language. score
1
and score
2
are the similarity for the first and second compo-
nents, respectively. We obtain translations from
Panlex, and use Wikipedia as our corpus for each
language.
a monolingual corpus in the target language (Fig-
ure 1). We additionally use supervised learning to
identify which target languages (or what weights
for each language) optimise the prediction of com-
positionality (Figure 2). We hypothesise that by
using multiple translations ? rather than only in-
formation from the source language ? we will be
able to better predict compositionality.
We optionally combine our proposed approach
with string similarity, calculated based on the
method of Salehi and Cook (2013), using LCS.
Below, we detail our method for calculating dis-
tributional similarity in a given language, the dif-
ferent methods for combining distributional simi-
larity scores into a single estimate of composition-
ality, and finally the method for selecting the target
languages to use in calculating compositionality.
3.1 Calculating Distributional Similarity
In order to be consistent across all languages and
be as language-independent as possible, we calcu-
CSmethod CSmethod 
Score1 for each language Score2 for each language 
21 )1( ss ?? ??
Compositionality  score 
s1 s2 
Figure 2: Outline of the method for combin-
ing distributional similarity scores from multiple
languages, across the components of the MWE.
CS
method
refers to one of the methods described
in Section 3.2 for calculating compositionality.
late distributional similarity in the following man-
ner for a given language.
Tokenisation is based on whitespace delimiters
and punctuation; no lemmatisation or case-folding
is carried out. Token instances of a given MWE
or component word are identified by full-token n-
gram matching over the token stream. We assume
that all full stops and equivalent characters for
other orthographies are sentence boundaries, and
chunk the corpora into (pseudo-)sentences on the
basis of them. For each language, we identify the
51st?1050th most frequent words, and consider
them to be content-bearing words, in the manner
of Sch?utze (1997). This is based on the assump-
tion that the top-50 most frequent words are stop
words, and not a good choice of word for calculat-
ing distributional similarity over. That is not to say
that we can?t calculate the distributional similarity
for stop words, however (as we will for the verb
particle construction dataset ? see Section 4.3.2)
they are simply not used as the dimensions in our
calculation of distributional similarity.
We form a vector of content-bearing words
across all token occurrences of the target word,
474
on the basis of these content-bearing words. Dis-
tributional similarity is calculated over these con-
text vectors using cosine similarity. Accord-
ing to Weeds (2003), using dependency rela-
tions with the neighbouring words of the target
word can better predict the meaning of the target
word. However, in line with our assumption of no
language-specific preprocessing, we just use word
co-occurrence.
3.2 Calculating Compositionality
First, we need to calculate a combined composi-
tionality score from the individual distributional
similarities between each component word and the
MWE. Following Reddy et al. (2011), we combine
the component scores using the weighted mean (as
shown in Figure 2):
comp = ?s
1
+ (1? ?)s
2
(1)
where s
1
and s
2
are the scores for the first and
the second component, respectively. We use dif-
ferent ? settings for each dataset, as detailed in
Section 4.3.
We experiment with a range of methods for cal-
culating compositionality, as follows:
CS
L1
: calculate distributional similarity using
only distributional similarity in the source
language corpus (This is the approach used
by Reddy et al. (2011), as discussed in Sec-
tion 2).
CS
L2N
: exclude the source language, and com-
pute the mean of the distributional similarity
scores for the best-N target languages. The
value of N is selected according to training
data, as detailed in Section 3.3.
CS
L1+L2N
: calculate distributional similarity
over both the source language (CS
L1
) and
the mean of the best-N languages (CS
L2N
),
and combine via the arithmetic mean.
3
This
is to examine the hypothesis that using
multiple target languages is better than just
using the source language.
CS
SVR(L1+L2 )
: train a support vector regressor
(SVR: Smola and Sch?olkopf (2004)) over the
distributional similarities for all 52 languages
(source and target languages).
3
We also experimented with taking the mean over all the
languages ? target and source ? but found it best to com-
bine the scores for the target languages first, to give more
weight to the source language.
CS
string
: calculate string similarity using the
LCS-based method of Salehi and Cook
(2013).
4
CS
string+L1
: calculate the mean of the string
similarity (CS
string
) and distributional sim-
ilarity in the source language (Salehi and
Cook, 2013).
CS
all
: calculate the mean of the string similarity
(CS
string
) and distributional similarity scores
(CS
L1
and CS
L2N
).
3.3 Selecting Target Languages
We experiment with two approaches for combin-
ing the compositionality scores from multiple tar-
get languages.
First, inCS
L2N
(andCS
L1+L2N
andCS
all
that
build off it), we use training data to rank the target
languages according to Pearson?s correlation be-
tween the predicted compositionality scores and
the gold-standard compositionality judgements.
Based on this ranking, we take the best-N lan-
guages, and combine the individual composition-
ality scores by taking the arithmetic mean. We se-
lect N by determining the value that optimises the
correlation over the training data. In other words,
the selection ofN and accordingly the best-N lan-
guages are based on nested cross-validation over
training data, independently of the test data for that
iteration of cross-validation.
Second in CS
SVR(L1+L2 )
, we combine the
compositionality scores from the source and all 51
target languages into a feature vector, and train an
SVR over the data using LIBSVM.
5
4 Resources
In this section, we describe the resources required
by our method, and also the datasets used to eval-
uate our method.
4.1 Monolingual Corpora for Different
Languages
We collected monolingual corpora for each of 52
languages (51 target languages + 1 source lan-
guage) from XML dumps of Wikipedia. These
languages are based on the 54 target languages
4
Due to differences in our random partitioning, our re-
ported results over the two English datasets differ slightly
over the results of Salehi and Cook (2013) using the same
method.
5
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm
475
used by Salehi and Cook (2013), excluding Span-
ish because we happened not to have a dump of
Spanish Wikipedia, and also Chinese and Japanese
because of the need for a language-specific word
tokeniser. The raw corpora were preprocessed us-
ing the WP2TXT toolbox
6
to eliminate XML tags,
HTML tags and hyperlinks, and then tokenisa-
tion based on whitespace and punctuation was per-
formed. The corpora vary in size from roughly
750M tokens for English, to roughly 640K tokens
for Marathi.
4.2 Multilingual Dictionary
To translate the MWEs and their components,
we follow Salehi and Cook (2013) in using Pan-
lex (Baldwin et al., 2010). This online dictio-
nary is massively multilingual, covering more than
1353 languages. For each MWE dataset (see Sec-
tion 4.3), we translate the MWE and component
words from the source language into each of the
51 languages.
In instances where there is no direct translation
in a given language for a term, we use a pivot lan-
guage to find translation(s) in the target language.
For example, the English noun compound silver
screen has direct translations in only 13 languages
in Panlex, including Vietnamese (ma`n bac) but
not French. There is, however, a translation of
ma`n bac into French (cine?ma), allowing us to
infer an indirect translation between silver screen
and cine?ma. In this way, if there are no direct
translations into a particular target language, we
search for a single-pivot translation via each of our
other target languages, and combine them all to-
gether as our set of translations for the target lan-
guage of interest.
In the case that no translation (direct or indirect)
can be found for a given source language term into
a particular target language, the compositionality
score for that target language is set to the average
across all target languages for which scores can be
calculated for the given term. If no translations are
available for any target language (e.g. the term is
not in Panlex) the compositionality score for each
target language is set to the average score for that
target language across all other source language
terms.
6
http://wp2txt.rubyforge.org/
4.3 Datasets
We evaluate our proposed method over three
datasets (two English, one German), as described
below.
4.3.1 English Noun Compounds (ENC)
Our first dataset is made up of 90 binary English
noun compounds, from the work of Reddy et al.
(2011). Each noun compound was annotated by
multiple annotators using the integer scale 0 (fully
non-compositional) to 5 (fully compositional). A
final compositionality score was then calculated
as the mean of the scores from the annotators.
If we simplistically consider 2.5 as the threshold
for compositionality, the dataset is relatively well
balanced, containing 48% compositional and 52%
non-compositional noun compounds. Following
Reddy et al. (2011), in combining the component-
wise distributional similarities for this dataset, we
weight the first component in Equation 1 higher
than the second (? = 0.7).
4.3.2 English Verb Particle Constructions
(EVPC)
The second dataset contains 160 English verb par-
ticle constructions (VPCs), from the work of Ban-
nard (2006). In this dataset, a verb particle con-
struction consists of a verb (the head) and a prepo-
sitional particle (e.g. hand in, look up or battle on).
For each component word (the verb and parti-
cle, respectively), multiple annotators were asked
whether the VPC entails the component word. In
order to translate the dataset into a regression task,
we calculate the overall compositionality as the
number of annotations of entailment for the verb,
divided by the total number of verb annotations for
that VPC. That is, following Bannard et al. (2003),
we only consider the compositionality of the verb
component in our experiments (and as such ? = 1
in Equation 1).
One area of particular interest with this dataset
will be the robustness of the method to function
words (the particles), both under translation and
in terms of calculating distributional similarity, al-
though the findings of Baldwin (2006) for English
prepositions are at least encouraging in this re-
spect. Additionally, English VPCs can occur in
?split? form (e.g. put your jacket on, from our
earlier example), which will complicate identifi-
cation, and the verb component will often be in-
flected and thus not match under our identification
strategy (for both VPCs and the component verbs).
476
Dataset Language Frequency Family
ENC
Italian 100 Romance
French 99 Romance
German 86 Germanic
Vietnamese 83 Viet-Muong
Portuguese 62 Romance
EVPC
Bulgarian 100 Slavic
Breton 100 Celtic
Occitan 100 Romance
Indonesian 100 Indonesian
Slovenian 100 Slavic
GNC
Polish 100 Slavic
Lithuanian 99 Baltic
Finnish 74 Uralic
Bulgarian 72 Slavic
Czech 40 Slavic
Table 1: The 5 best languages for the ENC, EVPC
and GNC datasets. The language family is based
on Voegelin and Voegelin (1977).
4.3.3 German Noun Compounds (GNC)
Our final dataset is made up of 246 German noun
compounds (von der Heide and Borgwaldt, 2009;
Schulte im Walde et al., 2013). Multiple anno-
tators were asked to rate the compositionality of
each German noun compound on an integer scale
of 1 (non-compositional) to 7 (compositional).
The overall compositionality score is then calcu-
lated as the mean across the annotators. Note that
the component words are provided as part of the
dataset, and that there is no need to perform de-
compounding. Following Schulte im Walde et al.
(2013), we weight the first component higher in
Equation 1 (? = 0.8) when calculating the overall
compositionality score.
This dataset is significant in being non-English,
and also in that German has relatively rich mor-
phology, which we expect to impact on the iden-
tification of both the MWE and the component
words.
5 Results
All experiments are carried out using 10 iterations
of 10-fold cross validation, randomly partitioning
the data independently on each of the 10 iterations,
and averaging across all 100 test partitions in our
presented results. In the case of CS
L2N
and other
methods that make use of it (i.e. CS
L1+L2N
and
CS
all
), the languages selected for a given training
fold are then used to compute the compositionality
scores for the instances in the test set. Figures 3a,
3b and 3c are histograms of the number of times
each N is selected over 100 folds on ENC, EVPC
and GNC datasets, respectively. From the his-
tograms, N = 6, N = 15 and N = 2 are the most
commonly selected settings for ENC, EVPC and
GNC, respectively. That is, multiple languages are
generally used, but more languages are used for
English VPCs than either of the compound noun
datasets. The 5 most-selected languages for ENC,
EVPC and GNC are shown in Table 1. As we
can see, there are some languages which are al-
ways selected for a given dataset, but equally the
commonly-selected languages vary considerably
between datasets.
Further analysis reveals that 32 (63%) target
languages for ENC, 25 (49%) target languages
for EVPC, and only 5 (10%) target languages for
GNC have a correlation of r ? 0.1 with gold-
standard compositionality judgements. On the
other hand, 8 (16%) target languages for ENC, 2
(4%) target languages for EVPC, and no target lan-
guages for GNC have a correlation of r ? ?0.1.
5.1 ENC Results
English noun compounds are relatively easy to
identify in a corpus,
7
because the components oc-
cur sequentially, and the only morphological vari-
ation is in noun number (singular vs. plural). In
other words, the precision for our token match-
ing method is very high, and the recall is also
acceptably high. Partly as a result of the ease
of identification, we get a high correlation of
r = 0.700 for CS
L1
(using only source language
data). Using only target languages (CS
L2N
), the
results drop to r = 0.434, but when we combine
the two (CS
L1+L2N
), the correlation is higher
than using only source or target language data, at
r = 0.725. When we combine all languages us-
ing SVR, the results rise slightly higher again to
r = 0.744, which is slightly above the correla-
tion of the state-of-the-art method of Salehi and
Cook (2013), which combines their method with
the method of Reddy et al. (2011) (CS
string+L1
).
These last two results support our hypothesis that
using translation data can improve the prediction
of compositionality. The results for string similar-
ity on its own (CS
string
, r = 0.644) are slightly
lower than those using only source language dis-
tributional similarity, but when combined with
7
Although see Lapata and Lascarides (2003) for discus-
sion of the difficulty of reliably identifying low-frequency
English noun compounds.
477
0 5 10 15 20 250
5
1015
2025
bestN
Frequency
(a) ENC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(b) EVPC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(c) GNC
Figure 3: Histograms displaying how many times a given N is selected as the best number of languages
over each dataset. For example, according to the GNC chart, there is a peak for N = 2, which shows
that over 100 folds, the best-2 languages achieved the highest correlation on 18 folds.
Method Summary of the Method ENC EVPC GNC
CS
L1
Source language 0.700 0.177 0.141
CS
L2N
Best-N target languages 0.434 0.398 0.113
CS
L1+L2N
Source + best-N target languages 0.725 0.312 0.178
CS
SVR(L1+L2 )
SVR (Source + all 51 target languages) 0.744 0.389 0.085
CS
string
String Similarity (Salehi and Cook, 2013) 0.644 0.385 0.372
CS
string+L1
CS
string
+CS
L1
(Salehi and Cook, 2013) 0.739 0.360 0.353
CS
all
CS
L1
+ CS
L2N
+ CS
string
0.732 0.417 0.364
Table 2: Pearson?s correlation on the ENC, EVPC and GNC datasets
CS
L1+L2N
(i.e. CS
all
) there is a slight rise in cor-
relation (from r = 0.725 to r = 0.732).
5.2 EVPC Results
English VPCs are hard to identify. As discussed
in Section 2, VPC components may not occur se-
quentially, and even when they do occur sequen-
tially, they may not be a VPC. As such, our sim-
plistic identification method has low precision and
recall (hand analysis of 927 identified VPC in-
stances would suggest a precision of around 74%).
There is no question that this is a contributor to
the low correlation for the source language method
(CS
L1
; r = 0.177). When we use target lan-
guages instead of the source language (CS
L2N
),
the correlation jumps substantially to r = 0.398.
When we combine English and the target lan-
guages (CS
L1+L2N
), the results are actually lower
than just using the target languages, because of
the high weight on the target language, which is
not desirable for VPCs, based on the source lan-
guage results. Even for CS
SVR(L1+L2 )
, the re-
sults (r = 0.389) are slightly below the target
language-only results. This suggests that when
predicting the compositionality of MWEs which
are hard to identify in the source language, it may
actually be better to use target languages only. The
results for string similarity (CS
string
: r = 0.385)
are similar to those for CS
L2N
. However, as with
the ENC dataset, when we combine string simi-
larity and distributional similarity (CS
all
), the re-
sults improve, and we achieve the state-of-the-art
for the dataset.
In Table 3, we present classification-based eval-
478
Method Precision Recall F-score (? = 1) Accuracy
Bannard et al. (2003) 60.8 66.6 63.6 60.0
Salehi and Cook (2013) 86.2 71.8 77.4 69.3
CS
all
79.5 89.3 82.0 74.5
Table 3: Results (%) for the binary compositionality prediction task on the EVPC dataset
uation over a subset of EVPC, binarising the com-
positionality judgements in the manner of Bannard
et al. (2003). Our method achieves state-of-the-art
results in terms of overall F-score and accuracy.
5.3 GNC Results
German is a morphologically-rich language, with
marking of number and case on nouns. Given
that we do not perform any lemmatization or other
language-specific preprocessing, we inevitably
achieve low recall for the identification of noun
compound tokens, although the precision should
be nearly 100%. Partly because of the resultant
sparseness in the distributional similarity method,
the results for CS
L1
are low (r = 0.141), al-
though they are lower again when using target lan-
guages (r = 0.113). However, when we combine
the source and target languages (CS
L1+L2N
) the
results improve to r = 0.178. The results for
CS
SVR(L1+L2 )
, on the other hand, are very low
(r = 0.085). Ultimately, simple string similar-
ity achieves the best results for the dataset (r =
0.372), and this result actually drops slightly when
combined with the distributional similarities.
To better understand the reason for the lacklus-
tre results using SVR, we carried out error analysis
and found that, unlike the other two datasets, about
half of the target languages return scores which
correlate negatively with the human judgements.
When we filter these languages from the data, the
score for SVR improves appreciably. For example,
over the best-3 languages overall, we get a corre-
lation score of r = 0.179, which is slightly higher
than CS
L1+L2N
.
We further investigated the reason for getting
very low and sometimes negative correlations with
many of our target languages. We noted that
about 24% of the German noun compounds in
the dataset do not have entries in Panlex. This
contrasts with ENC where only one instance does
not have an entry in Panlex, and EVPC where all
VPCs have translations in at least one language in
Panlex. We experimented with using string sim-
ilarity scores in the case of such missing transla-
tions, as opposed to the strategy described in Sec-
tion 4.2. The results for CS
SVR(L1+L2 )
rose to
r = 0.269, although this is still below the correla-
tion for just using string similarity.
Our results on the GNC dataset using string
similarity are competitive with the state-of-the-art
results (r = 0.45) using a window-based distribu-
tional similarity approach over monolingual Ger-
man data (Schulte im Walde et al., 2013). Note,
however, that their method used part-of-speech in-
formation and lemmatisation, where ours does not,
in keeping with the language-independent philos-
ophy of this research.
6 Conclusion and Future Work
In this study, we proposed a method to predict the
compositionality of MWEs based on monolingual
distributional similarity between the MWE and
each of its component words, under translation
into multiple target languages. We showed that
using translation and multiple target languages en-
hances compositionality modelling, and also that
there is strong complementarity between our ap-
proach and an approach based on string similarity.
In future work, we hope to address the ques-
tion of translation sparseness, as observed for the
GNC dataset. We also plan to experiment with un-
supervised morphological analysis methods to im-
prove identification recall, and explore the impact
of tokenization. Furthermore, we would like to in-
vestigate the optimal number of stop words and
content-bearing words for each language, and to
look into the development of general unsupervised
methods for compositionality prediction.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
479
References
Otavio Acosta, Aline Villavicencio, and Viviane Mor-
eira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Parsing and Generation to the Real
World, pages 101?109, Portland, USA.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37?
40, Beijing, China.
Timothy Baldwin. 2006. Distributional similarity and
preposition semantics. In Patrick Saint-Dizier, ed-
itor, Computational Linguistics Dimensions of Syn-
tax and Semantics of Prepositions, pages 197?210.
Springer, Dordrecht, Netherlands.
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proceedings of the ACL
2003 workshop on Multiword expressions: analysis,
acquisition and treatment-Volume 18, pages 65?72,
Sapporo, Japan.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 242?245, Los
Angeles, USA.
Ga?el Dias. 2003. Multiword unit hybrid extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 41?48, Sapporo, Japan.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):450?466.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of the 11th Conference of
the European Chapter for the Association of Compu-
tational Linguistics (EACL-2003), pages 235?242,
Budapest, Hungary.
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 317?324, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis, ac-
quisition and treatment-Volume 18, pages 73?80,
Sapporo, Japan.
Pavel Pecina. 2008. Lexical Association Measures:
Collocation Extraction. Ph.D. thesis, Faculty of
Mathematics and Physics, Charles University in
Prague, Prague, Czech Republic.
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2013), Seattle, USA.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66, Jeju Island, Ko-
rea.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
480
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly.
2012. Automatic identification of Persian light verb
constructions. In Proceedings of the 13th Inter-
national Conference on Intelligent Text Processing
Computational Linguistics (CICLing-2012), pages
201?210, New Delhi, India.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem. In Proceedings of the 6th
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 100?108,
Hong Kong, China.
Sabine Schulte im Walde, Stefan M?uller, and Stephen
Roller. 2013. Exploring vector space models to
predict the compositionality of German noun-noun
compounds. In Proceedings of the Second Joint
Conference on Lexical and Computational Seman-
tics, Atlanta, USA.
Hinrich Sch?utze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI Publications, Stanford, USA.
Alex J Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222.
Charles Frederick Voegelin and Florence Marie
Voegelin. 1977. Classification and index of the
world?s languages, volume 4. New York: Elsevier.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter, Basis und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages
51?74.
Julie Elizabeth Weeds. 2003. Measures and applica-
tions of lexical distributional similarity. Ph.D. the-
sis, University of Sussex.
481
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530?539,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence
and Topic Model Quality
Jey Han Lau
Dept of Philosophy
King?s College London
jeyhan.lau@gmail.com
David Newman
Google
dnewman@google.com
Timothy Baldwin
Dept of Computing and
Information Systems
The University of Melbourne
tb@ldwin.net
Abstract
Topic models based on latent Dirichlet al-
location and related methods are used in a
range of user-focused tasks including doc-
ument navigation and trend analysis, but
evaluation of the intrinsic quality of the
topic model and topics remains an open
research area. In this work, we explore
the two tasks of automatic evaluation of
single topics and automatic evaluation of
whole topic models, and provide recom-
mendations on the best strategy for per-
forming the two tasks, in addition to pro-
viding an open-source toolkit for topic and
topic model evaluation.
1 Introduction
Topic modelling based on Latent Dirichlet Alloca-
tion (LDA: Blei et al. (2003)) and related methods
is increasingly being used in user-focused tasks, in
contexts such as the evaluation of scientific impact
(McCallum et al., 2006; Hall et al., 2008), trend
analysis (Bolelli et al., 2009; Lau et al., 2012a)
and document search (Wang et al., 2007). The
LDA model is based on the assumption that doc-
ument collections have latent topics, in the form
of a multinomial distribution of words, which is
typically presented to users via its top-N highest-
probability words. In NLP, topic models are gener-
ally used as a means of preprocessing a document
collection, and the topics and per-document topic
allocations are fed into downstream applications
such as document summarisation (Haghighi and
Vanderwende, 2009), novel word sense detection
methods (Lau et al., 2012b) and machine transla-
tion (Zhao and Xing, 2007). In fields such as the
digital humanities, on the other hand, human users
interact directly with the output of topic models. It
is this context of topic modelling for direct human
consumption that we target in this paper.
The topics produced by topic models have a
varying degree of human-interpretability. To il-
lustrate this, we present two topics automatically
learnt from a collection of news articles:
1. ?farmers, farm, food, rice, agriculture?
2. ?stories, undated, receive, scheduled, clients?
The first topic is clearly related to agriculture.
The subject of the second topic, however, is less
clear, and may confuse users if presented to them
as part of a larger topic model. Measuring the
human-interpretability of topics and the overall
topic model is the core topic of this paper.
Various methodologies have been proposed for
measuring the semantic interpretability of topics.
In Chang et al. (2009), the authors proposed an
indirect approach based on word intrusion, where
?intruder words? are randomly injected into topics
and human users are asked to identify the intruder
words. The word intrusion task builds on the as-
sumption that the intruder words are more iden-
tifiable in coherent topics than in incoherent top-
ics, and thus the interpretability of a topic can be
estimated by measuring how readily the intruder
words can be manually identified by annotators.
Since its inception, the method of Chang et
al. (2009) has been used variously as a means
of assessing topic models (Paul and Girju, 2010;
Reisinger et al., 2010; Hall et al., 2012). Despite
its wide acceptance, the method relies on manual
annotation and has never been automated. This is
one of the primary contributions of this work: the
demonstration that we can automate the method of
Chang et al. (2009) at near-human levels of accu-
racy, as a result of which we can perform auto-
matic evaluation of the human-interpretability of
topics, as well as topic models.
There has been prior work to directly estimate
the human-interpretability of topics through au-
tomatic means. For example, Newman et al.
530
(2010) introduced the notion of topic ?coher-
ence?, and proposed an automatic method for es-
timating topic coherence based on pairwise point-
wise mutual information (PMI) between the topic
words. Mimno et al. (2011) similarly introduced
a methodology for computing coherence, replac-
ing PMI with log conditional probability. Musat
et al. (2011) incorporated the WordNet hierarchy
to capture the relevance of topics, and in Aletras
and Stevenson (2013a), the authors proposed the
use of distributional similarity for computing the
pairwise association of the topic words. One ap-
plication of these methods has been to remove in-
coherent topics before generating labels for topics
(Lau et al., 2011; Aletras and Stevenson, 2013b).
Ultimately, all these methodologies, and also
the word intrusion approach, attempt to assess the
same quality: the human-interpretability of top-
ics. The relationship between these methodolo-
gies, however, is poorly understood, and there is
no consensus on what is the best approach for
computing the semantic interpretability of topic
models. This is a second contribution of this pa-
per: we perform a systematic empirical compar-
ison of the different methods and find apprecia-
ble differences between them. We further go on to
propose an improved formulation of Newman et
al. (2010) based on normalised PMI. Finally, we
release a toolkit which implements the topic inter-
pretability measures described in this paper.
2 Related Work
Chang et al. (2009) challenged the conventional
wisdom that held-out likelihood ? often com-
puted as the perplexity of test data or unseen doc-
uments ? is the only way to evaluate topic mod-
els. To measure the human-interpretability of top-
ics, the authors proposed a word intrusion task
and conducted experiments using three topic mod-
els: Latent Dirichlet Allocation (LDA: Blei et al.
(2003)), Probabilistic Latent Semantic Indexing
(PLSI: Hofmann (1999)) and the Correlated Topic
Model (CTM: Blei and Lafferty (2005)). Contrary
to expectation, they found that perplexity corre-
lates negatively with topic interpretability.
In the word intrusion task, each topic is pre-
sented as a list of six words ? the five most proba-
ble topic words and a randomly-selected ?intruder
word?, which has low probability in the topic of
interest, but high probability in other topics ?
and human users are asked to identify the intruder
word that does not belong to the topic in question.
Newman et al. (2010) capture topic inter-
pretability using a more direct approach, by asking
human users to rate topics (represented by their
top-10 topic words) on a 3-point scale based on
how coherent the topic words are (i.e. their ob-
served coherence). They proposed several ways of
automating the estimation of the observed coher-
ence, and ultimately found that a simple method
based on PMI term co-occurrence within a sliding
context window over English Wikipedia produces
the consistently best result, nearing levels of inter-
annotator agreement over topics learnt from two
distinct document collections.
Mimno et al. (2011) proposed a closely-related
method for evaluating semantic coherence, replac-
ing PMI with log conditional probability. Rather
than using Wikipedia for sampling the word co-
occurrence counts, Mimno et al. (2011) used the
topic-modelled documents, and found that their
measure correlates well with human judgements
of observed coherence (where topics were rated
in the same manner as Newman et al. (2010),
based on a 3-point ordinal scale). To incorpo-
rate the evaluation of semantic coherence into the
topic model, the authors proposed to record words
that co-occur together frequently, and update the
counts of all associated words before and after the
sampling of a new topic assignment in the Gibbs
sampler. This variant of topic model was shown to
produce more coherent topics than LDA based on
the log conditional probability coherence measure.
Aletras and Stevenson (2013a) introduced dis-
tributional semantic similarity methods for com-
puting coherence, calculating the distributional
similarity between semantic vectors for the top-N
topic words using a range of distributional similar-
ity measures such as cosine similarity and the Dice
coefficient. To construct the semantic vector space
for the topic words, they used English Wikipedia
as the reference corpus, and collected words that
co-occur in a window of ?5 words. They showed
that their method correlates well with the observed
coherence rated by human judges.
3 Dataset
As one of the primary foci of this paper is the au-
tomation of the intruder word task of Chang et
al. (2009), our primary dataset is that used in the
original paper by Chang et al. (2009), which pro-
vides topics and human annotations for a range of
531
domains and topic model types. In the dataset,
two text collections were used: (1) 10,000 articles
from English Wikipedia (WIKI); and (2) 8,447 arti-
cles from the New York Times dating from 1987 to
2007 (NEWS). For each document collection, top-
ics were generated by three topic modelling meth-
ods: LDA, PLSI and CTM (see Section 2). For
each topic model, three settings of T (the num-
ber of topics) were used: T = 50, T = 100
and T = 150. In total, there were 9 topic mod-
els (3 models ? 3 T ) and 900 topics (3 models ?
(50 + 100 + 150)) for each dataset.
1
For some of topic interpretability estimation
methods, we require a reference corpus to sam-
ple lexical probabilities. We use two reference
corpora: (1) NEWS-FULL, which contains 1.2 mil-
lion New York Times articles from 1994 to 2004
(from the English Gigaword); and (2) WIKI-FULL,
which contains 3.3 million English Wikipedia ar-
ticles (retrieved November 28th 2009).
2
The ratio-
nale for choosing the New York Times and English
Wikipedia as the reference corpora is to ensure do-
main consistency with the word intrusion dataset;
the full collections are used to more robustly esti-
mate lexical probabilities.
4 Human-Interpretability at the Model
Level
In this section, we evaluate measures for estimat-
ing human-interpretability at the model level. That
is, for a measure ? human-judged or automated
? we first aggregate its coherence/interpretability
scores for all topics from a given topic model to
obtain the topic model?s average coherence score.
We then calculate the Pearson correlation coeffi-
cients between the two measures using the topic
models? average coherence scores. In summary,
the correlation is computed over nine sets of top-
ics (3 topic modellers ? 3 settings of T ) for each
of WIKI and NEWS.
4.1 Indirect Approach: Word Intrusion
The word intrusion task measures topic inter-
pretability indirectly, by computing the fraction
of annotators who successfully identify the in-
truder word. A limitation of the word intrusion
1
In the WIKI topics there were corrupted symbols in the
topic words for 24 topics. We removed these topics, reducing
the total number of topics to 876.
2
For both corpora we perform tokenisation and POS tag-
ging using OpenNLP and lemmatisation using Morpha (Min-
nen et al., 2001).
task is that it requires human annotations, there-
fore preventing large-scale evaluation. We begin
by proposing a methodology to fully automate the
word intrusion task.
Lau et al. (2010) proposed a methodology that
learns the most representative or best topic word
that summarises the semantics of the topic. Ob-
serving that the word intrusion task ? the task
of detecting the least representative word ? is
the converse of the best topic word selection task,
we adapt their methodology to automatically iden-
tify the intruder word for the word intrusion task,
based on the knowledge that there is a unique in-
truder word per topic.
The methodology works as follows: given a set
of topics (including intruder words), we compute
the word association features for each of the top-
N topic words of a topic,
3
and combine the fea-
tures in a ranking support vector regression model
(SVM
rank
: Joachims (2006)) to learn the intruder
words. Following Lau et al. (2010), we use three
word association measures:
PMI(w
i
) =
N?1
?
j
log
P (w
i
, w
j
)
P (w
i
)P (w
j
)
CP1(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
j
)
CP2(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
i
)
We additionally experiment with normalised
pointwise mutual information (NPMI: Bouma
(2009)):
NPMI(w
i
) =
N?1
?
j
log
P (w
i
,w
j
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
In the dataset of Chang et al. (2009) (see Sec-
tion 3), each topic was presented to 8 annota-
tors, with small variations in the displayed topic
words (including the intruder word) for each an-
notator. That is, each topic has essentially 8 subtly
different representations. To measure topic inter-
pretability, the authors defined ?model precision?:
the relative success of human annotators at identi-
fying the intruder word, across all representations
of the different topics. The model precision scores
produced by human judges are henceforth referred
to as WI-Human, and the scores produced by our
3
N is the number of topic words displayed to the human
users in the word intrusion task, including the intruder word.
532
Topic Ref. Pearson?s r with WI-Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI
WIKI
WIKI-FULL 0.947 0.936
NEWS-FULL 0.801 0.835
NEWS
NEWS-FULL 0.913 0.831
WIKI-FULL 0.811 0.750
Table 1: Pearson correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the model level.
automated method for the PMI and NPMI vari-
ants as WI-Auto-PMI and WI-Auto-NPMI respec-
tively.
4
The Pearson correlation coefficients between
WI-Human and WI-Auto-PMI/WI-Auto-NPMI at
the model level are presented in Table 1. Note
that our two reference corpora are used to inde-
pendently sample the lexical probabilities for the
word association features.
We see very strong correlation for in-domain
pairings (i.e. WIKI+WIKI-FULL and NEWS+NEWS-
FULL), achieving r > 0.9 in most cases for both
WI-Auto-PMI or WI-Auto-NPMI, demonstrating
the effectiveness of our methodology at automat-
ing the word intrusion task for estimating human-
interpretability at the model level. Overall, WI-
Auto-PMI outperforms WI-Auto-NPMI.
Note that although our proposed methodology
is supervised, as intruder words are synthetically
generated and no annotation is needed for the su-
pervised learning, the whole process of computing
topic coherence via word intrusion is fully auto-
matic, without the need for hand-labelled training
data.
4.2 Direct Approach: Observed Coherence
Newman et al. (2010) defined topic interpretabil-
ity based on a more direct approach, by asking hu-
man judges to rate topics based on the observed
coherence of the top-N topic words, and various
methodologies have since been proposed to auto-
mate the computation of the observed coherence.
In this section, we present all these methods and
compare them.
The word intrusion dataset is not annotated with
human ratings of observed coherence. To cre-
ate gold-standard coherence judgements, we used
Amazon Mechanical Turk:
5
we presented the top-
ics (with intruder words removed) to the Turkers
and asked them to rate the topics using on a 3-point
4
Note that both variants use CP1 and CP2 features, i.e.
WI-Auto-PMI uses PMI+CP1+C2 while WI-Auto-NPMI
uses NPMI+CP1+C2 features.
5
https://www.mturk.com/mturk/
ordinal scale, following Newman et al. (2010). In
total, we collected six to fourteen annotations per
topic (an average of 8.4 annotations per topic).
The observed coherence of a topic is computed
as the arithmetic mean of the annotators? ratings,
once again following Newman et al. (2010). The
human-judged observed topic coherence is hence-
forth referred to as OC-Human.
For the automated methods, we experimented
with the following methods for estimating the
human-interpretability of a topic t:
1. OC-Auto-PMI: Pairwise PMI of top-N
topic words (Newman et al., 2010):
OC-Auto-PMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)P (w
j
)
2. OC-Auto-NPMI: NPMI variant of OC-
Auto-PMI:
OC-Auto-NPMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
,w
i
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
3. OC-Auto-LCP: Pairwise log conditional
probability of top-N topic words (Mimno et
al., 2011):
6
OC-Auto-LCP(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)
4. OC-Auto-DS: Pairwise distributional simi-
larity of the top-N topic words, as described
in Aletras and Stevenson (2013a).
For OC-Auto-PMI, OC-Auto-NPMI and OC-
Auto-LCP, all topics are lemmatised and intruder
words are removed before coherence is com-
puted.
7
In-domain and cross-domain pairings of
6
Although the original method uses the topic-modelled
document collection and document co-occurrence for sam-
pling word counts, for a fairer comparison we use log condi-
tional probability only as a replacement to the PMI compo-
nent of the coherence computation (i.e. words are still sam-
pled using a reference corpus and a sliding window). For ad-
ditional evidence that the original method performs at a sub-
par level, see Lau et al. (2013) and Aletras and Stevenson
(2013a).
7
We once again use Morpha to do the lemmatisation, and
determine POS via the majority POS for a given word, aggre-
gated over all its occurrences in English Wikipedia.
533
Topic Ref. Pearson?s r with OC-Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL 0.490 0.903 0.959
0.859
NEWS-FULL 0.696 0.844 0.913
NEWS
NEWS-FULL 0.965 0.979 0.887
0.941
WIKI-FULL 0.931 0.964 0.872
Table 2: Pearson correlation of OC-Human and the automated methods ? OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS ? at the model level.
the topic domain and reference corpus are experi-
mented with for these measures.
For OC-Auto-DS, all topics are lemmatised, in-
truder words are removed and English Wikipedia
is used to generate the vector space for the topic
words. The size of the context window is set to
?5 word (i.e. 5 words to either side of the tar-
get word). We use PMI to weight the vectors,
cosine similarity for measuring the distributional
similarity between the top-N topic words, and the
?Topic Word Space? approach to reduce the di-
mensionality of the vector space. A complete de-
scription of the parameters can be found in Aletras
and Stevenson (2013a). Note that cross-domain
pairings of the topic domain and reference corpus
are not tested: in line with the original paper, we
use only English Wikipedia to generate the vector
space before distributional similarity.
We present the Pearson correlation coefficient
of OC-Human and the four automated methods at
the model level in Table 2. For OC-Auto-NPMI,
OC-Auto-LCP and OC-Auto-DS, we see that they
correlate strongly with the human-judged coher-
ence. Overall, OC-Auto-NPMI has the best per-
formance among the methods, and in-domain pair-
ings generally produce the best results for OC-
Auto-NPMI and OC-Auto-LCP. The results are
comparable to those for the automated intruder
word detection method in Section 4.1.
The non-normalised variant OC-Auto-PMI cor-
relates well for NEWS but performs poorly for WIKI,
producing a correlation of only 0.490 for the in-
domain pairing. We revisit this in Section 6, and
provide a qualitative analysis to explain the dis-
crepancy in results between OC-Auto-PMI and
OC-Auto-NPMI.
4.3 Word Intrusion vs. Observed Coherence
In the previous sections, we showed for both the
direct and indirect approaches that the automated
methods correlate strongly with the manually-
annotated human-interpretability of topics at the
model level (with the exception of OC-Auto-PMI).
One question that remains unanswered, however,
is whether word intrusion measures topic inter-
pretability differently to observed coherence. This
is the focus of this section.
From the results in Table 3 for the intruder
word model vs. observed coherence, we see a
strong correlation between WI-Human and OC-
Human. This observation is insightful: it shows
that the topic interpretability estimated by the two
approaches is almost identical at the model level.
Between WI-Human and the observed coher-
ence methods automated methods, overall we see
a strong correlation for the OC-Auto-NPMI, OC-
Auto-LCP and OC-Auto-DS methods. OC-Auto-
PMI once again performs poorly over WIKI, but
this is unsurprising given its previous results (i.e.
its poor correlation with OC-Human). In-domain
pairings tend to perform better, and the per-
formance of OC-Auto-NPMI, OC-Auto-LCP and
OC-Auto-DS is comparable, with no one clearly
best method.
5 Human-Interpretability at the Topic
Level
In this section, we evaluate the various methods
at the topic level. We group together all topics
for each dataset (without distinguishing the topic
models that produce them) and calculate the cor-
relation of one measure against another. That is,
the correlation coefficient is computed for 900 top-
ics/data points in the case of each of WIKI and
NEWS.
5.1 Indirect Approach: Word Intrusion
In Section 4.1, we proposed a novel methodol-
ogy to automate the word intrusion task (WI-Auto-
PMI and WI-Auto-NPMI). We now evaluate its
performance at the topic level, and present its
correlation with the human gold standard (WI-
Human) in Table 4.
The correlation of WI-Human and WI-Auto-
PMI/WI-Auto-NPMI at the topic level is consid-
erably worse, compared to its results at the model
534
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.900
0.638 0.927 0.911
0.907
NEWS-FULL 0.614 0.757 0.821
NEWS
NEWS-FULL
0.915
0.865 0.866 0.867
0.925
WIKI-FULL 0.838 0.874 0.893
Table 3: Word intrusion vs. observed coherence: Pearson correlation coefficient at the model level.
Topic Ref. Pearson?s r with WI-Human Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI Agreement
WIKI
WIKI-FULL 0.554 0.573
0.735
NEWS-FULL 0.622 0.592
NEWS
NEWS-FULL 0.602 0.612
0.770
WIKI-FULL 0.638 0.648
Table 4: Pearson correlation coefficient of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topic
level.
level (Table 1). The performance between WI-
Auto-PMI and WI-Auto-NPMI is not very differ-
ent, and the cross-domain pairing slightly outper-
forms the in-domain pairing.
To better understand the difficulty of the task,
we compute the agreement between human anno-
tators by calculating the Pearson correlation co-
efficient of model precisions produced by ran-
domised sub-group pairs in the topics.
8
That is, for
each topic, we randomly split the annotations into
two sub-groups, and compute the Pearson correla-
tion coefficient of the model precisions produced
by the first sub-group and that of the second sub-
group.
The original dataset has 8 annotations per topic.
Splitting the annotations into two sub-groups re-
duces the number of annotations to 4 per group,
which is not ideal for computing model precision.
We thus chose to expand the number of annota-
tions by sampling 300 random topics from each
domain (for a total of 600 topics) and following
the same process as Chang et al. (2009) to get in-
truder word annotations using Amazon Mechani-
cal Turk. On average, we obtained 11.7 additional
annotations per topic for these 600 topics. The hu-
man agreement scores (i.e. the Pearson correlation
coefficient of randomised sub-group pairs) for the
sampled 600 topics are presented in the last col-
umn of Table 4.
The sub-group correlation is around r = 0.75
for the topics from both datasets. As such, esti-
mating topic interpretability at the topic level is a
much harder task than model-level evaluation. Our
automated methods perform at a highly credible
8
To counter for the fact that annotators labelled varying
numbers of topics.
r = 0.6, but there is certainly room for improve-
ment. Note that the correlation values reported in
Newman et al. (2010) are markedly higher than
ours, as they evaluated based on Spearman rank
correlation, which isn?t attuned to the relative dif-
ferences in coherence values and returns higher
values for the task.
5.2 Direct Approach: Observed Coherence
We repeat the experiments of observed coherence
in Section 4.2, and evaluate the correlation of
the automated methods (OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS) on the
human gold standard (OC-Human) at the topic
level. Results are summarised in Table 5.
OC-Auto-PMI performs poorly at the topic
level in the WIKI domain, similar to what was
seen at the model level in Section 4.2. Over-
all, both OC-Auto-NPMI and OC-Auto-DS are the
most consistent methods. OC-Auto-LCP performs
markedly worse than these two methods.
To get a better understanding of how well hu-
man annotators perform at the task, we compute
the one-vs-rest Pearson correlation coefficient us-
ing the gold standard annotations. That is, for
each topic, we single out each rating/annotation
and compare it to the average of all other rat-
ings/annotations. The one-vs-rest correlation re-
sult is displayed in the last column (titled ?Hu-
man Agreement?) in Table 5. The best auto-
mated methods surpass the single-annotator per-
formance, indicating that they are able to per-
form the task as well as human annotators (unlike
the topic-level results for the word intrusion task
where humans were markedly better at the task
than the automated methods).
535
Topic Ref. Pearson?s r with OC-Human Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS Agreement
WIKI
WIKI-FULL 0.533 0.638 0.579
0.682 0.624
NEWS-FULL 0.582 0.667 0.496
NEWS
NEWS-FULL 0.719 0.741 0.471
0.682 0.634
WIKI-FULL 0.671 0.722 0.452
Table 5: Pearson correlation of OC-Human and the automated methods at the topic level.
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.665
0.472 0.557 0.547
0.639
NEWS-FULL 0.504 0.571 0.455
NEWS
NEWS-FULL
0.641
0.629 0.634 0.407
0.649
WIKI-FULL 0.604 0.633 0.390
Table 6: Word intrusion vs. observed coherence: pearson correlation results at the topic level.
5.3 Word Intrusion vs. Observed Coherence
In this section, we bring together the indirect ap-
proach of word intrusion and the direct approach
of observed coherence, and evaluate them against
each other at the topic level. Results are sum-
marised in Table 6.
We see that the correlation between the human
ratings of intruder words and observed coherence
is only modest, implying that there are topic-level
differences in the output of the two approaches. In
Section 6, we provide a qualitative analysis and
explanation as to what constitutes the differences
between the approaches.
For the automated methods, OC-Auto-DS has
the best performance, with OC-Auto-NPMI per-
forming relatively well (in particularly in the NEWS
domain).
6 Discussion
Normalised PMI (NPMI) was first introduced by
Bouma (2009) as a means of reducing the bias for
PMI towards words of lower frequency, in addition
to providing a standardised range of [?1, 1] for the
calculated values.
We introduced NPMI to the automated meth-
ods of word intrusion (WI-Auto-NPMI) and ob-
served coherence (OC-Auto-NPMI) to explore its
suitability for the task. For the latter, we saw
that NPMI achieves markedly higher correlation
than OC-Human (in particular, at the model level).
To better understand the impact of normalisation,
we inspected a list of WIKI topics that have simi-
lar scores for OC-Human and OC-Auto-NPMI but
very different OC-Auto-PMI scores. A sample of
these topics is presented in Table 7. WIKI-FULL
is used as the reference corpus for computing the
scores. Note that the presented OC-Auto-NPMI*
and OC-Auto-PMI* scores are post-normalised to
the range [0, 1] for ease of interpretation. To give
a sense of how readily these topic words occur in
the reference corpus, we additionally display the
frequency of the first topic word in the reference
corpus (last column).
All topics presented have an OC-Human score
of 3.0 (i.e. these topics are rated as being very co-
herent by human judges) and similar OC-Auto-
NPMI values. Their OC-Auto-PMI scores, how-
ever, are very different between the top-3 and
bottom-3 topics. The bias of PMI towards lower
frequency words is clear: topic words that occur
frequently in the corpus receive a lower OC-Auto-
PMI score compared to those that occur less fre-
quently, even though the human-judged observed
coherence is the same. OC-Auto-NPMI on the
other hand, correctly estimates the coherence.
We observed, however, that the impact of nor-
malising PMI is less in the word intrusion task.
One possible explanation is that for the automated
methods WI-Auto-PMI and WI-Auto-NPMI, the
PMI/NPMI scores are used indirectly as a feature
to a machine learning framework, and the bias
could be reduced/compensated by other features.
On the subject of the difference between ob-
served coherence and word intrusion in estimat-
ing topic interpretability, we observed that WI-
Human and OC-Human correlate only moderately
(r ? 0.6) at the topic level (Table 6). To better
understand this effect, we manually analysed top-
ics that have differing WI-Human and OC-Human
scores. A sample of topics with high divergence
in estimated coherence score is given in Table 8.
As before, the presented the OC-Human* and WI-
536
Topic
OC- OC- OC- Word
Human Auto-NPMI* Auto-PMI* Count
cell hormone insulin muscle receptor 3.0 0.59 0.61 #(cell) = 1.1M
electron laser magnetic voltage wavelength 3.0 0.52 0.54 #(electron) = 0.3M
magnetic neutrino particle quantum universe 3.0 0.55 0.55 #(magnetic) = 0.4M
album band music release song 3.0 0.56 0.37 #(album) = 12.5M
college education school student university 3.0 0.57 0.38 #(college) = 9.8M
city county district population town 3.0 0.52 0.34 #(city) = 22.0M
Table 7: A list of WIKI topics to illustrate the impact of NPMI.
Topic # Topic OC-Human* WI-Human*
1 business company corporation cluster loch shareholder 0.94 0.25
2 song actor clown play role theatre 1.00 0.50
3 census ethnic female male population village 0.92 0.25
4 composer singer jazz music opera piano 1.00 0.63
5 choice count give i.e. simply unionist 0.14 1.00
6 digital clown friend love mother wife 0.17 1.00
Table 8: A list of WIKI topics to illustrate the difference between observed coherence and word intrusion.
Boxes denote human chosen intruder words, and boldface denotes true intruder words.
Human* scores in the table are post-normalised to
the range [0, 1] for ease of comparison.
In general, there are two reasons for topics to
have high OC-Human and low WI-Human scores.
First, if a topic has an outlier word that is mildly
related to the topic, users tend to choose this word
as the intruder word in the word intrusion task,
yielding a low WI-Human score. If they are asked
to rate the observed coherence, however, the single
outlier word often does not affect its overall coher-
ence, resulting in a high OC-Human score. This is
observed in topics 1 and 2 in Table 8, where loch
and clown are chosen by annotators in the word in-
trusion task, as they detract from the semantics of
the topic. This results in low WI-Human scores,
but high observed coherence scores (OC-Human).
The second reason is the random selection of
intruder words related to the original topic. We
see this in topics 3 and 4, where related intruder
words (village and singer) were selected.
For topics with low OC-Human and high WI-
Human scores, the true intruder words are often
very different to the domain/focus of other topic
words. As such, annotators are consistently able
to single them out to yield high WI-Human scores,
even though the topic as a whole is not coherent.
Topics 5 and 6 in Table 8 exhibit this.
All topic evaluation measures described in this
paper are implemented in an open-source toolkit.
9
9
https://github.com/jhlau/topic_
interpretability
7 Conclusion
In this paper, we examined various methodologies
that estimate the semantic interpretability of top-
ics, at two levels: the model level and the topic
level. We looked first at the word intrusion task
proposed by Chang et al. (2009), and proposed
a method that fully automates the task. Next we
turned to observed coherence, a more direct ap-
proach to estimate topic interpretability. At the
model level, results were very positive for both the
word intrusion and observed coherence methods.
At the topic level, however, the results were more
mixed. For observed coherence, our best methods
(OC-Auto-NPMI and OC-Auto-DS) were able to
emulate human performance. For word intrusion,
the automated methods were slightly below human
performance, with some room for improvement.
We finally observed that there are systematic dif-
ferences in the topic-level scores derived from the
two task formulations.
Acknowledgements
This work was supported in part by the Australian
Research Council, and for author JHL, also partly
funded by grant ES/J022969/1 from the Economic
and Social Research Council of the UK. The au-
thors acknowledge the generosity of Nikos Ale-
tras and Mark Stevenson in providing their code
for OC-Auto-DS, and Jordan Boyd-Graber in pro-
viding the data used in Chang et al. (2009).
537
References
N. Aletras and M. Stevenson. 2013a. Evaluating
topic coherence using distributional semantics. In
Proceedings of the Tenth International Workshop on
Computational Semantics (IWCS-10), pages 13?22,
Potsdam, Germany.
N. Aletras and M. Stevenson. 2013b. Representing
topics using images. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
158?167, Atlanta, USA.
D. Blei and J. Lafferty. 2005. Correlated topic mod-
els. In Advances in Neural Information Processing
Systems 17 (NIPS-05), pages 147?154, Vancouver,
Canada.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning
Research, 3:993?1022.
L. Bolelli, S?. Ertekin, and C.L. Giles. 2009. Topic
and trend detection in text collections using Latent
Dirichlet Allocation. In Proceedings of ECIR 2009,
pages 776?780, Toulouse, France.
G. Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, pages 31?40,
Potsdam, Germany.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Advances in Neural In-
formation Processing Systems 21 (NIPS-09), pages
288?296, Vancouver, Canada.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics ? Human
Language Technologies 2009 (NAACL HLT 2009),
pages 362?370.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Study-
ing the history of ideas using topic models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 363?371, Honolulu, USA.
M. Hall, P. Clough, and M. Stevenson. 2012. Evalu-
ating the use of clustering for automatically organ-
ising digital library collections. In Proceedings of
the Second International Conference on Theory and
Practice of Digital Libraries, pages 323?334, Pa-
phos, Cyprus.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of 22nd International ACM-
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR?99), pages 50?57,
Berkeley, USA.
T. Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), Philadelphia, USA.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin.
2010. Best topic word selection for topic labelling.
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
Posters Volume, pages 605?613, Beijing, China.
J.H. Lau, K. Grieser, D. Newman, and T. Baldwin.
2011. Automatic labelling of topic models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT 2011), pages 1536?
1545, Portland, USA.
J.H. Lau, N. Collier, and T. Baldwin. 2012a. On-
line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1519?
1534, Mumbai, India.
J.H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012b. Word sense induction for novel
sense detection. In Proceedings of the 13th Con-
ference of the EACL (EACL 2012), pages 591?601,
Avignon, France.
J.H. Lau, T. Baldwin, and D. Newman. 2013. On
collocations and topic models. ACM Transactions
on Speech and Language Processing, 10(3):10:1?
10:14.
A McCallum, G.S. Mann, and D Mimno. 2006. Bib-
liometric impact measures leveraging topic analysis.
In Proceedings of the 6th ACM/IEEE-CS Joint Con-
ference on Digital Libraries 2006 (JCDL?06), pages
65?74, Chapel Hill, USA.
D. Mimno, H. Wallach, E. Talley, M. Leenders, and
A. McCallum. 2011. Optimizing semantic coher-
ence in topic models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2011), pages 262?272,
Edinburgh, UK.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
C. Musat, J. Velcin, S. Trausan-Matu, and M.A. Rizoiu.
2011. Improving topic evaluation using concep-
tual knowledge. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-2011), pages 1866?1871, Barcelona, Spain.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
538
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics.
In Proceedings of the 24th Annual Conference on
Artificial Intelligence (AAAI-10), Atlanta, USA.
J. Reisinger, A. Waters, B. Silverthorn, and R.J.
Mooney. 2010. Spherical topic models. In Proceed-
ings of the 27th International Conference on Ma-
chine Learning (ICML 2010), pages 903?910, Haifa,
Israel.
X. Wang, A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discovery, with an ap-
plication to information retrieval. In Proceedings
of the Seventh IEEE International Conference on
Data Mining (ICDM 2007), pages 697?702, Omaha,
USA.
B. Zhao and E.P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In Advances in Neural Information Processing
Systems (NIPS 2007), pages 1689?1696, Vancouver,
Canada.
539
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 215?220,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Sense per Tweeter ... and Other Lexical Semantic Tales of Twitter
Spandana Gella, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
sgella@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In recent years, microblogs such as Twit-
ter have emerged as a new communication
channel. Twitter in particular has become
the target of a myriad of content-based
applications including trend analysis and
event detection, but there has been little
fundamental work on the analysis of word
usage patterns in this text type. In this
paper ? inspired by the one-sense-per-
discourse heuristic of Gale et al. (1992)
? we investigate user-level sense distri-
butions, and detect strong support for ?one
sense per tweeter?. As part of this, we con-
struct a novel sense-tagged lexical sample
dataset based on Twitter and a web corpus.
1 Introduction
Social media applications such as Twitter enable
users from all over the world to create and share
web content spontaneously. The resulting user-
generated content has been identified as having
potential in a myriad of applications including
real-time event detection (Petrovi?c et al., 2010),
trend analysis (Lau et al., 2012) and natural dis-
aster response co-ordination (Earle et al., 2010).
However, the dynamism and conversational na-
ture of the text contained in social media can
cause problems for traditional NLP approaches
such as parsing (Baldwin et al., 2013), mean-
ing that most content-based approaches use sim-
ple keyword search or a bag-of-words representa-
tion of the text. This paper is a first step towards
full lexical semantic analysis of social media text,
in investigating the sense distribution of a range
of polysemous words in Twitter and a general-
purpose web corpus.
The primary finding of this paper is that there
are strong user-level lexical semantic priors in
Twitter, equivalent in strength to document-level
lexical semantic priors, popularly termed the ?one
sense per discourse? heuristic (Gale et al., 1992).
This has potential implications for future applica-
tions over Twitter which attempt to move beyond a
simple string-based meaning representation to ex-
plicit lexical semantic analysis.
2 Related Work
The traditional approach to the analysis of word-
level lexical semantics is via word sense dis-
ambiguation (WSD), where usages of a given
word are mapped onto discrete ?senses? in a pre-
existing sense inventory (Navigli, 2009). The most
popular sense inventory used in WSD research has
been WordNet (Fellbaum, 1998), although its fine-
grained sense distinctions have proven to be diffi-
cult to make for human annotators and WSD sys-
tems alike. This has resulted in a move towards
more coarse-grained sense inventories (Palmer et
al., 2004; Hovy et al., 2006; Navigli et al., 2007),
or alternatively away from pre-existing sense in-
ventories altogether, towards joint word sense in-
duction (WSI) and disambiguation (Navigli and
Vannella, 2013; Jurgens and Klapaftis, 2013).
Two heuristics that have proven highly powerful
in WSD and WSI research are: (1) first sense tag-
ging, and (2) one sense per discourse. First sense
tagging is based on the observation that sense dis-
tributions tend to be Zipfian, such that if the pre-
dominant or ?first? sense can be identified, simply
tagging all occurrences of a given word with this
sense can achieve high WSD accuracy (McCarthy
et al., 2007). Unsurprisingly, there are significant
differences in sense distributions across domains
(cf. cloud in the COMPUTING and METEOROLOG-
ICAL domains), motivating the need for unsuper-
vised first sense learning over domain-specific cor-
pora (Koeling et al., 2005).
One sense per discourse is the observation that
a given word will often occur with a single sense
across multiple usages in a single document (Gale
215
et al., 1992). Gale et al. established the heuristic
on the basis of 9 ambiguous words using a coarse-
grained sense inventory, finding that the probabil-
ity of a given pair of usages of a word taken from a
given document having the same sense was 94%.
However, Krovetz (1998) found that for a fine-
grained sense inventory, only 67% of words exhib-
ited the single-sense-per-discourse property for all
documents in a corpus.
A radically different view on WSD is word us-
age similarity, whereby two usages of a given
word are rated on a continuous scale for similar-
ity, in isolation of any sense inventory (Erk et al.,
2009). Gella et al. (2013) constructed a word us-
age similarity dataset for Twitter messages, and
developed a topic modelling approach to the task,
building on the work of Lui et al. (2012). To the
best of our knowledge, this has been the only at-
tempt to carry out explicit word-level lexical se-
mantic analysis of Twitter text.
3 Dataset Construction
In order to study sense distributions of words in
Twitter, we need a sense inventory to annotate
against, and also a set of Twitter messages to an-
notate. Further, as a point of comparison for the
sense distributions in Twitter, we require a second
corpus; here we use the ukWaC (Ferraresi et al.,
2008), a corpus built from web documents.
For the sense inventory, we chose the Macmil-
lan English Dictionary Online
1
(MACMILLAN,
hereafter), on the basis of: (1) its coarse-grained
general-purpose sense distinctions, and (2) its reg-
ular update cycle (i.e. it contains many recently-
emerged senses). These criteria are important
in terms of inter-annotator agreement (especially
as we crowdsourced the sense annotation, as de-
scribed below) and also sense coverage. The
other obvious candidate sense inventory which po-
tentially satisfied these criteria was ONTONOTES
(Hovy et al., 2006), but a preliminary sense-
tagging exercise indicated that MACMILLAN bet-
ter captured Twitter-specific usages.
Rather than annotating all words, we opted for
a lexical sample of 20 polysemous nouns, as listed
in Table 1. Our target nouns were selected to span
the high- to mid-frequency range in both Twitter
and the web corpus, and have at least 3 MACMIL-
LAN senses. The average sense ambiguity is 5.5.
1
http://www.macmillandictionary.com
band bar case charge deal
degree field form function issue
job light match panel paper
position post rule sign track
Table 1: The 20 target nouns used in this research
3.1 Data Sampling
We sampled tweets from a crawl made using the
Twitter Streaming API from January 3, 2012 to
February 29, 2012. The web corpus was built from
ukWaC (Ferraresi et al., 2008), which was based
on a crawl of the .uk domain from 2007. In con-
trast to ukWaC, the tweets are not restricted to doc-
uments from any particular country.
For both corpora, we first selected only the
English documents using langid.py, an off-the-
shelf language identification tool (Lui and Bald-
win, 2012). We next identified documents which
contained nominal usages of the target words,
based on the POS tags supplied with the corpus
in the case of ukWaC, and the output of the CMU
ARK Twitter POS tagger v2.0 (Owoputi et al.,
2012) in the case of Twitter.
For Twitter, we are interested in not just the
overall lexical distribution of each target noun,
but also per-user lexical distributions. As such,
we construct two Twitter-based datasets: (1)
TWITTER
RAND
, a random sample of 100 usages of
each target noun; and (2) TWITTER
USER
, 5 usages
of each target noun from each member of a ran-
dom sample of 20 Twitter users. Naively select-
ing users for TWITTER
USER
without filtering re-
sulted in a preponderance of messages from ac-
counts that were clearly bots, e.g. from commer-
cial sites with a single post per item advertised for
sale, with artificially-skewed sense distributions.
In order to obtain a more natural set of messages
from ?real? people, we introduced a number of
user-level filters, including removing users who
posted the same message with different user men-
tions or hashtags, and users who used the target
nouns more than 50 times over a 2-week period.
From the remaining users, we randomly selected
20 users per target noun, resulting in 20 nouns ?
20 users ? 5 messages = 2000 messages.
For ukWaC, we similarly constructed two
datasets: (1) UKWAC
RAND
, a random sample
of 100 usages of each target noun; and (2)
UKWAC
DOC
, 5 usages of each target noun from 20
documents which contained that noun in at least
216
Figure 1: Screenshot of a sense annotation HIT for position
5 sentences. 5 such sentences were selected for
annotation, resulting in a total of 20 nouns ? 20
documents ? 5 sentences = 2000 sentences.
3.2 Annotation Settings
We sense-tagged each of the four datasets using
Amazon Mechanical Turk (AMT). Each Human
Intelligence Task (HIT) comprised 5 occurrences
of a given target noun, with the target noun high-
lighted in each. Sense definitions and an exam-
ple sentence (where available) were provided from
MACMILLAN. Turkers were free to select multi-
ple sense labels where applicable, in line with best
practice in sense labelling (Mihalcea et al., 2004).
We also provided an ?Other? sense option, in cases
where none of the MACMILLAN senses were ap-
plicable to the current usage of the target noun. A
screenshot of the annotation interface for a single
usage is provided in Figure 1.
Of the five sentences in each HIT, one was a
heldout example sentence for one of the senses of
the target noun, taken from MACMILLAN. This
gold-standard example was used exclusively for
quality assurance purposes, and used to filter the
annotations as follows:
1. Accept all HITs from Turkers whose gold-
standard tagging accuracy was ? 80%;
2. Reject all HITs from Turkers whose gold-
standard tagging accuracy was ? 20%;
3. Otherwise, accept single HITs with correct
gold-standard sense tags, or at least 2/4 (non-
gold-standard) annotations in common with
Turkers who correctly annotated the gold-
standard usage; reject any other HITs.
This style of quality assurance has been shown
to be successful for sense tagging tasks on AMT
(Bentivogli et al., 2011; Vuurens et al., 2011), and
resulted in us accepting around 95% of HITs.
In total, the annotation was made up of 500
HITs (= 2000/4 usages per HIT) for each of the
four datasets, each of which was annotated by
5 Turkers. Our analysis of sense distribution is
based on only those HITs which were accepted in
accordance with the above methodology, exclud-
ing the gold-standard items. We arrive at a single
sense label per usage by unweighted voting across
the annotations, allowing multiple votes from a
single Turker in the case of multiple sense annota-
tions. In this, the ?Other? sense label is considered
as a discrete sense label.
Relative to the majority sense, inter-annotator
agreement post-filtering was respectably high in
terms of Fleiss? kappa at ? = 0.64 for both
UKWAC
RAND
and UKWAC
DOC
. For TWITTER
USER
,
the agreement was actually higher at ? = 0.71, but
for TWITTER
RAND
it was much weaker, ? = 0.47.
All four datasets have been released for pub-
lic use: http://www.csse.unimelb.edu.au/
~
tim/etc/twitter_sense.tgz.
4 Analysis
In TWITTER
USER
, the proportion of users who used
a target noun with one sense across all 5 usages
ranged from 7/20 for form to 20/20 for degree, at
an average of 65%. That is, for 65% of users, a
given noun (with average polysemy = 5.5 senses)
is used with the same sense across 5 separate mes-
sages. For UKWAC
DOC
the proportion of docu-
ments with a single sense of a given target noun
217
Partition Agreement (%)
Gale et al. (1992) document 94.4
TWITTER
USER
user 95.4
TWITTER
USER
? 62.9
TWITTER
RAND
? 55.1
UKWAC
DOC
document 94.2
UKWAC
DOC
? 65.9
UKWAC
RAND
? 60.2
Table 2: Pairwise agreement for each dataset,
based on different partitions of the data (??? indi-
cates no partitioning, and exhaustive comparison)
across all usages ranged from 1/20 for case to
20/20 for band, at an average of 63%. As such,
the one sense per tweeter heuristic is at least as
strong as the one sense per discourse heuristic in
UKWAC
DOC
.
Looking back to the original work of Gale et
al. (1992), it is important to realise that their re-
ported agreement of 94% was calculated pairwise
between usages in a given document. When we
recalculate the agreement in TWITTER
USER
and
UKWAC
DOC
using this methodology, as detailed
in Table 2 (calculating pairwise agreement within
partitions of the data based on ?user? and ?docu-
ment?, respectively), we see that the numbers for
our datasets are very close to those of Gale et al.
on the basis of more than twice as many nouns,
and many more instances per noun. Moreover, the
one sense per tweeter trend again appears to be
slightly stronger than the one sense per discourse
heuristic in UKWAC
DOC
.
One possible interpretation of these results is
that they are due to a single predominant sense,
common to all users/documents rather than user-
specific predominant senses. To test this hy-
pothesis, we calculate the pairwise agreement for
TWITTER
USER
and UKWAC
DOC
across all anno-
tations (without partitioning on user/document),
and also for TWITTER
RAND
and UKWAC
RAND
.
The results are, once again, presented in Ta-
ble 2 (with partition indicated as ??? for the
respective datasets), and are substantially lower
in all cases (< 66%). This indicates that the
first sense preference varies considerably between
users/documents. Note that the agreement is
slightly lower for TWITTER
RAND
and UKWAC
RAND
simply because of the absence of the biasing effect
for users/documents.
Comparing TWITTER
RAND
and UKWAC
RAND
,
there were marked differences in first sense pref-
erences, with 8/20 of the target nouns having a
different first sense across the two corpora. One
surprising observation was that the sense distri-
butions in UKWAC
RAND
were in general more
skewed than in TWITTER
RAND
, with the entropy of
the sense distribution being lower (= more biased)
in UKWAC
RAND
for 15/20 of the target nouns.
All datasets included instances of ?Other?
senses (i.e. usages which didn?t conform to any
of the MACMILLAN senses), with the highest rel-
ative such occurrence being in TWITTER
RAND
at
12.3%, as compared to 6.6% for UKWAC
RAND
.
Interestingly, the number of such usages in
the user/document-biased datasets was around
half these numbers, at 7.4% and 3.6% for
TWITTER
USER
and UKWAC
DOC
, respectively.
5 Discussion
It is worthwhile speculating why Twitter users
would have such a strong tendency to use a given
word with only one sense. This could arise in
part due to patterns of user behaviour, in a given
Twitter account being used predominantly to com-
ment on a favourite sports team or political events,
and as such is domain-driven. Alternatively, it can
perhaps be explained by the ?reactive? nature of
Twitter, in that posts are often emotive responses
to happenings in a user?s life, and while different
things excite different individuals, a given individ-
ual will tend to be excited by events of similar
kinds. Clearly more research is required to test
these hypotheses.
One highly promising direction for this research
would be to overlay analysis of sense distributions
with analysis of user profiles (e.g. Bergsma et al.
(2013)), and test the impact of geospatial and soci-
olinguistic factors on sense preferences. We would
also like to consider the impact of time on the one
sense per tweeter heuristic, and consider whether
?one sense per Twitter conversation? also holds.
To summarise, we have investigated sense dis-
tributions in Twitter and a general web corpus,
over both a random sample of usages and a sample
of usages from a single user/document. We found
strong evidence for Twitter users to use a given
word with a single sense, and also that individual
first sense preferences differ between users, sug-
gesting that methods for determining first senses
on a per user basis could be valuable for lexical se-
mantic analysis of tweets. Furthermore, we found
that sense distributions in Twitter are overall less
skewed than in a web corpus.
218
References
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Luisa Bentivogli, Marcello Federico, Giovanni
Moretti, and Michael Paul. 2011. Getting expert
quality from the crowd for machine translation
evaluation. Proceedings of the MT Summmit,
13:521?528.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
1010?1019, Atlanta, USA.
Paul Earle, Michelle Guy, Richard Buckmaster, Chris
Ostrum, Scott Horvath, and Amy Vaughan. 2010.
OMG earthquake! can Twitter improve earth-
quake response? Seismological Research Letters,
81(2):246?251.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint conference of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing
(ACL-IJCNLP 2009), pages 10?18, Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Mo-
rocco.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233?237.
Spandana Gella, Paul Cook, and Bo Han. 2013. Unsu-
pervised word usage similarity in social media texts.
In Proceedings of the Second Joint Conference on
Lexical and Computational Semantics (*SEM 2013),
pages 248?253, Atlanta, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Robert Krovetz. 1998. More than one sense per dis-
course. NEC Princeton NJ Labs., Research Memo-
randum.
Jey Han Lau, Nigel Collier, and Timothy Baldwin.
2012. On-line trend analysis with topic models:
#twitter trends detection topic model online. In Pro-
ceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1519?1534, Mumbai, India.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Workshop 2012 (ALTW 2012), pages
33?41, Dunedin, New Zealand.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, pages 25?28, Barcelona,
Spain.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
219
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Sasa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to twitter. In Proceedings of Human Lan-
guage Technologies: The 11th Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL HLT 2010),
pages 181?189, Los Angeles, USA.
Jeroen Vuurens, Arjen P de Vries, and Carsten Eick-
hoff. 2011. How much spam can you take? an anal-
ysis of crowdsourcing results to increase accuracy.
In Proc. ACM SIGIR Workshop on Crowdsourcing
for Information Retrieval (CIR 2011), pages 21?26.
220
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Chart Mining-based Lexical Acquisition with Precision Grammars
Yi Zhang,? Timothy Baldwin,?? Valia Kordoni,? David Martinez? and Jeremy Nicholson??
? DFKI GmbH and Dept of Computational Linguistics, Saarland University, Germany
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
yzhang@coli.uni-sb.de, tb@ldwin.net, kordoni@dfki.de,
{davidm,jeremymn}@csse.unimelb.edu.au
Abstract
In this paper, we present an innovative chart
mining technique for improving parse cover-
age based on partial parse outputs from preci-
sion grammars. The general approach of min-
ing features from partial analyses is applica-
ble to a range of lexical acquisition tasks, and
is particularly suited to domain-specific lexi-
cal tuning and lexical acquisition using low-
coverage grammars. As an illustration of the
functionality of our proposed technique, we
develop a lexical acquisition model for En-
glish verb particle constructions which oper-
ates over unlexicalised features mined from
a partial parsing chart. The proposed tech-
nique is shown to outperform a state-of-the-art
parser over the target task, despite being based
on relatively simplistic features.
1 Introduction
Parsing with precision grammars is increasingly
achieving broad coverage over open-domain texts
for a range of constraint-based frameworks (e.g.,
TAG, LFG, HPSG and CCG), and is being used in
real-world applications including information ex-
traction, question answering, grammar checking and
machine translation (Uszkoreit, 2002; Oepen et al,
2004; Frank et al, 2006; Zhang and Kordoni, 2008;
MacKinlay et al, 2009). In this context, a ?preci-
sion grammar? is a grammar which has been engi-
neered to model grammaticality, and contrasts with
a treebank-induced grammar, for example.
Inevitably, however, such applications demand
complete parsing outputs, based on the assumption
that the text under investigation will be completely
analysable by the grammar. As precision grammars
generally make strong assumptions about complete
lexical coverage and grammaticality of the input,
their utility is limited over noisy or domain-specific
data. This lack of complete coverage can make
parsing with precision grammars less attractive than
parsing with shallower methods.
One technique that has been successfully applied
to improve parser and grammar coverage over a
given corpus is error mining (van Noord, 2004;
de Kok et al, 2009), whereby n-grams with low
?parsability? are gathered from the large-scale out-
put of a parser as an indication of parser or (pre-
cision) grammar errors. However, error mining is
very much oriented towards grammar engineering:
its results are a mixture of different (mistreated) lin-
guistic phenomena together with engineering errors
for the grammar engineer to work through and act
upon. Additionally, it generally does not provide
any insight into the cause of the parser failure, and it
is difficult to identify specific language phenomena
from the output.
In this paper, we instead propose a chart min-
ing technique that works on intermediate parsing re-
sults from a parsing chart. In essence, the method
analyses the validity of different analyses for words
or constructions based on the ?lifetime? and prob-
ability of each within the chart, combining the con-
straints of the grammar with probabilities to evaluate
the plausibility of each.
For purposes of exemplification of the proposed
technique, we apply chart mining to a deep lexical
acquisition (DLA) task, using a maximum entropy-
based prediction model trained over a seed lexicon
and treebank. The experimental set up is the fol-
lowing: given a set of sentences containing puta-
tive instances of English verb particle constructions,
10
extract a list of non-compositional VPCs optionally
with valence information. For comparison, we parse
the same sentence set using a state-of-the-art statisti-
cal parser, and extract the VPCs from the parser out-
put. Our results show that our chart mining method
produces a model which is superior to the treebank
parser.
To our knowledge, the only other work that has
looked at partial parsing results of precision gram-
mars as a means of linguistic error analysis is that of
Kiefer et al (1999) and Zhang et al (2007a), where
partial parsing models were proposed to select a set
of passive edges that together cover the input se-
quence. Compared to these approaches, our pro-
posed chart mining technique is more general and
can be adapted to specific tasks and domains. While
we experiment exclusively with an HPSG grammar
in this paper, it is important to note that the proposed
method can be applied to any grammar formalism
which is compatible with chart parsing, and where it
is possible to describe an unlexicalised lexical entry
for the different categories of lexical item that are to
be extracted (see Section 3.2 for details).
The remainder of the paper is organised as fol-
lows. Section 2 defines the task of VPC extraction.
Section 3 presents the chart mining technique and
the feature extraction process for the VPC extraction
task. Section 4 evaluates the model performance
with comparison to two competitor models over sev-
eral different measures. Section 5 further discusses
the general applicability of chart mining. Finally,
Section 6 concludes the paper.
2 Verb Particle Constructions
The particular construction type we target for DLA
in this paper is English Verb Particle Constructions
(henceforth VPCs). VPCs consist of a head verb
and one or more obligatory particles, in the form
of intransitive prepositions (e.g., hand in), adjec-
tives (e.g., cut short) or verbs (e.g., let go) (Villav-
icencio and Copestake, 2002; Huddleston and Pul-
lum, 2002; Baldwin and Kim, 2009); for the pur-
poses of our dataset, we assume that all particles are
prepositional?by far the most common and produc-
tive of the three types?and further restrict our atten-
tion to single-particle VPCs (i.e., we ignore VPCs
such as get alng together).
One aspect of VPCs that makes them a partic-
ularly challenging target for lexical acquisition is
that the verb and particle can be non-contiguous (for
instance, hand the paper in and battle right on).
This sets them apart from conventional collocations
and terminology (cf., Manning and Schu?tze (1999),
Smadja (1993) and McKeown and Radev (2000))
in that they cannot be captured effectively using n-
grams, due to their variability in the number and type
of words potentially interceding between the verb
and the particle. Also, while conventional colloca-
tions generally take the form of compound nouns
or adjective?noun combinations with relatively sim-
ple syntactic structure, VPCs occur with a range of
valences. Furthermore, VPCs are highly productive
in English and vary in use across domains, making
them a prime target for lexical acquisition (Dehe?,
2002; Baldwin, 2005; Baldwin and Kim, 2009).
In the VPC dataset we use, there is an addi-
tional distinction between compositional and non-
compositional VPCs. With compositional VPCs,
the semantics of the verb and particle both corre-
spond to the semantics of the respective simplex
words, including the possibility of the semantics be-
ing specific to the VPC construction in the case of
particles. For example, battle on would be clas-
sified as compositional, as the semantics of bat-
tle is identical to that for the simplex verb, and
the semantics of on corresponds to the continua-
tive sense of the word as occurs productively in
VPCs (cf., walk/dance/drive/govern/... on). With
non-compositional VPCs, on the other hand, the
semantics of the VPC is somehow removed from
that of the parts. In the dataset we used for eval-
uation, we are interested in extracting exclusively
non-compositional VPCs, as they require lexicalisa-
tion; compositional VPCs can be captured via lexi-
cal rules and are hence not the target of extraction.
English VPCs can occur with a number of va-
lences, with the two most prevalent and productive
valences being the simple transitive (e.g., hand in
the paper) and intransitive (e.g., back off ). For the
purposes of our target task, we focus exclusively on
these two valence types.
Given the above, we define the English VPC ex-
traction task to be the production of triples of the
form ?v, p, s?, where v is a verb lemma, p is a prepo-
sitional particle, and s ? {intrans , trans} is the va-
11
lence; additionally, each triple has to be semantically
non-compositional. The triples are extracted relative
to a set of putative token instances for each of the
intransitive and transitive valences for a given VPC.
That is, a given triple should be classified as positive
if and only if it is associated with at least one non-
compositional token instance in the provided token-
level data.
The dataset used in this research is the one used
in the LREC 2008 Multiword Expression Workshop
Shared Task (Baldwin, 2008).1 In the dataset, there
is a single file for each of 4,090 candidate VPC
triples, containing up to 50 sentences that have the
given VPC taken from the British National Cor-
pus. When the valence of the VPC is ignored,
the dataset contains 440 unique VPCs among 2,898
VPC candidates. In order to be able to fairly com-
pare our method with a state-of-the-art lexicalised
parser trained over the WSJ training sections of the
Penn Treebank, we remove any VPC types from the
test set which are attested in the WSJ training sec-
tions. This removes 696 VPC types from the test
set, and makes the task even more difficult, as the
remaining testing VPC types are generally less fre-
quent ones. At the same time, it unfortunately means
that our results are not directly comparable to those
for the original shared task.2
3 Chart Mining for Parsing with a Large
Precision Grammar
3.1 The Technique
The chart mining technique we use in this paper
is couched in a constituent-based bottom-up chart
parsing paradigm. A parsing chart is a data struc-
ture that records all the (complete or incomplete) in-
termediate parsing results. Every passive edge on
the parsing chart represents a complete local analy-
sis covering a sub-string of the input, while each ac-
tive edge predicts a potential local analysis. In this
view, a full analysis is merely a passive edge that
spans the whole input and satisfies certain root con-
1Downloadable from http://www.csse.unimelb.
edu.au/research/lt/resources/vpc/vpc.tgz.
2In practice, there was only one team who participated in
the original VPC task (Ramisch et al, 2008), who used a vari-
ety of web- and dictionary-based features suited more to high-
frequency instances in high-density languages, so a simplistic
comparison would not have been meaningful.
ditions. The bottom-up chart parser starts with edges
instantiated from lexical entries corresponding to the
input words. The grammar rules are used to incre-
mentally create longer edges from smaller ones until
no more edges can be added to the chart.
Standardly, the parser returns only outputs that
correspond to passive edges in the parsing chart that
span the full input string. For those inputs without a
full-spanning edge, no output is generated, and the
chart becomes the only source of parsing informa-
tion.
A parsing chart takes the form of a hierarchy of
edges. Where only passive edges are concerned,
each non-lexical edge corresponds to exactly one
grammar rule, and is connected with one or more
daughter edge(s), and zero or more parent edge(s).
Therefore, traversing the chart is relatively straight-
forward.
There are two potential challenges for the chart-
mining technique. First, there is potentially a huge
number of parsing edges in the chart. For in-
stance, when parsing with a large precision gram-
mar like the HPSG English Resource Grammar
(ERG, Flickinger (2002)), it is not unusual for a
20-word sentence to receive over 10,000 passive
edges. In order to achieve high efficiency in pars-
ing (as well as generation), ambiguity packing is
usually used to reduce the number of productive
passive edges on the parsing chart (Tomita, 1985).
For constraint-based grammar frameworks like LFG
and HPSG, subsumption-based packing is used to
achieve a higher packing ratio (Oepen and Carroll,
2000), but this might also potentially lead to an in-
consistent packed parse forest that does not unpack
successfully. For chart mining, this means that not
all passive edges are directly accessible from the
chart. Some of them are packed into others, and the
derivatives of the packed edges are not generated.
Because of the ambiguity packing, zero or more
local analyses may exist for each passive edge on
the chart, and the cross-combination of the packed
daughter edges is not guaranteed to be compatible.
As a result, expensive unification operations must be
reapplied during the unpacking phase. Carroll and
Oepen (2005) and Zhang et al (2007b) have pro-
posed efficient k-best unpacking algorithms that can
selectively extract the most probable readings from
the packed parse forest according to a discrimina-
12
tive parse disambiguation model, by minimising the
number of potential unifications. The algorithm can
be applied to unpack any passive edges. Because
of the dynamic programming used in the algorithm
and the hierarchical structure of the edges, the cost
of the unpacking routine is empirically linear in the
number of desired readings, and O(1) when invoked
more than once on the same edge.
The other challenge concerns the selection of in-
formative and representative pieces of knowledge
from the massive sea of partial analyses in the pars-
ing chart. How to effectively extract the indicative
features for a specific language phenomenon is a
very task-specific question, as we will show in the
context of the VPC extraction task in Section 3.2.
However, general strategies can be applied to gener-
ate parse ranking scores on each passive edge. The
most widely used parse ranking model is the log-
linear model (Abney, 1997; Johnson et al, 1999;
Toutanova et al, 2002). When the model does not
use non-local features, the accumulated score on a
sub-tree under a certain (unpacked) passive edge can
be used to approximate the probability of the partial
analysis conditioned on the sub-string within that
span.3
3.2 The Application: Acquiring Features for
VPC Extraction
As stated above, the target task we use to illustrate
the capabilities of our chart mining method is VPC
extraction.
The grammar we apply our chart mining method
to in this paper is the English Resource Grammar
(ERG, Flickinger (2002)), a large-scale precision
HPSG for English. Note, however, that the method
is equally compatible with any grammar or grammar
formalism which is compatible with chart parsing.
The lexicon of the ERG has been semi-
automatically extended with VPCs extracted
by Baldwin (2005). In order to show the effective-
ness of chart mining in discovering ?unknowns?
and remove any lexical probabilities associated
with pre-existing lexical entries, we block the
3To have a consistent ranking model on any sub-analysis,
one would have to retrain the disambiguation model on every
passive edge. In practice, we find this to be intractable. Also,
the approximation based on full-parse ranking model works rea-
sonably well.
lexical entries for the verb in the candidate VPC
by substituting the input token with a DUMMY-V
token, which is coupled with four candidate lexical
entries of type: (1) intransitive simplex verb (v - e),
(2) transitive simplex verb (v np le), (3) intransitive
VPC (v p le), and (4) transitive VPC (v p-np le),
respectively. These four lexical entries represent the
two VPC valences we wish to distinguish between
in the VPC extraction task, and the competing
simplex verb candidates. Based on these lexical
types, the features we extract with chart mining are
summarised in Table 1. The maximal constituent
(MAXCONS) of a lexical entry is defined to be the
passive edge that is an ancestor of the lexical entry
edge that: (i) must span over the particle, and (ii)
has maximal span length. In the case of a tie,
the edge with the highest disambiguation score is
selected as the MAXCONS. If there is no edge found
on the chart that spans over both the verb and the
particle, the MAXCONS is set to be NULL, with a
MAXSPAN of 0, MAXLEVEL of 0 and MAXCRANK
of 4 (see Table 1). The stem of the particle is also
collected as a feature.
One important characteristic of these features is
that they are completely unlexicalised on the verb.
This not only leads to a fair evaluation with the ERG
by excluding the influence from the lexical coverage
of VPCs in the grammar, but it also demonstrates
that complete grammatical coverage over simplex
verbs is not a prerequisite for chart mining.
To illustrate how our method works, we present
the unpacked parsing chart for the candidate VPC
show off and input sentence The boy shows off his
new toys in Figure 1. The non-terminal edges are
marked with their syntactic categories, i.e., HPSG
rules (e.g., subjh for the subject-head-rule, hadj for
the head-adjunct-rule, etc.), and optionally their dis-
ambiguation scores. By traversing upward through
parent edges from the DUMMY-V edge, all features
can be efficiently extracted (see the third column in
Table 1).
It should be noted that none of these features are
used to deterministically dictate the predicted VPC
category. Instead, the acquired features are used as
inputs to a statistical classifier for predicting the type
of the VPC candidate at the token level (in the con-
text of the given sentence). In our experiment, we
used a maximum entropy-based model to do a 3-
13
Feature Description Examples
LE:MAXCONS
A lexical entry together with the maximal constituent
constructed from it
v - le:subjh, v np le:hadj,
v p le:subjh, v p-np le:subj
LE:MAXSPAN
A lexical entry together with the length of the span of
the maximal constituent constructed from the LE
v - le:7, v np le:5, v p le:4,
v p-np le:7
LE:MAXLEVEL
A lexical entry together with the levels of projections
before it reaches its maximal constituent
v - le:2, v np le:1, v p le:2,
v p-np le:3
LE:MAXCRANK
A lexical entry together with the relative disambigua-
tion score ranking of its maximal constituent among
all MaxCons from different LEs
v - le:4, v np le:3, v p le:1,
v p-np le:2
PARTICLE The stem of the particle in the candidate VPC off
Table 1: Chart mining features used for VPC extraction
his new toysoffshows
PREPPRTL
v_?_le
NP1
VP4?hcomp
NP2
VP5?hcomp
PP?hcomp
0 2 3 4 7
DUMMY?V
S1?subjh(.125)
S3?subjh(.875)
VP1?hadj VP3?hcomp
S2?subjh(.925)
VP2?hadj(.325)
v_p?np_lev_np_le v_p_le
the boy
Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG
category classification: non-VPC, transitive VPC,
or intransitive VPC. For the parameter estimation
of the ME model, we use the TADM open source
toolkit (Malouf, 2002). The token-level predictions
are then combined with a simple majority voting to
derive the type-level prediction for the VPC candi-
date. In the case of a tie, the method backs off to
the na??ve baseline model described in Section 4.2,
which relies on the combined probability of the verb
and particle forming a VPC.
We have also experimented with other ways of de-
riving type-level predictions from token-level classi-
fication results. For instance, we trained a separate
classifier that takes the token-level prediction as in-
put in order to determine the type-level VPC predic-
tion. Our results indicate no significant difference
between these methods and the basic majority vot-
ing approach, so we present results exclusively for
this simplistic approach in this paper.
4 Evaluation
4.1 Experiment Setup
To evaluate the proposed chart mining-based VPC
extraction model, we use the dataset from the LREC
2008 Multiword Expression Workshop shared task
(see Section 2). We use this dataset to perform three
distinct DLA tasks, as detailed in Table 2.
The chart mining feature extraction is imple-
mented as an extension to the PET parser (Callmeier,
14
Task Description
GOLD VPC Determine the valence for a verb?preposition combination which is known to occur
as a non-compositional VPC (i.e. known VPC, with unknown valence(s))
FULL Determine whether each verb?preposition combination is a VPC or not, and further
predict its valence(s) (i.e. unknown if VPC, and unknown valence(s))
VPC Determine whether each verb?preposition combination is a VPC or not ignoring va-
lence (i.e. unknown if VPC, and don?t care about valence)
Table 2: Definitions of the three DLA tasks
2001). We use a slightly modified version of the
ERG in our experiments, based on the nov-06 re-
lease. The modifications include 4 newly-added
dummy lexical entries for the verb DUMMY-V and
the corresponding inflectional rules, and a lexical
type prediction model (Zhang and Kordoni, 2006)
trained on the LOGON Treebank (Oepen et al, 2004)
for unknown word handling. The parse disambigua-
tion model we use is also trained on the LOGON
Treebank. Since the parser has no access to any of
the verbs under investigation (due to the DUMMY-
V substitution), those VPC types attested in the
LOGON Treebank do not directly impact on the
model?s performance. The chart mining feature ex-
traction process took over 10 CPU days, and col-
lected a total of 44K events for 4,090 candidate VPC
triples.4 5-fold cross validation is used to train/test
the model. As stated above (Section 2), the VPC
triples attested in the WSJ training sections of the
Penn Treebank are excluded in each testing fold for
comparison with the Charniak parser-based model
(see Section 4.2).
4.2 Baseline and Benchmark
For comparison, we first built a na??ve baseline model
using the combined probabilities of the verb and par-
ticle being part of a VPC. More specifically, P (c|v)
and P (c|p) are the probabilities of a given verb
v and particle p being part of a VPC candidate
of type s ? {intrans , trans , null}, for transitive
4Not all sentences in the dataset are successfully chart-
mined. Due to the complexity of the precision grammar we
use, the parser is unlikely to complete the parsing chart for ex-
tremely long sentences (over 50 words). Moreover, sentences
which do not receive any spanning edge over the verb and the
particle are not considered as an indicative event. Nevertheless,
the coverage of the chart mining is much higher than the full-
parse coverage of the grammar.
VPC, intransitive VPC, and non-VPC, respectively.
P? (s|v, p) = P (s|v) ? P (s|p) is used to approxi-
mate the joint probability of verb-particle (v, p) be-
ing of type s, and the prediction type is chosen ran-
domly based on this probabilistic distribution. Both
P (s|v) and P (s|p) can be estimated from a list of
VPC candidate types. If v is unseen, P (s|v) is set to
be 1|V |
?
vi?V P (s|vi) estimated over all verbs |V |
seen in the list of VPC candidates. The na??ve base-
line performed poorly, mainly because there is not
enough knowledge about the context of use of VPCs.
This also indicates that the task of VPC extraction
is non-trivial, and that context (evidence from sen-
tences in which the VPC putatively occurs) must be
incorporated in order to make more accurate predic-
tions.
As a benchmark VPC extraction system, we use
the Charniak parser (Charniak, 2000). This sta-
tistical parser induces a context-free grammar and
a generative parsing model from a training set of
gold standard parse trees. Traditionally, it has been
trained over the WSJ component of the Penn Tree-
bank, and for this work we decided to take the same
approach and train over sections 1 to 22, and use sec-
tion 23 for parameter-tuning. After parsing, we sim-
ply search for the VPC triples in each token instance
with tgrep2,5 and decide on the classification of
the candidate by majority voting over all instances,
breaking ties randomly.
5Noting that the Penn POS tagset captures essentially the
compositional vs. non-compositional VPC distinction required
in the extraction task, through the use of the RP (prepositional
particle, for non-compositional VPCs) and RB (adverb, for com-
positional VPCs) tags.
15
4.3 Results
The results of our experiments are summarised in
Table 3. For the na??ve baseline and the chart mining-
based models, the results are averaged over 5-fold
cross validation.
We evaluate the methods in the form of the three
tasks described in Table 2. Formally, GOLD VPC
equates to extracting ?v, p, s? tuples from the sub-
set of gold-standard ?v, p? tuples; FULL equates to
extracting ?v, p, s? tuples for all VPC candidates;
and VPC equates to extracting ?v, p? tuples (ignor-
ing valence) over all VPC candidates. In each case,
we present the precision (P), recall (R) and F-score
(? = 1: F). For multi-category classifications (i.e.
the two tasks where we predict the valence s, indi-
cated as ?All? in Table 3), we micro-average the pre-
cision and recall over the two VPC categories, and
calculate the F-score as their harmonic mean.
From the results, it is obvious that the chart
mining-based model performs best overall, and in-
deed for most of the measures presented. The Char-
niak parser-based extraction method performs rea-
sonably well, especially in the VPC+valence extrac-
tion task over the FULL task, where the recall was
higher than the chart mining method. Although
not reported here, we observe a marked improve-
ment in the results for the Charniak parser when
the VPC types attested in the WSJ are not filtered
from the test set. This indicates that the statisti-
cal parser relies heavily on lexicalised VPC infor-
mation, while the chart mining model is much more
syntax-oriented. In error analysis of the data, we ob-
served that the Charniak parser was noticeably more
accurate at extracting VPCs where the verb was fre-
quent (our method, of course, did not have access
to the base frequency of the simplex verb), under-
lining again the power of lexicalisation. This points
to two possibilities: (1) the potential for our method
to similarly benefit from lexicalisation if we were to
remove the constraint on ignoring any pre-existing
lexical entries for the verb; and (2) the possibility
for hybridising between lexicalised models for fre-
quent verbs and unlexicalised models for infrequent
verbs. Having said this, it is important to reinforce
that lexical acquisition is usually performed in the
absence of lexicalised probabilities, as if we have
prior knowledge of the lexical item, there is no need
to extract it. In this sense, the first set of results in
Table 3 over Gold VPCs are the most informative,
and illustrate the potential of the proposed approach.
From the results of all the models, it would ap-
pear that intransitive VPCs are more difficult to ex-
tract than transitive VPCs. This is partly because the
dataset we use is unbalanced: the number of transi-
tive VPC types is about twice the number of intran-
sitive VPCs. Also, the much lower numbers over
the FULL set compared to the GOLD VPC set are due
to the fact that only 1/8 of the candidates are true
VPCs.
5 Discussion and Future Work
The inventory of features we propose for VPC ex-
traction is just one illustration of how partial parse
results can be used in lexical acquisition tasks.
The general chart mining technique can easily be
adapted to learn other challenging linguistic phe-
nomena, such as the countability of nouns (Bald-
win and Bond, 2003), subcategorization properties
of verbs or nouns (Korhonen, 2002), and general
multiword expression (MWE) extraction (Baldwin
and Kim, 2009). With MWE extraction, e.g., even
though some MWEs are fixed and have no internal
syntactic variability, such as ad hoc, there is a very
large proportion of idioms that allow various de-
grees of internal variability, and with a variable num-
ber of elements. For example, the idiom spill the
beans allows internal modification (spill mountains
of beans), passivisation (The beans were spilled in
the latest edition of the report), topicalisation (The
beans, the opposition spilled), and so forth (Sag et
al., 2002). In general, however, the exact degree of
variability of an idiom is difficult to predict (Riehe-
mann, 2001). The chart mining technique we pro-
pose here, which makes use of partial parse results,
may facilitate the automatic recognition task of even
more flexible idioms, based on the encouraging re-
sults for VPCs.
The main advantage, though, of chart mining is
that parsing with precision grammars does not any
longer have to assume complete coverage, as has
traditionally been the case. As an immediate con-
sequence, the possibility of applying our chart min-
ing technique to evolving medium-sized grammars
makes it especially interesting for lexical acquisi-
16
Task VPC Type Na??ve Baseline Charniak Parser Chart-Mining
P R F P R F P R F
GOLD VPC
Intrans-VPC 0.300 0.018 0.034 0.549 0.753 0.635 0.845 0.621 0.716
Trans-VPC 0.676 0.348 0.459 0.829 0.648 0.728 0.877 0.956 0.915
All 0.576 0.236 0.335 0.691 0.686 0.688 0.875 0.859 0.867
FULL
Intrans-VPC 0.060 0.018 0.028 0.102 0.593 0.174 0.153 0.155 0.154
Trans-VPC 0.083 0.348 0.134 0.179 0.448 0.256 0.179 0.362 0.240
All 0.080 0.236 0.119 0.136 0.500 0.213 0.171 0.298 0.218
VPC 0.123 0.348 0.182 0.173 0.782 0.284 0.259 0.332 0.291
Table 3: Results for the different methods over the three VPC extraction tasks detailed in Table 2
tion over low-density languages, for instance, where
there is a real need for rapid-prototyping of language
resources.
The chart mining approach we propose in this
paper is couched in the bottom-up chart parsing
paradigm, based exclusively on passive edges. As
future work, we would also like to look into the
top-level active edges (those active edges that are
never completed), as an indication of failed assump-
tions. Moreover, it would be interesting to investi-
gate the applicability of the technique in other pars-
ing strategies, e.g., head-corner or left-corner pars-
ing. Finally, it would also be interesting to in-
vestigate whether by using the features we acquire
from chart mining enhanced with information on the
prevalence of certain patterns, we could achieve per-
formance improvements over broader-coverage tree-
bank parsers such as the Charniak parser.
6 Conclusion
We have proposed a chart mining technique for lex-
ical acquisition based on partial parsing with preci-
sion grammars. We applied the proposed method
to the task of extracting English verb particle con-
structions from a prescribed set of corpus instances.
Our results showed that simple unlexicalised fea-
tures mined from the chart can be used to effec-
tively extract VPCs, and that the model outperforms
a probabilistic baseline and the Charniak parser at
VPC extraction.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram. The first was supported by the German Excellence
Cluster of Multimodal Computing and Interaction.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23:597?618.
Timothy Baldwin and Francis Bond. 2003. Learning
the countability of English nouns from corpus data.
In Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2003),
pages 463?470, Sapporo, Japan.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC 2008 Work-
shop: Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Morocco.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Univer-
sita?t des Saarlandes, Saarbru?cken, Germany.
John Carroll and Stephan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proceedings of the 2nd International Joint Confer-
ence on Natural LanguageProcessing (IJCNLP 2005),
pages 165?176, Jeju Island, Korea.
Eugene Charniak. 2000. A maximum entropy-based
parser. In Proceedings of the 1st Annual Meeting of
the North American Chapter of Association for Com-
putational Linguistics (NAACL2000), Seattle, USA.
Daniel de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the ACL2009
Workshop on Grammar Engineering Across Frame-
works (GEAF), Singapore.
17
Nicole Dehe?. 2002. Particle Verbs in English: Syn-
tax, Information, Structure and Intonation. John Ben-
jamins, Amsterdam, Netherlands/Philadelphia, USA.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1?
17. CSLI Publications.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte Jo?rg, and Ul-
rich Scha?fer. 2006. Question answering from struc-
tured knowledge sources. Journal of Applied Logic,
Special Issue on Questions and Answers: Theoretical
and Applied Perspectives., 5(1):20?48.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic unifcation-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1999), pages 535?541, Mary-
land, USA.
Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, and
Rob Malouf. 1999. A Bag of Useful Techniques for
Efficient and Robust Parsing. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 473?480, Maryland, USA.
Anna Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Andrew MacKinlay, David Martinez, and Timothy Bald-
win. 2009. Biomedical event annotation with CRFs
and precision grammars. In Proceedings of BioNLP
2009: Shared Task, pages 77?85, Boulder, USA.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conferencde on Natural Language
Learning (CoNLL 2002), pages 49?55, Taipei, Taiwan.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing ? practical results. In
Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational
Linguistics (NAACL 2000), pages 162?169, Seattle,
USA.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik
Velldal, Dorothee Beermann, John Carroll, Dan
Flickinger, Lars Hellan, Janne Bondi Johannessen,
Paul Meurer, Torbj?rn Nordga?rd, and Victoria Rose?n.
2004. Som a? kapp-ete med trollet? Towards MRS-
Based Norwegian?English Machine Translation. In
Proceedings of the 10th International Conference on
Theoretical and Methodological Issues in Machine
Translation, Baltimore, USA.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the
extraction of multiword expressions. In Proceedings
of the LREC 2008 Workshop: Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 50?53,
Marrakech, Morocco.
Susanne Riehemann. 2001. A Constructional Approach
to Idioms and Word Formation. Ph.D. thesis, Stanford
University, CA, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-
2002), pages 1?15, Mexico City, Mexico.
Frank Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?178.
Masaru Tomita. 1985. An efficient context-free parsing
algorithm for natural languages. In Proceedings of the
9th International Joint Conference on Artificial Intel-
ligence, pages 756?764, Los Angeles, USA.
Kristina Toutanova, Christoper D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse ranking for a rich HPSG grammar. In Proceed-
ings of the 1st Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 253?263, Sozopol, Bul-
garia.
Hans Uszkoreit. 2002. New chances for deep linguis-
tic processing. In Proceedings of the 19th interna-
tional conference on computational linguistics (COL-
ING 2002), Taipei, Taiwan.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics), pages 446?453, Barcelona, Spain.
Aline Villavicencio and Ann Copestake. 2002. Verb-
particle constructions in a computational grammar of
English. In Proceedings of the 9th International Con-
ference on Head-Driven Phrase Structure Grammar
(HPSG-2002), Seoul, Korea.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 275?280, Genoa, Italy.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08), Marrakech, Morocco.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007a.
Partial parse selection for robust deep processing. In
Proceedings of ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 128?135, Prague, Czech Repub-
lic.
Yi Zhang, Stephan Oepen, and John Carroll. 2007b. Ef-
ficiency in unification-based N-best parsing. In Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies (IWPT 2007), pages 48?59, Prague,
Czech.
18
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100?108,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Topic Coherence
David Newman,?? Jey Han Lau,? Karl Grieser?, and Timothy Baldwin,??
? NICTA Victoria Research Laboratory, Australia
? Dept of Computer Science, University of California, Irvine
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Dept of Information Systems, University of Melbourne, Australia
newman@uci.edu, depthchargex@gmail.com,
kgrieser@csse.unimelb.edu.au, tb@ldwin.net
Abstract
This paper introduces the novel task of topic
coherence evaluation, whereby a set of words,
as generated by a topic model, is rated for
coherence or interpretability. We apply a
range of topic scoring models to the evaluation
task, drawing on WordNet, Wikipedia and the
Google search engine, and existing research
on lexical similarity/relatedness. In compar-
ison with human scores for a set of learned
topics over two distinct datasets, we show a
simple co-occurrence measure based on point-
wise mutual information over Wikipedia data
is able to achieve results for the task at or
nearing the level of inter-annotator correla-
tion, and that other Wikipedia-based lexical
relatedness methods also achieve strong re-
sults. Google produces strong, if less consis-
tent, results, while our results over WordNet
are patchy at best.
1 Introduction
There has traditionally been strong interest within
computational linguistics in techniques for learning
sets of words (aka topics) which capture the latent
semantics of a document or document collection, in
the form of methods such as latent semantic analysis
(Deerwester et al, 1990), probabilistic latent seman-
tic analysis (Hofmann, 2001), random projection
(Widdows and Ferraro, 2008), and more recently, la-
tent Dirichlet alocation (Blei et al, 2003; Griffiths
and Steyvers, 2004). Such methods have been suc-
cessfully applied to a myriad of tasks including word
sense discrimination (Brody and Lapata, 2009), doc-
ument summarisation (Haghighi and Vanderwende,
2009), areal linguistic analysis (Daume III, 2009)
and text segmentation (Sun et al, 2008). In each
case, extrinsic evaluation has been used to demon-
strate the effectiveness of the learned topics in the
application domain, but standardly, no attempt has
been made to perform intrinsic evaluation of the top-
ics themselves, either qualitatively or quantitatively.
In machine learning, on the other hand, researchers
have modified and extended topic models in a vari-
ety of ways, and evaluated intrinsically in terms of
model perplexity (Wallach et al, 2009), but there has
been less effort on qualitative understanding of the
semantic nature of the learned topics.
This research seeks to fill the gap between topic
evaluation in computational linguistics and machine
learning, in developing techniques to perform intrin-
sic qualitative evaluation of learned topics. That
is, we develop methods for evaluating the qual-
ity of a given topic, in terms of its coherence to
a human. After learning topics from a collection
of news articles and a collection of books, we ask
humans to decide whether individual learned top-
ics are coherent, in terms of their interpretability
and association with a single over-arching seman-
tic concept. We then propose models to predict
topic coherence, based on resources such as Word-
Net, Wikipedia and the Google search engine, and
methods ranging from ontological similarity to link
overlap and term co-occurrence. Over topics learned
from two distinct datasets, we demonstrate that there
is remarkable inter-annotator agreement on what is
a coherent topic, and additionally that our methods
based on Wikipedia are able to achieve nearly perfect
agreement with humans over the evaluation of topic
coherence.
This research forms part of a larger research
agenda on the utility of topic modelling in gist-
ing and visualising document collections, and ulti-
mately enhancing search/discovery interfaces over
100
document collections (Newman et al, to appeara).
Evaluating topic coherence is a component of the
larger question of what are good topics, what char-
acteristics of a document collection make it more
amenable to topic modelling, and how can the po-
tential of topic modelling be harnessed for human
consumption (Newman et al, to appearb).
2 Related Work
Most earlier work on intrinsically evaluating learned
topics has been on the basis of perplexity results,
where a model is learned on a collection of train-
ing documents, then the log probability of the un-
seen test documents is computed using that learned
model. Usually perplexity is reported, which is the
inverse of the geometric mean per-word likelihood.
Perplexity is useful for model selection and adjust-
ing parameters (e.g. number of topics T ), and is
the standard way of demonstrating the advantage of
one model over another. Wallach et al (2009) pre-
sented efficient and unbiased methods for computing
perplexity and evaluating almost any type of topic
model.
While statistical evaluation of topic models is
reasonably well understood, there has been much
less work on evaluating the intrinsic semantic qual-
ity of topics learned by topic models, which could
have a far greater impact on the overall value of
topic modeling for end-user applications. Some re-
searchers have started to address this problem, in-
cluding Mei et al (2007) who presented approaches
for automatic labeling of topics (which is core to the
question of coherence and semantic interpretabil-
ity), and Griffiths and Steyvers (2006) who applied
topic models to word sense discrimination tasks.
Misra et al (2008) used topic modelling to identify
semantically incoherent documents within a docu-
ment collection (vs. coherent topics, as targeted in
this research). Chang et al (2009) presented the
first human-evaluation of topic models by creating
a task where humans were asked to identify which
word in a list of five topic words had been ran-
domly switched with a word from another topic.
This work showed some possibly counter-intuitive
results, where in some cases humans preferred mod-
els with higher perplexity. This type of result shows
the need for further exploring measures other than
perplexity for evaluating topic models. In earlier
work, we carried out preliminary experimentation
using pointwise mutual information and Google re-
sults to evaluate topic coherence over the same set
of topics as used in this research (Newman et al,
2009).
Part of this research takes inspiration from the
work on automatic evaluation in machine translation
(Papineni et al, 2002) and automatic summarisation
(Lin, 2004). Here, the development of automated
methods with high correlation with human subjects
has opened the door to large-scale automated evalua-
tion of system outputs, revolutionising the respective
fields. While our aspirations are more modest, the
basic aim is the same: to develop a fully-automated
method for evaluating a well-grounded task, which
achieves near-human correlation.
3 Topic Modelling
In order to evaluate topic modelling, we require a
topic model and set of topics for a given document
collection. While the evaluation methodology we
describe generalises to any method which gener-
ates sets of words, all of our experiments are based
on Latent Dirichlet Allocation (LDA, aka Discrete
Principal Component Analysis), on the grounds that
it is a state-of-the-art method for generating topics.
LDA is a Bayesian graphical model for text docu-
ment collections represented by bags-of-words (see
Blei et al (2003), Griffiths and Steyvers (2004),
Buntine and Jakulin (2004)). In a topic model, each
document in the collection of D documents is mod-
elled as a multinomial distribution over T topics,
where each topic is a multinomial distribution over
W words. Typically, only a small number of words
are important (have high likelihood) in each topic,
and only a small number of topics are present in each
document.
The collapsed Gibbs sampled topic model simul-
taneously learns the topics and the mixture of topics
in documents by iteratively sampling the topic as-
signment z to every word in every document, using
the Gibbs sampling update:
p(zid = t|xid = w, z?id) ?
N?idwt + ?
?
w N?idwt + W?
N?idtd + ?
?
t N?idtd + T?
101
where zid = t is the assignment of the ith word in
document d to topic t, xid = w indicates that the
current observed word is w, and z?id is the vector of
all topic assignments not including the current word.
Nwt represents integer count arrays (with the sub-
scripts denoting what is counted), and ? and ? are
Dirichlet priors.
The maximum a posterior (MAP) estimates of the
topics p(w|t), t = 1 . . . T are given by:
p(w|t) = Nwt + ??
w Nwt + W?
We will follow the convention of representing a
topic via its top-n words, ordered by p(w|t). Here,
we use the top-ten words, as they usually provide
sufficient detail to convey the subject of a topic,
and distinguish one topic from another. For the
remainder of this paper, we will refer to individ-
ual topics by its list of top-ten words, denoted by
w = (w1, . . . , w10).
4 Topic Evaluation Methods
We experiment with scoring methods based on
WordNet (Section 4.1), Wikipedia (Section 4.2) and
the Google search engine (Section 4.3). In the case
of Google, we query for the entire topic, but with
WordNet and Wikipedia, this takes the form of scor-
ing each word-pair in a given topic w based on the
component words (w1, . . . , w10). Given some (sym-
metric) word-similarity measure D(wi, wj), two
straightforward ways of producing a combined score
from the 45 (i.e.
(10
2
)
) word-pair scores are: (1) the
arithmetic mean, and (2) the median, as follows:
Mean-D-Score(w) =
mean{D(wi, wj), ij ? 1 . . . 10, i < j}
Median-D-Score(w) =
median{D(wi, wj), ij ? 1 . . . 10, i < j}
Intuitively, the median seems the more natural rep-
resentation, as it is less affected by outlier scores,
but we experiment with both, and fall back to empir-
ical verification of which is the better combination
method.
4.1 WordNet similarity
WordNet (Fellbaum, 1998) is a lexical ontology
that represents word sense via ?synsets?, which
are structured in a hypernym/hyponym hierarchy
(nouns) or hypernym/troponym hierarchy (verbs).
WordNet additionally links both synsets and words
via lexical relations including antonymy, morpho-
logical derivation and holonymy/meronym.
In parallel with the development of WordNet, a
number of computational methods for calculating
the semantic relatedness/similarity between synset
pairs (i.e. sense-specified word pairs) have been de-
veloped, as we outline below. These methods ap-
ply to synset rather than word pairs, so to generate a
single score for a given word pair, we look up each
word in WordNet and exhaustively generate scores
for each sense pairing defined by them, and calcu-
late their arithmetic mean.1
The majority of the methods (all methods other
than HSO, VECTOR and LESK) are restricted to op-
erating strictly over hierarchical links within a sin-
gle hierarchy. As the verb and noun hierarchies are
not connected (other than via derivational links), this
means that it is generally not possible to calculate
the similarity between noun and verb senses, for ex-
ample. In such cases, we simply drop the synset
pairing in question from our calculation of the mean.
The least common subsumer (LCS) is a common
feature to a number of the measures, and is defined
as the deepest node in the hierarchy that subsumes
both of the synsets under question.
For all our experiments over WordNet, we use the
WordNet::Similarity package.
Path distance (PATH)
The simplest of the WordNet-based measures is
to count the number of nodes visited while going
from one word to another via the hypernym hierar-
chy. The path distance between two nodes is de-
fined as the number of nodes that lie on the short-
est path between two words in the hierarchy. This
1We also experimented with the median, and trialled filter-
ing the set of senses in a variety of ways, e.g. using only the
first sense (the sense with the highest prior) for a given word,
or using only the word senses associated with the POS with the
highest prior. In all cases, the overall trend was for the correla-
tion with the human scores to drop relative to the mean, so we
only present the numbers for the mean in this paper.
102
count of nodes includes the beginning and ending
word nodes.
Leacock-Chodorow (LCH)
The measure of semantic similarity devised by
Leacock et al (1998) finds the shortest path between
two WordNet synsets (sp(c1, c2)) using hypernym
and synonym relationships. This path length is then
scaled by the maximum depth of WordNet (D), and
the log likelihood taken:
simlch(c1, c2) = ? log
sp(c1, c2)
2 ?D
Wu-Palmer (WUP)
Wu and Palmer (1994) proposed to scale the depth
of the two synset nodes (depthc1 and depthc2) by
the depth of their LCS (depth(lcsc1,c2)):
simwup(c1, c2) =
2 ? depth(lcsc1,c2)
depthc1 + depthc2 + 2 ? depth(lcsc1,c2)
The scaling means that specific terms (deeper in the
hierarchy) that are close together are more semanti-
cally similar than more general terms, which have a
short path distance between them. Only hypernym
relationships are used in this measure, as the LCS
is defined by the common member in the concepts?
hypernym path.
Hirst-St Onge (HSO)
Hirst and St-Onge (1998) define a measure of se-
mantic similarity based on length and tortuosity of
the path between nodes. Hirst and St-Onge attribute
directions (up, down and horizontal) to the larger set
of WordNet relationships, and identify the path from
one word to another utilising all of these relation-
ships. The relatedness score is then computed by
the weighted sum of the path length between the two
words (len(c1, c2)) and the number of turns the path
makes (turns(c1, c2)) to take this route:
relhso(c1, c2) =
C ? len(c1, c2)? k ? turns(c1, c2)
where C and k are constants. Additionally, a set of
restrictions is placed on the path so that it may not
be more than a certain length, may not contain more
than a set number of turns, and may only take turns
in certain directions.
Resnik Information Content (RES)
Resnik (1995) presents a method for weighting
edges in WordNet (avoiding the assumption that all
edges between nodes have equal importance), by
weighting edges between nodes by their frequency
of use in textual corpora.
Resnik found that the most effective measure of
comparison using this methodology was to measure
the Information Content (IC(c) = ? log p(c)) of
the subsumer with the greatest Information Content
from the set of all concepts that subsumed the two
initial concepts (S(c1, c2)) being compared:
simres(c1, c2) = max
c?S(c1,c2)
[? log p(c)]
Lin (LIN)
Lin (1998) expanded on the Information Theo-
retic approach presented by Resnik by scaling the
Information Content of each node by the informa-
tion content of their LCS:
simlin(c1, c2) =
2? log p(lcsc1,c2)
log p(c1) + log p(c2)
This measure contrasts the joint content of the two
concepts with the difference between them.
Jiang-Conrath (JCN)
Jiang and Conrath (1997) define a measure that
utilises the components of the information content
of the LCS in a different manner:
simjcn(c1, c2) =
1
IC(a) + IC(b)? 2? IC(lcsa,b)
Instead of defining commonality and difference as
with Lin?s measure, the key determinant is the speci-
ficity of the two nodes compared with their LCS.
Lesk (LESK)
Lesk (1986) proposed a significantly different ap-
proach to lexical similarity to that proposed in the
methods presented above, using the lexical over-
lap in dictionary definitions (or glosses) to disam-
biguate word sense. The sense definitions that con-
tain the most words in common indicate the most
likely sense of the word given its co-occurrence with
similar word senses. Banerjee and Pedersen (2002)
103
adapted this method to utilise WordNet sense glosses
rather than dictionary definitions, and expand the
dictionary definitions via ontological links, and it is
this method we experiment with in this paper.
Vector (VECTOR)
Schu?tze (1998) uses the words surrounding a term
in a piece of text to form a context vector that de-
scribes the context in which the word sense appears.
For a set of words associated with a target sense, a
context vector is computed as the centroid vector of
these words. The centroid context vectors each rep-
resent a word sense. To compare word senses, the
cosine similarity of the context vectors is used.
4.2 Wikipedia
In the last few years, there has been a surge of in-
terest in using Wikipedia to calculate semantic sim-
ilarity, using the Wikipedia article content, in-article
links and document categories (Stru?be and Ponzetto,
2006; Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008). We present a selection of such meth-
ods below. There are a number of Wikipedia-based
scoring methods which we do not present results
for here (notably Stru?be and Ponzetto (2006) and
Gabrilovich and Markovitch (2007)), due to their
computational complexity and uncertainty about the
full implementation details of the methods.
As with WordNet, a given word will often have
multiple entries in Wikipedia, grouped in a disam-
biguation page. For MIW, RACO and DOCSIM,
we apply the same strategy as we did with Word-
Net, in exhaustively calculating the pairwise scores
between the sets of documents associated with each
term, and averaging across them.
Milne-Witten (MIW)
Milne and Witten (2008) adapted the Resnik
(1995) methodology to utilise the count of links
pointing to an article. As Wikipedia is self-
referential (articles link to related articles), no ex-
ternal data is needed to find the ?referred-to-edness?
of a concept. Milne and Witten use an adapted In-
formation Content measure that weights the number
of links from one article to another (c1 ? c2) by the
total number of links to the second article:
w(c1 ? c2) = |c1 ? c2| ? log
?
x?W
|W |
|c1, x)|
where x is an article in W , Wikipedia. This mea-
sure provides the similarity of one article to another,
however this is asymmetrical. The above metric is
used to find the weights of all outlinks from the two
articles being compared:
~c1 = (w(c1 ? l1), w(c1 ? l2), ? ? ? , w(c1 ? ln))
~c2 = (w(c2 ? l1), w(c2 ? l2), ? ? ? , w(c2 ? ln))
for the set of links l that is the union of the sets of
outlinks from both articles. The overall similarity
of the two articles is then calculated by taking the
cosine similarity of the two vectors.
Related Article Concept Overlap (RACO)
We also determine the category overlap of two
articles by examining the outlinks of both articles,
in the form of the Related Article Concept Overlap
(RACO) measure. The concept overlap of the sets
of respective outlinks is given by the union of the
two sets of categories from the outlinks from each
article:
overlap(c1, c1) =
?
?
(
?
l?ol(c1)
cat(l)
)
?
(
?
l?ol(c2)
cat(l)
)
?
?
where ol(c1) is the set of outlinks from article c1,
and cat(l) is the set of categories of which the arti-
cle at outlink l is a member. To account for article
size (and differing number of outlinks), the Jaccard
coefficient is used:
relraco(c1, c2) =
?
?
(
?
l?ol(c1) cat(l)
)
?
(
?
l?ol(c2) cat(l)
)
?
?
?
?
?
l?ol(c1) cat(l)
?
?+
?
?
?
l?ol(c2) cat(l)
?
?
Document Similarity (DOCSIM)
In addition to these two measures of semantic re-
latedness, we experiment with simple cosine simi-
larity of the text of Wikipedia articles as a measure
of semantic relatedness.
Term Co-occurrence (PMI)
Another variant is to treat Wikipedia as a single
meta-document and score word pairs using term co-
occurrence. Here, we calculate the pointwise mu-
tual information (PMI) of each word pair, estimated
104
Selected high-scoring topics (unanimous score=3):
[NEWS] space earth moon science scientist light nasa mission planet mars ...
[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...
[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...
[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...
Selected low-scoring topics (unanimous score=1):
[NEWS] king bond berry bill ray rate james treas byrd key ...
[NEWS] dog moment hand face love self eye turn young character ...
[BOOKS] soon short longer carried rest turned raised filled turn allowed ...
[BOOKS] act sense adv person ppr plant sax genus applied dis ...
Table 1: A selection of high-scoring and low-scoring topics
from the entire corpus of over two million English
Wikipedia articles (?1 billion words). PMI has been
studied variously in the context of collocation ex-
traction (Pecina, 2008), and is one measure of the
statistical independence of observing two words in
close proximity. Using a sliding window of 10-
words to identify co-occurrence, we computed the
PMI of all a given word pair (wi, wj) as, following
Newman et al (2009):
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
4.3 Search engine-based similarity
Finally, we present two search engine-based scor-
ing methods, based on Newman et al (2009). In
this case the external data source is the entire World
Wide Web, via the Google search engine. Unlike
the methods presented above, here we query for the
topic in its entirety,2 meaning that we return a topic-
level score rather than scores for individual word or
word sense pairs. In each case, we mark each search
term with the advanced search option + to search
for the terms exactly as is and prevent Google from
using synonyms or lexical variants of the term. An
example query is: +space +earth +moon +science
+scientist +light +nasa +mission +planet +mars.
Google title matches (TITLES)
Firstly, we score topics by the relative occurrence
of their component words in the titles of documents
returned by Google:
Google-titles-match(w) = 1 [wi = vj ]
2All queries were run on 15/09/2009.
where i = 1, . . . , 10 and j = 1, . . . , |V |, vj are
all the unique terms mentioned in the titles from the
top-100 search results, and 1 is the indicator function
to count matches. For example, in the top-100 re-
sults for our query above, there are 194 matches with
the ten topic words, so Google-titles-match(w) =
194.
Google log hits matches (LOGHITS)
Second, we issue queries as above, but return the
log number of hits for our query:
Google-log-hits(w) =
log10(# results from search for w)
where w is the search string +w1 +w2 +w3 . . .
+w10. For example, our query above returns
171,000 results, so Google-log-hits(w) = 5.2. and
the URL titles from the top-100 results include a to-
tal of 194 matches with the ten topic words, so for
this topic Google-titles-match(w)=194.
5 Experimental Setup
We learned topics for two document collections: a
collection of news articles, and a collection of books.
These collections were chosen to produce sets of
topics that have more variable quality than one typi-
cally observes when topic modeling highly uniform
content. The collection of D = 55, 000 news arti-
cles was selected from English Gigaword, and the
collection of D = 12, 000 books was downloaded
from the Internet Archive. We refer to these collec-
tions as NEWS and BOOKS, respectively.
Standard procedures were used to tokenize each
collection and create the bags-of-words. We learned
105
Resource Method Median Mean
WordNet
HSO ?0.29 0.34
JCN 0.08 0.22
LCH ?0.18 ?0.07
LESK 0.38 0.37
LIN 0.18 0.25
PATH 0.19 0.11
RES ?0.10 0.13
VECTOR 0.07 0.20
WUP 0.03 0.10
Wikipedia
RACO 0.61 0.63
MIW 0.69 0.60
DOCSIM 0.45 0.50
PMI 0.78 0.77
Google TITLES 0.80LOGHITS 0.46
Gold-standard IAA 0.79 0.73
Table 2: Spearman rank correlation ? values for the
different scoring methods over the NEWS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
topic models of NEWS and BOOKS using T = 200
and T = 400 topics respectively. We randomly
selected a total of 237 topics from the two collec-
tions for user scoring. We asked N = 9 users to
score each of the 237 topics on a 3-point scale where
3=?useful? (coherent) and 1=?useless? (less coher-
ent).
We provided annotators with a rubric and guide-
lines on how to judge whether a topic was useful
or useless. In addition to showing several examples
of useful and useless topics, we instructed users to
decide whether the topic was to some extent coher-
ent, meaningful, interpretable, subject-heading-like,
and something-you-could-easily-label. For our pur-
poses, the usefulness of a topic can be thought of
as whether one could imagine using the topic in a
search interface to retrieve documents about a par-
ticular subject. One indicator of usefulness is the
ease by which one could think of a short label to de-
scribe a topic.
Table 1 shows a selection of high- and low-
scoring topics, as scored by the N = 9 users. The
first topic illustrates the notion of labelling coher-
ence, as space exploration, e.g., would be an obvi-
ous label for the topic. The low-scoring topics dis-
play little coherence, and one would not expect them
Resource Method Median Mean
WordNet
HSO 0.15 0.59
JCN ?0.20 0.19
LCH ?0.31 ?0.15
LESK 0.53 0.53
LIN 0.09 0.28
PATH 0.29 0.12
RES 0.57 0.66
VECTOR ?0.08 0.27
WUP 0.41 0.26
Wikipedia
RACO 0.62 0.69
MIW 0.68 0.70
DOCSIM 0.59 0.60
PMI 0.74 0.77
Google TITLES 0.51LOGHITS ?0.19
Gold-standard IAA 0.82 0.78
Table 3: Spearman rank correlation ? values for the dif-
ferent scoring methods over the BOOKS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
to be useful as categories or facets in a search inter-
face. Note that the useless topics from both collec-
tions are not chance artifacts produced by the mod-
els, but are in fact stable and robust statistical fea-
tures in the data sets.
6 Results
The results for the different topic scoring methods
over the NEWS and BOOKS collections are pre-
sented in Tables 2 and 3, respectively. In each ta-
ble, we separate out the scoring methods into those
based on WordNet (from Section 4.1), those based
on Wikipedia (from Section 4.2), and those based on
Google (from Section 4.3).
As stated in Section 4, we experiment with two
methods for combining the word-pair scores (for all
methods other than the two Google methods, which
operate natively over a word set), namely the arith-
metic mean and median. We present the numbers
for these two methods in each table. In each case,
we evaluate via Spearman rank correlation, revers-
ing the sign of the calculated ? value for PATH (as it
is the only instance of a distance metric, where the
gold-standard is made up of similarity values).
We include the inter-annotator agreement (IAA)
in the final row of each table, which we consider
106
to be the upper bound for the task. This is calcu-
lated as the average Spearman rank correlation be-
tween each annotator and the mean/median of the
remaining annotators for that topic. Encouragingly,
there is relatively little difference in the IAA be-
tween the two datasets; the median-based calcula-
tion produces slightly higher ? values and is empiri-
cally the method of choice.3
Of all the topic scoring methods tested, PMI
(term co-occurrence via simple pointwise mutual in-
formation) is the most consistent performer, achiev-
ing the best or near-best results over both datasets,
and approaching or surpassing the inter-annotator
agreement. This indicates both that the task of
topic evaluation as defined in this paper is com-
putationally tractable, and that word-pair based co-
occurrence is highly successful at modelling topic
coherence.
Comparing the different resources, Wikipedia is
far and away the most consistent performing, with
PMI producing the best results, followed by MIW
and RACO, and finally DOCSIM. There is rela-
tively little difference in results between NEWS and
BOOKS for the Wikipedia methods. Google achieves
the best results over NEWS, for TITLES (actually
slightly above the IAA), but the results fall away
sharply over BOOKS. The reason for this can be
seen in the sample topics in Table 1: the topics for
BOOKS tend to be more varied in word class than
for NEWS, and contain less proper names; also, the
genre of BOOKS is less well represented on the web.
We hypothesise that Wikipedia?s encyclopedic na-
ture means that it has good coverage over both do-
mains, and thus more robust.
Turning to WordNet, the overall results are
markedly better over BOOKS, again largely because
of the relative sparsity of proper names in the re-
source. The results for individual methods are some-
what surprising. Whereas JCN and LCH have been
shown to be two of the best-performing methods
over lexical similarity tasks (Budanitsky and Hirst,
2005; Agirre et al, 2009), they perform abysmally
at the topic scoring task. Indeed, the spread of re-
sults across the WordNet similarity methods (no-
3Note that the choice of mean or median for IAA is in-
dependent of that for the scoring methods, as they are com-
bining different things: annotator scores in the one hand, and
word/concept pair scores on the other.
tably HSO, JCN, LCH, LIN, RES and WUP) is
much greater than we had expected. The single most
consistent method is LESK, which is based on lexi-
cal overlap in definition sentences and makes rela-
tively modest use of the WordNet hierarchy. Supple-
mentary evaluation where we filtered out all proper
nouns from the topics (based on simple POS priors
for each word learned from an automatically-tagged
version of the British National Corpus) led to a slight
increase in results for the WordNet methods; the full
results are omitted for reasons of space. In future
work, we intend to carry out error analysis to deter-
mine why some of the methods performed so badly,
or inconsistently across the two datasets.
There is no clear answer to the question of
whether the mean or median is the best method for
combining the pair-wise scores.
7 Conclusions
We have proposed the novel task of topic coher-
ence evaluation as a form of intrinsic topic evalu-
ation with relevance in document search/discovery
and visualisation applications. We constructed
a gold-standard dataset of topic coherence scores
over the output of a topic model for two distinct
datasets, and evaluated a wide range of topic scor-
ing methods over this dataset, drawing on WordNet,
Wikipedia and the Google search engine. The sin-
gle best-performing method was term co-occurrence
within Wikipedia based on pointwise mutual infor-
mation, which achieve results very close to the inter-
annotator agreement for the task. Google was also
found to perform well over one of the two datasets,
while the results for the WordNet-based methods
were overall surprisingly low.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
References
E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas?ca,
and A Soroa. 2009. A study on similarity and re-
107
latedness using distributional and WordNet-based ap-
proaches. In Proc. of HLT: NAACL 2009, pages 19?
27, Boulder, Colorado.
S Banerjee and T Pedersen. 2002. An adapted Lesk algo-
rithm for word sense disambiguation using WordNet.
Proc. of CICLing?02, pages 136?145.
DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S Brody and M Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL 2009, pages 103?111,
Athens, Greece.
A Budanitsky and G Hirst. 2005. Evaluating WordNet-
based Measures of Lexical Sematic Relatedness.
Computational Linguistics, 32(1):13?47.
WL Buntine and A Jakulin. 2004. Applying discrete
PCA in data analysis. In Proc. of UAI 2004, pages
59?66.
J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.
2009. Reading tea leaves: How humans interpret topic
models. In Proc. of NIPS 2009.
H Daume III. 2009. Non-parametric bayesian areal lin-
guistics. In Proc. of HLT: NAACL 2009, pages 593?
601, Boulder, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science, 41(6).
C Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, USA.
E Gabrilovich and S Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proc. of IJCAI?07, pages 1606?
1611, Hyderabad, India.
T Griffiths and M Steyvers. 2004. Finding scientific top-
ics. In Proc. of the National Academy of Sciences, vol-
ume 101, pages 5228?5235.
T Griffiths and M Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to
Meaning.
A Haghighi and L Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proc. of HLT: NAACL 2009, pages 362?370, Boulder,
USA.
G Hirst and D St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropism. In Fellbaum (Fellbaum, 1998), pages
305?332.
T Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of COLING?97, pages 19?33, Taipei, Taiwan.
C Leacock, G A Miller, and M Chodorow. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147?65.
M Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC?86,
pages 24?26, Toronto, Canada.
D Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING/ACL?98, pages 768?
774, Montreal, Canada.
C-Y Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proc. of the ACL 2004
Workshop on Text Summarization Branches Out (WAS
2004), pages 74?81, Barcelona, Spain.
Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling
of multinomial topic models. In Proc. of KDD 2007,
pages 490?499.
D Milne and IH Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence, pages 25?30,
Chicago, USA.
H Misra, O Cappe, and F Yvon. 2008. Using LDA to
detect semantically incoherent documents. In Proc. of
CoNLL 2008, pages 41?48, Manchester, England.
D Newman, S Karimi, and L Cavedon. 2009. External
evaluation of topic models. In Proc. of ADCS 2009,
pages 11?18, Sydney, Australia.
D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-
tinez, and J Zobel. to appeara. Visualizing docu-
ment collections and search results using topic map-
ping. Journal of Web Semantics.
D Newman, Y Noh, E Talley, S Karimi, and T Bald-
win. to appearb. Evaluating topic models for digital
libraries. In Proc. of JCDL/ICADL 2010, Gold Coast,
Australia.
K Papineni, S Roukos, T Ward, and W-J Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL 2002, pages 311?318,
Philadelphia, USA.
P Pecina. 2008. Lexical Association Measures: Colloca-
tion Extraction. Ph.D. thesis, Charles University.
P Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of IJ-
CAI?95, pages 448?453, Montreal, Canada.
H Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
M Stru?be and SP Ponzetto. 2006. WikiRelate! comput-
ing semantic relateness using Wikipedia. In Proc. of
AAAI?06, pages 1419?1424, Boston, USA.
Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation
with LDA-based Fisher kernel. In Proc. of ACL-08:
HLT, pages 269?272.
HM Wallach, I Murray, R Salakhutdinov, and
DM Mimno. 2009. Evaluation methods for
topic models. In Proc. of ICML 2009, page 139.
D Widdows and K Ferraro. 2008. Semantic Vectors:
A scalable open source package and online technol-
ogy management application. In Proc. of LREC 2008,
Marrakech, Morocco.
Z Wu and M Palmer. 1994. Verb selection and lexical
selection. In Proc. of ACL?94, pages 133?138, Las
Cruces, USA.
108
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229?237,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Language Identification: The Long and the Short of the Matter
Timothy Baldwin and Marco Lui
Dept of Computer Science and Software Engineering
University of Melbourne, VIC 3010 Australia
tb@ldwin.net, saffsd@gmail.com
Abstract
Language identification is the task of identify-
ing the language a given document is written
in. This paper describes a detailed examina-
tion of what models perform best under dif-
ferent conditions, based on experiments across
three separate datasets and a range of tokeni-
sation strategies. We demonstrate that the task
becomes increasingly difficult as we increase
the number of languages, reduce the amount
of training data and reduce the length of docu-
ments. We also show that it is possible to per-
form language identification without having to
perform explicit character encoding detection.
1 Introduction
With the growth of the worldwide web, ever-
increasing numbers of documents have become
available, in more and more languages. This growth
has been a double-edged sword, however, in that
content in a given language has become more preva-
lent but increasingly hard to find, due to the web?s
sheer size and diversity of content. While the ma-
jority of (X)HTML documents declare their charac-
ter encoding, only a tiny minority specify what lan-
guage they are written in, despite support for lan-
guage declaration existing in the various (X)HTML
standards.1 Additionally, a single encoding can gen-
erally be used to render a large number of languages
such that the document encoding at best filters out
a subset of languages which are incompatible with
the given encoding, rather than disambiguates the
source language. Given this, the need for automatic
means to determine the source language of web doc-
1http://dev.opera.com/articles/view/
mama-head-structure/
uments is crucial for web aggregators of various
types.
There is widespread misconception of language
identification being a ?solved task?, generally as
a result of isolated experiments over homogeneous
datasets with small numbers of languages (Hughes
et al, 2006; Xia et al, 2009). Part of the motivation
for this paper is to draw attention to the fact that, as
a field, we are still a long way off perfect language
identification of web documents, as evaluated under
realistic conditions.
In this paper we describe experiments on lan-
guage identification of web documents, focusing on
the broad question of what combination of tokenisa-
tion strategy and classification model achieves the
best overall performance. We additionally evalu-
ate the impact of the volume of training data and
the test document length on the accuracy of lan-
guage identification, and investigate the interaction
between character encoding detection and language
identification.
One assumption we make in this research, follow-
ing standard assumptions made in the field, is that all
documents are monolingual. This is clearly an un-
realistic assumption when dealing with general web
documents (Hughes et al, 2006), and we plan to re-
turn to investigate language identification over mul-
tilingual documents in future work.
Our contributions in this paper are: the demon-
stration that language identification is: (a) trivial
over datasets with smaller numbers of languages
and approximately even amounts of training data per
language, but (b) considerably harder over datasets
with larger numbers of languages with more skew
in the amount of training data per language; byte-
based tokenisation without character encoding de-
tection is superior to codepoint-based tokenisation
229
with character encoding detection; and simple co-
sine similarity-based nearest neighbour classifica-
tion is equal to or better than models including sup-
port vector machines and naive Bayes over the lan-
guage identification task. We also develop datasets
to facilitate standardised evaluation of language
identification.
2 Background Research
Language identification was arguably established as
a task by Gold (1967), who construed it as a closed
class problem: given data in each of a predefined set
of possible languages, human subjects were asked
to classify the language of a given test document. It
wasn?t until the 1990s, however, that the task was
popularised as a text categorisation task.
The text categorisation approach to language
identification applies a standard supervised classi-
fication framework to the task. Perhaps the best-
known such model is that of Cavnar and Tren-
kle (1994), as popularised in the textcat tool.2
The method uses a per-language character frequency
model, and classifies documents via their relative
?out of place? distance from each language (see
Section 5.1). Variants on this basic method in-
clude Bayesian models for character sequence pre-
diction (Dunning, 1994), dot products of word fre-
quency vectors (Darnashek, 1995) and information-
theoretic measures of document similarity (Aslam
and Frost, 2003; Martins and Silva, 2005). More
recently, support vector machines (SVMs) and ker-
nel methods have been applied to the task of lan-
guage identification task with success (Teytaud and
Jalam, 2001; Lodhi et al, 2002; Kruengkrai et al,
2005), and Markov logic has been used for joint in-
ferencing in contexts where there are multiple evi-
dence sources (Xia et al, 2009).
Language identification has also been carried out
via linguistically motivated models. Johnson (1993)
used a list of stop words from different languages to
identify the language of a given document, choos-
ing the language with the highest stop word over-
lap with the document. Grefenstette (1995) used
word and part of speech (POS) correlation to de-
termine if two text samples were from the same
or different languages. Giguet (1995) developed a
2http://www.let.rug.nl/vannoord/TextCat/
cross-language tokenisation model and used it to
identify the language of a given document based
on its tokenisation similarity with training data.
Dueire Lins and Gonc?alves (2004) considered the
use of syntactically-derived closed grammatical-
class models, matching syntactic structure rather
than words or character sequences.
The observant reader will have noticed that some
of the above approaches make use of notions such
as ?word?, typically based on the naive assumption
that the language uses white space to delimit words.
These approaches are appropriate in contexts where
there is a guarantee of a document being in one of
a select set of languages where words are space-
delimited, or where manual segmentation has been
performed (e.g. interlinear glossed text). However,
we are interested in language identification of web
documents, which can be in any language, includ-
ing languages that do not overtly mark word bound-
aries, such as Japanese, Chinese and Thai; while
relatively few languages fall into this categories,
they are among the most populous web languages
and therefore an important consideration. There-
fore, approaches that assume a language is space-
delimited are clearly not suitable for our purposes.
Equally, approaches which make assumptions about
the availability of particular resources for each lan-
guage to be identified (e.g. POS taggers, or the ex-
istence of precompiled stop word lists) cannot be
used.
Language identification has been applied in a
number of contexts, the most immediate applica-
tion being in multilingual text retrieval, where re-
trieval results are generally superior if the language
of the query is known, and the search is restricted
to only those documents predicted to be in that lan-
guage (McNamee and Mayfield, 2004). It can also
be used to ?word spot? foreign language terms in
multilingual documents, e.g. to improve parsing per-
formance (Alex et al, 2007), or for linguistic corpus
creation purposes (Baldwin et al, 2006; Xia et al,
2009; Xia and Lewis, 2009).
3 Datasets
In the experiments reported in this paper, we em-
ploy three novel datasets, with differing properties
relevant to language identification research:
230
Corpus Documents Languages Encodings Document Length (bytes)
EUROGOV 1500 10 1 17460.5?39353.4
TCL 3174 60 12 2623.2?3751.9
WIKIPEDIA 4963 67 1 1480.8?4063.9
Table 1: Summary of the three language identification datasets
Figure 1: Distribution of languages in the three datasets
(vector of languages vs. the proportion of documents in
that language)
EUROGOV: longer documents, all in a single en-
coding, spread evenly across a relatively small num-
ber (10) of Western European languages; this dataset
is comparable to the datasets conventionally used in
language identification research. As the name would
suggest, the documents were sourced from the Euro-
GOV document collection, as used in the 2005 Web-
CLEF task.
TCL: a larger number of languages (60) across a
wider range of language families, with shorter docu-
ments and a range of character encodings (12). The
collection was manually sourced by the Thai Com-
putational Linguistics Laboratory (TCL) in 2005
from online news sources.
WIKIPEDIA: a slightly larger number of lan-
guages again (67), a single encoding, and shorter
documents; the distribution of languages is intended
to approximate that of the actual web. This col-
lection was automatically constructed by taking the
dumps of all versions of Wikipedia with 1000 or
more documents in non-constructed languages, and
randomly selecting documents from them in a bias-
preserving manner (i.e. preserving the document
distribution in the full collection); this is intended to
represent the document language bias observed on
the web. All three corpora are available on request.
We outline the characteristics of the three datasets
in Table 1. We further detail the language distri-
bution in Figure 1, using a constant vector of lan-
guages for all three datasets, based on the order of
languages in the WIKIPEDIA dataset (in descending
order of documents per language). Of note are the
contrasting language distributions between the three
datasets, in terms of both the languages represented
and the relative skew of documents per language. In
the following sections, we provide details of the cor-
pus compilation and document sampling method for
each dataset.
4 Document Representation
As we are interested in performing language iden-
tification over arbitrary web documents, we re-
quire a language-neutral document representation
which does not make artificial assumptions about the
source language of the document. Separately, there
is the question of whether it is necessary to deter-
mine the character encoding of the document in or-
der to extract out character sequences, or whether
the raw byte stream is sufficient. To explore this
question, we experiment with two document repre-
sentations: (1) byte n-grams, and (2) codepoint n-
grams. In both cases, a document is represented as a
feature vector of token counts.
Byte n-grams can be extracted directly without
explicit encoding detection. Codepoint n-grams, on
the other hand, require that we know the character
encoding of the document in order to perform to-
kenisation. Additionally, they should be based on a
common encoding to prevent: (a) over-fragmenting
the feature space (e.g. ending up with discrete fea-
ture spaces for euc-jp, s-jis and utf-8 in
the case of Japanese); and (b) spurious matches be-
tween encodings (e.g. Japanese hiragana and Ko-
rean hangul mapping onto the same codepoint in
euc-jp and euc-kr, respectively). We use uni-
231
code as the common encoding for all documents.
In practice, character encoding detection is an is-
sue only for TCL, as the other two datasets are in
a single encoding. Where a character encoding was
provided for a document in TCL and it was possi-
ble to transcode the document to unicode based on
that encoding, we used the encoding information. In
cases where a unique encoding was not provided,
we used an encoding detection library based on the
Mozilla browser.3 Having disambiguated the encod-
ing for each document, we transcoded it into uni-
code.
5 Models
In our experiments we use a number of different
language identification models, as outlined below.
We first describe the nearest-neighbour and nearest-
prototype models, and a selection of distance and
similarity metrics combined with each. We then
present three standalone text categorisation models.
5.1 Nearest-Neighbour and Nearest-Prototype
Models
The 1-nearest-neighbour (1NN) model is a common
classification technique, whereby a test document
D is classified based on the language of the clos-
est training document Di (with language l(Di)), as
determined by a given distance or similarity metric.
In nearest-neighbour models, each training doc-
ument is represented as a single instance, mean-
ing that the computational cost of classifying a test
document is proportional to the number of training
documents. A related model which aims to reduce
this cost is nearest-prototype (AM), where each lan-
guage is represented as a single instance, by merging
all of the training instances for that language into a
single centroid via the arithmetic mean.
For both nearest-neighbour and nearest-prototype
methods, we experimented with three similarity and
distance measures in this research:
Cosine similarity (COS): the cosine of the angle
between two feature vectors, as measured by the dot
product of the two vectors, normalised to unit length.
Skew divergence (SKEW): a variant of Kullback-
Leibler divergence, whereby the second distribution
3http://chardet.feedparser.org/
(y) is smoothed by linear interpolation with the first
(x) using a smoothing factor ? (Lee, 2001):
s?(x, y) = D(x || ?y + (1? ?)x)
where:
D(x || y) =
?
i
xi(log2 xi ? log2 yi)
In all our experiments, we set ? to 0.99.
Out-of-place (OOP): a ranklist-based distance
metric, where the distance between two documents
is calculated as (Cavnar and Trenkle, 1994):
oop(Dx, Dy) =
?
t?Dx?Dy
abs(RDx(t)?RDy(t))
RD(t) is the rank of term t in document D, based
on the descending order of frequency in document
D; terms not occurring in document D are conven-
tionally given the rank 1 + maxi RD(ti).
5.2 Naive Bayes (NB)
Naive Bayes is a popular text classification model,
due to it being lightweight, robust and easy to up-
date. The language of test document D is predicted
by:
l?(D) = arg max
li?L
P (li)
|V |
?
j=1
P (tj |li)ND,tj
ND,tj !
where L is the set of languages in the training data,
ND,tj is the frequency of the jth term in D, V is the
set of all terms, and:
P (t|li) =
1 +
?|D |
k=1 Nk,tP (li|Dk)
|V |+ ?|V |j=1
?|D |
k=1 Nk,tjP (li|Dk)
In this research, we use the rainbow imple-
mentation of multinominal naive Bayes (McCallum,
1996).
5.3 Support Vector Machines (SVM)
Support vector machines (SVMs) are one of the
most popular methods for text classification, largely
because they can automatically weight large num-
bers of features, capturing feature interactions in the
process (Joachims, 1998; Manning et al, 2008). The
basic principle underlying SVMs is to maximize the
232
margin between training instances and the calculated
decision boundary based on structural risk minimi-
sation (Vapnik, 1995).
In this work, we have made use of bsvm,4 an
implementation of SVMs with multiclass classifica-
tion support (Hsu et al, 2008). We only report re-
sults for multi-class bound-constrained support vec-
tor machines with linear kernels, as they were found
to perform best over our data.
6 Experimental Methodology
We carry out experiments over the cross-product of
the following options, as described above:
model (?7): nearest-neighbour (COS1NN,
SKEW1NN, OOP1NN), nearest-prototype
(COSAM, SKEWAM),5 NB, SVM
tokenisation (?2): byte, codepoint
n-gram (?3): 1-gram, 2-gram, 3-gram
for a total of 42 distinct classifiers. Each classi-
fier is run across the 3 datasets (EUROGOV, TCL
and WIKIPEDIA) based on 10-fold stratified cross-
validation.
We evaluate the models using micro-averaged
precision (P?), recall (R?) and F-score (F?), as well
as macro-averaged precision (PM ), recall (RM ) and
F-score (FM ). The micro-averaged scores indicate
the average performance per document; as we al-
ways make a unique prediction per document, the
micro-averaged precision, recall and F-score are al-
ways identical (as is the classification accuracy).
The macro-averaged scores, on the other hand, indi-
cate the average performance per language. In each
case, we average the precision, recall and F-score
across the 10 folds of cross validation.6
As a baseline, we use a majority class, or ZeroR,
classifier (ZEROR), which assigns the language with
highest prior in the training data to each of the test
documents.
4http://www.csie.ntu.edu.tw/?cjlin/bsvm/
5We do not include the results for nearest-prototype classi-
fiers with the OOP distance metric as the results were consid-
erably lower than the other methods.
6Note that this means that the averaged FM is not necessar-
ily the harmonic mean of the averaged PM andRM .
Model Token PM RM FM P?/R?/F?
ZEROR ? .020 .084 .032 .100
COS1NN byte .975 .978 .976 .975
COS1NN codepoint .968 .973 .970 .971
COSAM byte .922 .938 .926 .937
COSAM codepoint .908 .930 .913 .931
SKEW1NN byte .979 .979 .979 .977
SKEW1NN codepoint .978 .978 .978 .976
SKEWAM byte .974 .972 .972 .969
SKEWAM codepoint .974 .972 .973 .970
OOP1NN byte .953 .952 .953 .949
OOP1NN codepoint .961 .960 .960 .957
NB byte .975 .973 .974 .971
NB codepoint .975 .973 .974 .971
SVM byte .989 .985 .987 .987
SVM codepoint .988 .985 .986 .987
Table 2: Results for byte vs. codepoint (bigram) tokeni-
sation over EUROGOV
7 Results
In our experiments, we first compare the different
models for fixed n-gram order, then come back to
vary the n-gram order. Subsequently, we examine
the relative performance of the different models on
test documents of differing lengths, and finally look
into the impact of the amount of training data for
a given language on the performance for that lan-
guage.
7.1 Results for the Different Models and
Tokenisation Strategies
First, we present the results for each of the classifiers
in Tables 2?4, based on byte or codepoint tokenisa-
tion and bigrams. In each case, we present the best
result in each column in bold.
The relative performance over EUROGOV and
TCL is roughly comparable for all methods barring
SKEW1NN, with near-perfect scores over all 6 eval-
uation metrics. SKEW1NN is near-perfect over EU-
ROGOV and TCL, but drops to baseline levels over
WIKIPEDIA; we return to discuss this effect in Sec-
tion 7.2.
In the case of EUROGOV, the near-perfect re-
sults are in line with our expectations for the dataset,
based on its characteristics and results reported for
comparable datasets. The results for WIKIPEDIA,
however, fall off considerably, with the best model
achieving an FM of .671 and F? of .869, due to
233
Model Token PM RM FM P?/R?/F?
ZEROR ? .003 .017 .005 .173
COS1NN byte .981 .975 .975 .982
COS1NN codepoint .931 .930 .925 .961
COSAM byte .967 .975 .965 .965
COSAM codepoint .979 .977 .974 .964
SKEW1NN byte .984 .974 .976 .987
SKEW1NN codepoint .910 .210 .320 .337
SKEWAM byte .962 .959 .950 .972
SKEWAM codepoint .968 .961 .957 .967
OOP1NN byte .964 .945 .951 .974
OOP1NN codepoint .901 .892 .893 .933
NB byte .905 .905 .896 .969
NB codepoint .722 .711 .696 .845
SVM byte .981 .973 .977 .984
SVM codepoint .979 .970 .974 .980
Table 3: Results for byte vs. codepoint (bigram) tokeni-
sation over TCL
the larger number of languages, smaller documents,
and skew in the amounts of training data per lan-
guage. All models are roughly balanced in the rel-
ative scores they attain for PM , RM and FM (i.e.
there are no models that have notably higherPM rel-
ative to RM , for example).
The nearest-neighbour models outperform the
corresponding nearest-prototype models to varying
degrees, with the one exception of SKEW1NN over
WIKIPEDIA. The nearest-prototype classifiers were
certainly faster than the nearest-neighbour classi-
fiers, by roughly an order of 10, but this is more
than outweighed by the drop in classification per-
formance. With the exception of SKEW1NN over
WIKIPEDIA, all methods were well above the base-
lines for all three datasets.
The two methods which perform consistently well
at this point are COS1NN and SVM, with COS1NN
holding up particularly well under micro-averaged
F-score while NB drops away over WIKIPEDIA, the
most skewed dataset; this is due to the biasing effect
of the prior in NB.
Looking to the impact of byte- vs. codepoint-
tokenisation on classifier performance over the three
datasets, we find that overall, bytes outperform
codepoints. This is most notable for TCL and
WIKIPEDIA, and the SKEW1NN and NB models.
Given this result, we present only results for byte-
based tokenisation in the remainder of this paper.
Model Token PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN byte .740 .646 .671 .869
COS1NN codepoint .685 .604 .625 .835
COSAM byte .587 .634 .573 .776
COSAM codepoint .486 .556 .483 .725
SKEW1NN byte .005 .013 .008 .304
SKEW1NN codepoint .006 .013 .007 .241
SKEWAM byte .605 .617 .588 .844
SKEWAM codepoint .552 .575 .532 .807
OOP1NN byte .619 .518 .548 .831
OOP1NN codepoint .598 .486 .520 .807
NB byte .496 .454 .442 .851
NB codepoint .426 .349 .360 .798
SVM byte .667 .545 .577 .845
SVM codepoint .634 .494 .536 .818
Table 4: Results for byte vs. codepoint (bigram) tokeni-
sation over WIKIPEDIA
The results for byte tokenisation of TCL are par-
ticularly noteworthy. The transcoding into unicode
and use of codepoints, if anything, hurts perfor-
mance, suggesting that implicit character encoding
detection based on byte tokenisation is the best ap-
proach: it is both more accurate and simplifies the
system, in removing the need to perform encoding
detection prior to language identification.
7.2 Results for Differing n-gram Sizes
We present results with byte unigrams, bigrams and
trigrams in Table 5 for WIKIPEDIA.7 We omit re-
sults for the other two datasets, as the overall trend is
the same as for WIKIPEDIA, with lessened relative
differences between n-gram orders due to the rela-
tive simplicity of the respective classification tasks.
SKEW1NN is markedly different to the other meth-
ods in achieving the best performance with uni-
grams, moving from the worst-performing method
by far to one of the best-performing methods. This
is the result of the interaction between data sparse-
ness and heavy-handed smoothing with the ? con-
stant. Rather than using a constant ? value for all
n-gram orders, it may be better to parameterise it
using an exponential scale such as ? = 1??n (with
7The results for OOP1NN over byte trigrams are missing
due to the computational cost associated with the method, and
our experiment hence not having run to completion at the time
of writing. Extrapolating from the results for the other two
datasets, we predict similar results to bigrams.
234
Model n-gram PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN 1 .644 .579 .599 .816
COS1NN 2 .740 .646 .671 .869
COS1NN 3 .744 .656 .680 .862
COSAM 1 .526 .543 .487 .654
COSAM 2 .587 .634 .573 .776
COSAM 3 .553 .632 .545 .761
SKEW1NN 1 .691 .598 .625 .848
SKEW1NN 2 .005 .013 .008 .304
SKEW1NN 3 .005 .013 .004 .100
SKEWAM 1 .552 .569 .532 .740
SKEWAM 2 .605 .617 .588 .844
SKEWAM 3 .551 .631 .554 .825
OOP1NN 1 .519 .446 .468 .747
OOP1NN 2 .619 .518 .548 .831
NB 1 .576 .578 .555 .778
NB 2 .496 .454 .442 .851
NB 3 .493 .435 .432 .863
SVM 1 .585 .505 .523 .812
SVM 2 .667 .545 .577 .845
SVM 3 .717 .547 .594 .840
Table 5: Results for different n-gram orders over
WIKIPEDIA
? = 0.01, e.g.), based on the n-gram order. We
leave this for future research.
For most methods, bigrams and trigrams are bet-
ter than unigrams, with the one notable exception
of SKEW1NN. In general, there is little separating
bigrams and trigrams, although the best result for is
achieved slightly more often for bigrams than for tri-
grams.
For direct comparability with Cavnar and Tren-
kle (1994), we additionally carried out a preliminary
experiment with hybrid byte n-grams (all of 1- to 5-
grams), combined with simple frequency-based fea-
ture selection of the top-1000 features for each n-
gram order. The significance of this setting is that it
is the strategy adopted by textcat, based on the
original paper of Cavnar and Trenkle (1994) (with
the one exception that we use 1000 features rather
than 300, as all methods other than OOP1NN bene-
fitted from more features). The results are shown in
Table 6.
Compared to the results in Table 5, SKEW1NN and
SKEWAM both increase markedly to achieve the best
overall results. OOP1NN, on the other hand, rises
slightly, while the remaining three methods actually
Model PM RM FM P?/R?/F?
ZEROR .004 .013 .007 .328
COS1NN .735 .664 .682 .865
COSAM .592 .626 .580 .766
SKEW1NN .789 .708 .729 .902
SKEWAM .681 .718 .680 .870
OOP1NN .697 .595 .626 .864
SVM .669 .500 .544 .832
Table 6: Results for mixed n-grams (1?5) and feature se-
lection over WIKIPEDIA (a la? Cavnar and Trenkle (1994))
drop back slightly. Clearly, there is considerably
more experimentation to be done here with mixed
n-gram models and different feature selection meth-
ods, but the results indicate that some methods cer-
tainly benefit from n-gram hybridisation and feature
selection, and also that we have been able to sur-
pass the results of Cavnar and Trenkle (1994) with
SKEW1NN in an otherwise identical framework.
7.3 Breakdown Across Test Document Length
To better understand the impact of test document
size on classification accuracy, we divided the test
documents into 5 equal-size bins according to their
length, measured by the number of tokens. We then
computed F? individually for each bin across the 10
folds of cross validation. We present the breakdown
of results for WIKIPEDIA in Figure 2.
WIKIPEDIA shows a pseudo-logarithmic growth
in F? (= P? = R?) as the test document size in-
creases. This fits with our intuition, as the model
has progressively more evidence to base the classi-
fication on. It also suggests that performance over
shorter documents appears to be the dominating fac-
tor in the overall ranking of the different methods.
In particular, COS1NN and SVM appear to be able to
classify shorter documents most reliably, leading to
the overall result of them being the best-performing
methods.
While we do not show the graph for reasons of
space, the equivalent graph for EUROGOV displays
a curious effect: F? drops off as the test documents
get longer. Error analysis of the data indicates that
this is due to longer documents being more likely
to be ?contaminated? with either data from a sec-
ond language or extra-linguistic data, such as large
tables of numbers or chemical names. This sug-
gests that all the models are brittle when the assump-
235
Figure 2: Breakdown of F? over WIKIPEDIA for test
documents of increasing length
Figure 3: Per-language FM for COS1NN, relative to the
training data size (in MB) for that language
tion of strict monolingualism is broken, or when
the document is dominated by extra-linguistic data.
Clearly, this underlines our assumption of monolin-
gual documents, and suggests multilingual language
identification is a fertile research area even in terms
of optimising performance over our ?monolingual?
datasets.
7.4 Performance Relative to Training Data Size
As a final data point in our analysis, we calculated
the FM for each language relative to the amount of
training data available for that language, and present
the results in the form of a combined scatter plot for
the three datasets in Figure 3. The differing distri-
butions of the three datasets are self-evident, with
most languages in EUROGOV (the squares) both
having reasonably large amounts of training data and
achieving high FM values, but the majority of lan-
guages in WIKIPEDIA (the crosses) having very lit-
tle data (including a number of languages with no
training data, as there is a singleton document in that
language in the dataset). As an overall trend, we can
observe that the greater the volume of training data,
the higher the FM across all three datasets, but there
is considerable variation between the languages in
terms of their FM for a given training data size (the
column of crosses for WIKIPEDIA to the left of the
graph is particularly striking).
8 Conclusions
We have carried out a thorough (re)examination of
the task of language identification, that is predict-
ing the language that a given document is written
in, focusing on monolingual documents at present.
We experimented with a total of 7 models, and
tested each over two tokenisation strategies (bigrams
vs. codepoints) and three token n-gram orders (un-
igrams, bigrams and trigrams). At the same time
as reproducing results from earlier research on how
easy the task can be over small numbers of lan-
guages with longer documents, we demonstrated
that the task becomes much harder for larger num-
bers of languages, shorter documents and greater
class skew. We also found that explicit character
encoding detection is not necessary in language de-
tection, and that the most consistent model overall
is either a simple 1-NN model with cosine similar-
ity, or an SVM with a linear kernel, using a byte
bigram or trigram document representation. We also
confirmed that longer documents tend to be easier to
classify, but also that multilingual documents cause
problems for the standard model of language identi-
fication.
Acknowledgements
This research was supported by a Google Research
Award.
References
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
236
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Javed A. Aslam and Meredith Frost. 2003. An
information-theoretic measure for document similar-
ity. In Proceedings of 26th International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 2003), pages 449?450, Toronto,
Canada.
Timothy Baldwin, Steven Bird, and Baden Hughes.
2006. Collecting low-density language materials on
the web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). http://www.ausweb.
scu.edu.au/ausweb06/edited/hughes/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Marc Darnashek. 1995. Gauging similarity with n-
grams: Language-independent categorization of text.
Science, 267:843?848.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Emmanuel Giguet. 1995. Categorization according to
language: A step toward combining linguistic knowl-
edge and statistic learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 5:447?474.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi Sta-
tistica dei Dati Testuali (JADT), pages 263?268.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2008. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence National Taiwan University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of the 10th European Confer-
ence on Machine Learning, pages 137?142, Chemnitz,
Germany.
Stephen Johnson. 1993. Solving the problem of lan-
guage recognition. Technical report, School of Com-
puter Studies, University of Leeds.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
Lillian Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Proceedings
of Artificial Intelligence and Statistics 2001 (AISTATS
2001), pages 65?72, Key West, USA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew Kachites McCallum. 1996. Bow: A toolkit for
statistical language modeling, text retrieval, classifica-
tion and clustering. http://www.cs.cmu.edu/
?mccallum/bow.
Paul McNamee and JamesMayfield. 2004. CharacterN -
gram Tokenization for European Language Text Re-
trieval. Information Retrieval, 7(1?2):73?97.
Olivier Teytaud and Radwan Jalam. 2001. Kernel-
based text categorization. In Proceedings of the
International Joint Conference on Neural Networks
(IJCNN?2001), Washington DC, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin, Germany.
Fei Xia and William Lewis. 2009. Applying NLP tech-
nologies to the collection and enrichment of language
data on the web to aid linguistic research. In Pro-
ceedings of the EACL 2009 Workshop on Language
Technology and Resources for Cultural Heritage, So-
cial Sciences, Humanities, and Education (LaTeCH ?
SHELT&R 2009), pages 51?59, Athens, Greece.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proceedings of the 12th Conference of the
EACL (EACL 2009), pages 870?878, Athens, Greece.
237
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 372?376,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Evaluating a Morphological Analyser of Inuktitut
Jeremy Nicholson?, Trevor Cohn? and Timothy Baldwin?
?Department of Computing and Information Systems, The University of Melbourne, Australia
?Department of Computer Science, The University of Sheffield, UK
jeremymn@csse.unimelb.edu.au, tcohn@dcs.shef.ac.uk, tb@ldwin.net
Abstract
We evaluate the performance of an morpho-
logical analyser for Inuktitut across a medium-
sized corpus, where it produces a useful anal-
ysis for two out of every three types. We
then compare its segmentation to that of sim-
pler approaches to morphology, and use these
as a pre-processing step to a word alignment
task. Our observations show that the richer ap-
proaches provide little as compared to simply
finding the head, which is more in line with
the particularities of the task.
1 Introduction
In this work, we evaluate a morphological analyser
of Inuktitut, whose polysynthetic morphosyntax can
cause particular problems for natural language pro-
cessing; but our observations are also relevant to
other languages with rich morphological systems.
The existing NLP task for Inuktitut is that of word
alignment (Martin et al, 2005), where Inuktitut to-
kens align to entire English clauses. While Langlais
et al (2005) theorises that a morphological analyser
could aid in this task, we observed little to no im-
provement over a baseline model by making use of
its segmentation. Nonetheless, morphological anal-
ysis does provide a great deal of information, but the
task structure tends to disprefer its contribution.
2 Background
2.1 Inuktitut
Inuktitut is a macrolanguage of many more-or-less
mutually intelligible dialects (Gordon, 2005). The
morphosyntax of Inuktitut is particularly marked by
a rich polysynthetic suffixing morphology, including
incorporation of arguments into verbal tokens, as in
natsiviniqtulauqsimavilli in (1). This phenomenon
causes an individual token in Inuktitut to be approx-
imately equivalent to an entire clause in English.
(1) natsiq-
seal
-viniq-
meat
-tuq-
eat
-lauq-
before
-sima-
ever
-vit
INT-2s
-li
but
?But have you ever eaten seal meat before??
Lowe (1996) analyses the morphology as a four-
place relationship: one head morpheme, zero or
more lexical morphemes, one or more grammatical
morphemes, and an optional enclitic. The morpho-
tactics causes, amongst other phenomena, the final
consonant of a morpheme to assimilate the manner
of the initial consonant of the following morpheme
(as in -villi), or to be dropped (as in natsiviniq-).
Consequently, morphemes are not readily accessible
from the realised surface form, thereby motivating
the use of a morphological analyser.
2.2 Morphological analysis
For many languages with a less rich morphol-
ogy than Inuktitut, an inflectional lexicon is of-
ten adequate for morphological analysis (for exam-
ple, CELEX for English (Burnage, 1990), Lefff for
French (Sagot et al, 2006) or Adolphs (2008) for
German). Another typical approach is to perform
morphological analysis at the same time as POS tag-
ging (as in Hajic? and Hladka? (1998) for the fusional
morphology in Czech), as it is often the case that
372
determining the part-of-speech and choosing the ap-
propriate inflectional paradigm are closely linked.
For highly inflecting languages more generally,
morphological analysis is often treated as a segment-
and-normalise problem, amenable to analysis by
weighted finite state transducer (wFST), for exam-
ple, Creutz and Lagus (2002) for Finnish.
3 Resources
3.1 A morphological analyser for Inuktitut
The main resource that we are evaluating in this
work is a morphological analyser of Inuktitut called
Uqa?Ila?Ut.1 It is a rule-based system based on reg-
ular morphological variations of about 3200 head,
350 lexical, and 1500 grammatical morphemes, with
heuristics for ranking the various readings. The head
and lexical morphemes are collated with glosses in
both English and French.
3.2 Word alignment
The training corpus we use in our experiments is a
sentence-aligned segment of the Nunavut Hansards
(Martin et al, 2003). The corpus consists of about
340K sentences, which comprise about 4.0M En-
glish tokens, and 2.2M Inuktitut. The challenge of
the morphology becomes apparent when we contrast
these figures with the types: about 416K for Inukti-
tut, but only 27K for English. On average, there are
only 5 token instances per Inuktitut type; some 338K
types (81%) are singletons.
Inuktitut formed part of one of the shared tasks
in the ACL 2005 workshop on building and us-
ing parallel texts (Martin et al, 2005); for this, the
above corpus was simplistically tokenised, and used
as unsupervised training data. 100 sentences from
this corpus were phrasally aligned by Inuit anno-
tators. These were then extended into word align-
ments, where phrasal alignments of one token in
both the source and target were (generally) called
sure alignments, and one-to-many or many-to-many
mappings were extended to their cartesian product,
and called probable. The test set was composed of
75 of these sentences (about 2K English tokens, 800
Inuktitut tokens, 293 gold-standard sure alignments,
1http://inuktitutcomputing.ca/Uqailaut/
en/IMA.html
and 1679 probable), which we use to evaluate word
alignments.
Our treatment of the alignment problem is most
similar to Schafer and Dra?bek (2005) who examine
four systems: GIZA++ models (Och and Ney, 2000)
for each source-target direction, another where the
Inuktitut input has been syllabised, and a wFST
model. They observe that aggregating these results
through voting can create a very competitive system
for Inuktitut word alignment.
4 Experimental approach
We used an out-of-the-box implementation of the
Berkeley Aligner (DeNero and Klein, 2007), a com-
petitive word alignment system, to construct an un-
supervised alignment over the 75 test sentences,
based on the larger training corpus. The default
implementation of the system involves two jointly-
trained HMMs (one for each source-target direc-
tion) over five iterations,2 with so-called compet-
itive thresholding in the decoding step; these are
more fully described in DeNero and Klein (2007)
and Liang et al (2006).
Our approach examines morphological pre-
processing of the Inuktitut training and test sets,
with the idea of leveraging the morphological in-
formation into a corpus which is more amenable to
alignment. The raw corpus appears to be under-
segmented, where data sparseness from the many
singletons would prevent reliable alignments. Seg-
mentation might aid in this process by making sub-
lexical units with semantic overlap transparent to the
alignment system, so that types appear to have a
greater frequency through the data. Through this,
we attempt to examine the hypothesis that one-to-
one alignments between English and Inuktitut would
hold with the right segmentation. On the other hand,
oversegmentation (for example, down to the charac-
ter level) can leave the resulting sub-lexical items se-
mantically meaningless and cause spurious matches.
We consider five different ways of tackling Inuk-
titut morphology:
1. None: simply treat each Inuktitut token as a
monolithic entity. This is our baseline ap-
proach.
2Better performance was observed with three iterations, but
we preferred to maintain the default parameters of the system.
373
2. Head: attempt to separate the head morpheme
from the non-head periphery. Our hypothesis
is that we will be able to align the clausal head
more reliably, as it tends to correspond to a sin-
gle English token more reliably than the other
morphemes, which may not be realised in the
same manner in English. Head morphs in Inuk-
titut correspond to the first one or two syllables
of a token; we treated them uniformly as two
syllables, as other values caused a substantial
degredation in performance.
3. Syllabification: treat the text as if Inuktitut
had isolating morphology, and transform each
token into a series of single-syllable pseudo-
morphs. This effectively turns the task on its
head, from a primarily one Inukitut-to-many
English token problem to that of one English-
to-many Inuktitut. Despite the overzealousness
of this approach (as most Inuktitut morphemes
are polysyllabic, and consequently there will be
many plausible but spurious matches between
tokens that share a syllable but no semantics),
Schafer and Dra?bek (2005) observed it to be
quite competitive.
4. Morphs: segment each word into morphs,
thereby treating the morphology problem as
pure segmentation. This uses the top output of
the morphological analyser as the oracle seg-
mentation of each Inuktitut token.
5. Morphemes: as previous, except include the
normalisation of each morph to a morpheme,
as provided by the morphological analyser, as
a sort of ?lemmatisation? step. The major ad-
vantage over the morph approach is due to the
regular morphophonemic effects in Inuktitut,
which cause equivalent morphemes to have dif-
ferent surface realisations.
5 Results
5.1 Analyser
In our analysis, the morphological analyser finds at
least one reading for about 218K (= about 65%) of
the Inuktitut types. Of the 120K types without read-
ings, resource contraints account for about 11K. 3
Another 6K types caused difficulties due to punctu-
ation, numerical characters or encoding issues, all of
which could be handled through more sophisticated
tokenisation.
A more interesting cause of gaps for
the analyser was typographical errors (e.g.
*kiinaujaqtaaruasirnirmut for kiinaujaqtaarusiar-
nirmut ?requests for proposals?). This was often
due to consonant gemination, where it was either
missing (e.g. nunavummut ?in Nunavut? appeared
in the corpus as *nunavumut) or added (e.g.
*tamakkununnga instead of tamakkununga ?at
these ones here?). While one might expect these
kinds of error to be rare, because Inuktitut has an
orthography that closely reflects pronunciation,
they instead are common, which means that the
morphological analyser should probably accept
incorrect gemination with a lower weighting.
More difficult to analyse directly is the impact
of foreign words (particularly names) ? these are
typically subjectively transliterated based on Inukti-
tut morphophonology. Schafer and Dra?bek (2005)
use these as motivation for an approach based on
a wFST, but found few instances to analyse its ac-
curacy. Finally, there are certainly missing roots,
and possibly some missing affixes as well, for ex-
ample pirru- ?accident? (cf. pirruaqi- ?to have an
accident?). Finding these automatically remains as
future work.
As for tokens, we briefly analysed the 768 tokens
in the test set, of which 228 (30%) were not given
a reading. Punctuation (typically commas and peri-
ods) account for 117 of these, and numbers another
7. Consonant gemination and foreign words cause
gaps for at least 16 and 6 tokens, respectively (that
we could readily identify).
5.2 Word Alignment
Following Och and Ney (2000), we assess using
alignment error rate (AER) and define precision with
respect to the probable set, and recall with respect to
3We only attempted to parse tokens of 30 characters or
shorter; longer tokens tended to cause exceptions ? this could
presumably be improved with a more efficient analyser. While
the number of analyses will continue to grow with the token
length, which has implications in agglutinative languages, here
there are only about 300 tokens of length greater than 40.
374
Approach Prec Rec AER
None 0.783 0.863 0.195
Head 0.797 0.922 0.176
Syllabification 0.789 0.881 0.192
Morphs 0.777 0.860 0.207
Morphemes 0.777 0.863 0.206
S&D E-I 0.646 0.829 0.327
S&D Syll 0.849 0.826 0.156
Table 1: Precision, recall, and alignment error rate for
various approaches to morphology, with Schafer and
Dra?bek (2005) for comparison
the sure set.
We present word alignment results of the vari-
ous methods ? contrasted with Schafer and Dra?bek
(2005) ? in Table 1. The striking result is in
terms of statistical significance: according to ?2,
most of the various approaches to morphology fail
to give a significantly (P < 0.05) different result
to the baseline system of using entire tokens. For
comparison, whereas our baseline system is signifi-
cantly better than the baseline system of Schafer and
Dra?bek (2005) ? which demonstrates the value that
the Berkeley Aligner provides by training in both
source-target directions ? their syllablised model
is significantly superior in precision (P < 0.001),
while their recall is still worse than our model (P <
0.05). Intuitively, this seems to indicate that their
model is making fewer judgments, but actually the
opposite is true. It seems that their model achieves
better performance than ours because it leverages
many candidate probable alignments into high qual-
ity aggregates using a most-likely heuristic on the
mapping of Inuktitut syllables to English words,
whereas the Berkeley Aligner culls the candidate set
in joint training.
Of the approaches toward morphology that we
consider, only the recall of the head?based sys-
tem improves upon the baseline (P < 0.025).
This squares with our intuitions, where segment-
ing the root morpheme from the larger token al-
lows for more effective alignment of the semanti-
cally straightforward sure alignments.
The three systems that involve a finer segmenta-
tion over the tokens are equivalent in performance to
the baseline system. The oversegmentation seemed
to caused the alignment system to abandon an im-
plicit preference for monotonicity of the order of
tokens between the source and target (which holds
pretty well for the baseline system over the test data,
thanks partly to the fidelity-focused structure of a
Hansard corpus): presumably because the aligner
perceives lexical similarity between disparate tokens
due to them sharing a sublexical unit. This relax-
ing of monotonicity is most apparent for punctua-
tion, where a comma with a correct alignment in the
baseline becomes incorrectly aligned to a different
comma in the sentence for the segmented system.
6 Conclusion
The only improvement toward the task that we ob-
served using morphological approaches is that of
head segmentation, where using two syllables as a
head-surrogate allowed us to capture more of the
sure (one-to-one) alignments in the test set. One
possible extension would be to take the head mor-
pheme as given the analyser, rather than the some-
what arbitrary syllabic approach. For other lan-
guages with rich morphology, it may be similarly
valuable to target substantives for segmentation to
improve alignment.
All in all, it appears that the lexical encoding of
morphology of Inuktitut is so strikingly different
than English, that the assumption of Inuktitut mor-
phemes aligning to English words is untrue or at
least unfindable within the current framework. Nu-
merous common morphemes have no English equiv-
alent, for example, -liaq- ?to go to? which seems to
act as a light verb, or -niq-, a (re-)nominaliser for
abstract nominals. While the output of the morpho-
logical analyser could probably be used more effec-
tively in other tasks, there are still important impacts
in word alignment and machine translation, includ-
ing leveraging a dictionary (which is based on mor-
phemes, not tokens, and as such requires segmenta-
tion and normalisation) or considering grammatical
forms for syntactic approaches.
References
Peter Adolphs. 2008. Acquiring a poor man?s inflec-
tional lexicon for German. In Proc. of the 6th LREC,
375
Marrakech, Morocco.
Gavin Burnage. 1990. CELEX: A guide for users. Tech-
nical report, University of Nijmegen.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proc. of the 6th Workshop
of ACL SIGPHON, pages 21?30, Philadelphia, USA.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Proc.
of the 45th Annual Meeting of the ACL, pages 17?24,
Prague, Czech Republic.
Raymund G. Gordon, Jr, editor. 2005. Ethnologue: Lan-
guages of the World, Fifteenth Edition. SIL Interna-
tional.
Jan Hajic? and Barbora Hladka?. 1998. Tagging inflective
languages: Prediction of morphological categories for
a rich, structured tagset. In Proc. of the 36th Annual
Meeting of the ACL and 17th International Conference
on COLING, pages 483?490, Montre?al, Canada.
Philippe Langlais, Fabrizio Gotti, and Guihong Cao.
2005. NUKTI: English-Inuktitut word alignment sys-
tem description. In Proc. of the ACL Workshop on
Building and Using Parallel Texts, pages 75?78, Ann
Arbor, USA.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of the HLT Conference
of the NAACL, pages 104?111, New York City, USA.
Ronald Lowe. 1996. Grammatical sketches: Inuktitut.
In Jacques Maurais, editor, Quebec?s Aboriginal Lan-
guages: History, Planning and Development, pages
204?232. Multilingual Matters.
Joel Martin, Howard Johnson, Benoit Farley, and Anna
Maclachlan. 2003. Aligning and using an English-
Inuktitut parallel corpus. In Proc. of the HLT-NAACL
2003 Workshop on Building and Using Parallel Texts,
pages 115?118, Edmonton, Canada.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word alignment for languages with scarce resources.
In Proc. of the ACL Workshop on Building and Using
Parallel Texts, pages 65?74, Ann Arbor, USA.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th Annual
Meeting of the ACL, pages 440?447, Saarbru?cken,
Germany.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff syntac-
tic lexicon for French: Architecture, acquisition, use.
In Proc. of the 5th LREC, pages 1348?1351, Genoa,
Italy.
Charles Schafer and Elliott Dra?bek. 2005. Models for
Inuktitut-English word alignment. In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
79?82, Ann Arbor, USA.
376
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368?378,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexical Normalisation of Short Text Messages: Makn Sens a #twitter
Bo Han and Timothy Baldwin
NICTA Victoria Research Laboratory
Department of Computer Science and Software Engineering
The University of Melbourne
hanb@student.unimelb.edu.au tb@ldwin.net
Abstract
Twitter provides access to large volumes of
data in real time, but is notoriously noisy,
hampering its utility for NLP. In this paper, we
target out-of-vocabulary words in short text
messages and propose a method for identify-
ing and normalising ill-formed words. Our
method uses a classifier to detect ill-formed
words, and generates correction candidates
based on morphophonemic similarity. Both
word similarity and context are then exploited
to select the most probable correction can-
didate for the word. The proposed method
doesn?t require any annotations, and achieves
state-of-the-art performance over an SMS cor-
pus and a novel dataset based on Twitter.
1 Introduction
Twitter and other micro-blogging services are highly
attractive for information extraction and text mining
purposes, as they offer large volumes of real-time
data, with around 65 millions tweets posted on Twit-
ter per day in June 2010 (Twitter, 2010). The quality
of messages varies significantly, however, ranging
from high quality newswire-like text to meaningless
strings. Typos, ad hoc abbreviations, phonetic sub-
stitutions, ungrammatical structures and emoticons
abound in short text messages, causing grief for text
processing tools (Sproat et al, 2001; Ritter et al,
2010). For instance, presented with the input u must
be talkin bout the paper but I was thinkin movies
(?You must be talking about the paper but I was
thinking movies?),1 the Stanford parser (Klein and
1Throughout the paper, we will provide a normalised version
of examples as a gloss in double quotes.
Manning, 2003; de Marneffe et al, 2006) analyses
bout the paper and thinkin movies as a clause and
noun phrase, respectively, rather than a prepositional
phrase and verb phrase. If there were some way of
preprocessing the message to produce a more canon-
ical lexical rendering, we would expect the quality
of the parser to improve appreciably. Our aim in this
paper is this task of lexical normalisation of noisy
English text, with a particular focus on Twitter and
SMS messages. In this paper, we will collectively
refer to individual instances of typos, ad hoc abbre-
viations, unconventional spellings, phonetic substi-
tutions and other causes of lexical deviation as ?ill-
formed words?.
The message normalisation task is challenging.
It has similarities with spell checking (Peterson,
1980), but differs in that ill-formedness in text mes-
sages is often intentional, whether due to the desire
to save characters/keystrokes, for social identity, or
due to convention in this text sub-genre. We propose
to go beyond spell checkers, in performing deabbre-
viation when appropriate, and recovering the canon-
ical word form of commonplace shorthands like b4
?before?, which tend to be considered beyond the
remit of spell checking (Aw et al, 2006). The free
writing style of text messages makes the task even
more complex, e.g. with word lengthening such as
goooood being commonplace for emphasis. In ad-
dition, the detection of ill-formed words is difficult
due to noisy context.
Our objective is to restore ill-formed words to
their canonical lexical forms in standard English.
Through a pilot study, we compared OOV words in
Twitter and SMS data with other domain corpora,
368
revealing their characteristics in OOV word distri-
bution. We found Twitter data to have an unsur-
prisingly long tail of OOV words, suggesting that
conventional supervised learning will not perform
well due to data sparsity. Additionally, many ill-
formed words are ambiguous, and require context
to disambiguate. For example, Gooood may refer to
Good or God depending on context. This provides
the motivation to develop a method which does not
require annotated training data, but is able to lever-
age context for lexical normalisation. Our approach
first generates a list of candidate canonical lexical
forms, based on morphological and phonetic vari-
ation. Then, all candidates are ranked according
to a list of features generated from noisy context
and similarity between ill-formed words and can-
didates. Our proposed cascaded method is shown
to achieve state-of-the-art results on both SMS and
Twitter data.
Our contributions in this paper are as follows: (1)
we conduct a pilot study on the OOV word distri-
bution of Twitter and other text genres, and anal-
yse different sources of non-standard orthography in
Twitter; (2) we generate a text normalisation dataset
based on Twitter data; (3) we propose a novel nor-
malisation approach that exploits dictionary lookup,
word similarity and word context, without requir-
ing annotated data; and (4) we demonstrate that our
method achieves state-of-the-art accuracy over both
SMS and Twitter data.
2 Related work
The noisy channel model (Shannon, 1948) has tradi-
tionally been the primary approach to tackling text
normalisation. Suppose the ill-formed text is T
and its corresponding standard form is S, the ap-
proach aims to find argmaxP (S|T ) by comput-
ing argmaxP (T |S)P (S), in which P (S) is usu-
ally a language model and P (T |S) is an error model.
Brill and Moore (2000) characterise the error model
by computing the product of operation probabilities
on slice-by-slice string edits. Toutanova and Moore
(2002) improve the model by incorporating pronun-
ciation information. Choudhury et al (2007) model
the word-level text generation process for SMS mes-
sages, by considering graphemic/phonetic abbrevi-
ations and unintentional typos as hidden Markov
model (HMM) state transitions and emissions, re-
spectively (Rabiner, 1989). Cook and Stevenson
(2009) expand the error model by introducing infer-
ence from different erroneous formation processes,
according to the sampled error distribution. While
the noisy channel model is appropriate for text nor-
malisation, P (T |S), which encodes the underlying
error production process, is hard to approximate
accurately. Additionally, these methods make the
strong assumption that a token ti ? T only depends
on si ? S, ignoring the context around the token,
which could be utilised to help in resolving ambigu-
ity.
Statistical machine translation (SMT) has been
proposed as a means of context-sensitive text nor-
malisation, by treating the ill-formed text as the
source language, and the standard form as the target
language. For example, Aw et al (2006) propose a
phrase-level SMT SMS normalisation method with
bootstrapped phrase alignments. SMT approaches
tend to suffer from a critical lack of training data,
however. It is labor intensive to construct an anno-
tated corpus to sufficiently cover ill-formed words
and context-appropriate corrections. Furthermore,
it is hard to harness SMT for the lexical normali-
sation problem, as even if phrase-level re-ordering
is suppressed by constraints on phrase segmenta-
tion, word-level re-orderings within a phrase are still
prevalent.
Some researchers have also formulated text nor-
malisation as a speech recognition problem. For ex-
ample, Kobus et al (2008) firstly convert input text
tokens into phonetic tokens and then restore them to
words by phonetic dictionary lookup. Beaufort et al
(2010) use finite state methods to perform French
SMS normalisation, combining the advantages of
SMT and the noisy channel model. Kaufmann and
Kalita (2010) exploit a machine translation approach
with a preprocessor for syntactic (rather than lexical)
normalisation.
Predominantly, however, these methods require
large-scale annotated training data, limiting their
adaptability to new domains or languages. In con-
trast, our proposed method doesn?t require annotated
data. It builds on the work on SMS text normalisa-
tion, and adapts it to Twitter data, exploiting multi-
ple data sources for normalisation.
369
Figure 1: Out-of-vocabulary word distribution in English Gigaword (NYT), Twitter and SMS data
3 Scoping Text Normalisation
3.1 Task Definition of Lexical Normalisation
We define the task of text normalisation to be a map-
ping from ?ill-formed? OOV lexical items to their
standard lexical forms, focusing exclusively on En-
glish for the purposes of this paper. We define the
task as follows:
? only OOV words are considered for normalisa-
tion;
? normalisation must be to a single-token word,
meaning that we would normalise smokin to
smoking, but not imo to in my opinion; a side-
effect of this is to permit lower-register contrac-
tions such as gonna as the canonical form of
gunna (given that going to is out of scope as a
normalisation candidate, on the grounds of be-
ing multi-token).
Given this definition, our first step is to identify
candidate tokens for lexical normalisation, where
we examine all tokens that consist of alphanumeric
characters, and categorise them into in-vocabulary
(IV) and out-of-vocabulary (OOV) words, relative to
a dictionary. The OOV word definition is somewhat
rough, because it includes neologisms and proper
nouns like hopeable or WikiLeaks which have not
made their way into the dictionary. However, it
greatly simplifies the candidate identification task,
at the cost of pushing complexity downstream to
the word detection task, in that we need to explic-
itly distinguish between correct OOV words and ill-
formed OOV words such as typos (e.g. earthquak
?earthquake?), register-specific single-word abbre-
viations (e.g. lv ?love?), and phonetic substitutions
(e.g. 2morrow ?tomorrow?).
An immediate implication of our task definition is
that ill-formed words which happen to coincide with
an IV word (e.g. the misspelling of can?t as cant) are
outside the scope of this research. We also consider
that deabbreviation largely falls outside the scope of
text normalisation, as abbreviations can be formed
freely in standard English. Note that single-word
abbreviations such as govt ?government? are very
much within the scope of lexical normalisation, as
they are OOV and match to a single token in their
standard lexical form.
Throughout this paper, we use the GNU aspell
dictionary (v0.60.6)2 to determine whether a token
is OOV. In tokenising the text, hyphenanted tokens
and tokens containing apostrophes (e.g. take-off and
won?t, resp.) are treated as a single token. Twit-
ter mentions (e.g. @twitter), hashtags (e.g. #twitter)
and urls (e.g. twitter.com) are excluded from consid-
eration for normalisation, but left in situ for context
modelling purposes. Dictionary lookup of Internet
slang is performed relative to a dictionary of 5021
items collected from the Internet.3
3.2 OOV Word Distribution and Types
To get a sense of the relative need for lexical nor-
malisation, we perform analysis of the distribution
of OOV words in different text types. In particular,
we calculate the proportion of OOV tokens per mes-
sage (or sentence, in the case of edited text), bin the
messages according to the OOV token proportion,
and plot the probability mass contained in each bin
for a given text type. The three corpora we compare
2We remove all one character tokens, except a and I, and
treat RT as an IV word.
3http://www.noslang.com
370
are the New York Times (NYT),4 SMS,5 and Twit-
ter.6 The results are presented in Figure 1.
Both SMS and Twitter have a relatively flat distri-
bution, with Twitter having a particularly large tail:
around 15% of tweets have 50% or more OOV to-
kens. This has implications for any context mod-
elling, as we cannot rely on having only isolated oc-
currences of OOV words. In contrast, NYT shows a
more Zipfian distribution, despite the large number
of proper names it contains.
While this analysis confirms that Twitter and SMS
are similar in being heavily laden with OOV tokens,
it does not shed any light on the relative similarity in
the makeup of OOV tokens in each case. To further
analyse the two data sources, we extracted the set
of OOV terms found exclusively in SMS and Twit-
ter, and analysed each. Manual analysis of the two
sets revealed that most OOV words found only in
SMS were personal names. The Twitter-specific set,
on the other hand, contained a heterogeneous col-
lection of ill-formed words and proper nouns. This
suggests that Twitter is a richer/noisier data source,
and that text normalisation for Twitter needs to be
more nuanced than for SMS.
To further analyse the ill-formed words in Twit-
ter, we randomly selected 449 tweets and manu-
ally analysed the sources of lexical variation, to
determine the phenomena that lexical normalisa-
tion needs to deal with. We identified 254 to-
ken instances of lexical normalisation, and broke
them down into categories, as listed in Table 1.
?Letter? refers to instances where letters are miss-
ing or there are extraneous letters, but the lexi-
cal correspondence to the target word form is triv-
ially accessible (e.g. shuld ?should?). ?Number
Substitution? refers to instances of letter?number
substitution, where numbers have been substituted
for phonetically-similar sequences of letters (e.g. 4
?for?). ?Letter&Number? refers to instances which
have both extra/missing letters and number substitu-
tion (e.g. b4 ?before?). ?Slang? refers to instances
4Based on 44 million sentences from English Gigaword.
5Based on 12.6 thousand SMS messages from How and Kan
(2005) and Choudhury et al (2007).
6Based on 1.37 million tweets collected from the Twitter
streaming API from Aug to Oct 2010, and filtered for mono-
lingual English messages; see Section 5.1 for details of the lan-
guage filtering methodology.
Category Ratio
Letter&Number 2.36%
Letter 72.44%
Number Substitution 2.76%
Slang 12.20%
Other 10.24%
Table 1: Ill-formed word distribution
of Internet slang (e.g. lol ?laugh out loud?), as found
in a slang dictionary (see Section 3.1). ?Other? is
the remainder of the instances, which is predomi-
nantly made up of occurrences of spaces having be-
ing deleted between words (e.g. sucha ?such a?). If
a given instance belongs to multiple error categories
(e.g. ?Letter&Number? and it is also found in a slang
dictionary), we classify it into the higher-occurring
category in Table 1.
From Table 1, it is clear that ?Letter? accounts
for the majority of ill-formed words in Twitter, and
that most ill-formed words are based on morpho-
phonemic variations. This empirical finding assists
in shaping our strategy for lexical normalisation.
4 Lexical normalisation
Our proposed lexical normalisation strategy in-
volves three general steps: (1) confusion set gen-
eration, where we identify normalisation candidates
for a given word; (2) ill-formed word identification,
where we classify a word as being ill-formed or not,
relative to its confusion set; and (3) candidate selec-
tion, where we select the standard form for tokens
which have been classified as being ill formed. In
confusion set generation, we generate a set of IV
normalisation candidates for each OOV word type
based on morphophonemic variation. We call this
set the confusion set of that OOV word, and aim to
include all feasible normalisation candidates for the
word type in the confusion set. The confusion can-
didates are then filtered for each token occurrence of
a given OOV word, based on their local context fit
with a language model.
4.1 Confusion Set Generation
Revisiting our manual analysis from Section 3.2,
most ill-formed tokens in Twitter are morphophone-
mically derived. First, inspired by Kaufmann and
Kalita (2010), any repititions of more than 3 let-
ters are reduced back to 3 letters (e.g. cooool is re-
371
Criterion Recall AverageCandidates
Tc ? 1 40.4% 24
Tc ? 2 76.6% 240
Tp = 0 55.4% 65
Tp ? 1 83.4% 1248
Tp ? 2 91.0% 9694
Tc ? 2 ? Tp ? 1 88.8% 1269
Tc ? 2 ? Tp ? 2 92.7% 9515
Table 2: Recall and average number of candidates for dif-
ferent confusion set generation strategies
duced to coool). Second, IV words within a thresh-
old Tc character edit distance of the given OOV
word are calculated, as is widely used in spell check-
ers. Third, the double metaphone algorithm (Philips,
2000) is used to decode the pronunciation of all IV
words, and IV words within a threshold Tp edit dis-
tance of the given OOV word under phonemic tran-
scription, are included in the confusion set; this al-
lows us to capture OOV words such as earthquick
?earthquake?. In Table 2, we list the recall and av-
erage size of the confusion set generated by the fi-
nal two strategies with different threshold settings,
based on our evaluation dataset (see Section 5.1).
The recall for lexical edit distance with Tc ? 2 is
moderately high, but it is unable to detect the correct
candidate for about one quarter of words. The com-
bination of the lexical and phonemic strategies with
Tc ? 2?Tp ? 2 is more impressive, but the number
of candidates has also soared. Note that increasing
the edit distance further in both cases leads to an ex-
plosion in the average number of candidates, with
serious computational implications for downstream
processing. Thankfully, Tc ? 2?Tp ? 1 leads to an
extra increment in recall to 88.8%, with only a slight
increase in the average number of candidates. Based
on these results, we use Tc ? 2?Tp ? 1 as the basis
for confusion set generation.
Examples of ill-formed words where we are un-
able to generate the standard lexical form are clip-
pings such as fav ?favourite? and convo ?conversa-
tion?.
In addition to generating the confusion set, we
rank the candidates based on a trigram language
model trained over 1.5GB of clean Twitter data, i.e.
tweets which consist of all IV words: despite the
prevalence of OOV words in Twitter, the sheer vol-
ume of the data means that it is relatively easy to col-
lect large amounts of all-IV messages. To train the
language model, we used SRILM (Stolcke, 2002)
with the -<unk> option. If we truncate the ranking
to the top 10% of candidates, the recall drops back
to 84% with a 90% reduction in candidates.
4.2 Ill-formed Word Detection
The next step is to detect whether a given OOVword
in context is actually an ill-formed word or not, rel-
ative to its confusion set. To the best of our knowl-
edge, we are the first to target the task of ill-formed
word detection in the context of short text messages,
although related work exists for text with lower rel-
ative occurrences of OOV words (Izumi et al, 2003;
Sun et al, 2007). Due to the noisiness of the data, it
is impractical to use full-blown syntactic or seman-
tic features. The most direct source of evidence is
IV words around an OOV word. Inspired by work
on labelled sequential pattern extraction (Sun et al,
2007), we exploit large-scale edited corpus data to
construct dependency-based features.
First, we use the Stanford parser (Klein and Man-
ning, 2003; de Marneffe et al, 2006) to extract de-
pendencies from the NYT corpus (see Section 3.2).
For example, from a sentence such as One obvious
difference is the way they look, we would extract
dependencies such as rcmod(way-6,look-8)
and nsubj(look-8,they-7). We then trans-
form the dependencies into relational features for
each OOV word. Assuming that way were an OOV
word, e.g., we would extract dependencies of the
form (look,way,+2), indicating that look oc-
curs 2 words after way. We choose dependencies to
represent context because they are an effective way
of capturing key relationships between words, and
similar features can easily be extracted from tweets.
Note that we don?t record the dependency type here,
because we have no intention of dependency parsing
text messages, due to their noisiness and the volume
of the data. The counts of dependency forms are
combined together to derive a confidence score, and
the scored dependencies are stored in a dependency
bank.
Given the dependency-based features, a linear
kernel SVM classifier (Fan et al, 2008) is trained
on clean Twitter data, i.e. the subset of Twitter mes-
sages without OOV words. Each word is repre-
372
sented by its IV words within a context window
of three words to either side of the target word,
together with their relative positions in the form
of (word1,word2,position) tuples, and their
score in the dependency bank. These form the pos-
itive training exemplars. Negative exemplars are
automatically constructed by replacing target words
with highly-ranked candidates from their confusion
set. Note that the classifier does not require any hand
annotation, as all training exemplars are constructed
automatically.
To predict whether a given OOV word is
ill-formed, we form an exemplar for each
of its confusion candidates, and extract
(word1,word2,position) features. If
all its candidates are predicted to be negative by the
model, we mark it as correct; otherwise, we treat
it as ill-formed, and pass all candidates (not just
positively-classified candidates) on to the candidate
selection step. For example, given the message
way yu lookin shuld be a sin and the OOV word
lookin, we would generate context features for each
candidate word such as (way,looking,-2),
and classify each such candidate.
In training, it is possible for the exact same fea-
ture vector to occur as both positive and negative ex-
emplars. To prevent positive exemplars being con-
taminated from the automatic generation, we re-
move all negative instances in such cases. The
(word1,word2,position) features are sparse
and sometimes lead to conservative results in ill-
formed word detection. That is, without valid fea-
tures, the SVM classifier tends to label uncertain
cases as correct rather than ill-formed words. This
is arguably the right approach to normalisation, in
choosing to under- rather than over-normalise in
cases of uncertainty.
As the context for a target word often contains
OOV words which don?t occur in the dependency
bank, we expand the dependency features to include
context tokens up to a phonemic edit distance of 1
from context tokens in the dependency bank. In
this way, we generate dependency-based features
for context words such as seee ?see? in (seee,
flm, +2) (based on the target word flm in the
context of flm to seee). However, expanded depen-
dency features may introduce noise, and we there-
fore introduce expanded dependency weights wd ?
{0.0, 0.5, 1.0} to ameliorate the effects of noise: a
weight of wd = 0.0 means no expansion, while 1.0
means expanded dependencies are indistinguishable
from non-expanded (strict match) dependencies.
We separately introduce a threshold td ?
{1, 2, ..., 10} on the number of positive predictions
returned by the detection classifier over the set of
normalisation candidates for a given OOV token: the
token is considered to be ill-formed iff td or more
candidates are positively classified, i.e. predicted to
be correct candidates.
4.3 Candidate Selection
For OOV words which are predicted to be ill-
formed, we select the most likely candidate from the
confusion set as the basis of normalisation. The final
selection is based on the following features, in line
with previous work (Wong et al, 2006; Cook and
Stevenson, 2009).
Lexical edit distance, phonemic edit distance,
prefix substring, suffix substring, and the longest
common subsequence (LCS) are exploited to cap-
ture morphophonemic similarity. Both lexical and
phonemic edit distance (ED) are normalised by the
reciprocal of exp(ED). The prefix and suffix fea-
tures are intended to capture the fact that leading
and trailing characters are frequently dropped from
words, e.g. in cases such as ish and talkin. We cal-
culate the ratio of the LCS over the maximum string
length between ill-formed word and the candidate,
since the ill-formed word can be either longer or
shorter than (or the same size as) the standard form.
For example, mve can be restored to either me or
move, depending on context. We normalise these ra-
tios following Cook and Stevenson (2009).
For context inference, we employ both language
model- and dependency-based frequency features.
Ranking by language model score is intuitively ap-
pealing for candidate selection, but our trigram
model is trained only on clean Twitter data and ill-
formed words often don?t have sufficient context for
the language model to operate effectively, as in bt
?but? in say 2 sum1 bt nt gonna say ?say to some-
one but not going to say?. To consolidate the con-
text modelling, we obtain dependencies from the de-
pendency bank used in ill-formed word detection.
Although text messages are of a different genre to
edited newswire text, we assume they form similar
373
dependencies based on the common goal of getting
across the message effectively. The dependency fea-
tures can be used in noisy contexts and are robust
to the effects of other ill-formed words, as they do
not rely on contiguity. For example, uz ?use? in i
did #tt uz me and yu, dependencies can capture rela-
tionships like aux(use-4, do-2), which is be-
yond the capabilities of the language model due to
the hashtag being treated as a correct OOV word.
5 Experiments
5.1 Dataset and baselines
The aim of our experiments is to compare the effec-
tiveness of different methodologies over text mes-
sages, based on two datasets: (1) an SMS corpus
(Choudhury et al, 2007); and (2) a novel Twitter
dataset developed as part of this research, based on
a random sampling of 549 English tweets. The En-
glish tweets were annotated by three independent
annotators. All OOV words were pre-identified,
and the annotators were requested to determine: (a)
whether each OOV word was ill-formed or not; and
(b) what the standard form was for ill-formed words,
subject to the task definition outlined in Section 3.1.
The total number of ill-formed words contained in
the SMS and Twitter datasets were 3849 and 1184,
respectively.7
The language filtering of Twitter to automatically
identify English tweets was based on the language
identification method of Baldwin and Lui (2010),
using the EuroGOV dataset as training data, a mixed
unigram/bigram/trigram byte feature representation,
and a skew divergence nearest prototype classifier.
We reimplemented the state-of-art noisy channel
model of Cook and Stevenson (2009) and SMT ap-
proach of Aw et al (2006) as benchmark meth-
ods. We implement the SMT approach in Moses
(Koehn et al, 2007), with synthetic training and
tuning data of 90,000 and 1000 sentence pairs, re-
spectively. This data is randomly sampled from the
1.5GB of clean Twitter data, and errors are gener-
ated according to distribution of SMS corpus. The
10-fold cross-validated BLEU score (Papineni et al,
2002) over this data is 0.81.
7The Twitter dataset is available at http://www.
csse.unimelb.edu.au/research/lt/resources/
lexnorm/.
In addition to comparing our method with com-
petitor methods, we also study the contribution of
different feature groups. We separately compare dic-
tionary lookup over our Internet slang dictionary,
the contextual feature model, and the word similar-
ity feature model, as well as combinations of these
three.
5.2 Evaluation metrics
The evaluation of lexical normalisation consists of
two stages (Hirst and Budanitsky, 2005): (1) ill-
formed word detection, and (2) candidate selection.
In terms of detection, we want to make sense of how
well the system can identify ill-formed words and
leave correct OOV words untouched. This step is
crucial to further normalisation, because if correct
OOV words are identified as ill-formed, the candi-
date selection step can never be correct. Conversely,
if an ill-formed word is predicted to be correct, the
candidate selection will have no chance to normalise
it.
We evaluate detection performance by token-level
precision, recall and F-score (? = 1). Previous work
over the SMS corpus has assumed perfect ill-formed
word detection and focused only on the candidate
selection step, so we evaluate ill-formed word de-
tection for the Twitter data only.
For candidate selection, we once again evalu-
ate using token-level precision, recall and F-score.
Additionally, we evaluate using the BLEU score
over the normalised form of each message, as the
SMT method can lead to perturbations of the token
stream, vexing standard precision, recall and F-score
evaluation.
5.3 Results and Analysis
First, we test the impact of the wd and td values
on ill-formed word detection effectiveness, based on
dependencies from either the Spinn3r blog corpus
(Blog: Burton et al (2009)) or NYT. The results for
precision, recall and F-score are presented in Fig-
ure 2.
Some conclusions can be drawn from the graphs.
First, higher detection threshold values (td) give bet-
ter precision but lower recall. Generally, as td is
raised from 1 to 10, the precision improves slightly
but recall drops dramatically, with the net effect that
the F-score decreases monotonically. Thus, we use a
374
Figure 2: Ill-formed word detection precision, recall and
F-score
smaller threshold, i.e. td = 1. Second, there are dif-
ferences between the two corpora, with dependen-
cies from the Blog corpus producing slightly lower
precision but higher recall, compared with the NYT
corpus. The lower precision for the Blog corpus ap-
pears to be due to the text not being as clean as NYT,
introducing parser errors. Nevertheless, the differ-
ence in F-score between the two corpora is insignif-
icant. Third, we obtain the best results, especially
in terms of precision, for wd = 0.5, i.e. with ex-
panded dependencies, but penalised relative to non-
expanded dependencies.
Overall, the best F-score is 71.2%, with a preci-
sion of 61.1% and recall of 85.3%, obtained over
the Blog corpus with td = 1 and wd = 0.5. Clearly
there is significant room for immprovements in these
results. We leave the improvement of ill-formed
word detection for future work, and perform eval-
uation of candidate selection for Twitter assuming
perfect ill-formed word detection, as for the SMS
data.
From Table 3, we see that the general perfor-
mance of our proposed method on Twitter is better
than that on SMS. To better understand this trend,
we examined the annotations in the SMS corpus, and
found them to be looser than ours, because they have
different task specifications than our lexical normal-
isation. In our annotation, the annotators only nor-
malised ill-formed word if they had high confidence
of how to normalise, as with talkin ?talking?. For
ill-formed words where they couldn?t be certain of
the standard form, the tokens were left untouched.
However, in the SMS corpus, annotations such as
sammis ?same? are also included. This leads to a
performance drop for our method over the SMS cor-
pus.
The noisy channel method of Cook and Stevenson
(2009) shares similar features with word similarity
(?WS?), However, when word similarity and con-
text support are combined (?WS+CS?), our method
outperforms the noisy channel method by about 7%
and 12% in F-score over SMS and Twitter corpora,
respectively. This can be explained as follows. First,
the Cook and Stevenson (2009) method is type-
based, so all token instances of a given ill-formed
word will be normalised identically. In the Twit-
ter data, however, the same word can be normalised
differently depending on context, e.g. hw ?how? in
so hw many time remaining so I can calculate it?
vs. hw ?homework? in I need to finish my hw first.
Second, the noisy channel method was developed
specifically for SMS normalisation, in which clip-
ping is the most prevalent form of lexical variation,
while in the Twitter data, we commonly have in-
stances of word lengthening for emphasis, such as
moviiie ?movie?. Having said this, our method is
superior to the noisy channel method over both the
SMS and Twitter data.
The SMT approach is relatively stable on the two
datasets, but well below the performance of our
method. This is due to the limitations of the training
data: we obtain the ill-formed words and their stan-
dard forms from the SMS corpus, but the ill-formed
words in the SMS corpus are not sufficient to cover
those in the Twitter data (and we don?t have suffi-
cient Twitter data to train the SMT method directly).
Thus, novel ill-formed words are missed in normal-
isation. This shows the shortcoming of supervised
data-driven approaches that require annotated data
to cover all possibilities of ill-formed words in Twit-
ter.
The dictionary lookup method (?DL?) unsurpris-
ingly achieves the best precision, but the recall
on Twitter is not competitive. Consequently, the
Twitter normalisation cannot be tackled with dictio-
nary lookup alone, although it is an effective pre-
processing strategy when combined with more ro-
375
Dataset Evaluation NC MT DL WS CS WS+CS DL+WS+CS
SMS
Precision 0.465 ? 0.927 0.521 0.116 0.532 0.756
Recall 0.464 ? 0.597 0.520 0.116 0.531 0.754
F-score 0.464 ? 0.726 0.520 0.116 0.531 0.755
BLEU 0.746 0.700 0.801 0.764 0.612 0.772 0.876
Twitter
Precision 0.452 ? 0.961 0.551 0.194 0.571 0.753
Recall 0.452 ? 0.460 0.551 0.194 0.571 0.753
F-score 0.452 ? 0.622 0.551 0.194 0.571 0.753
BLEU 0.857 0.728 0.861 0.878 0.797 0.884 0.934
Table 3: Candidate selection effectiveness on different datasets (NC = noisy channel model (Cook and Stevenson,
2009); MT = SMT (Aw et al, 2006); DL = dictionary lookup; WS = word similarity; CS = context support)
bust techniques such as our proposed method, and
effective at capturing common abbreviations such as
gf ?girlfriend?.
Of the component methods proposed in this re-
search, word similarity (?WS?) achieves higher pre-
cision and recall than context support (?CS?), sig-
nifying that many of the ill-formed words emanate
from morphophonemic variations. However, when
combined with word similarity features, context
support improves over the basic method at a level of
statistical significance (based on randomised estima-
tion, p < 0.05: Yeh (2000)), indicating the comple-
mentarity of the two methods, especially on Twitter
data. The best F-score is achieved when combin-
ing dictionary lookup, word similarity and context
support (?DL+WS+CS?), in which ill-formed words
are first looked up in the slang dictionary, and only
if no match is found do we apply our normalisation
method.
We found several limitations in our proposed ap-
proach by analysing the output of our method. First,
not all ill-formed words offer useful context. Some
highly noisy tweets contain almost all misspellings
and unique symbols, and thus no context features
can be extracted. This also explains why ?CS? fea-
tures often fail. For such cases, the method falls back
to context-independent normalisation. We found
that only 32.6% ill-formed words have all IV words
in their context windows. Moreover, the IV words
may not occur in the dependency bank, further de-
creasing the effectiveness of context support fea-
tures. Second, the different features are linearly
combined, where a weighted combination is likely
to give better results, although it also requires a cer-
tain amount of well-sampled annotations for tuning.
6 Conclusion and Future Work
In this paper, we have proposed the task of lexi-
cal normalisation for short text messages, as found
in Twitter and SMS data. We found that most ill-
formed words are based on morphophonemic varia-
tion and proposed a cascaded method to detect and
normalise ill-formed words. Our ill-formed word
detector requires no explicit annotations, and the
dependency-based features were shown to be some-
what effective, however, there was still a lot of
room for improvement at ill-formed word detection.
In normalisation, we compared our method with
two benchmark methods from the literature, and
achieved that highest F-score and BLEU score by
integrating dictionary lookup, word similarity and
context support modelling.
In future work, we propose to pursue a number of
directions. First, we plan to improve our ill-formed
word detection classifier by introducing an OOV
word whitelist. Furthermore, we intend to allevi-
ate noisy contexts with a bootstrapping approach, in
which ill-formed words with high confidence and no
ambiguity will be replaced by their standard forms,
and fed into the normalisation model as new training
data.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of the 21st International Con-
376
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 33?40, Sydney, Australia.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In
HLT ?10: Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
229?237, Los Angeles, USA.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 770?779, Uppsala, Sweden.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286?293, Hong Kong.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The
ICWSM 2009 Spinn3r Dataset. In Proceedings of the
Third Annual Conference on Weblogs and Social Me-
dia, San Jose, USA.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157?174.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC ?09: Proceedings of the Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 71?
78, Boulder, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006),
Genoa, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87?111.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Human Computer Interfaces International
(HCII 05), Las Vegas, USA.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai
Supnithi, and Hitoshi Isahara. 2003. Automatic er-
ror detection in the Japanese learners? English spoken
data. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics - Volume 2,
pages 145?148, Sapporo, Japan.
Joseph Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In International Con-
ference on Natural Language Processing, Kharagpur,
India.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15 (NIPS 2002), pages 3?10, Whistler,
Canada.
Catherine Kobus, Franois Yvon, and Graldine Damnati.
2008. Transcrire les SMS comme on reconnat la pa-
role. In Actes de la Confrence sur le Traitement Au-
tomatique des Langues (TALN?08), pages 128?138.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180, Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, USA.
James L. Peterson. 1980. Computer programs for de-
tecting and correcting spelling errors. Commun. ACM,
23:676?687, December.
Lawrence Philips. 2000. The double metaphone search
algorithm. C/C++ Users Journal, 18:38?43.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In HLT
?10: Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
172?180, Los Angeles, USA.
Claude Elwood Shannon. 1948. A mathematical the-
ory of communication. Bell System Technical Journal,
27:379?423, 623?656.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15(3):287 ? 333.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
377
ken Language Processing, pages 901?904, Denver,
USA.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and
Ming Zhou. 2007. Mining sequential patterns and tree
patterns to detect erroneous sentences. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 81?88, Prague, Czech
Republic.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?02, pages
144?151, Philadelphia, USA.
Twitter. 2010. Big goals, big game, big records.
http://blog.twitter.com/2010/06/
big-goals-big-game-big-records.html.
Retrieved 4 August 2010.
Wilson Wong, Wei Liu, and Mohammed Bennamoun.
2006. Integrated scoring for spelling error correction,
abbreviation expansion and case restoration in dirty
text. In Proceedings of the Fifth Australasian Con-
ference on Data Mining and Analytics, pages 83?89,
Sydney, Australia.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th Conference on Computational Linguistics -
Volume 2, COLING ?00, pages 947?953, Saarbru?cken,
Germany.
378
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1506?1515,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Collective Classification of Congressional Floor-Debate Transcripts
Clinton Burfoot, Steven Bird and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
{cburfoot, sb, tim}@csse.unimelb.edu.au
Abstract
This paper explores approaches to sentiment
classification of U.S. Congressional floor-
debate transcripts. Collective classification
techniques are used to take advantage of the
informal citation structure present in the de-
bates. We use a range of methods based on
local and global formulations and introduce
novel approaches for incorporating the outputs
of machine learners into collective classifica-
tion algorithms. Our experimental evaluation
shows that the mean-field algorithm obtains
the best results for the task, significantly out-
performing the benchmark technique.
1 Introduction
Supervised document classification is a well-studied
task. Research has been performed across many
document types with a variety of classification tasks.
Examples are topic classification of newswire ar-
ticles (Yang and Liu, 1999), sentiment classifica-
tion of movie reviews (Pang et al, 2002), and satire
classification of news articles (Burfoot and Baldwin,
2009). This and other work has established the use-
fulness of document classifiers as stand-alone sys-
tems and as components of broader NLP systems.
This paper deals with methods relevant to super-
vised document classification in domains with net-
work structures, where collective classification can
yield better performance than approaches that con-
sider documents in isolation. Simply put, a network
structure is any set of relationships between docu-
ments that can be used to assist the document clas-
sification process. Web encyclopedias and scholarly
publications are two examples of document domains
where network structures have been used to assist
classification (Gantner and Schmidt-Thieme, 2009;
Cao and Gao, 2005).
The contribution of this research is in four parts:
(1) we introduce an approach that gives better than
state of the art performance for collective classifica-
tion on the ConVote corpus of congressional debate
transcripts (Thomas et al, 2006); (2) we provide a
comparative overview of collective document classi-
fication techniques to assist researchers in choosing
an algorithm for collective document classification
tasks; (3) we demonstrate effective novel approaches
for incorporating the outputs of SVM classifiers into
collective classifiers; and (4) we demonstrate effec-
tive novel feature models for iterative local classifi-
cation of debate transcript data.
In the next section (Section 2) we provide a for-
mal definition of collective classification and de-
scribe the ConVote corpus that is the basis for our
experimental evaluation. Subsequently, we describe
and critique the established benchmark approach for
congressional floor-debate transcript classification,
before describing approaches based on three alterna-
tive collective classification algorithms (Section 3).
We then present an experimental evaluation (Sec-
tion 4). Finally, we describe related work (Section 5)
and offer analysis and conclusions (Section 6).
2 Task Definition
2.1 Collective Classification
Given a network and an object o in the network,
there are three types of correlations that can be used
1506
to infer a label for o: (1) the correlations between
the label of o and its observed attributes; (2) the cor-
relations between the label of o and the observed at-
tributes and labels of nodes connected to o; and (3)
the correlations between the label of o and the un-
observed labels of objects connected to o (Sen et al,
2008).
Standard approaches to classification generally
ignore any network information and only take into
account the correlations in (1). Each object is clas-
sified as an individual instance with features derived
from its observed attributes. Collective classification
takes advantage of the network by using all three
sources. Instances may have features derived from
their source objects or from other objects. Classifi-
cation proceeds in a joint fashion so that the label
given to each instance takes into account the labels
given to all of the other instances.
Formally, collective classification takes a graph,
made up of nodes V = {V1, . . . , Vn} and edges
E. The task is to label the nodes Vi ? V from
a label set L = {L1, . . . , Lq}, making use of the
graph in the form of a neighborhood function N =
{N1, . . . , Nn}, where Ni ? V \ {Vi}.
2.2 The ConVote Corpus
ConVote, compiled by Thomas et al (2006), is a
corpus of U.S. congressional debate transcripts. It
consists of 3,857 speeches organized into 53 debates
on specific pieces of legislation. Each speech is
tagged with the identity of the speaker and a ?for?
or ?against? label derived from congressional voting
records. In addition, places where one speaker cites
another have been annotated, as shown in Figure 1.
We apply collective classification to ConVote de-
bates by letting V refer to the individual speakers in a
debate and populatingN using the citation graph be-
tween speakers. We set L = {y, n}, corresponding
to ?for? and ?against? votes respectively. The text
of each instance is the concatenation of the speeches
by a speaker within a debate. This results in a corpus
of 1,699 instances with a roughly even class distri-
bution. Approximately 70% of these are connected,
i.e. they are the source or target of one or more cita-
tions. The remainder are isolated.
3 Collective Classification Techniques
In this section we describe techniques for perform-
ing collective classification on the ConVote cor-
pus. We differentiate between dual-classifier and
iterative-classifier approaches.
Dual-classifier approach: This approach uses
a collective classification algorithm that takes inputs
from two classifiers: (1) a content-only classifier that
determines the likelihood of a y or n label for an in-
stance given its text content; and (2) a citation clas-
sifier that determines, based on citation information,
whether a given pair of instances are ?same class? or
?different class?.
Let ? denote a set of functions representing the
classification preferences produced by the content-
only and citation classifiers:
? For each Vi ? V , ?i ? ? is a function ?i: L ?
R+ ? {0}.
? For each (Vi, Vj) ? E, ?ij ? ? is a function
?ij : L ? L ? R+ ? {0}.
Later in this section we will describe three collec-
tive classification algorithms capable of performing
overall classification based on these inputs: (1) the
minimum-cut approach, which is the benchmark for
collective classification with ConVote, established
by Thomas et al; (2) loopy belief propagation; and
(3) mean-field. We will show that these latter two
techniques, which are both approximate solutions
for Markov random fields, are superior to minimum-
cut for the task.
Figure 2 gives a visual overview of the dual-
classifier approach.
Iterative-classifier approach: This approach
incorporates content-only and citation features into
a single local classifier that works on the assump-
tion that correct neighbor labels are already known.
This approach represents a marked deviation from
the dual-classifier approach and offers unique ad-
vantages. It is fully described in Section 3.4.
Figure 3 gives a visual overview of the iterative-
classifier approach.
For a detailed introduction to collective classifica-
tion see Sen et al (2008).
1507
Debate 006
Speaker 400378 [against]
Mr. Speaker, . . . all over Washington and in the country, people are talking today about the
majority?s last-minute decision to abandon . . .
. . .
Speaker 400115 [for]
. . .
Mr. Speaker, . . . I just want to say to the gentlewoman from New York that every single member
of this institution . . .
. . .
Figure 1: Sample speech fragments from the ConVote corpus. The phrase gentlewoman from New York by speaker
400115 is annotated as a reference to speaker 400378.
Debate content
Citation vectorsContent-only vectors
Content-only classifications Citation classifications
Content-only and
citation scores
Overall classifications
Extract features Extract features
SVM SVM
NormaliseNormalise
MF/LBP/Mincut
Figure 2: Dual-classifier approach.
Debate content
Content-only vectors
Content-only classifications
Local vectors
Local classifications
Overall classifications
Extract features
SVM
Combine content-only
and citation features
SVM
Update citation features
Terminate iteration
Figure 3: Iterative-classifier approach.
3.1 Dual-classifier Approach with
Minimum-cut
Thomas et al use linear kernel SVMs as their base
classifiers. The content-only classifier is trained to
predict y or n based on the unigram presence fea-
tures found in speeches. The citation classifier is
trained to predict ?same class? or ?different class?
labels based on the unigram presence features found
in the context windows (30 tokens before, 20 tokens
after) surrounding citations for each pair of speakers
in the debate.
The decision plane distance computed by the
content-only SVM is normalized to a positive real
number and stripped of outliers:
?i(y) =
?
??
??
1 di > 2?i;(
1 + di2?i
)
/2 |di| ? 2?i;
0 di < ?2?i
where ?i is the standard deviation of the decision
plane distance, di, over all of the instances in the
debate and ?i(n) = 1??i(y). The citation classifier
output is processed similarly:1
?ij(y, y) =
?
?
?
0 dij < ?;
? ? dij/4?ij ? ? dij ? 4?ij ;
? dij > 4?ij
where ?ij is the standard deviation of the decision
plane distance, dij over all of the citations in the de-
bate and ?ij(n, n) = ?ij(y, y). The ? and ? vari-
ables are free parameters.
A given class assignment v is assigned a cost that
is the sum of per-instance and per-pair class costs
derived from the content-only and citation classifiers
respectively:
c(v) =
?
Vi?V
?i(v?i) +
?
(Vi,Vj)?E:vi 6=vj
?ij(vi, vi)
where vi is the label of node Vi and v?i denotes the
complement class of vi.
1Thomas et al classify each citation context window sep-
arately, so their ? values are actually calculated in a slightly
more complicated way. We adopted the present approach for
conceptual simplicity and because it gave superior performance
in preliminary experiments.
1508
The cost function is modeled in a flow graph
where extra source and sink nodes represent the y
and n labels respectively. Each node in V is con-
nected to the source and sink with capacities ?i(y)
and ?i(n) respectively. Pairs classified in the ?same
class? class are linked with capacities defined by ?.
An exact optimum and corresponding overall
classification is efficiently computed by finding the
minimum-cut of the flow graph (Blum and Chawla,
2001). The free parameters are tuned on a set of
held-out data.
Thomas et al demonstrate improvements over
content-only classification, without attempting to
show that the approach does better than any alter-
natives; the main appeal is the simplicity of the flow
graph model. There are a number of theoretical lim-
itations to the approach, which we now discuss.
As Thomas et al point out, the model has no way
of representing the ?different class? output from the
citation classifier and these citations must be dis-
carded. This, to us, is the most significant problem
with the model. Inspection of the corpus shows that
approximately 80% of citations indicate agreement,
meaning that for the present task the impact of dis-
carding this information may not be large. However,
the primary utility in collective approaches lies in
their ability to fill in gaps in information not picked
up by content-only classification. All available link
information should be applied to this end, so we
need models capable of accepting both positive and
negative information.
The normalization techniques used for converting
SVM outputs to graph weights are somewhat arbi-
trary. The use of standard deviations appears prob-
lematic as, intuitively, the strength of a classification
should be independent of its variance. As a case in
point, consider a set of instances in a debate all clas-
sified as similarly weak positives by the SVM. Use
of ?i as defined above would lead to these being er-
roneously assigned the maximum score because of
their low variance.
The minimum-cut approach places instances in
either the positive or negative class depending on
which side of the cut they fall on. This means
that no measure of classification confidence is avail-
able. This extra information is useful at the very
least to give a human user an idea of how much to
trust the classification. A measure of classification
confidence may also be necessary for incorporation
into a broader system, e.g., a meta-classifier (An-
dreevskaia and Bergler, 2008; Li and Zong, 2008).
Tuning the ? and ? parameters is likely to become
a source of inaccuracy in cases where the tuning and
test debates have dissimilar link structures. For ex-
ample, if the tuning debates tend to have fewer, more
accurate links the ? parameter will be higher. This
will not produce good results if the test debates have
more frequent, less accurate links.
3.2 Heuristics for Improving Minimum-cut
Bansal et al (2008) offer preliminary work describ-
ing additions to the Thomas et al minimum-cut ap-
proach to incorporate ?different class? citation clas-
sifications. They use post hoc adjustments of graph
capacities based on simple heuristics. Two of the
three approaches they trial appear to offer perfor-
mance improvements:
The SetTo heuristic: This heuristic works
through E in order and tries to force Vi and Vj into
different classes for every ?different class? (dij < 0)
citation classifier output where i < j. It does this by
altering the four relevant content-only preferences,
?i(y), ?i(n), ?j(y), and ?j(n). Assume without
loss of generality that the largest of these values is
?i(y). If this preference is respected, it follows that
Vj should be put into class n. Bansal et al instanti-
ate this chain of reasoning by setting:
? ??i(y) = max(?, ?i(y))
? ??j(n) = max(?, ?j(n))
where ?? is the replacement content-only function,
? is a free parameter ? (.5, 1], ??i(n) = 1 ? ?
?
i(y),
and ??j(y) = 1? ?
?
j(y).
The IncBy heuristic: This heuristic is a more
conservative version of the SetTo heuristic. Instead
of replacing the content-only preferences with fixed
constants, it increments and decrements the previous
values so they are somewhat preserved:
? ??i(y) = min(1, ?i(y) + ?)
? ??j(n) = min(1, ?j(n) + ?)
There are theoretical shortcomings with these ap-
proaches. The most obvious problem is the arbitrary
nature of the manipulations, which produce a flow
1509
graph that has an indistinct relationship to the out-
puts of the two classifiers.
Bensal et al trial a range of ? values, with vary-
ing impacts on performance. No attempt is made to
demonstrate a method for choosing a good ? value.
It is not clear that the tuning approach used to set ?
and ? would be successful here. In any case, having
a third parameter to tune would make the process
more time-consuming and increase the risks of in-
correct tuning, described above.
As Bansal et al point out, proceeding through E
in order means that earlier changes may be undone
for speakers who have multiple ?different class? ci-
tations.
Finally, we note that the confidence of the cita-
tion classifier is not embodied in the graph structure.
The most marginal ?different class? citation, classi-
fied just on the negative side of the decision plane, is
treated identically to the most confident one furthest
from the decision plane.
3.3 Dual-classifier Approach with Markov
Random Field Approximations
A pairwise Markov random field (Taskar et al,
2002) is given by the pair (G,?), where G and ?
are as previously defined, ? being re-termed as a set
of clique potentials. Given an assignment v to the
nodes V , the pairwise Markov random field is asso-
ciated with the probability distribution:
P (v) =
1
Z
?
Vi?V
?i(vi)
?
(Vi,Vj)?E
?ij(vi, vj)
where:
Z =
?
v?
?
Vi?V
?i(v
?
i)
?
(Vi,Vj)?E
?ij(v
?
i, v
?
j)
and v?i denotes the label of Vi for an alternative as-
signment in v?.
In general, exact inference over a pairwise
Markov random field is known to be NP-hard. There
are certain conditions under which exact inference
is tractable, but real-world data is not guaranteed to
satisfy these. A class of approximate inference al-
gorithms known as variational methods (Jordan et
al., 1999) solve this problem by substituting a sim-
pler ?trial? distribution which is fitted to the Markov
random field distribution.
Loopy Belief Propagation: Applied to a pair-
wise Markov random field, loopy belief propagation
is a message passing algorithm that can be concisely
expressed as the following set of equations:
mi?j(vj) = ?
?
vi?L
{?ij(vi, vj)?i(vi)
?
Vk?Ni?V\Vj
mk?i(vi),?vj ? L}
bi(vi) = ??i(vi)
?
Vj?Ni?V
mj?i(vi),?vi ? L
where mi?j is a message sent by Vi to Vj and ? is
a normalization constant that ensures that each mes-
sage and each set of marginal probabilities sum to 1.
The algorithm proceeds by making each node com-
municate with its neighbors until the messages sta-
bilize. The marginal probability is then derived by
calculating bi(vi).
Mean-Field: The basic mean-field algorithm can
be described with the equation:
bj(vj) = ??j(vj)
?
Vi?Nj?V
?
vi?L
?bi(vi)ij (vi, vj), vj ? L
where ? is a normalization constant that ensures
?
vj
bj(vj) = 1. The algorithm computes the fixed
point equation for every node and continues to do so
until the marginal probabilities bj(vj) stabilize.
Mean-field can be shown to be a variational
method in the same way as loopy belief propagation,
using a simpler trial distribution. For details see Sen
et al (2008).
Probabilistic SVM Normalisation: Unlike
minimum-cut, the Markov random field approaches
have inherent support for the ?different class? out-
put of the citation classifier. This allows us to ap-
ply a more principled SVM normalisation technique.
Platt (1999) describes a technique for converting the
output of an SVM classifier to a calibrated posterior
probability. Platt finds that the posterior can be fit
using a parametric form of a sigmoid:
P (y = 1|d) =
1
1 + exp(Ad+B)
This is equivalent to assuming that the output of
the SVM is proportional to the log odds of a positive
example. Experimental analysis shows error rate is
1510
improved over a plain linear SVM and probabilities
are of comparable quality to those produced using a
regularized likelihood kernel method.
By applying this technique to the base classifiers,
we can produce new, simpler ? functions, ?i(y) =
Pi and ?ij(y, y) = Pij where Pi is the probabilis-
tic normalized output of the content-only classifier
and Pij is the probabilistic normalized output of the
citation classifier.
This approach addresses the problems with the
Thomas et al method where the use of standard
deviations can produce skewed normalizations (see
Section 3.1). By using probabilities we also open
up the possibility of replacing the SVM classifiers
with any other model than can be made to produce
a probability. Note also that there are no parameters
to tune.
3.4 Iterative Classifier Approach
The dual-classifier approaches described above rep-
resent global attempts to solve the collective classifi-
cation problem. We can choose to narrow our focus
to the local level, in which we aim to produce the
best classification for a single instance with the as-
sumption that all other parts of the problem (i.e. the
correct labeling of the other instances) are solved.
The Iterative Classification Algorithm (Bilgic et
al., 2007), defined in Algorithm 1, is a simple tech-
nique for performing collective classification using
such a local classifier. After bootstrapping with a
content-only classifier, it repeatedly generates new
estimates for vi based on its current knowledge of
Ni. The algorithm terminates when the predictions
stabilize or a fixed number of iterations is com-
pleted. Each iteration is completed using a newly
generated ordering O, over the instances V .
We propose three feature models for the local
classifier.
Citation presence and Citation count: Given
that the majority of citations represent the ?same
class? relationship (see Section 3.1), we can an-
ticipate that content-only classification performance
will be improved if we add features to represent the
presence of neighbours of each class.
We define the function c(i, l) =
?
vj?Ni?V
?vj ,l
giving the number of neighbors for node Vi with la-
bel l, where ? is the Kronecker delta. We incorporate
these citation count values, one for the supporting
Algorithm 1 Iterative Classification Algorithm
for each node Vi ? V do {bootstrapping}
compute ~ai using only local attributes of node
vi ? f(~ai)
end for
repeat {iterative classification}
randomly generate ordering O over nodes in V
for each node Vi ? O do
{compute new estimate of vi}
compute ~ai using current assignments to Ni
vi ? f(~ai)
end for
until labels have stabilized or maximum iterations
reached
class and one for the opposing class, obtaining a new
feature vector (u1i , u
2
i , . . . , u
j
i , c(i, y), c(i, n)) where
u1i , u
2
i , . . . , u
j
i are the elements of ~ui, the binary un-
igram feature vector used by the content-only clas-
sifier to represent instance i.
Alternatively, we can represent neighbor labels
using binary citation presence values where any
non-zero count becomes a 1 in the feature vector.
Context window: We can adopt a more nu-
anced model for citation information if we incor-
porate the citation context window features into the
feature vector. This is, in effect, a synthesis of
the content-only and citation feature models. Con-
text window features come from the product space
L ? C, where C is the set of unigrams used in ci-
tation context windows and ~ci denotes the context
window features for instance i. The new feature vec-
tor becomes: (u1i , u
2
i , . . . , u
j
i , c
1
i , c
2
i , . . . , c
k
i ). This
approach implements the intuition that speakers in-
dicate their voting intentions by the words they use
to refer to speakers whose vote is known. Because
neighbor relations are bi-directional the reverse is
also true: Speakers indicate other speakers? voting
intentions by the words they use to refer to them.
As an example, consider the context window fea-
ture AGREE-FOR, indicating the presence of the
agree unigram in the citation window I agree with
the gentleman from Louisiana, where the label for
the gentleman from Louisiana instance is y. This
feature will be correctly correlated with the y label.
Similarly, if the unigram were disagree the feature
would be correlated with the n label.
1511
4 Experiments
In this section we compare the performance of our
dual-classifier and iterative-classifier approaches.
We also evaluate the performance of the three fea-
ture models for local classification.
All accuracies are given as the percentages of
instances correctly classified. Results are macro-
averaged using 10 ? 10-fold cross validation, i.e.
10 runs of 10-fold cross validation using different
randomly assigned data splits.
Where quoted, statistical significance has been
calculated using a two-tailed paired t-test measured
over all 100 pairs with 10 degrees of freedom. See
Bouckaert (2003) for an experimental justification
for this approach.
Note that the results presented in this section
are not directly comparable with those reported by
Thomas et al and Bansal et al because their exper-
iments do not use cross-validation. See Section 4.3
for further discussion of experimental configuration.
4.1 Local Classification
We evaluate three models for local classification: ci-
tation presence features, citation count features and
context window features. In each case the SVM
classifier is given feature vectors with both content-
only and citation information, as described in Sec-
tion 3.4.
Table 1 shows that context window performs the
best with 89.66% accuracy, approximately 1.5%
ahead of citation count and 3.5% ahead of citation
presence. All three classifiers significantly improve
on the content-only classifier.
These relative scores seem reasonable. Knowing
the words used in citations of each class is better
than knowing the number of citations in each class,
and better still than only knowing which classes of
citations exist.
These results represent an upper-bound for the
performance of the iterative classifier, which re-
lies on iteration to produce the reliable information
about citations given here by oracle.
4.2 Collective Classification
Table 2 shows overall results for the three collective
classification algorithms. The iterative classifier was
run separately with citation count and context win-
Method Accuracy (%)
Majority 52.46
Content-only 75.29
Citation presence 85.01
Citation count 88.18
Context window 89.66
Table 1: Local classifier accuracy. All three local
classifiers are significant over the in-isolation classifier
(p < .001).
dow citation features, the two best performing local
classification methods, both with a threshold of 30
iterations.
Results are shown for connected instances, iso-
lated instances, and all instances. Collective clas-
sification techniques can only have an impact on
connected instances, so these figures are most im-
portant. The figures for all instances show the per-
formance of the classifiers in our real-world task,
where both connected and isolated instances need to
be classified and the end-user may not distinguish
between the two types.
Each of the four collective classifiers outperform
the minimum-cut benchmark over connected in-
stances, with the iterative classifier (context win-
dow) (79.05%) producing the smallest gain of less
than 1% and mean-field doing best with a nearly
6% gain (84.13%). All show a statistically signif-
icant improvement over the content-only classifier.
Mean-field shows a statistically significant improve-
ment over minimum-cut.
The dual-classifier approaches based on loopy
belief propagation and mean-field do better than
the iterative-classifier approaches by an average of
about 3%.
Iterative classification performs slightly better
with citation count features than with context win-
dow features, despite the fact that the context win-
dow model performs better in the local classifier
evaluation. We speculate that this may be due to ci-
tation count performing better when given incorrect
neighbor labels. This is an aspect of local classi-
fier performance we do not otherwise measure, so a
clear conclusion is not possible. Given the closeness
of the results it is also possible that natural statistical
variation is the cause of the difference.
1512
The performance of the minimum-cut method is
not reliably enhanced by either the SetTo or IncBy
heuristics. Only IncBy(.15) gives a very small im-
provement (0.14%) over plain minimum-cut. All
of the other combinations tried diminished perfor-
mance slightly.
4.3 A Note on Error Propagation and
Experimental Configuration
Early in our experimental work we noticed that per-
formance often varied greatly depending on the de-
bates that were allocated to training, tuning and test-
ing. This observation is supported by the per-fold
scores that are the basis for the macro-average per-
formance figures reported in Table 2, which tend
to have large standard deviations. The absolute
standard deviations over the 100 evaluations for the
minimum-cut and mean-field methods were 11.19%
and 8.94% respectively. These were significantly
larger than the standard deviation for the content-
only baseline, which was 7.34%. This leads us to
conclude that the performance of collective classifi-
cation methods is highly variable.
Bilgic and Getoor (2008) offer a possible expla-
nation for this. They note that the cost of incor-
rectly classifying a given instance can be magnified
in collective classification, because errors are prop-
agated throughout the network. The extent to which
this happens may depend on the random interaction
between base classification accuracy and network
structure. There is scope for further work to more
fully explain this phenomenon.
From these statistical and theoretical factors we
infer that more reliable conclusions can be drawn
from collective classification experiments that use
cross-validation instead of a single, fixed data split.
5 Related work
Somasundaran et al (2009) use ICA to improve sen-
timent polarity classification of dialogue acts in a
corpus of multi-party meeting transcripts. Link fea-
tures are derived from annotations giving frame re-
lations and target relations. Respectively, these re-
late dialogue acts based on the sentiment expressed
and the object towards which the sentiment is ex-
pressed. Somasundaran et al provides another ar-
gument for the usefulness of collective classification
(specifically ICA), in this case as applied at a dia-
logue act level and relying on a complex system of
annotations for link information.
Somasundaran and Wiebe (2009) propose an un-
supervised method for classifying the stance of each
contribution to an online debate concerning the mer-
its of competing products. Concessions to other
stances are modeled, but there are no overt citations
in the data that could be used to induce the network
structure required for collective classification.
Pang and Lee (2005) use metric labeling to per-
form multi-class collective classification of movie
reviews. Metric labeling is a multi-class equiva-
lent of the minimum-cut technique in which opti-
mization is done over a cost function incorporat-
ing content-only and citation scores. Links are con-
structed between test instances and a set of k near-
est neighbors drawn only from the training set. Re-
stricting the links in this way means the optimization
problem is simple. A similarity metric is used to find
nearest neighbors.
The Pang and Lee method is an instance of im-
plicit link construction, an approach which is be-
yond the scope of this paper but nevertheless an im-
portant area for future research. A similar technique
is used in a variation on the Thomas et al experi-
ment where additional links between speeches are
inferred via a similarity metric (Burfoot, 2008). In
cases where both citation and similarity links are
present, the overall link score is taken as the sum of
the two scores. This seems counter-intuitive, given
that the two links are unlikely to be independent. In
the framework of this research, the approach would
be to train a link meta-classifier to take scores from
both link classifiers and output an overall link prob-
ability.
Within NLP, the use of LBP has not been re-
stricted to document classification. Examples of
other applications are dependency parsing (Smith
and Eisner, 2008) and alignment (Cromires and
Kurohashi, 2009). Conditional random fields
(CRFs) are an approach based on Markov random
fields that have been popular for segmenting and
labeling sequence data (Lafferty et al, 2001). We
rejected linear-chain CRFs as a candidate approach
for our evaluation on the grounds that the arbitrar-
ily connected graphs used in collective classification
can not be fully represented in graphical format, i.e.
1513
Connected Isolated All
Majority 52.46 46.29 50.51
Content only 75.31 78.90 76.28
Minimum-cut 78.31 78.90 78.40
Minimum-cut (SetTo(.6)) 78.22 78.90 78.32
Minimum-cut (SetTo(.8)) 78.01 78.90 78.14
Minimum-cut (SetTo(1)) 77.71 78.90 77.93
Minimum-cut (IncBy(.05)) 78.14 78.90 78.25
Minimum-cut (IncBy(.15)) 78.45 78.90 78.46
Minimum-cut (IncBy(.25)) 78.02 78.90 78.15
Iterative-classifier (citation count) 80.07? 78.90 79.69?
Iterative-classifier (context window) 79.05 78.90 78.93
Loopy Belief Propagation 83.37? 78.90 81.93?
Mean-Field 84.12? 78.90 82.45?
Table 2: Speaker classification accuracies (%) over connected, isolated and all instances. The marked results are
statistically significant over the content only benchmark (? p < .01, ? p < .001). The mean-field results are statistically
significant over minimum-cut (p < .05).
linear-chain CRFs do not scale to the complexity of
graphs used in this research.
6 Conclusions and future work
By applying alternative models, we have demon-
strated the best recorded performance for collective
classification of ConVote using bag-of-words fea-
tures, beating the previous benchmark by nearly 6%.
Moreover, each of the three alternative approaches
trialed are theoretically superior to the minimum-cut
approach approach for three main reasons: (1) they
support multi-class classification; (2) they support
negative and positive citations; (3) they require no
parameter tuning.
The superior performance of the dual-classifier
approach with loopy belief propagation and mean-
field suggests that either algorithm could be consid-
ered as a first choice for collective document classi-
fication. Their advantage is increased by their abil-
ity to output classification confidences as probabili-
ties, while minimum-cut and the local formulations
only give absolute class assignments. We do not dis-
miss the iterative-classifier approach entirely. The
most compelling point in its favor is its ability to
unify content only and citation features in a single
classifier. Conceptually speaking, such an approach
should allow the two types of features to inter-relate
in more nuanced ways. A case in point comes from
our use of a fixed size context window to build a
citation classifier. Future approaches may be able
to do away with this arbitrary separation of features
by training a local classifier to consider all words in
terms of their impact on content-only classification
and their relations to neighbors.
Probabilistic SVM normalization offers a conve-
nient, principled way of incorporating the outputs of
an SVM classifier into a collective classifier. An op-
portunity for future work is to consider normaliza-
tion approaches for other classifiers. For example,
confidence-weighted linear classifiers (Dredze et al,
2008) have been shown to give superior performance
to SVMs on a range of tasks and may therefore be a
better choice for collective document classification.
Of the three models trialled for local classifiers,
context window features did best when measured in
an oracle experiment, but citation count features did
better when used in a collective classifier. We con-
clude that context window features are a more nu-
anced and powerful approach that is also more likely
to suffer from data sparseness. Citation count fea-
tures would have been the less effective in a scenario
where the fact of the citation existing was less infor-
mative, for example, if a citation was 50% likely to
indicate agreement rather than 80% likely. There is
much scope for further research in this area.
1514
References
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In ACL,
pages 290?298.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In COL-
ING, pages 15?18.
Mustafa Bilgic and Lise Getoor. 2008. Effective label
acquisition for collective classification. In KDD, pages
43?51.
Mustafa Bilgic, Galileo Namata, and Lise Getoor. 2007.
Combining collective classification and link predic-
tion. In ICDM Workshops, pages 381?386. IEEE
Computer Society.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
ICML, pages 19?26.
Remco R. Bouckaert. 2003. Choosing between two
learning algorithms based on calibrated tests. In
ICML, pages 51?58.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In ACL-
IJCNLP Short Papers, pages 161?164.
Clint Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, pages 11?18. ALTA.
Minh Duc Cao and Xiaoying Gao. 2005. Combining
contents and citations for scientific document classifi-
cation. In 18th Australian Joint Conference on Artifi-
cial Intelligence, pages 143?152.
Fabien Cromires and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL, pages
166?174.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML, pages 264?271.
Zeno Gantner and Lars Schmidt-Thieme. 2009. Auto-
matic content-based categorization of Wikipedia ar-
ticles. In 2009 Workshop on The People?s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 32?37.
Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola,
Lawrence Saul, and David Heckerman. 1999. An in-
troduction to variational methods for graphical mod-
els. Machine Learning, 37:183?233.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In ACL, pages 257?260.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, pages 115?124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using ma-
chine learning techniques. In EMNLP, pages 79?86.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. In A. Smola, P. Bartlett, B. Scholkopf,
and D. Schuurmans, editors, Advances in Large Mar-
gin Classifiers, pages 61?74. MIT Press.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29:93?106.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In ACL-IJCNLP,
pages 226?234.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In EMNLP, pages
170?179.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In UAI.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP, pages
327?335.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings ACM SI-
GIR, pages 42?49.
1515
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1536?1545,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Labelling of Topic Models
Jey Han Lau,?? Karl Grieser,? David Newman,?? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
jhlau@csse.unimelb.edu.au, kgrieser@csse.unimelb.edu.au, newman@uci.edu, tb@ldwin.net
Abstract
We propose a method for automatically la-
belling topics learned via LDA topic models.
We generate our label candidate set from the
top-ranking topic terms, titles of Wikipedia ar-
ticles containing the top-ranking topic terms,
and sub-phrases extracted from the Wikipedia
article titles. We rank the label candidates us-
ing a combination of association measures and
lexical features, optionally fed into a super-
vised ranking model. Our method is shown to
perform strongly over four independent sets of
topics, significantly better than a benchmark
method.
1 Introduction
Topic modelling is an increasingly popular frame-
work for simultaneously soft-clustering terms and
documents into a fixed number of ?topics?, which
take the form of a multinomial distribution over
terms in the document collection (Blei et al,
2003). It has been demonstrated to be highly ef-
fective in a wide range of tasks, including multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008), information retrieval (Wei and
Croft, 2006) and image labelling (Feng and Lapata,
2010).
One standard way of interpreting a topic is to use
the marginal probabilities p(wi|tj) associated with
each term wi in a given topic tj to extract out the 10
terms with highest marginal probability. This results
in term lists such as:1
stock market investor fund trading invest-
ment firm exchange companies share
1Here and throughout the paper, we will represent a topic tj
via its ranking of top-10 topic terms, based on p(wi|tj).
which are clearly associated with the domain of
stock market trading. The aim of this research is to
automatically generate topic labels which explicitly
identify the semantics of the topic, i.e. which take us
from a list of terms requiring interpretation to a sin-
gle label, such as STOCK MARKET TRADING in the
above case.
The approach proposed in this paper is to first
generate a topic label candidate set by: (1) sourc-
ing topic label candidates from Wikipedia by query-
ing with the top-N topic terms; (2) identifying the
top-ranked document titles; and (3) further post-
processing the document titles to extract sub-strings.
We translate each topic label into features extracted
from Wikipedia, lexical association with the topic
terms in Wikipedia documents, and also lexical fea-
tures for the component terms. This is used as the
basis of a support vector regression model, which
ranks each topic label candidate.
Our contributions in this work are: (1) the genera-
tion of a novel evaluation framework and dataset for
topic label evaluation; (2) the proposal of a method
for both generating and scoring topic label candi-
dates; and (3) strong in- and cross-domain results
across four independent document collections and
associated topic models, demonstrating the ability
of our method to automatically label topics with re-
markable success.
2 Related Work
Topics are conventionally interpreted via their top-
N terms, ranked based on the marginal probability
p(wi|tj) in that topic (Blei et al, 2003; Griffiths and
Steyvers, 2004). This entails a significant cognitive
load in interpretation, prone to subjectivity. Topics
are also sometimes presented with manual post-hoc
labelling for ease of interpretation in research pub-
lications (Wang and McCallum, 2006; Mei et al,
1536
2006). This has obvious disadvantages in terms of
subjectivity, and lack of reproducibility/automation.
The closest work to our method is that of Mei et
al. (2007), who proposed various unsupervised ap-
proaches for automatically labelling topics, based
on: (1) generating label candidates by extracting ei-
ther bigrams or noun chunks from the document col-
lection; and (2) ranking the label candidates based
on KL divergence with a given topic. Their proposed
methodology generates a generic list of label can-
didates for all topics using only the document col-
lection. The best method uses bigrams exclusively,
in the form of the top-1000 bigrams based on the
Student?s t-test. We reimplement their method and
present an empirical comparison in Section 5.3.
In other work, Magatti et al (2009) proposed a
method for labelling topics induced by a hierarchi-
cal topic model. Their label candidate set is the
Google Directory (gDir) hierarchy, and label selec-
tion takes the form of ontological alignment with
gDir. The experiments presented in the paper are
highly preliminary, although the results certainly
show promise. However, the method is only applica-
ble to a hierarchical topic model and crucially relies
on a pre-existing ontology and the class labels con-
tained therein.
Pantel and Ravichandran (2004) addressed the
more specific task of labelling a semantic class
by applying Hearst-style lexico-semantic patterns
to each member of that class. When presented
with semantically homogeneous, fine-grained near-
synonym clusters, the method appears to work well.
With topic modelling, however, the top-ranking
topic terms tended to be associated and not lexically
similar to one another. It is thus highly questionable
whether their method could be applied to topic mod-
els, but it would certainly be interesting to investi-
gate whether our model could conversely be applied
to the labelling of sets of near-synonyms.
In recent work, Lau et al (2010) proposed to ap-
proach topic labelling via best term selection, i.e.
selecting one of the top-10 topic terms to label the
overall topic. While it is often possible to label top-
ics with topic terms (as is the case with the stock
market topic above), there are also often cases where
topic terms are not appropriate as labels. We reuse
a selection of the features proposed by Lau et al
(2010), and return to discuss it in detail in Section 3.
While not directly related to topic labelling,
Chang et al (2009) were one of the first to propose
human labelling of topic models, in the form of syn-
thetic intruder word and topic detection tasks. In the
intruder word task, they include a term w with low
marginal probability p(w|t) for topic t into the top-
N topic terms, and evaluate how well both humans
and their model are able to detect the intruder.
The potential applications for automatic labelling
of topics are many and varied. In document col-
lection visualisation, e.g., the topic model can be
used as the basis for generating a two-dimensional
representation of the document collection (Newman
et al, 2010a). Regions where documents have a
high marginal probability p(di|tj) of being associ-
ated with a given topic can be explicitly labelled
with the learned label, rather than just presented
as an unlabelled region, or presented with a dense
?term cloud? from the original topic. In topic model-
based selectional preference learning (Ritter et al,
2010; O` Se?aghdha, 2010), the learned topics can
be translated into semantic class labels (e.g. DAYS
OF THE WEEK), and argument positions for individ-
ual predicates can be annotated with those labels for
greater interpretability/portability. In dynamic topic
models tracking the diachronic evolution of topics
in time-sequenced document collections (Blei and
Lafferty, 2006), labels can greatly enhance the inter-
pretation of what topics are ?trending? at any given
point in time.
3 Methodology
The task of automatic labelling of topics is a natural
progression from the best topic term selection task
of Lau et al (2010). In that work, the authors use
a reranking framework to produce a ranking of the
top-10 topic terms based on how well each term ? in
isolation ? represents a topic. For example, in our
stock market investor fund trading ... topic example,
the term trading could be considered as a more rep-
resentative term of the overall semantics of the topic
than the top-ranked topic term stock.
While the best term could be used as a topic la-
bel, topics are commonly ideas or concepts that are
better expressed with multiword terms (for example
STOCK MARKET TRADING), or terms that might not
be in the top-10 topic terms (for example, COLOURS
1537
would be a good label for a topic of the form red
green blue cyan ...).
In this paper, we propose a novel method for au-
tomatic topic labelling that first generates topic label
candidates using English Wikipedia, and then ranks
the candidates to select the best topic labels.
3.1 Candidate Generation
Given the size and diversity of English Wikipedia,
we posit that the vast majority of (coherent) topics
or concepts are encapsulated in a Wikipedia article.
By making this assumption, the difficult task of gen-
erating potential topic labels is transposed to find-
ing relevant Wikipedia articles, and using the title of
each article as a topic label candidate.
We first use the top-10 topic terms (based on the
marginal probabilities from the original topic model)
to query Wikipedia, using: (a) Wikipedia?s native
search API; and (b) a site-restricted Google search.
The combined set of top-8 article titles returned
from the two search engines for each topic consti-
tutes the initial set of primary candidates.
Next we chunk parse the primary candidates us-
ing the OpenNLP chunker,2 and extract out all noun
chunks. For each noun chunk, we generate all com-
ponent n-grams (including the full chunk), out of
which we remove all n-grams which are not in them-
selves article titles in English Wikipedia. For exam-
ple, if the Wikipedia document title were the single
noun chunk United States Constitution, we would
generate the bigrams United States and States Con-
stitution, and prune the latter; we would also gen-
erate the unigrams United, States and Constitution,
all of which exist as Wikipedia articles and are pre-
served.
In this way, an average of 30?40 secondary labels
are produced for each topic based on noun chunk n-
grams. A good portion of these labels are commonly
stopwords or unigrams that are only marginally re-
lated to the topic (an artifact of the n-gram gener-
ation process). To remove these outlier labels, we
use the RACO lexical association method of Grieser
et al (2011).
RACO (Related Article Conceptual Overlap) uses
Wikipedia?s link structure and category membership
to identify the strength of relationship between arti-
2http://opennlp.sourceforge.net/
cles via their category overlap. The set of categories
related to an article is defined as the union of the cat-
egory membership of all outlinks in that article. The
category overlap of two articles (a and b) is the in-
tersection of the related category sets of each article.
The formal definition of this measure is as follows:
|(?p?O(a)C(p)) ? (?p?O(b)C(p))|
where O(a) is the set of outlinks from article a, and
C(p) is the set of categories of which article p is a
member. This is then normalised using Dice?s co-
efficient to generate a similarity measure. In the in-
stance that a term maps onto multiple Wikipedia ar-
ticles via a disambiguation page, we return the best
RACO score across article pairings for a given term
pair. The final score for each secondary label can-
didate is calculated as the average RACO score with
each of the primary label candidates. All secondary
labels with an average RACO score of 0.1 and above
are added to the label candidate set.
Finally, we add the top-5 topic terms to the set of
candidates, based on the marginals from the origi-
nal topic model. Doing this ensures that there are
always label candidates for all topics (even if the
Wikipedia searches fail), and also allows the pos-
sibility of labeling a topic using its own topic terms,
which was demonstrated by Lau et al (2010) to be a
baseline source of topic label candidates.
3.2 Candidate Ranking
After obtaining the set of topic label candidates, the
next step is to rank the candidates to find the best la-
bel for each topic. We will first describe the features
that we use to represent label candidates.
3.2.1 Features
A good label should be strongly associated with
the topic terms. To learn the association of a label
candidate with the topic terms, we use several lexical
association measures: pointwise mutual information
(PMI), Student?s t-test, Dice?s coefficient, Pearson?s
?2 test, and the log likelihood ratio (Pecina, 2009).
We also include conditional probability and reverse
conditional probability measures, based on the work
of Lau et al (2010). To calculate the association
measures, we parse the full collection of English
Wikipedia articles using a sliding window of width
1538
20, and obtain term frequencies for the label candi-
dates and topic terms. To measure the association
between a label candidate and a list of topic terms,
we average the scores of the top-10 topic terms.
In addition to the association measures, we in-
clude two lexical properties of the candidate: the raw
number of terms, and the relative number of terms in
the label candidate that are top-10 topic terms.
We also include a search engine score for each
label candidate, which we generate by querying a
local copy of English Wikipedia with the top-10
topic terms, using the Zettair search engine (based
on BM25 term similarity).3 For a given label candi-
date, we return the average score for the Wikipedia
article(s) associated with it.
3.2.2 Unsupervised and Supervised Ranking
Each of the proposed features can be used as the
basis for an unsupervised model for label candidate
selection, by ranking the label candidates for a given
topic and selecting the top-N . Alternatively, they
can be combined in a supervised model, by training
over topics where we have gold-standard labelling
of the label candidates. For the supervised method,
we use a support vector regression (SVR) model
(Joachims, 2006) over all of the features.
4 Datasets
We conducted topic labelling experiments using
document collections constructed from four distinct
domains/genres, to test the domain/genre indepen-
dence of our method:
BLOGS : 120,000 blog articles dated from August
to October 2008 from the Spinn3r blog dataset4
BOOKS : 1,000 English language books from the
Internet Archive American Libraries collection
NEWS : 29,000 New York Times news articles
dated from July to September 1999, from the
English Gigaword corpus
PUBMED : 77,000 PubMed biomedical abstracts
published in June 2010
3http://www.seg.rmit.edu.au/zettair/
4http://www.icwsm.org/data/
The BLOGS dataset contains blog posts that cover
a diverse range of subjects, from product reviews
to casual, conversational messages. The BOOKS
topics, coming from public-domain out-of-copyright
books (with publication dates spanning more than
a century), relate to a wide range of topics includ-
ing furniture, home decoration, religion and art,
and have a more historic feel to them. The NEWS
topics reflect the types and range of subjects one
might expect in news articles such as health, finance,
entertainment, and politics. The PUBMED topics
frequently contain domain-specific terms and are
sharply differentiated from the topics for the other
corpora. We are particularly interested in the perfor-
mance of the method over PUBMED, as it is a highly
specialised domain where we may expect lower cov-
erage of appropriate topic labels within Wikipedia.
We took a standard approach to topic modelling
each of the four document collections: we tokenised,
lemmatised and stopped each document,5 and cre-
ated a vocabulary of terms that occurred at least
ten times. From this processed data, we created a
bag-of-words representation of each document, and
learned topic models with T = 100 topics in each
case.
To focus our experiments on topics that were rela-
tively more coherent and interpretable, we first used
the method of Newman et al (2010b) to calculate
the average PMI-score for each topic, and filtered
all topics that had an average PMI-score lower than
0.4. We additionally filtered any topics where less
than 5 of the top-10 topic terms are default nomi-
nal in Wikipedia.6 The filtering criteria resulted in
45 topics for BLOGS, 38 topics for BOOKS, 60 top-
ics for NEWS, and 85 topics for PUBMED. Man-
ual inspection of the discarded topics indicated that
they were predominantly hard-to-label junk topics or
mixed topics, with limited utility for document/term
clustering.
Applying our label candidate generation method-
ology to these 228 topics produced approximately
6000 labels ? an average of 27 labels per topic.
5OpenNLP is used for tokenization, Morpha for lemmatiza-
tion (Minnen et al, 2001).
6As determined by POS tagging English Wikipedia with
OpenNLP, and calculating the coarse-grained POS priors (noun,
verb, etc.) for each term.
1539
Figure 1: A screenshot of the topic label evaluation task on Amazon Mechanical Turk. This screen constitutes a
Human Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated. Note
that been would be the stopword label in this example.
4.1 Topic Candidate Labelling
To evaluate our methods and train the supervised
method, we require gold-standard ratings for the la-
bel candidates. To this end, we used Amazon Me-
chanical Turk to collect annotations for our labels.
In our annotation task, each topic was presented
in the form of its top-10 terms, followed by 10 sug-
gested labels for the topic. This constitutes a Human
Intelligence Task (HIT); annotators are paid based
on the number of HITs they have completed. A
screenshot of a HIT seen by annotator is presented
in Figure 1.
In each HIT, annotators were asked to rate the la-
bels based on the following ordinal scale:
3: Very good label; a perfect description of the
topic.
2: Reasonable label, but does not completely cap-
ture the topic.
1: Label is semantically related to the topic, but
would not make a good topic label.
0: Label is completely inappropriate, and unrelated
to the topic.
To filter annotations from workers who did not
perform the task properly or from spammers, we ap-
1540
Domain Topic Terms Label Candidate AverageRating
BLOGS china chinese olympics gold olympic team win beijing medal sport 2008 summer olympics 2.60
BOOKS church arch wall building window gothic nave side vault tower gothic architecture 2.40
NEWS israel peace barak israeli minister palestinian agreement prime leader palestinians israeli-palestinian conflict 2.63
PUBMED cell response immune lymphocyte antigen cytokine t-cell induce receptor immunity immune system 2.36
Table 1: A sample of topics and topic labels, along with the average rating for each label candidate
plied a few heuristics to automatically detect these
workers. Additionally, we inserted a small num-
ber of stopwords as label candidates in each HIT
and recorded workers who gave high ratings to these
stopwords. Annotations from workers who failed to
passed these tests are removed from the final set of
gold ratings.
Each label candidate was rated in this way by at
least 10 annotators, and ratings from annotators who
passed the filter were combined by averaging them.
A sample of topics, label candidates, and the average
rating is presented in Table 1.7
Finally, we train the regression model over all
the described features, using the human rating-based
ranking.
5 Experiments
In this section we present our experimental results
for the topic labelling task, based on both the unsu-
pervised and supervised methods, and the methodol-
ogy of Mei et al (2007), which we denote MSZ for
the remainder of the paper.
5.1 Evaluation
We use two basic measures to evaluate the perfor-
mance of our predictions. Top-1 average rating is
the average annotator rating given to the top-ranked
system label, and has a maximum value of 3 (where
annotators unanimously rated all top-ranked system
labels with a 3). This is intended to give a sense of
the absolute utility of the top-ranked candidates.
The second measure is normalized discounted
cumulative gain (nDCG: Jarvelin and Kekalainen
(2002), Croft et al (2009)), computed for the top-1
(nDCG-1), top-3 (nDCG-3) and top-5 ranked sys-
tem labels (nDCG-5). For a given ordered list of
7The dataset is available for download from
http://www.csse.unimelb.edu.au/research/
lt/resources/acl2011-topic/.
scores, this measure is based on the difference be-
tween the original order, and the order when the list
is sorted by score. That is, if items are ranked op-
timally in descending order of score at position N ,
nDCG-N is equal to 1. nDCG is a normalised score,
and indicates how close the candidate label ranking
is to the optimal ranking within the set of annotated
candidates, noting that an nDCG-N score of 1 tells
us nothing about absolute values of the candidates.
This second evaluation measure is thus intended to
reflect the relative quality of the ranking, and com-
plements the top-1 average rating.
Note that conventional precision- and recall-based
evaluation is not appropriate for our task, as each
label candidate has a real-valued rating.
As a baseline for the task, we use the unsuper-
vised label candidate ranking method based on Pear-
son?s ?2 test, as it was overwhelmingly found to be
the pick of the features for candidate ranking.
5.2 Results for the Supervised Method
For the supervised model, we present both in-
domain results based on 10-fold cross-validation,
and cross-domain results where we learn a model
from the ratings for the topic model from a given
domain, and apply it to a second domain. In each
case, we learn an SVR model over the full set of fea-
tures described in Section 3.2.1. In practical terms,
in-domain results make the unreasonable assump-
tion that we have labelled 90% of labels in order
to be able to label the remaining 10%, and cross-
domain results are thus the more interesting data
point in terms of the expected results when apply-
ing our method to a novel topic model. It is valuable
to compare the two, however, to gauge the relative
impact of domain on the results.
We present the results for the supervised method
in Table 2, including the unsupervised baseline and
an upper bound estimate for comparison purposes.
The upper bound is calculated by ranking the candi-
1541
Test Domain Training Top-1 Average Rating nDCG-1 nDCG-3 nDCG-5All 1? 2? Top5
BLOGS
Baseline (unsupervised) 1.84 1.87 1.75 1.74 0.75 0.77 0.79
In-domain 1.98 1.94 1.95 1.77 0.81 0.82 0.83
Cross-domain: BOOKS 1.88 1.92 1.90 1.77 0.77 0.81 0.83
Cross-domain: NEWS 1.97 1.94 1.92 1.77 0.80 0.83 0.83
Cross-domain: PUBMED 1.95 1.95 1.93 1.82 0.80 0.82 0.83
Upper bound 2.45 2.26 2.29 2.18 1.00 1.00 1.00
BOOKS
Baseline (unsupervised) 1.75 1.76 1.70 1.72 0.77 0.77 0.79
In-domain 1.91 1.90 1.83 1.74 0.84 0.81 0.83
Cross-domain: BLOGS 1.82 1.88 1.79 1.71 0.79 0.81 0.82
Cross-domain: NEWS 1.82 1.87 1.80 1.75 0.79 0.81 0.83
Cross-domain: PUBMED 1.87 1.87 1.80 1.73 0.81 0.82 0.83
Upper bound 2.29 2.17 2.15 2.04 1.00 1.00 1.00
NEWS
Baseline (unsupervised) 1.96 1.76 1.87 1.70 0.80 0.79 0.78
In-domain 2.02 1.92 1.90 1.82 0.82 0.82 0.84
Cross-domain: BLOGS 2.03 1.92 1.89 1.85 0.83 0.82 0.84
Cross-domain: BOOKS 2.01 1.80 1.93 1.73 0.82 0.82 0.83
Cross-domain: PUBMED 2.01 1.93 1.94 1.80 0.82 0.82 0.83
Upper bound 2.45 2.31 2.33 2.12 1.00 1.00 1.00
PUBMED
Baseline (unsupervised) 1.73 1.74 1.68 1.63 0.75 0.77 0.79
In-domain 1.79 1.76 1.74 1.67 0.77 0.82 0.84
Cross-domain: BLOGS 1.80 1.77 1.73 1.69 0.78 0.82 0.84
Cross-domain: BOOKS 1.77 1.70 1.74 1.64 0.77 0.82 0.83
Cross-domain: NEWS 1.79 1.76 1.73 1.65 0.77 0.82 0.84
Upper bound 2.31 2.17 2.22 2.01 1.00 1.00 1.00
Table 2: Supervised results for all domains
dates based on the annotated human ratings. The up-
per bound for top-1 average rating is thus the high-
est average human rating of all label candidates for
a given topic, while the upper bound for the nDCG
measures will always be 1.
In addition to results for the combined candidate
set, we include results for each of the three candi-
date subsets, namely the primary Wikipedia labels
(?1??), the secondary Wikipedia labels (?2??) and
the top-5 topic terms (?Top5?); the nDCG results
are over the full candidate set only, as the numbers
aren?t directly comparable over the different subsets
(due to differences in the number of candidates and
the distribution of ratings).
Comparing the in-domain and cross-domain re-
sults, we observe that they are largely compara-
ble, with the exception of BOOKS, where there is
a noticeable drop in both top-1 average rating and
nDGC-1 when we use cross-domain training. We
see an appreciable drop in scores when we train
BOOKS against BLOGS (or vice versa), which we
analyse as being due to incompatibility in document
content and structure between these two domains.
Overall though, the results are very encouraging,
and point to the plausibility of using labelled topic
models from independent domains to learn the best
topic labels for a new domain.
Returning to the question of the suitability of la-
bel candidates for the highly specialised PUBMED
document collection, we first notice that the up-
per bound top-1 average rating is comparable to
the other domains, indicating that our method has
been able to extract equivalent-quality label can-
didates from Wikipedia. The top-1 average rat-
ings of the supervised method are lower than the
other domains. We hypothesise that the cause of
the drop is that the lexical association measures are
trained over highly diverse Wikipedia data rather
than biomedical-specific data, and predict that the
results would improve if we trained our features over
PubMed.
The results are uniformly better than the unsuper-
vised baselines for all four corpora, although there
is quite a bit of room for improvement relative to the
upper bound. To better gauge the quality of these
results, we carry out a direct comparison of our pro-
posed method with the best-performing method of
MSZ in Section 5.3.
1542
Looking to the top-1 average score results over the
different candidate sets, we observe first that the up-
per bound for the combined candidate set (?All?) is
higher than the scores for the candidate subsets in all
cases, underlining the complementarity of the differ-
ent candidate sets. We also observe that the top-5
topic term candidate set is the lowest performer out
of the three subsets across all four corpora, in terms
of both upper bound and the results for the super-
vised method. This reinforces our comments about
the inferiority of the topic word selection method of
Lau et al (2010) for topic labelling purposes. For
NEWS and PUBMED, there is a noticeable differ-
ence between the results of the supervised method
over the full candidate set and each of the candidate
subsets. In contrast, for BOOKS and BLOGS, the re-
sults for the primary candidate subset are at times
actually higher than those over the full candidate set
in most cases (but not for the upper bound). This is
due to the larger search space in the full candidate
set, and the higher median quality of candidates in
the primary candidate set.
5.3 Comparison with MSZ
The best performing method out of the suite of
approaches proposed by MSZ method exclusively
uses bigrams extracted from the document collec-
tion, ranked based on Student?s t-test. The potential
drawbacks to this approach are: all labels must be
bigrams, there must be explicit token instances of
a given bigram in the document collection for it to
be considered as a label candidate, and furthermore,
there must be enough token instances in the docu-
ment collection for it to have a high t score.
To better understand the performance difference
of our approach to that of MSZ, we perform direct
comparison of our proposed method with the bench-
mark method of MSZ.
5.3.1 Candidate Ranking
First, we compare the candidate ranking method-
ology of our method with that of MSZ, using the
label candidates extracted by the MSZ method.
We first extracted the top-2000 bigrams using the
N -gram Statistics Package (Banerjee and Pedersen,
2003). We then ranked the bigrams for each topic
using the Student?s t-test. We included the top-5 la-
bels generated for each topic by the MSZ method
in our Mechanical Turk annotation task, and use the
annotations to directly compare the two methods.
To measure the performance of candidate rank-
ing between our supervised method and MSZ?s, we
re-rank the top-5 labels extracted by MSZ using
our SVR methodology (in-domain) and compare the
top-1 average rating and nDCG scores. Results are
shown in Table 3. We do not include results for the
BOOKS domain because the text collection is much
larger than the other domains, and the computation
for the MSZ relevance score ranking is intractable
due to the number of n-grams (a significant short-
coming of the method).
Looking at the results for the other domains, it is
clear that our ranking system has the upper hand:
it consistently outperforms MSZ over every evalu-
ation metric.8 Comparing the top-1 average rating
results back to those in Table 2, we observe that
for all three domains, the results for MSZ are be-
low those of the unsupervised baseline, and well be-
low those of our supervised method. The nDCG re-
sults are more competitive, and the nDCG-3 results
are actually higher than our original results in Ta-
ble 2. It is important to bear in mind, however, that
the numbers are in each case relative to a different la-
bel candidate set. Additionally, the results in Table 3
are based on only 5 candidates, with a relatively flat
gold-standard rating distribution, making it easier to
achieve higher nDCG-5 scores.
5.3.2 Candidate Generation
The method of MSZ makes the implicit assump-
tion that good bigram labels are discoverable within
the document collection. In our method, on the other
hand, we (efficiently) access the much larger and
variable n-gram length set of English Wikipedia ar-
ticle titles, in addition to the top-5 topic terms. To
better understand the differences in label candidate
sets, and the relative coverage of the full label can-
didate set in each case, we conducted another survey
where human users were asked to suggest one topic
label for each topic presented.
The survey consisted, once again, of presenting
annotators with a topic, but in this case, we gave
them the open task of proposing the ideal label for
8Based on a single ANOVA, the difference in results is sta-
tistically significant at the 5% level for BLOGS, and 1% for
NEWS and PUBMED.
1543
Test Domain Candidate Ranking Top-1 nDCG-1 nDCG-3 nDCG-5System Avg. Rating
BLOGS
MSZ 1.26 0.65 0.76 0.87
SVR 1.41 0.75 0.85 0.92
Upper bound 1.87 1.00 1.00 1.00
NEWS
MSZ 1.37 0.73 0.81 0.90
SVR 1.66 0.88 0.90 0.95
Upper bound 1.86 1.00 1.00 1.00
PUBMED
MSZ 1.53 0.77 0.85 0.93
SVR 1.73 0.87 0.91 0.96
Upper bound 1.98 1.00 1.00 1.00
Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZ
the topic. In this, we did not enforce any restrictions
on the type or size of label (e.g. the number of terms
in the label).
Of the manually-generated gold-standard labels,
approximately 36% were contained in the original
document collection, but 60% were Wikipedia arti-
cle titles. This indicates that our method has greater
potential to generate a label of the quality of the ideal
proposed by a human in a completely open-ended
task.
6 Discussion
On the subject of suitability of using Amazon Me-
chanical Turk for natural language tasks, Snow et al
(2008) demonstrated that the quality of annotation
is comparable to that of expert annotators. With that
said, the PUBMED topics are still a subject of inter-
est, as these topics often contain biomedical terms
which could be difficult for the general populace to
annotate.
As the number of annotators per topic and the
number of annotations per annotator vary, there is
no immediate way to calculate the inter-annotator
agreement. Instead, we calculated the MAE score
for each candidate, which is an average of the ab-
solute difference between an annotator?s rating and
the average rating of a candidate, summed across all
candidates to get the MAE score for a given corpus.
The MAE scores for each corpus are shown in Ta-
ble 4, noting that a smaller value indicates higher
agreement.
As the table shows, the agreement for the
PUBMED domain is comparable with the other
datasets. BLOGS and NEWS have marginally better
Corpus MAE
BLOGS 0.50
BOOKS 0.56
NEWS 0.52
PUBMED 0.56
Table 4: Average MAE score for label candidate rating
over each corpus
agreement, almost certainly because of the greater
immediacy of the topics, covering everyday areas
such as lifestyle and politics. BOOKS topics are oc-
casionally difficult to label due to the breadth of the
domain; e.g. consider a topic containing terms ex-
tracted from Shakespeare sonnets.
7 Conclusion
This paper has presented the task of topic labelling,
that is the generation and scoring of labels for a
given topic. We generate a set of label candidates
from the top-ranking topic terms, titles of Wikipedia
articles containing the top-ranking topic terms, and
also a filtered set of sub-phrases extracted from the
Wikipedia article titles. We rank the label candidates
using a combination of association measures, lexical
features and an Information Retrieval feature. Our
method is shown to perform strongly over four inde-
pendent sets of topics, and also significantly better
than a competitor system.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
1544
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML 2006.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. In EACL 2009, pages 103?111.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans in-
terpret topic models. In NIPS, pages 288?296.
B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison
Wesley.
Y. Feng and M. Lapata. 2010. Topic models for im-
age annotation and text illustration. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 831?839, Los Angeles, USA, June.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using ontological and document similarity to
estimate museum exhibit relatedness. ACM Journal
on Computing and Cultural Heritage, 3(3):1?20.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In PNAS, volume 101, pages 5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
HLT: NAACL 2009, pages 362?370.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), pages 217?226,
New York, NY, USA. ACM.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin. 2010.
Best topic word selection for topic labelling. In Coling
2010: Posters, pages 605?613, Beijing, China.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA 2009, pages
1227?1232, Pisa, Italy.
Q. Mei, C. Liu, H. Su, and C. Zhai. 2006. A probabilistic
approach to spatiotemporal theme pattern mining on
weblogs. In WWW 2006, pages 533?542.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic labeling
of multinomial topic models. In SIGKDD, pages 490?
499.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Journal of Nat-
ural Language Processing, 7(3):207?223.
D. Newman, T. Baldwin, L. Cavedon, S. Karimi, D. Mar-
tinez, and J. Zobel. 2010a. Visualizing document col-
lections and search results using topic mapping. Jour-
nal of Web Semantics, 8(2-3):169?175.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010b. Automatic evaluation of topic coherence. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 100?108, Los Angeles,
USA, June. Association for Computational Linguis-
tics.
D. O` Se?aghdha. 2010. Latent variable models of selec-
tional preference. In ACL 2010.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In HLT/NAACL-04, pages
321?328.
P. Pecina. 2009. Lexical Association Measures: Collo-
cation Extraction. Ph.D. thesis, Charles University.
A. Ritter, Mausam, and O. Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL 2010.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?
263, Morristown, NJ, USA.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In WWW ?08,
pages 111?120.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In KDD, pages 424?433.
S. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR ?06, pages 178?
185.
1545
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266?270,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Relation Guided Bootstrapping of Semantic Lexicons
Tara McIntosh? Lars Yencken? James R. Curran? Timothy Baldwin?
? NICTA, Victoria Research Lab ? School of Information Technologies
Dept. of Computer Science and Software Engineering The University of Sydney
The University of Melbourne
nlp@taramcintosh.org james@it.usyd.edu.au
lars@yencken.org tb@ldwin.net
Abstract
State-of-the-art bootstrapping systems rely on
expert-crafted semantic constraints such as
negative categories to reduce semantic drift.
Unfortunately, their use introduces a substan-
tial amount of supervised knowledge. We
present the Relation Guided Bootstrapping
(RGB) algorithm, which simultaneously ex-
tracts lexicons and open relationships to guide
lexicon growth and reduce semantic drift.
This removes the necessity for manually craft-
ing category and relationship constraints, and
manually generating negative categories.
1 Introduction
Many approaches to extracting semantic lexicons
extend the unsupervised bootstrapping framework
(Riloff and Shepherd, 1997). These use a small set
of seed examples from the target lexicon to identify
contextual patterns which are then used to extract
new lexicon items (Riloff and Jones, 1999).
Bootstrappers are prone to semantic drift, caused
by selection of poor candidate terms or patterns
(Curran et al, 2007), which can be reduced by
semantically constraining the candidates. Multi-
category bootstrappers, such as NOMEN (Yangar-
ber et al, 2002) and WMEB (McIntosh and Curran,
2008), reduce semantic drift by extracting multiple
categories simultaneously in competition.
The inclusion of manually-crafted negative cate-
gories to multi-category bootstrappers achieves the
best results, by clarifying the boundaries between
categories (Yangarber et al, 2002). For exam-
ple, female names are often bootstrapped with
the negative categories flowers (e.g. Rose, Iris)
and gem stones (e.g. Ruby, Pearl) (Curran et al,
2007). Unfortunately, negative categories are dif-
ficult to design, introducing a substantial amount
of human expertise into an otherwise unsupervised
framework. McIntosh (2010) made some progress
towards automatically learning useful negative cate-
gories during bootstrapping.
In this work we identify an unsupervised source
of semantic constraints inspired by the Coupled Pat-
tern Learner (CPL, Carlson et al (2010)). In CPL,
relation bootstrapping is coupled with lexicon boot-
strapping in order to control semantic drift in the
target relation?s arguments. Semantic constraints
on categories and relations are manually crafted in
CPL. For example, a candidate of the relation IS-
CEOOF will only be extracted if its arguments can
be extracted into the ceo and company lexicons
and a ceo is constrained to not be a celebrity
or politician. Negative examples such as IS-
CEOOF(Sergey Brin, Google) are also introduced to
clarify boundary conditions. CPL employs a large
number of these manually-crafted constraints to im-
prove precision at the expense of recall (only 18 IS-
CEOOF instances were extracted). In our approach,
we exploit open relation bootstrapping to minimise
semantic drift, without any manual seeding of rela-
tions or pre-defined category lexicon combinations.
Orthogonal to these seeded and constraint-based
methods is the relation-independent Open Informa-
tion Extraction (OPENIE) paradigm. OPENIE sys-
tems, such as TEXTRUNNER (Banko et al, 2007),
define neither lexicon categories nor predefined re-
lationships. They extract relation tuples by exploit-
266
ing broad syntactic patterns that are likely to indi-
cate relations. This enables the extraction of inter-
esting and unanticipated relations from text. How-
ever these patterns are often too broad, resulting in
the extraction of tuples that do not represent rela-
tions at all. As a result, heavy (supervised) post-
processing or use of supervised information is nec-
essary. For example, Christensen et al (2010) im-
prove TEXTRUNNER precision by using deep pars-
ing information via semantic role labelling.
2 Relation Guided Bootstrapping
Rather than relying on manually-crafted category
and relation constraints, Relation Guided Bootstrap-
ping (RGB) automatically detects, seeds and boot-
straps open relations between the target categories.
These relations anchor categories together, e.g. IS-
CEOOF and ISFOUNDEROF anchor person and
company, preventing them from drifting into other
categories. Relations can also identify new terms.
We demonstrate that this relation guidance effec-
tively reduces semantic drift, with performance ap-
proaching manually-crafted constraints.
RGB can be applied to any multi-category boot-
strapper, and in these experiments we use WMEB
(McIntosh and Curran, 2008), as shown in Figure 1.
RGB alternates between two phases of WMEB, one
for terms and the other for relations, with a one-off
relation discovery phase in between.
Term Extraction
The first stage of RGB follows the term extraction
process of WMEB. Each category is initialised by a
set of hand-picked seed terms. In each iteration, a
category?s terms are used to identify candidate pat-
terns that can match the terms in the text. Seman-
tic drift is reduced by forcing the categories to be
mutually exclusive (i.e. patterns must be nominated
by only one category). The remaining patterns are
ranked according to reliability and relevance, and
the top-n patterns are then added to the pattern set.1
The reliability of a pattern for a given category is
the number of extracted terms in the category?s lex-
icon that match the pattern. A pattern?s relevance
weight is defined as the sum of the ?2 values be-
tween the pattern (p) and each of the lexicon terms
1In this work, n is set to 5.
WMEB
WMEB
lexicon
Person
get patterns
get terms
lexicon
Company
get patterns
get terms
relation
get patterns
get tuples
? ?
? ?
arg ? 
arg ? 
relation 
discovery
Lee Scott, Walmart
Sergey Brin, Google
Joe Bloggs, Walmart
T
e
r
m
 
e
x
t
r
a
c
t
i
o
n
R
e
l
a
t
i
o
n
 
e
x
t
r
a
c
t
i
o
n
Figure 1: Relation Guided Bootstrapping framework
(t): weight(p) =
?
t?T ?
2(p, t). These metrics are
symmetrical for both candidate terms and pattern.
In WMEB?s term selection phase, a category?s pat-
tern set is used to identify candidate terms. Like the
candidate patterns, terms matching multiple cate-
gories are excluded. The remaining terms are ranked
and the top-n terms are added to the lexicon.
Relation Discovery
In CPL (Carlson et al, 2010), a relation is instanti-
ated with manually-crafted seed tuples and patterns.
In RGB, the relations and their seeds are automati-
cally identified in relation discovery. Relation dis-
covery is only performed once after the first 20 iter-
ations of term extraction, which ensures the lexicons
have adequate coverage to form potential relations.
Each ordered pair of categories (C1, C2) = R1,2
is checked for open (not pre-defined) relations be-
tween C1 and C2. This check removes all pairs of
terms, tuples (t1, t2) ? C1 ? C2 with freq(t1, t2) <
5 and a cooccurrence score ?2(t1, t2) ? 0.2 If R1,2
has fewer than 10 remaining tuples, it is discarded.
The tuples for R1,2 are then used to find its ini-
tial set of relation patterns. Each pattern must match
more than one tuple and must be mutually exclusive
between the relations. If fewer than n relation pat-
terns are found forR1,2, it is discarded. At this stage
2This cut-off is used as the ?2 statistic is sensitive to low
frequencies.
267
TYPE 5gm 5gm + 4gm 5gm + DC
Terms 1 347 002
Patterns 4 090 412
Tuples 2 114 243 3 470 206 14 369 673
Relation Patterns 5 523 473 10 317 703 31 867 250
Table 1: Statistics of three filtered MEDLINE datasets
we have identified the open relations that link cate-
gories together and their initial extraction patterns.
Using the initial relation patterns, the top-n mu-
tually exclusive seed tuples are identified for the re-
lation R1,2. In CPL, these tuple seeds are manually
crafted. Note that R1,2 can represent multiple rela-
tions betweenC1 andC2, which may not apply to all
of the seeds, e.g. isCeoOf and isEmployedBy.
We discover two types of relations, inter-category
relations where C1 6= C2, and intra-category rela-
tions where C1 = C2.
Relation Extraction
The relation extraction phase involves running
WMEB over tuples rather than terms. If multiple re-
lations are found, e.g. R1,2 and R2,3, these are boot-
strapped simultaneously, competing with each other
for tuples and relation patterns. Mutual exclusion
constraints between the relations are also forced.
In each iteration, a relation?s set of tuples is used
to identify candidate relation patterns, as for term
extraction. The top-n non-overlapping patterns are
extracted for each relation, and are used to identify
the top-n candidate tuples. The tuples are scored
similarly to the relation patterns, and any tuple iden-
tified by multiple relations is excluded.
For tuple extraction, a relation R1,2 is constrained
to only consider candidates where either t1 or t2
has previously been extracted into C1 or C2, respec-
tively. To extract a candidate tuple with an unknown
term, the term must also be a valid candidate of its
associated category. That is, the term must match
at least one pattern assigned to the category and not
match patterns assigned to another category.
This type-checking anchors relations to the cat-
egories they link together, limiting their drift into
other relations. It also provides guided term growth
in the categories they link. The growth is ?guided?
because the relations define, semantically coher-
ent subregions of the category search spaces. For
example, ISCEOOF defines the subregion ceo
CAT DESCRIPTION
ANTI Antibodies: MAb IgG IgM rituximab infliximab
CELL Cells: RBC HUVEC BAEC VSMC SMC
CLNE Cell lines: PC12 CHO HeLa Jurkat COS
DISE Diseases: asthma hepatitis tuberculosis HIV malaria
DRUG Drugs: acetylcholine carbachol heparin penicillin
tetracyclin
FUNC Molecular functions and processes:
kinase ligase acetyltransferase helicase binding
MUTN Mutations: Leiden C677T C282Y 35delG null
PROT Proteins and genes: p53 actin collagen albumin IL-6
SIGN Signs and symptoms: anemia cough fever
hypertension hyperglycemia
TUMR Tumors: lymphoma sarcoma melanoma
neuroblastoma osteosarcoma
Table 2: The MEDLINE semantic categories
within person. This guidance reduces semantic
drift.
3 Experimental Setup
To compare the effectiveness of RGB we consider
the task of extracting biomedical semantic lexi-
cons, building on the work of McIntosh and Curran
(2008). Note however the method is equally appli-
cable to any corpus and set of semantic categories.
The corpus consists of approximately 18.5 mil-
lion MEDLINE abstracts (up to Nov 2009). The text
was tokenised and POS-tagged using bio-specific
NLP tools (Grover et al, 2006), and parsed using
the biomedical C&C CCG parser (Rimell and Clark,
2009; Clark and Curran, 2007).
The term extraction data is formed from the raw
5-grams (t1, t2, t3, t4, t5), where the set of candi-
date terms correspond to the middle tokens (t3) and
the patterns are formed from the surrounding tokens
(t1, t2, t4, t5). The relation extraction data is also
formed from the 5-grams. The candidate tuples cor-
respond to the tokens (t1, t5) and the patterns are
formed from the intervening tokens (t2, t3, t4).
The second relation dataset (5gm + 4gm), also in-
cludes length 2 patterns formed from 4-grams. The
final relation dataset (5gm + DC) includes depen-
dency chains up to length 5 as the patterns between
terms (Greenwood et al, 2005). These chains are
formed using the Stanford dependencies generated
by the Rimell and Clark (2009) parser. All candi-
dates occurring less than 10 times were filtered. The
sizes of the resulting datasets are shown in Table 1.
268
1-500 501-1000 1-1000
WMEB 76.1 56.4 66.3
+negative 86.9 68.7 77.8
intra-RGB 75.7 62.7 69.2
+negative 87.4 72.4 79.9
inter-RGB 80.5 69.9 75.1
+negative 87.7 76.4 82.0
mixed-RGB 74.7 69.9 72.3
+negative 87.9 73.5 80.7
Table 3: Performance comparison of WMEB and RGB
We follow McIntosh and Curran (2009) in us-
ing the 10 biomedical semantic categories and
their hand-picked seeds in Table 2, and manu-
ally crafted negative categories: amino acid,
animal, body part and organism. Our eval-
uation process involved manually judging each ex-
tracted term and we calculate the average precision
of the top-1000 terms over the 10 target categories.
We do not calculate recall, due to the open-ended
nature of the categories.
4 Results and Discussion
Table 3 compares the performance of WMEB and
RGB, with and without the negative categories. For
RGB, we compare intra-, inter- and mixed relation
types, and use the 5gm format of tuples and relation
patterns. In WMEB, drift dominates in the later iter-
ations with ?19% precision drop between the first
and last 500 terms. The manually-crafted negative
categories give a substantial boost in precision on
both the first and last 500 terms (+11.5% overall).
Over the top 1000 terms, RGB significantly out-
performs the corresponding WMEB with and with-
out negative categories (p < 0.05).3 In particu-
lar, inter-RGB significantly improves upon WMEB
with no negative categories (501-1000: +13.5%,
1-1000: +8.8%). In similar experiments, NEG-
FINDER, used during bootstrapping, was shown to
increase precision by ?5% (McIntosh, 2010). Inter-
RGB without negatives approaches the precision of
WMEB with the negatives, trailing only by 2.7%
overall. This demonstrates that RGB effectively re-
duces the reliance on manually-crafted negative cat-
egories for lexicon bootstrapping.
The use of intra-category relations was far less
3Significance was tested using intensive randomisation tests.
INTER-RGB 1-500 501-1000 1-1000
5gm 80.5 69.9 75.1
+negative 87.7 76.4 82.0
5gm + 4gm 79.6 71.5 75.5
+negative 87.7 76.1 81.9
5gm + DC 77.2 70.1 73.5
+negative 86.6 80.2 83.5
Table 4: Comparison of different relation pattern types
effective than inter-category relations, and the com-
bination of intra- and inter- was less effective than
just using inter-category relations. In intra-RGB the
categories are more susceptible to single-category
drift. The additional constraints provided by anchor-
ing two categories appear to make inter-RGB less
susceptible to drift. Many intra-category relations
represent listings commonly identified by conjunc-
tions. However, these patterns are identified by mul-
tiple intra-category relations and are excluded.
Through manual inspection of inter-RGB?s tuples
and patterns, we identified numerous meaningful re-
lations, such as isExpressedIn(prot, cell).
Relations like this helped to reduce semantic drift
within the CELL lexicon by up to 23%.
Table 4 compares the effect of different relation
pattern representations on the performance of inter-
RGB. The 5gm+4gm data, which doubles the num-
ber of possible candidate relation patterns, performs
similarly to the 5gm representation. Adding depen-
dency chains decreased and increased precision de-
pending on whether negative categories were used.
In Wu and Weld (2010), the performance of an
OPENIE system was significantly improved by us-
ing patterns formed from dependency parses. How-
ever in our DC experiments, the earlier bootstrap-
ping iterations were less precise than the simple
5gm+4gm and 5gm representations. Since the
chains can be as short as two dependencies, some
of these patterns may not be specific enough. These
results demonstrate that useful open relations can be
represented using only n-grams.
5 Conclusion
In this paper, we have proposed Relation Guided
Bootstrapping (RGB), an unsupervised approach to
discovering and seeding open relations to constrain
semantic lexicon bootstrapping.
269
Previous work used manually-crafted lexical and
relation constraints to improve relation extraction
(Carlson et al, 2010). We turn this idea on its head,
by using open relation extraction to provide con-
straints for lexicon bootstrapping, and automatically
discover the open relations and their seeds from the
expanding bootstrapped lexicons.
RGB effectively reduces semantic drift delivering
performance comparable to state-of-the-art systems
that rely on manually-crafted negative constraints.
Acknowledgements
We would like to thank Dr Cassie Thornley, our sec-
ond evaluator, and the reviewers for their helpful
feedback. NICTA is funded by the Australian Gov-
ernment as represented by the Department of Broad-
band, Communications and the Digital Economy
and the Australian Research Council through the
ICT Centre of Excellence program. This work has
been supported by the Australian Research Council
under Discovery Project DP1097291 and the Capital
Markets Cooperative Research Centre.
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, Hyderabad, India.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining,
pages 101?110, New York, USA.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 52?60, Los Angeles, California, USA, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180, Melbourne, Australia.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automatically
acquiring a linguistically motivated genic interaction
extraction system. In Proceedings of the 4th Learn-
ing Language in Logic Workshop, pages 46?52, Bonn,
Germany.
Claire Grover, Michael Matthews, and Richard Tobin.
2006. Tools to address the interdependence between
tokenisation and standoff annotation. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 19?26, Trento, Italy.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 97?105, Hobart, Australia.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, pages 396?404, Suntec, Singapore, Au-
gust.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 356?365,
Boston, USA.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the 16th National Conference
on Artificial Intelligence and the 11th Innovative Ap-
plications of Artificial Intelligence Conference, pages
474?479, Orlando, USA.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124,
Providence, USA.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, pages 852?865.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association of Computational
Linguistics, pages 118?127, Uppsala, Sweden.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 1135?1141, Taipei,
Taiwan.
270
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 25?30,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
langid.py: An Off-the-shelf Language Identification Tool
Marco Lui and Timothy Baldwin
NICTA VRL
Department of Computing and Information Systems
University of Melbourne, VIC 3010, Australia
mhlui@unimelb.edu.au, tb@ldwin.net
Abstract
We present langid.py, an off-the-shelf lan-
guage identification tool. We discuss the de-
sign and implementation of langid.py, and
provide an empirical comparison on 5 long-
document datasets, and 2 datasets from the mi-
croblog domain. We find that langid.py
maintains consistently high accuracy across
all domains, making it ideal for end-users that
require language identification without want-
ing to invest in preparation of in-domain train-
ing data.
1 Introduction
Language identification (LangID) is the task of de-
termining the natural language that a document is
written in. It is a key step in automatic processing
of real-world data, where a multitude of languages
may be present. Natural language processing tech-
niques typically pre-suppose that all documents be-
ing processed are written in a given language (e.g.
English), but as focus shifts onto processing docu-
ments from internet sources such as microblogging
services, this becomes increasingly difficult to guar-
antee. Language identification is also a key compo-
nent of many web services. For example, the lan-
guage that a web page is written in is an important
consideration in determining whether it is likely to
be of interest to a particular user of a search engine,
and automatic identification is an essential step in
building language corpora from the web. It has prac-
tical implications for social networking and social
media, where it may be desirable to organize com-
ments and other user-generated content by language.
It also has implications for accessibility, since it en-
ables automatic determination of the target language
for automatic machine translation purposes.
Many applications could potentially benefit from
automatic language identification, but building a
customized solution per-application is prohibitively
expensive, especially if human annotation is re-
quired to produce a corpus of language-labelled
training documents from the application domain.
What is required is thus a generic language identi-
fication tool that is usable off-the-shelf, i.e. with no
end-user training and minimal configuration.
In this paper, we present langid.py, a LangID
tool with the following characteristics: (1) fast,
(2) usable off-the-shelf, (3) unaffected by domain-
specific features (e.g. HTML, XML, markdown),
(4) single file with minimal dependencies, and (5)
flexible interface
2 Methodology
langid.py is trained over a naive Bayes clas-
sifier with a multinomial event model (McCallum
and Nigam, 1998), over a mixture of byte n-grams
(1?n?4). One key difference from conventional
text categorization solutions is that langid.py
was designed to be used off-the-shelf. Since
langid.py implements a supervised classifier,
this presents two primary challenges: (1) a pre-
trained model must be distributed with the classi-
fier, and (2) the model must generalize to data from
different domains, meaning that in its default con-
figuration, it must have good accuracy over inputs
as diverse as web pages, newspaper articles and mi-
croblog messages. (1) is mostly a practical consid-
eration, and so we will address it in Section 3. In
order to address (2), we integrate information about
the language identification task from a variety of do-
mains by using LD feature selection (Lui and Bald-
win, 2011).
Lui and Baldwin (2011) showed that it is rela-
tively easy to attain high accuracy for language iden-
25
Dataset Documents Langs Doc Length (bytes)
EUROGOV 1500 10 1.7?104 ?3.9?104
TCL 3174 60 2.6?103 ?3.8?103
WIKIPEDIA 4963 67 1.5?103 ?4.1?103
EMEA 19988 22 2.9?105 ?7.9?105
EUROPARL 20828 22 1.7?102 ?1.6?102
T-BE 9659 6 1.0?102 ?3.2?101
T-SC 5000 5 8.8?101 ?3.9?101
Table 1: Summary of the LangID datasets
tification in a traditional text categorization setting,
where we have in-domain training data. The task be-
comes much harder when trying to perform domain
adaptation, that is, trying to use model parameters
learned in one domain to classify data from a dif-
ferent domain. LD feature selection addresses this
problem by focusing on key features that are relevant
to the language identification task. It is based on In-
formation Gain (IG), originally introduced as a split-
ting criteria for decision trees (Quinlan, 1986), and
later shown to be effective for feature selection in
text categorization (Yang and Pedersen, 1997; For-
man, 2003). LD represents the difference in IG with
respect to language and domain. Features with a
high LD score are informative about language with-
out being informative about domain. For practi-
cal reasons, before the IG calculation the candidate
feature set is pruned by means of a term-frequency
based feature selection.
Lui and Baldwin (2011) presented empirical evi-
dence that LD feature selection was effective for do-
main adaptation in language identification. This re-
sult is further supported by our evaluation, presented
in Section 5.
3 System Architecture
The full langid.py package consists of the
language identifier langid.py, as well as two
support modules LDfeatureselect.py and
train.py.
langid.py is the single file which packages the
language identification tool, and the only file needed
to use langid.py for off-the-shelf language iden-
tification. It comes with an embedded model which
covers 97 languages using training data drawn from
5 domains. Tokenization and feature selection are
carried out in a single pass over the input document
via Aho-Corasick string matching (Aho and Cora-
sick, 1975). The Aho-Corasick string matching al-
gorithm processes an input by means of a determin-
istic finite automaton (DFA). Some states of the au-
tomaton are associated with the completion of one
of the n-grams selected through LD feature selec-
tion. Thus, we can obtain our document represen-
tation by simply counting the number of times the
DFA enters particular states while processing our in-
put. The DFA and the associated mapping from state
to n-gram are constructed during the training phase,
and embedded as part of the pre-trained model.
The naive Bayes classifier is implemented using
numpy,1 the de-facto numerical computation pack-
age for Python. numpy is free and open source, and
available for all major platforms. Using numpy in-
troduces a dependency on a library that is not in the
Python standard library. This is a reasonable trade-
off, as numpy provides us with an optimized im-
plementation of matrix operations, which allows us
to implement fast naive Bayes classification while
maintaining the single-file concept of langid.py.
langid.py can be used in the three ways:
Command-line tool: langid.py supports an
interactive mode with a text prompt and line-by-line
classification. This mode is suitable for quick in-
teractive queries, as well as for demonstration pur-
poses. langid.py also supports language identi-
fication of entire files via redirection. This allows a
user to interactively explore data, as well as to inte-
grate language identification into a pipeline of other
unix-style tools. However, use via redirection is
not recommended for large quantities of documents
as each invocation requires the trained model to be
unpacked into memory. Where large quantities of
documents are being processed, use as a library or
web service is preferred as the model will only be
unpacked once upon initialization.
Python library: langid.py can be imported as
a Python module, and provides a function that ac-
cepts text and returns the identified language of the
text. This use of langid.py is the fastest in a
single-processor setting as it incurs the least over-
head.
Web service: langid.py can be started as a
web service with a command-line switch. This
1http://numpy.scipy.org
26
allows language identitication by means of HTTP
PUT and HTTP POST requests, which return JSON-
encoded responses. This is the preferred method of
using langid.py from other programming envi-
ronments, as most languages include libraries for in-
teracting with web services over HTTP. It also al-
lows the language identification service to be run as
a network/internet service. Finally, langid.py is
WSGI-compliant,2 so it can be deployed in a WSGI-
compliant web server. This provides an easy way to
achieve parallelism by leveraging existing technolo-
gies to manage load balancing and utilize multiple
processors in the handling of multiple concurrent re-
quests for a service.
LDfeatureselect.py implements the LD
feature selection. The calculation of term frequency
is done in constant memory by index inversion
through a MapReduce-style sharding approach. The
calculation of information gain is also chunked to
limit peak memory use, and furthermore it is paral-
lelized to make full use of modern multiprocessor
systems. LDfeatureselect.py produces a list
of byte n-grams ranked by their LD score.
train.py implements estimation of parameters
for the multinomial naive Bayes model, as well as
the construction of the DFA for the Aho-Corasick
string matching algorithm. Its input is a list of byte
patterns representing a feature set (such as that se-
lected via LDfeatureselect.py), and a corpus
of training documents. It produces the final model as
a single compressed, encoded string, which can be
saved to an external file and used by langid.py
via a command-line option.
4 Training Data
langid.py is distributed with an embedded
model trained using the multi-domain language
identification corpus of Lui and Baldwin (2011).
This corpus contains documents in a total of 97 lan-
guages. The data is drawn from 5 different do-
mains: government documents, software documen-
tation, newswire, online encyclopedia and an inter-
net crawl, though no domain covers the full set of
languages by itself, and some languages are present
only in a single domain. More details about this cor-
pus are given in Lui and Baldwin (2011).
2http://www.wsgi.org
We do not perform explicit encoding detection,
but we do not assume that all the data is in the same
encoding. Previous research has shown that explicit
encoding detection is not needed for language iden-
tification (Baldwin and Lui, 2010). Our training data
consists mostly of UTF8-encoded documents, but
some of our evaluation datasets contain a mixture
of encodings.
5 Evaluation
In order to benchmark langid.py, we carried out
an empirical evaluation using a number of language-
labelled datasets. We compare the empirical results
obtained from langid.py to those obtained from
other language identification toolkits which incor-
porate a pre-trained model, and are thus usable off-
the-shelf for language identification. These tools are
listed in Table 3.
5.1 Off-the-shelf LangID tools
TextCat is an implementation of the method of
Cavnar and Trenkle (1994) by Gertjan van Noord.
It has traditionally been the de facto LangID tool of
choice in research, and is the basis of language iden-
tification/filtering in the ClueWeb09 Dataset (Callan
and Hoy, 2009) and CorpusBuilder (Ghani et al,
2004). It includes support for training with user-
supplied data.
LangDetect implements a Naive Bayes classi-
fier, using a character n-gram based representation
without feature selection, with a set of normaliza-
tion heuristics to improve accuracy. It is trained on
data from Wikipedia,3 and can be trained with user-
supplied data.
CLD is a port of the embedded language identi-
fier in Google?s Chromium browser, maintained by
Mike McCandless. Not much is known about the
internal design of the tool, and there is no support
provided for re-training it.
The datasets come from a variety of domains,
such as newswire (TCL), biomedical corpora
(EMEA), government documents (EUROGOV, EU-
ROPARL) and microblog services (T-BE, T-SC). A
number of these datasets have been previously used
in language identification research. We provide a
3http://www.wikipedia.org
27
Test Dataset langid.py LangDetect TextCat CLDAccuracy docs/s ?Acc Slowdown ?Acc Slowdown ?Acc Slowdown
EUROGOV 0.987 70.5 +0.005 1.1? ?0.046 31.1? ?0.004 0.5?
TCL 0.904 185.4 ?0.086 2.1? ?0.299 24.2? ?0.172 0.5?
WIKIPEDIA 0.913 227.6 ?0.046 2.5? ?0.207 99.9? ?0.082 0.9?
EMEA 0.934 7.7 ?0.820 0.2? ?0.572 6.3? +0.044 0.3?
EUROPARL 0.992 294.3 +0.001 3.6? ?0.186 115.4? ?0.010 0.2?
T-BE 0.941 367.9 ?0.016 4.4? ?0.210 144.1? ?0.081 0.7?
T-SC 0.886 298.2 ?0.038 2.9? ?0.235 34.2? ?0.120 0.2?
Table 2: Comparison of standalone classification tools, in terms of accuracy and speed (documents/second), relative
to langid.py
Tool Languages URL
langid.py 97 http://www.csse.unimelb.edu.au/research/lt/resources/langid/
LangDetect 53 http://code.google.com/p/language-detection/
TextCat 75 http://odur.let.rug.nl/vannoord/TextCat/
CLD 64+ http://code.google.com/p/chromium-compact-language-detector/
Table 3: Summary of the LangID tools compared
brief summary of the characteristics of each dataset
in Table 1.
The datasets we use for evaluation are differ-
ent from and independent of the datasets from
which the embedded model of langid.py was
produced. In Table 2, we report the accuracy of
each tool, measured as the proportion of documents
from each dataset that are correctly classified. We
present the absolute accuracy and performance for
langid.py, and relative accuracy and slowdown
for the other systems. For this experiment, we used
a machine with 2 Intel Xeon E5540 processors and
24GB of RAM. We only utilized a single core, as
none of the language identification tools tested are
inherently multicore.
5.2 Comparison on standard datasets
We compared the four systems on datasets used in
previous language identification research (Baldwin
and Lui, 2010) (EUROGOV, TCL, WIKIPEDIA), as
well as an extract from a biomedical parallel cor-
pus (Tiedemann, 2009) (EMEA) and a corpus of
samples from the Europarl Parallel Corpus (Koehn,
2005) (EUROPARL). The sample of EUROPARL
we use was originally prepared by Shuyo Nakatani
(author of LangDetect) as a validation set.
langid.py compares very favorably with other
language identification tools. It outperforms
TextCat in terms of speed and accuracy on all of
the datasets considered. langid.py is generally
orders of magnitude faster than TextCat, but this
advantage is reduced on larger documents. This is
primarily due to the design of TextCat, which re-
quires that the supplied models be read from file for
each document classified.
langid.py generally outperforms
LangDetect, except in datasets derived from
government documents (EUROGOV, EUROPARL).
However, the difference in accuracy between
langid.py and LangDetect on such datasets
is very small, and langid.py is generally faster.
An abnormal result was obtained when testing
LangDetect on the EMEA corpus. Here,
LangDetect is much faster, but has extremely
poor accuracy (0.114). Analysis of the results re-
veals that the majority of documents were classified
as Polish. We suspect that this is due to the early
termination criteria employed by LangDetect,
together with specific characteristics of the corpus.
TextCat also performed very poorly on this
corpus (accuracy 0.362). However, it is important
to note that langid.py and CLD both performed
very well, providing evidence that it is possible to
build a generic language identifier that is insensitive
to domain-specific characteristics.
langid.py also compares well with CLD. It is
generally more accurate, although CLD does bet-
ter on the EMEA corpus. This may reveal some
insight into the design of CLD, which is likely to
have been tuned for language identification of web
28
pages. The EMEA corpus is heavy in XML markup,
which CLD and langid.py both successfully ig-
nore. One area where CLD outperforms all other sys-
tems is in its speed. However, this increase in speed
comes at the cost of decreased accuracy in other do-
mains, as we will see in Section 5.3.
5.3 Comparison on microblog messages
The size of the input text is known to play a sig-
nificant role in the accuracy of automatic language
identification, with accuracy decreasing on shorter
input documents (Cavnar and Trenkle, 1994; Sibun
and Reynar, 1996; Baldwin and Lui, 2010).
Recently, language identification of short strings
has generated interest in the research community.
Hammarstrom (2007) described a method that aug-
mented a dictionary with an affix table, and tested it
over synthetic data derived from a parallel bible cor-
pus. Ceylan and Kim (2009) compared a number of
methods for identifying the language of search en-
gine queries of 2 to 3 words. They develop a method
which uses a decision tree to integrate outputs from
several different language identification approaches.
Vatanen et al (2010) focus on messages of 5?21
characters, using n-gram language models over data
drawn from UDHR in a naive Bayes classifier.
A recent application where language identifica-
tion is an open issue is over the rapidly-increasing
volume of data being generated by social media.
Microblog services such as Twitter4 allow users to
post short text messages. Twitter has a worldwide
user base, evidenced by the large array of languages
present on Twitter (Carter et al, to appear). It is es-
timated that half the messages on Twitter are not in
English. 5
This new domain presents a significant challenge
for automatic language identification, due to the
much shorter ?documents? to be classified, and is
compounded by the lack of language-labelled in-
domain data for training and validation. This has led
to recent research focused specifically on the task of
language identification of Twitter messages. Carter
et al (to appear) improve language identification in
Twitter messages by augmenting standard methods
4http://www.twitter.com
5http://semiocast.com/downloads/
Semiocast_Half_of_messages_on_Twitter_
are_not_in_English_20100224.pdf
with language identification priors based on a user?s
previous messages and by the content of links em-
bedded in messages. Tromp and Pechenizkiy (2011)
present a method for language identification of short
text messages by means of a graph structure.
Despite the recently published results on language
identification of microblog messages, there is no
dedicated off-the-shelf system to perform the task.
We thus examine the accuracy and performance of
using generic language identification tools to iden-
tify the language of microblog messages. It is im-
portant to note that none of the systems we test have
been specifically tuned for the microblog domain.
Furthermore, they do not make use of any non-
textual information such as author and link-based
priors (Carter et al, to appear).
We make use of two datasets of Twitter messages
kindly provided to us by other researchers. The first
is T-BE (Tromp and Pechenizkiy, 2011), which con-
tains 9659 messages in 6 European languages. The
second is T-SC (Carter et al, to appear), which con-
tains 5000 messages in 5 European languages.
We find that over both datasets, langid.py has
better accuracy than any of the other systems tested.
On T-BE, Tromp and Pechenizkiy (2011) report
accuracy between 0.92 and 0.98 depending on the
parametrization of their system, which was tuned
specifically for classifying short text messages. In
its off-the-shelf configuration, langid.py attains
an accuracy of 0.94, making it competitive with
the customized solution of Tromp and Pechenizkiy
(2011).
On T-SC, Carter et al (to appear) report over-
all accuracy of 0.90 for TextCat in the off-the-
shelf configuration, and up to 0.92 after the inclusion
of priors based on (domain-specific) extra-textual
information. In our experiments, the accuracy of
TextCat is much lower (0.654). This is because
Carter et al (to appear) constrained TextCat to
output only the set of 5 languages they considered.
Our results show that it is possible for a generic lan-
guage identification tool to attain reasonably high
accuracy (0.89) without artificially constraining the
set of languages to be considered, which corre-
sponds more closely to the demands of automatic
language identification to real-world data sources,
where there is generally no prior knowledge of the
languages present.
29
We also observe that while CLD is still the fastest
classifier, this has come at the cost of accuracy in an
alternative domain such as Twitter messages, where
both langid.py and LangDetect attain better
accuracy than CLD.
An interesting point of comparison between the
Twitter datasets is how the accuracy of all systems
is generally higher on T-BE than on T-SC, despite
them covering essentially the same languages (T-BE
includes Italian, whereas T-SC does not). This is
likely to be because the T-BE dataset was produced
using a semi-automatic method which involved a
language identification step using the method of
Cavnar and Trenkle (1994) (E Tromp, personal com-
munication, July 6 2011). This may also explain
why TextCat, which is also based on Cavnar and
Trenkle?s work, has unusually high accuracy on this
dataset.
6 Conclusion
In this paper, we presented langid.py, an off-the-
shelf language identification solution. We demon-
strated the robustness of the tool over a range of test
corpora of both long and short documents (including
micro-blogs).
Acknowledgments
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
string matching: an aid to bibliographic search. Com-
munications of the ACM, 18(6):333?340, June.
Timothy Baldwin and Marco Lui. 2010. Language iden-
tification: The long and the short of the matter. In Pro-
ceedings of NAACL HLT 2010, pages 229?237, Los
Angeles, USA.
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
to appear. Microblog language identification: Over-
coming the limitations of short, unedited and idiomatic
text. Language Resources and Evaluation Journal.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
of ACL2009, pages 1066?1074, Singapore.
George Forman. 2003. An Extensive Empirical Study
of Feature Selection Metrics for Text Classification.
Journal of Machine Learning Research, 3(7-8):1289?
1305, October.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building Minority Language Corpora by Learning to
Generate Web Search Queries. Knowledge and Infor-
mation Systems, 7(1):56?83, February.
Harald Hammarstrom. 2007. A Fine-Grained Model for
Language Identication. In Proceedings of iNEWS07,
pages 14?20.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 11.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of 5th International Joint Conference on Nat-
ural Language Processing, pages 553?561, Chiang
Mai, Thailand.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, Madison, USA.
J.R. Quinlan. 1986. Induction of Decision Trees. Ma-
chine Learning, 1(1):81?106, October.
Penelope Sibun and Jeffrey C. Reynar. 1996. Language
determination: Examining the issues. In Proceedings
of the 5th Annual Symposium on Document Analysis
and Information Retrieval, pages 125?135, Las Vegas,
USA.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. Recent Advances in Natural Language Process-
ing, V:237?248.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
Based N-gram Language Identification on Short Texts.
In Proceedings of Benelearn 2011, pages 27?35, The
Hague, Netherlands.
Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.
2010. Language identification of short text segments
with n-gram models. In Proceedings of LREC 2010,
pages 3423?3430.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML 97.
30
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?12,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Stacking-based Approach to Twitter User Geolocation Prediction
Bo Han,?? Paul Cook,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
hanb@student.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
We implement a city-level geolocation
prediction system for Twitter users. The
system infers a user?s location based on
both tweet text and user-declared metadata
using a stacking approach. We demon-
strate that the stacking method substan-
tially outperforms benchmark methods,
achieving 49% accuracy on a benchmark
dataset. We further evaluate our method
on a recent crawl of Twitter data to in-
vestigate the impact of temporal factors
on model generalisation. Our results sug-
gest that user-declared location metadata
is more sensitive to temporal change than
the text of Twitter messages. We also de-
scribe two ways of accessing/demoing our
system.
1 Introduction
In this paper, we present and evaluate a geoloca-
tion prediction method for Twitter users.1 Given
a user?s tweet data as input, the task of user level
geolocation prediction is to infer a primary loca-
tion (i.e., ?home location?: Mahmud et al (2012))
for the user from a discrete set of pre-defined loca-
tions (Cheng et al, 2010). For instance, President
Obama?s location might be predicted to be Wash-
ington D.C., USA, based on his public tweets and
profile metadata.
Geolocation information is essential to location-
based applications, like targeted advertising and
local event detection (Sakaki et al, 2010;
MacEachren et al, 2011). However, the means
to obtain such information are limited. Although
Twitter allows users to specify a plain text de-
scription of their location in their profile, these de-
scriptions tend to be ad hoc and unreliable (Cheng
1We only use public Twitter data for experiments and ex-
emplification in this study.
et al, 2010). Recently, user geolocation predic-
tion based on a user?s tweets has become popular
(Wing and Baldridge, 2011; Roller et al, 2012),
based on the assumption that tweets implicitly
contain locating information, and with appropri-
ate statistical modeling, the true location can be
inferred. For instance, if a user frequently men-
tions NYC, JFK and yankees, it is likely that they
are from New York City, USA.
In this paper, we discuss an implementation of
a global city-level geolocation prediction system
for English Twitter users. The system utilises both
tweet text and public profile metadata for model-
ing and inference. Specifically, we train multino-
mial Bayes classifiers based on location indica-
tive words (LIWs) in tweets (Han et al, 2012),
and user-declared location and time zone meta-
data. These base classifiers are further stacked
(Wolpert, 1992) using logistic regression as the
meta-classifier. The proposed stacking model is
compared with benchmarks on a public geolo-
cation dataset. Experimental results demonstrate
that our stacking model outperforms benchmark
methods by a large margin, achieving 49% accu-
racy on the test data. We further evaluate the stack-
ing model on a more recent crawl of public tweets.
This experiment tests the effectiveness of a geolo-
cation model trained on ?old? data when applied to
?new? data. The results reveal that user-declared
locations are more variable over time than tweet
text and time zone data.
2 Background and Related Work
Identifying the geolocation of objects has been
widely studied in the research literature over target
objects including webpages (Zong et al, 2005),
search queries (Backstrom et al, 2008), Flickr im-
ages (Crandall et al, 2009) and Wikipedia ed-
itors (Lieberman and Lin, 2009). Recently, a
considerable amount of work has been devoted
to extending geolocation prediction for Twitter
7
users (Cheng et al, 2010; Eisenstein et al, 2010).
The geolocations are usually represented by un-
ambiguous city names or a partitioning of the
earth?s surface (e.g., grid cells specified by lati-
tude/longitude). User geolocation is generally re-
lated to a ?home? location where a user regularly
resides, and user mobility is ignored. Twitter al-
lows users to declare their home locations in plain
text in their profile, however, this data has been
found to be unstructured and ad hoc in preliminary
research (Cheng et al, 2010; Hecht et al, 2011).
While popular for desktop machine geoloca-
tion, methods that map IP addresses to physical
locations (Buyukokkten et al, 1999) cannot be
applied to Twitter-based user geolocation, as IPs
are only known to the service provider and are
non-trivial to retrieve in a mobile Internet environ-
ment. Although social network information has
been proven effective in inferring user locations
(Backstrom et al, 2010; Sadilek et al, 2012; Rout
et al, 2013), we focus exclusively on message
and metadata information in this paper, as they are
more readily accessible.
Text data tends to contain salient geospatial ex-
pressions that are particular to specific regions.
Attempts to leverage this data directly have been
based on analysis of gazetted expressions (Leid-
ner and Lieberman, 2011) or the identification of
geographical entities (Quercini et al, 2010; Qin et
al., 2003). However these methods are limited in
their ability to capture informal geospatial expres-
sions (e.g. Brissie for Brisbane) and more non-
geospatial terms which are associated with partic-
ular locations (e.g. ferry for Seattle or Sydney).
Beyond identifying geographical references us-
ing off-the-shelf tools, more sophisticated meth-
ods have been introduced in the social media
realm. Cheng et al (2010) built a simple gen-
erative model based on tweet words, and fur-
ther added words which are local to particular re-
gions and applied smoothing to under-represented
locations. Kinsella et al (2011) applied differ-
ent similarity measures to the task, and investi-
gated the relative difficulty of geolocation predic-
tion at city, state, and country levels. Wing and
Baldridge (2011) introduced a grid-based repre-
sentation for geolocation modeling and inference
based on fixed latitude and longitude values, and
aggregated all tweets in a single cell. Their ap-
proach was then based on lexical similarity us-
ing KL-divergence. One drawback to the uniform-
sized cell representation is that it introduces class
imbalance: urban areas tend to contain far more
tweets than rural areas. Based on this observa-
tion, Roller et al (2012) introduced an adaptive
grid representation in which cells contain approx-
imately the same number of users, based on a KD-
tree partition. Given that most tweets are from
urban areas, Han et al (2012) consider a city-
based class division, and explore different feature
selection methods to extract ?location indicative
words?, which they show to improve prediction
accuracy. Additionally, time zone information has
been incorporated in a coarse-to-fine hierarchical
model by first determining the time zone, and then
disambiguating locations within it (Mahmud et al,
2012). Topic models have also been applied to the
task, in capturing regional linguistic differences
(Eisenstein et al, 2010; Yin et al, 2011; Hong et
al., 2012).
When designing a practical geolocation sys-
tem, simple models such as naive Bayes and near-
est prototype methods (e.g., based on KL diver-
gence) have clear advantages in terms of train-
ing and classification throughput, given the size of
the class set (often numbering in the thousands of
classes) and sheer volume of training data (poten-
tially in the terabytes of data). This is particularly
important for online systems and downstream ap-
plications that require timely predictions. As such,
we build off the text-based naive Bayes-based ge-
olocation system of Han et al (2012), which our
experiments have shown to have a good balance of
tractability and accuracy. By selecting a reduced
set of ?location indicative words?, prediction can
be further accelerated.
3 Methodology
In this study, we adopt the same city-based rep-
resentation and multinomial naive Bayes learner
as Han et al (2012). The city-based representa-
tion consists of 3,709 cities throughout the world,
and is obtained by aggregating smaller cities with
the largest nearby city. Han et al (2012) found
that using feature selection to identify ?location
indicative words? led to improvements in geoloca-
tion performance. We use the same feature selec-
tion technique that they did. Specifically, feature
selection is based on information gain ratio (IGR)
(Quinlan, 1993) over the city-based label set for
each word.
In the original research of Han et al (2012),
8
only the text of Twitter messages was used,
and training was based exclusively on geotagged
tweets, despite these accounting for only around
1% of the total public data on Twitter. In this
research, we include additional non-geotagged
tweets (e.g., posted from a non-GPS enabled de-
vice) for those users who have geotagged tweets
(allowing us to determine a home location for the
user).
In addition to including non-geotagged data in
modeling and inference, we further take advan-
tage of the text-based metadata embedded in a
user?s public profile (and included in the JSON ob-
ject for each tweet). This metadata is potentially
complementary to the tweet message and of bene-
fit for geolocation prediction, especially the user-
declared location and time zone, which we con-
sider here. Note that these are in free text rather
than a structured data format, and that while there
are certainly instances of formal place name de-
scriptions (e.g., Edinburgh, UK), they are often
informal (e.g., mel for Melbourne). As such, we
adopt a statistical approach to model each selected
metadata field, by capturing the text in the form
of character 4-grams, and training a multinomial
naive Bayes classifier for each field.
To combine together the tweet text and meta-
data fields, we use stacking (Wolpert, 1992). The
training of stacking consists of two steps. First,
a multinomial naive Bayes base classifier (L0) is
learned for each data type using 10-fold cross
validation. This is carried out for the tweet
text (TEXT), user-declared location (MB-LOC) and
user-declared time zone (MB-TZ). Next, a meta-
classifier (L1 classifier) is trained over the base
classifiers, using a logistic regression learner (Fan
et al, 2008).
4 Evaluation and Discussion
In this section, we compare our proposed stack-
ing approach with existing benchmarks on a public
dataset, and investigate the impact of time using a
recently collected dataset.
4.1 Evaluation Measures
In line with other work on user geolocation pre-
diction, we use three evaluation measures:
? Acc : The percentage of correct city-level
predictions.
? Acc@161 : The percentage of predicted lo-
cations which are within a 161km (100 mile)
Methods Acc Acc@161 Median
KL .117 .277 793
MB .126 .262 913
KL-NG .260 .487 181
MB-NG .280 .492 170
MB-LOC .405 .525 92
MB-TZ .064 .171 1330
STACKING .490 .665 9
Table 1: Results over WORLD
radius of the home location (Cheng et al,
2010), to capture near-misses (e.g., Edin-
burgh UK being predicted as Glasgow, UK).
? Median : The median distance from the pre-
dicted city to the home location (Eisenstein et
al., 2010).
4.2 Comparison with Benchmarks
We base our evaluation on the publicly-available
WORLD dataset of Han et al (2012). The dataset
contains 1.4M users whose tweets are primarily
identified as English based on the output of the
langid.py language identification tool (Lui and
Baldwin, 2012), and who have posted at least 10
geotagged tweets. The city-level home location
for a geotagged user is determined as follows.
First, each of a user?s geotagged tweets is mapped
to its nearest city (based on the same set of 3,709
cities used for the city-based location representa-
tion). Then, the most frequent city for a user is
selected as the home location.
To benchmark our method, we reimplement
two recently-published state-of-the-art methods:
(1) the KL-divergence nearest prototype method
of Roller et al (2012) based on KD-tree parti-
tioned grid cells, which we denote as KL; and
(2) the multinomial naive Bayes city-level geolo-
cation model of Han et al (2012), which we de-
note as MB. Because of the different class repre-
sentations, Acc numbers are not comparable be-
tween the benchmarks. To remedy this, we find
the closest city to the centroid of each grid cell in
the KD-tree representation, and map the classifi-
cation onto this city. We present results including
non-geotagged data for users with geotagged mes-
sages for the two methods, as KL-NG and MB-
NG, respectively. We also present results based
on the user-declared location (MB-LOC) and time
zone (MB-TZ), and finally the stacking method
(STACKING) which combines MB-NG, MB-LOC
and MB-TZ. The results are shown in Table 1.
9
The approximate doubling of Acc for KL-
NG and MB-NG over KL and MB, respectively,
demonstrates the high utility of non-geotagged
data in tweet text-based geolocation prediction.
Of the two original models, we can see that MB
is comparable to KL, in line with the findings of
Han et al (2012). The MB-LOC results are by far
the highest of all the base classifiers. Contrary to
the suggestion of Cheng et al (2010) that user-
declared locations are too unreliable to use for user
geolocation, we find evidence indicating that they
are indeed a valuable source of information for this
task. The best overall results are achieved for the
stacking approach (STACKING), assigning almost
half of the test users to the correct city-level lo-
cation, and improving more than four-fold on the
previous-best accuracy (i.e., MB). These results
also suggest that there is strong complementarity
between user metadata and tweet text.
4.3 Evaluation on Time-Heterogeneous Data
In addition to the original held-out test data
(WORLDtest) from WORLD, we also developed a
new geotagged evaluation dataset using the Twit-
ter Streaming API.2 This new LIVEtest dataset is
intended to evaluate the impact of time on predic-
tive accuracy. The training and test data in WORLD
are time-homogeneous as they are randomly sam-
pled from data collected in a relatively narrow time
window. In contrast, LIVEtest is much newer, col-
lected more than 1 year later than WORLD. Given
that Twitter users and topics change over time, an
essential question is whether the statistical model
learned from the ?old? training data is still effec-
tive over the ?new? test data?
The LIVEtest data was collected over 48 hours
from 2013/03/03 to 2013/03/05. By selecting
users with at least 10 geotagged tweets and a de-
clared language of English, 55k users were ob-
tained. For each user, their recent status updates
were aggregated, and non-English users were fil-
tered out based on the language predictions of
langid.py. For some users with geotagged
tweets from many cities, the most frequent city
might not be an appropriate representation of their
home location for evaluation. To improve the eval-
uation data quality, we therefore exclude users
who have less than 50% of their geotagged tweets
originating from a single city. After filtering, 32k
2https://dev.twitter.com/docs/api/1.1/
get/statuses/sample
LIVEtest Acc Acc@161 Median
MB-NG .268 (?.012) .510 (?.018) 151 ( ?19)
MB-LOC .326 (?.079) .465 (?.060) 306 (+214)
MB-TZ .065 (+.001) .160 (?.011) 1529 (+199)
STACKING .406 (?.084) .614 (?.051) 40 ( +31)
Table 2: Results over LIVEtest, and the absolute
fluctuation over the results for WORLDtest
users were obtained, forming the final LIVEtest
dataset. In the final LIVEtest, the smallest class
has only one test user, and the largest class has
569 users. The mean users per city is 27.76.
The results over LIVEtest, and the difference
in absolute score over WORLDtest, are shown in
Table 2. The stacked model accuracy numbers
drop 5?8% on LIVEtest, and the median error
distance increases moderately by 31km. Over-
all, the numbers suggest inference on WORLDtest,
which is time-homogenous with the training data
(taken from WORLD), is an easier classification
than LIVEtest, which is time-heterogeneous with
the training data. Training on ?old? data and test-
ing on ?new? data is certainly possible, however.
Looking over the results of the base classifiers, we
can see that the biggest hit is for MB-LOC clas-
sifier. In contrast, the accuracy for MB-NG and
MB-TZ is relatively stable (other than the sharp in-
crease in the median error distance for MB-TZ).
5 Architecture and Access
In this section, we describe the architecture of the
proposed geolocation system, as well as two ways
of accessing the live system.3 The core structure
of the system consists of two parts: (1) the inter-
face; (2) the back-end geolocation service.
We offer two interfaces to access the system: a
Twitter bot and a web interface. The Twitter bot
account is: @MELBLTFSD. A daemon process de-
tects any user mentions of the bot in tweets via
keyword matching through the Twitter search API.
The screen name of the tweet author is extracted
and sent to the back-end geolocation service, and
the predicted user geolocation is sent to the Twitter
user in a direct message, as shown in Figure 1.
Web access is via http://hum.csse.unimelb.
edu.au:9000/geo.html. Users can input a Twit-
ter user screen name through the web interface,
whereby a call is made to the back-end geoloca-
tion service to geolocate that user. The geoloca-
3The source code is available from https://github.
com/tq010or/acl2013
10
Figure 2: Web interface for user geolocation. The numbered green markers represent geotagged tweets.
These coordinates are utilised to validate our predictions, and are not used in the geolocation process.
The red marker is the predicted city-based user geolocation.
Figure 1: Twitter bot interface. When the Twit-
ter bot is mentioned in a tweet, that user is sent a
direct message with the predicted geolocation.
tion results are rendered on a map (along with any
geotagged tweets for the user) as in Figure 2.4
The back-end geolocation service crawls recent
tweets for a given user in real time,5 and word
and n-gram features are extracted from both the
text and the user metadata. These features are sent
to the L0 classifiers (TEXT, MB-LOC and MB-TZ),
and the L0 results are further fed into the L1 clas-
sifier for the final prediction.
6 Summary and Future Work
In this paper, we presented a city-level geoloca-
tion prediction system for Twitter users. Over a
public dataset, our stacking method ? exploiting
both tweet text and user metadata ? substantially
4Currently, only Google Chrome is supported. https:
//www.google.com/intl/en/chrome/
5Up to 200 tweets are crawled, the upper bound of mes-
sages returned per single request based on Twitter API v1.1.
outperformed benchmark methods. We further
evaluated model generalisation on a newer, time-
heterogeneous dataset. The overall results de-
creased by 5?8% in accuracy, compared with num-
bers on time-homogeneous data, primarily due to
the poor generalisation of the MB-LOC classifier.
In future work, we plan to further investigate
the cause of the MB-LOC classifier accuracy de-
crease on the new dataset. In addition, we?d like
to study differences in prediction accuracy across
cities. For cities with reliable predictions, the sys-
tem can be adapted as a preprocessing module for
downstream applications, e.g., local event detec-
tion based on users with reliable predictions.
Acknowledgements
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT centre
of Excellence programme.
References
Lars Backstrom, Jon Kleinberg, Ravi Kumar, and Jas-
mine Novak. 2008. Spatial variation in search en-
gine queries. In Proc. of WWW, pages 357?366,
Beijing, China.
Lars Backstrom, Eric Sun, and Cameron Marlow.
2010. Find me if you can: improving geographi-
cal prediction with social and spatial proximity. In
Proc. of WWW, pages 61?70, Raleigh, USA.
11
Orkut Buyukokkten, Junghoo Cho, Hector Garcia-
Molina, Luis Gravano, and Narayana Shivakumar.
1999. Exploiting geographical location informa-
tion of web pages. In ACM SIGMOD Workshop on
The Web and Databases, pages 91?96, Philadelphia,
USA.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based
approach to geo-locating twitter users. In Proc. of
CIKM, pages 759?768, Toronto, Canada.
David J. Crandall, Lars Backstrom, Daniel Hutten-
locher, and Jon Kleinberg. 2009. Mapping the
world?s photos. In Proc. of WWW, pages 761?770,
Madrid, Spain.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP,
pages 1277?1287, Cambridge, MA, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Ge-
olocation prediction in social media data by find-
ing location indicative words. In Proc. of COLING,
pages 1045?1062, Mumbai, India.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from justin bieber?s heart:
the dynamics of the location field in user profiles.
In Proc. of SIGCHI, pages 237?246, Vancouver,
Canada.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the twit-
ter stream. In Proc. of WWW, pages 769?778, Lyon,
France.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in glasgow?: mod-
eling locations with tweets. In Proc. of the 3rd In-
ternational Workshop on Search and Mining User-
generated Contents, pages 61?68, Glasgow, UK.
Jochen L. Leidner and Michael D. Lieberman. 2011.
Detecting geographical references in the form of
place names and associated spatial natural language.
SIGSPATIAL Special, 3(2):5?11.
Michael D. Lieberman and Jimmy Lin. 2009. You
are where you edit: Locating wikipedia contributors
through edit histories. In ICWSM.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proc.
of the ACL, pages 25?30, Jeju Island, Korea.
Alan M. MacEachren, Anuj Jaiswal, Anthony C.
Robinson, Scott Pezanowski, Alexander Savelyev,
Prasenjit Mitra, Xiao Zhang, and Justine Blanford.
2011. Senseplace2: Geotwitter analytics support for
situational awareness. In IEEE Conference on Vi-
sual Analytics Science and Technology, pages 181?
190, Rhode Island, USA.
Jalal Mahmud, Jeffrey Nichols, and Clemens Drews.
2012. Where is this tweet from? inferring home lo-
cations of twitter users. In Proc. of ICWSM, Dublin,
Ireland.
Teng Qin, Rong Xiao, Lei Fang, Xing Xie, and Lei
Zhang. 2003. An efficient location extraction algo-
rithm by leveraging web contextual information. In
Proc. of SIGSPATIAL, pages 55?62, San Jose, USA.
Gianluca Quercini, Hanan Samet, Jagan Sankara-
narayanan, and Michael D. Lieberman. 2010. De-
termining the spatial reader scopes of news sources
using local lexicons. In Proc. of the 18th Interna-
tional Conference on Advances in Geographic In-
formation Systems, pages 43?52, San Jose, USA.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo,
USA.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language mod-
els on an adaptive grid. In Proc. of EMNLP, pages
1500?1510, Jeju Island, Korea.
Dominic Rout, Kalina Bontcheva, Daniel Preot?iuc-
Pietro, and Trevor Cohn. 2013. Where?s @wally?:
a classification approach to geolocating users based
on their social ties. In Proc. of the 24th ACM Con-
ference on Hypertext and Social Media, pages 11?
20, Paris, France.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proc. of WSDM, pages 723?732,
Seattle, USA.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proc. of WWW,
pages 851?860, Raleigh, USA.
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL, pages 955?964, Portland,
USA.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5(2):241?259.
Zhijun Yin, Liangliang Cao, Jiawei Han, Chengxiang
Zhai, and Thomas Huang. 2011. Geographical
topic discovery and comparison. In Proc. of WWW,
pages 247?256, Hyderabad, India.
Wenbo Zong, Dan Wu, Aixin Sun, Ee-Peng Lim,
and Dion Hoe-Lian Goh. 2005. On assigning
place names to geography related web pages. In
ACM/IEEE Joint Conference on Digital Libraries,
pages 354?362.
12
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
Jey Han Lau,
?
Paul Cook,
?
Diana McCarthy,
?
Spandana Gella,
?
and Timothy Baldwin
?
? Dept of Philosophy, King?s College London
? Dept of Computing and Information Systems, The University of Melbourne
? University of Cambridge
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
Abstract
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren?t attested
in the corpus, and identifying novel senses
in the corpus which aren?t captured in the
sense inventory.
1 Introduction
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al, 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions ? and predominant senses too ?
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al, 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al, 2007; Li et
al., 2010; Lau et al, 2012; Preiss and Stevenson,
2013; Cai et al, 2007; Knopp et al, 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses ? i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
259
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses ? i.e. senses that are
attested in the corpus but not in the sense inven-
tory ? would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al, 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
2 Background and Related Work
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al, 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al, 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al, 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al, 2007). Mc-
Carthy et al (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al, 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (?corroborators?), each of which, in turn,
depends on the document?s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al
260
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al, 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al, 2012). In a similar vein,
Peirsman et al (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
3 Methodology
Our methodology is based on the WSI system
described in Lau et al (2012),
1
which has been
shown (Lau et al, 2012; Lau et al, 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al, 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.
2
Note that in
their original work, Lau et al (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
1
Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2
This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
marginal, we make no use of parser-based features
in this paper.
3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.
4
To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.
5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic?
sense alignment methodology with portability in
mind ? it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.
6
We then calculate the Jensen?
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense s
i
and topic t
j
is:
sim(s
i
, t
j
) = 1? JS(S?T ) (1)
where S and T are the multinomial distributions
3
For hyper-parameters ? and ?, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al (2006).
4
To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5
The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6
Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al, 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
261
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
over words for sense s
i
and topic t
j
, respectively,
and JS(X?Y ) is the Jensen?Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(s
i
, t
j
)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense s
i
is given as follows:
prevalence(s
i
) =
T
?
j
(sim(s
i
, t
j
)? P (t
j
)) (2)
=
T
?
j
(
sim(s
i
, t
j
)?
f(t
j
)
?
T
k
f(t
k
)
)
where f(t
j
) is the frequency of topic t
j
(i.e. the
number of usages assigned to topic t
j
), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
4 WordNet Experiments
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) Acc
UB
, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);
7
and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (Acc
UB
), calculated as follows:
ERR = 1?
Acc
UB
? Acc
Acc
UB
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar?s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p < 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p > 0.1). Note
that there is still much room for improvement with
7
The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
262
Dataset
FS
CORPUS
MKWC HDP-WSI
Acc
UB
Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
Table 2: WSD accuracy for MKWC and HDP-WSI
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FS
CORPUS
upper bound] is indicated in boldface).
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
Table 3: Sense distribution evaluation of MKWC
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al, 2009). To this
end, we introduce a second evaluation metric:
the Jensen?Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p < 0.05) but the results
for the other two datasets are not (p > 0.1 in each
case).
Dataset
FS
CORPUS
FS
DICT
HDP-WSI
Acc
UB
Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FS
CORPUS
upper bound] is indicated in boldface).
Dataset FS
CORPUS
FS
DICT
HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
Table 5: Sense distribution evaluation of HDP-
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.
8
The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al, 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
?flat? sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al, 2007) com-
pared to a dependency one,
9
it is not stated how
8
McCarthy et al (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
9
The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/
?
lindek/downloads.htm.
263
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
5 Macmillan Experiments
In our second set of experiments, we move to a
new dataset (Gella et al, to appear) based on text
from ukWaC (Ferraresi et al, 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary
10
(henceforth ?Macmillan?). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.
11
In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al, 2004; Navigli et al, 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3?Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al, 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
?Other? in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
10
http://www.macmillandictionary.com/
11
Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly ?Other?), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
5.1 Learning Sense Distributions
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (?FS
CORPUS
?), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (?FS
DICT
?), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FS
DICT
) over both datasets (McNe-
mar?s test; p < 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.
12
This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12
The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al (2005) is comparable to UKWAC.
264
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),
13
making
the task slightly easier in the Macmillan case.
5.2 Identification of Unattested Senses
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren?t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren?t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
st-affinity(s
i
) =
?
T
j
sim(s
i
, t
j
)
?
S
k
?
T
l
sim(s
k
, t
l
)
(3)
where sim(s
i
, t
j
) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13
Note that the set of lemmas differs between the respec-
tive datasets, so this isn?t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,
14
evaluated using
sense-level precision (P ), recall (R) and F-score
(F ) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092? 0.044 over UKWAC
and 0.125?0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
5.3 Identification of Novel Senses
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn?t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these ?novel senses?, and define ?novel
sense identification? to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren?t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn?t ob-
served in the corpus. Also, while we have annota-
tions of ?Other? usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can?t use the
?Other? annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
14
We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
265
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
20 0.0?0.2 0.052?0.009 0.35 0.42 0.36
9 0.2?0.4 0.089?0.024 0.24 0.59 0.29
6 0.4?0.6 0.061?0.004 0.63 0.64 0.63
Table 7: Classification of usages with novel sense for all target lemmas.
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
9 0.2?0.4 0.093?0.023 0.50 0.66 0.52
6 0.4?0.6 0.099?0.018 0.73 0.90 0.80
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0?0.2; medium = 0.2?
0.4; and high = 0.4?0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if > 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al, 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
ts-affinity(t
j
) =
?
S
i
sim(s
i
, t
j
)
?
T
l
?
S
k
sim(s
k
, t
l
)
(4)
where, once again, sim(s
i
, t
j
) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances ? instances from target lem-
mas that have a sense artificially removed and
those that do not ? impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
266
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0?0.2 0.4543
9 11 0.2?0.4 0.0391
6 14 0.4?0.6 0.0247
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
novelty(w) = min
t
j
(
max
s
i
sim(s
i
, t
j
)
f(t
j
)
)
(5)
where f(t
j
) is the frequency of topic t
j
in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as a low-probability topic for
a high-frequency word, or a high-probability topic
for a low-frequency word) are indicative of a novel
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of a one-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
novel sense) are statistically different.
15
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p < 0.05), but the results are less promising for
the low-frequency novel senses.
6 Discussion
Our methodologies for the two proposed tasks of
identifying unused and novel senses are simple
15
Note that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas in the dataset.
extensions to demonstrate the flexibility and ro-
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of sim(s
i
, t
j
) for com-
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
A natural next step for this research would be to
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised WSD of individual token
occurrences of a given word.
In summary, we have proposed a topic
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian Re-
search Council.
267
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33?41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136?145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277?281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum?e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435?1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010?
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89?96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28?34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49?65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128?135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47?54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
268
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561?568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics ? Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233?
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V?olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97?103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591?601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307?311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217?221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24?26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220?1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280?287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303?308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
269
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121?128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680?684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga?etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257?282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566?1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala, Sweden.
270
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 93?98,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automatic Detection of Multilingual Dictionaries on the Web
Gintar
?
e Grigonyt
?
e
?
Timothy Baldwin
?
? Department of Linguistics, Stockholm University
? Department of Computing and Information Systems, The University of Melbourne
gintare@ling.su.se tb@ldwin.net
Abstract
This paper presents an approach to query
construction to detect multilingual dictio-
naries for predetermined language combi-
nations on the web, based on the identifi-
cation of terms which are likely to occur
in bilingual dictionaries but not in general
web documents. We use eight target lan-
guages for our case study, and train our
method on pre-identified multilingual dic-
tionaries and the Wikipedia dump for each
of our languages.
1 Motivation
Translation dictionaries and other multilingual
lexical resources are valuable in a myriad of
contexts, from language preservation (Thieberger
and Berez, 2012) to language learning (Laufer
and Hadar, 1997), cross-language information
retrieval (Nie, 2010) and machine translation
(Munteanu and Marcu, 2005; Soderland et al,
2009). While there are syndicated efforts
to produce multilingual dictionaries for differ-
ent pairings of the world?s languages such as
freedict.org, more commonly, multilingual
dictionaries are developed in isolation for a spe-
cific set of languages, with ad hoc formatting,
great variability in lexical coverage, and no cen-
tral indexing of the content or existence of that
dictionary (Baldwin et al, 2010). Projects such
as panlex.org aspire to aggregate these dic-
tionaries into a single lexical database, but are
hampered by the need to identify individual multi-
lingual dictionaries, especially for language pairs
where there is a sparsity of data from existing dic-
tionaries (Baldwin et al, 2010; Kamholz and Pool,
to appear). This paper is an attempt to automate
the detection of multilingual dictionaries on the
web, through query construction for an arbitrary
language pair. Note that for the method to work,
we require that the dictionary occurs in ?list form?,
that is it takes the form of a single document (or
at least, a significant number of dictionary entries
on a single page), and is not split across multiple
small-scale sub-documents.
2 Related Work
This research seeks to identify documents of a
particular type on the web, namely multilingual
dictionaries. Related work broadly falls into
four categories: (1) mining of parallel corpora;
(2) automatic construction of bilingual dictionar-
ies/thesauri; (3) automatic detection of multilin-
gual documents; and (4) classification of docu-
ment genre.
Parallel corpus construction is the task of au-
tomatically detecting document sets that contain
the same content in different languages, com-
monly based on a combination of site-structural
and content-based features (Chen and Nie, 2000;
Resnik and Smith, 2003). Such methods could
potentially identify parallel word lists from which
to construct a bilingual dictionary, although more
realistically, bilingual dictionaries exist as single
documents and are not well suited to this style of
analysis.
Methods have also been proposed to automat-
ically construct bilingual dictionaries or thesauri,
e.g. based on crosslingual glossing in predictable
patterns such as a technical term being immedi-
ately proceeded by that term in a lingua franca
source language such as English (Nagata et al,
2001; Yu and Tsujii, 2009). Alternatively, com-
parable or parallel corpora can be used to extract
bilingual dictionaries based on crosslingual distri-
butional similarity (Melamed, 1996; Fung, 1998).
While the precision of these methods is generally
relatively high, the recall is often very low, as there
is a strong bias towards novel technical terms be-
ing glossed but more conventional terms not.
Also relevant to this work is research on lan-
93
guage identification, and specifically the detection
of multilingual documents (Prager, 1999; Yam-
aguchi and Tanaka-Ishii, 2012; Lui et al, 2014).
Here, multi-label document classification meth-
ods have been adapted to identify what mix of
languages is present in a given document, which
could be used as a pre-filter to locate documents
containing a given mixture of languages, although
there is, of course, no guarantee that a multilingual
document is a dictionary.
Finally, document genre classification is rele-
vant in that it is theoretically possible to develop
a document categorisation method which classi-
fies documents as multilingual dictionaries or not,
with the obvious downside that it would need to be
applied exhaustively to all documents on the web.
The general assumption in genre classification is
that the type of a document should be judged not
by its content but rather by its form. A variety
of document genre methods have been proposed,
generally based on a mixture of structural and
content-based features (Matsuda and Fukushima,
1999; Finn et al, 2002; zu Eissen and Stein, 2005).
While all of these lines of research are relevant
to this work, as far as we are aware, there has not
been work which has proposed a direct method
for identifying pre-existing multilingual dictionar-
ies in document collections.
3 Methodology
Our method is based on a query formulation ap-
proach, and querying against a pre-existing index
of a document collection (e.g. the web) via an in-
formation retrieval system.
The first intuition underlying our approach is
that certain words are a priori more ?language-
discriminating? than others, and should be pre-
ferred in query construction (e.g. sushi occurs as
a [transliterated] word in a wide variety of lan-
guages, whereas anti-discriminatory is found pre-
dominantly in English documents). As such, we
prefer search terms w
i
with a higher value for
max
l
P (l|w
i
), where l is the language of interest.
The second intuition is that the lexical cover-
age of dictionaries varies considerably, especially
with multilingual lexicons, which are often com-
piled by a single developer or small community
of developers, with little systematicity in what is
including or not included in the dictionary. As
such, if we are to follow a query construction ap-
proach to lexicon discovery, we need to be able
to predict the likelihood of a given word w
i
be-
ing included in an arbitrarily-selected dictionary
D
l
incorporating language l (i.e. P (w
i
|D
l
)). Fac-
tors which impact on this include the lexical prior
of the word in the language (e.g. P (paper|en) >
P (papyrus|en)), whether they are lemmas or not
(noting that multilingual dictionaries tend not to
contain inflected word forms), and their word class
(e.g. multilingual dictionaries tend to contain more
nouns and verbs than function words).
The third intuition is that certain word combi-
nations are more selective of multilingual dictio-
naries than others, i.e. if certain words are found
together (e.g. cruiser, gospel and noodle), the con-
taining document is highly likely to be a dictionary
of some description rather than a ?conventional?
document.
Below, we describe our methodology for query
construction based on these elements in greater de-
tail. The only assumption on the method is that
we have access to a selection of dictionaries D
(mono- or multilingual) and a corpus of conven-
tional (non-dictionary) documents C, and knowl-
edge of the language(s) contained in each dictio-
nary and document.
Given a set of dictionaries D
l
for a language l
and the complement set D
l
= D\D
l
, we first con-
struct the lexicon L
l
for that language as follows:
L
l
=
{
w
i
|w
i
? D
l
? w
i
/? D
l
}
(1)
This creates a language-discriminating lexicon for
each language, satisfying the first criterion.
Lexical resources differ in size, scope and cov-
erage. For instance, a well-developed, mature
multilingual dictionary may contain over 100,000
multilingual lexical records, while a specialised 5-
way multilingual domain dictionary may contain
as few as 100 multilingual lexical records. In line
with our second criterion, we want to select words
which have a higher likelihood of occurrence in
a multilingual dictionary involving that language.
To this end, we calculate the weight sdict(w
i,l
) for
each word w
i,l
? L
l
:
sdict(w
i,l
) =
?
d?D
l
{
|L
l
|?|d|
|L
l
|
if w
i,l
? d
?
|d|
|L
l
|
otherwise
(2)
where |d| is the size of dictionary d in terms of the
number of lexemes it contains.
The final step is to weight words by their typ-
icality in a given language, as calculated by their
94
likelihood of occurrence in a random document in
that language. This is estimated by the proportion
of Wikipedia documents in that language which
contain the word in question:
Score(w
i,l
) =
df(w
i,l
)
N
l
sdict(w
i,l
) (3)
where df(w
i,l
) is the count of Wikipedia docu-
ments of language l which contain w
i
, and N
l
is
the total number of Wikipedia documents in lan-
guage l.
In all experiments in this paper, we assume that
we have access to at least one multilingual dictio-
nary containing each of our target languages, but
in absence of such a dictionary, sdict(w
i,l
) could
be set to 1 for all words w
i,l
in the language.
The result of this term weighing is a ranked list
of words for each language. The next step is to
identify combinations of words that are likely to
be found in multilingual dictionaries and not stan-
dard documents for a given language, in accor-
dance with our third criterion.
3.1 Apriori-based query generation
We perform query construction for each language
based on frequent item set mining, using the Apri-
ori algorithm (Agrawal et al, 1993). For a given
combination of languages (e.g. English and Swa-
heli), queries are then formed simply by combin-
ing monolingual queries for the component lan-
guages.
The basic approach is to use a modified support
formulation within the Apriori algorithm to prefer
word combinations that do not cooccur in regular
documents. Based on the assumption that query-
ing a (pre-indexed) document collection is rela-
tively simple, we generate a range of queries of de-
creasing length and increasing likelihood of term
co-occurrence in standard documents, and query
until a non-empty set of results is returned.
The modified support formulation is as follows:
cscore(w
1
, ..., w
n
) =
{
0 if ?d,w
i
, w
j
: co
d
(w
i
, w
j
)
?
i
Score(w
i
) otherwise
where co
d
(w
i
, w
j
) is a Boolean function which
evaluates to true iff w
i
and w
j
co-occur in doc-
ument d. That is, we reject any combinations of
words which are found to co-occur in Wikipedia
documents for that language. Note that the actual
calculation of this co-occurrence can be performed
Figure 1: Examples of learned queries for different
languages
efficiently, as: (a) for a given iteration of Apri-
ori, it only needs to be performed between the new
word that we are adding to the query (?item set? in
the terminology of Apriori) and each of the other
words in a non-zero support itemset from the pre-
vious iteration of the algorithm (which are guaran-
teed to not co-occur with each other); and (b) the
determination of whether two terms collocate can
be performed efficiently using an inverted index of
Wikipedia for that language.
In our experiments, we apply the Apriori al-
gorithm exhaustively for a given language with a
support threshold of 0.5, and return the resultant
item sets in ranked order of combined score for
the component words.
A random selection of queries learned for each
of the 8 languages targeted in this research is pre-
sented in Figure 1.
4 Experimental methodology
We evaluate our proposed methodology in two
ways:
1. against a synthetic dataset, whereby we in-
jected bilingual dictionaries into a collection
of web documents, and evaluated the ability
of the method to return multilingual dictio-
naries for individual languages; in this, we
naively assume that all web documents in the
background collection are not multilingual
dictionaries, and as such, the results are po-
tentially an underestimate of the true retrieval
effectiveness.
2. against the open web via the Google search
API for a given combination of languages,
and hand evaluation of the returned docu-
ments
95
Lang Wikipedia articles (M) Dictionaries Queries learned Avg. query length
en 3.1 26 2546 3.2
zh 0.3 0 5034 3.6
es 0.5 2 356 2.9
ja 0.6 0 1532 3.3
de 1.0 13 634 2.7
fr 0.9 5 4126 3.0
it 0.6 4 1955 3.0
ar 0.1 2 9004 3.2
Table 1: Details of the training data and queries learned for each language
Note that the first evaluation with the synthetic
dataset is based on monolingual dictionary re-
trieval effectiveness because we have very few
(and often no) multilingual dictionaries for a given
pairing of our target languages. For a given lan-
guage, we are thus evaluating the ability of our
method to retrieve multilingual dictionaries con-
taining that language (and other indeterminate lan-
guages).
For both the synthetic dataset and open web ex-
periments, we evaluate our method based on mean
average precision (MAP), that is the mean of the
average precision scores for each query which re-
turns a non-empty result set.
To train our method, we use 52 bilingual Free-
dict (Freedict, 2011) dictionaries and Wikipedia
1
documents for each of our target languages. As
there are no bilingual dictionaries in Freedict for
Chinese and Japanese, the training of Score values
is based on the Wikipedia documents only. Mor-
phological segmentation for these two languages
was carried out using MeCab (MeCab, 2011) and
the Stanford Word Segmenter (Tseng et al, 2005),
respectively. See Table 1 for details of the num-
ber of Wikipedia articles and dictionaries for each
language.
Below, we detail the construction of the syn-
thetic dataset.
4.1 Synthetic dataset
The synthetic dataset was constructed using a sub-
set of ClueWeb09 (ClueWeb09, 2009) as the back-
ground web document collection. The original
ClueWeb09 dataset consists of around 1 billion
web pages in ten languages that were collected in
January and February 2009. The relative propor-
tions of documents in the different languages in
the original dataset are as detailed in Table 2.
We randomly downsampled ClueWeb09 to 10
1
Based on 2009 dumps.
Language Proportion
en (English) 48.41%
zh (Chinese) 17.05%
es (Spanish) 7.62%
ja (Japanese) 6.47%
de (German) 4.89%
fr (French) 4.79%
ko (Korean) 3.61%
it (Italian) 2.8%
pt (Portuguese) 2.62%
ar (Arabic) 1.74%
Table 2: Language proportions in ClueWeb09.
million documents for the 8 languages targeted
in this research (the original 10 ClueWeb09 lan-
guages minus Korean and Portuguese). We then
sourced a random set of 246 multilingual dic-
tionaries that were used in the construction of
panlex.org, and injected them into the docu-
ment collection. Each of these dictionaries con-
tains at least one of our 8 target languages, with
the second language potentially being outside the
8. A total of 49 languages are contained in the
dictionaries.
We indexed the synthetic dataset using Indri (In-
dri, 2009).
5 Results
First, we present results over the synthetic dataset
in Table 3. As our baseline, we simply query for
the language name and the term dictionary in the
local language (e.g. English dictionary, for En-
glish) in the given language.
For languages that had bilingual dictionaries for
training, the best results were obtained for Span-
ish, German, Italian and Arabic. Encouragingly,
the results for languages with only Wikipedia doc-
uments (and no dictionaries) were largely com-
parable to those for languages with dictionaries,
with Japanese achieving a MAP score compara-
ble to the best results for languages with dictio-
nary training data. The comparably low result for
96
Lang Dicts MAP Baseline
en 92 0.77 0.00
zh 7 0.75 0.00
es 34 0.98 0.04
ja 5 0.94 0.00
de 75 0.97 0.08
fr 34 0.84 0.03
it 8 0.95 0.01
ar 3 0.92 0.00
AVERAGE: 32.2 0.88 0.04
Table 3: Dictionary retrieval results over the syn-
thetic dataset (?Dicts? = the number of dictionaries
in the document collection for that language.
English is potentially affected by its prevalence
both in the bilingual dictionaries in training (re-
stricting the effective vocabulary size due to our
L
l
filtering), and in the document collection. Re-
call also that our MAP scores are an underestimate
of the true results, and some of the ClueWeb09
documents returned for our queries are potentially
relevant documents (i.e. multilingual dictionaries
including the language of interest). For all lan-
guages, the baseline results were below 0.1, and
substantially lower than the results for our method.
Looking next to the open web, we present in Ta-
ble 4 results based on querying the Google search
API with the 1000 longest queries for English
paired with each of the other 7 target languages.
Most queries returned no results; indeed, for the
en-ar language pair, only 49/1000 queries returned
documents. The results in Table 4 are based on
manual evaluation of all documents returned for
the first 50 queries, and determination of whether
they were multilingual dictionaries containing the
indicated languages.
The baseline results are substantially higher
than those for the synthetic dataset, almost cer-
tainly a direct result of the greater sophistication
and optimisation of the Google search engine (in-
cluding query log analysis, and link and anchor
text analysis). Despite this, the results for our
method are lower than those over the synthetic
dataset, we suspect largely as a result of the style
of queries we issue being so far removed from
standard Google query patterns. Having said this,
MAP scores of 0.32?0.92 suggest that the method
is highly usable (i.e. at any given cutoff in the doc-
ument ranking, an average of at least one in three
documents is a genuine multilingual dictionary),
and any non-dictionary documents returned by the
method could easily be pruned by a lexicographer.
Lang Dicts MAP Baseline
zh 16 0.55 0.19
es 17 0.92 0.13
ja 13 0.32 0.04
de 34 0.77 0.09
fr 36 0.77 0.08
it 23 0.69 0.11
ar 8 0.39 0.17
AVERAGE: 21.0 0.63 0.12
Table 4: Dictionary retrieval results over the open
web for dictionaries containing English and each
of the indicated languages (?Dicts? = the number
of unique multilingual dictionaries retrieved for
that language).
Among the 7 language pairs, en-es, en-de, en-fr
and en-it achieved the highest MAP scores. In
terms of unique lexical resources found with 50
queries, the most successful language pairs were
en-fr, en-de and en-it.
6 Conclusions
We have described initial results for a method de-
signed to automatically detect multilingual dictio-
naries on the web, and attained highly credible re-
sults over both a synthetic dataset and an exper-
iment over the open web using a web search en-
gine.
In future work, we hope to explore the ability
of the method to detect domain-specific dictionar-
ies (e.g. training over domain-specific dictionar-
ies from other language pairs), and low-density
languages where there are few dictionaries and
Wikipedia articles to train the method on.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments, and the Panlex devel-
opers for assistance with the dictionaries and ex-
perimental design. This research was supported
by funding from the Group of Eight and the Aus-
tralian Research Council.
References
Rakesh Agrawal, Tomasz Imieli?nski, and Arun Swami.
1993. Mining association rules between sets of
items in large databases. ACM SIGMOD Record,
22(2):207?216.
Timothy Baldwin, Jonathan Pool, and Susan M.
Colowick. 2010. PanLex and LEXTRACT: Trans-
lating all words of all languages of the world. In
97
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING 2010), Demo
Volume, pages 37?40, Beijing, China.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web text
mining for cross-language IR. In Proceedings of
Recherche d?Informations Assistee par Ordinateur
2000 (RIAO?2000), pages 62?77, College de France,
France.
ClueWeb09. 2009. The ClueWeb09 dataset. http:
//lemurproject.org/clueweb09/.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer for
information filtering. In Proceedings of the 24th Eu-
ropean Conference on Information Retrieval (ECIR
2002), pages 353?362, Glasgow, UK.
Freedict. 2011. Freedict dictionaries. http://www.
freedict.com.
Pascale Fung. 1998. A statistical view on bilin-
gual lexicon extraction: From parallel corpora to
non-parallel corpora. In Proceedings of Associa-
tion for Machine Translation in the Americas (AMTA
1998): Machine Translation and the Information
Soup, pages 1?17, Langhorne, USA.
Indri. 2009. Indri search engine. http://www.
lemurproject.org/indri/.
David Kamholz and Jonathan Pool. to appear. PanLex:
Building a resource for panlingual lexical transla-
tion. In Proceedings of the 9th International Confer-
ence on Language Resources and Evaluation (LREC
2014), Reykjavik, Iceland.
Batia Laufer and Linor Hadar. 1997. Assessing the
effectiveness of monolingual, bilingual, and ?bilin-
gualised? dictionaries in the comprehension and pro-
duction of new words. The Modern Language Jour-
nal, 81(2):189?196.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Associ-
ation for Computational Linguistics, 2(Feb):27?40.
Katsushi Matsuda and Toshikazu Fukushima. 1999.
Task-oriented world wide web retrieval by document
type classification. In Proceedings of the 1999 ACM
Conference on Information and Knowledge Man-
agement (CIKM 1999), pages 109?113, Kansas City,
USA.
MeCab. 2011. http://mecab.googlecode.
com.
I. Dan Melamed. 1996. Automatic construction of
clean broad-coverage translation lexicons. In Pro-
ceedings of the 2nd Conference of the Association
for Machine Translation in the Americas (AMTA
1996), Montreal, Canada.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Masaaki Nagata, Teruka Saito, and Kenji Suzuki.
2001. Using the web as a bilingual dictionary. In
Proceedings of the ACL 2001 Workshop on Data-
driven Methods in Machine Translation, pages 1?8,
Toulouse, France.
Jian-Yun Nie. 2010. Cross-language information
retrieval. Morgan and Claypool Publishers, San
Rafael, USA.
John M. Prager. 1999. Linguini: language identifi-
cation for multilingual documents. In Proceedings
the 32nd Annual Hawaii International Conference
on Systems Sciences (HICSS-32), Maui, USA.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Stephen Soderland, Christopher Lim, Mausam,
Bo Qin, Oren Etzioni, and Jonathan Pool. 2009.
Lemmatic machine translation. In Proceedings
of the Twelfth Machine Translation Summit (MT
Summit XII), Ottawa, Canada.
Nicholas Thieberger and Andrea L. Berez. 2012. Lin-
guistic data management. In Nicholas Thieberger,
editor, The Oxford Handbook of Linguistic Field-
work. Oxford University Press, Oxford, UK.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 969?978,
Jeju Island, Korea.
Kun Yu and Junichi Tsujii. 2009. Bilingual dictio-
nary extraction from Wikipedia. In Proceedings of
the Twelfth Machine Translation Summit (MT Sum-
mit XII), pages 379?386, Ottawa, Canada.
Sven Meyer zu Eissen and Benno Stein. 2005. Genre
classification of web pages. In Proceedings of the
27th Annual German Conference in AI (KI 2005),
pages 256?269, Ulm, Germany.
98
Automatic Detection and Language Identification of Multilingual Documents
Marco Lui??, Jey Han Lau? and Timothy Baldwin??
? Department of Computing and Information Systems
The University of Melbourne
? NICTA Victoria Research Laboratory
? Department of Philosophy
King?s College London
mhlui@unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net
Abstract
Language identification is the task of automat-
ically detecting the language(s) present in a
document based on the content of the docu-
ment. In this work, we address the problem
of detecting documents that contain text from
more than one language (multilingual docu-
ments). We introduce a method that is able to
detect that a document is multilingual, iden-
tify the languages present, and estimate their
relative proportions. We demonstrate the ef-
fectiveness of our method over synthetic data,
as well as real-world multilingual documents
collected from the web.
1 Introduction
Language identification is the task of automatically
detecting the language(s) present in a document
based on the content of the document. Language
identification techniques commonly assume that ev-
ery document is written in one of a closed set of
known languages for which there is training data,
and is thus formulated as the task of selecting the
most likely language from the set of training lan-
guages. In this work, we remove this monolingual
assumption, and address the problem of language
identification in documents that may contain text
from more than one language from the candidate set.
We propose a method that concurrently detects that a
document is multilingual, and estimates the propor-
tion of the document that is written in each language.
Detecting multilingual documents has a variety
of applications. Most natural language processing
techniques presuppose monolingual input data, so
inclusion of data in foreign languages introduces
noise, and can degrade the performance of NLP sys-
tems (Alex et al., 2007; Cook and Lui, 2012). Au-
tomatic detection of multilingual documents can be
used as a pre-filtering step to improve the quality of
input data. Detecting multilingual documents is also
important for acquiring linguistic data from the web
(Scannell, 2007; Abney and Bird, 2010), and has
applications in mining bilingual texts for statistical
machine translation from online resources (Resnik,
1999; Nie et al., 1999; Ling et al., 2013). There has
been particular interest in extracting text resources
for low-density languages from multilingual web
pages containing both the low-density language and
another language such as English (Yamaguchi and
Tanaka-Ishii, 2012; King and Abney, 2013). King
and Abney (2013, p1118) specifically mention the
need for an automatic method ?to examine a mul-
tilingual document, and with high accuracy, list the
languages that are present in the document?.
We introduce a method that is able to detect multi-
lingual documents, and simultaneously identify each
language present as well as estimate the propor-
tion of the document written in that language. We
achieve this with a probabilistic mixture model, us-
ing a document representation developed for mono-
lingual language identification (Lui and Baldwin,
2011). The model posits that each document is gen-
erated as samples from an unknown mixture of lan-
guages from the training set. We introduce a Gibbs
sampler to map samples to languages for any given
set of languages, and use this to select the set of lan-
guages that maximizes the posterior probability of
the document.
27
Transactions of the Association for Computational Linguistics, 2 (2014) 27?40. Action Editor: Kristina Toutanova.
Submitted 1/2013; Revised 7/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Our method is able to learn a language identi-
fier for multilingual documents from monolingual
training data. This is an important property as there
are no standard corpora of multilingual documents
available, whereas corpora of monolingual docu-
ments are readily available for a reasonably large
number of languages (Lui and Baldwin, 2011). We
demonstrate the effectiveness of our method empir-
ically, firstly by evaluating it on synthetic datasets
drawn from Wikipedia data, and then by applying it
to real-world data, showing that we are able to iden-
tify multilingual documents in targeted web crawls
of minority languages (King and Abney, 2013).
Our main contributions are: (1) we present a
method for identifying multilingual documents, the
languages contained therein and the relative propor-
tion of the document in each language; (2) we show
that our method outperforms state-of-the-art meth-
ods for language identification in multilingual doc-
uments; (3) we show that our method is able to es-
timate the proportion of the document in each lan-
guage to a high degree of accuracy; and (4) we show
that our method is able to identify multilingual doc-
uments in real-world data.
2 Background
Most language identification research focuses on
language identification for monolingual documents
(Hughes et al., 2006). In monolingual LangID, the
task is to assign each documentD a unique language
Li ? L. Some work has reported near-perfect accu-
racy for language identification of large documents
in a small number of languages (Cavnar and Tren-
kle, 1994; McNamee, 2005). However, in order to
attain such accuracy, a large number of simplifying
assumptions have to be made (Hughes et al., 2006;
Baldwin and Lui, 2010a). In this work, we tackle
the assumption that each document is monolingual,
i.e. it contains text from a single language.
In language identification, documents are mod-
eled as a stream of characters (Cavnar and Trenkle,
1994; Kikui, 1996), often approximated by the cor-
responding stream of bytes (Kruengkrai et al., 2005;
Baldwin and Lui, 2010a) for robustness over vari-
able character encodings. In this work, we follow
Baldwin and Lui (2010a) in training a single model
for languages that naturally use multiple encodings
(e.g. UTF8, Big5 and GB encodings for Chinese), as
issues of encoding are not the focus of this research.
The document representation used for language
identification generally involves estimating the rel-
ative distributions of particular byte sequences, se-
lected such that their distributions differ between
languages. In some cases the relevant sequences
may be externally specified, such as function words
and common suffixes (Giguet, 1995) or grammati-
cal word classes (Dueire Lins and Gonc?alves, 2004),
though they are more frequently learned from la-
beled data (Cavnar and Trenkle, 1994; Grefenstette,
1995; Prager, 1999a; Lui and Baldwin, 2011).
Learning algorithms applied to language identi-
fication fall into two general categories: Bayesian
classifiers and nearest-prototype (Rocchio-style)
classifiers. Bayesian approaches include Markov
processes (Dunning, 1994), naive Bayes methods
(Grefenstette, 1995; Lui and Baldwin, 2011; Tiede-
mann and Ljubes?ic?, 2012), and compressive mod-
els (Teahan, 2000). The nearest-prototype methods
vary primarily in the distance measure used, includ-
ing measures based on rank order statistics (Cav-
nar and Trenkle, 1994), information theory (Bald-
win and Lui, 2010a), string kernels (Kruengkrai et
al., 2005) and vector space models (Prager, 1999a;
McNamee, 2005).
Language identification has been applied in do-
mains such as USENET messages (Cavnar and
Trenkle, 1994), web pages (Kikui, 1996; Mar-
tins and Silva, 2005; Liu and Liang, 2008), web
search queries (Ceylan and Kim, 2009; Bosca and
Dini, 2010), mining the web for bilingual text
(Resnik, 1999; Nie et al., 1999), building minor-
ity language corpora (Ghani et al., 2004; Scannell,
2007; Bergsma et al., 2012) as well as a large-
scale database of Interlinear Glossed Text (Xia et al.,
2010), and the construction of a large-scale multilin-
gual web crawl (Callan and Hoy, 2009).
2.1 Multilingual Documents
Language identification over documents that contain
text from more than one language has been identified
as an open research question (Hughes et al., 2006).
Common examples of multilingual documents are
web pages that contain excerpts from another lan-
guage, and documents from multilingual organiza-
tions such as the European Union.
28
English French Italian German Dutch Japanese
character the pour di auf voo ?
byte 74 68 65 20 70 6F 75 7 20 64 69 20 20 61 75 66 76 6F 6 E3 81 AF
Table 1: Examples of per-language byte sequences selected by information gain.
The Australiasian Language Technology Work-
shop 2010 hosted a shared task where participants
were required to predict the language(s) present in a
held-out test set containing monolingual and bilin-
gual documents (Baldwin and Lui, 2010b). The
dataset was prepared using data from Wikipedia, and
bilingual documents were produced using a segment
from a page in one language, and a segment from the
same page in another language. We use the dataset
from this shared task for our initial experiments.
To the authors? knowledge, the only other work to
directly tackle identification of multiple languages
and their relative proportions in a single document is
the LINGUINI system (Prager, 1999a). The system
is based on a vector space model, and cosine simi-
larity between a feature vector for the test document
and a feature vector for each language Li, computed
as the sum of feature vectors for all the documents
for language Li in the training data. The elements
in the feature vectors are frequency counts over
byte n-grams (2?n?5) and words. Language iden-
tification for multilingual documents is performed
through the use of virtual mixed languages. Prager
(1999a) shows how to construct vectors representa-
tive of particular combinations of languages inde-
pendent of the relative proportions, and proposes a
method for choosing combinations of languages to
consider for any given document.
Language identification in multilingual docu-
ments could also be performed by application of su-
pervised language segmentation algorithms. Given
a system that can segment a document into la-
beled monolingual segments, we can then extract
the languages present as well as the relative propor-
tion of text in each language. Several methods for
supervised language segmentation have been pro-
posed. Teahan (2000) proposed a system based on
text compression that identifies multilingual docu-
ments by first segmenting the text into monolingual
blocks. Rehurek and Kolkus (2009) perform lan-
guage segmentation by computing a relevance score
between terms and languages, smoothing across ad-
joining terms and finally identifying points of transi-
tion between high and low relevance, which are in-
terpreted as boundaries between languages. Yam-
aguchi and Tanaka-Ishii (2012) use a minimum de-
scription length approach, embedding a compressive
model to compute the description length of text seg-
ments in each language. They present a linear-time
dynamic programming solution to optimize the lo-
cation of segment boundaries and language labels.
3 Methodology
Language identification for multilingual documents
is a multi-label classification task, in which a doc-
ument can be mapped onto any number of labels
from a closed set. In the remainder of this paper,
we denote the set of all languages by L. We de-
note a document D which contains languages Lx
and Ly as D ? {Lx, Ly}, where Lx, Ly ? L.
We denote a document that does not contain a lan-
guage Lx by D ? {Lx}, though we generally omit
all the languages not contained in the document for
brevity. We denote classifier output using .; e.g.
D . {La, Lb} indicates that document D has been
predicted to contain text in languages La and Lb.
3.1 Document Representation and Feature
Selection
We represent each document D as a frequency dis-
tribution over byte n-gram sequences such as those
in Table 1. Each document is converted into a vector
where each entry counts the number of times a par-
ticular byte n-gram is present in the document. This
is analogous to a bag-of-words model, where the vo-
cabulary of ?words? is a set of byte sequences that
has been selected to distinguish between languages.
The exact set of features is selected from the
training data using Information Gain (IG), an
information-theoretic metric developed as a split-
ting criterion for decision trees (Quinlan, 1993). IG-
based feature selection combined with a naive Bayes
classifier has been shown to be particularly effective
for language identification (Lui and Baldwin, 2011).
29
3.2 Generative Mixture Models
Generative mixture models are popular for text mod-
eling tasks where a mixture of influences governs the
content of a document, such as in multi-label doc-
ument classification (McCallum, 1999; Ramage et
al., 2009), and topic modeling (Blei et al., 2003).
Such models normally assume full exchangeability
between tokens (i.e. the bag-of-words assumption),
and label each token with a single discrete label.
Multi-label text classification, topic modeling and
our model for language identification in multilingual
documents share the same fundamental representa-
tion of the latent structure of a document. Each la-
bel is modeled with a probability distribution over
tokens, and each document is modeled as a proba-
bilistic mixture of labels. As presented in Griffiths
and Steyvers (2004), the probability of the ith token
(wi) given a set of T labels z1? ? ?zT is modeled as:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j) (1)
The set of tokens w is the document itself, which
in all cases is observed. In the case of topic model-
ing, the tokens are words and the labels are topics,
and z is latent. Whereas topic modeling is gener-
ally unsupervised, multi-label text classification is
a supervised text modeling task, where the labels
are a set of pre-defined categories (such as RUBBER,
IRON-STEEL, TRADE, etc. in the popular Reuters-
21578 data set (Lewis, 1997)), and the tokens are
individual words in documents. z is still latent, but
constrained in the training data (i.e. documents are
labeled but the individual words are not). Some ap-
proaches to labeling unseen documents require that
z for the training data be inferred, and methods for
doing this include an application of the Expectation-
Maximization (EM) algorithm (McCallum, 1999)
and Labeled LDA (Ramage et al., 2009).
The model that we propose for language identifi-
cation in multilingual documents is similar to multi-
label text classification. In the framework of Equa-
tion 1, each per-token label zi is a language and the
vocabulary of tokens is not given by words but rather
by specific byte sequences (Section 3.1). The key
difference with multi-label text classification is that
we use monolingual (i.e. mono-label) training data.
Hence, z is effectively observed for the training data
(since all tokens must share the same label). To infer
z for unlabeled documents, we utilize a Gibbs sam-
pler, closely related to that proposed by Griffiths and
Steyvers (2004) for LDA. The sampling probability
for a label zi for token w in a document d is:
P (zi = j|z?i, w) ? ?(w)j ? ?
(d)
j (2)
?(w)j = P (wi|zi = j, z?i, w?i)
?(d)j = P (zi = j|z?i)
In the LDA model, ?(d)j is assumed to have a Dirich-
let distribution with hyperparameter ?, and the word
distribution for each topic ?(w)j is also assumed to
have a Dirichlet distribution with hyperparameter
?. Griffiths (2002) describes a generative model for
LDA where both ?(w)j and ?(d)j are inferred from
the output of a Gibbs sampler. In our method, we
estimate ?(w)j using maximum likelihood estima-
tion (MLE) from the training data. Estimating ?(w)j
through MLE is equivalent to a multinomial Naive
Bayes model (McCallum and Nigam, 1998):
??(w)j =
n(w)j + ?
n(.)j +W?
(3)
where n(w)j is the number of times word w occurs
with label j, and n(.)j is the total number of words
that occur with label j. By setting ? to 1, we obtain
standard Laplacian smoothing. Hence, only ??(d)j is
updated at each step in the Gibbs sampler:
??(d)j =
n(d)?i,j + ?
n(d)?i + T?
(4)
where n(d)?i,j is the number of tokens in document d
that are currently mapped to language j, and n(d)?i is
the total number of tokens in document d. In both
cases, the current assignment of zi is excluded from
the count. T is the number of languages (i.e. the size
of the label set). For simplicity, we set ? to 0. We
note that in the LDA model, ? and ? influence the
sparsity of the solution, and so it may be possible
to tune these parameters for our model as well. We
leave this as an avenue for further research.
30
3.3 Language Identification in Multilingual
Documents
The model described in Section 3.2 can be used to
compute the most likely distribution to have gen-
erated an unlabeled document over a given set of
languages for which we have monolingual training
data, by letting the set of terms w be the byte n-gram
sequences we selected using per-language informa-
tion gain (Section 3.1), and allowing the labels z to
range over the set of all languages L. Using train-
ing data, we compute ??(w)j (Equation 3), and then
we infer P (Lj |D) for each Lj ? L for the unla-
beled document, by running the Gibbs sampler until
the samples for zi converge and then tabulating zi
over the whole d and normalizing by |d|. Naively,
we could identify the languages present in the doc-
ument by D . {Lx if ?(zi = Lx|D)}, but closely-
related languages tend to have similar frequency dis-
tributions over byte n-gram features, and hence it is
likely that some tokens will be incorrectly mapped to
a language that is similar to the ?correct? language.
We address this issue by finding the subset of lan-
guages ? from the training set L that maximizes
P (?|D) (a similar approach is taken in McCallum
(1999)). Through an application of Bayes? theorem,
P (?|D) ? P (D|?)?P (?), noting that P (D) is a
normalizing constant and can be dropped. We as-
sume that P (?) is constant (i.e. any subset of lan-
guages is equally likely, a reasonable assumption in
the absence of other evidence), and hence maximize
P (D|?). For any given D = w1? ? ?wn and ?, we
infer P (D|?) from the output of the Gibbs sampler:
P (D|?) =
N?
i=1
P (wi|?) (5)
=
N?
i=1
?
j??
P (wi|zi = j)P (zi = j) (6)
where both P (wi|zi = j) and P (zi = j) are esti-
mated by their maximum likelihood estimates.
In practice, exhaustive evaluation of the powerset
of L is prohibitively expensive, and so we greed-
ily approximate the optimal ? using Algorithm 1. In
essence, we initially rank all the candidate languages
by computing the most likely distribution over the
full set of candidate languages. Then, for each of
the top-N languages in turn, we consider whether
Algorithm 1 DetectLang(L,D)
LN ? top-N z ? L by P (z|D)
?? {Lu}
for each Lt ? LN do
?? ? ? ? Lt
if P (D|?) + t < P (D|??) then
?? ??
end if
end for
?? ? \ {Lu}
return D . ?
to add it to ?. ? is initialized with Lu, a dummy
language with a uniform distribution over terms (i.e.
P (w|Lu) = 1|w| ). A language is added if it improves
P (D|?) by at least t. The threshold t is required
to suppress the addition of spurious classes. Adding
languages gives the model additional freedom to fit
parameters, and so will generally increase P (D|?).
In the limit case, adding a completely irrelevant lan-
guage will result in no tokens being mapped to the
a language, and so the model will be no worse than
without the language. The threshold t is thus used to
control ?how much? improvement is required before
including the new language in ?.
3.4 Benchmark Approaches
We compare our approach to two methods for
language identification in multilingual documents:
(1) the virtual mixed languages approach (Prager,
1999a); and (2) the text segmentation approach (Ya-
maguchi and Tanaka-Ishii, 2012).
Prager (1999a) describes LINGUINI, a language
identifier based on the vector-space model com-
monly used in text classification and information re-
trieval. The document representation used by Prager
(1999a) is a vector of counts across a set of charac-
ter sequences. Prager (1999a) selects the feature set
based on a TFIDF-like approach. Terms with occur-
rence count m < n? k are rejected, where m is the
number of times the term occurs in the training data
(the TF component), n is the number of languages in
which the term occurred (the IDF component, where
?document? is replaced with ?language?), and k is a
parameter to control the overall number of terms se-
lected. In Prager (1999a), the value of k is reported
to be optimal in the region 0.3 to 0.5. In practice,
31
the value of k indirectly controls the number of fea-
tures selected. Values of k are not comparable across
datasets as m is not normalized for the size of the
training data, so in this work we do not report the
values of k and instead directly select the top-N fea-
tures, weighted by mn . In LINGUINI, each languageis modeled as a single pseudo-document, obtained
by concatenating all the training data for the given
language. A document is then classified according
to the vector with which it has the smallest angle;
this is implemented by finding the language vector
with the highest cosine with the document vector.
Prager (1999a) also proposes an extension to the
approach to allow identification of bilingual docu-
ments, and suggests how this may be generalized to
any number of languages in a document. The gist
of the method is simple: for any given pair of lan-
guages, the projection of a document vector onto
the hyperplane containing the language vectors of
the two languages gives the mixture proportions of
the two languages that minimizes the angle with the
document vector. Prager (1999a) terms this projec-
tion a virtual mixed language (VML), and shows
how to find the angle between the document vec-
tor and the VML. If this angle is less than that be-
tween the document vector and any individual lan-
guage vector, the document is labeled as bilingual in
the two languages from which the mixed vector was
derived. The practical difficulty presented by this
approach is that exhaustively evaluating all possible
combinations of languages is prohibitively expen-
sive. Prager (1999a) addresses this by arguing that in
multilingual documents, ?the individual component
languages will be close to d (the document vector)
? probably closer than most or all other languages?.
Hence, language mixtures are only considered for
combinations of the top m languages.
Prager (1999a) shows how to obtain the mixture
coefficients for bilingual VMLs, arguing that the
process generalizes. Prager (1999b) includes the
coefficients for 3-language VMLs, which are much
more complex than the 2-language variants. Us-
ing a computer algebra system, we verified the an-
alytic forms of the coefficients in the 3-language
VML. We also attempted to obtain an analytic form
for the coefficients in a 4-language VML, but these
were too complex for the computer algebra system
to compute. Thus, our evaluation of the VML ap-
proach proposed by Prager (1999a) is limited to 3-
language VMLs. Neither Prager (1999a) nor Prager
(1999b) include an empirical evaluation over mul-
tilingual documents, so to the best of our knowl-
edge this paper is the first empirical evaluation of
the method on multilingual documents. As no refer-
ence implementation of this method is available, we
have produced our own implementation, which we
have made freely available.1
The other benchmark we consider in this paper is
the method for text segmentation by language pro-
posed by Yamaguchi and Tanaka-Ishii (2012) (here-
after referred to as SEGLANG). The actual task ad-
dressed by Yamaguchi and Tanaka-Ishii (2012) is to
divide a document into monolingual segments. This
is formulated as the task of segmenting a document
D = x1, ? ? ? , x|D| (where xi denotes the ith char-
acter of D and |D| is the length of the document)
by finding a list of boundaries B = [B1, ? ? ? , B|B|]
where each Bi indicates the location of a language
boundary as an offset from the start of the document,
resulting in a list of segments X = [X0, ? ? ? , X|B|].
For each segment Xi, the system predicts Li, the
language associated with the segment, producing a
list of labellings L = [L0, ? ? ? , L|B|], with the con-
straint that adjacent elements in L must differ. Ya-
maguchi and Tanaka-Ishii (2012) solve the problem
of determining X and L for an unlabeled text us-
ing a method based on minimum description length.
They present a dynamic programming solution to
this problem, and analyze a number of parameters
that affect the overall accuracy of the system. Given
this method to determine X and L, it is then triv-
ial to label an unlabeled document according to
D . {Lx if ?Lx ? L}, and the length of each seg-
ment in X can then be used to determine the pro-
portions of the document that are in each language.
In this work, we use a reference implementation of
SEGLANG kindly provided to us by the authors.
Using the text segmentation approach of
SEGLANG to detect multilingual documents differs
from LINGUINI and our method primarily in that
LINGUINI and our method fragment the document
into small sequences of bytes, and discard informa-
tion about the relative order of the fragments. This
is in contrast to SEGLANG, where this information
1https://github.com/saffsd/linguini.py
32
System PM RM FM P? R? F?
Benchmark .497 .467 .464 .833 .826 .829
Winner .718 .703 .699 .932 .931 .932
SEGLANG .801 .810 .784 .866 .946 .905
LINGUINI .616 .535 .513 .713 .688 .700
Our method .753 .771 .748 .945 .922 .933
Table 2: Results on the ALTW2010 dataset.
?Benchmark? is the benchmark system proposed by
the shared task organizers. ?Winner? is the highest-
F? system submitted to the shared task.
is utilized in the sequential prediction of labels for
consecutive segments of text, and is thus able to
make better use of the locality of text (since there are
likely to be monolingual blocks of text in any given
multilingual document). The disadvantage of this is
that the underlying model becomes more complex
and hence more computationally expensive, as we
observe in Section 5.
3.5 Evaluation
We seek to evaluate the ability of each method:
(1) to correctly identify the language(s) present in
each test document; and (2) for multilingual doc-
uments, to estimate the relative proportion of the
document written in each language. In the first in-
stance, this is a classification problem, and the stan-
dard notions of precision (P), recall (R) and F-score
(F) apply. Consistent with previous work in lan-
guage identification, we report both the document-
level micro-average, as well as the language-level
macro-average. For consistency with Baldwin and
Lui (2010a), the macro-averaged F-score we report
is the average of the per-class F-scores, rather than
the harmonic mean of the macro-averaged precision
and recall; as such, it is possible for the F-score
to not fall between the precision and recall values.
As is common practice, we compute the F-score for
? = 1, giving equal importance to precision and
recall.2 We tested the difference in performance
for statistical significance using an approximate ran-
domization procedure (Yeh, 2000) with 10000 iter-
ations. Within each table of results (Tables 2, 3 and
2Intuitively, it may seem that the maximal precision and re-
call should be achieved when precision and recall are balanced.
However, because of the multi-label nature of the task and vari-
able number of labels assigned to a given document by our mod-
els, it is theoretically possible and indeed common in our results
for the maximal macro-averaged F-score to be achieved when
macro-averaged precision and recall are not balanced.
4), all differences between systems are statistically
significant at a p < 0.05 level.
To evaluate the predictions of the relative propor-
tions of a document D written in each detected lan-
guageLi, we compare the topic proportion predicted
by our model to the gold-standard proportion, mea-
sured as a byte ratio as follows:
gs(Li|D) =
length of Li part of D in bytes
length of D in bytes (7)
We report the correlation between predicted and ac-
tual proportions in terms of Pearson?s r coefficient.
We also report the mean absolute error (MAE) over
all document?language pairs.
4 Experiments on ALTW2010
Our first experiment utilizes the ALTW2010 shared
task dataset (Baldwin and Lui, 2010b), a synthetic
dataset of 10000 bilingual documents3 generated
from Wikipedia data, introduced in the ALTW2010
shared task,4 The dataset is organized into training,
development and test partitions. Following standard
machine learning practice, we train each system us-
ing the training partition, and tune parameters using
the development partition. We then report macro and
micro-averaged precision, recall and F-score on the
test partition, using the tuned parameters.
The results on the ALTW2010 shared task dataset
are summarized in Table 2. Each of the three sys-
tems we compare was re-trained using the training
data provided for the shared task, with a slight dif-
ference: in the shared task, participants were pro-
vided with multilingual training documents, but the
systems targeted in this research require monolin-
gual training data. We thus split the training doc-
uments into monolingual segments using the meta-
data provided with the dataset. The metadata was
only published after completion of the task and was
not available to task participants. For comparison,
we have included the benchmark results published
by the shared task organizers, as well as the score
attained by the winning entry (Tran et al., 2010).
3With a small number of monolingual documents, formed
by randomly selecting the two languages for a given docu-
ment independently, leaving the possibility of the same two lan-
guages being selected.
4http://comp.mq.edu.au/programming/task_
description/
33
We tune the parameters for each system using the
development partition of the dataset, and report re-
sults on the test partition. For LINGUINI, there is a
single parameter k to be tuned: the number of fea-
tures per language. We tested values between 10000
and 50000, and selected 46000 features as the opti-
mal value. For our method, there are two parameters
to be tuned: (1) the number of features selected for
each language, and (2) the threshold t for including
a language. We tested features-per-language counts
between 30 and 150, and found that adding features
beyond 70 per language had minimal effect. We
tested values of the threshold t from 0.01 to 0.15,
and found the best value was 0.14. For SEGLANG,
we introduce a threshold t on the minimum propor-
tion of a document (measured in bytes) that must
be labeled by a language before that language is in-
cluded in the output set. This was done because our
initial experiments indicate that SEGLANG tends to
over-produce labels. Using the development data,
we found the best value of t was 0.10.
We find that of the three systems tested, two out-
perform the winning entry to the shared task. This
is more evident in the macro-averaged results than
in the micro-averaged results. In micro-averaged
terms, our method is the best performer, whereas
on the macro-average, SEGLANG has the high-
est F-score. This suggests that our method does
well on higher-density languages (relative to the
ALTW2010 dataset), and poorly on lower-density
languages. This also accounts for the higher micro-
averaged precision but lower micro-averaged recall
for our method as compared to SEGLANG. The im-
proved macro-average F-score of SEGLANG comes
at a much higher computational cost, which in-
creases dramatically as the number of languages is
increased. In our testing on a 16-core worksta-
tion, SEGLANG took almost 24 hours to process the
ALTW2010 shared task test data, compared to 2
minutes for our method and 40 seconds for LIN-
GUINI. As such, SEGLANG is poorly suited to de-
tecting multilingual documents where a large num-
ber of candidate languages is considered.
The ALTW2010 dataset is an excellent starting
point for this research, but it predominantly contains
bilingual documents, making it difficult to assess the
ability of systems to distinguish multilingual docu-
ments from monolingual ones. Furthermore, we are
unable to use it to assess the ability of systems to
detect more than 2 languages in a document. To ad-
dress these shortcomings, we construct a new dataset
in a similar vein. The dataset and experiments per-
formed on it are described in the next section.
5 Experiments on WIKIPEDIAMULTI
To fully test the capabilities of our model, we gen-
erated WIKIPEDIAMULTI, a dataset that contains
a mixture of monolingual and multilingual docu-
ments. To allow for replicability of our results and
to facilitate research in language identification, we
have made the dataset publicly available.5 WIKI-
PEDIAMULTI is generated using excerpts from the
mediawiki sources of Wikipedia pages downloaded
from the Wikimedia foundation.6 The dumps we
used are from July?August 2010.
To generate WIKIPEDIAMULTI, we first normal-
ized the raw mediawiki documents. Mediawiki doc-
uments typically contain one paragraph per line, in-
terspersed with structural elements. We filtered each
document to remove all structural elements, and
only kept documents that exceeded 2500 bytes after
normalization. This yielded a collection of around
500,000 documents in 156 languages. From this
initial document set (hereafter referred to as WI-
KICONTENT), we only retained languages that had
more than 1000 documents (44 languages), and gen-
erated documents for WIKIPEDIAMULTI as follows:
1. randomly select the number of languages K
(1?K?5)
2. randomly select a set of K languages S =
{Li?L for i = 1? ? ?K} without replacement
3. randomly select a document for each Li?S
from WIKICONTENT without replacement
4. take the top 1K lines of the document5. join the K sections into a single document.
As a result of the procedure, the relative propor-
tion of each language in a multilingual document
tends not to be uniform, as it is conditioned on the
length of the original document from which it was
sourced, independent of the otherK?1 for the other
languages that it was combined with. Overall, the
average document length is 5500 bytes (standard de-
viation = 3800 bytes). Due to rounding up in taking
5http://www.csse.unimelb.edu.au/?tim/
6http://dumps.wikimedia.org
34
System PM RM FM P? R? F?
SEGLANG .809 .975 .875 .771 .975 .861
LINGUINI .853 .772 .802 .838 .774 .805
Our method .962 .954 .957 .963 .955 .959
Table 3: Results on the WIKIPEDIAMULTI dataset.
the top 1k lines (step 4), documents with higher Ktend to be longer (6200 bytes for K = 5 vs 5100
bytes for K = 1).
The WIKIPEDIAMULTI dataset contains training,
development and test partitions. The training parti-
tion consists of 5000 monolingual (i.e. K = 1) doc-
uments. The development partition consists of 5000
documents, 1000 documents for each value of K
where 1?K?5. The test partition contains 200 doc-
uments for each K, for a total of 1000 documents.
There is no overlap between any of the partitions.
5.1 Results over WIKIPEDIAMULTI
We trained each system using the monolingual train-
ing partition, and tuned parameters using the devel-
opment partition. For LINGUINI, we tested feature
counts between 10000 and 50000, and found that
the effect was relatively small. We thus use 10000
features as the optimum value. For SEGLANG, we
tested values for threshold t between 0.01 and 0.20,
and found that the maximal macro-averaged F-score
is attained when t = 0.06. Finally, for our method
we tested features-per-language counts between 30
and 130 and found the best performance with 120
features per language, although the actual effect of
varying this value is rather small. We tested values
of the threshold t for adding an extra language to
? from 0.01 to 0.15, and found that the best results
were attained when t = 0.02.
The results of evaluating each system on the
test partition are summarized in Table 3. In this
evaluation, our method clearly outperforms both
SEGLANG and LINGUINI. The results on WIKI-
PEDIAMULTI and ALTW2010 are difficult to com-
pare directly due to the different compositions of the
two datasets. ALTW2010 is predominantly bilin-
gual, whereas WIKIPEDIAMULTI contains docu-
ments with text in 1?5 languages. Furthermore, the
average document in ALTW2010 is half the length
of that in WIKIPEDIAMULTI. Overall, we observe
that SEGLANG has a tendency to over-label (despite
the introduction of the t parameter to reduce this ef-
fect), evidenced by high recall but lower precision.
LINGUINI is inherently limited in that it is only able
to detect up to 3 languages per document, causing
recall to suffer on WIKIPEDIAMULTI. However, it
also tends to always output 3 languages, regardless
of the actual number of languages in the document,
hurting precision. Furthermore, even on ALTW2010
it has lower recall than the other two systems.
6 Estimating Language Proportions
In addition to detecting multiple languages within
a document, our method also estimates the relative
proportions of the document that are written in each
language. This information may be useful for detect-
ing documents that are candidate bitexts for training
machine translation systems, since we may expect
languages in the document to be present in equal
proportions. It also allows us to identify the pre-
dominant language of a document.
A core element of our model of a document is
a distribution over a set of labels. Since each la-
bel corresponds to a language, as a first approxima-
tion, we take the probability mass associated with
each label as a direct estimate of the proportion of
the document written in that language. We examine
the results for predicting the language proportions
in the test partition of WIKIPEDIAMULTI. Mapping
label distributions directly to language proportions
produces excellent results, with a Pearson?s r value
of 0.863 and an MAE of 0.108.
Although labels have a one-to-one correspon-
dence with languages, the label distribution does
not actually correspond directly to the language pro-
portion, because the distribution estimates the pro-
portion of byte n-gram sequences associated with
a label and not the proportion of bytes directly.
The same number of bytes in different languages
can produce different numbers of n-gram sequences,
because after feature selection not all n-gram se-
quences are retained in the feature set. Hereafter,
we refer to each n-gram sequence as a token, and the
average number of tokens produced per byte of text
as the token emission rate.
We estimate the per-language token emission rate
(Figure 1) using the training partition of WIKIPE-
DIAMULTI. To improve our estimate of the lan-
guage proportions, we correct our label distribution
35
Original text the cat in the hat
n-gram features
?
???
???
he : 2 the : 2
hat : 1 in : 1
th : 1 the : 1
hat : 1 he c : 1
in t : 1 n th : 1
?
???
???
Emission rate #bytes#tokens = 1812 = 1.5 bytes/token
Figure 1: Example of calculating n-gram emission
rate for a text string.
using estimates of the per-language token emission
rate RLi in bytes per token for Li?L. Assume that
a document D of length |D| is estimated to contain
K languages in proportions Pi for i = 1? ? ?K. The
corrected estimate for the proportion of Li is:
Prop(Li) =
Pi ?RLi?K
j=1 (Pj ?RLj )
(8)
Note that the |D| term is common to the numerator
and denominator and has thus been eliminated.
This correction improves our estimates of lan-
guage proportions. After correction, the Pearson?s
r rises to 0.981, and the MAE is reduced to 0.024.
The improvement is most noticeable for language?
document pairs where the proportion of the docu-
ment in the given language is about 0.5 (Figure 2).
7 Real-world Multilingual Documents
So far, we have demonstrated the effectiveness of
our proposed approach using synthetic data. The
results have been excellent, and in this section we
validate the approach by applying it to a real-world
task that has recently been discussed in the lit-
erature. Yamaguchi and Tanaka-Ishii (2012) and
King and Abney (2013) both observe that in trying
to gather linguistic data for ?non-major? languages
from the web, one challenge faced is that documents
retrieved often contain sections in another language.
SEGLANG (the solution of Yamaguchi and Tanaka-
Ishii (2012)) concurrently detects multilingual doc-
uments and segments them by language, but the ap-
proach is computationally expensive and has a ten-
dency to over-label (Section 5). On the other hand,
the solution of King and Abney (2013) is incom-
plete, and they specifically mention the need for an
automatic method ?to examine a multilingual docu-
ment, and with high accuracy, list the languages that
are present in the document?. In this section, we
show that our method is able to fill this need. We
System P R F
Baseline 0.719 1.00 0.837
SEGLANG 0.779 0.991 0.872
LINGUINI 0.729 0.981 0.837
Our method 0.907 0.916 0.912
Table 4: Detection accuracy for English-language
inclusion in web documents from targeted web
crawls for low-density languages.
make use of manually-annotated data kindly pro-
vided to us by Ben King, which consists of 149 doc-
uments containing 42 languages retrieved from the
web using a set of targeted queries for low-density
languages. Note that the dataset described in King
and Abney (2013) was based on manual confirma-
tion of the presence of English in addition to the low-
density language of primary interest; our dataset
contains these bilingual documents as well as mono-
lingual documents in the low-density language of in-
terest. Our purpose in this section is to investigate
the ability of automatic systems to select this subset
of bilingual documents. Specifically, given a col-
lection of documents retrieved for a target language,
the task is to identify the documents that contain text
in English in addition to the target language. Thus,
we re-train each system for each target language, us-
ing only training data for English and the target lan-
guage. We reserve the data provided by Ben King
for evaluation, and train our methods using data sep-
arately obtained from the Universal Declaration of
Human Rights (UDHR). Where UDHR translations
for a particular language were not available, we used
data from Wikipedia or from a bible translation. Ap-
proximately 20?80 kB of data were used for each
language. As we do not have suitable development
data, we made use of the best parameters for each
system from the experiments on WIKIPEDIAMULTI.
We find that all 3 systems are able to detect that
each document contains the target language with
100% accuracy. However, systems vary in their abil-
ity to detect if a document also contains English in
addition to the target language. The detection accu-
racy for English-language inclusion is summarized
in Table 4.7 For comparison, we include a heuristic
baseline based on labeling all documents as contain-
7Note that Table 2 and Table 3 both report macro and micro-
averaged results across a number of languages. In contrast Ta-
ble 4 only reports results for English, and the values are not
directly comparable to our earlier evaluation.
36
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.863MAE: 0.108
(a) without emission rate correction
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.981MAE: 0.0241
(b) with emission rate correction
Figure 2: Scatterplot of the predicted vs. actual language proportions in a document for the test partition of
WIKIPEDIAMULTI (predictions are from our method; each point corresponds to a document-language pair).
ing English. We find that, like the heuristic base-
line, SEGLANG and LINGUINI both tend to over-
label documents, producing false positive labels of
English, resulting in increased recall at the expense
of precision. Our method produces less false pos-
itives (but slightly more false negatives). Overall,
our method attains the best F for detecting En-
glish inclusions. Manual error analysis suggests that
the false negatives for our method generally occur
where a relatively small proportion of the document
is written in English.
8 Future Work
Document segmentation by language could be ac-
complished by a combination of our method and the
method of King and Abney (2013), which could be
compared to the method of Yamaguchi and Tanaka-
Ishii (2012) in the context of constructing corpora
for low-density languages using the web. Another
area we have identified in this paper is the tuning
of the parameters ? and ? in our model (currently
? = 0 and ? = 1), which may have some effect on
the sparsity of the model.
Further work is required in dealing with cross-
domain effects, to allow for ?off-the-shelf? language
identification in multilingual documents. Previous
work has shown that it is possible to generate a docu-
ment representation that is robust to variation across
domains (Lui and Baldwin, 2011), and we intend to
investigate if these results are also applicable to lan-
guage identification in multilingual documents. An-
other open question is the extension of the genera-
tive mixture models to ?unknown? language identi-
fication (i.e. eliminating the closed-world assump-
tion (Hughes et al., 2006)), which may be possible
through the use of non-parametric mixture models
such as Hierarchical Dirichlet Processes (Teh et al.,
2006).
9 Conclusion
We have presented a system for language identifi-
cation in multilingual documents using a generative
mixture model inspired by supervised topic model-
ing algorithms, combined with a document represen-
tation based on previous research in language iden-
tification for monolingual documents. We showed
that the system outperforms alternative approaches
from the literature on synthetic data, as well as on
real-world data from related research on linguistic
corpus creation for low-density languages using the
web as a resource. We also showed that our system
is able to accurately estimate the proportion of the
document written in each of the languages identi-
fied. We have made a full reference implementation
of our system freely available,8 as well as the syn-
thetic dataset prepared for this paper (Section 5), in
order to facilitate the adoption of this technology and
further research in this area.
8https://github.com/saffsd/polyglot
37
Acknowledgments
We thank Hiroshi Yamaguchi for making a reference
implementation of SEGLANG available to us, and
Ben King for providing us with a collection of real-
world multilingual web documents. This work was
substantially improved as a result of the insightful
feedback received from the reviewers.
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 88?97. Association for Computational Lin-
guistics.
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Timothy Baldwin and Marco Lui. 2010a. Language
identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 229?237, Los Angeles,
USA.
Timothy Baldwin and Marco Lui. 2010b. Multilin-
gual language identification: ALTW 2010 shared task
dataset. In Proceedings of the Australasian Language
Technology Workshop 2010 (ALTW 2010), pages 5?7,
Melbourne, Australia.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings the Second Workshop on
Language in Social Media (LSM2012), pages 65?74,
Montre?al, Canada.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Alessio Bosca and Luca Dini. 2010. Language identi-
fication strategies for cross language information re-
trieval. In Working Notes of the Cross Language Eval-
uation Forum (CLEF).
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1066?1074, Singapore.
Paul Cook and Marco Lui. 2012. langid.py for bet-
ter language modelling. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2012, pages 107?112, Dunedin, New Zealand.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building minority language corpora by learning to
generate web search queries. Knowledge and Infor-
mation Systems, 7(1):56?83.
Emmanuel Giguet. 1995. Categorisation according to
language: A step toward combining linguistic knowl-
edge and statistical learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi
Statistica dei Dati Testuali (JADT), pages 263?268,
Rome, Italy.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas Griffiths. 2002. Gibbs sampling in the gener-
ative model of latent Dirichlet allocation. Technical
Report, Stanford University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
38
Genitiro Kikui. 1996. Identifying the coding system
and language of on-line documents on the internet. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING ?96), pages 652?
657, Kyoto, Japan.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119, At-
lanta, Georgia.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
David D. Lewis. 1997. The Reuters-21578 data set.
available at http://www.daviddlewis.
com/resources/testcollections/
reuters21578/.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 176?186, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jicheng Liu and Chunyan Liang. 2008. Text Categoriza-
tion of Multilingual Web Pages in Specific Domain.
In Proceedings of the 12th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
PAKDD?08, pages 938?944, Osaka, Japan.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, pages Available as
Technical Report WS?98?05, AAAI Press., Madison,
USA.
Andrew Kachites McCallum. 1999. Multi-label text
classification with a mixture model trained by EM. In
Proceedings of AAAI 99 Workshop on Text Learning.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94?101.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings
of 22nd International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR?99), pages 74?81, Berkeley, USA.
John M. Prager. 1999a. Linguini: language identification
for multilingual documents. In Proceedings the 32nd
Annual Hawaii International Conference on Systems
Sciences (HICSS-32), Maui, Hawaii.
John M. Prager. 1999b. Linguini: Language identifica-
tion for multilingual documents. Journal of Manage-
ment Information Systems, 16(3):71?101.
John Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, USA.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), pages 248?256, Singapore.
Radim Rehurek and Milan Kolkus. 2009. Language
Identification on the Web: Extending the Dictionary
Method. In Proceedings of Computational Linguis-
tics and Intelligent Text Processing, 10th International
Conference (CICLing 2009), pages 357?368, Mexico
City, Mexico.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 527?534,
College Park, USA.
Kevin P Scannell. 2007. The Cru?bada?n Project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, pages 5?15, Louvain-
la-Neuve, Belgium.
W. J. Teahan. 2000. Text Classification and Seg-
mentation Using Minimum Cross-Entropy. In Pro-
ceedings the 6th International Conference ?Recherche
d?Information Assistee par Ordinateur? (RIAO?00),
pages 943?961, Paris, France.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
Jo?rg Tiedemann and Nikola Ljubes?ic?. 2012. Efficient
discrimination between closely related languages. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
2619?2634, Mumbai, India.
Giang Binh Tran, Dat Ba Nguyen, and Bin Thanh
Kieu. 2010. N-gram based approach for mul-
tilingual language identification. poster. available
39
at http://comp.mq.edu.au/programming/
task_description/VILangTek.pdf.
Fei Xia, Carrie Lewis, and William D. Lewis. 2010. Lan-
guage ID for a thousand languages. In LSA Annual
Meeting Extended Abstracts, Baltimore,USA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 969?978, Jeju Is-
land, Korea.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
40
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21?26,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific
Articles
Su Nam Kim,? Olena Medelyan,? Min-Yen Kan? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Pingar LP, Auckland, New Zealand
? School of Computing, National University of Singapore, Singapore
sunamkim@gmail.com, medelyan@gmail.com,
kanmy@comp.nus.edu.sg, tb@ldwin.net
Abstract
This paper describes Task 5 of the
Workshop on Semantic Evaluation 2010
(SemEval-2010). Systems are to automat-
ically assign keyphrases or keywords to
given scientific articles. The participating
systems were evaluated by matching their
extracted keyphrases against manually as-
signed ones. We present the overall rank-
ing of the submitted systems and discuss
our findings to suggest future directions
for this task.
1 Task Description
Keyphrases
1
are words that capture the main top-
ics of a document. As they represent these key
ideas, extracting high-quality keyphrases can ben-
efit various natural language processing (NLP) ap-
plications such as summarization, information re-
trieval and question-answering. In summariza-
tion, keyphrases can be used as a form of se-
mantic metadata (Barzilay and Elhadad, 1997;
Lawrie et al, 2001; D?Avanzo and Magnini,
2005). In search engines, keyphrases can supple-
ment full-text indexing and assist users in formu-
lating queries.
Recently, a resurgence of interest in keyphrase
extraction has led to the development of several
new systems and techniques for the task (Frank
et al, 1999; Witten et al, 1999; Turney, 1999;
Hulth, 2003; Turney, 2003; Park et al, 2004;
Barker and Corrnacchia, 2000; Hulth, 2004; Mat-
suo and Ishizuka, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007; Wan and Xiao, 2008; Liu et al, 2009;
Medelyan, 2009; Nguyen and Phan, 2009). These
1
We use ?keyphrase? and ?keywords? interchangeably to
refer to both single words and phrases.
?
Min-Yen Kan?s work was funded by National Research
Foundation grant ?Interactive Media Search? (grant # R-252-
000-325-279).
have showcased the potential benefits of keyphrase
extraction to downstream NLP applications.
In light of these developments, we felt that this
was an appropriate time to conduct a shared task
for keyphrase extraction, to provide a standard as-
sessment to benchmark current approaches. A sec-
ond goal of the task was to contribute an additional
public dataset to spur future research in the area.
Currently, there are several publicly available
data sets.
2
For example, Hulth (2003) contributed
2,000 abstracts of journal articles present in In-
spec between the years 1998 and 2002. The data
set contains keyphrases (i.e. controlled and un-
controlled terms) assigned by professional index-
ers ? 1,000 for training, 500 for validation and
500 for testing. Nguyen and Kan (2007) col-
lected a dataset containing 120 computer science
articles, ranging in length from 4 to 12 pages.
The articles contain author-assigned keyphrases
as well as reader-assigned keyphrases contributed
by undergraduate CS students. In the general
newswire domain, Wan and Xiao (2008) devel-
oped a dataset of 308 documents taken from DUC
2001 which contain up to 10 manually-assigned
keyphrases per document. Several databases, in-
cluding the ACM Digital Library, IEEE Xplore,
Inspec and PubMed provide articles with author-
assigned keyphrases and, occasionally, reader-
assigned ones. Medelyan (2009) automatically
generated a dataset using tags assigned by the
users of the collaborative citation platform CiteU-
Like. This dataset additionally records how many
people have assigned the same keyword to the
same publication. In total, 180 full-text publi-
cations were annotated by over 300 users.
3
De-
spite the availability of these datasets, a standard-
ized benchmark dataset with a well-defined train-
2
All data sets listed below are available for
download from http://github.com/snkim/
AutomaticKeyphraseExtraction
3
http://bit.ly/maui-datasets
21
ing and test split is needed to maximize compara-
bility of results.
For the SemEval-2010 Task 5, we have
compiled a set of 284 scientific articles with
keyphrases carefully chosen by both their authors
and readers. The participants? task was to develop
systems which automatically produce keyphrases
for each paper. Each team was allowed to sub-
mit up to three system runs, to benchmark the
contributions of different parameter settings and
approaches. Each run consisted of extracting a
ranked list of 15 keyphrases from each docu-
ment, ranked by their probability of being reader-
assigned keyphrases.
In the remainder of the paper, we describe
the competition setup, including how data collec-
tion was managed and the evaluation methodol-
ogy (Section 2). We present the results of the
shared task, and discuss the immediate findings of
the competition in Section 3. In Section 4 we as-
sess the human performance by comparing reader-
assigned keyphrases to those assigned by the au-
thors. This gives an approximation of an upper-
bound performance for this task.
2 Competition Setup
2.1 Data
We collected trial, training and test data from
the ACM Digital Library (conference and work-
shop papers). The input papers ranged from 6
to 8 pages, including tables and pictures. To en-
sure a variety of different topics was represented
in the corpus, we purposefully selected papers
from four different research areas for the dataset.
In particular, the selected articles belong to the
following four 1998 ACM classifications: C2.4
(Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial In-
telligence ? Multiagent Systems) and J4 (Social
and Behavioral Sciences ? Economics). All three
datasets (trial, training and test) had an equal dis-
tribution of documents from among the categories
(see Table 1). This domain specific information
was provided with the papers (e.g. I2.4-1 or H3.3-
2), in case participant systems wanted to utilize
this information. We specifically decided to strad-
dle different areas to see whether participant ap-
proaches would work better within specific areas.
Participants were provided with 40, 144, and
100 articles, respectively, in the trial, training and
test data, distributed evenly across the four re-
search areas in each case. Note that the trial data is
a subset of the training data. Since the original for-
mat for the articles was PDF, we converted them
into (UTF-8) plain text using pdftotext, and sys-
tematically restored full words that were originally
hyphenated and broken across two lines. This pol-
icy potentially resulted in valid hyphenated forms
having their hyphen (-) removed.
All collected papers contain author-assigned
keyphrases, part of the original PDF file. We addi-
tionally collected reader-assigned keyphrases for
each paper. We first performed a pilot annotation
task with a group of students to check the stabil-
ity of the annotations, finalize the guidelines, and
discover and resolve potential issues that may oc-
cur during the actual annotation. To collect the ac-
tual reader-assigned keyphrases, we then hired 50
student annotators from the Computer Science de-
partment of the National University of Singapore.
We assigned 5 papers to each annotator, esti-
mating that assigning keyphrases to each paper
should take about 10-15 minutes. Annotators were
explicitly told to extract keyphrases that actually
appear in the text of each paper, rather than to cre-
ate semantically-equivalent phrases, but could ex-
tract phrases from any part of the document (in-
cluding headers and captions). In reality, on av-
erage 15% of the reader-assigned keyphrases did
not appear in the text of the paper, but this is still
less than the 19% of author-assigned keyphrases
that did not appear in the papers. These values
were computed using the test documents only. In
other words, the maximum recall that the partici-
pating systems can achieve on these documents is
85% and 81% for the reader- and author-assigned
keyphrases, respectively.
As some keyphrases may occur in multiple
forms, in our evaluation we accepted two differ-
ent versions of genitive keyphrases: A of B ? B
A (e.g. policy of school = school policy) and A?s
B ? A B (e.g. school?s policy = school pol-
icy). In certain cases, such alternations change the
semantics of the candidate phrase (e.g., matter of
fact vs. ?fact matter). We judged borderline cases
by committee and do not include alternations that
were judged to be semantically distinct.
Table 1 shows the distribution of the trial, train-
ing and test documents over the four different re-
search areas, while Table 2 shows the distribution
of author- and reader-assigned keyphrases.
Interestingly, among the 387 author-assigned
22
Dataset Total Document Topic
C H I J
Trial 40 10 10 10 10
Training 144 34 39 35 36
Test 100 25 25 25 25
Table 1: Number of documents per topic in the
trial, training and test datasets, across the four
ACM document classifications
Dataset Author Reader Combined
Trial 149 526 621
Training 559 1824 2223
Test 387 1217 1482
Table 2: Number of author- and reader-assigned
keyphrases in the different datasets
keywords, 125 keywords match exactly with
reader-assigned keywords, while many more near-
misses (i.e. partial matches) occur.
2.2 Evaluation Method and Baseline
Traditionally, automatic keyphrase extraction sys-
tems have been assessed using the proportion of
top-N candidates that exactly match the gold-
standard keyphrases (Frank et al, 1999; Witten et
al., 1999; Turney, 1999). In some cases, inexact
matches, or near-misses, have also been consid-
ered. Some have suggested treating semantically-
similar keyphrases as correct based on simi-
larities computed over a large corpus (Jarmasz
and Barriere, 2004; Mihalcea and Tarau, 2004),
or using semantic relations defined in a the-
saurus (Medelyan and Witten, 2006). Zesch and
Gurevych (2009) compute near-misses using an n-
gram based approach relative to the gold standard.
For our shared task, we follow the traditional ex-
act match evaluation metric. That is, we match the
keyphrases in the answer set with those the sys-
tems provide, and calculate micro-averaged preci-
sion, recall and F-score (? = 1). In the evaluation,
we check the performance over the top 5, 10 and
15 candidates returned by each system. We rank
the participating systems by F-score over the top
15 candidates.
Participants were required to extract ex-
isting phrases from the documents. Since
it is theoretically possible to retrieve author-
assigned keyphrases from the original PDF arti-
cles, we evaluate the participating systems over
the independently-generated and held-out reader-
assigned keyphrases, as well as the combined set
of keyphrases (author- and reader-assigned).
All keyphrases in the answer set are stemmed
using the English Porter stemmer for both the
training and test dataset.
4
We computed a TF?IDF n-gram based baseline
using both supervised and unsupervised learning
systems. We use 1, 2, 3-grams as keyphrase can-
didates, used Na??ve Bayes (NB) and Maximum
Entropy (ME) classifiers to learn two supervised
baseline systems based on the keyphrase candi-
dates and gold-standard annotations for the train-
ing documents. In total, there are three baselines:
two supervised and one unsupervised. The per-
formance of the baselines is presented in Table 3,
where R indicates reader-assigned keyphrases and
C indicates combined (both author- and reader-
assigned) keyphrases.
3 Competition Results
The trial data was downloaded by 73 different
teams, of which 36 teams subsequently down-
loaded the training and test data. 21 teams partici-
pated in the final competition, of which two teams
withdrew their systems.
Table 4 shows the performance of the final 19
submitted systems. 5 teams submitted one run,
6 teams submitted two runs and 8 teams sub-
mitted the maximum number of three runs. We
rank the best-performing system from each team
by micro-averaged F-score over the top 15 can-
didates. We also show system performance over
reader-assigned keywords in Table 5, and over
author-assigned keywords in Table 6. In all these
tables, P, R and F denote precision, recall and F-
score, respectively.
The best results over the reader-assigned and
combined keyphrase sets are 23.5% and 27.5%,
respectively, achieved by the HUMB team. Most
systems outperformed the baselines. Systems also
generally did better over the combined set, as the
presence of a larger gold-standard answer set im-
proved recall.
In Tables 7 and 8, we ranked the teams by F-
score, computed over the top 15 candidates for
each of the four ACM document classifications.
The numbers in brackets are the actual F-scores
4
Using the Perl implementation available at http://
tartarus.org/
?
martin/PorterStemmer/; we in-
formed participants that this was the stemmer we would be
using for the task, to avoid possible stemming variations be-
tween implementations.
23
Method Top 5 candidates Top 10 candidates Top 15 candidates
by P R F P R F P R F
TF?IDF R 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
C 22.0% 7.5% 11.2% 17.7% 12.1% 14.4% 14.9% 15.3% 15.1%
NB R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
ME R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
Table 3: Baseline keyphrase extraction performance for one unsupervised (TF?IDF) and two supervised
(NB and ME) systems
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 39.0% 13.3% 19.8% 32.0% 21.8% 26.0% 27.2% 27.8% 27.5%
WINGNUS 2 40.2% 13.7% 20.5% 30.5% 20.8% 24.7% 24.9% 25.5% 25.2%
KP-Miner 3 36.0% 12.3% 18.3% 28.6% 19.5% 23.2% 24.9% 25.5% 25.2%
SZTERGAK 4 34.2% 11.7% 17.4% 28.5% 19.4% 23.1% 24.8% 25.4% 25.1%
ICL 5 34.4% 11.7% 17.5% 29.2% 19.9% 23.7% 24.6% 25.2% 24.9%
SEERLAB 6 39.0% 13.3% 19.8% 29.7% 20.3% 24.1% 24.1% 24.6% 24.3%
KX FBK 7 34.2% 11.7% 17.4% 27.0% 18.4% 21.9% 23.6% 24.2% 23.9%
DERIUNLP 8 27.4% 9.4% 13.9% 23.0% 15.7% 18.7% 22.0% 22.5% 22.3%
Maui 9 35.0% 11.9% 17.8% 25.2% 17.2% 20.4% 20.3% 20.8% 20.6%
DFKI 10 29.2% 10.0% 14.9% 23.3% 15.9% 18.9% 20.3% 20.7% 20.5%
BUAP 11 13.6% 4.6% 6.9% 17.6% 12.0% 14.3% 19.0% 19.4% 19.2%
SJTULTLAB 12 30.2% 10.3% 15.4% 22.7% 15.5% 18.4% 18.4% 18.8% 18.6%
UNICE 13 27.4% 9.4% 13.9% 22.4% 15.3% 18.2% 18.3% 18.8% 18.5%
UNPMC 14 18.0% 6.1% 9.2% 19.0% 13.0% 15.4% 18.1% 18.6% 18.3%
JU CSE 15 28.4% 9.7% 14.5% 21.5% 14.7% 17.4% 17.8% 18.2% 18.0%
LIKEY 16 29.2% 10.0% 14.9% 21.1% 14.4% 17.1% 16.3% 16.7% 16.5%
UvT 17 24.8% 8.5% 12.6% 18.6% 12.7% 15.1% 14.6% 14.9% 14.8%
POLYU 18 15.6% 5.3% 7.9% 14.6% 10.0% 11.8% 13.9% 14.2% 14.0%
UKP 19 9.4% 3.2% 4.8% 5.9% 4.0% 4.8% 5.3% 5.4% 5.3%
Table 4: Performance of the submitted systems over the combined author- and reader-assigned keywords,
ranked by F-score
for each team. Note that in the case of a tie in
F-score, we ordered teams by descending F-score
over all the data.
4 Discussion of the Upper-Bound
Performance
The current evaluation is a testament to the gains
made by keyphrase extraction systems. The sys-
tem performance over the different keyword cat-
egories (reader-assigned and author-assigned) and
numbers of keyword candidates (top 5, 10 and 15
candidates) attest to this fact.
The top-performing systems return F-scores in
the upper twenties. Superficially, this number is
low, and it is instructive to examine how much
room there is for improvement. Keyphrase extrac-
tion is a subjective task, and an F-score of 100% is
infeasible. On the author-assigned keyphrases in
our test collection, the highest a system could the-
oretically achieve was 81% recall
5
and 100% pre-
cision, which gives a maximum F-score of 89%.
However, such a high value would only be possi-
ble if the number of keyphrases extracted per doc-
ument could vary; in our task, we fixed the thresh-
olds at 5, 10 and 15 keyphrases.
5
The remaining 19% of keyphrases do not actually appear
in the documents and thus cannot be extracted.
Another way of computing the upper-bound
performance would be to look into how well peo-
ple perform the same task. We analyzed the
performance of our readers, taking the author-
assigned keyphrases as the gold standard. The au-
thors assigned an average of 4 keyphrases to each
paper, whereas the readers assigned 12 on average.
These 12 keyphrases cover 77.8% of the authors?
keyphrases, which corresponds to a precision of
21.5%. The F-score achieved by the readers on the
author-assigned keyphrases is 33.6%, whereas the
F-score of the best-performing system on the same
data is 19.3% (for top 15, not top 12 keyphrases,
see Table 6).
We conclude that there is definitely still room
for improvement, and for any future shared tasks,
we recommend against fixing any threshold on the
number of keyphrases to be extracted per docu-
ment. Finally, as we use a strict exact matching
metric for evaluation, the presented evaluation fig-
ures are a lower bound for performance, as se-
mantically equivalent keyphrases are not counted
as correct. For future runs of this challenge, we
believe a more semantically-motivated evaluation
should be employed to give a more accurate im-
pression of keyphrase acceptability.
24
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 30.4% 12.6% 17.8% 24.8% 20.6% 22.5% 21.2% 26.4% 23.5%
KX FBK 2 29.2% 12.1% 17.1% 23.2% 19.3% 21.1% 20.3% 25.3% 22.6%
SZTERGAK 3 28.2% 11.7% 16.6% 23.2% 19.3% 21.1% 19.9% 24.8% 22.1%
WINGNUS 4 30.6% 12.7% 18.0% 23.6% 19.6% 21.4% 19.8% 24.7 22.0%
ICL 5 27.2% 11.3% 16.0% 22.4% 18.6% 20.3% 19.5% 24.3% 21.6%
SEERLAB 6 31.0% 12.9% 18.2% 24.1% 20.0% 21.9% 19.3% 24.1% 21.5%
KP-Miner 7 28.2% 11.7% 16.5% 22.0% 18.3% 20.0% 19.3% 24.1% 21.5%
DERIUNLP 8 22.2% 9.2% 13.0% 18.9% 15.7% 17.2% 17.5% 21.8% 19.5%
DFKI 9 24.4% 10.1% 14.3% 19.8% 16.5% 18.0% 17.4% 21.7% 19.3%
UNICE 10 25.0% 10.4% 14.7% 20.1% 16.7% 18.2% 16.0% 19.9% 17.8%
SJTULTLAB 11 26.6% 11.1% 15.6% 19.4% 16.1% 17.6% 15.6% 19.4% 17.3%
BUAP 12 10.4% 4.3% 6.1% 13.9% 11.5% 12.6% 14.9% 18.6% 16.6%
Maui 13 25.0% 10.4% 14.7% 18.1% 15.0% 16.4% 14.9% 18.5% 16.1%
UNPMC 14 13.8% 5.7% 8.1% 15.1% 12.5% 13.7% 14.5% 18.0% 16.1%
JU CSE 15 23.4% 9.7% 13.7% 18.1% 15.0% 16.4% 14.4% 17.9% 16.0%
LIKEY 16 24.6% 10.2% 14.4% 17.9% 14.9% 16.2% 13.8% 17.2% 15.3%
POLYU 17 13.6% 5.7% 8.0% 12.6% 10.5% 11.4% 12.0% 14.9% 13.3%
UvT 18 20.4% 8.5% 12.0% 15.6% 13.0% 14.2% 11.9% 14.9% 13.2%
UKP 19 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
Table 5: Performance of the submitted systems over the reader-assigned keywords, ranked by F-score
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 21.2% 27.4% 23.9% 15.4% 39.8% 22.2% 12.1% 47.0% 19.3%
KP-Miner 2 19.0% 24.6% 21.4% 13.4% 34.6% 19.3% 10.7% 41.6% 17.1%
ICL 3 17.0% 22.0% 19.2% 13.5% 34.9% 19.5% 10.5% 40.6% 16.6%
Maui 4 20.4% 26.4% 23.0% 13.7% 35.4% 19.8% 10.2% 39.5% 16.2%
SEERLAB 5 18.8% 24.3% 21.2% 13.1% 33.9% 18.9% 10.1% 39.0% 16.0%
SZTERGAK 6 14.6% 18.9% 16.5% 12.2% 31.5% 17.6% 9.9% 38.5% 15.8%
WINGNUS 7 18.6% 24.0% 21.0% 12.6% 32.6% 18.2% 9.3% 36.2% 14.8%
DERIUNLP 8 12.6% 16.3% 14.2% 9.7% 25.1% 14.0% 9.3% 35.9% 14.7%
KX FBK 9 13.6% 17.6% 15.3% 10.0% 25.8% 14.4% 8.5% 32.8% 13.5%
BUAP 10 5.6% 7.2% 6.3% 8.1% 20.9% 11.7% 8.3% 32.0% 13.2%
JU CSE 11 12.0% 15.5% 13.5% 8.5% 22.0% 12.3% 7.5% 29.0% 11.9%
UNPMC 12 7.0% 9.0% 7.9% 7.7% 19.9% 11.1% 7.1% 27.4% 11.2%
DFKI 13 12.8% 16.5% 14.4% 8.5% 22.0% 12.3% 6.6% 25.6% 10.5%
SJTULTLAB 14 9.6% 12.4% 10.8% 7.8% 20.2% 11.3% 6.2% 24.0% 9.9%
Likey 15 11.6% 15.0% 13.1% 7.9% 20.4% 11.4% 5.9% 22.7% 9.3%
UvT 16 11.4% 14.7% 12.9% 7.6% 19.6% 11.0% 5.8% 22.5% 9.2%
UNICE 17 8.8% 11.4% 9.9% 6.4% 16.5% 9.2% 5.5% 21.5% 8.8%
POLYU 18 3.8% 4.9% 4.3% 4.1% 10.6% 5.9% 4.1% 16.0% 6.6%
UKP 19 1.6% 2.1% 1.8% 0.9% 2.3% 1.3% 0.8% 3.1% 1.3%
Table 6: Performance of the submitted systems over the author-assigned keywords, ranked by F-score
5 Conclusion
This paper has described Task 5 of the Workshop
on Semantic Evaluation 2010 (SemEval-2010), fo-
cusing on keyphrase extraction. We outlined the
design of the datasets used in the shared task and
the evaluation metrics, before presenting the offi-
cial results for the task and summarising the im-
mediate findings. We also analyzed the upper-
bound performance for this task, and demon-
strated that there is still room for improvement
over the task. We look forward to future advances
in automatic keyphrase extraction based on this
and other datasets.
References
Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases. In
Proceedings of BCCSCSI : Advances in Artificial In-
telligence. 2000, pp.96?103.
Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10?17.
Ernesto D?Avanzo and Bernado Magnini. A
Keyphrase-Based Approach to Summarization: the
LAKE System at DUC-2005. In Proceedings of
DUC. 2005.
Eibe Frank and Gordon W. Paynter and Ian H. Witten
and Carl Gutwin and Craig G. Nevill-Manning. Do-
main Specific Keyphrase Extraction. In Proceedings
of IJCAI. 1999, pp.668?673.
Annette Hulth. Improved automatic keyword extrac-
tion given more linguistic knowledge. In Proceed-
ings of EMNLP. 2003, 216?223.
Annette Hulth. Enhancing Linguistically Oriented
Automatic Keyword Extraction. In Proceedings of
HLT/NAACL. 2004, pp. 17?20.
Mario Jarmasz and Caroline Barriere. Using semantic
similarity over tera-byte corpus, compute the perfor-
mance of keyphrase extraction. In Proceedings of
CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosen-
berg. Finding Topic Words for Hierarchical Summa-
rization. In Proceedings of SIGIR. 2001, pp. 349?
357.
25
Rank Group C Group H Group I Group J
1 HUMB(28.3%) HUMB(30.2%) HUMB(24.2%) HUMB(27.4%)
2 ICL(27.2%) WINGNUS(28.9%) SEERLAB(24.2%) WINGNUS(25.4%)
3 KP-Miner(25.5%) SEERLAB(27.8%) KP-Miner(22.8%) ICL(25.4%)
4 SZTERGAK(25.3%) KP-Miner(27.6%) KX FBK(22.8%) SZTERGAK(25.17%)
5 WINGNUS(24.2%) SZTERGAK(27.6%) WINGNUS(22.3%) KP-Miner(24.9%)
6 KX FBK(24.2%) ICL(25.5%) SZTERGAK(22.25%) KX FBK(24.6%)
7 DERIUNLP(23.6%) KX FBK(23.9%) ICL(21.4%) UNICE(23.5%)
8 SEERLAB(22.0%) Maui(23.9%) DERIUNLP(20.1%) SEERLAB(23.3%)
9 DFKI(21.7%) DERIUNLP(23.6%) DFKI(19.3%) DFKI(22.2%)
10 Maui(19.3%) UNPMC(22.6%) BUAP(18.5%) Maui(21.3%)
11 BUAP(18.5%) SJTULTLAB(22.1%) SJTULTLAB(17.9%) DERIUNLP(20.3%)
12 JU CSE(18.2%) UNICE(21.8%) JU CSE(17.9%) BUAP(19.7%)
13 Likey(18.2%) DFKI(20.5%) Maui(17.6%) JU CSE(18.6%)
14 SJTULTLAB(17.7%) BUAP(20.2%) UNPMC(17.6%) UNPMC(17.8%)
15 UvT(15.8%) UvT(20.2%) UNICE(14.7%) Likey(17.2%)
16 UNPMC(15.2%) Likey(19.4%) Likey(11.3%) SJTULTLAB(16.7%)
17 UNIC(14.3%) JU CSE(17.3%) POLYU(13.6%) POLYU(14.3%)
18 POLYU(12.5%) POLYU(15.8%) UvT(10.3%) UvT(12.6%)
19 UKP(4.4%) UKP(5.0%) UKP(5.4%) UKP(6.8%)
Table 7: System ranking (and F-score) for each ACM classification: combined keywords
Rank Group C Group H Group I Group J
1 ICL(23.3%) HUMB(25.0%) HUMB(21.7%) HUMB(24.7%)
2 KX FBK(23.3%) WINGNUS(23.5%) KX FBK(21.4%) WINGNUS(24.4%)
3 HUMB(22.7%) SEERLAB(23.2%) SEERLAB(21.1%) SZTERGAK(24.4%)
4 SZTERGAK(22.7%) KP-Miner(22.4%) WINGNUS(19.9%) KX FBK(24.4%)
5 DERIUNLP(21.5%) SZTERGAK(21.8%) KP-Miner(19.6%) UNICE(23.8%)
6 KP-Miner(21.2%) KX FBK(21.2%) SZTERGAK(19.6%) ICL(23.5%)
7 WINGNUS(20.0%) ICL(20.1%) ICL(19.6%) KP-Miner(22.6%)
8 SEERLAB(19.4%) DERIUNLP(20.1%) DFKI(18.5%) SEERLAB(22.0%)
9 DFKI(19.4%) DFKI(19.5%) SJTULTLAB(17.6%) DFKI(21.7%)
10 JU CSE(17.0%) SJTULTLAB(19.5%) DERIUNLP(17.3%) BUAP(19.6%)
11 Likey(16.4%) UNICE(19.2%) JU CSE(16.7%) DERIUNLP(19.0%)
12 SJTULTLAB(15.8%) Maui(18.1%) BUAP(16.4%) Maui(17.8%)
13 BUAP(15.5%) UNPMC(18.1%) UNPMC(16.1%) JU CSE(17.9%)
14 Maui(15.2%) Likey(16.9%) Maui(14.9%) Likey(17.5%)
15 UNICE(14.0%) UvT(16.4%) UNICE(14.0%) UNPMC(16.6%)
16 UvT(14.0%) POLYU(15.5%) POLYU(11.9%) SJTULTLAB(16.3%)
17 UNPMC(13.4%) BUAP(14.9%) Likey(10.4%) POLYU(13.3%)
18 POLYU(12.5%) JU CSE(12.6%) UvT(9.5%) UvT(13.0%)
19 UKP(4.5%) UKP(4.3%) UKP(5.4%) UKP(6.9%)
Table 8: System ranking (and F-score) for each ACM classification: reader-assigned keywords
Zhiyuan Liu and Peng Li and Yabin Zheng and Sun
Maosong. Clustering to Find Exemplar Terms for
Keyphrase Extraction. In Proceedings of EMNLP.
2009, pp. 257?266.
Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
occurrence Statistical Information. International
Journal on Artificial Intelligence Tools. 2004, 13(1),
pp. 157?169.
Olena Medelyan and Ian H. Witten. Thesaurus based
automatic keyphrase indexing. In Proceedings of
ACM/IEED-CS JCDL. 2006, pp. 296?297.
Olena Medelyan. Human-competitive automatic topic
indexing. PhD Thesis. University of Waikato. 2009.
Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP. 2004,
pp. 404?411.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientific Publications. In Proceed-
ings of ICADL. 2007, pp. 317?326.
Chau Q. Nguyen and Tuoi T. Phan. An ontology-based
approach for key phrase extraction. In Proceedings
of the ACL-IJCNLP. 2009, pp. 181?184.
Youngja Park and Roy J. Byrd and Branimir Bogu-
raev. Automatic Glossary Extraction Beyond Termi-
nology Identification. In Proceedings of COLING.
2004, pp. 48?55.
Peter Turney. Learning to Extract Keyphrases from
Text. In National Research Council, Institute for In-
formation Technology, Technical Report ERB-1057.
1999.
Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434?
439.
Xiaojun Wan and Jianguo Xiao. CollabRank: to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969?976.
Ian H. Witten and Gordon Paynter and Eibe
Frank and Car Gutwin and Graig Nevill-Manning.
KEA:Practical Automatic Key phrase Extraction.
In Proceedings of ACM conference on Digital li-
braries. 1999, pp. 254?256.
Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In
Proceedings of RANLP. 2009.
26
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 100?104,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Combining resources for MWE-token classification
Richard Fothergill and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
VIC 3010 Australia
r.fothergill@student.unimelb.edu.au, tb@ldwin.net
Abstract
We study the task of automatically disam-
biguating word combinations such as jump
the gun which are ambiguous between a lit-
eral and MWE interpretation, focusing on the
utility of type-level features from an MWE
lexicon for the disambiguation task. To
this end we combine gold-standard idiomatic-
ity of tokens in the OpenMWE corpus with
MWE-type-level information drawn from the
recently-published JDMWE lexicon. We find
that constituent modifiability in an MWE-type
is more predictive of the idiomaticity of its
tokens than other constituent characteristics
such as semantic class or part of speech.
1 Introduction
A multiword expression (MWE) is a phrase or
sequence of words which exhibits idiosyncratic be-
haviour (Sag et al, 2002; Baldwin and Kim, 2009).
The nature of this idiosyncracy may be purely dis-
tributional ? such as hot and cold being more com-
mon than cold and hot ? but in this paper we study
MWEs with idiosyncratic semantics. Specifically
we are concerned with expressions such as jump the
gun which are ambiguous between a literal interpre-
tation of ?to leap over a firearm?, and an idiomatic
interpretation of ?to act prematurely?.
While MWEs are increasingly entering the main-
stream of NLP, the accurate identification of MWEs
remains elusive for current methods, particularly in
the absence of MWE type-specialised training data.
This paper builds on the work of Hashimoto et al
(2006) and Fothergill and Baldwin (2011) in ex-
ploring whether type-level MWE properties sourced
from an idiom dictionary can boost the accuracy of
crosstype MWE-token classification. That is, we
attempt to determine whether token occurrences of
ambiguous expressions such as Kim jumped the gun
on this issue are idiomatic or literal, based on: (a)
annotated instances for MWEs other than jump the
gun (e.g. we may only have token-level annotations
for kick the bucket and throw in the towel), and (b)
dictionary-based information on the syntactic prop-
erties of the idiom in question.
We find that constituent modifiability judgments
extracted from the idiom dictionary are more predic-
tive of the idiomaticity of tokens than other features
of the idiom?s constituents such as part of speech
or lexeme. However, violations of the dictionary?s
modifiability rules have variable utility for machine
learning classification, being suggestive of the literal
class but not definitive. Finally, we present novel re-
sults illuminating the effectiveness of contextual se-
mantic vectors at MWE-token classification.
2 Related Work
The OpenMWE corpus (Hashimoto and Kawahara,
2009) is a gold-standard corpus of over 100, 000
Japanese MWE-tokens covering 146 types. It is the
largest resource we are aware of which has hand-
annotated instances of MWEs which are ambiguous
between a literal and idiomatic interpretation, and
has been used by Hashimoto and Kawahara (2009)
and Fothergill and Baldwin (2011) for supervised
classification of MWE-tokens using features cap-
turing lexico-syntactic variation and traditional se-
mantic features borrowed from word sense disam-
biguation (WSD) . Similar work in other languages
has been performed by Li and Sporleder (2010) and
Diab and Bhutada (2009). We build on this work in
exploring the use of MWE-type-level features drawn
from an idiom dictionary for MWE identification.
100
Hashimoto and Kawahara (2009) developed a va-
riety of features capturing lexico-syntactic variation
but only one ? a Boolean feature for ?internal mod-
ification?, which fired only when a non-constituent
word appeared between constituent words in an
MWE-token ? had an appreciable impact on classi-
fication. However, they found that this effect was far
overshadowed by semantic context features inspired
by WSD. That is, treating each MWE-type as a word
with two senses and performing sense disambigua-
tion was far more successful than any features based
on lexico-syntactic characteristics of idioms. Intu-
itively, we would expect that if we had access to a
rich inventory of expression-specific type-level fea-
tures encoding the ability of the expression to partic-
ipate in different syntactic alternations, we should be
better equipped to disambiguate token occurrences
of that expression. Indeed, the work of Fazly et al
(2009) would appear to support this hypothesis, in
that the authors used unsupervised methods to learn
type-level preferences for a range of MWE types,
and demonstrated that these could be successfully
applied to a token-level disambiguation task.
Hashimoto and Kawahara (2009) trained indi-
vidual classifiers for each MWE-type in their cor-
pus and tested them only on instances of the type
they were trained on. In contrast to this type-
specialised classification, Fothergill and Baldwin
(2011) trained classifiers on a subset of MWE-types
and tested on instances of the remaining held-out
MWE-types. The motivation for this crosstype
classification was to test the use of data from the
OpenMWE corpus for MWE-token classification of
MWE-types with no gold-standard data available
(which are by far the majority). Fothergill and Bald-
win (2011) introduced features for crosstype classi-
fication which captured features of the MWE-type,
reasoning that similar expressions would have sim-
ilar propensity for idiomaticity. We introduce new
MWE-type features expressing the modifiability of
constituents based on information extracted from an
MWE dictionary with wide coverage.
Fothergill and Baldwin (2011) expected that
WSD features ? however successful at type spe-
cialised classification ? would lose their advantage
in crosstype classification because of the lack of a
common semantics between MWE-types. However,
this turned out not to be the case, with by far the
most successful results arising again from use of
WSD features. This surprising result raises the pos-
sibility of distributional similarity between the con-
texts of idiomatic MWE-tokens of different MWE-
types, however the result was not explained or ex-
plored further. In this paper we offer new insights
into the distributional similarity hypothesis.
The recently-published JDMWE (Japanese Dic-
tionary of Multiword Expressions) encodes type-
level information on thousands of Japanese MWEs
(Shudo et al, 2011). A subset of the dictionary has
been released, and overlaps to some extent with the
MWE-types in the OpenMWE corpus. JDMWE en-
codes information about lexico-syntactic variations
allowed by each MWE-type it contains. For exam-
ple, the expression hana wo motaseru ? literally
?to have [someone] hold flowers? but figuratively
?to let [someone] take the credit? ? has the syntac-
tic form entry [N wo] *V30. The asterix indicates
modifiability, telling us that the head [V]erb mo-
taseru ?cause to hold? allows modification by non-
constituent dependents ? such as adverbs ? but the
dependent [N]oun hana ?flowers? does not.
3 Features for classification
We introduce features based on the lexico-syntactic
flexibility constraints encoded in JDMWE and com-
pare them with similar features from related work.
3.1 Type-level features
We extracted the modifiability flags from the syntac-
tic field of entries in JDMWE and generated a feature
for each modifiable constituent, identified by its po-
sition in the type?s parse tree. The motivation for
this is to allow machine learning algorithms to cap-
ture any similarities in idiomaticity between MWE-
types with similar modifiability.
Fothergill and Baldwin (2011) also aimed to
exploit crosstype similarity with their type fea-
tures. They extracted lexical features (part-of-
speech, lemma and semantic category) of the type
headword and other constituents. We use these fea-
tures as point of contrast.
3.2 Token features
An internal modifier is a dependent of a constituent
which is not a constituent itself but divides an MWE-
token into two parts, such as the word seven in kick
101
seven buckets. Features in related work have flagged
the presence of any internal modifier uncondition-
ally (Hashimoto and Kawahara, 2009; Fothergill and
Baldwin, 2011). We introduce a refined feature
which fires only when a MWE-token has an internal
modifier which violates the constituent modification
constraints encoded in JDMWE.
JDMWE modifiability constraints could also be
construed to proscribe external modifiers. Sentential
subjects and other external arguments of the head
verb are too common to be sensibly proscribed but
we did include a feature flagging proscribed exter-
nal modification of leaf constituents such as wa-
ter in kick the bucket of water. This feature effec-
tively refines the adnominal modification feature of
Hashimoto and Kawahara (2009) which indiscrimi-
nately flags external modifications on a leaf noun.
We include in our analysis a contrast of these fea-
tures to token-based features in related work. The
closest related features are those focussed on the
MWE characteristic of lexico-syntactic fixedness
termed idiom features by Hashimoto and Kawahara
(2009) and Fothergill and Baldwin (2011):
? the flag for internal modification;
? the flag for adnominal modification;
? lexical features such as part-of-speech, lemma
and semantic category extracted from an inter-
nal or adnominal modifier;
? inflections of the head constituent.
Additionally, we include WSD-inspired features
used by Hashimoto and Kawahara (2009) and
Fothergill and Baldwin (2011). These are all lexi-
cal features extracted from context, including part-
of-speech, lemma and semantic category of words
in the paragraph, local and syntactic contexts of the
MWE-token. These features set the high water mark
for classification accuracy in both type-specialised
and crosstype classification scenarios.
3.3 Example JDMWE feature extraction
The following is a short literal token of the example
type from Section 2, with numbered constituents:
kireina hanawo(2) motaseta(1) (?[He] had [me] hold
the pretty flowers?). The JDMWE features emitted
for this token are the type feature modifiable(1) and
the token feature proscribed premodifier(2).
4 Results
We worked with a subset of the OpenMWE cor-
pus comprising those types having: (a) an entry in
the released subset of the JDMWE, and (b) both lit-
eral and idiomatic classes represented by at least 50
MWE-tokens each in the corpus. This leaves only 27
MWE-types and 23, 392 MWE-tokens and means
that our results are not directly comparable to those
of Hashimoto and Kawahara (2009) and Fothergill
and Baldwin (2011). The release of the full JDMWE
should enable more comparable results.
We constructed a crosstype classification task
by ten-fold cross validation of the MWE-types in
the OpenMWE subset, with micro-averaged results.
Training sets were the union of all MWE-tokens of
MWE-types in a partition. The majority class was
the idiomatic sense and provided a baseline accu-
racy of 0.594. Support Vector Machine models with
linear kernels were trained on various feature com-
binations using the libSVM package.
Our JDMWE type-level features performed com-
paratively well at the crosstype task, with an accu-
racy of 0.647, at 5.3 percentage points above the
baseline. This is a marked improvement on the lex-
ical type-level features from related work, which
achieved an accuracy of 4.0 points above baseline.
As has been observed in related work, the accuracy
gained by using type-level features is much smaller
than the token-level WSD features. However, the
relative performance of the JDMWE type features to
the lexical type features is sustained in combination
with other feature types, as shown in Figure 1a.
Our JDMWE token-level features on the other
hand perform quite badly at crosstype classification.
When measured against the baseline or used to aug-
ment other token features, they degraded or only
marginally improved performance. The fact that us-
ing these features resulted in worse-than-baseline
performance suggests that the constituent modifia-
bility features extracted from JDMWE may not be
strict constraints as they are construed.
To better examine the quality of the JDMWE con-
stituent modifiability constraint features, we con-
structed a heuristic classifier. The classifier applies
the idiomatic class by default, but the literal class to
any MWE-token which violates the JDMWE con-
stituent modifiability constraints. This classifier?s
102
(a) Accuracy using JDMWE type-level
features and lexical type-level features in
combination with various token-level fea-
tures
(b) Recall for idiomatic instances for var-
ious feature combinations with and with-
out WSD context features, in a type-
specialised classification setting
(c) Recall for literal instances for vari-
ous feature combinations with and with-
out WSD context features, in a type-
specialised classification setting.
Figure 1: Results
precision on the literal class was 0.624, meaning that
fully 0.376 of modifiability constraint violations in
the corpus occured for idiomatic tokens.
However, the classifier was correct in its literal
class labels more than half the time so it achieved a
better accuracy than the majority class classifer, at
0.612. As such, the heuristic classifier comfortably
outperformed the Support Vector Machine classifier
based on the same features. This shows that our poor
results with regards to the JDMWE constraint viola-
tion features are due mainly to failures of the ma-
chine learning model to take advantage of them.
As to the strength of the constraints encoded in
JDMWE, we found that 4.4% of all idiomatic tokens
in the corpus violated constituent modification con-
straints, and 10.8% of literal tokens. Thus the con-
straints seem sound but not as rigid as presented by
the JDMWE developers.
Figure 1a shows that even with our improvements
to type-level features, the finding of Fothergill and
Baldwin (2011) that WSD context features perform
best at crosstype classification still holds. We can-
not fully account for this, but one observation re-
garding the results of our type-specialised evaluation
may have bearing on the crosstype scenario.
For our type-specialised classification task we
performed cross-validation for each MWE-type in
isolation, aggregating final results. Some types had
a literal majority class, so the baseline accuracy was
0.741. Figure 1b shows that type-specialised classi-
fication performance is basically constant when re-
stricting analysis to only the idiomatic test instances.
The huge performance boost produced through the
use of WSD features occurs only on literal instances
(see Figure 1c). That is, our type-specialised clas-
sifiers are capturing distributional similarity of con-
text for the literal instances of a MWE-type but not
for the idiomatic instances. Since the contexts of id-
iomatic instances of the same MWE-type do not ex-
hibit a usable distributional similarity, it is unlikely
that crosstype similarities between idiomatic MWE-
token contexts can explain the efficacy of WSD fea-
tures for crosstype classification.
5 Conclusion
Using a MWE dictionary as input to a supervised
crosstype MWE-token classification task we have
shown that the constituents? modifiability character-
istics tell more about idiomaticity than their lexical
characteristics. We found that the constituent modi-
fication constraints in JDMWE are not hard-and-fast
rules but do show up statistically in the OpenMWE
corpus. Finally, we found that distributional simi-
larity of the contexts of idiomatic MWE-tokens is
unlikely to be the source of the success of WSD fea-
tures on MWE-token classification accuracy.
103
References
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
pages 267?292. CRC Press, Boca Raton, USA, 2nd
edition.
Mona T. Diab and Pravin Bhutada. 2009. Verb noun
construction MWE token supervised classification. In
MWE ?09: Proceedings of the Workshop on Multiword
Expressions, pages 17?22, Singapore.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Richard Fothergill and Timothy Baldwin. 2011. Flesh-
ing it out: A supervised approach to MWE-token and
MWE-type classification. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, Chiang Mai, Thailand.
Chikara Hashimoto and Daisuke Kawahara. 2009. Com-
pilation of an idiom example database for supervised
idiom identification. Language Resources and Evalu-
ation, 43:355?384.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Detecting Japanese idioms with a linguistically
rich dictionary. Language Resources and Evaluation,
40:243?252.
Linlin Li and Caroline Sporleder. 2010. Linguistic cues
for distinguishing literal and non-literal usages. In
Coling 2010: Posters, pages 683?691, Beijing, China.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 189?206, Mexico City, Mexico.
Kosho Shudo, Akira Kurahone, and Toshifumi Tanabe.
2011. A comprehensive dictionary of multiword ex-
pressions. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, Portland, USA.
104
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228?236,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Effects of Semantic Annotations on Precision Parse Ranking
Andrew MacKinlay??, Rebecca Dridan??, Diana McCarthy?? and Timothy Baldwin??
? Dept. of Computing and Information Systems, University of Melbourne, Australia
? NICTA Victoria Research Laboratories, University of Melbourne, Australia
? Department of Informatics, University of Oslo, Norway
? Computational Linguistics and Phonetics, Saarland University, Germany
amack@csse.unimelb.edu.au, rdridan@ifi.uio.no,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
We investigate the effects of adding semantic
annotations including word sense hypernyms
to the source text for use as an extra source
of information in HPSG parse ranking for the
English Resource Grammar. The semantic an-
notations are coarse semantic categories or en-
tries from a distributional thesaurus, assigned
either heuristically or by a pre-trained tagger.
We test this using two test corpora in different
domains with various sources of training data.
The best reduces error rate in dependency F-
score by 1% on average, while some methods
produce substantial decreases in performance.
1 Introduction
Most start-of-the-art natural language parsers (Char-
niak, 2000; Clark and Curran, 2004; Collins, 1997)
use lexicalised features for parse ranking. These are
important to achieve optimal parsing accuracy, and
yet these are also the features which by their nature
suffer from data-sparseness problems in the training
data. In the absence of reliable fine-grained statis-
tics for a given token, various strategies are possible.
There will often be statistics available for coarser
categories, such as the POS of the particular token.
However, it is possible that these coarser represen-
tations discard too much, missing out information
which could be valuable to the parse ranking. An
intermediate level of representation could provide
valuable additional information here. For example,
?This research was conducted while the second author was
a postdoctoral researcher within NICTA VRL.
?The third author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
assume we wish to correctly attach the prepositional
phrases in the following examples:
(1) I saw a tree with my telescope
(2) I saw a tree with no leaves
The most obvious interpretation in each case has the
prepositional phrase headed by with attaching in dif-
ferent places: to the verb phrase in the first example,
and to the noun tree in the second. Such distinctions
are difficult for a parser to make when the training
data is sparse, but imagine we had seen examples
such as the following in the training corpus:
(3) Kim saw a eucalypt with his binoculars
(4) Sandy observed a willow with plentiful foliage
There are few lexical items in common, but in each
case the prepositional phrase attachment follows the
same pattern: in (3) it attaches to the verb, and in
(4) to the noun. A conventional lexicalised parser
would have no knowledge of the semantic similarity
between eucalypt and tree, willow and tree, binoc-
ulars and telescope, or foliage and leaves, so would
not be able to make any conclusions about the earlier
examples on the basis of this training data. However
if the parse ranker has also been supplied with in-
formation about synonyms or hypernyms of the lex-
emes in the training data, it could possibly have gen-
eralised, to learn that PPs containing nouns related
to seeing instruments often modify verbs relating to
observation (in preference to nouns denoting inani-
mate objects), while plant flora can often be modi-
fied by PPs relating to appendages of plants such as
leaves. This is not necessarily applicable only to PP
attachment, but may help in a range of other syntac-
tic phenomena, such as distinguishing between com-
plements and modifiers of verbs.
228
The synonyms or hypernyms could take the form
of any grouping which relates word forms with se-
mantic or syntactic commonality ? such as a label
from the WordNet (Miller, 1995) hierarchy, a sub-
categorisation frame (for verbs) or closely related
terms from a distributional thesaurus (Lin, 1998).
We present work here on using various levels
of semantic generalisation as an attempt to im-
prove parse selection accuracy with the English Re-
source Grammar (ERG: Flickinger (2000)), a preci-
sion HPSG-based grammar of English.
2 Related Work
2.1 Parse Selection for Precision Grammars
The focus of this work is on parsing using hand-
crafted precision HPSG-based grammars, and in
particular the ERG. While these grammars are care-
fully crafted to avoid overgeneration, the ambiguity
of natural languages means that there will unavoid-
ably be multiple candidate parses licensed by the
grammar for any non-trivial sentence. For the ERG,
the number of parses postulated for a given sentence
can be anywhere from zero to tens of thousands. It
is the job of the parse selection model to select the
best parse from all of these candidates as accurately
as possible, for some definition of ?best?, as we dis-
cuss in Section 3.2.
Parse selection is usually performed by training
discriminative parse selection models, which ?dis-
criminate? between the set of all candidate parses.
A widely-used method to achieve this is outlined
in Velldal (2007). We feed both correct and incor-
rect parses licensed by the grammar to the TADM
toolkit (Malouf, 2002), and learn a maximum en-
tropy model. This method is used by Zhang et al
(2007) and MacKinlay et al (2011) inter alia. One
important implementation detail is that rather than
exhaustively ranking all candidates out of possibly
many thousands of trees, Zhang et al (2007) showed
that it was possible to use ?selective unpacking?,
which means that the exhaustive parse forest can be
represented compactly as a ?packed forest?, and the
top-ranked trees can be successively reconstructed,
enabling faster parsing using less memory.
2.2 Semantic Generalisation for parse ranking
Above, we outlined a number of reasons why
semantic generalisation of lexemes could enable
parsers to make more efficient use of training data,
and indeed, there has been some prior work investi-
gating this possibility. Agirre et al (2008) applied
two state-of-the-art treebank parsers to the sense-
tagged subset of the Brown corpus version of the
Penn Treebank (Marcus et al, 1993), and added
sense annotation to the training data to evaluate their
impact on parse selection and specifically on PP-
attachment. The annotations they used were oracle
sense annotations, automatic sense recognition and
the first sense heuristic, and it was this last method
which was the best performer in general. The sense
annotations were either the WordNet synset ID or
the coarse semantic file, which we explain in more
detail below, and replaced the original tokens in
the training data. The largest improvement in pars-
ing F-score was a 6.9% reduction in error rate for
the Bikel parser (Bikel, 2002), boosting the F-score
from 0.841 to 0.852, using the noun supersense only.
More recently, Agirre et al (2011) largely repro-
duced these results with a dependency parser.
Fujita et al (2007) add sense information to im-
prove parse ranking with JaCy (Siegel and Bender,
2002), an HPSG-based grammar which uses simi-
lar machinery to the ERG. They use baseline syn-
tactic features, and also add semantic features based
on dependency triples extracted from the semantic
representations of the sentence trees output by the
parser. The dataset they use has human-assigned
sense tags from a Japanese lexical hierarchy, which
they use as a source of annotations. The dependency
triples are modified in each feature set by replacing
elements of the semantic triples with corresponding
senses or hypernyms. In the best-performing con-
figuration, they use both syntactic and semantic fea-
tures with multiple levels of the the semantic hier-
archy from combined feature sets. They achieve a
5.6% improvement in exact match parsing accuracy.
3 Methodology
We performed experiments in HPSG parse rank-
ing using the ERG, evaluating the impact on parse
selection of semantic annotations such as coarse
sense labels or synonyms from a distributional the-
229
WESCIENCE LOGON
Total Sentences 9632 9410
Parseable Sentences 9249 8799
Validated Sentences 7631 8550
Train/Test Sentences 6149/1482 6823/1727
Tokens/sentence 15.0 13.6
Training Tokens 92.5k 92.8k
Table 1: Corpora used in our experiments, with total sen-
tences, how many of those can be parsed, how many of
the parseable sentences have a single gold parse (and are
used in these experiments), and average sentence length
saurus. Our work here differs from the aforemen-
tioned work of Fujita et al (2007) in a number of
ways. Firstly, we use purely syntactic parse selec-
tion features based on the derivation tree of the sen-
tence (see Section 3.4.3), rather than ranking using
dependency triples, meaning that our method is in
principle able to be integrated into a parser more eas-
ily, where the final set of dependencies would not be
known in advance. Secondly, we do not use human-
created sense annotations, instead relying on heuris-
tics or trained sense-taggers, which is closer to the
reality of real-world parsing tasks.
3.1 Corpora
Following MacKinlay et al (2011), we use two pri-
mary training corpora. First, we use the LOGON
corpus (Oepen et al, 2004), a collection of En-
glish translations of Norwegian hiking texts. The
LOGON corpus contains 8550 sentences with ex-
actly one gold parse, which we partitioned ran-
domly by sentence into 10 approximately equal sec-
tions, reserving two sections as test data, and us-
ing the remainder as our training corpus. These
sentences were randomly divided into training and
development data. Secondly, we use the We-
Science (Ytrest?l et al, 2009) corpus, a collection
of Wikipedia articles related to computational lin-
guistics. The corpus contains 11558 sentences, from
which we randomly chose 9632, preserving the re-
mainder for future work. This left 7631 sentences
with a single gold tree, which we divided into a
training set and a development set in the same way.
The corpora are summarised in Table 1.
With these corpora, we are able to investigate in-
domain and cross-domain effects, by testing on a
different corpus to the training corpus, so we can
examine whether sense-tagging alleviates the cross-
domain performance penalty noted in MacKinlay et
al. (2011). We can also use a subset of each training
corpus to simulate the common situation of sparse
training data, so we can investigate whether sense-
tagging enables the learner to make better use of a
limited quantity of training data.
3.2 Evaluation
Our primary evaluation metric is Elementary De-
pendency Match (Dridan and Oepen, 2011). This
converts the semantic output of the ERG into a set
of dependency-like triples, and scores these triples
using precision, recall and F-score as is conven-
tional for other dependency evaluation. Following
MacKinlay et al (2011), we use the EDMNA mode
of evaluation, which provides a good level of com-
parability while still reflecting most the semantically
salient information from the grammar.
Other work on the ERG and related grammars has
tended to focus on exact tree match, but the granu-
lar EDM metric is a better fit for our needs here ?
among other reasons, it is more sensitive in terms
of error rate reduction to changes in parse selection
models (MacKinlay et al, 2011). Additionally, it is
desirable to be able to choose between two different
parses which do not match the gold standard exactly
but when one of the parses is a closer match than the
other; this is not possible with exact match accuracy.
3.3 Reranking for parse selection
The features we are adding to the parse selection
procedure could all in principle be applied by the
parser during the selective unpacking stage, since
they all depend on information which can be pre-
computed. However, we wish to avoid the need for
multiple expensive parsing runs, and more impor-
tantly the need to modify the relatively complex in-
ternals of the parse ranking machinery in the PET
parser (Callmeier, 2000). So instead of performing
the parse ranking in conjunction with parsing, as is
the usual practice, we use a pre-parsed forest of the
top-500 trees for each corpus, and rerank the forest
afterwards for each configuration shown.
The pre-parsed forests use the same models which
were used in treebanking. Using reranking means
that the set of candidate trees is held constant, which
230
means that parse selection models never get the
chance to introduce a new tree which was not in
the original parse forest from which the gold tree
was annotated, which may provide a very small per-
formance boost (although when the parse selection
models are similar as is the case for most of the mod-
els here, this effect is likely to be very small).
3.4 Word Sense Annotations
3.4.1 Using the WordNet Hierarchy
Most experiments we report on here make some
use of the WordNet sense inventory. Obviously we
need to determine the best sense and corresponding
WordNet synset for a given token. We return to this
in Section 3.4.2, but for now assume that the sense
disambiguation is done.
As we are concerned primarily with making
commonalities between lemmas with different base
forms apparent to the parse selection model, the fine-
grained synset ID will do relatively little to provide
a coarser identifier for the token ? indeed, if two
tokens with identical forms were assigned different
synset IDs, we would be obscuring the similarity.1
We can of course make use of the WordNet hier-
archy, and use hypernyms from the hierarchy to tag
each candidate token, but there are a large number
of ways this can be achieved, particularly when it
is possibly to assign multiple labels per token as is
the case here (which we discuss in Section 3.4.3).
We apply two relatively simple strategies. We noted
in Section 2.2 that Agirre et al (2008) found that
the semantic file was useful. This is the coarse lex-
icographic category label, elsewhere denoted super-
sense (Ciaramita and Altun, 2006), which is the
terminology we use. Nouns are divided into 26
coarse categories such as ?animal?, ?quantity? or
?phenomenon?, and verbs into 15 categories such as
?perception? or ?consumption?. In some configura-
tions, denoted SS, we tag each open-class token with
one of the supersense labels.
Another configuration attempts to avoid making
assumptions about which level of the hierarchy will
be most useful for parse disambiguation, instead
leaving it the MaxEnt parse ranker to pick those la-
bels from the hierarchy which are most useful. Each
1This could be useful for verbs since senses interact strongly
subcategorisation frames, but that is not our focus here.
open class token is labelled with multiple synsets,
starting with the assigned leaf synset and travelling
as high as possible up the hierarchy, with no distinc-
tion made between the different levels in the hier-
archy. Configurations using this are designated HP,
for ?hypernym path?.
3.4.2 Disambiguating senses
We return now to the question of determination
of the synset for a given token. One frequently-
used and robust strategy is to lemmatise and POS-
tag each token, and assign it the first-listed sense
from WordNet (which may or may not be based on
actual frequency counts). We POS-tag using TnT
(Brants, 2000) and lemmatise using WordNet?s na-
tive lemmatiser. This yields a leaf-level synset, mak-
ing it suitable as a source of annotations for both SS
and HP. We denote this ?WNF? for ?WordNet First?
(shown in parentheses after SS or HP).
Secondly, to evaluate whether a more informed
approach to sense-tagging helps beyond the naive
WNF method, in the ?SST? method, we use the out-
puts of SuperSense Tagger (Ciaramita and Altun,
2006), which is optimised for assigning the super-
senses described above, and can outperform a WNF-
style baseline on at least some datasets. Since this
only gives us coarse supersense labels, it can only
provide SS annotations, as we do not get the leaf
synsets needed for HP. The input we feed in is POS-
tagged with TnT as above, for comparability with
the WNF method, and to ensure that it is compati-
ble with the configuration in which the corpora were
parsed ? specifically, the unknown-word handling
uses a version of the sentences tagged with TnT. We
ignore multi-token named entity outputs from Su-
perSense Tagger, as these would introduce a con-
founding factor in our experiments and also reduce
comparability of the results with the WNF method.
3.4.3 A distributional thesaurus method
A final configuration attempts to avoid the need
for curated resources such as WordNet, instead us-
ing an automatically-constructed distributional the-
saurus (Lin, 1998). We use the thesaurus from
McCarthy et al (2004), constructed along these
lines using the grammatical relations from RASP
(Briscoe and Carroll, 2002) applied to 90 millions
words of text from the British National Corpus.
231
root_frag
np_frg_c
hdn_bnp_c
aj-hdn_norm_c
legal_a1
"legal"
n_pl_olr
issue_n1
"issues"
Figure 1: ERG derivation tree for the phrase Legal issues
[n_-_c_le "issues"]
[n_pl_olr n_-_c_le "issues"]
[aj-hdn_norm_c n_pl_olr n_-_c_le "issues"]
(a) Original features
[n_-_c_le noun.cognition]
[n_pl_olr n_-_c_le noun.cognition]
[aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition]
(b) Additional features in leaf mode, which augment the original
features
[noun.cognition "issues"]
[n_pl_olr noun.cognition "issues"]
[aj-hdn_norm_c n_pl_olr noun.cognition "issues"]
(c) Additional features in leaf-parent (?P?) mode, which augment
the original features
Figure 2: Examples of features extracted from for
"issues" node in Figure 1 with grandparenting level
of 2 or less
To apply the mapping, we POS-tag the text with
TnT as usual, and for each noun, verb and adjec-
tive we lemmatise the token (with WordNet again,
falling back to the surface form if this fails), and
look up the corresponding entry in the thesaurus. If
there is a match, we select the top five most simi-
lar entries (or fewer if there are less than five), and
use these new entries to create additional features,
as well as adding a feature for the lemma itself in all
cases. This method is denoted LDT for ?Lin Distri-
butional Thesaurus?. We note that many other meth-
ods could be used to select these, such as different
numbers of synonyms, or dynamically changing the
number of synonyms based on a threshold against
the top similarity score, but this is not something we
evaluate in this preliminary investigation.
Adding Word Sense to Parse Selection Models
We noted above that parse selection using the
methodology established by Velldal (2007) uses
human-annotated incorrect and correct derivation
trees to train a maximum entropy parse selection
model. More specifically, the model is trained using
features extracted from the candidate HPSG deriva-
tion trees, using the labels of each node (which are
the rule names from the grammar) and those of a
limited number of ancestor nodes.
As an example, we examine the noun phrase Le-
gal issues from the WESCIENCE corpus, for which
the correct ERG derivation tree is shown in Figure 1.
Features are created by examining each node in the
tree and at least its parent, with the feature name set
to the concatenation of the node labels. We also gen-
erally make used of grandparenting features, where
we examine earlier ancestors in the derivation tree.
A grandparenting level of one means we would also
use the label of the grandparent (i.e. the parent?s par-
ent) of the node, a level of two means we would add
in the great-grandparent label, and so on. Our exper-
iments here use a maximum grandparenting level of
three. There is also an additional transformation ap-
plied to the tree ? the immediate parent of each leaf
is, which is usually a lexeme, is replaced with the
corresponding lexical type, which is a broader par-
ent category from the type hierarchy of the grammar,
although the details of this are not relevant here.
For the node labelled "issues" in Figure 1 with
grandparenting levels from zero to two, we would
extract the features as shown in Figure 2(a) (where
the parent node issue_n1 has already been re-
placed with its lexical type n_-c_le).
In this work here, we create variants of these fea-
tures. A preprocessing script runs over the training
or test data, and for each sentence lists variants of
each token using standoff markup indexed by char-
acter span, which are created from the set of addi-
tional semantic tags assigned to each token by the
word sense configuration (from those described in
Section 3.4) which is currently in use. These sets of
semantic tags for a given word could be a single su-
persense tag, as in SS, a set of synset IDs as in HP
or a set of replacement lemmas in LDT. In all cases,
the set of semantic tags could also be empty ? if ei-
ther the word has a part of speech which we are not
232
Test Train SS (WNF) SSp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 ?0.11
WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42
LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 ?0.21
LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 ?0.00 91.90/90.13/91.01 +0.07
WESC
WESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22
WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26
LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31
LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 ?0.11 85.89/82.76/84.30 ?0.13
Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500
sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training
data. Subscript ?p? indicates mappings were applied to leaf parents rather than leaves.
Test Train SS (SST) SSp(SST)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37
WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35
LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20
LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 ?0.10 91.47/89.96/90.71 ?0.23
WESC
WESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 ?0.04 87.32/84.48/85.88 +0.27
WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 ?0.05 89.50/86.56/88.00 ?0.05
LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23
LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 ?0.16
Table 3: Results for SS (SST) (supersense from SuperSense Tagger)
Test Train HPWNF HPp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 ?0.32 84.74/82.20/83.45 ?0.15
WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 ?0.04
LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05
LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 ?0.01 91.66/89.85/90.75 ?0.19
WESC
WESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 ?0.08 87.18/84.43/85.78 +0.18
WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 ?0.01
LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 ?0.04 83.47/81.00/82.22 ?0.33
LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 ?0.33 85.75/83.03/84.37 ?0.06
Table 4: Results for HPWNF (hypernym path from first WordNet sense)
Test Train LDTp(5)
P/ R/ F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 ?0.28
WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19
LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 ?0.28
LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 ?0.82
WESC
WESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 ?0.78
WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 ?1.12
LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 ?0.19
LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 ?0.11
Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)
233
attempting to tag semantically, or if our method has
no knowledge of the particular word.
The mapping is applied at the point of feature ex-
traction from the set of derivation trees ? at model
construction time for the training set and at rerank-
ing time for the development set. If a given leaf to-
ken has some set of corresponding semantic tags, we
add a set of variant features for each semantic tag,
duplicated and modified from the matching ?core?
features described above. There are two ways these
mappings can be applied, since it is not immedi-
ately apparent where the extra lexical generalisation
would be most useful. The ?leaf? variant applies to
the leaf node itself, so that in each feature involving
the leaf node, add a variant where the leaf node sur-
face string has been replaced with the new seman-
tic tag. The ?parent? variant, which has a subscript
?P? (e.g. SSp(WNF) ) applies the mapping to the
immediate parent of the leaf node, leaving the leaf
itself unchanged, but creating variant features with
the parent nodes replaced with the tag.
For our example here, we assume that we have
an SS mapping for Figure 2(a), and that this has
mapped the token for "issues" to the WordNet
supersense noun.cognition. For the leaf vari-
ant, the extra features that would be added (either for
considering inclusion in the model, or for scoring a
sentence when reranking) are shown in Figure 2(b),
while those for the parent variant are in Figure 2(c).
3.4.4 Evaluating the contribution of sense
annotations
Wewish to evaluate whether adding sense annota-
tions improve parser accuracy against the baseline of
training a model in the conventional way using only
syntactic features. As noted above, we suspect that
this semantic generalisation may help in cases where
appropriate training data is sparse ? that is, where
the training data is from a different domain or only
a small amount exists. So to evaluate the various
methods in these conditions, we train models from
small (23k token) training sets and large (96k token)
training sets created from subsets of each corpus
(WESCIENCE and LOGON). For the baseline, we
train these models without modification. For each
of the various methods of adding semantic tags, we
then re-use each of these training sets to create new
models after adding the appropriate additional fea-
tures as described above, to evaluate whether these
additional features improve parsing accuracy
4 Results
We present an extensive summary of the results ob-
tained using the various methods in Tables 2, 3, 4
and 5. In each case we show results for applying
to the leaf and to the parent. Aggregating the re-
sults for each method, the differences range between
substantially negative and modestly positive, with a
large number of fluctuations due to statistical noise.
LDT is the least promising performer, with only
one very modest improvement, and the largest de-
creases in performance, of around 1%. The HP-
WNF and HPp(WNF) methods make changes in
either direction ? on average, over all four train-
ing/test combinations, there are very small drops
in F-score of 0.02% for HPWNF, and 0.06% for
HPp(WNF), which indicates that neither of the
methods is likely to be useful in reliably improving
parser performance.
The SS methods are more promising. SS (WNF)
and SSp(WNF) methods yield an average im-
provement of 0.10% each, while SS (SST) and
SSp(SST) give average improvements of 0.12%
and 0.13% respectively (representing an error rate
reduction of around 1%). Interestingly, the increase
in tagging accuracy we might expect using Super-
Sense Tagger only translates to a modest (and prob-
ably not significant) increase in parser performance,
possibly because the tagger is not optimised for the
domains in question. Amongst the statistical noise
it is hard to discern overall trends; surprisingly, it
seems that the size of the training corpus has rela-
tively little to do with the success of adding these su-
persense annotations, and that the corpus being from
an unmatched domain doesn?t necessarily mean that
sense-tagging will improve accuracy either. There
may be a slight trend for sense annotations to be
more useful when WESCIENCE is the training cor-
pus (either in the small or the large size).
To gain a better insight into how the effects
change as the size of the training corpus changes for
the different domains, we created learning curves for
the best-performing method, SSp(SST) (although
as noted above, all SS methods give similar levels
of improvement), shown in Figure 3. Overall, these
234
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on LOGON Corpus
Test Corpus
LOGON*
LOGON* +SS
WeSc
WeSc +SS
(a) LOGON
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on WeScience Corpus
Test Corpus
LOGON
LOGON +SS
WeSc*
WeSc* +SS
(b) WESCIENCE
Figure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger). ?*? denotes in-domain
training corpus.
graphs support the same conclusions as the tables
? the gains we see are very modest and there is a
slight tendency for WESCIENCE models to benefit
more from the semantic generalisation, but no strong
tendencies for this to work better for cross-domain
training data or small training sets.
5 Conclusion
We have presented an initial study evaluat-
ing whether a fairly simple approach to using
automatically-created coarse semantic annotations
can improve HPSG parse selection accuracy using
the English Resource Grammar. We have provided
some weak evidence that adding features based on
semantic annotations, and in particular word super-
sense, can provide modest improvements in parse
selection performance in terms of dependency F-
score, with the best-performing method SSp(SST)
providing an average reduction in error rate over 4
training/test corpus combinations of 1%. Other ap-
proaches were less promising. In all configurations,
there were instances of F-score decreases, some-
times substantial.
It is somewhat surprising that we did not achieve
reliable performance gains which were seen in the
related work described above. One possible expla-
nation is that the model training parameters were
suboptimal for this data set since the characteris-
tics of the data are somewhat different than with-
out sense annotations. The failure to improve some-
what mirrors the results of Clark (2001), who was at-
tempting to improve the parse ranking performance
of the unification-based based probabilistic parser of
Carroll and Briscoe (1996). Clark (2001) used de-
pendencies to rank parses, and WordNet-based tech-
niques to generalise this model and learn selectional
preferences, but failed to improve performance over
the structural (i.e. non-dependency) ranking in the
original parser. Additionally, perhaps the changes
we applied in this work to the parse ranking could
possibly have been more effective with features
based on semantic dependences as used by Fujita
et al (2007), although we outlined reasons why we
wished to avoid this approach.
This work is preliminary and there is room for
more exploration in this space. There is scope for
much more feature engineering on the semantic an-
notations, such as using different levels of the se-
mantic hierarchy, or replacing the purely lexical fea-
tures instead of augmenting them. Additionally,
more error analysis would reveal whether this ap-
proach was more useful for avoiding certain kinds
of parser errors (such as PP-attachment).
Acknowledgements
NICTA is funded by the Australian Government as
represented by the Department of Broadband, Com-
munications and the Digital Economy and the Aus-
tralian Research Council through the ICT Centre of
Excellence program.
235
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL-08: HLT, pages
317?325, Columbus, Ohio, June.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing
with semantic classes. In Proceedings of the 49th An-
nual Meeting of the Association of Computational Lin-
guistics, ACL-HLT 2011 Short Paper, Portland, Ore-
gon?.
D. M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of the second international conference on Human
Language Technology Research, pages 178?182, San
Francisco, CA, USA.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Ap-
plied Natural Language Processing, pages 224?231,
Seattle, Washington, USA, April.
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504.
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing techniques. Nat.
Lang. Eng., 6(1):99?107.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr pars- ing system
through evaluation. In Proceedings of the SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92?100, Philadelphia, PA.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 132?139.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia,
July.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111.
S. Clark. 2001. Class-based Statistical Models for Lex-
ical Knowledge Acquisition. Ph.D. thesis, University
of Sussex.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain, July.
R. Dridan and S. Oepen. 2011. Parser evaluation us-
ing elementary dependency matching. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies, pages 225?230, Dublin, Ireland, October.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Nat. Lang. Eng., 6
(1):15?28.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for HPSG parse selec-
tion. In ACL 2007 Workshop on Deep Linguistic Pro-
cessing, pages 25?32, Prague, Czech Republic, June.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2,
pages 768?774.
A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.
2011. Cross-domain effects on parse selection for pre-
cision grammars. Research on Language & Computa-
tion, 8(4):299?340.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
the Sixth Conference on Natural Language Learning
(CoNLL-2002), pages 49?55.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
the penn treebank. Comput. Linguist., 19(2):313?330.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 279?es.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods: A rich and dynamic
treebank for HPSG. Research on Language & Com-
putation, 2(4):575?596.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of japanese. In Proceedings of the 3rd work-
shop on Asian language resources and international
standardization-Volume 12, pages 1?8.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo Department of Informatics.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and annotating Wikipedia sub-domains ? to-
wards a new eScience community resourc. In Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories, Groeningen, The
Netherlands, January.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in
unification-based n-best parsing. In IWPT ?07: Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 48?59, Morristown, NJ, USA.
236
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 133?137, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Umelb: Cross-lingual Textual Entailment with Word Alignment and String
Similarity Features
Yvette Graham Bahar Salehi Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
{ygraham,bsalehi,tbaldwin}@unimelb.edu.au
Abstract
This paper describes The University of Mel-
bourne NLP group submission to the Cross-
lingual Textual Entailment shared task, our
first tentative attempt at the task. The ap-
proach involves using parallel corpora and au-
tomatic word alignment to align text fragment
pairs, and statistics based on unaligned words
as features to classify items as forward and
backward before a compositional combination
into the final four classes, as well as exper-
iments with additional string similarity fea-
tures.
1 Introduction
Cross-lingual Textual Entailment (CLTE) (Negri et
al., 2012) proposes the task of automatically iden-
tifying the kind of relation that exists between pairs
of semantically-related text fragments written in two
distinct languages, a variant of the traditional Rec-
ognizing Textual Entailment (RTE) task (Bentivogli
et al, 2009; Bentivogli et al, 2010). The task tar-
gets the cross-lingual content synchronization sce-
nario proposed in Mehdad et al (2010, 2011). Com-
positional classification can be used by training two
distinct binary classifiers for forward and backward
entailment classification, before combining labels
into the four final entailment categories that now in-
clude bidirectional and no entailment labels. The
most similar previous work to this work is the cross-
lingual approach of the FBK system (Mehdad et
al., 2012) from Semeval 2012 (Negri et al, 2012),
in which the entailment classification is obtained
without translating T1 into T2 for the Spanish?
English language pair. We apply the cross-lingual
approach to German?English and instead of cross-
lingual matching features, we use Giza++ (Och et
al., 1999) and Moses (Koehn et al, 2007) to auto-
matically word align text fragment pairs to compute
statistics of unaligned words. In addition, we in-
clude some additional experiments using string sim-
ilarity features.
2 Compositional Classification
Given a pair of topically related fragments, T1 (Ger-
man) and T2 (English), we automatically annotate it
with one of the following entailment labels: bidi-
rectional, forward, backward, no entailment. We
take the compositional approach and separately train
a forward, as well as a backward binary classifier.
Each classifier is run separately on the set of text
fragment pairs to produce two binary labels for for-
ward and backward entailment. The two sets of la-
bels are logically combined to produce a final clas-
sification for each test pair of forward, backward,
bidirectional or no entailment.
3 Word Alignment Features
The test set of topically-related text fragments, T1
(German) and T2 (English) were added to Europarl
German?English parallel text (Koehn, 2005) and
Giza++ was used for automatic word alignment in
both language directions. Moses (Koehn et al,
2007) was then used for symmetrization with the
grow diag final and algorithm. This produces a
many-to-many alignment between the words of the
133
German, T1, and English, T2, with words also re-
maining unaligned.
The following features are computed for each test
pair feature scores for the forward classifier:
? A1: count of unaligned words in T2
? A2: count of words comprised soley of digits
in T2 not in T1
? A3: count of unaligned words in T2 with low
probability of appearing unaligned in Europarl
(with threshold p=0.11)
The number of words in T2 (English) that are not
aligned with anything in T1 (German) should pro-
vide an indication that, for example, the English text
fragment contains information not present in the cor-
responding German text fragment and subsequently
evidence against the presence of forward entailment.
We there include the feature, A1, that is simply a
count of unaligned words in English T2. In addi-
tion, we hypothesize that the absence of a number
from T2 may be a more significant missing element
of T2 from T1. We therefore include as a feature
the count of tokens comprised of digits in T2 that
are not also present in T1. The final word align-
ment feature attempts to refine A1, by distinguishing
words that are rarely unaligned in German?English
translations. Statistics are computed for every lexi-
cal item from German?English Europarl translations
to produce a lexical unalignment probability, com-
puted for each lexical item based on its relative fre-
quency in the corpus when it is not aligned to any
other word.
The backward classifier uses the same features but
computed for each test pair on counts of unaligned
T1 words.
4 Results
Results for several combinations of features are
shown in Table 1 when the system is trained on
the 500-pair development set training corpus and
tested on the 500-pair held-out development test set
(DEV), in addition to results for feature combina-
tions when trained on the entire 1000-pair develop-
ment data and tested on the held-out 500-pair gold
standard (TEST) (Negri et al, 2011), when the sys-
tem is evaluated as two separate binary forward and
backward classifiers (2-CLASS) as well as the final
evaluation including all four entailment classes (4-
CLASS). The highest accuracy is achieved by the
classifier using the single feature of counts of un-
aligned words, A1, of 34.6%. As two separate bi-
nary classifiers, the alignment features, A1+A2+A3,
achieve a relatively high accuracy of 74.0% for for-
ward with somewhat less accurate for backward
(65.8%) classification (both over the DEV data).
When combined to the final four CLTE classes, how-
ever, accuracy drops significantly to an overall accu-
racy of 50% (also over DEV). A main cause is inac-
curate labeling of no entailment gold standard test
pairs, as the most severe decline is for recall of test
pairs for this label (38.4%).
Accuracy on the development set for the word
alignment features, A1+A2+A3, compared to the
test set shows a sever decline, from 50% to 32%. On
the test data, however, a main cause of inaccuracy
is that backward gold standard test pairs, although
achieving close accuracy to forward when evaluated
as binary classifiers, are inaccurately labeled in the
4-class evaluation, as recall for backward drops to
only 18.4% for this label.
Another insight revealed for the alignment fea-
tures, A1+A2+A3, in the 4-class evaluation is that
when run on the development set, the classes for-
ward and backward achieve significantly higher
f-scores compared to no entailment. However,
the contrary is observed for the test data, as
no entailment achieve higher results than both uni-
directional classes. This appears at first to be a
somewhat counter-intuitive result, but in this case,
the system is simply better at predicting forward and
backward when no entailment exists for a translation
pair compared to when a unidirectional entailment is
present.
4.1 String Similarity Features
In addition to the word alignment features, subse-
quent to submitting results to the shared task, we
have carried out additional experiments using string
similarity features, based on our recent success in
apply string similarity to both the estimation of com-
positionality of MWEs (Salehi and Cook, to appear)
and also the estimation of similarity between short
134
2-CLASS 4-CLASS
Acc. Prec Recall F1 Acc. Prec Recall F1
D
E
V
A1 + A2 + A3
bwrd 65.80 63.12 76.00 68.96 50.00 bwrd 54.80 59.20 56.90
fwrd 74.00 72.22 78.00 75.00 fwrd 54.80 45.60 49.80
none 50.50 38.40 43.60
bidir 42.80 56.80 48.80
S1 + S2 + S3
bwrd 58.20 57.75 61.20 59.42 27.40 bwrd 14.30 0.80 1.50
fwrd 47.00 47.17 50.00 59.42 fwrd 0.00 0.00 0.00
none 30.70 39.70 39.70
bidir 25.60 52.80 34.50
T
E
S
T
A1
bwrd 57.00 58.54 48.00 52.75 34.60 bwrd 25.50 19.20 21.90
fwrd 58.40 58.75 56.40 57.55 fwrd 34.90 36.00 35.40
none 36.70 48.80 41.90
bidir 38.70 34.40 36.40
A2
bwrd 50.00 0.00 0.00 0.00 33.60 bwrd 24.70 18.40 21.10
fwrd 51.60 50.85 95.20 66.29 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A3
bwrd 54.80 55.61 47.60 51.29 34.20 bwrd 32.70 26.40 29.20
fwrd 61.20 61.57 59.60 60.57 fwrd 33.30 34.40 33.90
none 36.90 46.40 41.10
bidir 32.70 29.60 31.10
A1+A2
bwrd 57.60 57.72 56.80 57.26 33.60 bwrd 24.70 18.40 21.10
fwrd 59.80 58.84 65.20 61.86 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A1+A3
bwrd 57.20 57.96 52.40 55.04 33.00 bwrd 26.60 20.00 22.80
fwrd 58.60 58.05 62.00 59.96 fwrd 31.90 34.40 33.10
none 36.70 40.80 38.60
bidir 34.80 36.80 35.80
A2+A3
bwrd 54.80 55.83 46.00 50.44 33.40 bwrd 32.30 25.60 28.60
fwrd 61.00 61.70 58.00 59.79 fwrd 32.80 33.60 33.20
none 34.90 46.40 39.90
bidir 32.70 28.00 30.20
A1 + A2 + A3
bwrd 57.60 57.72 56.80 57.26 32.00 bwrd 24.00 18.40 20.80
fwrd 59.20 58.39 64.00 61.07 fwrd 32.30 32.00 32.10
none 36.20 37.60 36.90
bidir 34.70 41.60 37.80
S1 + S2 + S3
bwrd 53.20 53.77 45.60 49.35 26.00 bwrd 20.00 1.50 29.50
fwrd 48.60 48.36 41.20 44.49 fwrd 16.70 0.80 31.50
none 28.00 63.20 38.80
bidir 23.70 39.20 29.50
A1 + A2 + A3 + S1
bwrd 57.40 58.30 52.00 54.97 33.00 bwrd 27.60 19.20 22.60
fwrd 59.80 58.84 65.20 61.86 fwrd 29.80 33.60 31.60
none 38.20 41.60 39.80
bidir 34.60 37.60 36.00
A1 + A2 + A3 + S2
bwrd 57.80 58.52 53.60 55.95 32.60 bwrd 26.70 19.20 22.30
fwrd 59.60 58.70 64.80 61.60 fwrd 30.70 33.60 32.10
none 37.30 40.00 38.60
bidir 33.80 37.60 35.60
A1 + A2 + A3 +S3
bwrd 58.20 58.51 56.40 57.44 32.80 bwrd 24.70 19.20 21.60
fwrd 59.60 58.82 64.00 61.30 fwrd 32.00 32.80 32.40
none 37.40 39.20 38.30
bidir 34.70 40.00 37.20
Table 1: Cross-lingual Textual Entailment Results for Word alignment Features and String Similarity Measures, A1
= count of unaligned words in T2, A2 = count of unaligned numbers in T2, A3 = count of unaligned words in T2
with unaligned probability < 0.11, S1 = Number of matched words in the aligned sequence given by Smith-Waterman
algorithm, S2 = Penalty of aligning sentences using Smith-Waterman algorithm, S3 = Levenshtein distance between
the sentences
135
texts in the *SEM 2013 Shared Task (Gella et al,
to appear). Using the alignments, we replace each
English word with its corresponding word in Ger-
man. The resulting German sentence is compared
with the actual one using string similarity measures.
As the structure of both English and German sen-
tences are usually SVO, we hypothesize that when
there is no entailment between the two given sen-
tences, the newly-made German sentence and the
original German sentence will differ a lot in word
order.
In order to compare the two German sentences,
we use the Levenshtein (Levenshtein, 1966) and the
Smith-Waterman (Smith and Waterman, 1981) al-
gorithm. The Levenshtein algorithm measures the
number of world-level edits to change one sentence
into another. The edit operators consist of insertion
and deletion. We consider substitution as two edits
(combination of insertion and deletion) based on the
findings of Baldwin (2009).
We also use Smith-Waterman (SW) algorithm,
which was originally developed to find the most sim-
ilar region between two proteins. The algorithm
looks for the longest common substring, except that
it permits small numbers of penalized editions con-
sisting of insertion, deletion and substitution. We
call the best found substring the ?SW aligned se-
quence?. In this experiment, we consider the number
of matched words and the number of penalties in the
SW aligned sequence as features.
Results for the string similarity features are shown
in Table 1. Since the string similarity feature scores
do not take the entailment direction into account,
i.e. there is a single set of feature scores for each
text fragment pair as there is no distinction between
forward and backward entailment, and they are not
suited for standalone use in compositional classifica-
tion. We do, however, include these scores in Table
1 to illustrate how with the compositional approach
using the same set of features for forward and back-
ward ultimately results in a classification of test pairs
as either bidirectional or no entailment.
When individual string similarity features are
added to the word alignment features, minor gains in
accuracy are achieved over the word alignment fea-
tures alone, +1% for S1, +0.6% for S2 and +0.8%
for S3 (= Levenstein).
5 Possible Additions: Dictionary Features
We hypothesize that when there is no entailment be-
tween the two sentences, the aligner may not accu-
rately align words. An on-line dictionary contain-
ing lemmatized words, such as Panlex (Baldwin and
Colowick, 2010), could be used to avoid errors in
such cases. Dictionary-based feature scores based
on the presence or absence of alignments in the dic-
tionary could then be applied.
6 Conclusions
This paper describes a compositional cross-lingual
approach to CLTE with experiments carried out
for the German-English language pair. Our results
showed that in the first stages of binary classification
as forward and backward, the word alignment fea-
tures alone achieved good accuracy but when com-
bined suffer severely. Accuracy of the approach
using word alignment features could benefit from
a more directional multi-class classification as op-
posed to the compositional approach we used. In
addition, results showed minor increases in accuracy
can be achieved using string similarity measures.
Acknowledgments
This work was supported by the Australian Research
Council.
References
Timothy Baldwin and Jonathan Pool Susan M. Colowick.
2010. Panlex and lextract: Translating all words of all
languages of the world. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 37?40.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In TAC 2009 Workshop
Proceedings, Gaithersburg, MD.
L. Bentivogli, P. Clark, I. Dagan, H. T. Dang, and D. Gi-
ampiccolo. 2010. The sixth PASCAL recognizing
textual entailment challenge. In TAC 2010 Workshop
Proceedings, Gaithersburg, MD.
Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser,
Paul Cook, and Timothy Baldwin. to appear. Integrat-
ing predictions from multiple domains and feature sets
136
for estimating semantic textual similarity. In Proceed-
ings of *SEM 2013 Shared Task STS.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan HerbstHieu Hoang. 2007. Moses:
Open Source Toolkit for Statistical Machine Transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), demonstration session,
Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand.
Vladimir I Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
cross-lingual textual entailment. In Proceedings of
NAACL-HLT.
Y. Mehdad, M. Negri, and M. Federico. 2011. Using par-
allel corpora for cross-lingual textual entailment. In
Proceedings of ACL-HLT 2011.
Yashar Mehdad, Matteo Negri, and Jose G. C. de Souza.
2012. Fbk: Cross-lingual textual entailment with-out
translation. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval2012).
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entailment
for content synchronization. In First Joint Conference
on Lexical and Computational Semantics, pages 399?
407, Montreal, Canada.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20?28, College Park, MD.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings of
the Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM 2013).
Temple F Smith and Michael S Waterman. 1981. The
identification of common molecular subsequences.
Journal of Molecular Biology, 147:195?197.
137
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction for Web Snippet
Clustering
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for Task 11
of SemEval-2013. In the task, participants
are provided with a set of ambiguous search
queries and the snippets returned by a search
engine, and are asked to associate senses with
the snippets. The snippets are then clus-
tered using the sense assignments and sys-
tems are evaluated based on the quality of the
snippet clusters. Our system adopts a pre-
existing Word Sense Induction (WSI) method-
ology based on Hierarchical Dirichlet Process
(HDP), a non-parametric topic model. Our
system is trained over extracts from the full
text of English Wikipedia, and is shown to per-
form well in the shared task.
1 Introduction
The basic premise behind research on word sense
disambiguation (WSD) is that there exists a static,
discrete set of word senses that can be used to la-
bel distinct usages of a given word (Agirre and Ed-
monds, 2006; Navigli, 2009). There are various pit-
falls underlying this premise, including: (1) what
sense inventory is appropriate for a particular task
(given that sense inventories can vary considerably
in their granularity and partitioning of word usages)?
(2) given that word senses tend to take the form of
prototypes, is discrete labelling a felicitous represen-
tation of word usages, especially for non-standard
word usages? (3) how should novel word usages be
captured under this model? and (4) given the rapid
pace of language evolution on real-time social me-
dia such as Twitter and Facebook, is it reasonable
to assume a static sense inventory? Given this back-
drop, there has been a recent growth of interest in the
task of word sense induction (WSI), where the word
sense representation for a given word is automati-
cally inferred from a given data source, and word
usages are labelled (often probabilistically) accord-
ing to that data source. While WSI has considerable
appeal as a task, intrinsic cross-comparison of WSI
systems is fraught with many of the same issues as
WSD (Agirre and Soroa, 2007; Manandhar et al,
2010), leading to a move towards task-based WSI
evaluation, such as in Task 11 of SemEval-2013, ti-
tled ?Evaluating Word Sense Induction & Disam-
biguation within an End-User Application?.
This paper presents the UNIMELB system entry to
SemEval-2013 Task 11. Our method is based heav-
ily on the WSI methodology proposed by Lau et
al. (2012) for novel word sense detection. Largely
the same methodology was also applied to SemEval-
2013 Task 13 on WSI (Lau et al, to appear).
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012) for the task of novel word
sense detection. The core machinery of our sys-
tem is driven by a Latent Dirichlet Allocation (LDA)
topic model (Blei et al, 2003). In LDA, the model
learns latent topics for a collection of documents,
and associates these latent topics with every docu-
ment in the collection. A topic is represented by
a multinomial distribution of words, and the asso-
ciation of topics with documents is represented by a
multinomial distribution of topics, with one distribu-
tion per document. The generative process of LDA
217
for drawing word w in document d is as follows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
The number of topics, T , is a parameter in LDA,
and the model tends to be highly sensitive to this set-
ting. To remove the need for parameter tuning over
development data, we make use of a non-parametric
variant of LDA, in the form of a Hierarchical Dirich-
let Process (HDP: Teh et al (2006)). HDP learns the
number of topics based on data, and the concentra-
tion parameters ? and ?0 control the variability of
topics in the documents (for details of HDP please
refer to the original paper, Teh et al (2006)).
To apply HDP in the context of WSI, the latent
topics are interpreted as the word senses, and the
documents are usages that contain the target word of
interest (or search query in the case of Task 11). That
is, given a search query (e.g. Prince of Persia), a
?document? in our application is a sentence/snippet
containing the target word. In addition to the bag of
words surrounding the target word, we also include
positional context word information, as used in the
original methodology of Lau et al (2012). That is,
we introduce an additional word feature for each of
the three words to the left and right of the target
word. An example of the topic model features for
a context sentence is given in Table 1.
2.1 Background Corpus and Preprocessing
As part of the task setup, we were provided with
snippets for each search query, constituting the doc-
uments for the topic model for that query (each
search query is topic-modelled separately). Our sys-
tem uses only the text of the snippets as features, and
ignores the URL information. The text of the snip-
pets is tokenised and lemmatised using OpenNLP
and Morpha (Minnen et al, 2001).
As there are only 64 snippets for each query in
the test dataset, which is very small by topic mod-
elling standards, we turn to English Wikipedia to
expand the data, by extracting all context sentences
that contain the search query in the full collection
of Wikipedia articles.1 Each extracted usage is a
three-sentence context containing the search query:
the original sentence that contains the actual usage
and its preceding and succeeding sentences. The
extraction of usages from Wikipedia significantly
increases the amount of information for the topic
model to learn the senses for the search queries. To
give an estimate: for very ambiguous queries such
as queen we extracted almost 150,000 usages from
Wikipedia; for most queries, however, this number
tends to be a few thousand usages.
To summarise, for each search query we apply the
HDP model to the combined collection of the 64
snippets and the extracted usages from Wikipedia.
The topic model learns the senses/topics for all
documents in the collection, but we only use the
sense/topic distribution for the 64 snippets as they
are the documents that are evaluated in the shared
task.
Our English Wikipedia collection is tokenised and
lemmatised using OpenNLP and Morpha (Minnen et
al., 2001). The search queries provided in the task,
however, are not lemmatised. Two approaches are
used to extract the usages of search queries from
Wikipedia:
HDP-CLUSTERS-LEMMA Search queries are lem-
matised using Morpha (Minnen et al, 2001),
and both the original and lemmatised forms are
used for extraction;2
HDP-CLUSTERS-NOLEMMA Search queries are
not lemmatised and only their original forms
are used for extraction.
1The Wikipedia dump was retrieved on November 28th
2009.
2Morpha requires the part-of-speech (POS) of a given word,
which is determined by the majority POS aggregated over all of
that word?s occurrences in Wikipedia.
218
Search query dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of topic model features.
System F1 ARI RI JI
Avg. No. of Avg. Cluster
Clusters Size
HDP-CLUSTERS-LEMMA 0.6830 0.2131 0.6522 0.3302 6.6300 11.0756
HDP-CLUSTERS-NOLEMMA 0.6803 0.2149 0.6486 0.3375 6.5400 11.6803
TASK11.DULUTH.SYS1.PK2 0.5683 0.0574 0.5218 0.3179 2.5300 26.4533
TASK11.DULUTH.SYS7.PK2 0.5878 0.0678 0.5204 0.3103 3.0100 25.1596
TASK11.DULUTH.SYS9.PK2 0.5702 0.0259 0.5463 0.2224 3.3200 19.8400
TASK11-SATTY-APPROACH1 0.6709 0.0719 0.5955 0.1505 9.9000 6.4631
TASK11-UKP-WSI-WACKY-LLR 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434
TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702
TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098
RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441
SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000
ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000
GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630
Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON
and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.3 For each
search query, we apply HDP to induce the senses,
and a distribution of senses is produced for each
?document? in the model. As the snippets in the test
dataset correspond to the documents in the model
and evaluation is based on ?hard? clusters of snip-
pets, we assign a sense to each snippet based on the
sense (= topic) which has the highest probability for
that snippet.
The task requires participants to produce a ranked
list of snippets for each induced sense, based on the
relative fit between the snippet and the sense. We in-
duce the ranking based on the sense probabilities as-
signed to the senses, such that snippets that have the
highest probability of the induced sense are ranked
highest, and snippets with lower sense probabilities
3Our implementation can be accessed via https://
github.com/jhlau/hdp-wsi.
are ranked lower.
Two classes of evaluation are used in the shared
task:
1. cluster quality measures: Jaccard Index (JI),
RandIndex (RI), Adjusted RandIndex (ARI)
and F1;
2. diversification of search results: Subtopic Re-
call@K and Subtopic Precision@r.
Details of the evaluation measures are described in
Navigli and Vannella (2013).
The idea behind the second form of evaluation
(i.e. diversification of search results) is that search
engine results should cluster the results based on
senses (of the query term in the documents) given an
ambiguous query. For example, if a user searches for
apple, the search engine may return results related to
both the computer brand sense and the fruit sense of
apple. Given this assumption, the best WSI/WSD
system is the one that can correctly identify the di-
versity of senses in the snippets.
219
Figure 1: Subtopic Recall@K for all participating systems.
Cluster quality, subtopic recall@K and subtopic
precision@r results for all systems entered in the
task are presented in Table 2, Figure 1 and Figure 2,
respectively.
In terms of cluster quality, our systems
(HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA) consistently outperform the other teams
for all measures except for the Jaccard Index (where
we rank second and third, by a narrow margin). The
average number of induced clusters and the average
cluster size of our systems are similar to those
of the gold standard system (GOLD), indicating
that our systems are learning an appropriate sense
granularity.
In terms of diversification of search results, our
systems perform markedly better than most teams,
other than RAKESH which trails closely behind our
systems (despite a relatively low ranking in terms of
the cluster quality evaluation). Overall, the results
are encouraging and our system performs very well
over the task.
4 Discussion and Conclusion
Our system adopts the WSI system proposed in Lau
et al (2012) with no parameters tuned for this task,
and performs very well over it. Parameter tuning and
exploiting URL information in the snippets could
potentially boost the system performance further.
Other background corpora (such as news articles)
could also be used to increase the size of the training
data. We leave these ideas for future work.
Inspecting the difference between the HDP-
CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA approaches, only 6 out of the 100
lemmas have a lemmatised form which differs from
the original query composition: pods (pod), ten
commandments (ten commandment), guild wars
(guild war), stand by me (stand by i), sisters of
mercy (sister of mercy) and lord of the flies (lord of
the fly). In most cases, including the lemmatised
query results in the extraction of additional useful
usages, e.g. using only the original form lord of
the flies would extract no usages from Wikipedia
(because this corpus has itself been lemmatised).
In other cases, however, including the lemmatised
forms results in many common noun usages, e.g.
the number of usages of the lemmatised pod is
significantly greater than that of the original form
pods (which corresponds to proper noun usages in
the lemmatised corpus), resulting in senses being
induced only for common noun usages of pods. The
220
Figure 2: Subtopic Precision@r for all participating systems.
advantages and disadvantages of both approaches
are reflected in the results: performance is mixed
and no one method clearly outperforms the other.
To conclude, we apply a topic model-based WSI
methodology to the task of web result clustering, us-
ing English Wikipedia as an external resource for ex-
tracting additional usages. Our system is completely
unsupervised and requires no annotated resources,
and appears to perform very well on the task.
References
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer, Dordrecht, Netherlands.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proc. of the 4th International Work-
shop on Semantic Evaluations, pages 7?12, Prague,
Czech Republic.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction. In Proc. of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 task 11: Evaluating word sense induction & dis-
ambiguation within an end-user application. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
221
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 307?311, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for shared
task 13 ?Word Sense Induction for Graded and
Non-Graded Senses? of SemEval-2013. The
task is on word sense induction (WSI), and
builds on earlier SemEval WSI tasks in ex-
ploring the possibility of multiple senses be-
ing compatible to varying degrees with a sin-
gle contextual instance: participants are asked
to grade senses rather than selecting a sin-
gle sense like most word sense disambigua-
tion (WSD) settings. The evaluation measures
are designed to assess how well a system per-
ceives the different senses in a contextual in-
stance. We adopt a previously-proposed WSI
methodology for the task, which is based on a
Hierarchical Dirichlet Process (HDP), a non-
parametric topic model. Our system requires
no parameter tuning, uses the English ukWaC
as an external resource, and achieves encour-
aging results over the shared task.
1 Introduction
In our previous work (Lau et al, 2012) we devel-
oped a word-sense induction (WSI) system based on
topic modelling, specifically a Hierarchical Dirich-
let Process (Teh et al, 2006). In evaluations over
the SemEval-2007 and SemEval-2010 WSI tasks we
achieved performance on par with the current state-
of-the art. The SemEval-2007 and SemEval-2010
WSI tasks assumed that each usage of a word has
a single gold-standard sense. In this paper we apply
this WSI method ?off-the-shelf?, with no adaptation,
to the novel SemEval-2013 task of ?Word Sense In-
duction for Graded and Non-Graded Senses?. Given
that the topic model allocates a multinomial distri-
bution over topics to each word usage (?document?,
in topic modelling terms), the SemEval-2013 WSI
task is an ideal means for evaluating this aspect of
the topic model.
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012), and also applied to
SemEval-2013 Task 11 on WSI for web snippet
clustering (Lau et al, to appear). The core machin-
ery of our system is driven by a Latent Dirichlet Al-
location (LDA) topic model (Blei et al, 2003). In
LDA, the model learns latent topics for a collection
of documents, and associates these latent topics with
every document in the collection. A topic is repre-
sented by a multinomial distribution of words, and
the association of topics with documents is repre-
sented by a multinomial distribution of topics, a dis-
tribution for each document. The generative process
of LDA for drawing word w in document d is as fol-
lows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
307
The number of topics, T , is a parameter in LDA.
We relax this assumption by extending the model
to be non-parametric, using a Hierarchical Dirichlet
Process (HDP: (Teh et al, 2006)). HDP learns the
number of topics based on data, with the concentra-
tion parameters ? and ?0 controlling the variability
of topics in the documents (for details of HDP please
refer to the original paper).
To apply HDP to WSI, the latent topics are in-
terpreted as the word senses, and the documents are
usages that contain the target word of interest. That
is, given a target word (e.g. paper), a ?document?
in our application is a sentence context surround-
ing the target word. In addition to the bag of words
surrounding the target word, we also include posi-
tional context word information, which was used in
our earlier work (Lau et al, 2012). That is, we in-
troduce an additional word feature for each of the
three words to the left and right of the target word.
An example of the topic model features is given in
Table 1.
2.1 Background Corpus and Preprocessing
The test dataset provides us with contextual in-
stances for each target word, and these instances
constitute the documents for the topic model. The
text of the test data is tokenised and lemmatised us-
ing OpenNLP and Morpha (Minnen et al, 2001).
Note, however, that there are only 100 instances
for most target words in the test dataset, and as such
the dataset may be too small for the topic model
to induce meaningful senses. To this end, we turn
to the English ukWaC ? a web corpus of approxi-
mately 1.9 billion tokens ? to expand the data, by
extracting context sentences that contain the target
word. Each extracted usage is a three-sentence con-
text containing the target word: the original sentence
that contains the actual usage and its preceding and
succeeding sentences. The extraction of usages from
the ukWaC significantly increases the amount of in-
formation for the topic model to learn the senses for
the target words from. However, HDP is compu-
tationally intensive, so we limit the number of ex-
tracted usages from the ukWaC using two sampling
approaches:
UNIMELB (5P) Take a 5% random sample of us-
ages;
UNIMELB (50K) Limit the maximum number of
randomly-sampled usages to 50,000 instances.
The usages from the ukWaC are tokenised and
lemmatised using TreeTagger (Schmid, 1994), as
provided by the corpus.
To summarise, for each target word we apply
the HDP model to the combined collection of the
test instances (provided by the shared task) and
the extracted usages from the English ukWaC (not-
ing that each instance/usage corresponds to a topic
model ?document?). The topic model learns the
senses/topics for all documents in the collection, but
we only use the sense/topic distribution for the test
instances as they are the ones evaluated in the shared
task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.1 For each
target word, we apply HDP to induce the senses, and
a distribution of senses is produced for each ?docu-
ment? in the model. To grade the senses for the in-
stances in the test dataset, we apply the sense proba-
bilities learnt by the topic model as the sense weights
without any modification.
To illustrate the senses induced by our model, we
present the top-10 words of the induced senses for
the verb strike in Table 2. Although 13 senses in
total are induced and some of them do not seem very
coherent, only the first 8 senses ? the more coherent
ones ? are observed (i.e., have non-zero probability
for any usage) in the test dataset.
Two forms of evaluation are used in the task:
WSD evaluation and clustering comparison. For
WSD evaluation, three measures are used: (1)
Jaccard Index (JI), which measures the degree of
overlap between the induced senses and the gold
senses; (2) positionally-weighted Kendall?s tau (KT:
(Kumar and Vassilvitskii, 2010)), which measures
the correlation between the ranking of the induced
senses and that of the gold senses; and (3) nor-
malised discounted cumulative gain (NDCG), which
1These settings were considered ?vague? priors in Teh et
al. (2006). They were tested in Lau et al (2012) and the
model was shown to be robust under different parameter set-
tings. As such we decided to keep the settings. The imple-
mentation of our WSI system can be accessed via GitHub:
https://github.com/jhlau/hdp-wsi.
308
Target word dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of the topic model features.
Sense Num Top-10 Terms
1 strike @card@ worker union war iraq week pay government action
2 strike hand god head n?t look face fall leave blow
3 strike @card@ balance court company case need balance #1 order claim
4 strike ball @card@ minute game goal play player shot half
5 strike @card@ people fire disaster area road car ship lightning
6 @card@ strike new news post deal april home business week
7 strike n?t people thing think way life book find new
8 @card@ strike coin die john church police age house william
9 div ukl syn color hunter text-decoration australian verb condom font-size
10 invent rocamadour cost mp3 terminal total wav honor omen node
11 training run rush kata performance marathon exercise technique workout interval
12 wrong qha september/2000 sayd ? hawksmoor thyna pan salt common
13 zidane offering stone blow zidane #-1 type type #2 zidane #1 blow #3 materials
Table 2: The top-10 terms for each of the senses induced for the verb strike by the HDP model.
measures the correlation between the weights of
the induced senses and that of the gold senses.
For clustering comparison, fuzzy normalised mu-
tual information (FNMI) and fuzzy b-cubed (FBC)
are used. Note that the WSD systems participat-
ing in this shared task are not evaluated with clus-
tering comparison metrics, as they do not induce
senses/clusters in the same manner as WSI systems.
WSI systems produce senses that are different to
the gold standard sense inventory (WordNet 3.1),
and the induced senses are mapped to the gold stan-
dard senses using the 80/20 validation setting. De-
tails of this mapping procedure are described in Jur-
gens (2012).
Results for all test instances are presented in Ta-
ble 3. Note that many baselines are used, only some
of which we present in this paper, namely: (1) RAN-
DOM ? label instances with one of three random in-
duced senses; (2) SEMCOR MFS ? label instances
with the most frequently occurring sense in Semcor;
(3) TEST MFS ? label instances with the most fre-
quently occurring sense in the test dataset. To bench-
mark our method, we present one or two of the best
systems from each team.
Looking at Table 3, our system performs encour-
agingly well. Although not the best system, we
achieve results close to the best system for each eval-
uation measure.
Most of the instances in the data were annotated
with only one sense; only 11% were annotated with
two senses, and 0.5% with three. As a result, the
task organisers categorised the instances into single-
sense instances and multi-sense instances to bet-
ter analyse the performance of participating sys-
tems. Results for single-sense and multi-sense in-
stances are presented in Table 4 and Table 5, re-
spectively. Note that for single-sense instances, only
precision is used for WSD evaluation as the Jaccard
Index, positionally-weighted Kendall?s tau and nor-
malised discounted cumulative gain are not applica-
ble. Our system performs relatively well, and trails
marginally behind the best system in most cases.
4 Conclusion
We adopt a WSI methodology from Lau et al (2012)
for the task of grading senses in a WSD setting.
309
System JI KT NDCG FNMI FBC
RANDOM 0.244 0.633 0.287 0.018 0.382
SEMCOR MFS 0.455 0.465 0.339 ? ?
TEST MFS 0.552 0.560 0.412 ? ?
AI-KU 0.197 0.620 0.387 0.065 0.390
AI-KU (REMOVE5-AD1000) 0.244 0.642 0.332 0.039 0.451
LA SAPIENZA (2) 0.149 0.510 0.383 ? ?
UOS (TOP-3) 0.232 0.625 0.374 0.045 0.448
UNIMELB (5P) 0.218 0.614 0.365 0.056 0.459
UNIMELB (50K) 0.213 0.620 0.371 0.060 0.483
Table 3: Results for all instances. The best-performing system is indicated in boldface.
System Precision FNMI FBC
RANDOM 0.555 0.010 0.359
SEMCOR MFS 0.477 ? ?
TEST MFS 0.578 ? ?
AI-KU 0.641 0.045 0.351
AI-KU (REMOVE5-AD1000) 0.628 0.026 0.421
UOS (TOP-3) 0.600 0.028 0.414
UNIMELB (5P) 0.596 0.035 0.421
UNIMELB (50K) 0.605 0.039 0.441
Table 4: Results for single-sense instances. The best-performing system is indicated in boldface.
System JI KT NDCG FNMI FBC
RANDOM 0.429 0.548 0.236 0.006 0.113
SEMCOR MFS 0.283 0.373 0.197 ? ?
TEST MFS 0.354 0.426 0.248 ? ?
AI-KU 0.394 0.617 0.317 0.029 0.078
AI-KU (REMOVE5-AD1000) 0.434 0.586 0.291 0.004 0.116
LA SAPIENZA (2) 0.263 0.531 0.365 ? ?
UOS (#WN SENSES) 0.387 0.628 0.314 0.036 0.037
UNIMELB (5P) 0.426 0.586 0.287 0.019 0.130
UNIMELB (50K) 0.414 0.602 0.299 0.021 0.134
Table 5: Results for multi-sense instances. The best-performing system is indicated in boldface.
310
With no parameter tuning and using only the English
ukWaC as an external resource, our system performs
relatively well at the task.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
David Jurgens. 2012. An evaluation of graded sense
disambiguation using word sense induction. In Proc.
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012), pages 189?198,
Montre?al, Canada.
Ravi Kumar and Sergei Vassilvitskii. 2010. Generalized
distances between rankings. In Proc. of the 19th Inter-
national Conference on the World Wide Web (WWW
2010), pages 571?580, Raleigh, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction for web snippet clustering. In Proc. of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of the Confer-
ence on New Methods in Natural Language Process-
ing, Manchester, 1994.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
311
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 15?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Intelligent Linux Information Access by Data Mining: the ILIAD Project
Timothy Baldwin,? David Martinez,? Richard B. Penman,? Su Nam Kim,?
Marco Lui,? Li Wang? and Andrew MacKinlay?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
Abstract
We propose an alternative to conventional in-
formation retrieval over Linux forum data,
based on thread-, post- and user-level analysis,
interfaced with an information retrieval engine
via reranking.
1 Introduction
Due to the sheer scale of web data, simple keyword
matching is an effective means of information ac-
cess for many informational web queries. There
still remain significant clusters of information access
needs, however, where keyword matching is less
successful. One such instance is technical web fo-
rums and mailing lists (collectively termed ?forums?
for the purposes of this paper): technical forums
are a rich source of information when troubleshoot-
ing, and it is often possible to resolve technical
queries/problems via web-archived data. The search
facilities provided by forums and web search en-
gines tend to be over-simplistic, however, and there
is a desperate need for more sophisticated search (Xi
et al, 2004; Seo et al, 2009), including: favour-
ing threads which have led to a successful resolu-
tion; reflecting the degree of clarity/reproducibility
of the proposed solution in a given thread; repre-
senting threads via their threaded rather than sim-
ple chronological structure; the ability to highlight
key aspects of the thread, in terms of the problem
description and solution which led to a successful
resolution; and ideally, the ability to represent the
problem and solution in normalised form via infor-
mation extraction.
This paper provides a brief outline of an attempt
to achieve these and other goals in the context of
Linux web user forum data, in the form of the IL-
IAD (Intelligent Linux Information Access by Data
Mining) project. Linux users and developers rely
particularly heavily on web user forums and mail-
ing lists, due to the nature of the community, which
is highly decentralised ? with massive proliferation
of packages and distributions? and notoriously bad
at maintaining up-to-date documentation at a level
suitable for newbie and even intermediate users.
2 Project Outline
Our proposed solution is as follows: (1) crawl data
from a variety of web user forums; (2) analyse each
thread, to identify named entities and generate meta-
data; (3) analyse post-level linkages; (4) predict
user-level features which are expected to impinge on
the quality of search results; and finally (5) draw to-
gether the features from (1) to (4) to enhance the
quality of a traditional ranked IR approach. We
briefly review each step below. Given space limi-
tations, we focus on outlining our interpretation of
the task in this paper. For further details and results,
the reader is referred to the key papers cited herein.
2.1 Crawling
The first step is to crawl data from a variety of fo-
rums and mailing lists, for which we have developed
open-source scraping software in the form of SITE-
SCRAPER.1 SITESCRAPER is designed such that the
user simply copies relevant content from a browser-
rendered version of a given set of pages, which it
interprets as a structured record, and translates into
a generalised XPATH query.
2.2 Thread-level analysis
Next, we perform named entity recognition (NER)
over each thread to identify entities such as package
and distribution names, version numbers and snip-
pets of code; as part of this, we perform version
1http://sitescraper.googlecode.com/
15
anchoring, in identifying what entity each version
number relates to.
To generate thread-level metadata, we classify
each thread for the following three features, based
on an ordinal scale of 1?5 (Baldwin et al, 2007):
Complete: Is the problem description complete?
Solved: Is a solution provided in the thread?
Task Oriented: Is the thread about a specific
problem?
We additionally automatically classify the nature
of the thread content, in terms of, e.g., whether it
contains documentation or installation details, or re-
lates to software, hardware or programming.
Our experiments on thread-level classification are
based on a set of 250 annotated threads from Lin-
uxQuestions and other forums, as well as a dataset
from CNET.
2.3 Post-level analysis
We automatically analyse the post-to-post discourse
structure of each thread, in terms of which (preced-
ing) post(s) each post relates to, and how, building
off the work of Rose? et al (1995) and Wolf and Gib-
son (2005). For example, a given post may refute
the solution proposed in an earlier post, and also
propose a novel solution in response to the initiat-
ing post.
Separately, we are developing techniques for
identifying whether a new post to a given forum
is sufficiently similar to other (ideally resolved)
threads that the author should be prompted to first
check the existing threads for redundancy before a
new thread is initiated.
Our experiments on post-level analysis are, once
again, based on data from LinuxQuestions and
CNET.
2.4 User-level analysis
We are also experimenting with profiling users vari-
ously, based on a 5-point ordinal scale across a range
of user characteristics. Our experiments are based
on data from LinuxQuestions (Lui, 2009).
2.5 IR ranking
The various features are interfaced with an ad hoc
information retrieval (IR) system via a learning-to-
rank approach (Cao et al, 2007). In order to carry
out IR evaluation, we have developed a set of queries
and relevance judgements over a large-scale set of
forum data.
Our experiments to date have been based on com-
bination over three IR engines (LUCENE, ZETTAIR
and LEMUR), and involved thread-level metadata
only, but we have achieved encouraging results, sug-
gesting that thread-level metadata can enhance IR
effectiveness.
3 Conclusions
This paper provides an outline of the ILIAD project,
focusing on the tasks of crawling, thread-level anal-
ysis, post-level analysis, user-level analysis and IR
reranking. We have designed a series of class sets
for the component tasks, and carried out experimen-
tation over a range of data sources, achieving en-
couraging results.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
T Baldwin, D Martinez, and RB Penman. 2007. Auto-
matic thread classification for Linux user forum infor-
mation access. In Proc of ADCS 2007.
Z Cao, T Qin, TY Liu, MF Tsai, and H Li. 2007. Learn-
ing to rank: from pairwise approach to listwise ap-
proach. In Proc of ICML 2007.
M Lui. 2009. Impact of user characteristics on online fo-
rum classification tasks. Honours thesis, University of
Melbourne. http://repository.unimelb.edu.
au/10187/5745.
CP Rose?, B Di Eugenio, LS Levin, and C Van Ess-
Dykema. 1995. Discourse processing of dialogues
with multiple threads. In Proc of ACL 1995.
J Seo, WB Croft, and DA Smith. 2009. Online commu-
nity search using thread structure. In Proc of CIKM
2009.
F Wolf and E Gibson. 2005. Representing discourse co-
herence: A corpus-based study. Comp Ling, 31(2).
W Xi, J Lind, and E Brill. 2004. Learning effective rank-
ing functions for newsgroup search. In Proc of SIGIR
2004.
16
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 192?202,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Tagging and Linking Web Forum Posts
Su Nam Kim, Li Wang and Timothy Baldwin
Dept of Computer Science and Software Engineering
University of Melbourne, Australia
sunamkim@gmail.com, li.wang.d@gmail.com, tb@ldwin.net
Abstract
We propose a method for annotating post-
to-post discourse structure in online user
forum data, in the hopes of improving
troubleshooting-oriented information ac-
cess. We introduce the tasks of: (1) post
classification, based on a novel dialogue
act tag set; and (2) link classification. We
also introduce three feature sets (structural
features, post context features and seman-
tic features) and experiment with three dis-
criminative learners (maximum entropy,
SVM-HMM and CRF).We achieve above-
baseline results for both dialogue act and
link classification, with interesting diver-
gences in which feature sets perform well
over the two sub-tasks, and go on to per-
form preliminary investigation of the inter-
action between post tagging and linking.
1 Introduction
With the advent of Web 2.0, there has been an ex-
plosion of web authorship from individuals of all
walks of life. Notably, social networks, blogs and
web user forums have entered the mainstream of
modern-day society, creating both new opportuni-
ties and challenges for organisations seeking to en-
gage with clients or users of any description. One
area of particular interest is web-based user sup-
port, e.g. to aid a user in purchasing a gift for a
friend, or advising a customer on how to config-
ure a newly-acquired wireless router. While such
interactions traditionally took place on an indi-
vidual basis, leading to considerable redundancy
for frequently-arising requests or problems, user
forums support near-real-time user interaction in
the form of a targeted thread made up of individ-
ual user posts. Additionally, they have the poten-
tial for perpetual logging to allow other users to
benefit from them. This in turn facilitates ?sup-
port sharing??i.e. the ability for users to look
over the logs of past support interactions to deter-
mine whether there is a documented, immediately-
applicable solution to their current problem?on a
scale previously unimaginable. This research is
targeted at this task of enhanced support sharing,
in the form of text mining over troubleshooting-
oriented web user forum data (Baldwin et al, to
appear).
One facet of our proposed strategy for enhanc-
ing information access to troubleshooting-oriented
web user forum data is to preprocess threads to
uncover the ?content structure? of the thread, in
the form of its post-to-post discourse structure.
Specifically, we identify which earlier post(s) a
given post responds to (linking) and in what man-
ner (tagging), in an amalgam of dialogue act tag-
ging (Stolcke et al, 2000) and coherence-based
discourse analysis (Carlson et al, 2001; Wolf and
Gibson, 2005). The reason we do this is gauge
the relative role/import of individual posts, to in-
dex and weight component terms accordingly, ul-
timately in an attempt to enhance information ac-
cess. Evidence to suggest that this structure can
enhance information retrieval effectiveness comes
from Xi et al (2004) and Seo et al (2009) (see
Section 2).
To illustrate the task, consider the thread from
the CNET forum shown in Figure 1, made up of
5 posts (Post 1, ..., Post 5) with 4 distinct partici-
pants (A, B, C, D). In the first post, A initiates the
thread by requesting assistance in creating a web
form. In response, B proposes a Javascript-based
solution (i.e. responds to the first post with a pro-
posed solution), and C proposes an independent
solution based on .NET (i.e. also responds to the
first post with a proposed solution). Next, A re-
sponds to C?s post asking for details of how to in-
clude this in a web page (i.e. responds to the third
post asking for clarification), and in the final post,
D proposes a different solution again (i.e. responds
to the first post with a different solution again).
192
HTML Input Code - CNET Coding & scripting Forums
User A HTML Input Code
Post 1 . . . Please can someone tell me how to create an
input box that asks the user to enter their ID,
and then allows them to press go. It will then
redirect to the page . . .
User B Re: html input code
Post 2 Part 1: create a form with a text field. See
. . . Part 2: give it a Javascript action . . .
User C asp.net c# video
Post 3 Ive prepared for you video.link click . . .
User A Thank You!
Post 4 Thanks a lot for that . . . I have Microsoft Vi-
sual Studio 6, what program should I do this
in? Lastly, how do I actually include this in my
site?. . .
User D A little more help
Post 5 . . . You would simply do it this way: . . . You
could also just . . . An example of this is:. . .
Figure 1: Snippeted posts in a CNET thread



	
A



	ABCDAEBF
EBFEBF
Figure 2: Post links and dialogue act labels for the
example thread in Figure 1
In this, we therefore end up with a tree-based de-
pendency link structure, with each post (other than
the initial post) relating back to a unique preced-
ing post via a range of link types, as indicated in
Figure 2. Note, however, that more generally, it
is possible for a post to link to multiple preced-
ing posts (e.g. refuting one proposed solution, and
proposing a different solution to the problem in the
initial post).
Our primary contributions in this paper are: (1)
a novel post label set for post structure in web
forum data, and associated dataset; and (2) a se-
ries of results for post dependency linking and la-
belling, which achieve strong results for the re-
spective tasks.
2 Related Work
Related work exists in the broad fields of dialogue
processing, discourse analysis and information re-
trieval, and can be broken down into the following
tasks: (1) dialogue act tagging; (2) discourse ?dis-
entanglement?; (3) community question answer-
ing; and (4) newsgroup/user forum search.
Dialogue act (DA) tagging is a means of cap-
turing the function of a given utterance relative
to an encompassing discourse, and has been pro-
posed variously as a means of enhancing dialogue
summarisation (Murray et al, 2006), and track-
ing commitments and promises in email (Cohen
et al, 2004; Lampert et al, 2008), as well as be-
ing shown to improve speech recognition accu-
racy (Stolcke et al, 2000). A wide range of DA
tag sets have been proposed, usually customised
to a particular medium such as speech dialogue
(Stolcke et al, 2000; Shriberg et al, 2004), task-
focused email (Cohen et al, 2004; Wang et al,
2007; Lampert et al, 2008) or instant messag-
ing (Ivanovic, 2008). The most immediately rel-
evant DA-based work we are aware of is that of
Xi et al (2004), who proposed a 5-way classifi-
cation for newsgroup data (including QUESTION
and AGREEMENT/AMMENDMENT), but did not
present any results based on the tagset.
A range of supervised models have been applied
to DA classification, including graphical mod-
els (Ji and Bilmes, 2005), kernel methods (Wang
et al, 2007), dependency networks (Carvalho
and Cohen, 2005), transformation-based learning
(Samuel et al, 1998), maxent models (Ang et
al., 2005) and HMMs (Ivanovic, 2008). There is
some contention about the import of context in DA
classification, with the prevailing view being that
context aids classification (Carvalho and Cohen,
2005; Ang et al, 2005; Ji and Bilmes, 2005), but
also evidence to suggest that strictly local mod-
elling is superior (Ries, 1999; Serafin and Di Eu-
genio, 2004).
In this work, we draw on existing work (esp.
Xi et al (2004)) in proposing a novel DA tag
set customised to the analysis of troubleshooting-
oriented web user forums (Section 3), and com-
pare a range of text classification and structured
classification methods for post-level DA classifi-
cation.
Discourse disentanglement is the process of
automatically identifying coherent sub-discourses
in a single thread (in the context of user fo-
rums/mailing lists), chat session (in the context of
IRC chat data: Elsner and Charniak (2008)), sys-
tem interaction (in the context of HCI: Lemon et
al. (2002)) or document (Wolf and Gibson, 2005).
The exact definition of what constitutes a sub-
discourse varies across domains, but for our pur-
poses, entails an attempt to resolve the informa-
193
tion need of the initiator by a particular approach;
if there are competing approaches proposed in a
single thread, multiple sub-discourses will neces-
sarily arise. The data structure used to represent
the disentangled discourse varies from a simple
connected sub-graph (Elsner and Charniak, 2008),
to a stack/tree (Grosz and Sidner, 1986; Lemon
et al, 2002; Seo et al, 2009), to a full directed
acyclic graph (DAG: Rose? et al (1995), Wolf and
Gibson (2005), Schuth et al (2007)). Disentan-
glement has been carried out via analysis of di-
rect citation/user name references (Schuth et al,
2007; Seo et al, 2009), topic modelling (Lin et al,
2009), and clustering over content-based features
for pairs of posts, optionally incorporating various
constraints on post recency (Elsner and Charniak,
2008; Wang et al, 2008; Seo et al, 2009).
In this work, we follow Rose? et al (1995) and
Wolf and Gibson (2005) in adopting a DAG repre-
sentation of discourse structure, and draw on the
wide set of features used in discourse entangle-
ment to model coherence.
Community question answering (cQA) is the
task of identifying question?answer pairs in a
given thread, e.g. for the purposes of thread sum-
marisation (Shrestha and McKeown, 2004) or au-
tomated compilation of resources akin to Yahoo!
Answers. cQA has been applied to both mail-
ing list and user forum threads, conventionally
based on question classification, followed by rank-
ing of candidate answers relative to each question
(Shrestha and McKeown, 2004; Ding et al, 2008;
Cong et al, 2008; Cao et al, 2009). The task is
somewhat peripheral to our work, but relevant in
that it involves the implicit tagging of certain posts
as containing questions/answers, as well as link-
ing the posts together. Once again, we draw on the
features used in cQA in this research.
There has been a spike of recent interest in
newsgroup/user forum search. Xi et al (2004)
proposed a structured information retrieval (IR)
model for newsgroup search, based on author fea-
tures, thread structure (based on the tree defined by
the reply-to structure), thread ?topology? features
and content-based features, and used a supervised
ranking method to improve over a baseline IR sys-
tem. Elsas and Carbonell (2009) ? building on
earlier work on blog search (Elsas et al, 2008) ?
proposed a probabilistic IR approach which ranks
user forum threads relative to selected posts in the
overall thread, and again demonstrated the superi-
ority of this method over a model which ignores
thread structure. Finally, Seo et al (2009) auto-
matically derived thread structure from user forum
threads, and demonstrated that the IR effectiveness
over the ?threaded? structure was superior to that
using a monolithic document representation.
The observations and results of Xi et al (2004)
and Seo et al (2009) that threading information
(or in our case ?disentangled? DAG structure) en-
hances IR effectiveness is a core motivator for this
research.
3 Post Label Set
Our post label set contains 12 categories, intended
to capture the typical interactions that take place in
troubleshooting-oriented threads on technical fo-
rums. There are 2 super-categories (QUESTION,
ANSWER) and 3 singleton classes (RESOLUTION,
REPRODUCTION, and OTHER). QUESTION, in
turn, contains 4 sub-classes (QUESTION, ADD,
CONFIRMATION, CORRECTION), while ANSWER
contains 5 sub-classes (ANSWER, ADD, CONFIR-
MATION, CORRECTION, and OBJECTION), par-
tially mirroring the sub-structure of QUESTION.
We represent the amalgam of a super- and sub-
class as QUESTION-ADD, for example.
All tags other than QUESTION-QUESTION and
OTHER are relational, i.e. relate a given post to a
unique earlier post. A given post can potentially
be labelled with multiple tags (e.g. confirm details
of a proposed solution, in addition to providing ex-
tra details of the problem), although, based on the
strictly chronological ordering of posts in threads,
a post can only link to posts earlier in the thread
(and can also not cross thread boundaries). Addi-
tionally, the link structure is assumed to be tran-
sitive, in that if post A links to post B and post B
to post C, post A is implicitly linked to post C. As
such, an explicit link from post A to post C should
exist only in the case that the link between them is
not inferrable transitively.
Detailed definitions of each post tag are given
below. Note that initiator refers to the user who
started the thread with the first post.
QUESTION-QUESTION (Q-Q): the post con-
tains a new question, independent of the
thread context that precedes it. In general,
QUESTION-QUESTION is reserved for the
first post in a given thread.
QUESTION-ADD (Q-ADD): the post supple-
194
ments a question by providing additional
information, or asking a follow-up question.
QUESTION-CONFIRMATION (Q-CONF): the
post points out error(s) in a question without
correcting them, or confirms details of the
question.
QUESTION-CORRECTION (Q-CORR): the post
corrects error(s) in a question.
ANSWER-ANSWER (A-A): the post proposes an
answer to a question.
ANSWER-ADD (A-ADD): the post supplements
an answer by providing additional informa-
tion.
ANSWER-CONFIRMATION (A-CONF): the
post points out error(s) in an answer without
correcting them, or confirms details of the
answer.
ANSWER-CORRECTION (A-CORR): the post
corrects error(s) in an answer.
ANSWER-OBJECTION (A-OBJ): the post ob-
jects to an answer on experiential or theoreti-
cal grounds (e.g. It won?t work.).
RESOLUTION (RES): the post confirms that an
answer works, on the basis of implementing
it.
REPRODUCTION (REP): the post either: (1)
confirms that the same problem is being ex-
perienced (by a non-initiator, e.g. I?m seeing
the same thing.); or (2) confirms that the an-
swer should work.
OTHER (OTHER): the post does not belong to
any of the above classes.
4 Feature Description
In this section, we describe our post feature repre-
sentation, in the form of four feature types.
4.1 Lexical features
As our first feature type, we use simple lexical fea-
tures, in the form of unigram and bigram tokens
contained within a given post (without stopping).
We also POS tagged and lemmatised the posts,
postfixing the lemmatised token with its POS tag
(using Lingua::EN::Tagger and morpha (Min-
nen et al, 2001)). Finally, we bin together the
counts for each token, and represent it via its raw
frequency.
4.2 Structural features
The identity of the post author, and position of the
post within the thread, can be indicators of the
post/link structure of a given post. We represent
the post author as a simple binary feature indicat-
ing whether s/he is the thread initiator, and the post
position via its relative position in the thread (as a
ratio, relative to the total number of posts).
4.3 Post context features
As mentioned in Section 2, post context has gen-
erally (but not always) been shown to enhance the
classification accuracy of DA tagging tasks, in the
form of Markov features providing predicted post
labels for previous posts, or more simply, post-to-
post similarity. We experiment with a range of
post context features, all of which are compatible
with features both from the same label set as that
being classified (e.g. link features for link classifi-
cation), as well as features from a second label set
(e.g. DA label features for link classification).
Previous Post: There is a strong prior for posts
to link to their immediately preceding post (as ob-
served for 79.9% of the data in our dataset), and
also strong sequentiality in our post label set (e.g.
a post following a Q-Q is most likely to be an A-
A). As such, we represent the predicted post label
of the immediately preceding post, as a first-order
Markov feature, as well as a binary feature to in-
dicate whether the author of the previous post also
authored the current post.
Previous Post from Same Author: A given
user tends to author posts of the same basic type
(e.g. QUESTION or ANSWER) in a given thread,
and pairings such as A-A and A-CONF from a
given author are very rare. To capture this obser-
vation, we look to see if the author of the current
post has posted earlier in the thread, and if so, in-
clude the label and relative location (in posts) of
their most recent previous post.
Full History: As a final option, we include the
predictions for all posts P1, ..., Pi?1 preceding the
current post Pi.
4.4 Semantic features
We tested four semantic features based on post
content and title.
195
Title Similarity: For forums such as CNET
which include titles for individual posts (as rep-
resented in Figure 1), a post having the same or
similar title as a previous post is often a strong
indicator that it responds to that post. This both
provides a strong indicator of which post a given
post responds (links) to, and can aid in DA tag-
ging. We use simple cosine similarity to find the
post with the most-similar title, and represent its
relative location to the current post.
Post Similarity: Posts of the same general type
tend to have similar content and be linked. For
example, A-A and A-ADD posts tend to share
content. We capture this by identifying the post
with most-similar content based on cosine similar-
ity, and represent its relative location to the current
post.
Post Characteristics: We separately represent
the number of question marks, exclamation marks
and URLs in the current post. In general, ques-
tion marks occur in QUESTION and CONFIRMA-
TION posts, exclamation marks occur in RES and
OBJECTION posts, and URLs occur in A-A and
A-ADD posts.
User Profile: Some authors tend to answer ques-
tions more, while others tend to ask more ques-
tions. We capture the class priors for the author of
the current post by the distribution of post labels
in their posts in the training data.
5 Experimental Setup
As our dataset, we collected 320 threads contain-
ing a total of 1,332 posts from the Operating Sys-
tem, Software, Hardware, and Web Development
sub-forums of CNET.1
The annotation of post labels and links was car-
ried by two annotators in a custom-built web inter-
face which supported multiple labels and links for
a given post. For posts with multilabels, we used
a modified version of Cohen?s Kappa, which re-
turned ? values of 0.59 and 0.78 for the post label
and link annotations, respectively. Any disagree-
ments in labelling were resolved through adjudi-
cation.
Of the 1332 posts, 65 posts have multiple labels
(which possibly link to a common post) and 22
posts link to two different links. The majority post
label in the dataset is A-A (40.30%).
1http://forums.cnet.com/?tag=
TOCleftColumn.0
We built machine learners using a conven-
tional Maximum Entropy (ME) learner,2 as well as
two structural learners, namely: (1) SVM-HMMs
(Joachims et al, 2009), as implemented in SVM-
struct3, with a linear kernel; and (2) conditional
random fields (CRFs) using CRF++.4 SVM-
HMMs and CRFs have been successfully applied
to a range of sequential tagging tasks such as
syllabification (Bartlett et al, 2009), chunk pars-
ing (Sha and Pereira, 2003) and word segmen-
tation (Zhao et al, 2006). Both are discrimina-
tive models which capture structural dependen-
cies, which is highly desirable in terms of mod-
elling sequential preferences between post labels
(e.g. A-CONF typically following a A-A). SVM-
HMM has the additional advantage of scaling to
large numbers of features (namely the lexical fea-
tures). As such, we only experiment with lexical
features for SVM-HMM and ME.
All of our evaluation is based on stratified 10-
fold cross-validation, stratifying at the thread level
to ensure that if a given post is contained in the
test data for a given iteration, all other posts in
that same thread are also in the test data (or more
pertinently, not in the training data). We evalu-
ate using micro-averaged precision, recall and F-
score (? = 1). We test the statistical significance
of all above-baseline results using randomised es-
timation (p < 0.05; Yeh (2000)), and present all
such results in bold in our results tables.
In our experiments, we first look at the post
classification task in isolation (i.e. we predict
which labels to associate with each post, under-
specifying which posts those labels relate to). We
then move on to look at the link classification task,
again in isolation (i.e. we predict which previous
posts each post links to, underspecifying the na-
ture of the link). Finally, we perform preliminary
investigation of the joint task of DA and link clas-
sification, by incorporating DA class features into
the link classification task.
6 DA Classification Results
Our first experiment is based on post-level dia-
logue act (DA) classification, ignoring link struc-
ture in the first instance. That is, we predict the
labels on edges emanating from each post in the
DAG representation of the post structure, without
2http://maxent.sourceforge.net/
3http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
4http://crfpp.sourceforge.net/
196
Features CRF SVM-HMM ME
Lexical ? .566 .410
Structural .742 .638 .723
Table 1: DA classification F-score with lexical and
structural features (above-baseline results in bold)
specifying the edge destination. Returning to our
example in Figure 2, e.g., the gold-standard clas-
sification for Post 1 would be Q-Q, Post 2 would
be A-A, etc.
As a baseline for DA classification, simple ma-
jority voting attains an F-score of 0.403, based on
the A-A class. A more realistic baseline, how-
ever, is a position-conditioned variant, where the
first post is always classified as Q-Q, and all sub-
sequent posts are classified as A-A, achieving an
F-score of 0.641.
6.1 Lexical and structural features
First, we experiment with lexical and structural
features (recalling that we are unable to scale the
CRF model to full lexical features). Lexical fea-
tures produce below-baseline performance, while
simple structural features immediately lead to an
improvement over the baseline for CRF and ME.
The reason for the poor performance with lex-
ical features is that our dataset contains only
around 1300 posts, each of which is less than 100
words in length on average. The models are sim-
ply unable to generalise over this small amount of
data, and in the case of SVM-HMM, the presence
of lexical features, if anything, appears to obscure
the structured nature of the labelling task (i.e. the
classifier is unable to learn the simple heuristic
used by the modified majority class baseline).
The success of the structural features, on the
other hand, points to the presence of predictable
sequences of post labels in the data. That SVM-
HMM is unable to achieve baseline performance
with structural features is slightly troubling.
6.2 Post context features
Next, we test the two post context features: Previ-
ous Post (P) and Previous Post from Same Author
(A). Given the success of structural features, we
retain these in our experiments. Note that the la-
bels used in the post context are those which are
interactively learned by that model for the previ-
ous posts.
Table 2 presents the results for structural fea-
Features CRF SVM-HMM ME
Struct+R .740 .640 .632
Struct+A .742 .676 .693
Struct+F .744 .641 .577
Struct+RA .397 .636 .665
Struct+AF .405 .642 .586
Table 2: DA classification F-score with structural
and DA-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
tures combined with DA-based post context; we
do not present any combinations of Previous Post
and Full History, as Full History includes the Pre-
vious Post.
Comparing back to the original results using
only the structural results, we can observe that Pre-
vious Post from Same Author and Full History (A
and F, resp., in the table) lead to a slight incre-
ment in F-score for both CRF and SVM-HMM,
but degrade the performance of ME. Previous Post
leads to either a marginal improvement, or a drop
in results, most noticeably for ME. It is slightly
surprising that the CRF should benefit from con-
text features at all, given that it is optimising over
the full tag sequence, but the impact is relatively
localised, and when all sets of context features
are used, the combined weight of noisy features
appears to swamp the learner, leading to a sharp
degradation in F-score.
6.3 Semantic features
We next investigate the relative impact of the se-
mantic features, once again including structural
features in all experiments. Table 3 presents the F-
score using the different combinations of semantic
features.
Similarly to the post context features, the se-
mantic features produced slight increments over
the structural features in isolation, especially for
CRF and ME. For the first time, SVM-HMM
achieved above-baseline results, when incorporat-
ing title similarity and post characteristics. Of the
individual semantic features, title and post simi-
larity appear to be the best performers. Slightly
disappointingly, the combination of semantic fea-
tures generally led to a degradation in F-score, al-
most certainly due to data sparseness. The best
overall result was achieved with CRF, incorporat-
197
Features CRF SVM-HMM ME
Struct+T .751 .636 .660
Struct+P .747 .636 .662
Struct+C .738 .587 .630
Struct+U .722 .564 .620
Struct+TP .740 .627 .720
Struct+TC .744 .646 .589
Struct+TU .738 .600 .609
Struct+PC .745 .630 .583
Struct+PU .736 .626 .605
Struct+CU .730 .599 .619
Struct+TPC .739 .622 .580
Struct+TPU .729 .613 .6120
Struct+TCU .750 .611 .6120
Struct+PCU .738 .616 .614
Struct+TPCU .737 .619 .605
Table 3: DA classification F-score with semantic
features (T = ?Title Similarity?, P = ?Post Simi-
larity?, C = ?Post Characteristics?, and U = ?User
Profile?; above-baseline results in bold)
ing structural features and title similarity, at an F-
score of 0.751.
To further explore the interaction between post
context and semantic features, we built CRF clas-
sifiers for different combinations of post context
and semantic features, and present the results in
Table 4.5 We achieved moderate gains in F-score,
with all post context features, in combination with
structural features, post similarity and post char-
acteristics achieving an F-score of 0.753, slightly
higher than the best result achieved for just struc-
tural and post context features.
It is important to refer back to the results for
lexical features (comparable to what would have
been achieved with a standard text categorisation
approach to the task), and observe that we have
achieved far higher F-scores using features cus-
tomised to user forum data. It is also important
to reflect that post context (in terms of the features
and the structured classification results of CRF)
appears to markedly improve our results, contrast-
ing with the results of Ries (1999) and Serafin and
Di Eugenio (2004).
5We omit the results for Full History post context for rea-
sons of space, but there is relatively little deviation from the
numbers presented.
Features R A RA
Struct+T .649 .649 .649
Struct+P .737 .736 .742
Struct+C .741 .741 .742
Struct+U .745 .742 .737
Struct+TP .645 .656 .658
Struct+TC .383 .402 .408
Struct+TU .650 .652 .652
Struct+PC .730 .743 .753
Struct+PU .232 .232 .286
Struct+CU .719 .471 .710
Struct+TPC .498 .469 .579
Struct+TPU .248 .232 .248
Struct+TCU .388 .377 .380
Struct+PCU .231 .231 .261
Struct+TPCU .231 .231 .231
Table 4: DA classification F-score for CRF with
different combinations of post context features and
semantic features (R = ?Previous Post?, and A
= ?Previous Post from Same Author?; T = ?Ti-
tle Similarity?, P = ?Post Similarity?, C = ?Post
Characteristics?, and U = ?User Profile?; above-
baseline results in bold)
7 Link Classification Results
Our second experiment is based on link classifi-
cation in isolation. Here, we predict unlabelled
edges, e.g. in Figure 2, the gold-standard classifi-
cation for Post 1 would be NULL, Post 2 would be
Post 1, Post 3 would be Post 1, etc.
Note that the initial post cannot link to any other
post, and also that the second post always links
to the first post. As this is a hard constraint on
the data, and these posts simply act to inflate the
overall numbers, we exclude all first and second
posts from our evaluation of link classification.
We experimented with a range of baselines as
presented in Table 5, but found that the best per-
former by far was the simple heuristic of linking
each post (except for the initial post) to its imme-
diately preceding post. This leads to an F-score of
0.631, comparable to that for the post classifica-
tion task.
7.1 Lexical and structural features
Once again, we started by exploring the effective-
ness of lexical and structural features using the
three learners, as detailed in Table 6.
Similarly to the results for post classification,
198
Baseline Prec Rec F-score
Previous post .641 .622 .631
First post .278 .269 .274
Title similarity .311 .301 .306
Post similarity .255 .247 .251
Table 5: Baselines for link classification
Features CRF SVM-HMM ME
Lexical ? .154 .274
Structural .446 .220 .478
Table 6: Link classification F-score with lexical
and structural features (above-baseline results in
bold)
structural features are more effective than lexical
features for link classification, but this time, nei-
ther feature set approaches the baseline F-score
for any of the learners. Once again, the results for
SVM-HMM are well below those for the other two
learners.
7.2 Post context features
Next, we experiment with link-based post con-
text features, in combination with the structural
features, as the results were found to be consis-
tently better when combined with the structural
features (despite the below-baseline performance
of the structural features in this case). The link-
based post context features in all cases are gener-
ated using the CRF with structural features from
Table 6. As before, we do not present any combi-
nations of Previous Post and Full History, as Full
History includes the Previous Post
As seen in Table 9, here, for the first time, we
achieve an above-baseline result for link classifi-
cation, for SVM and ME based on Previous Post
from Same Author in isolation, and also some-
times in combination with the other feature sets.
The results for CRF also improve, but not to a
level of statistical significance over the baseline.
Similarly to the results for DA classification, the
results for CRF drop appreciably when we com-
bine feature sets.
7.3 Semantic features
Finally, we experiment with semantic features,
once again in combination with structural features.
The results are presented in Table 8.
The results for semantic features largely mir-
Features CRF SVM-HMM ME
Struct+R .234 .605 .618
Struct+A .365 .665 .665
Struct+F .624 .648 .615
Struct+RA .230 .615 .661
Struct+AF .359 .663 .621
Table 7: Link classification F-score with structural
and link-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
Features CRF SVM-HMM ME
Struct+T .464 .223 .477
Struct+P .433 .198 .453
Struct+C .438 .213 .419
Struct+U .407 .160 .376
Struct+TP .459 .194 .491
Struct+TC .449 .229 .404
Struct+TU .456 .174 .353
Struct+PC .422 .152 .387
Struct+PU .439 .166 .349
Struct+CU .397 .178 .366
Struct+TPC .449 .185 .418
Struct+TPU .449 .160 .365
Struct+TCU .459 .185 .358
Struct+PCU .439 .161 .358
Struct+TPCU .443 .163 .365
Table 8: Link classification F-score with semantic
features (T = ?Title Similarity?, P = ?Post Simi-
larity?, C = ?Post Characteristics?, and U = ?User
Profile?; above-baseline results in bold)
ror those for post classification: small improve-
ments are observed for title similarity with CRF,
but otherwise, the results degrade across the board,
and the combination of different feature sets com-
pounds this effect.
The best overall result achieved for link classifi-
cation is thus the 0.743 for CRF with the structural
and post context features.
We additionally experimented with combina-
tions of features as for post classification, but were
unable to improve on this result.
7.4 Link Classification using DA Features
Ultimately, we require both DA and link classifica-
tion of each post, which is possible by combining
the outputs of the component classifiers described
199
Features CRF SVM-HMM ME
Struct+R .586 .352 .430
Struct+A .591 .278 .568
Struct+F .704 .477 .546
Struct+RA .637 .384 .551
Struct+AF .743 .527 .603
Table 9: Link classification F-score with structural
and post-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
above, by rolling the two tasks into a single clas-
sification task, or alternatively by looking to joint
modelling methods. As a preliminary step in this
direction, and means of exploring the interaction
between the two tasks, we repeat the experiment
based on post context features from above (see
Section 7.2), but rather than using link-based post
context, we use DA-based post context.
As can be seen in Table 9, the results for SVM-
HMM and ME drop appreciably as compared to
the results using link-based post context in Table 9,
while the results for CRF jump to the highest level
achieved for the task for all three learners. The
effect can be ascribed to the ability of CRF to
natively model the (bidirectional) link classifica-
tion history in the process of performing structured
learning, and the newly-introduced post features
complementing the link classification task.
8 Discussion and Future Work
Ultimately, we require both DA and link classifica-
tion of each post, which is possible in (at least) the
following three ways: (1) by combining the out-
puts of the component classifiers described above;
(2) by rolling the two tasks into a single classifi-
cation task; or (3) by looking to joint modelling
methods. Our results in Section 7.4 are suggestive
of the empirical potential of performing the two
tasks jointly, which we hope to explore in future
work.
One puzzling effect observed in our experi-
ments was the generally poor results for SVM. Er-
ror analysis indicates that the classifier was heav-
ily biased towards the high-frequency classes, e.g.
classifying all posts as either Q-Q or A-A for DA
classification. The classifications for the other two
learners were much more evenly spread across the
different classes.
CRF was limited in that it was unable to cap-
ture lexical features, but ultimately, lexical fea-
tures were found to be considerably less effec-
tive than structural and post context features for
both tasks, and the ability of the CRF to opti-
mise the post labelling over the full sequence of
posts in a thread more than compensated for this
shortcoming. Having said this, there is more work
to be done exploring synergies between the dif-
ferent feature sets, especially for DA classifica-
tion where all feature sets were found to produce
above-baseline results.
Another possible direction for future research is
to explore the impact of inter-post time on link
structure, based on the observation that follow-
up posts from the initiator tend to be tempo-
rally adjacent to posts they respond to with rela-
tively short time intervals, while posts from non-
initiators which are well spaced out tend not to re-
spond to one another. Combining this with pro-
filing of the cross-thread behaviour of individual
forum participants (Weimer et al, 2007; Lui and
Baldwin, 2009), and formal modelling of ?forum
behaviour? is also a promising line of research,
taking the lead from the work of Go?tz et al (2009),
inter alia.
9 Conclusion
In this work, we have proposed a method for
analysing post-to-post discourse structure in on-
line user forum data, in the form of post link-
ing and dialogue act tagging. We introduced
three feature sets: structural features, post con-
text features and semantic features. We exper-
imented with three learners (maximum entropy,
SVM-HMM and CRF), and established that CRF
is the superior approach to the task, achieving
above-baseline results for both post and link clas-
sification. We also demonstrated the complemen-
tarity of the proposed feature sets, especially for
the post classification task, and carried out a pre-
liminary exploration of the interaction between the
linking and dialogue act tagging tasks.
Acknowledgements
This research was supported in part by funding
from Microsoft Research Asia.
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
200
tion in multiparty meetings. In Proceedings of
the 2005 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2005),
pages 1061?1064, Philadelphia, USA.
Timothy Baldwin, David Martinez, Richard Penman,
Su Nam Kim, Marco Lui, Li Wang, and Andrew
MacKinlay. to appear. Intelligent Linux informa-
tion access by data mining: the ILIAD project. In
Proceedings of the NAACL 2010 Workshop on Com-
putational Linguistics in a World of Social Media:
#SocialMedia, Los Angeles, USA.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics ? Human
Language Technologies 2009 (NAACL HLT 2009),
pages 308?316, Boulder, USA.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2009),
pages 265?274, Hong Kong, China.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged
corpus in the framework of rhetorical structure the-
ory. In Proceedings of the Second SIGdial Work-
shop on Discourse and Dialogue, pages 1?10, Aal-
borg, Denmark. Association for Computational Lin-
guistics Morristown, NJ, USA.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?.
In Proceedings of 28th International ACM-SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR 2005), pages 345?352.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 309?316,
Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In Proceedings of
31st International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?08), pages 467?474, Singapore.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract context and answers of questions from online
forums. In Proceedings of the 46th Annual Meet-
ing of the ACL: HLT (ACL 2008), pages 710?718,
Columbus, USA.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Inter-
national ACM-SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?09),
pages 714?715, Boston, USA.
Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and
Jaime G. Carbonell. 2008. Retrieval and feed-
back models for blog feed search. In Proceedings of
31st International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?08), pages 347?354, Singapore.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?
842, Columbus, USA.
Michaela Go?tz, Jure Leskovec, Mary McGlohon, and
Christos Faloutsos. 2009. Modeling blog dynamics.
In Proceedings of the Third International Confer-
ence on Weblogs and Social Media (ICWSM 2009),
pages 26?33, San Jose, USA.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of
the 2005 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2005),
pages 33?36, Philadelphia, USA.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Machine Learning, 77(1):27?59.
Andrew Lampert, Robert Dale, and Ce?cile Paris.
2008. The nature of requests and commitments in
email messages. In Proceedings of the AAAI 2008
Workshop on Enhanced Messaging, pages 42?47,
Chicago, USA.
Oliver Lemon, Alex Gruenstein, and Stanley Pe-
ters. 2002. Collaborative activities and multi-
tasking in dialogue systems. Traitement Automa-
tique des Langues (TAL), Special Issue on Dialogue,
43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling se-
mantics and structure of discussion threads. In Pro-
ceedings of the 18th International Conference on the
World Wide Web (WWW 2009), pages 1103?1104,
Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse.
In Proceedings of the Fourteenth Australasian Doc-
ument Computing Symposium (ADCS 2009), Syd-
ney, Australia.
201
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and dis-
course features into speech summarization. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367?374.
Klaus Ries. 1999. HMM and neural network
based speech act detection. In Proceedings of the
1999 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP-99), pages
497?500, Phoenix, USA.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995.
Discourse processing of dialogues with multiple
threads. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 31?38, Cambridge, USA.
Ken Samuel, Carbeery Sandra Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proceedings of
the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguis-
tics (COLING/ACL-98), pages 1150?1156, Mon-
treal, Canada.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in com-
ments on news-articles. In Proceedings of the 9th
Annual ACM International Workshop on Web Infor-
mation and Data Management, pages 97?104, Lis-
boa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Riccardo Serafin and Barbara Di Eugenio. 2004.
FLSA: Extending latent semantic analysis with fea-
tures for dialogue act classification. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), pages 692?
699, Barcelona, Spain.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meeting
of the NAACL (HLT-NAACL 2003), pages 213?220,
Edmonton, Canada.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conver-
sations. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 889?895, Geneva, Switzerland.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat,
Jeremy Ang, and Hannah Carvey. 2004. The ICSI
meeting recorder dialog act (MRDA) corpus. In
Proceedings of the 5th SIGdial Workshop on Dis-
course and Dialogue, pages 97?100, Cambridge,
USA.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Pail Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of
Conversational Speech. Computational Linguistics,
26(3):339?373.
Yi-ChiaWang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions (ACL 2007), pages 73?76, Prague, Czech Re-
public.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages
152?160, Seattle, USA.
Markus Weimer, Iryna Gurevych, and Max
Mu?hlha?user. 2007. Automatically assessing
the post quality in online discussions on software.
In Proceedings of the 45th Annual Meeting of
the ACL: Interactive Poster and Demonstration
Sessions, pages 125?128, Prague, Czech Republic.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Com-
putational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search.
In Proceedings of 27th International ACM-SIGIR
Conference on Research and Development in In-
formation Retrieval (SIGIR 2004), pages 394?401.
Sheffield, UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
947?953, Saarbru?cken, Germany.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162?165. Sydney, Australia.
202
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), page 1,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
MWEs and Topic Modelling:
Enhancing Machine Learning with Linguistics
Timothy Baldwin
University of Melbourne, Australia
tim@csse.unimelb.edu.au
Abstract
Topic modelling is a popular approach to joint clus-
tering of documents and terms, e.g. via Latent
Dirichlet Allocation. The standard document repre-
sentation in topic modelling is a bag of unigrams,
ignoring both macro-level document structure and
micro-level constituent structure. In this talk, I
will discuss recent work on consolidating the micro-
level document representation with multiword ex-
pressions, and present experimental results which
demonstrate that linguistically-richer document rep-
resentations enhance topic modelling.
Biography
Tim Baldwin is an Associate Professor and Deputy
Head of the Department of Computer Science and
Software Engineering, University of Melbourne and
a contributed research staff member of the NICTA
Victoria Research Laboratories. He has previously
held visiting positions at the University of Wash-
ington, University of Tokyo, University of Saarland,
and NTT Communication Science Laboratories. His
research interests cover topics including deep lin-
guistic processing, multiword expressions, deep lex-
ical acquisition, computer-assisted language learn-
ing, information extraction and web mining, with a
particular interest in the interface between compu-
tational and theoretical linguistics. Current projects
include web user forum mining, information person-
alisation in museum contexts, biomedical text min-
ing, online linguistic exploration, and intelligent in-
terfaces for Japanese language learners. He is Pres-
ident of the Australasian Language Technology As-
sociation in 2011-2012.
Tim completed a BSc(CS/Maths) and
BA(Linguistics/Japanese) at the University of
Melbourne in 1995, and an MEng(CS) and
PhD(CS) at the Tokyo Institute of Technology in
1998 and 2001, respectively. Prior to commencing
his current position at the University of Melbourne,
he was a Senior Research Engineer at the Center for
the Study of Language and Information, Stanford
University (2001-2004).
1
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33?41,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Continuous Measurement Scales in
Human Evaluation of Machine Translation
Yvette Graham Timothy Baldwin Alistair Moffat Justin Zobel
Department of Computing and Information Systems, The University of Melbourne
{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au
Abstract
We explore the use of continuous rat-
ing scales for human evaluation in the
context of machine translation evaluation,
comparing two assessor-intrinsic quality-
control techniques that do not rely on
agreement with expert judgments. Ex-
periments employing Amazon?s Mechan-
ical Turk service show that quality-control
techniques made possible by the use of
the continuous scale show dramatic im-
provements to intra-annotator agreement
of up to +0.101 in the kappa coefficient,
with inter-annotator agreement increasing
by up to +0.144 when additional standard-
ization of scores is applied.
1 Introduction
Human annotations of language are often required
in natural language processing (NLP) tasks for
evaluation purposes, in order to estimate how well
a given system mimics activities traditionally per-
formed by humans. In tasks such as machine
translation (MT) and natural language generation,
the system output is a fully-formed string in a tar-
get language. Annotations can take the form of
direct estimates of the quality of those outputs or
be structured as the simpler task of ranking com-
peting outputs from best-to-worst (Callison-Burch
et al, 2012).
A direct estimation method of assessment, as
opposed to ranking outputs from best-to-worst,
has the advantage that it includes in annotations
not only that one output is better than another,
but also the degree to which that output was bet-
ter than the other. In addition, direct estimation
of quality within the context of machine transla-
tion extends the usefulness of the annotated data
to other tasks such as quality-estimation (Callison-
Burch et al, 2012).
For an evaluation to be credible, the annotations
must be credible. The simplest way of establish-
ing this is to have the same data point annotated by
multiple annotators, and measure the agreement
between them. There has been a worrying trend
in recent MT shared tasks ? whether the evalu-
ation was structured as ranking translations from
best-to-worst, or by direct estimation of fluency
and adequacy ? of agreement between annotators
decreasing (Callison-Burch et al, 2008; Callison-
Burch et al, 2009; Callison-Burch et al, 2010;
Callison-Burch et al, 2011; Callison-Burch et al,
2012). Inconsistency in human evaluation of ma-
chine translation calls into question conclusions
drawn from those assessments, and is the target
of this paper: by revising the annotation process,
can we improve annotator agreement, and hence
the quality of human annotations?
Direct estimates of quality are intrinsically con-
tinuous in nature, but are often collected using an
interval-level scale with a relatively low number
of categories, perhaps to make the task cognitively
easier for human assessors. In MT evaluation,
five and seven-point interval-level scales are com-
mon (Callison-Burch et al, 2007; Denkowski and
Lavie, 2010). However, the interval-level scale
commonly used for direct estimation of translation
quality (and other NLP annotation tasks) forces
human judges to discretize their assessments into
a fixed number of categories, and this process
could be a cause of inconsistency in human judg-
ments. In particular, an assessor may be repeatedly
forced to choose between two categories, neither
of which really fits their judgment. The contin-
uous nature of translation quality assessment, as
well as the fact that many statistical methods ex-
ist that can be applied to continuous data but not
interval-level data, motivates our trial of a contin-
uous rating scale.
We use human judgments of translation fluency
as a test case and compare consistency levels when
33
the conventional 5-point interval-level scale and a
continuous visual analog scale (VAS) are used for
human evaluation. We collected data via Ama-
zon?s Mechanical Turk, where the quality of an-
notations is known to vary considerably (Callison-
Burch et al, 2010). As such, we test two quality-
control techniques based on statistical significance
? made possible by the use of the continuous rating
scale ? to intrinsically assess the quality of individ-
ual human judges. The quality-control techniques
are not restricted to fluency judgments and are rel-
evant to more general MT evaluation, as well as
other NLP annotation tasks.
2 Machine Translation Fluency
Measurement of fluency as a component of MT
evaluation has been carried out for a number of
years (LDC, 2005), but it has proven difficult
to acquire consistent judgments, even from ex-
pert assessors. Evaluation rounds such as the an-
nual Workshop on Statistical Machine Translation
(WMT) use human judgments of translation qual-
ity to produce official rankings in shared tasks, ini-
tially using an two-item assessment of fluency and
adequacy as separate attributes, and more recently
by asking judges to simply rank system outputs
against one another according to ?which transla-
tion is better?. However, the latter method also re-
ports low levels of agreement between judges. For
example, the 2007 WMT reported low levels of
consistency in fluency judgments in terms of both
intra-annotator agreement (intra-aa), with a kappa
coefficient of ? = 0.54 (moderate), and inter-
annotator agreement (inter-aa), with ? = 0.25
(slight). Adequacy judgments for the same data
received even lower scores: ? = 0.47 for intra-aa,
and ? = 0.23 for inter-aa.
While concerns over annotator agreement have
seen recent WMT evaluations move away from us-
ing fluency as an evaluation component, there can
be no question that fluency is a useful means of
evaluating translation output. In particular, it is not
biased by reference translations. The use of auto-
matic metrics is often criticized by the fact that
a system that produces a good translation which
happens not to be similar to the reference trans-
lations will be unfairly penalized. Similarly, if
human annotators are provided with one or more
reference sentences, they may inadvertently favor
translations that are similar to those references. If
fluency is judged independently of adequacy, no
reference translation is needed, and the bias is re-
moved.
In earlier work, we consider the possibility
that translation quality is a hypothetical construct
(Graham et al, 2012), and suggest applying meth-
ods of validating measurement of psychological
constructs to the validation of measurements of
translation quality. In psychology, a scale that em-
ploys more items as opposed to fewer is consid-
ered more valid. Under this criteria, a two-item
(fluency and adequacy) scale is more valid than a
single-item translation quality measure.
3 Measurement Scales
Direct estimation methods are designed to elicit
from the subject a direct quantitative estimate of
the magnitude of an attribute (Streiner and Nor-
man, 1989). We compare judgments collected
on a visual analog scale (VAS) to those using an
interval-level scale presented to the human judge
as a sequence of radio-buttons. The VAS was first
used in psychology in the 1920?s, and prior to the
digital age, scales used a line of fixed length (usu-
ally 100mm in length), with anchor labels at both
ends, and to be marked by hand with an ?X? at the
desired location (Streiner and Norman, 1989).
When an interval-scale is used in NLP evalua-
tion or other annotation tasks, it is commonly pre-
sented in the form of an adjectival scale, where
categories are labeled in increasing/decreasing
quality. For example, an MT evaluation of fluency
might specify 5 = ?Flawless English?, 4 = ?Good
English?, 3 = ?Non-native English?, 2 = ?Disfluent
English?, and 1 = ?Incomprehensible? (Callison-
Burch et al, 2007; Denkowski and Lavie, 2010).
With both a VAS and an adjectival scale, the
choice of labels can be critical. In medical re-
search, patients? ratings of their own health have
been shown to be highly dependent on the ex-
act wording of descriptors (Seymour et al, 1985).
Alexandrov (2010) provides a summary of the ex-
tensive literature on the numerous issues associ-
ated with adjectival scale labels, including bias
resulting from positively and negatively worded
items not being true opposites of one another, and
items intended to have neutral intensity in fact
proving to have unique conceptual meanings.
Likert scales avoid the problems associated with
adjectival labels, by structuring the question as
a simple statement that the respondent registers
their level of (dis)agreement with. Figure 1 shows
34
Figure 1: Amazon Mechanical Turk interface for fluency judgments with a Likert-type scale.
Figure 2: Continuous rating scale for fluency judgments with two anchors.
the Likert-type interval-level scale we use to col-
lect fluency judgments of MT output, and Fig-
ure 2 shows an equivalent VAS using the two
most extreme anchor labels, strongly disagree and
strongly agree.
4 Crowd-sourcing Judgments
The volume of judgments required for evaluation
of NLP tasks can be large, and employing experts
to undertake those judgments may not always be
feasible. Crowd-sourcing services via the Web of-
fer an attractive alternative, and have been used in
conjunction with a range of NLP evaluation and
annotation tasks. Several guides exist for instruct-
ing researchers from various backgrounds on us-
ing Amazon?s Mechanical Turk (AMT) (Gibson et
al., 2011; Callison-Burch, 2009), and allowance
for the use of AMT is increasingly being made
in research grant applications, as a cost-effective
way of gathering data. Issues remain in connec-
tion with low payment levels (Fort et al, 2011);
nevertheless, Ethics Approval Boards are typically
disinterested in projects that make use of AMT, re-
garding AMT as being a purchased service rather
than a part of the experimentation that may affect
human subjects.
The use of crowd-sourced judgments does,
however, introduce the possibility of increased in-
consistency, with service requesters typically hav-
ing no specific or verifiable knowledge about any
given worker. Hence, the possibility that a worker
is acting in good faith but not performing the task
well must be allowed for, as must the likelihood
that some workers will quite ruthlessly seek to
minimize the time spent on the task, by deliber-
ately giving low-quality or fake answers. Some
workers may even attempt to implement auto-
mated responses, so that they get paid without hav-
ing to do the work they are being paid for.
For example, if the task at hand is that of assess-
ing the fluency of text snippets, it is desirable to
employ native speakers. With AMT the requester
has the ability to restrict responses to only workers
who have a specified skill. But that facility does
not necessarily lead to confidence ? there is noth-
ing stopping a worker employing someone else
to do the test for them. Devising a test that reli-
ably evaluates whether or not someone is a native
speaker is also not at all straightforward.
Amazon allow location restrictions, based on
the registered residential address of the Turker,
which can be used to select in favor of those likely
to have at least some level of fluency (Callison-
Burch et al, 2010). We initially applied this re-
striction to both sets of judgments in experiments,
setting the task up so that only workers regis-
tered in Germany could evaluate the to-German
translations, for example. However, very low re-
35
sponse rates for languages other than to-English
were problematic, and we also received a number
of apparently-genuine requests from native speak-
ers residing outside the target countries. As a
result, we removed all location restrictions other
than for the to-English tasks.1
Crowd-sourcing judgments has the obvious risk
of being vulnerable to manipulation. On the other
hand, crowd-sourced judgments also offer the po-
tential of being more valid than those of experts,
since person-in-the-street abilities might be a more
useful yardstick for some tasks than informed aca-
demic judgment, and because a greater number of
judges may be available.
Having the ability to somehow evaluate the
quality of the work undertaken by a Turker is thus
highly desirable. We would like to be able to put
in place a mechanism that filters out non-native
speakers; native speakers with low literacy levels;
cheats; and robotic cheats. That goal is considered
in the next section.
5 Judge-Intrinsic Quality Control
One common method of quality assessment for a
new process is to identify a set of ?gold-standard?
items that have been judged by experts and whose
merits are agreed, present them to the new process
or assessor, and then assess the degree to which
the new process and the experts ?agree? on the
outcomes (Snow et al, 2008; Callison-Burch et
al., 2010). A possible concern is that even experts
can be expected to disagree (and hence have low
inter-aa levels), meaning that disagreement with
the new process will also occur, even if the new
process is a reliable one. In addition, the qual-
ity of the judgments collected is also assessed via
agreement levels, meaning that any filtering based
on a quality-control measure that uses agreement
will automatically increase consistency, even to
the extent of recalibrating non-expert workers? re-
sponses to more closely match expert judgments
(Snow et al, 2008). Moreover, if an interval-level
scale is used, standardized scores cannot be em-
ployed, so a non-expert who is more lenient than
the experts, but in a reliable and systematic man-
ner, might still have their assessments discarded.
For judgments collected on a continuous scale,
statistical tests based on difference of means (over
assessors) are possible. We structure our human
1It has also been suggested that AMT restricts Turker reg-
istration by country; official information is unclear about this.
T1 initial :
T1 repeat :
d1 
Bad Ref for T2 :
d2 
T2 initial :
Figure 3: Intrinsic quality-control distributions for
an individual judge.
intelligence tasks (HITs) on Mechanical Turk in
groups of 100 in a way that allows us to control
assignment of repeat item pairs to workers, so that
statistical tests can later be applied to an individ-
ual worker?s score distributions for repeat items.
Workers were made aware of the task structure
before accepting it ? the task preview included a
message This HIT consists of 100 fluency assess-
ments, you have 0 so far complete.
We refer to the repeat items in a HIT as
ask again translations. In addition, we inserted a
number of bad reference pairs into each HIT, with
a bad reference pair consisting of a genuine MT
system output, and a distorted sentence derived
from it, expecting that its fluency was markedly
worse than that of the corresponding system out-
put. This was done by randomly selecting two
words in the sentence and duplicating them in ran-
dom locations not adjacent to the original word
and not in the initial or sentence-final position.
Any other degradation method could also be used,
so long as it has a high probability of reducing the
fluency of the text, and provided that it is not im-
mediately obvious to the judges.
Insertion of ask again and bad reference pairs
into the HITs allowed two measurements to be
made for each worker: when presented with
an ask again pair, we expect a conscientious
judge to give similar scores (but when using
a continuous scale, certainly not identical), and
on bad reference pairings a conscientious judge
should reliably give the altered sentence a lower
score. The wide separation of the two appear-
ances of an ask again pair makes it unlikely that
a judge would remember either the sentence or
their first reaction to it, and backwards movement
through the sentences comprising each HIT was
not possible. In total, each HIT contained 100 sen-
36
Figure 4: Welch?s t-test reliability estimates plot-
ted against mean seconds per judgment.
tences, including 10 bad reference pairs, and 10
ask again pairs.
Figure 3 illustrates these two types of pairs,
presuming that over the course of one or more
HITs each worker has assessed multiple ask again
pairs generating the distribution indicated by d1,
and also multiple bad reference pairs, generating
the distribution indicated by d2. As an estimate
of the reliability of each individual judge we ap-
ply a t-test to compare ask again differences with
bad reference differences, with the expectation
that for a conscientious worker the latter should
be larger than the former. Since there is no guar-
antee that the two distributions of d1 and d2 have
the same variance, we apply Welch?s adaptation of
the Student t-test.
The null hypothesis to be tested for each AMT
worker is that the score difference for ask again
pairs is not less than the score difference for
bad reference pairs. Lower p values mean more
reliable workers; in the experiments that are re-
ported shortly, we use p < 0.05 as a threshold
of reliability. We also applied the non-parametric
Mann-Whitney test to the same data, for the pur-
pose of comparison, since there is no guarantee
that d1 and d2 will be normally distributed for a
given assessor.
The next section provides details of the experi-
mental structure, and then describes the outcomes
in terms of their effect on overall system rank-
ings. As a preliminary indication of Turker be-
havior, Figure 4 summarizes some of the data that
was obtained. Each plotted point represents one
AMT worker who took part in our experiments,
and the horizontal axis reflects their average per-
judgment time (noting that this is an imprecise
measurement, since they may have taken phone
calls or answered email while working through a
HIT, or simply left the task idle to help obscure
a lack of effort). The vertical scale is the p value
obtained for that worker when the ask again distri-
bution is compared to their bad reference distribu-
tion, with a line at p = 0.05 indicating the upper
limit of the zone for which we are confident that
they had a different overall response to ask again
pairs than they did to bad reference pairs. Note the
small number of very fast, very inaccurate work-
ers at the top left; we have no hesitation in call-
ing them unconscientious (and declining to pay
them for their completed HITs). Note also the very
small number of workers for which it was possi-
ble to reliably distinguish their ask again behavior
from their bad reference behavior.
6 Experiments
HIT Structure
A sample of 560 translations was selected at
random from the WMT 2012 published shared
task dataset for a range of language pairs, with
segments consisting of 70 translations, each as-
signed to a total of eight distinct HITs. The sen-
tences were generated as image files, as recom-
mended for judgment of translations (Callison-
Burch, 2009). Each HIT was presented to a worker
as a set of 100 sentences including a total of 30
quality control items, with only one sentence visi-
ble on-screen at any given time. Each quality con-
trol item comprised a pair of corresponding trans-
lations, widely separated within the HIT. Three
kinds of quality control pairs were used:
? ask again: system output and exact repeat;
? bad reference: system output and an altered
version of it with noticeably lower fluency;
and
? good reference: system output and the corre-
sponding human produced reference transla-
tion (as provided in the released WMT data).
Each HIT consisted of 10 groups, each containing
10 sentences: 7 ?normal? translations, plus one
of each type of quality control translation drawn
37
from one of the other groups in the HIT in such
a way that 40?60 judgments would be completed
between the elements of any quality-control pair.
Consistency of Human Judgments
Using judgments collected on the continuous rat-
ing scale, we first examine assessor consistency
based on Welch?s t-test and the non-parametric
Mann-Whitney U-test. In order to examine the de-
gree to which human assessors assign consistent
scores, we compute mean values of d1 (Figure 3)
when ask again pairs are given to the same judge,
and across pairs of judges. Three sets of results
are shown: the raw unfiltered data; data filtered
according to p < 0.05 according to the quality-
control regime described in the previous section
using the Welch?s t-test; and data filtered using
the Mann-Whitney U-test. Table 1 shows that the
t-test indicates that only 13.1% of assessors meet
quality control hurdle, while a higher proportion,
35.7%, of assessors are deemed acceptable.
The stricter filter, Welch?s t-test, yields more
consistent scores for same-judge repeat items: de-
creases of 4.5 (mean) and 4.2 (sd) are observed
when quality control is applied. In addition, re-
sults for Welch?s t-test show high levels of con-
sistency for same-judge repeat items: an average
difference of only 9.5 is observed, which is not
unreasonable, given that the scale is 100 points
in length and a 10-point difference corresponds to
just 60 pixels on the screen.
For repeat items rated by distinct judges, both
filtering methods decrease the mean difference in
scores compared to the unfiltered baseline, with
the two tests giving similar improvements.
When an interval-level scale is used to evaluate
the data, the Kappa coefficient is commonly used
to evaluate consistency levels of human judges
(Callison-Burch et al, 2007), where Pr(a) is the
relative observed agreement among raters, and
Pr(e) is the hypothetical probability of chance
agreement:
? =
Pr(a)? Pr(e)
1? Pr(e)
In order to use the Kappa coefficient to compare
agreement levels for the interval-level and contin-
uous scales, we convert continuous scale scores to
a target number of interval categories. We do this
primarily for a target number of five, as this best
provides a comparison between scores for the 5-
point interval-level scale. But we also present re-
sults for targets of four and two categories, since
the continuous scale is marked at the midway and
quarter points, providing implicit intervals. A two-
category is also interesting if the assessment pro-
cess is regarded as dichotomizing to only include
for each translation whether or not the judge con-
sidered it to be ?good? or ?bad?. Use of statisti-
cal difference of means tests on interval-level data
is not recommended; but for the purpose of illus-
tration, we also applied Welch?s t-test to quality
control workers that completed the interval-level
HITs, with the same threshold of p < 0.05.
Tables 2 and 3 show intra-annotator agreement
for the five-point interval scale and continuous
scales, with and without quality control.2 Results
for repeat items on the interval-level scale show
that quality control only alters intra-aa marginally
(Pr(a) increases by 1%), and that inter-aa levels
worsen (Pr(a) decreases by 6.2%). This confirms
that applying statistical tests to interval-level data
is not a suitable way of filtering out low quality
workers.
When comparing consistency levels of asses-
sors using the interval-level scale to those of the
continuous scale, we observe marginally lower ?
coefficients for both intra-aa (?0.009) and inter-
aa (?0.041) for the continuous scale. However,
this is likely to be in part due to the fact that the
continuous scale corresponds more intuitively to 4
categories, and agreement levels for the unfiltered
4-category continuous scale are higher than those
collected on the interval-level scale by +0.023
intra-aa and +0.014 inter-aa.
Applying quality-control on the continuous
scale results in dramatic increases in intra-aa lev-
els: +0.152 for 5-categories (5-cat), +0.100 for
4-categories (4-cat) and +0.096 for 2-categories
(2-cat). When considering inter-aa levels, quality-
control does not directly result in as dramatic an
increase, as inter-aa levels increase by +0.010
for 5-cat, +0.006 for 4-cat and +0.004 for 2-cat.
It is likely, however, that apparent disagreement
between assessors might be due to different as-
sessors judging fluency generally worse or better
than one another. The continuous scale allows for
scores to be standardized by normalizing scores
with respect to the mean and standard deviation
of all scores assigned by a given individual judge.
We therefore transform scores of each judge into
2Note that the mapping from continuous scores to cate-
gories was not applied for quality control.
38
same judge distinct judges
workers judgments mean sd mean sd
Unfiltered 100.0% 100.0% 14.0 18.4 28.9 23.5
Welch?s t-test 13.1% 23.5% 9.5 14.2 25.2 21.0
Mann-Whitney U-test 35.7% 48.8% 13.1 17.7 25.0 22.6
Table 1: Mean and standard deviation of score differences for continuous scale with ask again items
within a given judge and across two distinct judges, for no quality control (unfiltered), Welch?s t-test and
Mann-Whitney U-test with a quality-control threshold of p < 0.05.
# 5-pt. interval 5-pt. interval continuous continuous
categ- unfiltered filtered unfiltered filtered
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 60.4% 0.505 61.4% 0.517 59.7% 0.496 71.8% 0.647
4 - - - - 64.6% 0.528 72.1% 0.629
2 - - - - 85.2% 0.704 90.0% 0.800
Table 2: Intra-annotator (same judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
corresponding z-scores and use percentiles of the
combined set of all scores to map z-scores to cat-
egories where a score falling in the bottom 20 th
percentile corresponds to strongly disagree, scores
between the 20 th and 40 th percentile to disagree,
and so on. Although this method of transformation
is somewhat harsh on the continuous scale, since
scores no longer correspond to different locations
on the original scale, it nevertheless shows an in-
crease in consistency of +0.05 (5-cat), +0.086 (4-
cat) and +0.144 (2-cat). However, caution must
be taken when interpreting consistency for stan-
dardized scores, as can be seen from the increase
in agreement observed when unfiltered scores are
standardized.
Table 4 shows a breakdown by target language
of the proportion of judgments collected whose
scores met the significance threshold of p < 0.05.
Results appear at first to have shockingly low lev-
els of high quality work, especially for English and
German. When running the tasks in Mechanical
Turk, it is worth noting that we did not adopt statis-
tical tests to automatically accept/reject HITs and
we believe this would be rather harsh on workers.
Our method of quality control is a high bar to reach
and it is likely that many workers that do not meet
the significance threshold would still have been
working in good faith. In practice, we individually
examined mean scores for reference translation,
system outputs and bad reference pairs, and only
declined payment when there was no doubt the re-
English German French Spanish
10.0% 0% 57.9% 62.5%
Table 4: High quality judgments, by language.
sponse was either automatic or extremely careless.
The structure of the task and the fact that the
quality-control items were somewhat hidden may
have lulled workers into a false sense of compla-
cency, and perhaps encouraged careless responses.
However, even taking this into consideration, the
fact that none of the German speaking asses-
sors and just 10% of English speaking assessors
reached our standards serves to highlight the im-
portance of good quality-control techniques when
employing services like AMT. In addition, the risk
of getting low quality work for some languages
might be more risky than for others. The response
rate for high quality work for Spanish and French
was so much higher than German and English,
perhaps by chance, or perhaps the result of factors
that will be revealed in future experimentation.
System Rankings
As an example of the degree to which system
rankings are affected by applying quality control,
for the language direction for which we achieved
the highest number of high quality assessments,
English-to-Spanish, we include system rankings
by mean score with each measurement scale, with
and without quality control and for mean z-scores
39
# 5-pt. interval 5-pt. interval continuous continuous cont. standrdzed. cont. standrdzed.
categ- unfiltered qual.-controlled unfiltered qual.-controlled unfiltered qual.-controlled
ories Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ? Pr(a) ?
5 33.0% 0.16 26.8% 0.084 29.5% 0.119 30.3% 0.128 30.2% 0.1272 33.5% 0.169
4 - - - - 38.1% 0.174 38.5% 0.180 35.5% 0.1403 44.5% 0.260
2 - - - - 66.5% 0.331 66.8% 0.335 75.5% 0.5097 73.8% 0.475
Table 3: Inter-annotator (distinct judge) agreement levels for 5-point interval and continuous scales for
unfiltered judgments and judgments of workers with p < 0.05 for Welch?s t-test.
z-scores
5-pt. 5-pt. continuous continuous continuous
unfiltered qual.-controlled unfiltered qual.-controlled qual.-controlled
Sys A 2.00 Sys A 2.00 Sys E 69.60 Sys E 74.39 Sys E 0.43
Sys B 1.98 Sys D 1.97 Sys B 61.78 Sys F 65.07 Sys B 0.16
Sys C 1.98 Sys F 1.95 Sys G 60.21 Sys G 64.51 Sys G 0.08
Sys D 1.98 Sys C 1.95 Sys F 59.38 Sys B 63.68 Sys D 0.06
Sys E 1.98 Sys E 1.95 Sys D 59.05 Sys D 63.52 Sys C 0.02
Sys F 1.97 Sys B 1.94 Sys A 57.44 Sys C 61.33 Sys F 0.01
Sys G 1.97 Sys G 1.93 Sys I 56.31 Sys A 58.43 Sys H ?0.03
Sys H 1.96 Sys H 1.90 Sys C 55.82 Sys I 57.46 Sys I ?0.07
Sys I 1.96 Sys I 1.88 Sys H 55.27 Sys H 57.04 Sys A ?0.10
Sys J 1.94 Sys J 1.81 Sys J 50.46 Sys J 50.73 Sys J ?0.23
Sys K 1.90 Sys K 1.76 Sys K 44.62 Sys K 41.25 Sys K ?0.47
Table 5: WMT system rankings based on approximately 80 randomly-selected fluency judgments per
system, with and without quality control for radio button and continuous input types, based on German-
English. The quality control method applied is annotators who score worsened system output and gen-
uine system outputs with statistically significant lower scores according to paired Student?s t-test.
when raw scores are normalized by individual as-
sessor mean and standard deviation. The results
are shown in Table 5. (Note that we do not claim
that these rankings are indicative of actual system
rankings, as only fluency of translations was as-
sessed, using an average of just 55 translations per
system.)
When comparing system rankings for unfiltered
versus quality-controlled continuous scales, firstly
the overall difference in ranking is not as dramatic
as one might expect, as many systems retain the
same rank order, with only a small number of sys-
tems changing position. This happens because
random-clickers cannot systematically favor any
system, and positive and negative random scores
tend to cancel each other out. However, even hav-
ing two systems ordered incorrectly is of concern;
careful quality control, and the use of normaliza-
tion of assessors? scores may lead to more consis-
tent outcomes. We also note that incorrect system
orderings may lead to flow-on effects for evalua-
tion of automatic metrics.
The system rankings in Table 5 also show how
the use of the continuous scale can be used to rank
systems according to z-scores, so that individual
assessor preferences over judgments can be ame-
liorated. Interestingly, the system that scores clos-
est to the mean, Sys F, corresponds to the baseline
system for the shared task with a z-score of 0.01.
7 Conclusion
We have compared human assessor consistency
levels for judgments collected on a five-point
interval-level scale to those collected on a contin-
uous scale, using machine translation fluency as
a test case. We described a method for quality-
controlling crowd-sourced annotations that results
in marked increases in intra-annotator consistency
and does not require judges to agree with experts.
In addition, the use of a continuous scale allows
scores to be standardized to eliminate individual
judge preferences, resulting in higher levels of
inter-annotator consistency.
40
Acknowledgments
This work was funded by the Australian Research
Council. Ondr?ej Bojar, Rosa Gog, Simon Gog,
Florian Hanke, Maika Vincente Navarro, Pavel
Pecina, and Djame Seddah provided translations
of task instructions, and feedback on published
HITs.
References
A. Alexandrov. 2010. Characteristics of single-item
measures in Likert scale format. The Electronic
Journal of Business Research Methods, 8:1?12.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine
translation. In Proc. 2nd Wkshp. Statistical Machine
Translation, pages 136?158, Prague, Czech Repub-
lic.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz,
and J. Schroeder. 2008. Further meta-evaluation of
machine translation. In Proc. 3rd Wkshp. Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. 4th Wkshp. Statistical Machine Translation,
pages 1?28, Athens, Greece.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 Joint Workshop on Statistical Machine Trans-
lation and Metrics for Machine Translation. In Proc.
5th Wkshp. Statistical Machine Translation, pages
17?53, Uppsala, Sweden.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. 6th Wkshp. Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada.
C. Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. Conf. Empirical Methods in
Natural Language Processing, pages 286?295, Sin-
gapore.
M. Denkowski and A. Lavie. 2010. Choosing the right
evaluation for machine translation: An examination
of annotator and automatic metric performance on
human judgement tasks. In Proc. 9th Conf. Assoc.
Machine Translation in the Americas (AMTA), Den-
ver, Colorado.
K. Fort, G. Adda, and K. B. Cohen. 2011. Amazon
Mechanical Turk: Gold mine or coal mine? Com-
putational Linguistics, 37(2):413?420.
E. Gibson, S. Piantadosi, and K. Fedorenko. 2011. Us-
ing Mechanical Turk to obtain and analyze English
acceptability judgments. Language and Linguistics
Compass, 5/8:509?524.
Y. Graham, T. Baldwin, A. Harwood, A. Moffat, and
J. Zobel. 2012. Measurement of progress in ma-
chine translation. In Proc. Australasian Language
Technology Wkshp., pages 70?78, Dunedin, New
Zealand.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Technical report, Linguistic Data Consortium. Re-
vision 1.5.
R. A. Seymour, J. M. Simpson, J. E. Charlton, and
M. E. Phillips. 1985. An evaluation of length and
end-phrase of visiual analogue scales in dental pain.
Pain, 21:177?185.
R. Snow, B. O?Connor, D. Jursfsky, and A. Y. Ng.
2008. Cheap and fast ? but is it good? Evalu-
ating non-expert annotations for natural language
tasks. In Proc. Conf. Empirical Methods in Natu-
ral Language Processing, pages 254?263, Honolulu,
Hawaii.
D. L. Streiner and G. R. Norman. 1989. Health Mea-
surement Scales: A Practical Guide to their Devel-
opment and Use. Oxford University Press, fourth
edition.
41
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 17?25,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Accurate Language Identification of Twitter Messages
Marco Lui and Timothy Baldwin
NICTA VRL
Department of Computing and Information Systems
University of Melbourne, VIC 3010, Australia
mhlui@unimelb.edu.au, tb@ldwin.net
Abstract
We present an evaluation of ?off-the-
shelf? language identification systems as
applied to microblog messages from Twit-
ter. A key challenge is the lack of an ad-
equate corpus of messages annotated for
language that reflects the linguistic diver-
sity present on Twitter. We overcome this
through a ?mostly-automated? approach to
gathering language-labeled Twitter mes-
sages for evaluating language identifica-
tion. We present the method to con-
struct this dataset, as well as empirical
results over existing datasets and off-the-
shelf language identifiers. We also test
techniques that have been proposed in the
literature to boost language identification
performance over Twitter messages. We
find that simple voting over three specific
systems consistently outperforms any spe-
cific system, and achieves state-of-the-art
accuracy on the task.
1 Introduction
Twitter
1
has captured the attention of various re-
search communities as a potent data source, be-
cause of the immediacy of the information pre-
sented, the volume and variability of the data con-
tained, the potential to analyze networking effects
within the data, and the ability to (where GPS
data is available) geolocate messages (Krishna-
murthy et al., 2008). Although individual mes-
sages range from inane through mundane right up
to insane, the aggregate of these messages can lead
to profound insights in real-time. Examples in-
clude real-time detection of earthquakes (Sakaki
1
http://www.twitter.com
et al., 2010), analysis of the location and preva-
lence of flu epidemics (Lampos et al., 2010; Cu-
lotta, 2010), news event detection (Petrovi?c et al.,
2010), and prediction of sporting match outcomes
(Sinha et al., 2013).
Text analysis of social media has quickly be-
come one of the ?frontier? areas of Natural Lan-
guage Processing (NLP), with major conferences
opening entire tracks for it in recent years. The
challenges in NLP for social media are many,
stemming primarily from the ?noisy? nature of the
content. Research indicates that English Twitter
in particular is more dissimilar to the kinds of ref-
erence corpora used in NLP to date, compared
to other forms of social media such as blogs and
comments (Baldwin et al., 2013). This has led
to the development of techniques to ?normalize?
Twitter messages (Han et al., 2013), as well as
Twitter-specific approaches to conventional NLP
tasks such as part-of-speech tagging (Gimpel et
al., 2011) and information extraction (Bontcheva
et al., 2013). Even so, a precondition of NLP
techniques is that the language of the input data
is known, and this has led to interest in ?language
identification? (LangID) of Twitter messages. Re-
search has shown that ?off-the-shelf? LangID sys-
tems appear to perform fairly well on Twitter (Lui
and Baldwin, 2012), but Twitter-specific systems
seem to perform better (Carter et al., 2013; Tromp
and Pechenizkiy, 2011; Bergsma et al., 2012;
Goldszmidt et al., 2013).
Twitter recognizes the utility of language meta-
data in enabling new applications, and as of March
2013 includes language predictions with results
from its API (Roomann-Kurrik, 2013). These pre-
dictions are not perfect (see Section 3.2), and at
time of writing do not cover some languages (e.g.
Romanian). Furthermore, some research groups
17
have collected a substantial cache of Twitter data
from before the availability of built-in predictions.
Motivated by the need to work with monolingual
subsets of historical data, we investigate the most
practical means of carrying out LangID of Twitter
messages, balancing accuracy with ease of imple-
mentation. In this work, we present an evaluation
of ?off-the-shelf? language identifiers, combined
with techniques that have been proposed for boost-
ing accuracy on Twitter messages.
A major challenge that we have had to over-
come is the lack of annotated data for evaluation.
Bergsma et al. (2012) point out that in LangID
research on microblog messages to date, only a
small number of European languages has been
considered. Baldwin and Lui (2010) showed that,
when considering full documents, good perfor-
mance on just European languages does not nec-
essarily imply equally good performance when a
larger set of languages is considered. This does
not detract from work to date on European lan-
guages (Tromp and Pechenizkiy, 2011; Carter et
al., 2013), but rather highlights the need for fur-
ther research in LangID for microblog messages.
Manual annotation of Twitter messages is a
challenging and laborious process. Furthermore,
Twitter is highly multilingual, making it very dif-
ficult to obtain annotators for all of the languages
represented. Previous work has attempted to
crowdsource part of this process (Bergsma et al.,
2012), but such an approach requires substantial
monetary investment, as well as care in ensuring
the quality of the final annotations. In this paper,
we propose an alternative, ?mostly-automated?
approach to gathering language-labeled Twitter
messages for evaluating LangID. A corpus con-
structed by direct application of automatic LangID
to Twitter messages would obviously be unsuit-
able for evaluating the accuracy of LangID tools.
Even with manual post-filtering, the remaining
dataset would be biased towards messages that
are easy for automated systems to classify cor-
rectly. The novelty of our approach is to leverage
user identity, allowing us to construct a corpus of
language-labeled Twitter messages without using
automated tools to determine the languages of the
messages. This quality makes the corpus suitable
for use in the evaluation of automated LangID of
Twitter messages.
Our main contributions are: (1) we provide
a manually-labeled dataset of Twitter messages,
adding Chinese (zh) and Japanese (ja) to the set of
Twitter messages with human annotation for lan-
guage; (2) we provide a second dataset constructed
using a mostly-automated approach, covering 65
languages; (3) we detail the method for construct-
ing the dataset; (4) we provide a comprehensive
empirical evaluation of the accuracy of off-the-
shelf LangID systems on Twitter messages, using
published datasets in addition to the new datasets
we have introduced; and (5) we discuss and eval-
uate a simple voting-based ensemble for LangID,
and find that it outperforms any individual system
to achieve state-of-the-art results.
2 Background
LangID is the problem of mapping a document
onto the language(s) it is written in. The best-
known technique classifies documents according
to rank order statistics over character n-gram se-
quences between a document and a global lan-
guage profile (Cavnar and Trenkle, 1994). Other
statistical approaches applied to LangID include
Markov models over n-gram frequency profiles
(Dunning, 1994), dot products of word frequency
vectors (Darnashek, 1995), and string kernels
in support vector machines (Kruengkrai et al.,
2005). In contrast to purely statistical meth-
ods, linguistically-motivated models for LangID
have also been proposed, such as the use of stop
word lists (Johnson, 1993), where a document is
classified according to its degree of overlap with
lists for different languages. Other approaches
include word and part-of-speech (POS) corre-
lation (Grefenstette, 1995), cross-language tok-
enization (Giguet, 1995) and grammatical-class
models (Dueire Lins and Gonc?alves, 2004).
LangID of short strings has attracted recent
interest from the research community. Ham-
marstrom (2007) describes a method that aug-
ments a dictionary with an affix table, and tests
it over synthetic data derived from a parallel bible
corpus. Ceylan and Kim (2009) compare a num-
ber of methods for identifying the language of
search engine queries of 2 to 3 words. They de-
velop a method which uses a decision tree to in-
tegrate outputs from several different LangID ap-
proaches. Vatanen et al. (2010) focus on mes-
sages of 5?21 characters, using n-gram language
models over data drawn from UDHR in a naive
Bayes classifier. Carter et al. (2013) focus specifi-
cally on LangID in Twitter messages by augment-
18
ing standard methods with LangID priors based
on a user?s previous messages and the content
of links embedded in messages, and this is also
the method used in TwitIE (Bontcheva et al.,
2013). Tromp and Pechenizkiy (2011) present
a method for LangID of short text messages by
means of a graph structure, extending the stan-
dard ?bag? model of text to include information
about the relative order of tokens. Bergsma et
al. (2012) examine LangID for creating language-
specific twitter collections, finding that a compres-
sive method trained over out-of-domain data from
Wikipedia and standard text corpora performed
better than the off-the-shelf language identifiers
they tested. Goldszmidt et al. (2013) propose
a method based on rank-order statistics, using a
bootstrapping process to acquire in-domain train-
ing data from unlabeled Twitter messages. Recent
work has also put some emphasis on word-level
rather than document-level LangID (Yamaguchi
and Tanaka-Ishii, 2012; King and Abney, 2013),
including research on identifying the language of
each word in multilingual online communications
(Nguyen and Dogruoz, 2013; Ling et al., 2013).
In this paper, we focus on monolingual messages,
as despite being simpler, LangID of monolingual
Twitter messages is far from solved.
In Section 1, we discussed some work to date
on LangID on Twitter data. Some authors have re-
leased accompanying datasets: the dataset used by
Tromp and Pechenizkiy (2011) was made avail-
able in its entirety, consisting of 9066 messages
in 6 Western European languages. Other au-
thors have released message identifiers with as-
sociated language labels, including Carter et al.
(2013), with 5000 identifiers in 5 Western Euro-
pean languages, and Bergsma et al. (2012), pro-
viding 13190 identifiers across 9 languages from
3 language families (Arabic, Cyrillic and Devana-
gari). To date, only the dataset of Tromp and
Pechenizkiy (2011) has been used by other re-
searchers (Goldszmidt et al., 2013). With the kind
co-operation of the authors, we have obtained the
full datasets of Carter et al. (2013) and Bergsma
et al. (2012), allowing us to present the most ex-
tensive empirical evaluation of LangID of Twitter
messages to date. However, the total set of lan-
guages covered is still very small. In Section 2.1,
we present our own manually-annotated dataset,
adding Chinese (zh) and Japanese (ja) to the lan-
guages that have manually-annotated data.
English Chinese Japanese
Initial 0.906 0.773 0.989
Post-review 0.930 0.916 0.998
Table 1: Inter-annotator agreement measured us-
ing Fleiss? kappa (Fleiss, 1971) over annotations
for TWITTER.
2.1 Manual annotation of ZHENJA
A manual approach to constructing a LangID
dataset from Twitter data is difficult due to the
wide variety of languages present on Twitter ?
Bergsma et al. (2012) report observing 65 lan-
guages in a 10M message sample, and Baldwin
et al. (2013) report observing 97 languages in a
1M message sample. While this is encouraging
in terms of sourcing data for lower-density lan-
guages, the distribution of languages is Zipfian,
and the relative proportion of data in most lan-
guages is very small. Manually retrieving all avail-
able messages in a language would require a na-
tive speaker to view and reject a huge number
of messages in other languages in order to col-
lect the small number that are written in the tar-
get language. We initially attempted this, build-
ing ZHENJA, a dataset derived from a set of 5000
messages randomly sampled from a larger body
of 622192 messages collected from the Twitter
streaming API over a single 24-hour period in Au-
gust 2010. The messages are a 1% representative
sample of the total public messages posted on that
day. Each of the 5000 selected messages was an-
notated by speakers of three languages, English,
Japanese and Mandarin Chinese. For each mes-
sage, three annotators were asked if the message
contained any text in languages which they spoke,
as well as if it appeared to contain text in (unspeci-
fied) languages which they did not speak. The lat-
ter label was introduced in order to make a distinc-
tion between text in languages not spoken by our
annotators (e.g. Portuguese) and text with no lin-
guistic content (e.g. URLs). After the initial anno-
tation, annotators were asked to review messages
where there was disagreement, and messages were
assigned labels given by a majority of annotators
post-review. Inter-annotator agreement (Table 1)
is strong for the task: only 20 out of 5000 mes-
sages have less than 80% majority in annotations.
In many instances, the disagreement was due to
messages consisting entirely of a short sequence
of hanzi/kanji, which both Chinese and Japanese
speakers recognized as valid (these messages are
19
excluded from our set of labeled messages). Out
of the 5000 messages, 1953 (39.1%) were labeled
as English, 16 were labeled as Chinese (0.3%) and
1047 were labeled as Japanese (20.9%), for a total
of 3016 labeled messages.
A total of 8 annotators each invested 2?4 hours
in this annotation task, and the final dataset only
covers 3 languages (which includes the top-2
highest-density languages in Twitter). Obviously,
constructing a dataset of language-labeled Twit-
ter messages is a labor-intensive process, and the
lower density the language, the more expensive
our methodology becomes (as more and more doc-
uments need to be looked over to find documents
in the language of interest). Ideally, we would like
to be able to use some form of automated LangID
to accelerate the process without biasing the data
towards easy-to-classify messages.
2.2 A broad-coverage Twitter corpus
Based on our discussion so far, our desiderata for a
LangID dataset of Twitter messages are as follows:
(1) achieve broader coverage of languages than ex-
isting datasets; (2) minimize manual annotation;
and (3) avoid bias induced by selecting messages
using LangID. (2) and (3) may seem to be con-
flicting objectives, but we sidestep the problem by
first identifying monolingual users, then produce a
dataset by sampling messages by these users from
a held-out collection.
The overall workflow for constructing a dataset
is summarized in Algorithm 1. For each user we
consider, we divide all their messages into two dis-
joint sets. One set (M
main
u
) is used to determine
the language(s) spoken by the user. If only one
language is detected, the user is added to a pool
of candidate users (U
accept
). A fixed number of
users is sampled for each language (U
sample
), and
for each sampled user a fixed number of messages
is sampled from the held-out set (M
heldout
u
) and
added to the final dataset. We sample a fixed num-
ber of users per language to limit the amount of
data in the more-frequent languages, and we only
sample a small number of messages per user in
order to avoid biasing the dataset towards the lin-
guistic idiosyncrasies of any specific individual.
For both sampling steps, if the number of items
available is less than the number required, all the
available items are returned.
Algorithm 1 uses automated LangID to detect
the language of messages in M
main
u
(line 8). The
Algorithm 1 Procedure for building a Twitter
LangID dataset
1: U ? active users
2: L
accept
,M
accept
, U
accept
? {}, {}, {}
3: for each u ? U do
4: M
u
? all messages by user u
5: M
main
u
,M
heldout
u
? RandomSplit(M
u
)
6: L
u
? {}
7: for each m ?M
main
u
do
8: l
u
? LangID(m)
9: if l
u
6= unknown then
10: L
u
? L
u
? {l
u
}
11: end if
12: end for
13: if len(L
u
) = 1 then
14: U
accept
? U
accept
? {(u, L
u
)}
15: L
accept
? L
accept
? L
u
16: end if
17: end for
18: for each l ? L
accept
do
19: U
sample
? Sample(U
accept
l
,K)
20: for each u ? U
sample
do
21: M
sample
? Sample(M
heldout
u
, N)
22: M
accept
?M
accept
? {(M
sample
, l)}
23: end for
24: end for
25: return M
accept
accuracy of this identifier is not critical, as any
misclassifications for a monolingual user would
cause them to be rejected, as they would appear
multilingual. Hence, the risk of false positives at
the user-level LangID is very low. However, in-
correctly rejecting users reduces the pool of data
available for sampling, so a higher-accuracy solu-
tion is preferable. We compared the performance
of 8 off-the-shelf (i.e. pre-trained) LangID systems
to determine which would be the most suitable for
this role.
langid.py (Lui and Baldwin, 2012): an n-
gram feature set selected using data from multi-
ple sources, combined with a multinomial naive
Bayes classifier.
CLD2 (McCandless, 2010): the language iden-
tifier embedded in the Chrome web browser;
2
it
uses a naive Bayes classifier and script-specific to-
kenization strategies.
LangDetect (Nakatani, 2010): a naive Bayes
classifier, using a character n-gram based repre-
sentation without feature selection, with a set of
normalization heuristics to improve accuracy.
LDIG (Nakatani, 2012): a Twitter-specific
LangID tool, which uses a document representa-
tion based on tries, combined with normalization
2
http://www.google.com/chrome
20
heuristics and Bayesian classification, trained on
Twitter data.
whatlang (Brown, 2013): a vector-space
model with per-feature weighting over character
n-grams.
YALI (Majli
?
s, 2012): computes a per-language
score using the relative frequency of a set of byte
n-grams selected by term frequency.
TextCat (Scheelen, 2003); an implementation
of Cavnar and Trenkle (1994), which uses an ad-
hoc rank-order statistic over character n-grams.
MSR-LID (Goldszmidt et al., 2013): based on
rank-order statistics over character n-grams, and
Spearman?s ? to measure correlation. Twitter-
specific training data is acquired through a boot-
strapping approach. We use the 49-language
model provided by the authors, and the best pa-
rameters reported in the paper.
We investigated the performance of the systems
using manually-labeled datasets of Twitter mes-
sages (Table 2), including the ZHENJA set de-
scribed in Section 2.1.
3
We find that all the sys-
tems tested perform well on TROMP, with the
exception of TextCat. CARTER covers a very
similar set of languages to TROMP, yet all sys-
tems consistently perform worse on it. This sug-
gests that TROMP is biased towards messages that
LangID systems are likely to identify correctly
(also observed by Goldszmidt et al. (2013)). This
is due in part to the post-processing applied to the
messages, but also suggests a bias in how mes-
sages were selected. LDIG is the best performer
on TROMP and CARTER, albeit falling slightly
short of the 99.1% accuracy reported by the author
(Nakatani, 2012). However, it is only trained on
17 languages and thus is not able to fully support
BERGSMA and ZHENJA, and so we cannot draw
any conclusions on whether the method will gen-
eralize well to more languages. The system that
supports the most languages by far is whatlang,
but as a result its accuracy on Twitter messages
suffers. Manual analysis suggests this is due to
Twitter-specific ?noise? tipping the model in fa-
vor of lower-density languages. On BERGSMA,
LangDetect is the best performer, likely due
to its specific heuristics for distinguishing certain
language pairs (Nakatani, 2010), which happen to
be present in the BERGSMA dataset. Overall, in
3
We do not limit the comparison to languages supported
by each system as this would bias evaluation towards systems
that support few languages that are easy to discriminate.
their off-the-shelf configuration, only three sys-
tems (langid.py, CLD2, LangDetect) per-
form consistently well on LangID of Twitter mes-
sages. Even so, the macro-averaged F-Scores ob-
served were as low as 83%, indicating that whilst
performance is good, the problem of LangID of
Twitter messages is far from solved.
Given that the set of languages covered and ac-
curacy varies between systems, we investigated a
simple voting-based approach to combining the
predictions. For each dataset, we considered all
combinations of 3, 5, and 7 systems, combin-
ing the predictions using a simple majority vote.
The single-best combination for each dataset is re-
ported in Table 3. In all cases, the macro-averaged
F-score is improved upon, showing the effective-
ness of the voting approach. Hence, for purposes
of LangID in Algorithm 1, we chose to use a
majority-vote ensemble of langid.py, CLD2
and LangDetect, a combination that generally
performs well on all datasets.
4
Where all 3 sys-
tems disagree, the message is labeled as unknown,
which does not count as a separate language for
determining if a user is multilingual, mitigating
the risk of wrongly rejecting a monolingual user
due to misclassifying a particular message. This
ensemble is hereafter referred to as VOTING.
To build our final dataset, we collected all mes-
sages by active users from the 1% feed made avail-
able by Twitter over the course of 31 days, be-
tween 8 January 2012 and 7 February 2012. We
deemed users active if they had posted at least
5 messages in a single day on at least 7 differ-
ent days in the 31-day period we collected data
for. This gave us a set of approximately 2M
users. For each user, we partitioned their mes-
sages (RandomSplit in Algorithm 1) by selecting
one day at random. All of the messages posted
by the user on this day were treated as heldout
data (M
heldout
u
), and the remainder of the user?s
messages (M
main
u
) were used to determine the
language(s) spoken by the user. The day cho-
sen was randomly selected per-user to avoid any
bias that may be introduced by messages from
a particular day or date. Of the active users,
we identified 85.0% to be monolingual, cover-
ing a set of 65 languages. 50.6% of these users
spoke English (en), 14.1% spoke Japanese (ja),
and 13.0% spoke Portuguese (pt); this user-level
4
MSR-LID was excluded due to technical difficulties in
applying it to a large collection of messages because of its
oversized model.
21
Dataset langid.py CLD2 LangDetect LDIG whatlang YALI TextCat MSR-LID
TROMP 0.983 0.972 0.959 0.986 0.950 0.911 0.814 0.983
CARTER 0.917 0.902 0.891 0.943 0.834 0.824 0.510 0.927
BERGSMA 0.847 0.911 0.923 0.000 0.719 0.428 0.046 0.546
ZHENJA 0.871 0.884 0.831 0.315 0.622 0.877 0.313 0.848
Table 2: Macro-averaged F-Score on manually-annotated Twitter datasets. Italics denotes results where
the dataset contains languages not supported by the identifier.
Dataset
Single Best Voting 3-System
System F-Score Systems F-Score F-Score
TROMP LDIG 0.986 CLD2, MSR-LID, LDIG 0.992 0.986
CARTER LDIG 0.943 MSR-LID, langid.py, LDIG 0.948 0.927
BERGSMA LangDetect 0.923 CLD2, LangDetect, langid.py 0.935 0.935
ZHENJA CLD2 0.884 CLD2, MSR-LID, LDIG, YALI, langid.py 0.969 0.941
Table 3: System combination by majority voting. All combinations of 3, 5 and 7 systems were con-
sidered. For each dataset, we report the single-best system, the best combination, and F-score of the
majority-vote combination of langid.py, CLD2 and LangDetect.
language distribution largely mirrors the message-
level language distribution reported by Baldwin et
al. (2013) and others. From this set of users, we
randomly selected up to 100 users per language,
leaving us with a pool of 26011 held-out mes-
sages from 2914 users. Manual inspection of these
messages revealed a number of English messages
mislabeled with another language, indicating that
even predominantly monolingual users occasion-
ally introduce English into their online commu-
nications. Such messages are generally entirely
English, with code-switching (i.e. multiple lan-
guages in the same message) very rarely observed.
In order to eliminate mislabeled messages, we ap-
plied all 8 systems to this pool of 26011 messages.
Where at least 5 systems agree and the predicted
language does not match the user?s language, we
discarded the message. Where 3 or 4 systems
agree, we manually inspected the messages and
eliminated those that were clearly mislabeled (this
is the only manual step in the construction of this
dataset). Overall, we retained 24220 messages
(93.1%). From these, we sampled up to 5 mes-
sages per unique user, producing a final dataset of
14178 messages across 65 languages (hereafter re-
ferred to as the TWITUSER dataset).
3 Evaluating off-the-shelf language
identifiers on Twitter
Given TWITUSER, our broad-coverage Twitter
corpus, we return to the task of examining the
performance of the off-the-shelf LangID systems
we discussed in Section 2.2 (Table 4, left side).
In terms of macro-averaged F-Score across the
full set of 65 languages, CLD2 is the single best-
performing system. Unlike langid.py and
LangDetect, CLD2 does not always produce a
prediction, and instead has an in-built threshold
for it to output a prediction of ?unknown?. This
is reflected in the elevated precision, at the ex-
pense of decreased recall and message-level ac-
curacy. Systems like langid.py which always
make a prediction have reduced precision, bal-
anced by increased recall and message-level ac-
curacy. As with the manually-annotated datasets,
we experimented with a simple voting-based ap-
proach to combining multiple classifiers. We
again experimented with all possible combina-
tions of 3, 5 and 7 classifiers, and found that on
TWITUSER, a majority-vote ensemble of CLD2,
langid.py and LangDetect attains the best
macro-averaged F-Score, and also outperforms
any individual system on all of the metrics con-
sidered. We note that this is exactly the VOTING
ensemble of Section 2.2, validating its choice as
LangID(m) in Algorithm 1.
3.1 Adapting off-the-shelf LangID to Twitter
Tromp and Pechenizkiy (2011) propose to remove
links, usernames, hashtags and smilies before at-
tempting LangID, as they are Twitter specific. We
experimented with applying this cleaning proce-
dure to each message body before passing it to
our off-the-shelf systems (Table 4, right side). For
LDIG and MSR-LID, the results are exactly the
same with and without cleaning. These two sys-
tems are specifically targeted at Twitter messages,
and thus may include a similar normalization as
part of their processing pipeline. This also sug-
gests that the systems do not leverage this Twitter-
22
Tool
Without Cleaning With Cleaning
P R F Acc P R F Acc
langid.py 0.767 0.861 0.770 0.842 0.759 0.861 0.766 0.840
CLD2 0.852 0.814 0.806 0.775 0.866 0.823 0.820 0.780
LangDetect 0.618 0.680 0.626 0.839 0.623 0.687 0.634 0.854
LDIG 0.167 0.239 0.189 0.447 0.167 0.239 0.189 0.447
whatlang 0.749 0.655 0.663 0.624 0.739 0.667 0.663 0.623
YALI 0.441 0.564 0.438 0.710 0.449 0.560 0.443 0.705
TextCat 0.327 0.245 0.197 0.257 0.316 0.295 0.230 0.316
MSR-LID 0.533 0.609 0.536 0.848 0.533 0.609 0.536 0.848
VOTING 0.920 0.876 0.887 0.861 0.919 0.883 0.889 0.868
Table 4: Macro-averaged Precision/Recall/F-Score, as well as message-level accuracy for each system
on TWITUSER. The right side of the table reports results after applying message-level cleaning (Tromp
and Pechenizkiy, 2011).
specific content in making predictions. Other sys-
tems generally show a small improvement with
cleaning, except for langid.py. The VOTING
ensemble also benefits from cleaning, due to the
improvement in two of its component classifiers
(CLD2 and LangDetect). This cleaning pro-
cedure is trivial to implement, so despite the im-
provement being small, it may be worth imple-
menting if adapting off-the-shelf language identi-
fiers to Twitter messages.
Goldszmidt et al. (2013) suggest bootstrap-
ping a Twitter-specific language identifier using
an off-the-shelf language identifier and an unla-
beled collection of Twitter messages. We tested
this approach, using the 3 systems that provide
tools to generate new models from labeled data
(LangDetect, langid.py and TextCat).
We constructed bootstrap collections by: (1) us-
ing the off-the-shelf tools to directly identify
the language of messages; and (2) using Algo-
rithm 1. Overall, the bootstrapped identifiers are
not better than their off-the-shelf counterparts.
For TextCat there is an increase in accuracy
using bootstrapped models, but the accuracy of
TextCat with bootstrapped models is still infe-
rior to LangDetect and langid.py in their
off-the-shelf configuration. For LangDetect,
utilizing bootstrapped models does not always in-
crease the accuracy of LangID of Twitter mes-
sages. Where it does help, the bootstrap collec-
tions that are effective vary with the target dataset.
For langid.py, none of the bootstrapped mod-
els outperformed the off-the-shelf model. This
suggests that for LangID, the same features that
are predictive of language in other domains are
equally applicable to Twitter messages, and that
the cross-domain feature selection procedure pro-
posed utilized by langid.py (Lui and Baldwin,
Dataset Period Proportion
CARTER Jan ? Apr 2010 76.4%
BERGSMA May 2007 ? Feb 2012 92.2%
TWITUSER Jan ? Feb 2012 79.7%
Table 5: Proportion of messages from each dataset
that were still accessible as of August 2013.
2011) is able to identify these features effectively.
Bontcheva et al. (2013) report positive results
from the integration of LangID priors (Carter et
al., 2013), but we did not experiment with them,
as the calculation of priors is relatively expensive
compared to the other adaptations we have con-
sidered, in terms of both run time and developer
effort. Furthermore, there is a number of open is-
sues that are likely to affect the effectiveness of the
priors, such as the size and the scope of the mes-
sage collection used to determine the prior. This
is an interesting avenue of future work but is be-
yond the scope of this particular paper. However,
we observe that priors based on user identity (e.g
the ?Blogger? prior) are likely to be artificially ef-
fective on TWITUSER, because the messages have
been sampled from users that we have identified
as monolingual.
3.2 Twitter API predictions
For CARTER, BERGSMA and TWITUSER, we
have access to the original identifiers for each mes-
sage, which use used to download the messages
via the Twitter API.
5
Table 5 reports the propor-
tion of each dataset that is still accessible as of
August 2013. For the messages that we were able
to recover, the full response from the API now
includes language predictions. We do not report
quantitative results on the accuracy of the Twitter
API predictions as the Twitter API terms of ser-
5
http://dev.twitter.com
23
vice forbid benchmarking (?You will not attempt
... to ... use or access the Twitter API ... for ...
benchmarking or competitive purposes?). Further-
more, any results would be impossible to replicate:
the set of messages that are accessible is likely to
continue to decrease, and the accuracy of Twitter?s
predictions may vary as updates are made to the
API.
Error analysis of the language predictions pro-
vided by the Twitter API shows that at the time
of writing, for the languages supported the accu-
racy of the Twitter API is not substantially better
than the best off-the-shelf language identifiers we
examined in this paper. However, about a quarter
of the languages present in TWITUSER are never
offered as predictions. This has implications for
the precision of LangID in other languages: one
notable example is poor precision in Italian, due
to some Romanian messages being identified as
Italian (no messages are identified as Romanian).
This suggests that caution must be taken in tak-
ing the language predictions offered by the Twit-
ter API as goldstandard. The accuracy of the pre-
dictions is not perfect, and highlights the need for
further research into improving the scope and ac-
curacy of LangID for Twitter messages.
4 Conclusion
In this paper, we presented ZHENJA and TWIT-
USER, two novel datasets of language-labeled
Twitter messages. ZHENJA is constructed us-
ing a conventional manual annotation approach,
whereas TWITUSER is constructed using a novel
mostly-automated method that leverages user
identity. Using these new datasets alongside
three previously-published datasets, we com-
pared 8 off-the-shelf LangID systems over Twit-
ter messages, and found that a simple major-
ity vote across three specific systems (CLD2,
langid.py, LangDetect) consistently out-
performs any individual system. We also found
that removing Twitter-specific content from mes-
sages improves the performance of off-the-shelf
systems. We reported that the predictions provided
by the Twitter API are not better than state-of-the-
art off-the-shelf systems, and that a number of lan-
guages in use on Twitter appear to be unsupported
by the Twitter API, underscoring the need for fur-
ther research to broaden the scope and accuracy of
language identification from Twitter messages.
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identifi-
cation: The long and the short of the matter. In Proceed-
ings of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL HLT 2010),
pages 229?237, Los Angeles, USA.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKin-
lay, and Li Wang. 2013. How noisy social media text,
how diffrnt social media sources? In Proceedings of the
6th International Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton
Fink, and Theresa Wilson. 2012. Language identifica-
tion for creating language-specific Twitter collections. In
Proceedings the Second Workshop on Language in Social
Media (LSM2012), pages 65?74, Montr?eal, Canada.
Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark A.
Greenwood, Diana Maynard, and Niraj Aswani. 2013.
TwitIE: An open-source information extraction pipeline
for microblog text. In Proceedings of Recent Advances
in Natural Language Processing (RANLP 2013), Hissar,
Buglaria.
Ralf Brown. 2013. Selecting and weighting n-grams to
identify 1100 languages. In Proceedings of the 16th in-
ternational conference on text, speech and dialogue (TSD
2013), Plze?n, Czech Republic.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias.
2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Lan-
guage Resources and Evaluation, pages 1?21.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Re-
trieval, pages 161?175, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language iden-
tification of search engine queries. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 1066?1074,
Singapore.
Aron Culotta. 2010. Towards detecting influenza epidemics
by analyzing Twitter messages. In Proceedings of the
KDD Workshop on Social Media Analytics.
Marc Darnashek. 1995. Gauging similarity with n-grams:
Language-independent categorization of text. Science,
267:843?848.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Automatic
language identification of written texts. In Proceedings of
the 2004 ACM Symposium on Applied Computing (SAC
2004), pages 1128?1133, Nicosia, Cyprus.
Ted Dunning. 1994. Statistical identification of language.
Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
24
Joseph L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 76(5):378?
382.
Emmanuel Giguet. 1995. Categorisation according to lan-
guage: A step toward combining linguistic knowledge and
statistical learning. In Proceedings of the 4th Interna-
tional Workshop on Parsing Technologies (IWPT-1995),
Prague, Czech Republic.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael Heil-
man, Dani Yogatama, Jeffrey Flanigan, and Noah A.
Smith. 2011. Part-of-speech tagging for Twitter: An-
notation, features, and experiments. In Proceedings of
the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies (ACL
HLT 2011), pages 42?47, Portland, USA.
Moises Goldszmidt, Marc Najork, and Stelios Paparizos.
2013. Boot-strapping language identifiers for short col-
loquial postings. In Proceedings of the European Confer-
ence on Machine Learning and Principles and Practice of
Knowledge Discovery in Databases (ECMLPKDD 2013),
Prague, Czech Republic.
Gregory Grefenstette. 1995. Comparing two language iden-
tification schemes. In Proceedings of Analisi Statistica dei
Dati Testuali (JADT), pages 263?268, Rome, Italy.
Harald Hammarstrom. 2007. A Fine-Grained Model for
Language Identication. In Proceedings of Improving Non
English Web Searching (iNEWS07), pages 14?20.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lexical
normalization for social media text. ACM Trans. Intell.
Syst. Technol., 4(1):5:1?5:27, February.
Stephen Johnson. 1993. Solving the problem of language
recognition. Technical report, School of Computer Stud-
ies, University of Leeds.
Ben King and Steven Abney. 2013. Labeling the languages
of words in mixed-language documents using weakly su-
pervised methods. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 1110?1119, Atlanta, Georgia.
Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt.
2008. A few chirps about Twitter. In Proceedings of the
First Workshop on Online Social Networks (WOSN 2008),
pages 19?24, Seattle, USA.
Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlert-
lamvanich, and Hitoshi Isahara. 2005. Language identifi-
cation based on string kernels. In Proceedings of the 5th
International Symposium on Communications and Infor-
mation Technologies (ISCIT-2005), pages 896?899, Bei-
jing, China.
Vasileios Lampos, Tijl De Bie, and Nello Cristianini. 2010.
Flu Detector ? tracking epidemics on Twitter. In Pro-
ceedings of the European Conference on Machine Learn-
ing and Principles and Practice of Knowledge Discov-
ery in Databases (ECML PKDD 2010), pages 599?602,
Barcelona, Spain.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Is-
abel Trancoso. 2013. Microblogs as parallel corpora. In
Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers),
pages 176?186, Sofia, Bulgaria.
Marco Lui and Timothy Baldwin. 2011. Cross-domain fea-
ture selection for language identification. In Proceedings
of the 5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2011), pages 553?561, Chiang
Mai, Thailand.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-
the-shelf language identification tool. In Proceedings of
the 50th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2012) Demo Session, pages 25?
30, Jeju, Republic of Korea.
Martin Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at the 13th
Conference of the European Chapter of the Association
for Computational Linguistics, pages 46?54, Avignon,
France.
Michael McCandless. 2010. Accuracy and performance
of google?s compact language detector. blog post. avail-
able at http://blog.mikemccandless.com/
2011/10/accuracy-and-performance-of-
googles.html.
Shuyo Nakatani. 2010. Language detection library
(slides). http://www.slideshare.net/shuyo/
language-detection-library-for-java.
Retrieved on 21/06/2013.
Shuyo Nakatani. 2012. Short text language detec-
tion with infinity-gram. blog post. available at
http://shuyo.wordpress.com/2012/05/
17/short-text-language-detection-with-
infinity-gram/.
Dong Nguyen and A. Seza Dogruoz. 2013. Word level
language identification in online multilingual communi-
cation. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing, pages
857?862, Seattle, USA.
S. Petrovi?c, Miles Osborne, and Victor Lavrenko. 2010.
Streaming first story detection with application to twitter.
In Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL
HLT 2010), pages 181?189, Los Angeles, USA.
Arne Roomann-Kurrik. 2013. Introducing new meta-
data fro tweets. blog post. available at https:
//dev.twitter.com/blog/introducing-
new-metadata-for-tweets.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In Proceedings of the 19th International
Conference on the World Wide Web (WWW 2010), pages
851?860, Raleigh, USA.
Frank Scheelen, 2003. libtextcat. Software avail-
able at http://software.wise-guys.nl/
libtextcat/.
Shiladitya Sinha, Chris Dyer, Kevin Gimpel, and Noah A.
Smith. 2013. Predicting the NFL using Twitter. In
Proceedings of the ECML/PKDD Workshop on Machine
Learning and Data Mining for Sports Analytics, Prague,
Czech Republic.
Erik Tromp and Mykola Pechenizkiy. 2011. Graph-based
n-gram language identification on short texts. In Proceed-
ings of Benelearn 2011, pages 27?35, The Hague, Nether-
lands.
Tommi Vatanen, Jaakko J. Vayrynen, and Sami Virpioja.
2010. Language identification of short text segments
with n-gram models. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evaluation
(LREC 2010), pages 3423?3430.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012. Text
segmentation by language using minimum description
length. In Proceedings the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1: Long
Papers), pages 969?978, Jeju Island, Korea.
25
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266?274,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Randomized Significance Tests in Machine Translation
Yvette Graham Nitika Mathur Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
ygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.net
Abstract
Randomized methods of significance test-
ing enable estimation of the probability
that an increase in score has occurred sim-
ply by chance. In this paper, we examine
the accuracy of three randomized meth-
ods of significance testing in the context
of machine translation: paired bootstrap
resampling, bootstrap resampling and ap-
proximate randomization. We carry out
a large-scale human evaluation of shared
task systems for two language pairs to
provide a gold standard for tests. Re-
sults show very little difference in accu-
racy across the three methods of signif-
icance testing. Notably, accuracy of all
test/metric combinations for evaluation of
English-to-Spanish are so low that there is
not enough evidence to conclude they are
any better than a random coin toss.
1 Introduction
Automatic metrics, such as BLEU (Papineni et
al., 2002), are widely used in machine translation
(MT) as a substitute for human evaluation. Such
metrics commonly take the form of an automatic
comparison of MT output text with one or more
human reference translations. Small differences
in automatic metric scores can be difficult to inter-
pret, however, and statistical significance testing
provides a way of estimating the likelihood that a
score difference has occurred simply by chance.
For several metrics, such as BLEU, standard sig-
nificance tests cannot be applied due to scores
not comprising the mean of individual sentence
scores, justifying the use of randomized methods.
Bootstrap resampling was one of the early ran-
domized methods proposed for statistical signifi-
cance testing of MT (Germann, 2003; Och, 2003;
Kumar and Byrne, 2004; Koehn, 2004), to assess
for a pair of systems how likely a difference in
BLEU scores occurred by chance. Empirical tests
detailed in Koehn (2004) show that even for test
sets as small as 300 translations, BLEU confidence
intervals can be computed as accurately as if they
had been computed on a test set 100 times as large.
Approximate randomization was subsequently
proposed as an alternate to bootstrap resam-
pling (Riezler and Maxwell, 2005). Theoretically
speaking, approximate randomization has an ad-
vantage over bootstrap resampling, in that it does
not make the assumption that samples are repre-
sentative of the populations from which they are
drawn. Both methods require some adaptation in
order to be used for the purpose of MT evalua-
tion, such as combination with an automatic met-
ric, and therefore it cannot be taken for granted
that approximate randomization will be more ac-
curate in practice. Within MT, approximate ran-
domization for the purpose of statistical testing is
also less common.
Riezler and Maxwell (2005) provide a compar-
ison of approximate randomization with bootstrap
resampling (distinct from paired bootstrap resam-
pling), and conclude that since approximate ran-
domization produces higher p-values for a set of
apparently equally-performing systems, it more
conservatively concludes statistically significant
differences, and recommend preference of approx-
imate randomization over bootstrap resampling
for MT evaluation. Conclusions drawn from ex-
periments provided in Riezler and Maxwell (2005)
are oft-cited, with experiments interpreted as ev-
idence that bootstrap resampling is overly opti-
mistic in reporting significant differences (Riezler
and Maxwell, 2006; Koehn and Monz, 2006; Gal-
ley and Manning, 2008; Green et al., 2010; Monz,
2011; Clark et al., 2011).
Our contribution in this paper is to revisit sta-
tistical significance tests in MT ? namely, boot-
strap resampling, paired bootstrap resampling and
266
approximate randomization ? and find problems
with the published formulations. We redress these
issues, and apply the tests in statistical testing of
two language pairs. Using human judgments of
translation quality, we find only very minor differ-
ences in significance levels across the three tests,
challenging claims made in the literature about rel-
ative merits of tests.
2 Revisiting Statistical Significance Tests
for MT Evaluation
First, we revisit the formulations of bootstrap
resampling and approximate randomization al-
gorithms as presented in Riezler and Maxwell
(2005). At first glance, both methods appear to
be two-tailed tests, with the null hypothesis that
the two systems perform equally well. To facili-
tate a two-tailed test, absolute values of pseudo-
statistics are computed before locating the abso-
lute value of the actual statistic (original differ-
ence in scores). Using absolute values of pseudo-
statistics is not problematic in the approximate
randomization algorithm, and results in a reason-
able two-tailed significance test. However, the
bootstrap algorithm they provide uses an addi-
tional shift-to-zero method of simulating the null
hypothesis. The way in which this shift-to-zero
and absolute values of pseudo-statistics are ap-
plied is non-standard. Combining shift-to-zero
and absolute values of pseudo-statistics results
in all pseudo-statistics that fall below the mean
pseudo-statistic to be omitted from computation of
counts later used to compute p-values. The ver-
sion of the bootstrap algorithm, as provided in the
pseudo-code, is effectively a one-tailed test, and
since this does not happen in the approximate ran-
domization algorithm, experiments appear to com-
pare p-values from a one-tailed bootstrap test di-
rectly with those of a two-tailed approximate ran-
domization test. This inconsistency is not recog-
nized, however, and p-values are compared as if
both tests are two-tailed.
A better comparison of p-values would first re-
quire doubling the values of the one-sided boot-
strap, leaving those of the two-sided approximate
randomization algorithm as-is. The results of the
two tests on this basis are extremely close, and
in fact, in two out of the five comparisons, those
of the bootstrap would have marginally higher p-
values than those of approximate randomization.
As such, it is conceivable to conclude that the ex-
periments actually show no substantial difference
in Type I error between the two tests, which is con-
sistent with results published in other fields of re-
search (Smucker et al., 2007). We also note that
the pseudo-code contains an unconventional com-
putation of mean pseudo-statistics, ?
B
, for shift-
to-zero.
Rather than speculate over whether these is-
sues with the original paper were simply presen-
tational glitches or the actual basis of the experi-
ments reported on in the paper, we present a nor-
malized version of the two-sided bootstrap algo-
rithm in Figure 1, and report on the results of our
own experiments in Section 4. We compare this
method with approximate randomization and also
paired bootstrap resampling (Koehn, 2004), which
is widely used in MT evaluation. We carry out
evaluation over a range of MT systems, not only
including pairs of systems that perform equally
well, but also pairs of systems for which one
system performs marginally better than the other.
This enables evaluation of not only Type I error,
but the overall accuracy of the tests. We carry out
a large-scale human evaluation of all WMT 2012
shared task participating systems for two language
pairs, and collect sufficient human judgments to
facilitate statistical significance tests. This hu-
man evaluation data then provides a gold-standard
against which to compare randomized tests. Since
all randomized tests only function in combina-
tion with an automatic MT evaluation metric, we
present results of each randomized test across four
different MT metrics.
3 Randomized Significance Tests
3.1 Bootstrap Resampling
Bootstrap resampling provides a way of estimat-
ing the population distribution by sampling with
replacement from a representative sample (Efron
and Tibshirani, 1993). The test statistic is taken
as the difference in scores of the two systems,
S
X
? S
Y
, which has an expected value of 0 under
the null hypothesis that the two systems perform
equally well. A bootstrap pseudo-sample consists
of the translations by the two systems (X
b
, Y
b
) of
a bootstrapped test set (Koehn, 2004), constructed
by sampling with replacement from the original
test set translations. The bootstrap distribution
S
boot
of the test statistic is estimated by calculat-
ing the value of the pseudo-statistic S
X
b
? S
Y
b
for
each pseudo-sample.
267
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
Calculate sample mean ?
B
=
1
B
B?
b=1
S
X
b
? S
Y
b
over
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic S
X
b
? S
Y
b
on bootstrap data
If |S
X
b
? S
Y
b
? ?
B
| ? |S
X
? S
Y
|
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 1: Two-sided bootstrap resampling statisti-
cal significance test for automatic MT evaluation
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
For random shuffles r = 1, ..., R
For sentences in test set
Shuffle variable tuples between systems X and Y
with probability 0.5
Compute pseudo-statistic S
X
r
? S
Y
r
on shuffled data
If S
X
r
? S
Y
r
? S
X
? S
Y
c = c+ 1
If c/R ? ?
Reject the null hypothesis
Figure 2: Approximate randomization statistical
significance test for automatic MT evaluation
The null hypothesis distribution S
H
0
can be es-
timated from S
boot
by applying the shift method
(Noreen, 1989), which assumes that S
H
0
has the
same shape but a different mean than S
boot
. Thus,
S
boot
is transformed into S
H
0
by subtracting the
mean bootstrap statistic from every value in S
boot
.
Once this shift-to-zero has taken place, the null
hypothesis is rejected if the probability of observ-
ing a more extreme value than the actual statistic
is lower than a predetermined p-value ?, which is
typically set to 0.05. In other words, the score dif-
ference is significant at level 1? ?.
Figure 3 provides a one-sided implementation
of bootstrap resampling, whereH
0
is that the score
of System X is less than or equal to the score of
Set c = 0
Compute actual statistic of score differences S
X
? S
Y
on test data
Calculate sample mean ?
B
=
1
B
B?
b=1
S
X
b
? S
Y
b
over
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic S
X
b
? S
Y
b
on bootstrap data
If S
X
b
? S
Y
b
? ?
B
? S
X
? S
Y
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 3: One-sided Bootstrap resampling statisti-
cal significance test for automatic MT evaluation
Set c = 0
For bootstrap samples b = 1, ..., B
If S
X
b
< S
Y
b
c = c+ 1
If c/B ? ?
Reject the null hypothesis
Figure 4: Paired bootstrap resampling randomized
significance test
System Y . Figure 5 includes a typical example of
bootstrap resampling applied to BLEU, for a pair
of systems for which differences in scores are sig-
nificant, while Figure 6 shows the same for ME-
TEOR but for a pair of systems with no significant
difference in scores.
3.2 Approximate Randomization
Unlike bootstrap, approximate randomization
does not make any assumptions about the popula-
tion distribution. To simulate a distribution for the
null hypothesis that the scores of the two systems
are the same, translations are shuffled between the
two systems so that 50% of each pseudo-sample
is drawn from each system. In the context of ma-
chine translation, this can be interpreted as each
translation being equally likely to have been pro-
duced by one system as the other (Riezler and
Maxwell, 2005).
The test statistic is taken as the difference in
scores of the two systems, S
X
? S
Y
. If there is
268
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Paired Bootstrap Res. BLEU
originc = 13
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Bootstrap Resampling BLEU
actual statisticc = 14
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Approximate Randomization  BLEU
actual statisticc = 11
Figure 5: Pseudo-statistic distributions for a typical pair of systems with close BLEU scores for each
randomized test (System F vs. System G).
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Paired Bootstrap Res. METEOR
originc = 269
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Bootstrap Resampling METEOR
actual statisticc = 275
?0.015 ?0.005 0.005 0.015
0
100
200
300
400
Approximate Randomization  METEOR
actual statisticc = 260
Figure 6: Pseudo-statistic distributions of METEOR with randomized tests (System D vs. System A).
a total of S sentences, then a total of 2
S
shuffles is
possible. If S is large, instead of generating all 2
S
possible combinations, we instead generate sam-
ples by randomly permuting translations between
the two systems with equal probability. The distri-
bution of the test statistic under the null hypoth-
esis is approximated by calculating the pseudo-
statistic, S
X
r
? S
Y
r
, for each sample. As before,
the null hypothesis is rejected if the probability of
observing a more extreme value than the actual
test statistic is lower than ?.
Figure 2 provides a one-sided implementation
of approximate randomization for MT evaluation,
where the null hypothesis is that the score of Sys-
tem X is less than or equal to the score of System
Y . Figure 5 shows a typical example of pseudo-
statistic distributions for approximate randomiza-
tion for a pair of systems with a small but signifi-
cant score difference according to BLEU, and Fig-
ure 6 shows the same for METEOR applied to a
pair of systems where no significant difference is
concluded.
3.3 Paired Bootstrap Resampling
Paired bootstrap resampling (Koehn, 2004) is
shown in Figure 4. Unlike the other two random-
ized tests, this method makes no attempt to simu-
late the null hypothesis distribution. Instead, boot-
strap samples are used to estimate confidence in-
tervals of score differences, with confidence inter-
vals not containing 0 implying a statistically sig-
nificant difference.
We compare what takes place with the two other
tests, by plotting differences in scores for boot-
strapped samples, S
X
b
? S
Y
b
, as shown in Fig-
ure 5 for BLEU and Figure 6 for METEOR. Instead
of computing counts with reference to the actual
statistic, the line through the origin provides the
cut-off for counts.
269
Adequacy Fluency Combined
p-value
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E Syst
em.F
Syst
em.G Syst
em.H Syst
em.J
Syst
em.K Syst
em.L
Syst
em.M
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Figure 7: Human evaluation pairwise significance tests for Spanish-to-English systems (colored cells
denote scores for System row being significantly greater than System column .
4 Evaluation
In order to evaluate the accuracy of the three ran-
domized significance significance tests, we com-
pare conclusions reached in a human evaluation
of shared task participant systems. We carry out
a large-scale human evaluation of all participating
systems from WMT 2012 (Callison-Burch et al.,
2012) for the Spanish-to-English and English-to-
Spanish translation tasks. Large numbers of hu-
man assessments of translations were collected us-
ing Amazon?s Mechanical Turk, with strict qual-
ity control filtering (Graham et al., 2013). A to-
tal of 82,100 human adequacy assessments and
62,400 human fluency assessments were collected.
After the removal of quality control items and
filtering of judgments from low-quality workers,
this resulted in an average of 1,280 adequacy and
1,013 fluency assessments per system for Spanish-
to-English (12 systems), and 1,483 adequacy and
1,534 fluency assessments per system for English-
to-Spanish (11 systems). To remove bias with re-
spect to individual human judge preference scor-
ing severity/leniency, scores provided by each hu-
man assessor were standardized according to the
mean and standard deviation of all scores provided
by that individual.
Significance tests were carried out over the
scores for each pair of systems separately for
adequacy and fluency assessments using the
Wilcoxon rank-sum test. Figure 7 shows pairwise
significance test results for fluency, adequacy and
the combination of the two tests, for all pairs of
Spanish-to-English systems. Combined fluency
and adequacy significance test results are con-
structed as follows: if a system?s adequacy score is
significantly greater than that of another, the com-
bined conclusion is that it is significantly better,
at that significance level. Only when a tie in ad-
equacy scores occurs are fluency judgments used
to break the tie. In this case, p-values from signifi-
cance tests applied to fluency scores of that system
pair are used. For example, in Figure 7, adequacy
scores of System B are not significantly greater
than those of Systems C, D and E, while fluency
scores for System B are significantly greater than
those of the three other systems. The combined re-
sult for each pair of systems is therefore taken as
the p-value from the corresponding fluency signif-
icance test.
We use the combined human evaluation pair-
wise significant tests as a gold standard against
which to evaluate the randomized methods of sta-
tistical significance testing. We evaluate paired
bootstrap resampling (Koehn, 2004) and bootstrap
resampling as shown in Figure 3 and approxi-
mate randomization as shown in Figure 2, each
in combination with four automatic MT metrics:
BLEU (Papineni et al., 2002), NIST (NIST, 2002),
METEOR (Banerjee and Lavie, 2005) and TER
(Snover et al., 2006).
4.1 Results and Discussion
Figure 8 shows the outcome of pairwise random-
ized significance tests for each metric for Spanish-
to-English systems, and Table 1 shows numbers of
correct conclusions and accuracy of each test.
When we compare conclusions made by the
three randomized tests for Spanish-to-English sys-
tems, there is very little difference in p-values for
all pairs of systems. For both BLEU and NIST,
270
Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
? Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
0.05
BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
NIST 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2]
METEOR 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
TER 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
0.01
BLEU 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
NIST 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
TER 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
0.001
BLEU 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
NIST 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 52 78.8 [67.0, 87.9]
TER 50 75.8 [63.6, 85.5] 51 77.3 [65.3, 86.7] 52 78.8 [67.0, 87.9]
Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automatic
metrics, based on the WMT 2012 participant systems.
Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
? Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
0.05
BLEU 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6]
NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7]
TER 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
0.01
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 31 56.4 [42.3, 69.7] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0]
0.001
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 33 60.0 [45.9, 73.0] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 31 56.4 [42.3, 69.7]
Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automatic
metrics, based on the WMT 2012 participant systems.
all three randomized methods produce p-values
so similar that when ? thresholds are applied, all
three tests produce precisely the same set of pair-
wise conclusions for each metric. When tests are
combined with METEOR and TER, similar results
are observed: at the ? thresholds of 0.05 and 0.01,
precisely the same conclusions are drawn for both
metrics combined with each of the three tests, and
at most a difference of two conclusions at the low-
est ? level.
Table 2 shows the accuracy of each test on the
English-to-Spanish data, showing much the same
set of conclusions at all ? levels. For BLEU and
NIST, all three tests again produce precisely the
same conclusions, at p < 0.01 there is at most a
single different conclusion for METEOR, and only
at the lowest p-value level is there a single differ-
ence for TER.
271
M
E
T
E
O
R
T
E
R
N
I
S
T
B
L
E
U
Paired Bootstrap Bootstrap Approximate
Resampling Resampling Randomization
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
Syst
em.A
Syst
em.B
Syst
em.C
Syst
em.D
Syst
em.E
Syst
em.F
Syst
em.G
Syst
em.H
Syst
em.J
Syst
em.K
Syst
em.L
Syst
em.M
System.MSystem.L
System.KSystem.J
System.HSystem.G
System.FSystem.E
System.DSystem.C
System.BSystem.A
Figure 8: Automatic metric pairwise randomized significance test results for Spanish-to-English systems
(colored cells denote scores for System row significantly greater than System column).
Finally, we examine which combination of met-
ric and test is most accurate for each language
pair at the conventional significance level of p <
0.05. For Spanish-to-English evaluation, NIST
combined with any of the three randomized tests
is most accurate, making 54 out of 66 (82%) cor-
rect conclusions. For English-to-Spanish, BLEU
in combination with any of the three randomized
tests, is most accurate at 62%. For both language
pairs, however, differences in accuracy for metrics
272
are not significant (Chi-square test).
For English-to-Spanish evaluation, an accuracy
as low as 62% should be a concern. This level
of accuracy for significance testing ? only making
the correct conclusion in 6 out of 10 tests ? acts
as a reminder that no matter how sophisticated the
significance test, it will never make up for flaws in
an underlying metric. When we take into account
the fact that lower confidence limits all fall below
50%, significance tests based on these metrics for
English-to-Spanish are effectively no better than a
random coin toss.
5 Conclusions
We provided a comparison of bootstrap resam-
pling and approximate randomization significance
tests for a range of automatic machine trans-
lation evaluation metrics. To provide a gold-
standard against which to evaluate randomized
tests, we carried out a large-scale human evalua-
tion of all shared task participating systems for the
Spanish-to-English and English-to-Spanish trans-
lation tasks from WMT 2012. Results showed for
many metrics and significance levels that all three
tests produce precisely the same set of conclu-
sions, and when conclusions do differ, it is com-
monly only by a single contrasting conclusion,
which is not significant. For English-to-Spanish
MT, the results of the different MT evaluation met-
ric/significance test combinations are not signifi-
cantly higher than a random baseline.
Acknowledgements
We wish to thank the anonymous reviewers for their valuable
comments. This research was supported by funding from the
Australian Research Council.
References
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65?
73, Ann Arbor, MI. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10?51, Montreal, Canada. ACL.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In Proc. of the 49th Annual Meeting of the As-
soc. Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 176?
181, Portland, OR. ACL.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York City,
NY.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, pages 848?856, Ed-
inburgh, Scotland. ACL.
U. Germann. 2003. Greedy decoding for statisti-
cal machine translation in almost linear time. In
Proc. of the 2003 Conference of the North American
Chapter of the Assoc. Computational Linguistics on
Human Language Technology-Volume 1, pages 1?8,
Edmonton, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. & Interoperability with Dis-
course, pages 33?41, Sofia, Bulgaria. ACL.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics, pages 867?875, Los Angeles, CA. ACL.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between European
languages. In Proceedings of the Workshop on Sta-
tistical Machine Translation, pages 102?121, New
York City, NY. ACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of Empiri-
cal Methods in Natural Language Processing, pages
388?395, Barcelona, Spain. ACL.
S. Kumar and W. J. Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In
HLT-NAACL, pages 169?176, Boston, MA. ACL.
C. Monz. 2011. Statistical machine translation with lo-
cal language models. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 869?879, Edniburgh, Scotland. ACL.
NIST. 2002. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-Occurrence Statis-
tics. Technical report.
E. W. Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley, New York City, NY.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. 41st Ann. Meet-
ing of the Assoc. Computational Linguistics, pages
160?167, Sapporo, Japan. ACL.
273
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. A method for automatic evaluation of ma-
chine translation. In Proc. 40th Ann. Meeting of the
Assoc. Computational Linguistics, pages 311?318,
Philadelphia, PA. ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
mt. In Proc. of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, pages 57?64, Ann Ar-
bor, MI. ACL.
S. Riezler and J. T. Maxwell. 2006. Grammatical
machine translation. In Proc. of the Main Confer-
ence on Human Language Technology Conference of
the North American Chapter of the Assoc. Computa-
tional Linguistics, pages 248?255, New York City,
NY. ACL.
M. Smucker, J. Allan, and B. Carterette. 2007. A com-
parison of statistical significance tests for informa-
tion retrieval evaluation. In Proc. of the Sixteenth
ACM Conference on Information and Knowledge
Management (CIKM 2007), pages 623?632, Lisbon,
Portugal. ACM.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223?231, Boston, MA. ACL.
274
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 129?138,
Dublin, Ireland, August 23 2014.
Exploring Methods and Resources for Discriminating Similar Languages
Marco Lui
??
, Ned Letcher
?
, Oliver Adams
?
,
Long Duong
??
, Paul Cook
?
and Timothy Baldwin
??
?
Department of Computing and Information Systems
The University of Melbourne
?
NICTA Victoria
mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au,
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici-
pants to build an automatic language identification system to discriminate between 13 languages
in 6 groups of highly-similar languages (or national varieties of the same language). In this
paper, we describe the submissions made by team UniMelb-NLP, which took part in both the
closed and open categories. We present the text representations and modeling techniques used,
including cross-lingual POS tagging as well as fine-grained tags extracted from a deep grammar
of English, and discuss additional data we collected for the open submissions, utilizing custom-
built web corpora based on top-level domains as well as existing corpora.
1 Introduction
Language identification (LangID) is the problem of determining what natural language a document is
written in. Studies in the area often report high accuracy (Cavnar and Trenkle, 1994; Dunning, 1994;
Grefenstette, 1995; Prager, 1999; Teahan, 2000). However, recent work has shown that high accu-
racy is only achieved under ideal conditions (Baldwin and Lui, 2010), and one area that needs further
work is accurate discrimination between closely-related languages (Ljube?si?c et al., 2007; Tiedemann
and Ljube?si?c, 2012). The problem has been explored for specific groups of confusable languages, such
as Malay/Indonesian (Ranaivo-Malancon, 2006), South-Eastern European languages (Tiedemann and
Ljube?si?c, 2012), as well as varieties of English (Lui and Cook, 2013), Portuguese (Zampieri and Gebre,
2012), and Spanish (Zampieri et al., 2013). The Discriminating Similar Language (DSL) shared task
(Zampieri et al., 2014) was hosted at the VarDial workshop at COLING 2014, and brings together the
work on these various language groups by proposing a task on a single dataset containing text from 13
languages in 6 groups, drawn from a variety of news text datasets (Tan et al., 2014).
In this paper, we describe the entries made by team UniMelb NLP to the DSL shared task. We took
part in both the closed and the open categories, submitting to the main component (Groups A-E) as well
as the separate English component (Group F). For our closed submissions, we focused on comparing a
conventional LangID methodology based on individual words and language-indicative letter sequences
(Section 2.1) to a methodology that uses a de-lexicalized representation of language (Section 2.3). For
Groups A-E we use cross-lingual POS-tagger adaptation (Section 2.3.1) to convert the raw text to a
POS stream using a per-group tagger, and use n-grams of POS tags as our de-lexicalized representation.
For English, we also use a de-lexicalized representation based on lexical types extracted from a deep
grammar (Section 2.3.2), which can be thought of as a very fine-grained tagset. For the open submissions,
we constructed new web-based corpora using a standard methodology, targeting per-language top-level
domains (Section 2.4.2). We also compiled additional training data from existing corpora (Section 2.4.1).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
129
2 Overview
Our main focus was to explore novel methods and sources of training data for discriminating similar
languages. In this section, we describe techniques and text representations that we tested, as well as the
external data sources that we used to build language identifiers for this task.
2.1 Language-Indicative Byte Sequences
Lui and Baldwin (2011) introduced the LD feature set, a document representation for LangID that is
robust to variation in languages across different sources of text. The LD feature set can be thought of as
language-indicative byte sequences, i.e. sequences of 1 to 4 bytes that have been selected to be strongly
characteristic of a particular language or set of languages regardless of the text source. Lui and Baldwin
(2012) present langid.py,
1
an off-the-shelf LangID system that utilizes the LD feature set. In this
work, we re-train langid.py using the training data provided by the shared task organizers, and use
this as a baseline result representative of the state-of-the-art in LangID.
2.2 Hierarchical LangID
In LangID research to date, systems generally do not take into account any form of structure in the
class space. In this shared task, languages are explicitly grouped into 6 disjoint groups. We make use
of this structure by introducing a two-level LangID model. The first level implements a single group-
level classifier, which takes an input sentence and identifies the language group (A?F) that the sentence
is from. The output of this group-level classifier is used to select a corresponding per-group classifier,
that is trained only on data for languages in the group. This per-group classifier is applied to the input
sentence and the output thereof is the final label for the sentence.
2.3 De-Lexicalized Text Representation for DSL
One of the challenges in a machine learning approach to discriminating similar languages is to learn
differences between languages that are truly representative of the distinction between varieties, rather
than differences that are merely representative of peculiarities of the training data (Kilgarriff, 2001). One
possible confounding factor is the topicality of the training data ? if the data for each variety is drawn
from different datasets, it is possible that a classifier will simply learn the topical differences between
datasets. Diwersy et al. (2014) carried out a study of colligations in French varieties, where the variation
in the grammatical function of noun lemmas was studied across French-language newspapers from six
countries. In their initial analysis the found that the characteristic features of each country included the
name of the country and other country-specific proper nouns, which resulted in near 100% classification
accuracy but do not provide any insight into national varieties from a linguistic perspective.
One strategy that has been proposed to mitigate the effect of such topical differences is the use of
a de-lexicalized text representation (Lui and Cook, 2013). The de-lexicalization is achieved through
the use of a Part-Of-Speech tagger, which labels each word in a sentence according to its word class
(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first
considered for native language identification (NLI), where they were used as a proxy for syntax in order
to capture certain types of grammatical errors (Wong and Dras, 2009). Syntactic structure is known to
vary across national dialects (Trudgill and Hannah, 2008), so Lui and Cook (2013) investigated POS plus
function word n-grams as a proxy for syntactic structure, and used this representation to build classifiers
to discriminate between Canadian, British and American English. They found that classifiers using such a
representation achieved above-baseline results, indicating some systematic differences between varieties
could be captured through the use of such a de-lexicalized representation. In this work, we explore this
idea further ? in particular, we examine (1) the applicability of de-lexicalized text representations to
other languages using automatically-induced crosslingual POS taggers, and (2) the difference in accuracy
for discriminating English varieties between representations based on a coarse-grained universal tagset
(Section 2.3.1) as compared to a very fine-grained tagset used in deep parsing (Section 2.3.2).
1
http://github.com/saffsd/langid.py
130
Sandy quit on Tuesday Sandy quit Tuesday
UT NOUN VERB ADP NOUN NOUN VERB NOUN
LTT n - pn v np
*
p np i-tmp n - c-dow n - pn v np
*
n - c-dow
British English American English
Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical
type tagset (LTT).
2.3.1 Crosslingual POS Tagging
A key issue in generating de-lexicalized text representations based on POS tags is the lack of availability
of POS taggers for many languages. While some languages have some tools available for POS tagging
(e.g. Treaties (Schmid, 1994) has parameter files for Spanish and Portuguese), the availability of POS
taggers is far from universal. To address this problem for the purposes of discriminating similar lan-
guages, we draw on previous work in unsupervised cross-lingual POS tagging (Duong et al., 2013) to
build a POS tagger for each group of languages, a method which we will refer to hereafter as ?UMPOS?.
UMPOS employs a 12-tag Universal Tagset introduced by Petrov et al. (2012), which consists of the
tags NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner or article), ADP
(preposition or postposition), NUM (numeral), CONJ (conjunction), PRT (particle), PUNCT (punctua-
tion), and X (all other categories, e.g., foreign words or abbreviations). These twelve basic tags constitute
a ?universal? tagset in that they can be used to describe the morphosyntax of any language at a coarse
level.
UMPOS generates POS taggers for new languages in an unsupervised fashion, by making use of
parallel data and an existing POS tagger. The input for UMPOS is: (1) parallel data between the source
and target languages; and (2) a supervised POS tagger for the source language. The output will be the
tagger for the target language. The parallel data acts as a bridge to transfer POS annotation information
from the source language to the target language.
The steps used in UMPOS are as follow. First, we collect parallel data which has English as the source
language, drawing from Europarl (Koehn, 2005) and EUbookshop (Skadin??s et al., 2014). UMPOS word-
aligns the parallel data using the Giza++ alignment tool (Och and Ney, 2003). The English side is POS-
tagged using the Stanford POS tagger (Toutanova et al., 2003), and the POS tags are then projected from
English to the target language based solely on one-to-one mappings. Using the sentence alignment score,
UMPOS ranks the ?goodness? of projected sentences and builds a seed model for the target language on
a subset of the parallel data. To further improve accuracy, UMPOS builds the final model by applying
self-training with revision to the rest of the data as follows: (1) the parallel corpus data is divided into
different blocks; (2) the first block is tagged using the seed model; (3) the block is revised based on
alignment confidence; (4) a new tagger is trained on the first block and then used to tag the second block.
This process continues until all blocks are tagged. In experiments on a set of 8 languages, Duong et al.
(2013) report accuracy of 83.4%, which is state-of-the-art for unsupervised POS tagging.
2.3.2 English Tagging Using ERG Lexical Types
Focusing specifically on language Group F ? British English and American English ? we leveraged
linguistic information from the analyses produced by the English Resource Grammar (ERG: Flickinger
(2002)), a broad-coverage, handcrafted grammar of English in the HPSG framework (Pollard and Sag,
1994) and developed within the DELPH-IN
2
research initiative. In particular, we extracted the lexical
types assigned to tokens by the parser for the best analysis of each input string. In accordance with
the heavily lexicalized nature of HPSG, lexical types are the primary means of distinguishing between
different morphosyntactic contexts in which a given lexical entry can occur. They can be thought of as
fine-grained POS tags, containing subcategorisation information in addition to part of speech informa-
tion, and semantic information in cases that it directly impacts on morphosyntax. The version of the
ERG we used (the ?1212? release) has almost 1000 lexical types.
Table 1 illustrates an example of the type of syntactic variation that can be captured with the finer-
2
http://www.delph-in.net
131
Group Language Code
Web Corpora Existing Corpora
TLD # words # datasets # words
A Bosnian bs .ba 817383 4 715602
A Croatian hr .hr 43307311 5 1536623
A Serbian sr .rs 1374787 4 1204684
B Indonesian id .id 23812382 3 564824
B Malaysian my .my 2596378 3 535221
C Czech cz .cz 17103140 8 2181486
C Slovakian sk .sk 17253001 8 2308083
D Brazilian Portuguese pt-BR .br 27369673 4 860065
D European Portuguese pt-PT .pt 22620401 8 2860321
E Argentine Spanish es-AR .ar 45913651 2 619500
E Peninsular Spanish es-ES .es 30965338 9 3458462
F British English en-GB .uk 20375047 1 523653
F American English en-US .us 21298230 1 527915
Table 2: Word count of training data used for open submissions.
grained lexical types, that would be missed with the coarse-grained universal tagset. In American En-
glish, both Sandy resigned on Tuesday and Sandy resigned on Tuesday are acceptable whereas British
English does not permit the omission of the preposition before dates. In the coarse-grained tagset, the
American English form results in a sequence VERB : NOUN, which is not particularly interesting as we
expect this to occur in both English varieties, whereas the fine-grained lexical types allow us to capture
the sequence v np
*
ntr : n - c-dow (verb followed by count noun [day of week]), which we expect
to see in American English but not in British English.
Since the ERG models a sharp notion of grammaticality, not all inputs receive an analysis ? whether
due to gaps in the coverage of the grammar or genuinely ungrammatical input. The ERG achieved
a coverage of 86% over the training data across both British English and American English. Sentences
which failed to parse were excluded from use as input into the classifier. However the inability to classify
any sentence which we cannot parse is unsatisfactory. We solved this problem by generating lexical type
features for sentences which failed to parse using the ERG-trained ?ubertagger of Dridan (2013), which
performs both tokenisation and supertagging of lexical types and improves parser efficiency by reducing
ambiguity in the input lattice to the parser.
2.4 External Corpora
The DSL shared task invited two categories of participation: (1) Closed, using only training data provided
by the organizers (Tan et al., 2014); and (2) Open, using any training data available to participants. To
participate in the latter category, we sourced additional training data through: (1) collection of data
relevant to this task from existing text corpora; and (2) automatic construction of web corpora. The
information about the additional training data is shown in Table 2.
2.4.1 Existing Corpora
We collected training data from a number of existing corpora, as shown in Table 3. Many of the cor-
pora that we used are part of OPUS (Tiedemann, 2012), which is a collection of sentence-aligned text
corpora commonly used for research in machine translation. The exceptions are: (1) debian, which
was constructed using translations of message strings from the Debian operating system,
3
; (2) BNC ?
the British National Corpus (Burnard, 2000); (3) OANC ? the open component of the Second Release
of the American National Corpus (Ide and Macleod, 2001), and (4) Reuters Corpus Volume 2 (RCV2);
4
a corpus of news stories by local reporters in 13 languages. We sampled approximately 19000 sentences
from each of the BNC and OANC, which we used as training data to generate ERG lextype features (Sec-
tion 2.3.2) for British English (en-GB) and American English (en-US), respectively. From RCV2 we
3
http://www.debian.org
4
http://trec.nist.gov/data/reuters/reuters.html
132
bs hr sr pt-PT pt-BR id my cz sk es-ES es-AR en-US en-GB
BNC X
debian X X X X X X X X X X X
ECB X X X X
EMEA X X X X
EUconst X X X X
Europarl X X X X
hrenWaC X
KDE4 X X X X X X X X X
KDEdoc X X X X
OANC X
OpenSubtitles X X X X X X X X X X
RCV2 X X
SETIMES2 X X X
Tatoeba X X
Table 3: Training data compiled from existing corpora.
used the Latin American Spanish news stories as a proxy for Argentine Spanish (es-AR). Note that, for
a given text source, we didn?t necessarily use data for all available languages. For example, debian
contains British English and American English translations, which we did not use.
2.4.2 Web Corpus Construction
Each existing corpus we describe in Section 2.4.1 provides incomplete coverage over the set of languages
in the shared task dataset. In order to have a resource that covers all the languages in the shared task
drawn from a single source, we constructed web corpora for each language. Our approach was strongly
inspired by the approach used to create ukWaC (Ferraresi et al., 2008), and the creation of each sub-
language?s corpus involved crawling the top level domains of the primary countries associated with
those sub-languages. Based on the findings of Cook and Hirst (2012), the assumption underlying this
approach is that text found in the top-level domains (TLDs) of those countries will primarily be of
the sub-language dominant in that country. For instance, we assume that Portuguese text found when
crawling the .pt TLD will primarily be European Portuguese, while the Portuguese found in .br will
be primarily Brazilian Portuguese.
The process of creating a corpus for each sub-language involved translating a sample of 200 of the
original ukWaC queries into each language using Panlex (Baldwin et al., 2010).
5
These queries were then
submitted to the Bing Search API using the BootCaT tools (Baroni and Bernardini, 2004), constraining
results to the relevant TLD. For each query, we took the first 10 URLs yielded by Bing and appended
them to a list of seed URLs for that language. After deduplication, the seed URLs were then fed to a
Heritrix 3.1.1
6
instance with default settings other than constraining the crawled content to the relevant
TLD.
Corpora were then created from the data gathered by Heritrix. Following the ukWaC approach,
only documents with a MIME type of HTML and size between 5k and 200k bytes were used. Jus-
text (Pomik?alek, 2011) was used to extract text from the selected documents. langid.py (Lui and
Baldwin, 2012) was then used to discard documents whose text was not in the relevant language or lan-
guage group. The corpus was then refined through deduplication. First, near-deduplication was done at
the paragraph level using Onion (Pomik?alek, 2011) with its default settings. Then, exact-match sentence-
level deduplication, ignoring whitespace and case, was applied.
3 Results and Discussion
Table 4 summarizes the runs submitted by team UniMelb NLP to the VarDial DSL shared task. We
submitted the maximum number of runs allowed, i.e. 3 closed runs and 3 open runs, to both the ?general?
Groups A?E subtask as well as the English-specific Group F subtask. We applied different methods to
Group F, as some of the tools (the ERG) and resources (BNC/OANC) were specific to English. For clarity
in discussion, we have labeled each of our runs according to a 3-letter code: the first letter indicates the
5
A sample of the queries was used because of time and resource limitations.
6
https://webarchive.jira.com/wiki/display/Heritrix
133
Run Description
Macro-avg F-Score
dev tst
Grp A-E closed
AC1 langid.py 13-way 0.822 0.817
AC2 langid.py per-group 0.923 0.918
AC3 POS features 0.683 0.671
Grp F closed
FC1 Lextype features 0.559 0.415
FC2 langid.py per-group 0.548 0.403
FC3 POS features 0.545 0.435
Grp A-E open
AO1 Ext Corpora (word-level model) 0.705 0.703
AO2 Web Corpora (word-level model) 0.771 0.767
AO3 5-way voting 0.881 0.878
Grp F open
FO1 Lextype features using BNC/OANC training data 0.491 0.572
FO2 Web Corpora (word-level model) 0.490 0.581
FO3 5-way voting 0.574 0.442
Table 4: Summary of the official runs submitted by UniMelbNLP. ?dev? indicates scores from our
internal testing on the development partition of the dataset.
subtask (A for Groups A?E, F for Group F), the second indicates Closed (?C?) or Open (?O?), and the
final digit indicates the run number.
AC1 represents a benchmark result based on the LangID system (Lui and Baldwin, 2012). We used
the training tools provided with langid.py to generate a new model using the training data provided
by the shared task organizers, noting that as only data from a single source is used, we are not able to
fully exploit the cross-domain feature selection (Lui and Baldwin, 2011) implemented by langid.py.
The macro-averaged F-score across groups is substantially lower than that on standard LangID datasets
(Lui and Baldwin, 2012).
AC2 and FC2 are a straightforward implementation of hierarchical LangID (Section 2.2), using
mostly-default settings of langid.py. A 6-way group-level classifier is trained, and well as 6 different
per-group classifiers. We increase the number of features selected per class (i.e. group or language) to
500 from the default of 300, to compensate for the smaller number of classes (langid.py off-the-
shelf supports 97 languages). In our internal testing on the provided development data, the group-level
classifier achieved 100% accuracy in classifying sentences at the group level, essentially reducing the
problem to within-group disambiguation. Despite being one of the simplest approaches, overall this
was our best-performing submission for Groups A?E. It also represents a substantial improvement on
AC1, further emphasizing the need to implement hierarchical LangID in order to attain high accuracy in
discriminating similar languages.
AC3 and FC3 are based solely on POS-tag sequences generated by UMPOS, and implement a hierar-
chical LangID approach similar to AC2/FC2. Each sentence in the training data is mapped to a POS-tag
sequence in the 12-tag universal tagset, using the per-group POS tagger for the language group. Each tag
was represented using a single character, allowing us to make use of langid.py to train 6 per-group
classifiers based on n-grams of POS-tags. We used n-grams of order 1?6, and selected 5000 top-ranked
sequences per-language. To classify test data, the same group-level classifier used in AC2 was used to
map sentences to language groups, and then the per-group POS tagger was applied to derive the corre-
sponding stream of POS tags for each sentence. The corresponding per-group classifier trained on POS
tag sequences was then applied to produce the final label for the sentence. For Groups A?E, we find that
134
bs hr sr id my cz sk
T 53.0 23.2 60.0 VHN 0.9 1.3 .1.1 1.0 1.2
TV 32.4 13.2 43.3 DHN 0.1 0.1 1.1. 2.0 2.2
NT 31.5 13.9 43.2 N.1. 12.1 3.1 1.1 4.0 4.4
TVN 24.8 9.8 34.3 N.N 63.3 48.0 .N.1 0.5 0.7
VT 19.4 6.1 27.1 .DNV 1.8 1.1 .C 39.0 33.5
TN 29.1 10.9 29.6 DH 1.7 2.1 .1.. 0.7 1.0
NTV 18.6 8.4 29.4 N.DN 3.2 2.0 .P 51.2 41.8
TVNN 16.8 6.9 23.7 VH 11.3 14.9 1. 14.0 13.9
NVT 11.2 2.9 15.5 PNV1 0.5 0.4 1.. 1.2 1.6
VTV 11.0 3.2 17.0 .1. 13.2 3.8 .R 44.0 30.0
pt-BR pt-PT es-AR es-ES en-GB en-US
X 3.4 2.8 .. 22.6 43.3 NNN 48.2 43.2
N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4
.NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0
XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9
NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5
D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4
NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5
.NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8
N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3
NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6
Table 5: Top 10 POS features per-group by Information Gain, along with percentage of sentences in each
language in which the feature appears. The notation used is as follows: . = punctuation, J = adjective,
P = pronoun, R = adverb, C = conjunction, D = determiner/article, N = noun, 1 = numeral, H = pronoun,
T = particle, V = verb, and X = others
the POS-tag sequence features are not as effective as the character n-grams used in AC2. Nonetheless,
the results attained are above baseline, indicating that there are systematic differences between languages
in each group that can be captured by an unsupervised approach to POS-tagging using a coarse-grained
tagset. This extends the similar observation made by Lui and Cook (2013) on varieties of English, show-
ing that the same is true for the other language groups in this shared task. Also of interest is the higher
accuracy attained by the POS-tag features on Groups A?E (i.e. AC3) than on English (Group F, FC3).
The top-10 sequences per-group are presented in Table 5, where it can be seen that the sequences are
often slightly more common in one language in the group than the other language(s). One limitation of
the Information Gain based feature selection used in langid.py is that each feature is scored inde-
pendently, and each language receives a binarized score. This can be seen in the features selected for
Group A, where all the top-10 features selected involve particles (labelled T). Overall, this indicates that
Croatian (hr) appears to use particles much less frequently than Serbian (sr) or Bosnian (bs), which is
an intriguing finding. However, most of the top-10 features are redundant in that they all convey very
similar information.
Similar to FC3, a hierarchical LangID approach is used in FC1, in conjunction with per-group classi-
fiers based on a sequence of tags derived from the original sentence. The difference between the taggers
used for FC3 and FC1 is that the FC3 tagger utilizes the 12-tag universal tagset, whereas the FC1 tagger
uses the English-specific lexical types from the ERG (Section 2.3.2), a set of approximately 1000 tags.
There is hence a trade-off to be made between the degree of distinction between tags, and the relative
sparsity of the data ? having a larger tagset means that any given sequence of tags is proportionally less
likely to occur. On the basis of the results of FC1 and FC3 on the dev data, the lexical type features
marginally outperform the coarse-grained universal tagset. However, this result is made harder to inter-
pret by the mismatch between the dev and tst partitions of the shared task dataset. We will discuss
this issue in more detail below, in the context of examining the results on Group F for the open category.
In the open category, we focused primarily on the effect of using different sources of training data.
AO1 and AO2 both implement a hierarchical LangID approach, again using the group-level classifier
from AC2. For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level
representation, with feature selection by Information Gain. The difference between the two is that A01
uses samples from existing text corpora (Section 2.4.1), whereas A02 uses web corpora that we prepared
specifically for this shared task (Section 2.4.2). In terms of accuracy, both types of corpora perform
135
substantially better than baseline, indicating that at the word level, there are differences between the
language varieties that are consistent across the different corpus types. This result is complementary to
Cook and Hirst (2012), who found that web corpora from specific top-level domains were representative
of national varieties of English. AO2 (web corpora) outperforms AO1 (existing corpora), further high-
lighting the relevance of web corpora as a source of training data for discriminating similar languages.
However, our models trained on external data were not able to outperform the models trained on the
official training data for Groups A?E. A03 consists of a 5-way majority vote between results AC1, AC2,
AC3, AO1 and AO2. Including the predictions from the closed submissions substantially improves the
result with respect to AO1/AO2, but overall our best result for Groups A?E was obtained by run AC2.
For Group F, FO1 utilizes ERG lexical type features in the same manner as FC1, the difference being
that FC1 uses the shared task trn partition, whereas FO1 uses sentences sampled from existing corpora,
specifically BNC for en-GB and OANC for en-US. FO2 implements the same concept as AO2, namely a
word-level naive Bayes model trained using web corpora. For the Group F (i.e. English) subtask, this was
our best-performing submission overall. FO3 is a 5-way vote between FC1, FC2, FC3, FO1 and FO2,
similar to AO3. Notably, our Group F submissions based on the supplied training data all performed
substantially better on the dev partition of the shared task dataset than on the tst partition. The inverse
is true for our submissions based on external corpora, where all our entries performed substantially better
on the tst partition than on the dev partition. Furthermore, the differences are fairly large, particularly
since Group F is a binary classification task with a 50% baseline. This implies that, at least under our
models, the en-GB portion of the trn partition is a better model of the en-US portion of the tst partition
than the en-GB portion thereof. This is likely due to the manual intervention that was only carried out
on the test portion of the dataset (Zampieri et al., 2014).
Our Group F results appear to be inferior to previous work on discriminating English varieties (Lui
and Cook, 2013). However, there are a number of differences that make it difficult to compare the
results: Lui and Cook (2013) studied differences between Australian, British and Canadian English,
whereas the shared task focused on differences between British and American English. Lui and Cook
(2013) also draw on training data from a variety of domains (national corpora, web corpora and Twitter
messages), whereas the shared task used a dataset collected from newspaper texts (Tan et al., 2014).
Consistent with Cook and Hirst (2012) and Lui and Cook (2013), we found that web corpora appear to be
representative of national varieties, and consistent with Lui and Cook (2013) we found that de-lexicalized
representations of text are able to provide better than baseline discrimination between national varieties.
Overall, these results highlight the need for further research into discriminating between varieties of
English.
4 Conclusion
Discriminating between similar languages is an interesting sub-problem in language identification, and
the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater
detail. Our most successful methods implement straightforward hierarchical LangID, firstly identifying
the language group that a sentence belongs to, before identifying the specific language. We examined a
number of text representations for the per-group language identifiers, including a standard representation
for language identification based on language-indicative byte sequences, as well as with de-lexicalized
text representations. We found that the performance of de-lexicalized representations was above baseline,
however we were not able to fully investigate approaches to integrating predictions from lexicalized
and de-lexicalized text representations due to time constraints. We also found that when using external
corpora, web corpora constructed by scraping per-country top-level domains performed as well as (if
not better than) data collected from existing text corpora, supporting the hypothesis that web corpora
are representative of national varieties of respective languages. Overall, our best result was obtained by
applying two-level hierarchical LangID, firstly identifying the language group that a sentence belongs
to, and then disambiguating within each group. Our best result was achieved by applying an existing
LangID method (Lui and Baldwin, 2012) to both the group-level and the per-group classification tasks.
136
Acknowledgments
The authors wish to thank Li Wang, Rebecca Dridan and Bahar Salehi for their kind assistance with
this research. NICTA is funded by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian Research Council through the
ICT Centre of Excellence program.
References
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter
of the Association for Computational Linguistics (NAACL HLT 2010), pages 229?237, Los Angeles, USA.
Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of
all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics:
Demonstrations, pages 37?40, Beijing, China.
Marco Baroni and Silvia Bernardini. 2004. BootCaT: Bootstrapping corpora and terms from the Web. In Pro-
ceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004).
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford University
Computing Services.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of the Third
Symposium on Document Analysis and Information Retrieval, pages 161?175, Las Vegas, USA.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties of
English? In Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages
281?293, Li`ege, Belgium.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A weakly supervised multivariate approach to the study
of language variation. In Benedikt Szmrecsanyi and Bernhard W?alchli, editors, Aggregating Dialectology,
Typology, and Register Analysis. Linguistic Variation in Text and Speech. De Gruyter, Berlin.
Rebecca Dridan. 2013. Ubertagging. Joint segmentation and supertagging for English. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing, pages 1201?1212, Seattle, USA.
Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS 940-273, Computing Research
Laboratory, New Mexico State University.
Long Duong, Paul Cook, Steven Bird, and Pavel Pecina. 2013. Simpler unsupervised POS tagging with bilin-
gual projections. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 634?639.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop:
Can we beat Google, pages 47?54, Marrakech, Morocco.
Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering. CSLI Publi-
cations, Stanford, USA.
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of Analisi Statistica
dei Dati Testuali (JADT), pages 263?268, Rome, Italy.
Nancy Ide and Catherine Macleod. 2001. The American National Corpus: A standardized resource of American
English. In Proceedings of Corpus Linguistics 2001, pages 274?280, Lancaster, UK.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):97?133.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the Tenth
Machine Translation Summit (MT Summit X), pages 79?86, Phuket, Thailand.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identification : how to distinguish similar
languages ? In 29th International Conference on Information Technology Interfaces, pages 541?546.
Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In Proceed-
ings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 553?561,
Chiang Mai, Thailand.
137
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages
25?30, Jeju, Republic of Korea.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013, pages 5?15, Brisbane, Australia.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of
the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.
European Language Resources Association (ELRA).
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase Structure Grammar. University of Chicago Press,
Chicago, USA.
Jan Pomik?alek. 2011. Removing Boilerplate and Duplicate Content from Web Corpora. Ph.D. thesis, Masaryk
University.
John M. Prager. 1999. Linguini: language identification for multilingual documents. In Proceedings of the 32nd
Annual Hawaii International Conference on Systems Sciences (HICSS-32), Maui, Hawaii.
Bali Ranaivo-Malancon. 2006. Automatic Identification of Close Languages - Case study : Malay and Indonesian.
ECTI Transaction on Computer and Information Technology, 2(2):126?134.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Conference
on New Methods in Natural Language Processing, Manchester, 1994.
Raivis Skadin??s, J?org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free:
Building and using the EU Bookshop corpus. In Proceedings of the 9th International Conference on Language
Resources and Evaluation (LREC-2014), Reykjavik, Iceland.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
W. J. Teahan. 2000. Text Classification and Segmentation Using Minimum Cross-Entropy. In Proceedings the 6th
International Conference ?Recherche dInformation Assistee par Ordinateur? (RIAO00), pages 943?961, Paris,
France.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2619?
2634, Mumbai, India.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the 8th International
Conference on Language Resources and Evaluation (LREC 2012), pages 2214?2218, Istanbul, Turkey.
Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL
?03), pages 173?180, Edmonton, Canada.
Peter Trudgill and Jean Hannah. 2008. International English: A guide to varieties of Standard English. Hodder
Education, London, UK, 5th edition.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proceed-
ings of the Australasian Language Technology Workshop 2009 (ALTW 2009), pages 53?61, Sydney, Australia.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS 2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN 2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri, Liling Tan, Nikola Ljube?si?c, and J?org Tiedemann. 2014. A report on the DSL shared task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial), Dublin, Ireland.
138
