Proceedings of the ACL 2010 Conference Short Papers, pages 231?235,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Online Generation of Locality Sensitive Hash Signatures
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Baltimore, MD 21211 USA
Ashwin Lall
College of Computing
Georgia Institute of Technology
Atlanta, GA 30332 USA
Abstract
Motivated by the recent interest in stream-
ing algorithms for processing large text
collections, we revisit the work of
Ravichandran et al (2005) on using the
Locality Sensitive Hash (LSH) method of
Charikar (2002) to enable fast, approxi-
mate comparisons of vector cosine simi-
larity. For the common case of feature
updates being additive over a data stream,
we show that LSH signatures can be main-
tained online, without additional approxi-
mation error, and with lower memory re-
quirements than when using the standard
offline technique.
1 Introduction
There has been a surge of interest in adapting re-
sults from the streaming algorithms community to
problems in processing large text collections. The
term streaming refers to a model where data is
made available sequentially, and it is assumed that
resource limitations preclude storing the entirety
of the data for offline (batch) processing. Statis-
tics of interest are approximated via online, ran-
domized algorithms. Examples of text applica-
tions include: collecting approximate counts (Tal-
bot, 2009; Van Durme and Lall, 2009a), finding
top-n elements (Goyal et al, 2009), estimating
term co-occurrence (Li et al, 2008), adaptive lan-
guage modeling (Levenberg and Osborne, 2009),
and building top-k ranklists based on pointwise
mutual information (Van Durme and Lall, 2009b).
Here we revisit the work of Ravichandran et al
(2005) on building word similarity measures from
large text collections by using the Locality Sensi-
tive Hash (LSH) method of Charikar (2002). For
the common case of feature updates being addi-
tive over a data stream (such as when tracking
lexical co-occurrence), we show that LSH signa-
tures can be maintained online, without additional
approximation error, and with lower memory re-
quirements than when using the standard offline
technique.
We envision this method being used in conjunc-
tion with dynamic clustering algorithms, for a va-
riety of applications. For example, Petrovic et al
(2010) made use of LSH signatures generated over
individual tweets, for the purpose of first story de-
tection. Streaming LSH should allow for the clus-
tering of Twitter authors, based on the tweets they
generate, with signatures continually updated over
the Twitter stream.
2 Locality Sensitive Hashing
We are concerned with computing the cosine sim-
ilarity of feature vectors, defined for a pair of vec-
tors ~u and ~v as the dot product normalized by their
lengths:
cosine?similarity(~u,~v) =
~u ? ~v
|~u||~v|
.
This similarity is the cosine of the angle be-
tween these high-dimensional vectors and attains
a value of one (i.e., cos (0)) when the vectors are
parallel and zero (i.e., cos (pi/2)) when orthogo-
nal.
Building on the seminal work of Indyk and
Motwani (1998) on locality sensitive hashing
(LSH), Charikar (2002) presented an LSH that
maps high-dimensional vectors to a much smaller
dimensional space while still preserving (cosine)
similarity between vectors in the original space.
The LSH algorithm computes a succinct signature
of the feature set of the words in a corpus by com-
puting d independent dot products of each feature
vector ~v with a random unit vector ~r, i.e.,
?
i viri,
and retaining the sign of the d resulting products.
Each entry of ~r is drawn from the distribution
N(0, 1), the normal distribution with zero mean
and unit variance. Charikar?s algorithm makes use
of the fact (proved by Goemans and Williamson
231
(1995) for an unrelated application) that the an-
gle between any two vectors summarized in this
fashion is proportional to the expected Hamming
distance of their signature vectors. Hence, we can
retain length d bit-signatures in the place of high
dimensional feature vectors, while preserving the
ability to (quickly) approximate cosine similarity
in the original space.
Ravichandran et al (2005) made use of this al-
gorithm to reduce the computation in searching
for similar nouns by first computing signatures for
each noun and then computing similarity over the
signatures rather than the original feature space.
3 Streaming Algorithm
In this work, we focus on features that can be
maintained additively, such as raw frequencies.1
Our streaming algorithm for this problem makes
use of the simple fact that the dot product of the
feature vector with random vectors is a linear op-
eration. This permits us to replace the vi ? ri op-
eration by vi individual additions of ri, once for
each time the feature is encountered in the stream
(where vi is the frequency of a feature and ri is the
randomly chosen Gaussian-distributed value asso-
ciated with this feature). The result of the final
computation is identical to the dot products com-
puted by the algorithm of Charikar (2002), but
the processing can now be done online. A simi-
lar technique, for stable random projections, was
independently discussed by Li et al (2008).
Since each feature may appear multiple times
in the stream, we need a consistent way to retrieve
the random values drawn from N(0, 1) associated
with it. To avoid the expense of computing and
storing these values explicitly, as is the norm, we
propose the use of a precomputed pool of ran-
dom values drawn from this distribution that we
can then hash into. Hashing into a fixed pool en-
sures that the same feature will consistently be as-
sociated with the same value drawn from N(0, 1).
This introduces some weak dependence in the ran-
dom vectors, but we will give some analysis show-
ing that this should have very limited impact on
the cosine similarity computation, which we fur-
ther support with experimental evidence (see Ta-
ble 3).
Our algorithm traverses a stream of words and
1Note that Ravichandran et al (2005) used pointwise mu-
tual information features, which are not additive since they
require a global statistic to compute.
Algorithm 1 STREAMING LSH ALGORITHM
Parameters:
m : size of pool
d : number of bits (size of resultant signature)
s : a random seed
h1, ..., hd : hash functions mapping ?s, fi? to {0, . . . ,m?1}
INITIALIZATION:
1: Initialize floating point array P [0, . . . ,m? 1]
2: Initialize H , a hashtable mapping words to floating point
arrays of size d
3: for i := 0 . . .m? 1 do
4: P [i] := random sample from N(0, 1), using s as seed
ONLINE:
1: for each word w in the stream do
2: for each feature fi associated with w do
3: for j := 1 . . . d do
4: H[w][j] := H[w][j] + P [hj(s, fi)]
SIGNATURECOMPUTATION:
1: for each w ? H do
2: for i := 1 . . . d do
3: if H[w][i] > 0 then
4: S[w][i] := 1
5: else
6: S[w][i] := 0
maintains some state for each possible word that
it encounters (cf. Algorithm 1). In particular, the
state maintained for each word is a vector of float-
ing point numbers of length d. Each element of the
vector holds the (partial) dot product of the feature
vector of the word with a random unit vector. Up-
dating the state for a feature seen in the stream for
a given word simply involves incrementing each
position in the word?s vector by the random value
associated with the feature, accessed by hash func-
tions h1 through hd. At any point in the stream,
the vector for each word can be processed (in time
O(d)) to create a signature computed by checking
the sign of each component of its vector.
3.1 Analysis
The update cost of the streaming algorithm, per
word in the stream, is O(df), where d is the target
signature size and f is the number of features asso-
ciated with each word in the stream.2 This results
in an overall cost of O(ndf) for the streaming al-
gorithm, where n is the length of the stream. The
memory footprint of our algorithm isO(n0d+m),
where n0 is the number of distinct words in the
stream and m is the size of the pool of normally
distributed values. In comparison, the original
LSH algorithm computes signatures at a cost of
O(nf + n0dF ) updates and O(n0F + dF + n0d)
memory, where F is the (large) number of unique
2For the bigram features used in ? 4, f = 2.
232
features. Our algorithm is superior in terms of
memory (because of the pooling trick), and has the
benefit of supporting similarity queries online.
3.2 Pooling Normally-distributed Values
We now discuss why it is possible to use a
fixed pool of random values instead of generating
unique ones for each feature. Let g be the c.d.f.
of the distribution N(0, 1). It is easy to see that
picking x ? (0, 1) uniformly results in g?1(x) be-
ing chosen with distribution N(0, 1). Now, if we
select for our pool the values
g?1(1/m), g?1(2/m), . . . , g?1(1? 1/m),
for some sufficiently large m, then this is identical
to sampling from N(0, 1) with the caveat that the
accuracy of the sample is limited. More precisely,
the deviation from sampling from this pool is off
from the actual value by at most
max
i=1,...,m?2
{g?1((i+ 1)/m)? g?1(i/m)}.
By choosing m to be sufficiently large, we can
bound the error of the approximate sample from
a true sample (i.e., the loss in precision expressed
above) to be a small fraction (e.g., 1%) of the ac-
tual value. This would result in the same relative
error in the computation of the dot product (i.e.,
1%), which would almost never affect the sign of
the final value. Hence, pooling as above should
give results almost identical to the case where all
the random values were chosen independently. Fi-
nally, we make the observation that, for large m,
randomly choosing m values from N(0, 1) results
in a set of values that are distributed very similarly
to the pool described above. An interesting avenue
for future work is making this analysis more math-
ematically precise.
3.3 Extensions
Decay The algorithm can be extended to support
temporal decay in the stream, where recent obser-
vations are given higher relative weight, by mul-
tiplying the current sums by a decay value (e.g.,
0.9) on a regular interval (e.g., once an hour, once
a day, once a week, etc.).
Distributed The algorithm can be easily dis-
tributed across multiple machines in order to pro-
cess different parts of a stream, or multiple differ-
ent streams, in parallel, such as in the context of
the MapReduce framework (Dean and Ghemawat,
(a)
(b)
Figure 1: Predicted versus actual cosine values for 50,000
pairs, using LSH signatures generated online, with d = 32 in
Fig. 1(a) and d = 256 in Fig. 1(b).
2004). The underlying operation is a linear op-
erator that is easily composed (i.e., via addition),
and the randomness between machines can be tied
based on a shared seed s. At any point in process-
ing the stream(s), current results can be aggregated
by summing the d-dimensional vectors for each
word, from each machine.
4 Experiments
Similar to the experiments of Ravichandran et
al. (2005), we evaluated the fidelity of signature
generation in the context of calculating distribu-
tional similarity between words across a large
text collection: in our case, articles taken from
the NYTimes portion of the Gigaword corpus
(Graff, 2003). The collection was processed as a
stream, sentence by sentence, using bigram fea-
233
d 16 32 64 128 256
SLSH 0.2885 0.2112 0.1486 0.1081 0.0769
LSH 0.2892 0.2095 0.1506 0.1083 0.0755
Table 1: Mean absolute error when using signatures gener-
ated online (StreamingLSH), compared to offline (LSH).
tures. This gave a stream of 773,185,086 tokens,
with 1,138,467 unique types. Given the number
of types, this led to a (sparse) feature space with
dimension on the order of 2.5 million.
After compiling signatures, fifty-thousand
?x, y? pairs of types were randomly sampled
by selecting x and y each independently, with
replacement, from those types with at least 10 to-
kens in the stream (where 310,327 types satisfied
this constraint). The true cosine values between
each such x and y was computed based on offline
calculation, and compared to the cosine similarity
predicted by the Hamming distance between the
signatures for x and y. Unless otherwise specified,
the random pool size was fixed at m = 10, 000.
Figure 1 visually reaffirms the trade-off in LSH
between the number of bits and the accuracy of
cosine prediction across the range of cosine val-
ues. As the underlying vectors are strictly posi-
tive, the true cosine is restricted to [0, 1]. Figure 2
shows the absolute error between truth and predic-
tion for a similar sample, measured using signa-
tures of a variety of bit lengths. Here we see hori-
zontal bands arising from truly orthogonal vectors
leading to step-wise absolute error values tracked
to Hamming distance.
Table 1 compares the online and batch LSH al-
gorithms, giving the mean absolute error between
predicted and actual cosine values, computed for
the fifty-thousand element sample, using signa-
tures of various lengths. These results confirm that
we achieve the same level of accuracy with online
updates as compared to the standard method.
Figure 3 shows how a pool size as low as m =
100 gives reasonable variation in random values,
and that m = 10, 000 is sufficient. When using a
standard 32 bit floating point representation, this
is just 40 KBytes of memory, as compared to, e.g.,
the 2.5 GBytes required to store 256 random vec-
tors each containing 2.5 million elements.
Table 2 is based on taking an example for each
of three part-of-speech categories, and reporting
the resultant top-5 words as according to approx-
imated cosine similarity. Depending on the in-
tended application, these results indicate a range
Figure 2: Absolute error between predicted and true co-
sine for a sample of pairs, when using signatures of length
log2(d) ? {4, 5, 6, 7, 8}, drawn with added jitter to avoid
overplotting.
Pool Size
Mean
 Abso
lute E
rror
0.2
0.4
0.6
0.8 l
l
l l l l l
101 102 103 104 105
Figure 3: Error versus pool size, when using d = 256.
of potentially sufficient signature lengths.
5 Conclusions
We have shown that when updates to a feature vec-
tor are additive, it is possible to convert the offline
LSH signature generation method into a stream-
ing algorithm. In addition to allowing for on-
line querying of signatures, our approach leads to
space efficiencies, as it does not require the ex-
plicit representation of either the feature vectors,
nor the random matrix. Possibilities for future
work include the pairing of this method with algo-
rithms for dynamic clustering, as well as exploring
algorithms for different distances (e.g., L2) and es-
timators (e.g., asymmetric estimators (Dong et al,
2009)).
234
London
Milan.97, Madrid.96, Stockholm.96, Manila.95, Moscow.95
ASHER0, Champaign0, MANS0, NOBLE0, come0
Prague1, Vienna1, suburban1, synchronism1, Copenhagen2
Frankfurt4, Prague4, Taszar5, Brussels6, Copenhagen6
Prague12, Stockholm12, Frankfurt14, Madrid14, Manila14
Stockholm20, Milan22, Madrid24, Taipei24, Frankfurt25
in
during.99, on.98, beneath.98, from.98, onto.97
Across0, Addressing0, Addy0, Against0, Allmon0
aboard0, mishandled0, overlooking0, Addressing1, Rejecting1
Rejecting2, beneath2, during2, from3, hamstringing3
during4, beneath5, of6, on7, overlooking7
during10, on13, beneath15, of17, overlooking17
sold
deployed.84, presented.83, sacrificed.82, held.82, installed.82
Bustin0, Diors0, Draining0, Kosses0, UNA0
delivered2, held2, marks2, seared2, Ranked3
delivered5, rendered5, presented6, displayed7, exhibited7
held18, rendered18, presented19, deployed20, displayed20
presented41, rendered42, held47, leased47, reopened47
Table 2: Top-5 items based on true cosine (bold), then using
minimal Hamming distance, given in top-down order when
using signatures of length log2(d) ? {4, 5, 6, 7, 8}. Ties bro-
ken lexicographically. Values given as subscripts.
Acknowledgments
Thanks to Deepak Ravichandran, Miles Osborne,
Sasa Petrovic, Ken Church, Glen Coppersmith,
and the anonymous reviewers for their feedback.
This work began while the first author was at the
University of Rochester, funded by NSF grant IIS-
1016735. The second author was supported in
part by NSF grant CNS-0905169, funded under
the American Recovery and Reinvestment Act of
2009.
References
Moses Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of STOC.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified Data Processing on Large Clusters.
In Proceedings of OSDI.
Wei Dong, Moses Charikar, and Kai Li. 2009. Asym-
metric distance estimation with sketches for similar-
ity search in high-dimensional spaces. In Proceed-
ings of SIGIR.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum
cut and satisfiability problems using semidefinite
programming. JACM, 42:1115?1145.
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Abby Levenberg and Miles Osborne. 2009. Stream-
based Randomised Language Models for SMT. In
Proceedings of EMNLP.
Ping Li, Kenneth W. Church, and Trevor J. Hastie.
2008. One Sketch For All: Theory and Application
of Conditional Random Sampling. In Advances in
Neural Information Processing Systems 21.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with appli-
cation to Twitter. In Proceedings of NAACL.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized Algorithms and NLP:
Using Locality Sensitive Hash Functions for High
Speed Noun Clustering. In Proceedings of ACL.
David Talbot. 2009. Succinct approximate counting of
skewed data. In Proceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009a. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming Pointwise Mutual Information. In Ad-
vances in Neural Information Processing Systems
22.
235
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 18?23,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Efficient Online Locality Sensitive Hashing via Reservoir Counting
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Ashwin Lall
Mathematics and Computer Science
Denison University
Abstract
We describe a novel mechanism called Reser-
voir Counting for application in online Local-
ity Sensitive Hashing. This technique allows
for significant savings in the streaming setting,
allowing for maintaining a larger number of
signatures, or an increased level of approxima-
tion accuracy at a similar memory footprint.
1 Introduction
Feature vectors based on lexical co-occurrence are
often of a high dimension, d. This leads to O(d) op-
erations to calculate cosine similarity, a fundamental
tool in distributional semantics. This is improved in
practice through the use of data structures that ex-
ploit feature sparsity, leading to an expected O(f)
operations, where f is the number of unique features
we expect to have non-zero entries in a given vector.
Ravichandran et al (2005) showed that the Lo-
cality Sensitive Hash (LSH) procedure of Charikar
(2002), following from Indyk and Motwani (1998)
and Goemans and Williamson (1995), could be suc-
cessfully used to compress textually derived fea-
ture vectors in order to achieve speed efficiencies
in large-scale noun clustering. Such LSH bit signa-
tures are constructed using the following hash func-
tion, where ~v ? Rd is a vector in the original feature
space, and ~r is randomly drawn from N(0, 1)d:
h(~v) =
{
1 if ~v ? ~r ? 0,
0 otherwise.
If hb(~v) is the b-bit signature resulting from b such
hash functions, then the cosine similarity between
vectors ~u and ~v is approximated by:
cos(~u,~v) = ~u?~v|~u||~v| ? cos(
D(hb(~u),hb(~v))
b ? pi),
where D(?, ?) is Hamming distance, the number of
bits that disagree. This technique is used when
b  d, which leads to faster pair-wise comparisons
between vectors, and a lower memory footprint.
Van Durme and Lall (2010) observed1 that if
the feature values are additive over a dataset (e.g.,
when collecting word co-occurrence frequencies),
then these signatures may be constructed online by
unrolling the dot-product into a series of local oper-
ations: ~v ?~ri = ?t~vt ?~ri, where ~vt represents features
observed locally at time t in a data-stream.
Since updates may be done locally, feature vec-
tors do not need to be stored explicitly. This di-
rectly leads to significant space savings, as only one
counter is needed for each of the b running sums.
In this work we focus on the following observa-
tion: the counters used to store the running sums
may themselves be an inefficient use of space, in
that they may be amenable to compression through
approximation.2 Since the accuracy of this LSH rou-
tine is a function of b, then if we were able to reduce
the online requirements of each counter, we might
afford a larger number of projections. Even if a
chance of approximation error were introduced for
each hash function, this may be justified in greater
overall fidelity from the resultant increase in b.
1A related point was made by Li et al (2008) when dis-
cussing stable random projections.
2A b bit signature requires the online storage of b?32 bits of
memory when assuming a 32-bit floating point representation
per counter, but since here the only thing one cares about these
sums are their sign (positive or negative) then an approximation
to the true sum may be sufficient.
18
Thus, we propose to approximate the online hash
function, using a novel technique we call Reservoir
Counting, in order to create a space trade-off be-
tween the number of projections and the amount of
memory each projection requires. We show experi-
mentally that this leads to greater accuracy approx-
imations at the same memory cost, or similar accu-
racy approximations at a significantly reduced cost.
This result is relevant to work in large-scale distribu-
tional semantics (Bhagat and Ravichandran, 2008;
Van Durme and Lall, 2009; Pantel et al, 2009; Lin
et al, 2010; Goyal et al, 2010; Bergsma and Van
Durme, 2011), as well as large-scale processing of
social media (Petrovic et al, 2010).
2 Approach
While not strictly required, we assume here to be
dealing exclusively with integer-valued features. We
then employ an integer-valued projection matrix in
order to work with an integer-valued stream of on-
line updates, which is reduced (implicitly) to a
stream of positive and negative unit updates. The
sign of the sum of these updates is approximated
through a novel twist on Reservoir Sampling. When
computed explicitly this leads to an impractical
mechanism linear in each feature value update. To
ensure our counter can (approximately) add and sub-
tract in constant time, we then derive expressions for
the expected value of each step of the update. The
full algorithms are provided at the close.
Unit Projection Rather than construct a projec-
tion matrix from N(0, 1), a matrix randomly pop-
ulated with entries from the set {?1, 0, 1} will suf-
fice, with quality dependent on the relative propor-
tion of these elements. If we let p be the percent
probability mass allocated to zeros, then we create
a discrete projection matrix by sampling from the
multinomial: (1?p2 : ?1, p : 0,
1?p
2 : +1). An
experiment displaying the resultant quality is dis-
played in Fig. 1, for varied p. Henceforth we assume
this discrete projection matrix, with p = 0.5.3 The
use of such sparse projections was first proposed by
Achlioptas (2003), then extended by Li et al (2006).
3Note that if using the pooling trick of Van Durme and Lall
(2010), this equates to a pool of the form: (-1,0,0,1).
Percent.Zeros
Mean
.Abso
lute.E
rror
0.1
0.2
0.3
0.4
0.5
0.2 0.4 0.6 0.8 1.0
MethodDiscreteNormal
Figure 1: With b = 256, mean absolute error in cosine
approximation when using a projection based onN(0, 1),
compared to {?1, 0, 1}.
Unit Stream Based on a unit projection, we can
view an online counter as summing over a stream
drawn from {?1, 1}: each projected feature value
unrolled into its (positive or negative) unary repre-
sentation. For example, the stream: (3,-2,1), can be
viewed as the updates: (1,1,1,-1,-1,1).
Reservoir Sampling We can maintain a uniform
sample of size k over a stream of unknown length
as follows. Accept the first k elements into an reser-
voir (array) of size k. Each following element at po-
sition n is accepted with probability kn , whereupon
an element currently in the reservoir is evicted, and
replaced with the just accepted item. This scheme
is guaranteed to provide a uniform sample, where
early items are more likely to be accepted, but also at
greater risk of eviction. Reservoir sampling is a folk-
lore algorithm that was extended by Vitter (1985) to
allow for multiple updates.
Reservoir Counting If we are sampling over a
stream drawn from just two values, we can implic-
itly represent the reservoir by counting only the fre-
quency of one or the other elements.4 We can there-
fore sample the proportion of positive and negative
unit values by tracking the current position in the
stream, n, and keeping a log2(k + 1)-bit integer
4For example, if we have a reservoir of size 5, containing
three values of ?1, and two values of 1, then the exchangeabil-
ity of the elements means the reservoir is fully characterized by
knowing k, and that there are two 1?s.
19
counter, s, for tracking the number of 1 values cur-
rently in the reservoir.5 When a negative value is
accepted, we decrement the counter with probability
s
k . When a positive update is accepted, we increment
the counter with probability (1? sk ). This reflects an
update evicting either an element of the same sign,
which has no effect on the makeup of the reservoir,
or decreasing/increasing the number of 1?s currently
sampled. An approximate sum of all values seen up
to position n is then simply: n(2sk ? 1). While this
value is potentially interesting in future applications,
here we are only concerned with its sign.
Parallel Reservoir Counting On its own this
counting mechanism hardly appears useful: as it is
dependent on knowing n, then we might just as well
sum the elements of the stream directly, counting in
whatever space we would otherwise use in maintain-
ing the value of n. However, if we have a set of tied
streams that we process in parallel,6 then we only
need to track n once, across b different streams, each
with their own reservoir.
When dealing with parallel streams resulting from
different random projections of the same vector, we
cannot assume these will be strictly tied. Some pro-
jections will cancel out heavier elements than oth-
ers, leading to update streams of different lengths
once elements are unrolled into their (positive or
negative) unary representation. In practice we have
found that tracking the mean value of n across b
streams is sufficient. When using a p = 0.5 zeroed
matrix, we can update n by one half the magnitude
of each observed value, as on average half the pro-
jections will cancel out any given element. This step
can be found in Algorithm 2, lines 8 and 9.
Example To make concrete what we have cov-
ered to this point, consider a given feature vec-
tor of dimensionality d = 3, say: [3, 2, 1]. This
might be projected into b = 4, vectors: [3, 0, 0],
[0, -2, 1], [0, 0, 1], and [-3, 2, 0]. When viewed as
positive/negative, loosely-tied unit streams, they re-
spectively have length n: 3, 3, 1, and 5, with mean
length 3. The goal of reservoir counting is to effi-
ciently keep track of an approximation of their sums
(here: 3, -1, 1, and -1), while the underlying feature
5E.g., a reservoir of size k = 255 requires an 8-bit integer.
6Tied in the sense that each stream is of the same length,
e.g., (-1,1,1) is the same length as (1,-1,-1).
k n m mean(A) mean(A?)
10 20 10 3.80 4.02
10 20 1000 37.96 39.31
50 150 1000 101.30 101.83
100 1100 100 8.88 8.72
100 10100 10 0.13 0.10
Table 1: Average over repeated calls to A and A?.
vector is being updated online. A k = 3 reservoir
used for the last projected vector, [-3, 2, 0], might
reasonably contain two values of -1, and one value
of 1.7 Represented explicitly as a vector, the reser-
voir would thus be in the arrangement:
[1, -1, -1], [-1, 1, -1], or [-1, -1, 1].
These are functionally equivalent: we only need to
know that one of the k = 3 elements is positive.
Expected Number of Samples Traversingm con-
secutive values of either 1 or ?1 in the unit stream
should be thought of as seeing positive or negative
m as a feature update. For a reservoir of size k, let
A(m,n, k) be the number of samples accepted when
traversing the stream from position n+ 1 to n+m.
A is non-deterministic: it represents the results of
flipping m consecutive coins, where each coin is in-
creasingly biased towards rejection.
Rather than computing A explicitly, which is lin-
ear inm, we will instead use the expected number of
updates, A?(m,n, k) = E[A(m,n, k)], which can
be computed in constant time. Where H(x) is the
harmonic number of x:8
A?(m,n, k) =
n+m?
i=n+1
k
i
= k(H(n+m)?H(n))
? k loge(
n+m
n
).
For example, consider m = 30, encountered at
position n = 100, with a reservoir of k = 10. We
will then accept 10 loge(
130
100) ? 3.79 samples of 1.
As the reservoir is a discrete set of bins, fractional
portions of a sample are resolved by a coin flip: if
a = k loge(
n+m
n ), then accept u = dae samples
with probability (a ? bac), and u = bac samples
7Other options are: three -1?s, or one -1 and two 1?s.
8With x a positive integer,H(x) =
?x
i=1 1/x ? loge(x)+
?, where ? is Euler?s constant.
20
otherwise. These steps are found in lines 3 and 4
of Algorithm 1. See Table 1 for simulation results
using a variety of parameters.
Expected Reservoir Change We now discuss
how to simulate many independent updates of the
same type to the reservoir counter, e.g.: five updates
of 1, or three updates of -1, using a single estimate.
Consider a situation in which we have a reservoir of
size k with some current value of s, 0 ? s ? k, and
we wish to perform u independent updates. We de-
note by U ?k(s, u) the expected value of the reservoir
after these u updates have taken place. Since a sin-
gle update leads to no change with probability sk , we
can write the following recurrence for U ?k:
U ?k(s, u) =
s
k
U ?k(s, u?1)+
k ? s
k
U ?k(s+1, u?1),
with the boundary condition: for all s, U ?k(s, 0) = s.
Solving the above recurrence, we get that the ex-
pected value of the reservoir after these updates is:
U ?k(s, u) = k + (s? k)
(
1?
1
k
)u
,
which can be mechanically checked via induction.
The case for negative updates follows similarly (see
lines 7 and 8 of Algorithm 1).
Hence, instead of simulating u independent up-
dates of the same type to the reservoir, we simply
update it to this expected value, where fractional up-
dates are handled similarly as when estimating the
number of accepts. These steps are found in lines 5
through 9 of Algorithm 1, and as seen in Fig. 2, this
can give a tight estimate.
Comparison Simulation results over Zipfian dis-
tributed data can be seen in Fig. 3, which shows the
use of reservoir counting in Online Locality Sensi-
tive Hashing (as made explicit in Algorithm 2), as
compared to the method described by Van Durme
and Lall (2010).
The total amount of space required when using
this counting scheme is b log2(k + 1) + 32: b reser-
voirs, and a 32 bit integer to track n. This is com-
pared to b 32 bit floating point values, as is standard.
Note that our scheme comes away with similar lev-
els of accuracy, often at half the memory cost, while
requiring larger b to account for the chance of ap-
proximation errors in individual reservoir counters.
Expected
True
50
100
150
200
250
50 100 150 200 250
Figure 2: Results of simulating many iterations of U ?,
for k = 255, and various values of s and u.
Algorithm 1 RESERVOIRUPDATE(n, k,m, ?, s)
Parameters:
n : size of stream so far
k : size of reservoir, also maximum value of s
m : magnitude of update
? : sign of update
s : current value of reservoir
1: if m = 0 or ? = 0 then
2: Return without doing anything
3: a := A?(m,n, k) = k loge
(
n+m
n
)
4: u := dae with probability a? bac, bac otherwise
5: if ? = 1 then
6: s? := U ?(s, a) = k + (s? k) (1? 1/k)u
7: else
8: s? := U ?(s, a) = s (1? 1/k)u
9: Return ds?e with probability s??bs?c, bs?c otherwise
Bits.Required
Mean
.Abso
lute.E
rror
0.06
0.07
0.08
0.09
0.10
0.11
0.12
l
l
l l
l
l
l
l l
l
l
1000 2000 3000 4000 5000 6000 7000 8000
log2.kl 8l 32
bl 64128192256512
Figure 3: Online LSH using reservoir counting (red) vs.
standard counting mechanisms (blue), as measured by the
amount of total memory required to the resultant error.
21
Algorithm 2 COMPUTESIGNATURE(S ,k,b,p)
Parameters:
S : bit array of size b
k : size of each reservoir
b : number of projections
p : percentage of zeros in projection, p ? [0, 1]
1: Initialize b reservoirs R[1, . . . , b], each represented
by a log2(k + 1)-bit unsigned integer
2: Initialize b hash functions hi(w) that map features w
to elements in a vector made up of ?1 and 1 each
with proportion 1?p2 , and 0 at proportion p.
3: n := 0
4: {Processing the stream}
5: for each feature value pair (w,m) in stream do
6: for i := 1 to b do
7: R[i] := ReservoirUpdate(n, k,m, hi(w), R[i])
8: n := n+ bm(1? p)c
9: n := n+1 with probabilitym(1?p)?bm(1?p)c
10: {Post-processing to compute signature}
11: for i := 1 . . . b do
12: if R[i] > k2 then
13: S[i] := 1
14: else
15: S[i] := 0
3 Discussion
Time and Space While we have provided a con-
stant time, approximate update mechanism, the con-
stants involved will practically remain larger than
the cost of performing single hardware addition
or subtraction operations on a traditional 32-bit
counter. This leads to a tradeoff in space vs. time,
where a high-throughput streaming application that
is not concerned with online memory requirements
will not have reason to consider the developments in
this article. The approach given here is motivated
by cases where data is not flooding in at breakneck
speed, and resource considerations are dominated by
a large number of unique elements for which we
are maintaining signatures. Empirically investigat-
ing this tradeoff is a matter of future work.
Random Walks As we here only care for the sign
of the online sum, rather than an approximation of
its actual value, then it is reasonable to consider in-
stead modeling the problem directly as a random
walk on a linear Markov chain, with unit updates
directly corresponding to forward or backward state
-4 -3 -2 -1 0 1 2 3
Figure 4: A simple 8-state Markov chain, requiring
lg(8) = 3 bits. Dark or light states correspond to a
prediction of a running sum being positive or negative.
States are numerically labeled to reflect the similarity to
a small bit integer data type, one that never overflows.
transitions. Assuming a fixed probability of a posi-
tive versus negative update, then in expectation the
state of the chain should correspond to the sign.
However if we are concerned with the global statis-
tic, as we are here, then the assumption of a fixed
probability update precludes the analysis of stream-
ing sources that contain local irregularities.9
In distributional semantics, consider a feature
stream formed by sequentially reading the n-gram
resource of Brants and Franz (2006). The pair: (the
dog : 3,502,485), can be viewed as a feature value
pair: (leftWord=?the? : 3,502,485), with respect to
online signature generation for the word dog. Rather
than viewing this feature repeatedly, spread over a
large corpus, the update happens just once, with
large magnitude. A simple chain such as seen in
Fig. 4 will be ?pushed? completely to the right or
the left, based on the polarity of the projection, irre-
spective of previously observed updates. Reservoir
Counting, representing an online uniform sample, is
agnostic to the ordering of elements in the stream.
4 Conclusion
We have presented a novel approximation scheme
we call Reservoir Counting, motivated here by a de-
sire for greater space efficiency in Online Locality
Sensitive Hashing. Going beyond our results pro-
vided for synthetic data, future work will explore ap-
plications of this technique, such as in experiments
with streaming social media like Twitter.
Acknowledgments
This work benefited from conversations with Daniel
S?tefonkovic? and Damianos Karakos.
9For instance: (1,1,...,1,1,-1,-1,-1), is overall positive, but
locally negative at the end.
22
References
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-lindenstrauss with binary coins.
J. Comput. Syst. Sci., 66:671?687, June.
Shane Bergsma and Benjamin Van Durme. 2011. Learn-
ing Bilingual Lexicons using the Visual Similarity of
Labeled Web Images. In Proc. of the International
Joint Conference on Artificial Intelligence (IJCAI).
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Surface
Patterns. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Michel X. Goemans and David P. Williamson. 1995.
Improved approximation algorithms for maximum cut
and satisfiability problems using semidefinite pro-
gramming. JACM, 42:1115?1145.
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and
Suresh Venkatasubramanian. 2010. Sketch Tech-
niques for Scaling Distributional Similarity to the
Web. In Proceedings of the ACL Workshop on GEo-
metrical Models of Natural Language Semantics.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections. In Proceedings of
the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ?06,
pages 287?296, New York, NY, USA. ACM.
Ping Li, Kenneth W. Church, and Trevor J. Hastie. 2008.
One Sketch For All: Theory and Application of Con-
ditional Random Sampling. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-Scale
Distributional Similarity and Entity Set Expansion. In
Proc. of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming First Story Detection with applica-
tion to Twitter. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Benjamin Van Durme and Ashwin Lall. 2009. Streaming
Pointwise Mutual Information. In Proc. of the Confer-
ence on Advances in Neural Information Processing
Systems (NIPS).
Benjamin Van Durme and Ashwin Lall. 2010. Online
Generation of Locality Sensitive Hash Signatures. In
Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL).
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37?57, March.
23
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 687?692,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exponential Reservoir Sampling for Streaming Language Models
Miles Osborne
?
School of Informatics
University of Edinburgh
Ashwin Lall
Mathematics and Computer Science
Denison University
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Abstract
We show how rapidly changing textual
streams such as Twitter can be modelled in
fixed space. Our approach is based upon
a randomised algorithm called Exponen-
tial Reservoir Sampling, unexplored by
this community until now. Using language
models over Twitter and Newswire as a
testbed, our experimental results based on
perplexity support the intuition that re-
cently observed data generally outweighs
that seen in the past, but that at times,
the past can have valuable signals enabling
better modelling of the present.
1 Introduction
Work by Talbot and Osborne (2007), Van Durme
and Lall (2009) and Goyal et al (2009) consid-
ered the problem of building very large language
models via the use of randomized data structures
known as sketches.
1
While efficient, these struc-
tures still scale linearly in the number of items
stored, and do not handle deletions well: if pro-
cessing an unbounded stream of text, with new
words and phrases being regularly added to the
model, then with a fixed amount of space, errors
will only increase over time. This was pointed
out by Levenberg and Osborne (2009), who inves-
tigated an alternate approach employing perfect-
hashing to allow for deletions over time. Their
deletion criterion was task-specific and based on
how a machine translation system queried a lan-
guage model.
?
Corresponding author: miles@inf.ed.ac.uk
1
Sketches provide space efficiencies that are measured on
the order of individual bits per item stored, but at the cost
of being lossy: sketches trade off space for error, where the
less space you use, the more likely you will get erroneous
responses to queries.
Here we ask what the appropriate selection
criterion is for streaming data based on a non-
stationary process, when concerned with an in-
trinsic measure such as perplexity. Using Twitter
and newswire, we pursue this via a sampling strat-
egy: we construct models over sentences based on
a sample of previously observed sentences, then
measure perplexity of incoming sentences, all on
a day by day, rolling basis. Three sampling ap-
proaches are considered: A fixed-width sliding
window of most recent content, uniformly at ran-
dom over the stream and a biased sample that
prefers recent history over the past.
We show experimentally that a moving window
is better than uniform sampling, and further that
exponential (biased) sampling is best of all. For
streaming data, recently encountered data is valu-
able, but there is also signal in the previous stream.
Our sampling methods are based on reser-
voir sampling (Vitter, 1985), a popularly known
method in some areas of computer science, but
which has seen little use within computational lin-
guistics.
2
Standard reservoir sampling is a method
for maintaining a uniform sample over a dynamic
stream of elements, using constant space. Novel
to this community, we consider a variant owing to
Aggarwal (2006) which provides for an exponen-
tial bias towards recently observed elements. This
exponential reservoir sampling has all of the guar-
antees of standard reservoir sampling, but as we
show, is a better fit for streaming textual data. Our
approach is fully general and can be applied to any
streaming task where we need to model the present
and can only use fixed space.
2
Exceptions include work by Van Durme and Lall (2011)
and Van Durme (2012), aimed at different problems than that
explored here.
687
2 Background
We address two problems: language changes over
time, and the observation that space is a problem,
even for compact sketches.
Statistical language models often assume either
a local Markov property (when working with ut-
terances, or sentences), or that content is gener-
ated fully i.i.d. (such as in document-level topic
models). However, language shows observable
priming effects, sometimes called triggers, where
the occurrence of a given term decreases the sur-
prisal of some other term later in the same dis-
course (Lau et al, 1993; Church and Gale, 1995;
Beeferman et al, 1997; Church, 2000). Conven-
tional cache and trigger models typically do not
deal with new terms and can be seen as adjusting
the parameters of a fixed model.
Accounting for previously unseen entries in a
language model can be naively simple: as they ap-
pear in new training data, add them to the model!
However in practice we are constrained by avail-
able space: how many unique phrases can we
store, given the target application environment?
Our work is concerned with modeling language
that might change over time, in accordance with
current trending discourse topics, but under a strict
space constraint. With a fixed amount of memory
available, we cannot allow our list of unique words
or phrases to grow over time, even while new top-
ics give rise to novel names of people, places, and
terms of interest. Thus we need an approach that
keeps the size of the model constant, but that is
geared to what is being discussed now, as com-
pared to some time in the past.
3 Reservoir Sampling
3.1 Uniform Reservoir Sampling
The reservoir sampling algorithm (Vitter, 1985) is
the classic method of sampling without replace-
ment from a stream in a single pass when the
length of the stream is of indeterminate or un-
bounded length. Say that the size of the desired
sample is k. The algorithm proceeds by retain-
ing the first k items of the stream and then sam-
pling each subsequent element with probability
f(k, n) = k/n, where n is the length of the stream
so far. (See Algorithm 1.) It is easy to show via in-
duction that, at any time, all the items in the stream
so far have equal probability of appearing in the
reservoir.
The algorithm processes the stream in a single
pass?that is, once it has processed an item in the
stream, it does not revisit that item unless it is
stored in the reservoir. Given this restriction, the
incredible feature of this algorithm is that it is able
to guarantee that the samples in the reservoir are a
uniformly random sample with no unintended bi-
ases even as the stream evolves. This makes it an
excellent candidate for situations when the stream
is continuously being updated and it is computa-
tionally infeasible to store the entire stream or to
make more than a single pass over it. Moreover,
it is an extremely efficient algorithm as it requires
O(1) time (independent of the reservoir size and
stream length) for each item in the stream.
Algorithm 1 Reservoir Sampling Algorithm
Parameters:
k: maximum size of reservoir
1: Initialize an empty reservoir (any container
data type).
2: n := 1
3: for each item in the stream do
4: if n < k then
5: insert current item into the reservoir
6: else
7: with probability f(n, k), eject an ele-
ment of the reservoir chosen uniformly
at random and insert current item into the
reservoir
8: n := n+ 1
3.2 Non-uniform Reservoir Sampling
Here we will consider generalizations of the reser-
voir sampling algorithm in which the sample
items in the reservoir are more biased towards the
present. Put another way, we will continuously
decay the probability that an older item will ap-
pear in the reservoir. Models produced using such
biases put more modelling stress on the present
than models produced using data that is selected
uniformly from the stream. The goal here is to
continuously update the reservoir sample in such
a way that the decay of older items is done consis-
tently while still maintaining the benefits of reser-
voir sampling, including the single pass and mem-
ory/time constraints.
The time-decay scheme we will study in this
paper is exponential bias towards newer items in
the stream. More precisely, we wish for items that
688
0 2000 4000 6000 8000 10000time
0.0
0.2
0.4
0.6
0.8
1.0
prob
abil
ity o
f ap
pea
ring
 in r
ese
rvoi
r
uniformexponential (various beta)
Figure 1: Different biases for sampling a stream
have age a in the stream to appear with probability
g(a) = c ? exp (?a/?),
where a is the age of the item, ? is a scale param-
eter indicating how rapidly older items should be
deemphasized, and c is a normalization constant.
To give a sense of what these time-decay proba-
bilities look like, some exponential distributions
are plotted (along with the uniform distribution)
in Figure 1.
Aggarwal (2006) studied this problem and
showed that by altering the sampling probability
(f(n, k) in Algorithm 1) in the reservoir sampling
algorithm, it is possible to achieve different age-
related biases in the sample. In particular, he
showed that by setting the sampling probability to
the constant function f(n, k) = k/?, it is possible
to approximately achieve exponential bias in the
sample with scale parameter ? (Aggarwal, 2006).
Aggarwal?s analysis relies on the parameter ? be-
ing very large. In the next section we will make
the analysis more precise by omitting any such as-
sumption.
3.3 Analysis
In this section we will derive an expression for the
bias introduced by an arbitrary sampling function
f in Algorithm 1. We will then use this expression
to derive the precise sampling function needed to
achieve exponential decay.
3
Careful selection of
f allows us to achieve anything from zero decay
(i.e., uniform sampling of the entire stream) to
exponential decay. Once again, note that since
we are only changing the sampling function, the
3
Specifying an arbitrary decay function remains an open
problem.
one-pass, memory- and time-efficient properties
of reservoir sampling are still being preserved.
In the following analysis, we fix n to be the size
of the stream at some fixed time and k to be the
size of the reservoir. We assume that the ith el-
ement of the stream is sampled with probability
f(i, k), for i ? n. We can then derive the proba-
bility that an element of age a will still be in the
reservoir as
g(a) = f(n? a, k)
n
?
t=n?a+1
(
1?
f(t, k)
k
)
,
since it would have been sampled with probability
f(n? a, k) and had independent chances of being
replaced at times t = n?a+1, . . . , n with proba-
bility f(t, k)/k. For instance, when f(x, k) =
k
x
,
the above formula simplifies down to g(a) =
k
n
(i.e., the uniform sampling case).
For the exponential case, we fix the sampling
rate to some constant f(n, k) = p
k
, and we wish
to determine what value to use for p
k
to achieve
a given exponential decay rate g(a) = ce
?a/?
,
where c is the normalization constant (to make g a
probability distribution) and ? is the scale param-
eter of the exponential distribution. Substituting
f(n, k) = p
k
in the above formula and equating
with the decay rate, we get that p
k
(1 ? p
k
/k)
a
?
ce
?a/?
, which must hold true for all possible val-
ues of a. After some algebra, we get that when
f(x, k) = p
k
= k(1 ? e
?1/?
), the probability
that an item with age a is included in the reser-
voir is given by the exponential decay rate g(a) =
p
k
e
?a/?
. Note that, for very large values of ?, this
probability is approximately equal to p
k
? k/?
(by using the approximation e
?x
? 1 ? x, when
|x| is close to zero), as given by Aggarwal, but our
formula gives the precise sampling probability and
works even for smaller values of ?.
4 Experiments
Our experiments use two streams of data to illus-
trate exponential sampling: Twitter and a more
conventional newswire stream. The Twitter data is
interesting as it is very multilingual, bursty (for ex-
ample, it talks about memes, breaking news, gos-
sip etc) and written by literally millions of differ-
ent people. The newswire stream is a lot more well
behaved and serves as a control.
4.1 Data, Models and Evaluation
We used one month of chronologically ordered
Twitter data and divided it into 31 equal sized
689
Stream Interval Total (toks) Test (toks)
Twitter Dec 2013 3282M 105M
Giga 1994 ? 2010 635.5M 12M
Table 1: Stream statistics
blocks (roughly corresponding with days). We
also used the AFP portion of the Giga Word corpus
as another source of data that evolves at a slower
pace. This data was divided into 50 equal sized
blocks. Table 1 gives statistics about the data. As
can be seen, the Twitter data is vastly larger than
newswire and arrives at a much faster rate.
We considered the following models. Each one
(apart from the exact model) was trained using the
same amount of data:
? Static. This model was trained using data
from the start of the duration and never var-
ied. It is a baseline.
? Exact. This model was trained using all
available data from the start of the stream and
acts as an upper bound on performance.
? Moving Window. This model used all data
in a fixed-sized window immediately before
the given test point.
? Uniform. Here, we use uniform reservoir
sampling to select the data.
? Exponential. Lastly, we use exponen-
tial reservoir sampling to select the data.
This model is parameterised, indicating how
strongly biased towards the present the sam-
ple will be. The ? parameter is a multiplier
over the reservoir length. For example, a ?
value of 1.1 with a sample size of 10 means
the value is 11. In general, ? always needs to
be bigger than the reservoir size.
We sample over whole sentences (or Tweets)
and not ngrams.
4
Using ngrams instead would
give us a finer-grained control over results, but
would come at the expense of greatly complicat-
ing the analysis. This is because we would need to
reason about not just a set of items but a multiset
of items. Note that because the samples are large
5
,
variations across samples will be small.
4
A consequence is that we do not guarantee that each sam-
ple uses exactly the same number of grams. This can be tack-
led by randomly removing sampled sentences.
5
Each day consists of approximately four million Tweets
and we evaluate on a whole day.
Day Uniform ? value
? 1.1 1.3 1.5 2.0
5 619.4 619.4 619.4 619.4 619.4
6 601.0 601.0 603.8 606.6 611.1
7 603.0 599.4 602.7 605.6 612.1
8 614.6 607.7 611.9 614.3 621.6
9 623.3 611.5 615.0 620.0 628.1
10 656.2 643.1 647.2 650.1 658.0
12 646.6 628.9 633.0 636.5 644.6
15 647.7 628.7 630.4 634.5 641.6
20 636.7 605.3 608.4 610.8 618.4
25 631.5 601.9 603.3 604.4 610.0
Table 2: Perplexities for different ? values over
Twitter (sample size = five days). Lower is better.
We test the model on unseen data from all of the
next day (or block). Afterwards, we advance to the
next day (block) and repeat, potentially incorpo-
rating the previously seen test data into the current
training data. Evaluation is in terms of perplexity
(which is standard for language modelling).
We used KenLM for building models and eval-
uating them (Heafield, 2011). Each model was
an unpruned trigram, with Kneser-Ney smoothing.
Increasing the language model order would not
change the results. Here the focus is upon which
data is used in a model (that is, which data is added
and which data is removed) and not upon making
it compact or making retraining efficient.
4.2 Varying the ? Parameter
Table 2 shows the effect of varying the ? param-
eter (using Twitter). The higher the ? value, the
more uniform the sampling. As can be seen, per-
formance improves when sampling becomes more
biased. Not shown here, but for Twitter, even
smaller ? values produce better results and for
newswire, results degrade. These differences are
small and do not affect any conclusions made here.
In practise, this value would be set using a devel-
opment set and to simplify the rest of the paper, all
other experiments use the same ? value (1.1).
4.3 Varying the Amount of Data
Does the amount of data used in a model affect re-
sults? Table 3 shows the results for Twitter when
varying the amount of data in the sample and us-
ing exponential sampling (? = 1.1). In paren-
theses for each result, we show the corresponding
moving window results. As expected, using more
data improves results. We see that for each sample
size, exponential sampling outperforms our mov-
ing window. In the limit, all sampling methods
would produce the same results.
690
Day Sample Size (Days)
1 2 3
5 652.5 (661.2) 629.1 (635.8) 624.8 (625.9)
6 635.4 (651.6) 611.6 (620.8) 604.0 (608.7)
7 636.0 (647.3) 611.0 (625.2) 603.7 (612.5)
8 654.8 (672.7) 625.6 (641.6) 614.6 (626.9)
9 653.9 (662.8) 628.3 (643.0) 618.8 (632.2)
10 679.1 (687.8) 654.3 (666.8) 646.6 (659.7)
12 671.1 (681.9) 645.8 (658.6) 633.8 (647.5)
15 677.7 (697.9) 647.4 (668.0) 636.4 (652.6)
20 648.1 (664.6) 621.4 (637.9) 612.2 (627.6)
25 657.5 (687.5) 625.3 (664.4) 613.4 (641.8)
Table 3: Perplexities for different sample sizes
over Twitter. Lower is better.
4.4 Alternative Sampling Strategies
Table 4 compares the two baselines against the two
forms of reservoir sampling. For Twitter, we see
a clear recency effect. The static baseline gets
worse and worse as it recedes from the current
test point. Uniform sampling does better, but it
in turn is beaten by the Moving Window Model.
However, this in turn is beaten by our exponential
reservoir sampling.
Day Static Moving Uniform Exp Exact
5 619.4 619.4 619.4 619.4 619.4
6 664.8 599.7 601.8 601.0 597.6
7 684.4 602.8 603.0 599.3 595.6
8 710.1 612.0 614.6 607.7 603.5
9 727.0 617.9 623.3 613.0 608.7
10 775.6 651.2 656.2 642.0 640.5
12 776.7 639.0 646.6 628.7 627.5
15 777.1 638.3 647.7 626.7 627.3
20 800.9 619.1 636.7 604.9 607.3
25 801.4 621.7 631.5 601.5 597.6
Table 4: Perplexities for differently selected sam-
ples over Twitter (sample size = five days, ? =
1.1). Results in bold are the best sampling results.
Lower is better.
4.5 GigaWord
Twitter is a fast moving, rapidly changing multi-
lingual stream and it is not surprising that our ex-
ponential reservoir sampling proves beneficial. Is
it still useful for a more conventional stream that
is drawn from a much smaller population of re-
porters? We repeated our experiments, using the
same rolling training and testing evaluation as be-
fore, but this time using newswire for data.
Table 5 shows the perplexities when using the
Gigaword stream. We see the same general trends,
albeit with less of a difference between exponen-
tial sampling and our moving window. Perplexity
values are all lower than for Twitter.
Block Static Moving Uniform Exp
11 416.5 381.1 382.0 382.0
15 436.7 353.3 357.5 352.8
20 461.8 347.0 354.4 344.6
25 315.6 214.9 222.2 211.3
30 319.1 200.5 213.5 199.5
40 462.5 304.4 313.2 292.9
Table 5: Perplexities for differently selected sam-
ples over Gigaword (sample size = 10 blocks, ? =
1.1). Lower is better.
4.6 Why does this work for Twitter?
Although the perplexity results demonstrate that
exponential sampling is on average beneficial, it
is useful to analyse the results in more detail. For
a large stream size (25 days), we built models us-
ing uniform, exponential (? = 1.1) and our moving
window sampling methods. Each approach used
the same amount of data. For the same test set
(four million Tweets), we computed per-Tweet log
likelihoods and looked at the difference between
the model that best explained each tweet and the
second best model (ie the margin). This gives us
an indication of how much a given model better
explains a given Tweet. Analysing the results, we
found that most gains came from short grams and
very few came from entire Tweets being reposted
(or retweeted). This suggests that the Twitter re-
sults follow previously reported observations on
how language can be bursty and not from Twitter-
specific properties.
5 Conclusion
We have introduced exponential reservoir sam-
pling as an elegant way to model a stream of un-
bounded size, yet using fixed space. It naturally al-
lows one to take account of recency effects present
in many natural streams. We expect that our lan-
guage model could improve other Social Media
tasks, for example lexical normalisation (Han and
Baldwin, 2011) or even event detection (Lin et
al., 2011). The approach is fully general and not
just limited to language modelling. Future work
should look at other distributions for sampling and
consider tasks such as machine translation over
Social Media.
Acknowledgments This work was carried out
when MO was on sabbatical at the HLTCOE and
CLSP.
691
References
Charu C Aggarwal. 2006. On biased reservoir sam-
pling in the presence of stream evolution. In Pro-
ceedings of the 32nd international conference on
Very large data bases, pages 607?618. VLDB En-
dowment.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. A model of lexical attractions and repulsion.
In Proceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics and Eighth
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 373?380.
Association for Computational Linguistics.
K. Church and W. A. Gale. 1995. Poisson mixtures.
Natural Language Engineering, 1:163?190.
Kenneth W Church. 2000. Empirical estimates of
adaptation: the chance of two noriegas is closer to
p/2 than p 2. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 180?
186. Association for Computational Linguistics.
Amit Goyal, Hal Daum?e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language Modeling. In Proceedings of NAACL.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ?11,
pages 368?378, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Raymond Lau, Ronald Rosenfeld, and SaIim Roukos.
1993. Trigger-based language models: A maximum
entropy approach. In Acoustics, Speech, and Signal
Processing, 1993. ICASSP-93., 1993 IEEE Interna-
tional Conference on, volume 2, pages 45?48. IEEE.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
Jimmy Lin, Rion Snow, and William Morgan. 2011.
Smoothing techniques for adaptive online language
models: topic tracking in tweet streams. In Proceed-
ings of the 17th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 422?429. ACM.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2009. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2011. Effi-
cient online locality sensitive hashing via reservoir
counting. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 18?23. Association for Computa-
tional Linguistics.
Benjamin Van Durme. 2012. Streaming analysis of
discourse participants. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 48?58. Association for
Computational Linguistics.
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37?57, March.
692
