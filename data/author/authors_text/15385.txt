Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457?465,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic induction of FrameNet lexical units
Marco Pennacchiotti(?), Diego De Cao(?), Roberto Basili(?), Danilo Croce(?), Michael Roth(?)
(?) Computational Linguistics
Saarland University
Saarbru?cken, Germany
{pennacchiotti,mroth}@coli.uni-sb.de
(?) DISP
University of Roma Tor Vergata
Roma, Italy
{decao,basili,croce}@info.uniroma2.it
Abstract
Most attempts to integrate FrameNet in NLP
systems have so far failed because of its lim-
ited coverage. In this paper, we investigate the
applicability of distributional and WordNet-
based models on the task of lexical unit induc-
tion, i.e. the expansion of FrameNet with new
lexical units. Experimental results show that
our distributional and WordNet-based models
achieve good level of accuracy and coverage,
especially when combined.
1 Introduction
Most inference-based NLP tasks require a large
amount of semantic knowledge at the predicate-
argument level. This type of knowledge allows to
identify meaning-preserving transformations, such
as active/passive, verb alternations and nominal-
izations, which are crucial in several linguistic in-
ferences. Recently, the integration of NLP sys-
tems with manually-built resources at the predi-
cate argument-level, such as FrameNet (Baker et
al., 1998) and PropBank (Palmer et al, 2005) has
received growing interest. For example, Shen and
Lapata (2007) show the potential improvement that
FrameNet can bring on the performance of a Ques-
tion Answering (QA) system. Similarly, several
other studies (e.g. (Bar-Haim et al, 2005; Garoufi,
2007)) indicate that frame semantics plays a central
role in Recognizing Textual Entailment (RTE). Un-
fortunately, most attempts to integrate FrameNet or
similar resources in QA and RTE systems have so
far failed, as reviewed respectively in (Shen and La-
pata, 2007) and (Burchardt and Frank, 2006). These
studies indicate limited coverage as the main reason
of insuccess. Indeed, the FrameNet database only
contains 10,000 lexical units (LUs), far less than
the 210,000 entries in WordNet 3.0. Also, frames
are based on more complex information than word
senses, so that their manual development is much
more demanding (Burchardt et al, 2006; Subirats
and Petruck, 2003).
Therefore, there is nowadays a pressing need to
adopt learning approaches to extend the coverage
of the FrameNet lexicon by automatically acquiring
new LUs, a task we call LU induction, as recently
proposed at SemEval-2007 (Baker et al, 2007). Un-
fortunately, research in this area is still somehow
limited and fragmentary. The aim of our study is
to pioneer in this field by proposing two unsuper-
vised models for LU induction, one based on dis-
tributional techniques and one using WordNet as a
support; and a combined model which mixes the
two. The goal is to investigate to what extent distri-
butional and WordNet-based models can be used to
induce frame semantic knowledge in order to safely
extend FrameNet, thus limiting the high costs of
manual annotation.
In Section 2 we introduce the LU induction task
and present related work. In Sections 3, 4 and 5 we
present our distributional, WordNet-based and com-
bined models. Then, in Section 6 we report experi-
mental results and comparative evaluations. Finally,
in Section 7 we draw final conclusions and outline
future work.
2 Task Definition and Related Work
As defined in (Fillmore, 1985), a frame is a con-
ceptual structure modeling a prototypical situation,
evoked in texts through the occurrence of its lex-
ical units. A lexical unit (LU) is a predicate that
linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-
guments. For example the frame KILLING has lex-
ical units such as assassin, assassinate, blood-bath,
fatal, murderer, kill, suicide that share semantic ar-
guments such as KILLER, INSTRUMENT, CAUSE,
VICTIM. Building on this frame-semantic model,
the Berkeley FrameNet project (Baker et al, 1998)
has been developing a frame-semantic lexicon for
457
the core vocabulary of English since 1997. The
current FrameNet release contains 795 frames and
about 10,000 LUs. Part of FrameNet is also a cor-
pus of 135,000 annotated example sentences from
the British National Corpus (BNC).
LU induction is a fairly new task. Formally,
it can be defined as the task of assigning a
generic lexical unit not yet present in the FrameNet
database (hereafter called unknown LU) to the cor-
rect frame(s). As the number of frames is very
large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then
be mapped to more than one frame (for example
the word tea could map both to FOOD and SO-
CIAL EVENT). Also, even unambiguous words can
be assigned to more than one frame ? e.g. child maps
to both KINSHIP and PEOPLE BY AGE.
LU induction is relevant to many NLP tasks, such
as the semi-automatic creation of new FrameNets,
and semantic role labelling. LU induction has been
integrated at SemEval-2007 as part of the Frame Se-
mantic Structure Extraction shared task (Baker et
al., 2007), where systems are requested to assign
the correct frame to a given LU, even when the
LU is not yet present in FrameNet. Johansson and
Nugues (2007) approach the task as a machine learn-
ing problem: a Support Vector Machine trained on
existing LUs is applied to assign unknown LUs to
the correct frame, using features derived from the
WordNet hierarchy. Tested on the FrameNet gold
standard, the method achieves an accuracy of 0.78,
at the cost of a low coverage of 31% (i.e. many LUs
are not assigned). Johansson and Nugues (2007)
also experiment with a simple model based on stan-
dard WordNet similarity measures (Pedersen et al,
2004), achieving lower performance. Burchardt and
colleagues (2005) present Detour, a rule-based sys-
tem using words in a WordNet relation with the un-
known LU to find the correct frame. The system
achieves an accuracy of 0.39 and a coverage of 87%.
Unfortunately this algorithm requires the LU to be
previously disambiguated, either by hand or using
contextual information.
In a departure from previous work, our first model
leverages distributional properties to induce LUs, in-
stead of relying on pre-existing lexical resources as
WordNet. This guarantees two main advantages.
First, it can predict a frame for any unknown LU,
while WordNet based approaches can be applied
only to words having a WordNet entry. Second, it
allows to induce LUs in languages for which Word-
Net is not available or has limited coverage. Our
second WordNet-based model uses sense informa-
tion to characterize the frame membership for un-
known LU, by adopting a semantic similarity mea-
sure which is sensitive to all the known LUs of a
frame.
3 Distributional model
The basic idea behind the distributional approach is
to induce new LUs by modelling existing frames and
unknown LUs in a semantic space, where they are
represented as distributional co-occurrence vectors
computed over a corpus.
Semantic spaces are widely used in NLP for rep-
resenting the meaning of words or other lexical en-
tities. They have been successfully applied in sev-
eral tasks, such as information retrieval (Salton et al,
1975) and harvesting thesauri (Lin, 1998). The intu-
ition is that the meaning of a word can be described
by the set of textual contexts in which it appears
(Distributional Hypothesis (Harris, 1964)), and that
words with similar vectors are semantically related.
In our setting, the goal is to find a semantic space
model able to capture the notion of frame ? i.e. the
property of ?being characteristic of a frame?. In
such a model, an unknown LU is induced by first
computing the similarity between its vector and the
vectors of the existing frames, and then assigning the
LU to the frame with the highest similarity.
3.1 Assigning unknown LUs to frames
In our model, a LU l is represented by a vector ~l
whose dimensions represent the set of contexts C
of the semantic space. The value of each dimen-
sion is given by the co-occurrence value of the LU
with a contextual feature c ? C, computed over a
large corpus using an association measure. We ex-
periment with two different association measures:
normalized frequency and pointwise mutual infor-
mation. We approximate these measures by using
Maximum Likelihood Estimation, as follows:
458
F (l, c) =MLE |l, c||?, ?|
MI(l, c) =MLE |l, c||?, ?||?, c||l, ?|
(1)
where |l, c| denotes the co-occurrence counts
of the pair (l, c) in the corpus, |?, c| =?
l?L |l, c|, |l, ?| =
?
c?C |l, c| and finally |?, ?| =?
l?L,c?C |l, c|.
A frame f is modeled by a vector ~f , representing
the distributional profile of the frame in the seman-
tic space. We here assume that a frame can be fully
described by the set of its lexical units F . We imple-
ment this intuition by computing ~f as the weighted
centroid of the set F , as follows:
~f =
?
l?F
wlf ?~l (2)
where wlf is a weighting factor, accounting for
the relevance of a given lexical unit with respect to
the frame, estimated as:
wlf = |l|?
l?F
|l|
(3)
where |l| denotes the counts of l in the corpus.
From a more cognitive perspective, the vector ~f rep-
resents the prototypical lexical unit of the frame.
Given the set of all framesN and an unknown lex-
ical unit ul, we assign ul to the frame fmaxul which
is distributionally most similar ? i.e. we intuitively
map an unknown lexical unit to the frame whose
prototypical lexical unit ~f has the highest similarity
with ~ul:
fmaxul = argmaxf?N simD(~ul, ~f) (4)
In our model, we used the traditional cosine simi-
larity:
simcos(ul, f) =
~ul ? ~f
|~ul| ? |~f |
(5)
3.2 Choosing the space
Different types of contexts C define spaces with dif-
ferent semantic properties. We are here looking for
a space able to capture the properties which charac-
terise a frame. The most relevant of these properties
is that LUs in the same frame tend to be either co-
occurring or substitutional words (e.g. assassin/kill
or assassinate/kill) ? i.e. they are either in paradig-
matic and syntagmatic relation. In an ideal space,
a high similarity value simD would be then given
both to assassinate/kill and to assassin/kill. We ex-
plore three spaces which seem to capture the above
property well:
Word-based space: Contexts are words appear-
ing in a n-window of the lexical unit. Such spaces
model a generic notion of semantic relatedness.
Two LUs close in the space are likely to be re-
lated by some type of generic semantic relation,
either paradigmatic (e.g. synonymy, hyperonymy,
antonymy) or syntagmatic (e.g. meronymy, concep-
tual and phrasal association).1
Syntax-based space: Contexts are syntactic re-
lations (e.g. X-VSubj-man where X is the LU), as
described in (Pado?, 2007). These spaces are good
at modeling semantic similarity. Two LUs close in
the space are likely to be in a paradigmatic relation,
i.e. to be close in a is-a hierarchy (Budanitsky and
Hirst, 2006; Lin, 1998; Pado?, 2007). Indeed, as con-
texts are syntactic relations, targets with the same
part of speech are much closer than targets of differ-
ent types.
Mixed space: In a combination of the two above
spaces, contexts are words connected to the LU by a
dependency path of at most length n. Unlike word-
based spaces, contexts are selected in a more princi-
pled way: only syntactically related words are con-
texts, while other (possibly noisy) material is filtered
out. Unlike syntax-based spaces, the context c does
not explicitly state the type of syntactic relation with
the LU: this usually allows to capture both paradig-
matic and syntagmatic relations.
4 WordNet-based model
In a departure from previous work, our WordNet-
based model does not rely on standard WordNet sim-
ilarity measures (Pedersen et al, 2004), as these
measures can only be applied to pairs of words,
while we here need to capture the meaning of whole
frames, which typically consist of larger sets of LUs.
Our intuition is that senses able to evoke a frame can
be detected via WordNet, by jointly considering the
WordNet synsets activated by all LUs of the frame.
We implement this intuition in a weakly-
supervised model, where each frame f is repre-
sented as a set of specific sub-graphs of the WordNet
1See (Pado?, 2007; Sahlgren, 2006) for an in depth analysis.
459
hyponymy hierarchy. As different parts of speech
have different WordNet hierarchies, we build a sub-
graph for each of them: Snf for nouns, Svf for verbs
and Saf for adjectives.2 These sub-graphs repre-
sent the lexical semantic properties characterizing
the frame. An unknown LU ul of a given part of
speech is assigned to the frame whose correspond-
ing sub-graph is semantically most similar to one of
the senses of ul:
fmaxul = argmaxf?N simWN (ul, f) (6)
where simWN is a WordNet-based similarity
measure. In the following subsections we will de-
scribe how we build sub-graphs and model the sim-
ilarity measure for the different part of speech.
Figure 1 reports an excerpt of the noun sub-
graph for the frame PEOPLE BY AGE, cover-
ing the suitable senses of its nominal LUs
{adult, baby, boy, kid, youngster, youth}. The
relevant senses (e.g. sense 1 of youth out of the 6
potential ones) are generally selected, as they share
the most specific generalizations in WordNet with
the other words.
Nouns. To compute similarity for nouns we adopt
conceptual density (cd) (Agirre and Rigau, 1996),
a semantic similarity model previously applied to
word sense disambiguation tasks.
Given a frame f and its set of nominal lexical
units Fn, the nominal subgraph Snf is built as fol-
lows. All senses of all words in Fn are activated
in WordNet. All hypernyms Hnf of these senses are
then retrieved. Every synset ? ? Hnf is given a cd
score, representing the density of the WordNet sub-
hierarchy rooted at ? in representing the set of nouns
Fn. The intuition behind this model is that the larger
the number of LUs in Fn that are generalized by ? is,
the better it captures the lexical semantics intended
by the frame f . Broader generalizations are penal-
ized as they give rise to bigger hierarchies, not well
correlated with the full set of targets Fn.
To build the final sub-graph Snf , we apply the
greedy algorithm proposed by Basili and colleagues
(2004). It first computes the set of WordNet synsets
that generalize at least two LUs in Fn, and then se-
lects the subset of most dense ones Snf ? Hnf that
2Our WordNet model does not cover the limited number of
LUs which are not nouns, verbs or adjectives.
cover Fn. If a LU has no common hypernym with
other members of Fn, it is not represented in Snf , and
its similarity is set to 0 . Snf disambiguates words in
Fn as only the lexical senses with at least one hyper-
nym in Snf are considered.
Figure 1 shows the nominal sub-graph automati-
cally derived using conceptual density for the frame
PEOPLE BY AGE. The word boy is successfully dis-
ambiguated, as its only hypernym in the sub-graph
refers to its third sense (a male human offspring)
which correctly maps to the given frame. Notice
that this model departs from the first sense heuris-
tics largely successful in word sense disambigua-
tion: most frames in fact are characterized by non
predominant senses. The only questionable disam-
biguation is for the word adult: the wrong sense
(adult mammal) is selected. However, even in these
cases, the cd values are very low (about 10?4), so
that they do not impact much on the quality of the
resulting inference.
Figure 1: The noun sub-graph for the frame PEO-
PLE BY AGE as evoked by a subset of the words. Sense
numbers #n refers to WordNet 2.0.
Using this model, LU induction is performed as
follows. Given an unknown lexical unit ul, for each
frame f ? N we first build the sub-graph Snf from
the set Fn ? {ul}. We then compute simWN (f, ul)
as the maximal cd of any synset ? ? Snf that gener-
alizes one of the lexical senses of ul. In the example
baby would receive a score of 0.117 according to its
first sense in WordNet 2.0 (?baby,babe,infant?). In
a final step, we assign the LU to the most similar
frame, according to Eq. 6
Verbs and Adjectives. As the conceptual density
algorithm can be used only for nouns, we apply dif-
ferent similarity measures for verbs and adjectives.
460
For verbs we exploit the co-hyponymy relation:
the sub-graph Svf is given by all hyponyms of all
verbs Fv in the frame f . Similarity simWN (f, ul)
is computed as follows:
simWN (ul, f) =
?
???
???
1 iff ?K ? F such that
|K| > ? AND
?l ? K, l is a co-hyponym of ul
? otherwise
(7)
As for adjectives, WordNet does not provide a hy-
ponymy hierarchy. We then compute similarity sim-
ply on the basis of the synonymy relation, as fol-
lows:
simWN (ul, f) =
?
?
?
1 iff ?l ? F such that
l is a synonym of ul
? otherwise
(8)
5 Combined model
The methods presented so far use two independent
information sources to induce LUs: distributional
similarity simD and WordNet similarity simWN .
We also build a joint model, leveraging both ap-
proaches: we expect the combination of different
information to raise the overall performance. We
here choose to combine the two approaches using a
simple back-off model, that uses the WordNet-based
model as a default and backs-off to the distributional
one when no frame is proposed by the former. The
intuition is that WordNet should guarantee the high-
est precision in the assignment, while distributional
similarity should recover cases of low coverage.
6 Experiments
In this section we present a comparative evaluation
of our models on the task of inducing LUs, in a
leave-one-out setting over a reference gold standard.
6.1 Experimental Setup
Our gold standard is the FrameNet 1.3 database,
containing 795 frames and a set L of 7,522 unique
LUs (in all there are 10,196 LUs possibly assigned
to more than one frame). Given a lexical unit l ? L,
we simulate the induction task by executing a leave-
one-out procedure, similarly to Burchardt and col-
leagues (2005). First, we remove l from all its origi-
nal frames. Then, we ask our models to reassign it to
the most similar frame(s) f , according to the simi-
larity measure3. We repeat this procedure for all lex-
ical units. Though our experiment is not completely
realistic (we test over LUs already in FrameNet), it
has the advantage of a reliable gold standard pro-
duced by expert annotators. A second, more re-
alistic, small-scale experiment is described in Sec-
tion 6.2.
We compute accuracy as the fraction of LUs in L
that are correctly re-assigned to the original frame.
Accuracy is computed at different levels k: a LU l is
correctly assigned if its gold standard frame appears
among the best-k frames f ranked by the model us-
ing the sim(l, f) measure. As LUs can have more
than one correct frame, we deem as correct an as-
signment for which at least one of the correct frames
is among the best-k.
We also measure coverage, intended as the per-
centage of LUs that have been assigned to at least
one frame by the model. Notice that when no
sense preference can be found above the threshold ?,
the WordNet-based model cannot predict any frame,
thus decreasing coverage.
We present results for the following models and
parametrizations (further parametrizations have re-
vealed comparable performance).
Dist-word : the word-based space described in
Section 3. Contextual features correspond to the
set of the 4,000 most frequent words in the BNC.4
The association measure between LUs and contexts
is the pointwise mutual information. Valid contexts
for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described
in Section 3. Context features are the 10,000 most
frequent syntactic relations in the BNC5. As associ-
ation measure we apply log-likelihood ratio (Dun-
ning, 1993) to normalized frequency. Syntactic rela-
tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec-
3In the distributional model, we recompute the centroids for
each frame f in which the LU appeared, applying Eq. 2 to the
set F ? {l}.
4We didn?t use the FrameNet corpus directly, as it is too
small to obtain reliable statistics.
5Specifically, we use the minimum context selection func-
tion and the plain path value function described in Pado (2007).
461
tion 3. As for the Dist-word model, contextual fea-
tures are 4,000 and pointwise mutual information is
the association measure. The maximal dependency
path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described
in Section 4.
WNet-bsense : this model is computed as WNet-
full but using only the most frequent sense for each
LU as defined in WordNet.
Combined : the combined method presented in
Section 5. Specifically, it uses WNet-full as a default
and Dist-word as back-off.
Baseline-rnd : a baseline model, randomly as-
signing LUs to frames.
Baseline-mostfreq : a model predicting as best-k
frames the most likely ones in FrameNet ? i.e. those
containing the highest number of LUs.
6.2 Experimental Results
Table 1 reports accuracy and coverage results for the
different models, considering only 6792 LUs with
frequency higher than 5 in the BNC, and frames
with more than 2 lexical units (to allow better gen-
eralizations in all models). Results show that all our
models largely outperform both baselines, achieving
a good level of accuracy and high coverage. In
particular, accuracy for the best-10 frames is high
enoungh to support tasks such as the semi-automatic
creation of new FrameNets. This claim is supported
by a further task-driven experiment, in which we
asked 3 annotators to assign 60 unknown LUs (from
the Detour system log) to frames, with and without
the support of the Dist-word model?s predictions as
suggestions6. We verified that our model guarantee
an annotation speed-up of 25% ? i.e. in average an
annotator saves 25% of annotation time by using
the system?s suggestions.
Distributional vs. WordNet-based models.
WordNet-based models are significantly better than
distributional ones, for several reasons. First, distri-
butional models acquire information only from the
contexts in the corpus. As we do not use a FrameNet
annotated corpus, there is no guarantee that the us-
age of a LU in the texts reflects exactly the semantic
6For this purpose, the dataset is evenly split in two parts.
properties of the LU in FrameNet. In the extreme
cases of polysemous LUs, it may happen that the
textual contexts refer to senses which are not ac-
counted for in FrameNet. In our study, we explicitly
ignore the issue of polisemy, which is a notoriously
hard task to solve in semantics spaces (see (Schu?tze,
1998)), as the occurrences of different word senses
need to be clustered separately. We will approach
the problem in future work. The WordNet-based
model suffers from the problem of polisemy to a
much lesser extent, as all senses are explicitly rep-
resented and separated in WordNet, including those
related to the FrameNet gold standard.
A second issue regards data sparseness. The vec-
torial representation of LUs with few occurrences in
the corpus is likely to be semantically incomplete,
as not enough statistical evidence is available. Par-
ticularly skewed distributions can be found when
some frames are very rarely represented in the cor-
pus. A more in-depth descussion on these two issues
is given later in this section.
Regarding the WordNet-based models, WNet-full
in most cases outperforms WNet-bsense. The first
sense heuristic does not seem to be as effective as
in other tasks, such as Word Sense Disambigua-
tion. Although sense preferences (or predominance)
across two general purpose resources, such as Word-
Net and FrameNet, should be a useful hint, the con-
ceptual density algorithm seems to produce better
distributions (i.e. higher accuracy), especially when
several solutions are considered. Indeed, for many
LUs the first WordNet sense is not the one repre-
sented in the FrameNet database.
As for distributional models, results show that the
Dist-word model performs best. In general, syntac-
tic relations (Dist-syntax model) do not help to cap-
ture frame semantic properties better than a simple
window-based approach. This seems to indicate that
LUs in a same frame are related both by paradig-
matic and syntagmatic relations, in accordance to
the definition given in Section 3.2 ? i.e. they are
mostly semantically related, but not similar.
Coverage. Distributional models show a coverage
15% higher than WordNet-based ones. Indeed, as far
as corpus evidence is available (i.e. the unknown LU
appears in the corpus), distributional methods are al-
ways able to predict a frame. WordNet-based mod-
462
MODEL B-1 B-2 B-3 B-4 B-5 B-6 B-7 B-8 B-9 B-10 COVERAGE
Dist-word 0.27 0.36 0.42 0.46 0.49 0.51 0.53 0.55 0.56 0.57 95%
Dist-syntax 0.22 0.29 0.34 0.38 0.41 0.44 0.46 0.48 0.50 0.51 95%
Dist-mixed 0.25 0.35 0.40 0.44 0.47 0.49 0.51 0.53 0.54 0.56 95%
WNet-full 0.47 0.59 0.65 0.69 0.72 0.73 0.75 0.76 0.77 0.78 80%
WNet-bsense 0.52 0.61 0.64 0.66 0.67 0.68 0.69 0.69 0.70 0.70 72%
Combined 0.43 0.54 0.60 0.64 0.66 0.68 0.70 0.71 0.72 0.73 95%
Baseline-rnd 0.02 0.03 0.05 0.06 0.08 0.10 0.11 0.12 0.14 0.15
Baseline-mostfreq 0.02 0.05 0.07 0.08 0.10 0.11 0.13 0.14 0.15 0.17
Table 1: Accuracy and coverage of different models on best-k ranking with frequency threshold 5 and frame threshold
2
els cannot make predictions in two specific cases.
First, when the LU is not present in WordNet. Sec-
ond, when the function simWN does not has suffi-
cient relational information to find a similar frame.
This second factor is particularly evident for adjec-
tives, as Eq. 8 assigns a frame only when a synonym
of the unknown LU is found. It is then not surpris-
ing that 68% of the missed assignment are indeed
adjectives.
Results for the Combined model suggest that
the integration of distributional and WordNet-based
methods can offer a viable solution to the cover-
age problem, as it achieves an accuracy comparable
to the pure WordNet approaches, while keeping the
coverage high.
Figure 2: Dist-word model accuracy at different LU fre-
quency cuts.
Data Sparseness. A major issue when using dis-
tributional approaches is that words with low fre-
quency tend to have a very sparse non-meaningful
representation in the vector space. This highly im-
pacts on the accuracy of the models. To measure
the impact of data sparseness, we computed the ac-
curacy at different frequency cuts ? i.e. we exclude
LUs below a given frequency threshold from cen-
troid computation and evaluation. Figure 2 reports
the results for best-10 assignment at different cuts,
for the Dist-word model. As expected, accuracy im-
proves by excluding infrequent LUs. Only at a fre-
quency cut of 200 performance becomes stable, as
statistical evidence is enough for a reliable predic-
tion. Yet, in a real setting the improvement in accu-
racy implies a lower coverage, as the system would
not classify LUs below the threshold. For example,
by discarding LUs occurring less than 200 times in
the corpus, we obtain a +0.12 improvement in accu-
racy, but the coverage decreases to 57%. However,
uncovered LUs are also the most rare ones and their
relevance in an application may be negligible.
Lexical Semantics, Ambiguity and Plausible As-
signments. The overall accuracies achieved by
our methods are ?pessimistic?, in the sense that they
should be intended as lower-bounds. Indeed, a qual-
itative analysis of erroneous predictions reveals that
in many cases the frame assignments produced by
the models are semantically plausible, even if they
are considered incorrect in the leave-one-out test.
Consider for example the LU guerrilla, assigned in
FrameNet to the frame PEOPLE BY VOCATION. Our
mixed model proposes as two most similar frames
MILITARY and TERRORISM, which could still be
considered plausible assignment. The same holds
for the LU caravan, for which the most similar
frame is VEHICLE, while in FrameNet the LU is as-
signed only to the frame BUILDINGS. These cases
are due to the low FrameNet coverage, i.e LUs are
not fully annotated and they appear only in a subset
of their potential frames. The real accuracy of our
463
models is therefore expected to be higher.
To explore the issue, we carried out a qualita-
tive analysis of 5 words (i.e. abandon.v, accuse.v,
body.n, charge.v and partner.n). For each of them,
we randomly picked 60 sentences from the BNC
corpus, and asked two human annotators to assign
to the correct frame the occurrence of the word in
the given sentence. For 2 out of 5 words, no frame
could be found for most of the sentences, suggesting
that the most frequent frames for these words were
missing from FrameNet7. We can then conclude that
100% accuracy cannot be considered as the upper-
bound of our experiment, as word usage in texts is
not well reflected in the FrameNet modelling.
Further experiments. We also tested our models
on a realistic gold-standard set of 24 unknown LUs
extracted from the SemEval-2007 corpus (Baker et
al., 2007). These are words not present in FrameNet
1.3 which have been assigned by human annotators
to an existing frame8. WNet-full achieves an accu-
racy of 0.25 for best-1 and 0.69 for best-10, with a
coverage of 67%. A qualitative analysis showed that
the lower performance wrt to our main experiment is
due to higher ambiguity of the LUs (e.g. we assign
tea to SOCIAL EVENT instead of FOOD).
Comparison to other approaches. We compare
our models to the system presented by Johans-
son and Nugues (2007) and Burchardt and col-
leagues (2005). Johansson and Nugues (2007) eval-
uate their machine learning system using 7,000
unique LUs to train the Support Vector Machine, and
the remaining LUs as test. They measure accuracy at
different coverage levels. At 80% coverage accuracy
is about 0.42, 10 points below our best WordNet-
based system. At 90% coverage, the system shows
an accuracy below 0.10 and is significantly out-
performed by both our distributional and combined
methods. These results confirm that WordNet-based
approaches, while being highly accurate wrt dis-
tributional ones, present strong weaknesses as far
as coverage is concerned. Furthermore, Johansson
and Nugues (2007) show that their machine learn-
7Note that the need of new frames to account for seman-
tic phenomena in free texts has been also demonstrated by the
SemEval-2007 competition.
8The set does not contain 4 LUs which have no frame in
FrameNet.
ing approach outperforms a simple approach based
on WordNet similarity: thus, our results indirectly
prove that our WordNet-based method is more ef-
fective than the application of the similarity measure
presented in (Pedersen et al, 2004).
We also compare our results to those reported
by Burchardt and colleagues (2005) for Detour.
Though the experimental setting is slightly different
(LU assignment is done at the text-level), they use
the same gold standard and leave-one-out technique,
reporting a best-1 accuracy of 0.38 and a coverage
of 87%. Our WordNet-based models significantly
outperform Detour on best-1 accuracy, at the cost of
lower coverage. Yet,our combined model is signifi-
cantly better both on accuracy (+5%) and coverage
(+8%). Also, in most cases Detour cannot predict
more than one frame (best-1), while our accuracies
can be improved by relaxing to any best-k level.
7 Conclusions
In this paper we presented an original approach for
FrameNet LU induction. Results show that mod-
els combining distributional and WordNet informa-
tion offer the most viable solution to model the no-
tion of frame, as they allow to achieve a reasonable
trade-off between accuracy and coverage. We also
showed that in contrast to previous work, simple se-
mantic spaces are more helpful than complex syn-
tactic ones. Results are accurate enough to support
the creation and the development of new FrameNets.
As future work, we will evaluate new types of
spaces (e.g. dimensionality reduction methods) to
improve the generalization capabilities of the space
models. We will also address the data sparseness is-
sue, by testing smoothing techniques to better model
low frequency LUs. Finally, we will implement
the presented models in a complex architecture for
semi-supervised FrameNets development, both for
specializing the existing English FrameNet in spe-
cific domains, and for creating new FrameNets in
other languages.
Acknowledgements
This work has partly been funded by the German Re-
search Foundation DFG (grant PI 154/9-3). Thanks
to Richard Johansson and Aljoscha Burchardt for
providing the data of their systems.
464
References
E. Agirre and G. Rigau. 1996. Word Sense Disam-
biguation using Conceptual Density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June.
Roy Bar-Haim, Idan Szpektor, and Oren Glickman.
2005. Definition and Analysis of Intermediate Entail-
ment Levels. In ACL-05 Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A
semantic similarity measure for unsupervised semantic
disambiguation. In Proceedings of LREC-04, Lisbon,
Portugal.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of PASCAL RTE2 Workshop.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet Detour to FrameNet. In Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8 of Computer Studies in Language
and Speech. Peter Lang, Frankfurt/Main.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC, Genova,
Italy.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 18(1):61?74.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?254.
K. Garoufi. 2007. Towards a better understanding of
applied textual entailment: Annotation and evaluation
of the rte-2 dataset. M.Sc. thesis, saarland university.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Phi-
losophy of Linguistics, New York. Oxford University
Press.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, Tartu, Estonia, May 24.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Sebastian Pado?. 2007. Cross-Lingual Annotation Projec-
tion Models for Role-Semantic Information. Saarland
University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1).
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18:613620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of EMNLP-CoNLL, pages 12?21, Prague.
C. Subirats and M. Petruck. 2003. Surprise! Spanish
FrameNet! In Proceedings of the Workshop on Frame
Semantics at the XVII. International Congress of Lin-
guists, Prague.
465
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Combining Word Sense and
Usage for Modeling Frame
Semantics
Diego De Cao1
Danilo Croce1
Marco Pennacchiotti2
Roberto Basili1
1University of Rome Tor Vergata (Italy)
2University of the Saarland (Germany)
email: decao@info.uniroma2.it
Abstract
Models of lexical semantics are core paradigms in most NLP applica-
tions, such as dialogue, information extraction and document understand-
ing. Unfortunately, the coverage of currently available resources (e.g.
FrameNet) is still unsatisfactory. This paper presents a largely applicable
approach for extending frame semantic resources, combining word sense
information derived fromWordNet and corpus-based distributional infor-
mation. We report a large scale evaluation over the English FrameNet,
and results on extending FrameNet to the Italian language, as the basis of
the development of a full FrameNet for Italian.
85
86 De Cao, Croce, Pennacchiotti, and Basili
1 Introduction and Related Work
Models of lexical meaning are explicit or implicit basic components of any text pro-
cessing system devoted to information extraction, question answering or dialogue.
Several paradigms proposed for a variety of notions, such as word sense (Miller et al,
1990) or frame semantics (Baker et al, 1998), have given rise to large scale resources,
respectively WordNet and FrameNet. Recent studies (e.g. Shen and Lapata (2007))
show that the use of FrameNet can potentially improve the performance of Question
Answering systems. Yet, Shen and Lapata (2007) also point out that the low cov-
erage of the current version of FrameNet significantly limits the expected boost in
performance. Other studies have shown similar evidences for Recognizing Textual
Entailment (RTE) (Clark et al, 2007; Burchardt et al, 2008): most examples of the
RTE challenges corpora can be solved at the predicate-argument structure level, but
FrameNet coverage is still a major problem.
Approaches to (semi-)automatically acquire frame information are then today a
priority to solve these problems. Despite this, not many efforts have been paid so far
in this direction. Burchardt et al (2005) presented Detour, a system for predicting
frame assignment of potential lexical units not covered by FrameNet, by using the
paradigmatic information enclosed in WordNet. Although the authors do not fully
solve the problem related to the fuzzy relationships between senses and frames, they
propose an empirical association measure for ranking frame candidates according to
sense information as stored in WordNet. To our knowledge, this is the only work
trying to bridge frame membership to referential properties of lexical senses. Pitel
(2006) presents a preliminary study on the applicability of semantic spaces and space
geometrical transformations (namely, Latent Semantic Analysis) to expand FrameNet,
but the investigation is too limited in scope to draw relevant conclusions. Finally, Pad?
et al (2008) propose a method to automatically label unknown semantic roles of event
nominalizations in FrameNet, but their method needs a large amount of annotated
verbal data.
Another important limitation of FrameNet is the limited support to multilingual-
ity, which is becoming a critical issue in real NLP applications. In recent years,
some efforts have focused on the manual adaptation of the English FrameNet to other
languages (e.g., German (Burchardt et al, 2006) and Spanish (Subirats and Petruck,
2003)). Unlike PropBank, FrameNet is in fact suitable to cross-lingual induction, as
frames are mostly defined at the conceptual level, thus allowing cross-lingual interpre-
tation. Yet, all these efforts consist in manually defining frame linguistic knowledge
(e.g. lexical units) in the specific language, and in annotating a large corpus, thus
requiring a large human effort. While attempts to automate the annotation process
are quite promising (Pado and Lapata, 2007), they require the availability of a par-
allel corpus, and leave open the issue of inducing the resource as a whole in a new
language.
In this work, we investigate novel methods for automatically expanding the English
FrameNet, and supporting the creation of new ones in other languages (namely Ital-
ian), thus tackling the abovementioned problems of coverage and multilinguality. The
proposed methods are inspired by the basic hypothesis that FrameNet can be automat-
ically modeled by a fruitful interaction between advanced distributional techniques,
and paradigmatic properties derived from WordNet.
Combining Word Sense and Usage for Modeling Frame Semantics 87
In particular, in this paper we focus on the application of such methods to study
the semantics of the core elements of FrameNet, i.e. the lexical units (hereafter LUs).
Lexical units are predicates (nouns, verbs, adjectives, etc.) that linguistically express
the situation described by a frame. Lexical units of the same frame share semantic
arguments. For example the frame KILLING has lexical units such as: assassin, blood-
bath, fatal, massacre, kill, suicide. These predicates share semantic arguments such
as KILLER, INSTRUMENT and VICTIM. Our goal is to combine corpus distributional
evidence with WordNet information to supply three tasks: the induction of new LUs
not already in FrameNet (unknown LUs); the reduction of LUs polysemy by mapping
them to WordNet synsets; the translation of English LUs into Italian.
The paper is organized as follows. In Section 2 we describe our FrameNet paradig-
matic and distributional model, and we discuss how these two models can be com-
bined in a single framework. In Section 3 we analyze the applicability of these models
to the three proposed experimental tasks, and discuss the results. Finally, in Section 4
we draw final conclusions and outline future work.
2 Paradigmatic and distributional models of frame semantics
In this section we describe our paradigmatic, distributional and combined models for
representing FrameNet. The general goal of each of these three methods is to offer
a computational model of FrameNet. In such a model, frames and LUs should have
a specific computational representation (e.g. vectors), and allow the computation of
similarity either among different LUs or between a frame and a LU. Such model thus
offers explicit means to use FrameNet in a NLP task or to expand FrameNet, e.g. by
assigning unknown LUs to its most similar frame, or by mapping a LU to its proper
WordNet synset(s). A key notion for these tasks is the definition of a principled and
reliable semantic similarity measure sim to be applied to frames and LUs.
2.1 Paradigmatic model
The basic intuition behind our paradigmatic model is that knowledge about predicates
of a frame, through a (possibly limited) set of LUs, allows to detect the set of the
suitable WordNet senses able to evoke the same frame. These senses are topologically
related to (one or more) sub-hierarchies capturing the lexical semantics implicit in the
frame. We propose a weakly-supervised approach to discover these structures. The
main idea is that frames correspond to specific sub-graphs of the WordNet hyponymy
hierarchy, so that these latter can be used to predict frames valid for other LUs, not
yet coded in FrameNet. Figure 1 reports the WordNet sub-hierarchy covering the
frame PEOPLE_BY_AGE: here, the frame?s nominal LUs {adult, adolescent, baby,
boy, infant, kid, geezer, teenager, youngster, youth} are all represented with the senses
correctly referring to the frame. The correct senses (e.g. sense 1 of youth out of its
6 potential senses) are selected as they share most specific generalizations with the
other LUs. This graph can be intended as an ?explanation? of the lexical semantic
properties characterizing the frame: future predictions about new LUs can be done on
the basis of the graph as a paradigmatic model for PEOPLE_BY_AGE. We call such a
graph the WordNet model of the frame. As WordNet organizes nouns, verbs and other
parts-of-speech in different hierarchies, three independent WordNet models (one for
each part-of-speech) are created for each frame.
88 De Cao, Croce, Pennacchiotti, and Basili
Formally, given the set F of the LUs of a frame, a WordNet model is built around
the subset SF of WordNet synsets able to generalize the largest number of words in
F1.
Figure 1: The WordNet model for the frame People_by_Age as evoked by the set of
its nouns. Sense numbers #n refer to WordNet 2.0
The WordNet modelWNF(?,W ) of a frame F , is a graph
(3) WNF(?,W ) =<W,SF ,LF ,h,simWN ,m >
where: W ? F are the subset of all LUs in F having the same part-of-speech ? ?
{verb,noun,ad jective}, SF is the subset of synsets in WN needed to generalize words
w ?W ; LF ? SF are the lexical senses of w ?W subsumed by SF ; h ? SF ?SF is the
projection of the hyponymy relation of WN in SF ; m?W ?2LF is the lexical relation
between words w ?W and synsets in LF ; simWN : SF ? ? is a weighting function that
expresses the relevance of each sense ? ? SF for the frame, as it is represented by its
words in F .
The model exemplified in Figure 1 is
WNPeople_by_Age(noun, {adult, ..., youth}), where LF = {adult#1, adolescent#1, baby#1,
boy#1, boy#2, boy#3, ..., youth#1} and the set SF corresponds to the sub-hierarchies dom-
inated by the synsets #6026, #9622621, #9285271 and #9015843.
The overall goal of computing the WordNet model is to determine the similarity
function simWN : SF ? ?, expressing the relevance of a synset ? ? SF as a good
representative of a frame F . This is what is hereafter referred to as the paradigmatic
similarity model between words senses and frames.
Paradigmatic Similarity measures
Given the WordNet hierarchy separation on part-of-speaches, the similarity function
simWN is independently defined for verbs, nouns and adjectives.
1In the following, we will use the same notation for a frame F and for the set of its known lexical units,
as in our approach we use LU membership as a basic definition of a frame.
Combining Word Sense and Usage for Modeling Frame Semantics 89
For nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996; Basili et al,
2004), a semantic similarity measure defined for word sense disambiguation tasks.
The cd score for a sense ? ? SF is the density of the WordNet sub-hierarchy rooted
at ? in representing the set of nouns in F . The intuition behind this model is that the
larger is the number of all and only LUs in F that are generalized by ?, the better
it captures the lexical semantics intended by the frame. Coarse generalizations (i.e.
synsets higher in the hierarchy) are penalized, as they give rise to bushy hierarchies,
covering too many words not in the target F . The greedy algorithm proposed in Basili
et al (2004) selects the subset of synsets able to ?cover? (i.e. generalize) all the input
words and characterized by the highest cd values. The set of such synsets and their
corresponding sub-hierarchies forms a graph derived from a set of LUs F . The result
is the WordNet model WNF for F , i.e. the minimal subset of WordNet that explains
all (the possible) LUs in F with the maximally similar senses.
Figure 1 shows that correct senses (e.g. the sense 1 of youth out of the 6 potential
senses) are generally detected and preserved in the model. Irrelevant senses that do
not share any common hypernym with other words in F are neglected. Conceptual
density scores can be used to rank individual senses as in the case of boy.
Given a frame F , the above model can be naturally used to compute the similarity
between a noun n /? F and F . This is particularly useful in LU induction task, as de-
scribed in Section 3.1. To do so, the similarity simWN(F,n) between n and F is derived
by computing the cd scores over the set F ?{n}. The simWN(F,n) is the maximal cd
of any synset ?n ? SF that is also hypernym of a lexical sense of n. In the exam-
ple, the noun boy would receive a score of 0.117 through the hypernym {child,kid},
according to its third sense in WordNet 2.0 (?{son,boy}?).
As conceptual density can be only applied to nouns, when verbs v are consid-
ered, we exploit the synonymy and co-hyponymy relations. The following similarity
simWN(F,v) is computed:
(4) simWN(F,v) =
?
?
?
1 iff ?K ? F such that |K| > ? AND
?w ? K w is a co-hyponym of v
? otherwise
For adjectives, the similarity simWN(F,a), is computed on the basis of the syn-
onymy relation, as follows:
(5) simWN(F,a) =
?
?
?
1 iff ?w ? F such that
w is a synonym of tw
? otherwise
The overall model WNF is used to predict if a frame F is a correct situation for a
given unknownLU ul /?F (a noun, a verb or an adjective), whenever simWN(F,ul) > ?.
This can be used as a frame predictor for a ul currently not foreseen in the Berkley
database but possibly very frequent in a specific corpus, as described in Section 3.1.
2.2 Distributional model
The distributional model is based on the intuition that FrameNet frames and LUs can
be modelled in a semantic space, where they are represented as distributional co-
occurrence vectors computed over a corpus. Such framework, it is possible to compute
90 De Cao, Croce, Pennacchiotti, and Basili
the similarity between a LU and a frame, by evaluating the distance of their vectors in
the space.
Semantic spaces have been widely applied in several NLP tasks, ranging from in-
formation retrieval to paraphrase rules extraction (Lin and Pantel, 2001). The intuition
is that the meaning of a word can be described by the set of textual contexts in which
it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vec-
tors are semantically related. This distributional approach has been often claimed to
support the language in use view on meaning. Word space models (Sch?tze, 1993)
have been shown to emphasize different aspects of lexical semantics: associative (i.e.
topical) information between words, as well as paradigmatic information (i.e. in ab-
sentia) or syntagmatic information (i.e. in presentia).
In our setting, the goal is to leverage semantic spaces to capture the notion of frame
? i.e. the property of ?being characteristic of a frame?. To do so, we model a lexical
unit l as a vector ~l, whose dimensions represent the set of contexts of the semantic
space. In our space, contexts are words appearing in a n-window of the lexical unit:
such a space models a generic notion of semantic relatedness ? i.e. two LUs close in
the space are likely to be either in paradigmatic or syntagmatic relation (Pado, 2007;
Sahlgren, 2006). The overall semantic space is then represented by a matrix M, whose
rows describe LUs and whose columns describe contexts.
We reduce in dimensionality the matrix M by applying Singular Value Decom-
position (SVD) (Landauer and Dumais, 1997), a decomposition process that creates
an approximation of the original matrix, aiming to capture semantic dependencies
between source vectors, i.e. contexts. The original space is replaced by a lower di-
mensional space Mk, called k-space in which each dimension is a derived concept.
The matrix M is transformed in the product of three new matrices: U , S, and V such
that M = USVT . Truncating M to its first k dimensions means neglecting the least
meaningful dimensions according to the original distribution. Mk captures the same
statistical information in a new k-dimensional space, where each dimension is a linear
combination of some original features. These newly derived features may be thought
of as artificial concepts, each one representing an emerging meaning component as a
linear combination of many different words (or contexts).
The SVD reduction has two main advantages. First, the overall computational cost
of the model is reduced, as similarities are computed on a space with much fewer
dimensions. Secondly, it allows to capture second-order relations among LUs, thus
improving the quality of the similarity measure.
Once the vectors~l for all FrameNet LUs are available in the reduced space, it is
also possible to derive a vectorial representation ~F of a whole frame F . Intuitively,
~F should be computed as the geometric centroid of the vectors of its lexical units.
Unfortunately, such a simple approach is prone to errors due to the semantic nature
of frames. Indeed, even if the LUs of a given frame describe the same particular
situation, they can typically do that in different type of contexts. For example, the
LUs assassinate and holocaust evoke the KILLING frame, but are likely to appear in
very different linguistic contexts. Then, the vectors of the two words are likely to be
distant in the space. Consequently, different regions of the semantic space may act
as good representations for the same frame: these regions corresponds to clusters of
LUs which appear in similar contexts (e.g. {holocaust,extermination,genocide} and
Combining Word Sense and Usage for Modeling Frame Semantics 91
{suicide,euthanasia}).
We then adopt a clustering approach to model frames: each frame is represented
by the set of clustersCF of its lexical units. ClustersCF are composed by lexical units
close in the space and can have different size. They are computed by using an adaptive
(unsupervised) algorithm, based on k-means (Heyer et al, 1999; Basili et al, 2007),
applied to all known LUs of F . Each cluster c ? CF is represented in the space by a
vector~c, computed as the geometric centroid of all its lexical units.
In this framework, it is then possible to compute the similarity between an unknown
LU ul and a frame F , as the the cosine distance between the vector ~ul and the closest
centroid ~c ?CF :
(6) simLSF (F,ul) = argmaxc?CF simcos(~ul, ~CF )
Given this measure, it is finally possible to assign an unknown ul to one or more of
the most similar frames.
2.3 Combining paradigmatic and distributional models
In order to be effective in a NLP task, a model of lexical meaning should typically ac-
count for both the paradigmatic and distributional similarity. The following definition
thus hold:
(7) ?(F,w) = ?(D(w,F),P(w,F))
where ?(F,w) is the association between a word w and a frame F , ? is a com-
position operator applied to the corpus-driven distributional measure D(F,w) and
to the paradigmatic similarity P(F,w). Notice that in this work simLSF (F,w) and
simWN(F,w) are used as models of D(F,w) and P(F,W ), respectively. Different com-
binations ? are here possible, from simple algebraic operations (e.g. linear combina-
tions) to more complex algorithmics. We will explore this latter issue in section 3.1
where the evaluation of a combined model for LU induction is reported.
3 Experiments
In this section we experiment our proposed models on three different tasks: induc-
tion of new LUs (Section 3.1), mapping LUs to WordNet synsets (Section 3.2), and
automatic acquisition of LUs in Italian (Section 3.3). In all the experiments we use
FrameNet 1.3, consisting of 795 frames and about 10,196 LUs (7,522 unique LUs), as
source information and as a gold standard. As regardsWordNet, we adopt version 2.0,
with all mappings from 1.6 applied through the Italian component of MultiWordNet
(Pianta et al, 2008)2
For computing vectors in the distributional model, we use the TREC-2002 Vol.2
corpus, consisting of about 110 million words for English. The contexts for the de-
scription of LUs are obtained as ?5 windows around each individual LU occurrence:
each word occurring in this windows is retained as a potential context 3. A resulting
set of about 30,000 contexts (i.e. individual words) has been obtained. The vector~l
2http://www.lsi.upc.es/~nlp/tools/download-map.php
3For all occurrences of feature words, the POS tag has been neglected in order to verify the applicability
of the model even with a shallow preprocessing. Words occurring less than 10 times in all windows are
neglected in our experiments.
92 De Cao, Croce, Pennacchiotti, and Basili
representing an individual LU is derived by computing pointwise mutual information
between the LU and each context. The SVD reduction has been run over the result-
ing 7.522? 30,000 matrix, with a dimension cut of k = 50, other values resulting in
non-statistically different outcomes.
Experiments for Italian are run against the italian component of the Europarliament
corpus (Koehn, 2002), made of about 1 million sentences for about 36 millions tokens,
for which about 87,000 contexts are used for the targeted LUs. Also for Italian a
dimension cut of k = 50 has been applied.
3.1 Lexical Unit induction
The goal of this experiment is to tackle the FrameNet low coverage problem, by check-
ing if our models are good in expanding FrameNet with new LUs. Formally, we de-
fine LU induction as the task of assigning a generic unknown lexical unit ul not yet
present in the FrameNet database to the correct frame(s). As the number of frames
is very large, the task is intuitively hard to solve. A further complexity regards mul-
tiple assignments. Lexical units are sometimes ambiguous and can then be mapped
to more than one frame (for example the word tea could map both to FOOD and SO-
CIAL_EVENT). Also, even unambiguous words can be assigned to more than one
frame ? e.g. child maps to both KINSHIP and PEOPLE_BY_AGE.
In the experiment, we simulate the induction task by executing a leave-one-out
procedure over the set of existing FrameNet LUs, as follows. First, we remove a LU
from all its original frames. Then, we ask our model to reassign it to the most similar
frame(s), according to its similarity measure. We repeat this procedure for all lexical
units and compute the accuracy in the assignment.
We experiment all three models: distributional, paradigmatic and the combined
one. In particular, the combined model is applied as follows. First, for each frame
F we create its cluster set CF in the LSA space. Then, at each iteration of the leave-
one-out a different LU ul is removed from FrameNet, and the following steps are
performed:
? We recompute the clusters for all frames Ful which contain ul, by neglecting
ul.4
? We compute the similarity simLFS(F,ul) between ul and all frames. During
the computation we empirically impose a threshold: if a cluster c ? C has a
similarity cos(c,ul) < 0.1 (i.e. poorly similar to ul), it is neglected. Finally, all
suggested frames are ranked according to simLFS(F,ul).
? For each frame F we also compute the similarity simWN(F,ul) according to the
paradigmatic model, by neglecting ul in the computation of the WordNet model
of each frame.
? We combine the distributional and paradigmatic similarities following the gen-
eral Equation 7, by applying the following specific equation:
(8) sim(F,ul) = simLFS(F,ul) ? simWN(F,ul)
4Note that all appearances of ul in the database are neglected (irrespectively from its POS tag, e.g.
march as a verb vs. march as a noun)
Combining Word Sense and Usage for Modeling Frame Semantics 93
Table 1: The Gold Standard for the test over English and Italian
English Number of frames: 220
Number of LUs: 5042
Most likely frames: Self_Motion (p=0.015), Clothing (p=0.014)
Italian Number of frames: 10
Number of LUs: 112
Frames: Buildings, Clothing, Killing, Kinship, Make_noise
Medical_conditions, Natural_Features, Possession, Self_Motion, Text
Note, that in practice sim(F,ul) acts as a re-ranking function of the previously
obtained clusters CF
? We execute LU induction, by mapping ul to the most similar k frames according
to sim(F,ul).
Evaluation
We evaluate the model by computing the accuracy over the FrameNet gold standard.
Accuracy is defined as the fraction of LUs that are correctly re-assigned to the original
frame during the leave-one-out. Accuracy is computed at different levels k: a LU is
correctly assigned if its gold standard frame appears among the best-k frames ranked
by the model. We experimented both on English (using FrameNet version 1.3), and on
Italian. Since an Italian FrameNet is not available, we manually created a gold stan-
dard of 11 frames. Overall statistics on the data are reported in Table 1: the number of
frames and LUs analysed is slightly reduced wrt FrameNet as we ignored the predicate
words absent from the targeted corpus (e.g. moo in MAKE_NOISE) and multiwords
expressions as it was not possible to locate them unambiguously in the corpus (e.g.
shell out in COMMERCE_PAY). Also, in order to get reliable distributional statistics,
we filter out LUs occurring less than 50 times in the corpus, and frames with less than
10 LUs.
For all the experiments, the parameter ? in the Equation 4 of the paradigmaticmodel
has been set to 2. As a baseline, we adopt a model predicting as best-k frames the most
likely ones in FrameNet ? i.e. those containing the highest number of LUs.
Results for English are reported in Figure 2.
As shown, all methods improve significantly the baseline whereas accuracy values
naturally improve along increasing values for k. The performance of the paradigmatic
model are significantly high even for very small k. The best model is given by the
combination of distributional and paradigmatic similarity, producing significant im-
provements wrt the paradigmatic model alone.
Results for Italian are reported in Figure 3. The leave-one-out test has been applied
as for English, but over a manually compiled set of 527 LUs for the 11 frames used
as gold standard. These LUs have been obtained via direct translation of the English
Framenet LUs. In order to evaluate LUs for which a consistent distributional model
was available, only those occurring at least 50 times in the Europarliament corpus have
been selected: this amounts to a total number of 112 Italian LUs. The paradigmatic
94 De Cao, Croce, Pennacchiotti, and Basili
Figure 2: Accuracy of the leave-one-out over the English FrameNet 1.3
model for the test has been obtained using as source the LUs in the English FrameNet.
As the computation of the simWN(F,w) depends only on the hyponymy hierarchy, for
each Italian noun n the conceptual density computation over the set {n}?F is applied,
where F is given by the LUs in English. The interlingual index is here used to map
every n to its lexical senses (i.e. synsets) in the English WN. Then, the computation
of the greedy algorithm is applied exactly as in the monolingual process. The same
approach has been used for verbs (Equation 4) and adjectives (Equation 5).
Although the limited scale of the experiment (only 11 frames are targeted), the ev-
idence are similar as for the test over English: the combined model is always superior
to the individual ones. High levels of accuracy are achieved, although the ?most likely
frame? baseline is much higher than for the English test. Similar trends are also ob-
served for the paradigmatic model, reaching a plateau for smaller values of k. Overall
results indicate that reliable predictions can be obtained for unknown LUs also when
a whole Italian FrameNet is not yet available. Our method can then be used to support
lexicographers in the task of building a new FrameNet, in the specific stage of adding
LUs to frames.
Results suggest that the WordNet models derived from the English LUs are valid
predictors also for Italian words, as confirmed by the experiments in the next sections.
3.2 Assessing WordNet models of Frames
The goal of the experiment is to validate the notion of WordNet model of a frame as
derived through the method discussed in Section 2.1. Formally, given the set of all
possible WordNet senses Sl of a given LU l, we aim at mapping each sense s ? Sl to
the correct frame f ? Fl , where Fl is the set of frames in which l appears. If a frame
cannot be found for a given sense, the sense is simply neglected.
For example, the LU burn has 15 senses in WordNet and it belongs to 3 frames:
EMOTION_HEAT, EXPERIENCE_BODILY_HARM and PERCEPTION_BODY. Figure 2
Combining Word Sense and Usage for Modeling Frame Semantics 95
Figure 3: Accuracy of the leave-one-out tests over 11 frames in Italian
reports some of the possible correct mapping between senses and frames. Other
senses, such as ?destroy by fire? cannot be mapped to any existing frame.
By creating such an automatic mapping we achieve three goals. First, we disam-
biguate FrameNet lexical units. Second, we enrich WordNet synsets with new infor-
mation ? i.e. a computational description of the situations they refer to, as repre-
sented in FrameNet. Third, we derive a language independent model of frames based
on WordNet synsets.
The mapping targeted by the experiment is carried out according to the discussion
in Section 2.1. The WordNet model of a frame F for nouns is the outcome of the
greedy cd computation over the set F of all frame?s LUs: given a LU, a sense is
accepted if it is a member of the set LF in the model. For verbs and adjectives all co-
hyponyms and synonyms used in Equations 4 and 5 are included in LF . The procedure
for developing a WordNet model is completely automatic, this avoiding the costs of
manual annotation.5
Note that our approach is easily portable to languages different from English. In-
deed, the WordNet hierarchy is the backbone of sense repositories in other languages
(as for example in MultiWordNet (Pianta et al, 2008)). The English modelsWNF can
be then interpreted in a different language, by applying the interlingual indexes to all
synsets LF in WNF . The corresponding sets of synonyms are natural candidates as
LUs in the target language.
Evaluation
In this experiment, in order to account for data sparseness we reduce the dataset in
two ways. First, we neglect low frequency lexical units: LUs occurring less than 50
times in the corpus are not considered. Second, we exclude frames that have less than
5In this experiment we focus on verbs and nouns, since they are core predicates expressing the targeted
situation in sentences.
96 De Cao, Croce, Pennacchiotti, and Basili
Table 2: Mapping between WordNet senses and frames for verb burn, as induced by
the paradigmatic method
Synset Evoked FRAME Co-Hyponyms WordNet Definition
1775952 EMOTION_HEAT chafe, fume, smolder Feel strong emotion, especially anger or
passion; ?She was burning with anger?;
?He was burning to try out his new
skies?
189569 EXPERIENCE_BODILY_HARM break, bruise, hurt,
injure
Burn with heat, fire, or radiation; ?The
iron burnt a hole in my dress?
2059143 PERCEPTION_BODY itch, sting Cause a sharp or stinging pain or dis-
comfort; ?The sun burned his face?
10 LUs. This leaves us with 220 frames, involving 2,200 nominal LUs and 2,180
verbal LUs. Table 3 reports overall statistics. Over the 2,200 nouns and 2,180 verbs
examined, the vast majority is covered by WordNet (fourth row). For these words, a
large set of lexical senses exist in WordNet giving an average polysemy between 3 and
6 senses per word (sixth row). Our paradigmatic method is able to significantly reduce
the average polysemy: only 1.79 senses per verb survive among the initial 5.29, while
only 1.29 among the 3.62 are retained for nouns. Moreover, the number of senses
used to entirely represent a frame in a paradigmatic model (i.e. SF ) is about 3,512 and
2,718 respectively for nouns and verbs, as averaged across all frames. An example of
the mapping produced by our method is reported in Table 2.
The above statistics suggest that a consistent reduction in average polysemy can be
obtained when the context of a frame is used to model semantic similarity among LUs
in WordNet.
Table 3: Statistics on nominal and verbal senses in the paradigmatic model of the
English FrameNet
Nouns Verbs
Targeted Frames 220 220
Involved LUs 2,200 2,180
Average LUs per frame 10.0 9.91
LUs covered by WordNet 2,187 2,169
Number of Evoked Senses 7,443 11,489
Average Polysemy 3.62 5.97
Represented words (i.e. ?FWF ) 2,145 1,270
Average represented LUs 9.94 9.85
Active Lexical Senses (LF ) 3,095 2,282
Average Active Lexical Senses (|LF |/|WF |) per word over frames 1.27 1.79
Active synsets (SF ) 3,512 2,718
Average Active synsets (|SF |/|WF |) per word over frames 1.51 2.19
We evaluated the quality of the above process through manual validation. Given a
frame, for each LU we provided two annotators with the list of all its WordNet senses,
and asked to select those that correctly map to FrameNet. Then, we evaluated our
automatic mapping method by computing standard Precision and Recall. In all, we
Combining Word Sense and Usage for Modeling Frame Semantics 97
analysed all 786 senses of 306 LUs in 4 frames (i.e. KILLING, PEOPLE_BY_AGE,
STATEMENT and CLOTHING). The Cohen?s kappa, computed over two frames (i.e.
KILLING and PEOPLE_BY_AGE for 192 senses of 77 words) results in a 0.90 inter-
annotator agreement: this indicates that senses and frames are highly correlated and
their mapping is consistent and motivated, as Table 2 suggests.
The system is considered to accept a sense ? for a given frame F iff the conceptual
density score characterizing such a sense is positive, i.e. the ? ? LF . Our method
obtained a Precision of 0.803 and a Recall of 0.79 (F-measure=0.796). Among the 786
senses tested, 85 false positives and 92 false negatives have been found: 346 senses
have been correctly accepted and 263 true negatives have been rejected by the sytem.
It must be also noticed that the conceptual density scores obtained are well correlated
with correct senses. If senses of a word with significantly lower cd scores than others
are removed from the set LF of a frame, a significant improvement in precision can
be obtained. For example, tie in CLOTHING has 9 senses, of which 3 are proposed by
the system, corresponding to 1 true positives, 2 false positives and 6 true negatives.
It is interesting to note that the true positive sense (i.e. {necktie, tie} as ?a neckwear
consisting of a long narrow piece of material worn ...?) has a cd score of 0.492, while
0.018 is the score of all the three false positives (i.e. sense #5 {link,linkup tie, tie-in}
as ?a fastener that serves to join or link?; sense #8 {tie, railroad tie, crosstie, sleeper}
as ?one of the cross braces that support the rails on a railway track?; sense #9 {tie} as
?a cord with which something is tied?). A careful selection policy can be thus easily
devised to deal with such skewed preference distributions and achieve higher values
of precision by neglecting lower preferences.
These results show that the proposed frame WordNet model is not only effective
in reducing the average lexical polysemy (as shown in Table 3), but it is also a rather
accurate method to capture the lexical semantics implied by frames. The achieved
level of accuracy justifies the adoption of the model defined in (3) for the development
of FrameNets in languages other than English.
3.3 Development of an Italian FrameNet
In this section we explore the use of our English paradigmatic model to automatically
support the building process of a FrameNet in a different language, namely Italian. In
particular, we leverage the model WNF for the English language to induce new LUs
for F in the new language. To do so, we proceed as follows.
For each frame F in FrameNet we first generate the WordNet model for English
WNF using all the LUs available in the database, as discussed in Section 2.1. Then,
we use an interlingual index (e.g. MultiWordNet) to obtain words in the new language
corresponding to lexical senses LF in the model WNF . Each of these translated LU
l is a cross-lingual synonym of at least a sense in SF and is a candidate LUs for the
frame in the new language, since it satisfies simWN(F, l) > ?.
Evaluation
In the experiment we focus on Italian, for which a full FrameNet is not yet available,
though a manual building process is currently underway (Tonelli and Pianta, 2008).
As interlingual index we adopt the Italian component of MultiWordNet (Pianta et al,
2008). As shown in Table 4, the WordNet model allows to generate approximately
15,000 Italian LUs, partitioned in 6,600 nouns, 8,300 verbs and 130 adjectives.
98 De Cao, Croce, Pennacchiotti, and Basili
Table 4: Number of generated Lexical Units
Number of LUs
Nouns 6611
Verbs 8332
Adjectives 129
Total 15072
To evaluate the quality of the translated LUs we performed two different tests. In
the first test, we collected the 776 most frequent words in the Europarliament corpus,
including many generic nouns and verbs, such as produrre (to_produce/make), fare
(to_make/fabricate), avere (to_have). Then we manually validated all the 1,500 sys-
tem decisions regarding these words. A decision is accepted if the frame suggested
for the word is correct for at least one of its senses. Accuracy is computed as the per-
centage of the correct system decisions over the number of validated cases. For some
words no frame was predicted, as they were not in Wordnet or as no Wordnet model
was able to correctly generalize them. The percentage of words receiving at least one
correct prediction, i.e. assigned to at least one frame accepted by the annotators, is
here called Coverage, and reported with the accuracy scores in the second line of Ta-
ble 5. The above test was repeated also for more specific words, with a number of
occurrences in the corpus ranging from 100 to 200. Results are reported in the third
line of Table 5. These outcomes are surprisingly good especially considering that the
computation of the individual simWN(F, l) scores is fully automatic.
Table 5: Manual validation of the italian LUs generated through WordNet
Frequency Numb. of Numb. of
Range Test pairs Words Acc. Cov.
[722;55,000] 1,500 776 0.79 93.0%
[100;200] 558 357 0.87 94.3%
In a second test, the results generated by our method were compared against the
words of the oracle manually developed for the experiment in Section 3.1. In this
case, the predictions of the method about frames and words are compared with the
oracle. As no filter has been here applied with respect to the corpus, all the 527
< LU,Frame > pairs in the manually created oracle have been used6, although only
437 pairs were represented through the MultiWordNet resources. The system was
not able to decide for 71 words, and produced wrong guesses for 49 words: 317
correct guesses are thus produced. The results is a Precision of 0.87 and a Recall of
0.72. The recall value, lower than the coverage observed in the previous test, is also
by a significant generative effect: the method discovers a number of new entries not
accounted for in the oracle.
6Notice that no LU was multiply assigned to different frames in the oracle, so that the number of predi-
cate words here is exactly the number of different pairs.
Combining Word Sense and Usage for Modeling Frame Semantics 99
Table 6: Excerpt of the Italian LUs in four different frames
FRAME FRAME definition Italian LUs
BUILDINGS Words which name permanent
fixed structures forming an
enclosure and providing pro-
tection from the elements
autorimessa, cuccia, casolare,
casotto, dependance, masseria,
palazzina, ...
CLOTHING Anything that people conven-
tionally wear
cappotto, calzetta, cami-
cia_da_notte, duepezzi, ...
SELF_MOTION The Self mover, a living being,
moves under its own power in a
directed fashion
annaspare, arrancare, buttarsi,
claudicare, giro, ...
TEXT An entity that contains linguis-
tic, symbolic information on a
Topic, created by an Author at
the Time of creation
arringa, articolo_di_fondo,
canzonetta, conto, polizza,
vademecum, ...
Typical new LUs introduced in the oracle are words not accounted for in the En-
glish Framenet, as reported in Table 6. The table shows that most of the new guesses
of the system are indeed highly plausible. They represent widely used dialectal forms
(e.g. masseria in the frame BUILDING), jargon (e.g. duepezzi in CLOTHING), techni-
cal terms (e.g. polizza in TEXT) and specific nouns (e.g. autorimessa in BUILDINGS).
Although it is certainly questionable if words like articolo_di_fondo (i.e. main article
in a newspaper) are worth to be considering as LUs for the frame TEXT, it is clear that
if the application domain requires frame-like information, the presented model (even
only the paradigmatic association here discussed) provides an effective tool for fast
and robust prototyping. Notice that the low Recall (only 0.60 of the oracle words are
correctly addressed), can be compensated by combining paradigmatic and distribu-
tional similarity (Equation 8), as experiments reported in Figure 3 suggest. We leave
this last point as a future work.
4 Conclusions
We presented a combined model for representing frame semantics through paradig-
matic and distributional evidence. We reported three experiments, which indicate pos-
sible application scenarios of these models. First, the combination of the presented
models has been applied to extend FrameNet in a LU induction task, for English and
Italian. In both cases the evaluation has shown that the combination of the two models
achieves better performance against their independent uses, and that the level of accu-
racy is high enough to support lexicographers in the task of building FrameNets. In a
second experiment, we showed that a strong association exists between lexical senses,
as defined by WordNet, and the frame?s lexical units in FrameNet. Its automatic de-
tection, as proposed in this paper, results in a significant reduction of the polysemy of
LUs and in a highly accurate selection of those lexical senses semantically related to
the situations represented by a frame. Finally, we demonstrated that this paradigmatic
information can be used to develop a FrameNet resource in another language. For
100 De Cao, Croce, Pennacchiotti, and Basili
Italian, we automatically generated a very large and accurate set of 15,000 LUs.
The overall framework has encouraged us to develop a robust toolbox for the large
scale acquisition of FrameNet-like lexicons in different domains and languages. The
tool will be made publicly available for research studies in this area. Future work is on
going on the adoption of richer models for Framenet, able to take into account more
evidence than LUs, such as frame elements and syntagmatic information. Moreover,
the use of the derived space as a model for the recognition of frames in free-texts is
expected to speed-up the development of a large collection of annotated sentences for
the Italian language.
References
Agirre, E. and G. Rigau (1996). Word sense disambiguation using conceptual density.
In Proceedings of COLING-96, Copenhagen, Denmark.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In Proceedings of COLING-ACL, Montreal, Canada.
Basili, R., M. Cammisa, and F. Zanzotto (2004). A semantic similarity measure for
unsupervised semantic disambiguation. In Proceedings of LREC-04, Lisbon, Por-
tugal.
Basili, R., D. D. Cao, P. Marocco, and M. Pennacchiotti (2007). Learning selectional
preferences for entailment or paraphrasing rules. In Proceedings of RANLP 07.
Burchardt, A., K. Erk, and A. Frank (2005). A wordnet detour to framenet. In Pro-
ceedings of the GLDV 2005 GermaNet II Workshop, Bonn, Germany.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado, and M. Pinkal (2006). The
salsa corpus: a german corpus resource for lexical semantics. In Proceedings of
the 5th International Conference on Language Resources and Evaluation, Genova,
Italy.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2008). Assessing the
impact of frame semantics on textual entailment. Journal of Natural Language
Engineering (to appear).
Clark, P., P. Harrison, J. Thompson, W. Murray, J. Hobbs, and C. Fellbaum (2007,
June). On the Role of Lexical and World Knowledge in RTE3. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Prague, pp.
54?59. Association for Computational Linguistics.
Harris, Z. (1964). Distributional structure. In J. J. Katz and J. A. Fodor (Eds.), The
Philosophy of Linguistics, New York. Oxford University Press.
Heyer, L., S. Kruglyak, and S. Yooseph (1999). Exploring expression data: Identifi-
cation and analysis of coexpressed genes. Genome Research 9, 1106?1115.
Koehn, P. (2002). Europarl: A multilingual corpus for evaluation of machine transla-
tion. Draft.
Combining Word Sense and Usage for Modeling Frame Semantics 101
Landauer, T. and S. Dumais (1997). A solution to plato?s problem: The latent se-
mantic analysis theory of acquisition, induction and representation of knowledge.
Psychological Review 104, 211?240.
Lin, D. and P. Pantel (2001). DIRT-discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discovery and Data Mining (KDD-01),
San Francisco, CA.
Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. (1990). An on-line
lexical database. International Journal of Lexicography 13(4), 235?312.
Pado, S. (2007). Cross-Lingual Annotation Projection Models for Role-Semantic In-
formation, Volume 21 of Saarbr?cken Dissertations in Computational Linguistics
and Language Technology. Saarland University.
Pado, S. and M. Lapata (2007). Dependency-based construction of semantic space
models. Computational Linguistics 33(2), 161?199.
Pad?, S., M. Pennacchiotti, and C. Sporleder (2008). Semantic role assignment for
event nominalisations by leveraging verbal data. In Proceedings of COLING 2008,
Manchester, UK.
Pianta, E., L. Bentivogli, and C. Girardi (2008). MultiWordNet: Developing an
aligned multilingual database. In Proceedings of the 1st International Global
WordNet Conference, Marrakech, Morocco, pp. 293?302.
Pitel, G. (2006). Using bilingual lsa for framenet annotation of french text from
generic resources. In Workshop on Multilingual Semantic Annotation: Theory and
Applications, Saarbr?ijcken, Germany.
Sahlgren, M. (2006). The Word-Space Model. Ph. D. thesis, Department of Linguis-
tics, Stockholm University.
Sch?tze, H. (1993). Word space. In S. Hanson, J. Cowan, and C. Giles (Eds.), Ad-
vances in Neural Information Processing Systems 5. MorganKaufmann Publishers.
Shen, D. and M. Lapata (2007). Using semantic roles to improve question answer-
ing. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing and on Computational Natural Language Learning, Prague, pp. 12?21.
Subirats, C. and M. Petruck (2003). Surprise! Spanish FrameNet! In Proceedings of
the Workshop on Frame Semantics at the XVII. International Congress of Linguists,
Prague.
Tonelli, S. and E. Pianta (2008). Frame Information Transfer from English to Italian.
In Proceedings of LREC 2008, Marrakech, Morocco.
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 24?32,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Robust and Efficient Page Rank for Word Sense Disambiguation
Diego De Cao, Roberto Basili, Matteo Luciani, Francesco Mesiano, Riccardo Rossi
Dept. of Computer Science,
University of Roma Tor Vergata, Roma, Italy
{decao,basili}@info.uniroma2.it
{matteo.lcn,fra.mesiano,ricc.rossi}@gmail.com
Abstract
Graph-based methods that are en vogue
in the social network analysis area, such
as centrality models, have been recently
applied to linguistic knowledge bases, in-
cluding unsupervised Word Sense Disam-
biguation. Although the achievable accu-
racy is rather high, the main drawback of
these methods is the high computational
demanding whenever applied to the large
scale sense repositories. In this paper
an adaptation of the PageRank algorithm
recently proposed for Word Sense Dis-
ambiguation is presented that preserves
the reachable accuracy while significantly
reducing the requested processing time.
Experimental analysis over well-known
benchmarks will be presented in the paper
and the results confirm our hypothesis.
1 Introduction
Lexical ambiguity is a fundamental aspect of natu-
ral language. Word Sense Disambiguation (WSD)
investigates methods to automatically determine
the intended sense of a word in a given context
according to a predefined set of sense definitions,
provided by a semantic lexicon. Intuitively, WSD
can be usefully exploited in a variety of NLP (e.g.
Machine Translation (Chan et al, 2007; Carpuat
and Wu, 2007)) and Information Retrieval tasks
such as ad hoc retrieval (Krovetz, 1997; Kim et
al., 2004) or Question Answering (Beale et al,
2004). However controversial results have been
often obtained, as for example the study on text
classification reported in (Moschitti and Basili,
2004). The impact of WSD on IR tasks is still an
open issue and large scale assessment is needed.
For this reason, unsupervised approaches to in-
ductive WSD are appealing. In contrast with su-
pervised methods that strongly rely on manually
labeled data sets, those methods do not require an-
notated examples for all words and can thus sup-
port realistic (large scale) benchmarks, as needed
in IR research.
In recent years different approaches to Word
Sense Disambiguation task have been evaluated
through comparative campaigns, such as the ear-
lier Senseval evaluation exercises. (Palmer et al,
2001; Snyder and Palmer, 2004) or the most recent
(Pradhan et al, 2007).
The best accuracy is reached by WSD based on
supervised methods that exploit large amounts of
hand-tagged data to train discriminative or gen-
erative disambiguation models. The common al-
ternative to supervised systems are knowledge-
based WSD systems that try to exploit informa-
tion made available by large Lexical Knowledge
Bases (LKB). They enable the definition of sev-
eral metrics to estimate semantic similarity (e.g.
(Lesk, 1986) or (Agirre and Rigau, 1996), (Basili
et al, 2004) methods) and then use it to rank the
alternative senses according to the incoming con-
text. Moreover they make available large relation-
ship sets between pairs of lexical meaning units,
such as synonymy, hyponymy or meronymy. The
resulting networks represent at various grains and
degrees of approximation models of the mental
lexicons. It is not by chance that early research
on WSD based on semantic dictionaries were ap-
plying models of network activation processes (in
particular simulated annealing as in (Cowie et al,
1992)) for precise and fast disambiguation.
It has been more recently that graph-based
methods for knowledge-based WSD have gained
much attention in the NLP community ((Sinha
and Mihalcea, 2007), (Navigli and Lapata, 2007),
(Agirre and Soroa, 2008), (Agirre and Soroa,
2009)). In these methods a graph representa-
tion for senses (nodes) and relation (edges) is first
built. Then graph-based techniques that are sen-
sible to the structural properties of the graph are
used to find the best senses for words in the in-
coming contexts. The relation employed by the
different methods are of several types such as syn-
onymy, antonymy but also co-occurrence based
lexical similarity computed externally over a cor-
pus. These give rise to real-valued weights that
determine large weighted directed graphs. Usu-
24
ally, the employed disambiguation is carried out
by ranking the graph nodes. Thus the concepts
with highest ranks are assigned to the correspond-
ing words. In (Agirre and Soroa, 2009), a com-
parative analysis of different graph-based mod-
els over two well known WSD benchmarks is re-
ported. In the paper two variants of the random
surfer model as defined by PageRank model (Brin
and Page, 1998) are analyzed. A special emphasis
for the resulting computational efficiency is also
posed there. In particular, a variant called Per-
sonalized PageRank (PPR) is proposed (Agirre
and Soroa, 2009) that tries to trade-off between
the amount of the employed lexical information
and the overall efficiency. In synthesis, along the
ideas of the Topic sensitive PageRank (Haveli-
wala, 2002), PPR suggests that a proper initial-
ization of the teleporting vector ~p suitably captures
the context information useful to drive the random
surfer PageRank model over the graph to converge
towards the proper senses in fewer steps. The ba-
sic idea behind the adoption of PPR is to impose
a personalized vector that expresses the contexts
of all words targeted by the disambiguation. This
method improves on the complexity of the previ-
ously presented methods (e.g. (Agirre and Soroa,
2008)) as it allows to contextualize the behaviors
of PageRank over a sentence, without asking for
a different graph: in this way the WordNet graph
is always adopted, in a word or sentence oriented
fashion. Moreover, it is possible to avoid to rebuild
a graph for each target word, as the entire sen-
tence can be coded into the personalization vector.
In (Agirre and Soroa, 2009), a possible, and more
accurate alternative, is also presented called PPR
word2word (PPRw2w) where a different person-
alization vector is used for each word in a sen-
tence. Although clearly less efficient in terms of
time complexity, this approach guarantees the best
accuracy, so that it can be considered the state-of-
the art in unsupervised WSD.
In this paper a different approach to personal-
ization of the PageRank is presented, aiming at
preserving the suitable efficiency of the sentence
oriented PPR algorithm for WSD but achieving
an accuracy at least as high as the PPRw2w one.
We propose to use distributional evidence that can
be automatically acquired from a corpus to define
the topical information encoded by the personal-
ization vector, in order to amplify the bias on the
resulting PPR and improve the performance of
the sentence oriented version. The intuition is that
distributional evidence is able to cover the gap be-
tween word oriented usages of the PPR as for the
PPRw2w defined in (Agirre and Soroa, 2009),
and its sentence oriented counterpart. In this way
we can preserve higher accuracy levels while lim-
iting the number of PageRank runs, i.e. increasing
efficiency.
The paper is structured as follows. We first give
a more detailed overview of the PageRank and
Personalized PageRank algorithms in Section 2.
In Section, 3 a description of our distributional ap-
proach to the personalized PageRank is provided.
A comparative evaluation with respect to previous
works is then reported in Section 4 while section 5
is left for conclusions.
2 Graph-based methods for Word Sense
Disambiguation
Word sense disambiguation algorithms in the
class of graph-based method are unsupervised ap-
proaches to WSD that rely almost exclusively on
the lexical KB graph structure for inferring the rel-
evance of word senses for a given context. Much
current work in WSD assume that meaning dis-
tinctions are provided by a reference lexicon (the
LKB), which encodes a discrete set of senses
for each individual word. Although the largely
adopted reference resource is WordNet (Miller et
al., 1990), the graph-based algorithms are not lim-
ited to this particular lexicon. In these methods,
nodes are derived from the sense units, i.e. the
synsets, and edges are derived from semantic re-
lations established between synsets. We will here-
after use WordNet to discuss the details of the dif-
ferent steps. Every algorithm can be decomposed
in a set of general steps:
Building the graph. The first step proceeds
to the definition of the graph structure. As in-
troduced before, WordNet is mapped into a graph
whose nodes are concepts (represented by synsets
(i.e., synonym sets)) and whose edges are seman-
tic relations between concepts (e.g., hyperonymy,
meronymy). For each sentence, a graph G =
(V,E) is built, which is derived from the entire
graph of the reference lexicon. More formally,
given a sentence ? = w1, w2, . . . , wn, where wi
is a word, the following steps are executed to
build G: (1) the sense vocabulary V? is derived
as V? :=
?n
i=1 Senses(wi), where Senses(wi)
is the set of senses of any of the wi of the sen-
25
tence. (2) For each node v ? V?, a visit of the
WordNet graph is performed: every time a node
v? ? V?(v? 6= v) is encountered along a path
v ? v1 ? . . . ? vk ? v? all intermedi-
ate nodes and edges on the path from v to v? are
added to the graph: V := V
?
{v1, . . . , vk} and
E := E
?
{(v, v1), . . . , (vk, v?)}. The constructed
graph is the subgraph covering the nodes and rela-
tions of all the relevant vocabulary in the sentence.
Sense Ranking. The derived graph is then used
with different ranking models to find the correct
senses of words into the sentence ?. A suitable in-
terpretation of the source sentence can be in fact
obtained by ranking each vertex in the graph G
according its centrality. In (Navigli and Lapata,
2007) different ranking models are described. The
specific algorithm presented in (Agirre and Soroa,
2008) is the major inspiration of the present pa-
per, and makes use of PageRank (Brin and Page,
1998) to rank edges in the graph G. PageRank
tries to separate these nodes from the other candi-
date synsets of words in ?, which are expected to
activate less relations on average and remain iso-
lated. Let the vector ~Rank express the probability
to reach any of the vertices V?, and letM represent
the edge information. The expected rank between
senses satisfies:
~Rank = (1? ?)M ? ~Rank + ?~p (1)
whereas 0 ? ? ? 1. ? is called the damping
factor. It models the amount of likelihood that
a generic Web surfer, standing at a vertex, ran-
domly follows a link from this vertex toward any
other vertex in the graph: the uniform probability
pi = 1N ?i, is assigned to each one of the N ver-
tices in G. While it guarantees the convergence of
the algorithm, it expresses the trade-off between
the probability of following links provided by the
Web graph and the freedom to violate them. An
interesting aspect of the ranking process is the ini-
tial state. Many algorithms (as well as the one pro-
posed by (Agirre and Soroa, 2009)) initialize the
ranks of the vertex at a uniform value (usually 1/N
for a graph with N vertices). Then Equation 1 is
iterated until convergence is achieved or a maxi-
mum fix number of iterations has been reached.
Disambiguation. Finally, the disambiguation
step is performed by assigning to each word wi in
the source sentence ?, the associated j-th concept
senseij (i.e. the j-th valid interpretation for wi)
associated to the maximum resulting rank. In case
of ties all the concepts with maximum rank are as-
signed to wi ? ?.
The above process has several sources of com-
plexity, but the major burden is related to the Sense
ranking step. While complex methods have been
proposed (as discussed in (Navigli and Lapata,
2007)), sentence oriented algorithms, that build
the graph G once per each sentence ?, whatever
the number of wi ? ? is, are much more efficient.
The problem is twofold:
? How different sentences can be targeted with-
out major changes in the graph G? How the
matrix M can be made as much reusable as
possible?
? How to encode in Eq. 1 the incoming con-
text in order to properly address the different
words in the sentence ??
In order to address the above problems, in line
with the notion of topic-sensitive PageRank, a per-
sonalized PageRank approach has been recently
devised (Agirre and Soroa, 2009) as discussed in
the next section.
2.1 Personalizing PageRank for WSD
In (Agirre and Soroa, 2009), a novel use of PageR-
ank for word sense disambiguation is presented. It
aims to present an optimized version of the algo-
rithm previously discussed in (Agirre and Soroa,
2008). The main difference concerns the method
used to initialize and use the graph G for disam-
biguating a sentence with respect to the overall
graph (hereafter GKB) that represents the com-
plete lexicon.
Previous methods (such as (Agirre and Soroa,
2008)) derive G as the subgraph of GKB whose
vertices and edges are particularly relevant for the
given input sentence ?. Such a subgraph is often
called the disambiguation subgraph ?, GD(?).
GD is a subgraph of the original GKB, obtained
by computing the shortest paths between the con-
cepts of the words co-occurring in the context.
These are expected to capture most of the infor-
mation relevant to the disambiguation (i.e. sense
ranking) step.
The alternative proposed in (Agirre and Soroa,
2009) allows a more static use of the full LKB.
Context words are newly introduced into the graph
G as nodes, and linked with directed edges (i.e.
the lexical relations) to their respective concepts
(i.e. synsets). Topic-sensitive PageRank over the
graph G (Haveliwala, 2002) is then applied: the
initial probability mass is concentrated uniformly
26
over the newly introduced word nodes through the
setting of the personalization vector ~p in Eq. 1
(Haveliwala, 2002). Words are linked to the con-
cepts by directed edges that act as sources to prop-
agate probability into the GKB concepts they are
associated with. A personalized PageRank vector
is finally produced that defines a measure of the
(topological) relevance of the GKB nodes (con-
cepts) activated by the input context. The overall
time complexity is limited by the above sketched
Personalized PageRank approach (PPR) as a sin-
gle initialization of the graph GKB is requested
for an entire target sentence. This sentence ori-
ented method reuses the GKB of the entire lexi-
con, while the second step runs the sense ranking
once for all the words. This method reduces the
number of invocations of PageRank thus lowering
the average disambiguation time.
A word oriented version of the algorithm is
also proposed in (Agirre and Soroa, 2009). It
defines different initializations for the different
words wi ? ?: these are obtained by setting the
initial probability mass in ~p to 0 for all the senses
Sense(wi) of the targetedwi. In this way, only the
context words and not the target are used for the
personalization step1. This approach to the per-
sonalized PageRank is termed word-by-word or
PPRw2w version in (Agirre and Soroa, 2009).
PPRw2w is run on the same graph but with n
different initializations where n is the number of
words in ?. Although less efficient, PPRw2w is
shown to outperform the sentence oriented PPR
model.
3 A distributional extension
of PageRank
The key idea in (Agirre and Soroa, 2009) is to
adapt the matrix initialization step in order to ex-
ploit the available contextual evidence. Notice that
personalization in Word Sense Disambiguation
is inspired by the topic-sensitive PageRank ap-
proach, proposed in (Haveliwala, 2002), for Web
search tasks. It exploits a context dependent defi-
nition of the vector ~p in Eq. 1 to influence the link-
based sense ranking achievable over a sentence.
Context is used as only words of the sentence
(or words co-occurring with the target wi in the
w2w method) are given non zero probability mass
1This seems to let the algorithm to avoid strong biases
toward pairs of senses of a given word that may appear in
some semantic relations (thus connected in the graph), that
would be wrongly emphasized by the PPR method.
in ~p: this provides a topical bias to PageRank.
A variety of models of topical information have
been proposed in IR (e.g. (Landauer and Dumais,
1997)) to generalize documents or shorter texts
(e.g. query). They can be acquired through large
scale corpus analysis in the so called distributional
approaches to language modeling. While contexts
can be defined in different ways (e.g as the set
of words surrounding a target word), their anal-
ysis over large corpora has been shown to effec-
tively capture topical and paradigmatic relations
(Sahlgren, 2006). We propose to use the topical
information about a sentence ?, acquired through
Latent Semantic Analysis (Landauer and Dumais,
1997), as a source information for the initializa-
tion of the vector ~p in the PPR (or PPRw2w)
disambiguation methods.
SVD usually improves the word similarity com-
putation for three different reasons. First, SVD
tends to remove the random noise present in the
source matrix. Second, it allows to discover the
latent meanings of a target word through the cor-
pus, and to compute second-order relations among
targets, thus improving the similarity computation.
Third, similarities are computed in a lower dimen-
sional space, thus speeding up the computation.
For the above reasons by mapping a word, or a
sentence, in the corresponding Latent Semantic
Space, we can estimate the set of its similar words
according to implicit semantic relations acquired
in an unsupervised fashion. This can be profitably
used as a personalization model for PPR.
For the WSD task, our aim is to exploit an ex-
ternally acquired semantic space to expand the in-
coming sentence ? into a set of novel terms, dif-
ferent but semantically related with the words in
?. In analogy with topic-driven PageRank, the use
of these words as a seed for the iterative algorithm
is expected to amplify the effect of local informa-
tion (i.e. ?) onto the recursive propagation across
the lexical network: the interplay of the global in-
formation provided by the whole lexical network
with the local information characterizing the ini-
tialization lexicon is expected to maximize their
independent effect.
More formally, let the matrix Wk := UkSk be
the matrix that represents the lexicon in the k-
dimensional LSA space. Given an input sentence
?, a vector representation??wi for each term wi in ?
is made available. The corresponding representa-
tion of the sentence can be thus computed as the
27
linear combination through the original tf ? idf
scores of the corresponding ??wi: this provides al-
ways an unique representation??? for the sentence.
??? locates the sentence in the LSA space and the
set of terms that are semantically related to the
sentence ? can be easily found in the neighbor-
hood. A lower bound can be imposed on the co-
sine similarity scores over the vocabulary to com-
pute the lexical expansion of ?, i.e. the set of terms
that are enough similar to ??? in the k dimensional
space. Let D be the vocabulary of all terms, we
define as the lexical expansion T (?) ? D of??? as
follows:
T (?) = {wj ? D : sim(??wj ,??? ) > ?} (2)
where ? represents a real-valued threshold in the
set [0, 1). In order to improve precision it is also
possible to impose a limit on the cardinality of
T (?) and discard terms characterized by lower
similarity factors.
Let the t = |T (?)| be the number of terms in the
expansion, we extend the original set ? of terms in
the sentence, so that the new seed vocabulary is
? ? T (?) = {w1, w2, . . . , wn, wn+1, . . . , wn+t}.
The nodes in the graph G will be thus computed
as V ext? :=
?n+t
i=1 Senses(wi) and a new per-
sonalization vector ~pext will then replace ~p in Eq.
1: it will assign a probability mass to the words
w1, ..., wn+t proportional to their similarity to ??? ,
i.e.
pki =
sim(??wi,??? )
?n+t
j=1 sim(
??wj ,??? )
?i = 1, ..., n+ t (3)
whereas ki is the index of the node corresponding
to the word wi in the graph. Finally, the later steps
of the PPR methods remain unchanged, and the
PageRank works over the corresponding graph G
are carried out as described in Section 2.
4 Empirical Evaluation
The evaluation of the proposed model was focused
on two main aspects. First we want to measure
the impact of the topical expansion at sentence
level on the accuracy reachable by the personal-
ized PageRank PPR. This will be done also com-
paratively with the state of the art of unsupervised
systems over a consolidated benchmark, i.e. Se-
meval 2007. In Table 1 a comparison between
the official Semeval 2007 results for unsupervised
methods is reported. Table 1 shows also the re-
sults of the standard PPR methods over the Se-
meval 2007 dataset. Second, we want to analyze
the efficiency of the algorithm and its impact in a
sentence (i.e. PPR) or word oriented (i.e. w2w)
perspective. This will allow to asses its applicabil-
ity to realistic tasks, such as query processing or
document indexing.
Experimental Set-up In order to measure ac-
curacy, the Senseval 2007 coarse WSD dataset2
(Navigli et al, 2007) has been employed. It in-
cludes 245 sentences for a total number of 2,269
ambiguous words. In line with the results reported
in (Agirre and Soroa, 2009), experiments against
two different WordNet versions, 1.7 and 3.0, have
been carried out. Notice that the best results in
(Agirre and Soroa, 2009) were obtained over the
enriched version of the LKB, i.e. the combination
of WordNet and extra information supplied by ex-
tended WordNet (Harabagiu and Moldovan, 1999).
The adopted vector space has been acquired
over a significant subset of the BNC 2.0 corpus,
made of 923k sentences. The most frequent 200k
words (i.e. the contextual features) were acquired
through LSA. The corpus has been processed with
the LTH parser (Johansson and Nugues, 2007) to
obtain POS tags for every token. Moreover, a di-
mensionality reduction factor of k = 100 was ap-
plied.
In subsection 4.1, a comparative analysis of the
accuracy achieved in the disambiguation task is
discussed. Subsection 4.2 presents a correspond-
ing study of the execution times aiming to com-
pare the relative efficiency of the methods and
their application into a document semantic tagging
task.
4.1 Comparative evaluation: accuracy on the
Semeval ?07 data
The approaches proposed in Semeval 2007 can be
partitioned into two major types. The supervised
or semi-supervised approaches and the unsuper-
vised ones that rely usually on WordNet. As the
basic Page Rank as well as our LSA extension
makes no use of sense labeled data, we will mainly
focus on the comparative evaluation among unsu-
pervised WSD systems. In order to compare the
quality of the proposed approach, the results of the
personalized PageRank proposed in (Agirre and
Soroa, 2009) over the same dataset are reported in
Table 1 (The * systems, denoted by UKB). As also
suggested in (Agirre and Soroa, 2009) the best per-
2The dataset is publicly available from
http://nlp.cs.swarthmore.edu/semeval/tasks/task07/data.shtml
28
System P R F1
LSA UKB 1.7x 71.66 71.53 71.59
UKB 1.7x * 71.38 71.13 71.26
TKB-UO 70.21 70.21 70.21
UKB 3.0g * 68.47 68.05 68.26
LSA UKB 3.0g 67.02 66.73 66.87
LSA UKB 1.7 66.96 65.66 66.31
LSA UKB 3.0 66.60 65.31 65.95
RACAI-SYNWSD 65.71 65.71 65.71
UKB 3.0 * 63.29 61.92 62.60
SUSSX-FR 71.73 52.23 60.44
UKB 1.7 * 59.30 57.99 58.64
UOFL 52.59 48.74 50.60
SUSSX-C-WD 54.54 39.71 45.96
SUSSX-CR 54.30 39.53 45.75
Table 1: Official Results over the Semeval?07
dataset. The * systems was presented in (Agirre
and Soroa, 2009). The LSA UKB 1.7 and
LSA UKB 3.0 show the rank of the model pro-
posed in this paper.
formances are obtained according to the PPRw2w
word oriented approach.
For sake of comparison we applied the LSA-
based expansion to the Personalized Page Rank in
a sentence oriented fashion (i.e., only one PageR-
ank is run for all the target words of a sentence,
PPR). Notice that PPR models the context of
the sentence with a single iterative run of PageR-
ank, while PPRw2w disambiguates each word
with a dedicated PageRank. In line with (Agirre
and Soroa, 2009), different types of WordNet
graphs are employed in our experiments:
WN17 all hyponymy links between synsets of the
WN1.7 dictionary are considered;
WN17x all hyponymy links as well as the ex-
tended 1.7 version of WordNet, whereas the
syntactically parsed glosses, are semantically
disambiguated and connected to the corre-
sponding synsets;
WN3.0 all hyponymy links between synsets of
the WN3.0 dictionary are considered;
WN30g all hyponymy links as well as the ex-
tended 3.0 version of WordNet, whereas the
syntactically parsed glosses, are semantically
disambiguated and connected to the corre-
sponding synsets;
The impact of the LSA sentence expansion
technique proposed in this paper on the different
involved resources, i.e. WN1.7 to WN30g, has
been measured. The 1.7 configuration provides
PPR w2w
Model Iter. Prec Rec F1 Prec Rec F1
17 LSA100
5 65.8 64.5 65.2 65.7 64.4 65.1
15 65.6 64.3 65.0 66.3 65.0 65.7
17 UKB
5 60.9 59.7 60.3 65.3 63.8 64.5
15 61.3 60.1 60.7 61.6 60.2 60.9
17x LSA100
5 71.5 71.4 71.5 71.1 71.0 71.1
15 71.5 71.4 71.4 71.6 71.5 71.5
17x UKB
5 67.4 67.3 67.4 70.9 70.6 70.7
15 67.5 67.4 67.5 71.3 71.1 71.2
30 LSA100
5 66.5 65.2 65.8 65.7 64.4 65.1
15 66.9 65.6 66.2 66.6 65.3 65.9
30 UKB
5 61.7 60.5 61.1 64.7 63.3 64.0
15 63.5 62.2 62.8 63.2 61.9 62.6
30g LSA100
5 66.6 66.3 66.4 66.6 66.3 66.5
15 66.7 66.4 66.5 67.0 66.7 66.8
30g UKB
5 60.8 60.5 60.6 68.1 67.7 67.9
15 60.7 60.5 60.6 68.4 68.0 68.2
Table 2: Accuracy of the LSA-based sentence ex-
pansion PageRank model, as compared with the
sentence (PPR) and word oriented (w2w) ver-
sions of the personalized PageRank over the Se-
meval 2007 datasets. 17x and 30g refer to the ex-
tended resources of WordNet 1.7 and 3.0, respec-
tively.
the most efficient one as it runs the original PPR
against a graph built around the only hyponymy
relations among synsets. We used the Senseval?02
and Senseval?03 datasets to fine tune parameters
of our LSA model, that are: (1) the dimensional-
ity cut k to derive the LSA space; (2) the thresh-
old ? to determine the expansion dictionary in the
LSA space for every POS tag (e.g. noun or ad-
jectives), that may require different values; (3)
the damping factor ? and (4) the number of iter-
ation over the graph. In (Agirre and Soroa, 2009)
the suggested parameters are ? = 0.85 as the
damping factor and 30 as the upper limit to the
PageRank iterations. We always adopted this set-
ting to estimate the performances of the standard
PPR and PPRw2w algorithms referred through
UKB. Due the novel configuration of the graph
that in our model also includes many other simi-
lar terms, the damping factor and the number of
iterations have been re-estimated. k has been set
to 100 as different values did not seem to influ-
ence accuracy. We adopted fixed limits for sen-
tence expansion where values from 20 up to 150
terms have been tested. The good scores obtained
on the development set suggested that a number of
iterations lower than 30 is in general enough to get
good accuracy levels: 15 iterations, instead of 30,
have been judged adequate. Finally, on average,
the total number of lexical items in the expanded
sentence T (?) includes about 40% of nouns, 30%
of verbs, 20% of adjectives and 10% of adverbs.
29
Finally, a damping factor ? = 0.98 has been used.
Table 2 reports Precision, Recall and F1 scores
of the different models as obtained over the test
SemEval ?07 data. Every row pair compares
the LSA model with the original corresponding
UKB version over a given graph (from WN1.7
to WN30g). For each model the accuracy corre-
sponding to two iterations (5 and 15) is reported
to analyze also the overall trend during PageRank.
The best F1 scores between any pair are empha-
sized in bold, to comparatively asses the results.
As a confirmation of the outcome in (Agirre and
Soroa, 2009), different lexical resources achieve
different results. In general by adopting the graph
derived from WN3.0 (i.e. WN30 and WN30g)
lower performance can be achieved. Moreover,
the word-by-word model (last three columns for
the w2w side of the Table) is evidently superior.
Interestingly, almost on every type of graph and
for every approach (sentence or word oriented) the
LSA-based method outperforms the original UKB
PPR. This confirms that the impact of the topical
information provided by the LSA expansion of the
sentence is beneficial for a better use of the lexical
graph. An even more interesting outcome is that
the improvement implied by the proposed LSA
method on the sentence oriented model (i.e. the
standard PPR method of (Agirre and Soroa, 2009))
is higher, so that the difference between the per-
formances of the PPRw2w model are no longer
strikingly better than the PPR one. For exam-
ple, on the simple WN1.7 hyponymy network the
PPR ? LSA100 3 method abolishes the gap of
about 4% previously observed for the PPR-UKB
model. When LSA is used, it seems that the word-
by-word approach is no longer required. On the
contrary, in the WN17x case the best figure af-
ter 5 iterations is obtained by the PPR-LSA100
method instead of the w2w-LSA100 one (71.5%
vs. 71.1%). The good accuracy reachable by the
sentence oriented strategy (i.e. LSA100 and w2w)
is also very interesting as for the higher efficiency
of the PPR approach with respect to the word-by-
word PPRw2w one.
4.2 Time Efficiency
In the attempt to validate the hypothesis that LSA
is helpful to improve time complexity of the WSD,
we analyzed the processing times of the different
data sets, in order to cross compare methods and
3100 refers to the dimension k of the LSA space
resources4. The aim of the evaluation is to study
the contribution of the sentence expansion using
Latent Semantic Analysis and the Page Rank al-
gorithm. Tests were performed comparing dif-
ferent parameter values (e.g. cardinality t of the
sentence expansion, different values for the ac-
ceptability threshold) as well as several settings
of the damping factor for the personalized PageR-
ank algorithm (Eq 1) and the number of iterations
over the KB Graph. In figure 1, the processing
speed, measured as seconds per sentence, has been
plot for different graphs and configurations. No-
tice that one sentence is equivalent on average to
9,6 target words. As clearly shown in the figure,
the processing times for the word-by-word method
over the extended WN 1.7 (i.e. WN17x) are not
acceptable for IR tasks such as query processing,
or document indexing. For an entire document
of about 20 sentences the overall amount of pro-
cessing required by the w2w 17x UKB method is
about 45 minutes. Word-by-word methods are just
slightly more efficient whenever applied to graphs
with lower connectivity (e.g. WN17 vs. WN17x
as in Fig. 1 left plot). The same tasks with PPR
methods are solved in a quite faster way, with a
general ratio of 1:14 with the extended versions
and 1:6 with the hyponymy graphs. The process-
ing time of the proposed LSA method is thus at
least 6 times faster than the UKB method with the
comparable accuracy level. Moreover, as accu-
racy between PPR and w2w is comparable when
LSA is adopted, this efficiency can be guaranteed
at no loss in accuracy. By integrating the evi-
dence of Figure 1 with the ones of Table 1, we
observe that accuracy reachable by LSA-UKB is
independent by the standard or word-by-word con-
figuration so that the overall process can be made
about 10 times faster. Notice that the representa-
tion in the LSA space that is projected for a tar-
get sentence can be easily obtained also for longer
text fragments. Moreover, as for the one sense
per discourse hypothesis it is possible that every
word can be processed once in an entire text. This
suggests that a document oriented usage of the
personalized PageRank based on LSA can be de-
signed achieving the maximal efficiency. In or-
der to evaluate the corresponding impact on accu-
racy a dedicated dataset has been defined and more
tests have been run, as discussed hereafter.
4Tests were carried out on a 32-bit machine with a 3.2
Ghz CPU and 2 Gbyte Memory. Gnu/Linux operative system
is installed on it, with the kernel 2.6.28-16-generic.
30
Figure 1: Processing Times for the PPR, w2w and LSA methods as applied on the WN 1.7 (left plot)
and WN 3.0 (right plot) resources, respectively: 17x and 30g refer to test over the extended resources.
4.3 Document oriented PPR
While the LSA model has been actually applied
to determine an expansion for the entire target
sentence, nothing prevents to apply it to larger
text units, in order to bias the PageRank for all
words in a document. In order to verify if such a
process disambiguation could preserve the same
accuracy, we measured the accuracy reachable
over the same Semeval?07 data organized in doc-
uments. The sentences have been grouped in 5
documents, made of about about 250 sentences:
during the tagging process, the system generates
a lexical expansion for an entire document, about
450 target words on average. Then PageRank is
carried out and the resulting ranking is projected
to the senses of all the targeted words in the doc-
ument. Due to the much wider locality managed
in this process, a larger cardinality for the expan-
sion is used and the most similar 400 words are
collected as a bias for the PageRank. The accu-
racy reachable is reported in Table 4.3. As ex-
pected, the same trends as for the sentence based
approach are observed: the best resource is still
the WN17x for which the best results is obtained.
However, the crucial result here is that no drop in
performance is also observed. This implies that
the much more efficient document oriented strat-
egy can be always applied through LSA without
major changes in accuracy. Also results related to
the processing time follow the trends of the sen-
tence based method. Accordingly 28 seconds re-
quired to process a document in the worst case is
an impressive achievement because the same ac-
curacy was obtained, without LSA, in 2 orders of
magnitude more time.
5 Conclusions
In this paper an extension of a PageRank-based al-
gorithm for Word Sense Disambiguation has been
Model Iter. Prec Rec F1
PPR 17 LSA400
5 0.6670 0.6540 0.6604
15 0.6800 0.6668 0.6733
PPR 17 UKB
5 0.6440 0.6316 0.6377
15 0.6360 0.6236 0.6297
PPR 17x LSA400
5 0.7130 0.7118 0.7124
15 0.7152 0.7140 0.7146
PPR 17x UKB
5 0.7108 0.7096 0.7102
15 0.7073 0.7060 0.7067
PPR 30 LSA400
5 0.6593 0.6465 0.6529
15 0.6688 0.6558 0.6622
PPR 30 UKB
5 0.6445 0.6320 0.6382
15 0.6724 0.6593 0.6658
PPR 30g LSA400
5 0.6636 0.6606 0.6621
15 0.6653 0.6624 0.6639
PPR 30g UKB
5 0.6543 0.6514 0.6528
15 0.6565 0.6536 0.6550
Table 3: Accuracy of the LSA-based PPR model
when applied in a document oriented fashion on
the Semeval ?07 dataset. LSA400 stands for the
size t of the applied sentence expansion T (?).
presented. It suggests a kind of personalization
based on sentence expansion, obtained as a side
effect of Latent Semantic Analysis. The major re-
sults achieved are in terms of improved efficiency
that allows to use smaller resources or less iter-
ations with similar accuracy results. The result-
ing speed-up can be also improved when the dis-
ambiguation is run in a document oriented fash-
ion, and the PageRank is run once per each doc-
ument. The overall results can achieve a speed-
up of two order of magnitude at no cost in accu-
racy. Moreover the presented approach constitutes
the state-of-the-art among the unsupervised WSD
algorithms over the Semeval?07 datasets, while
improving the efficiency of the PPR method by
a factor 10 in the worst case. This work opens
perspectives towards more sophisticated distribu-
tional models (such as syntax-driven ones) as well
as cross-linguistic applications supported by mul-
tilingual lexical sense repositories.
31
References
E. Agirre and G. Rigau. 1996. Word sense disam-
biguation using conceptual density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Eneko Agirre and Aitor Soroa. 2008. Using
the multilingual central repository for graph-based
word sense disambiguation. In Proceedings of the
LREC?08, Marrakech, Morocco, May.
E. Agirre and A. Soroa. 2009. Personalizing pagerank
for word sense disambiguation. In Proceedings of
the 12th conference of EACL ?09, Athens, Greece,
March 30 - April 3.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004.
A semantic similarity measure for unsupervised se-
mantic disambiguation. In Proceedings of LREC-
04, Lisbon, Portugal.
Stephen Beale, Benoit Lavoie, Marjorie McShane,
Sergei Nirenburg, and Tanya Korelsky. 2004. Ques-
tion answering using ontological semantics. In
TextMean ?04: Proceedings of the 2nd Workshop on
Text Meaning and Interpretation, pages 41?48, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems,
30(1?7):107?117.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the Joint Conference
EMNLP-CoNLL ?09, Prague, Czech Republic.
Y. Chan, H. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the ACL ?09, Prague, Czech
Republic.
Jim Cowie, Louise Guthrie, and Joe Guthrie. 1992.
Lexical disambiguation using simulated annealing.
In Proc. of 14th Int. Conf. COLING ?92, pages 359?
365, Nantes, France.
Sanda M. Harabagiu and Dan I. Moldovan. 1999.
Enriching the wordnet taxonomy with contextual
knowledge acquired from text. In in Iwanska, L.M.,
and Shapiro, S.C. eds 2000. Natural Language Pro-
cessing and Knowledge Representation: Language,
pages 301?334. AAAI/MIT Press.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proc. of 11th Int. Conf. on World Wide Web, page
517526, New York, USA. ACM.
Richard Johansson and Pierre Nugues. 2007. Se-
mantic structure extraction using nonprojective de-
pendency trees. In Proceedings of SemEval-2007,
Prague, Czech Republic, June 23-24.
S. B. Kim, H. Seo, and H. Rim. 2004. Information
retrieval using word senses: root sense tagging ap-
proach. In Proceedings of the International ACM-
SIGIR Conference ?09, Sheffield, UK, July.
H. Krovetz. 1997. Homonymy and polysemy in in-
formation retrieval. In Proceedings of the 35th ACL
?09.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104:211?240.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference
on Systems documentation, New York, NY, USA.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. An on-line lexical database. Inter-
national Journal of Lexicography, 13(4):235?312.
Alessandro Moschitti and Roberto Basili. 2004. Com-
plex linguistic features for text classification: A
comprehensive study. In Proc. of the European
Conf. on IR, ECIR, pages 181?196, New York, USA.
Roberto Navigli and Mirella Lapata. 2007. Graph
connectivity measures for unsupervised word sense
disambiguation. In Proceedings of IJCAI?07, pages
1683?1688, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: coarse-
grained english all-words task. In SemEval ?07,
pages 30?35, Morristown, NJ, USA. Association for
Computational Linguistics.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H.T.
Dang. 2001. English tasks: All-words and verb
lexical sample. In Proceedings of SENSEVAL-2,
Tolouse, France, July.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007. Semeval-2007 task-17: English lexical sam-
ple srl and all words. In Proceedings of SemEval-
2007, Prague, Czech Republic, June.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In IEEE ICSC
2007.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proceeding of ACL 2004 Senseval-3 Work-
shop, Barcelona, Spain, July.
32
