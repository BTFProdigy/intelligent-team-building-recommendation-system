Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 177?180,
Sydney, July 2006. c?2006 Association for Computational Linguistics
POC-NLW Template for Chinese Word Segmentation 
 
 
Bo Chen 
chb615@gmail.com 
Weiran Xu 
xuweiran@263.net 
Tao Peng 
ppttbupt@gmail.com 
Jun Guo 
guojun@bupt.edu.cn 
Pattern Recognition and Intelligent System Lab 
Beijing University of Posts and Telecommunications 
Beijing 100876, P. R. China 
 
  
 
Abstract 
In this paper, a language tagging tem-
plate named POC-NLW (position of a 
character within an n-length word) is pre-
sented. Based on this template, a two-
stage statistical model for Chinese word 
segmentation is constructed. In this 
method, the basic word segmentation is 
based on n-gram language model, and a 
Hidden Markov tagger based on the 
POC-NLW template is used to imple-
ment the out-of-vocabulary (OOV) word 
identification. The system participated in 
the MSRA_Close and UPUC_Close 
word segmentation tracks at SIGHAN 
Bakeoff 2006. Results returned by this 
bakeoff are reported here. 
1 Introduction 
In Chinese word segmentation, there are two 
problems still remain, one is the resolution of 
ambiguity, and the other is the identification of 
so-called out-of-vocabulary (OOV) or unknown 
words. In order to resolve these two problems, a 
two-stage statistical word segmentation strategy 
is adopted in our system. The first stage is op-
tional, and the whole segmentation can be ac-
complished in the second stage. In the first stage, 
the n-gram language model is employed to im-
plement basic word segmentation including dis-
ambiguation. In the second stage, a language 
tagging template named POC-NLW (position of 
a character within an n-length word) is intro-
duced to accomplish unknown word identifica-
tion as template-based character tagging. 
The remainder of this paper is organized as 
follows. In section 2 and section 3, a briefly de-
scription of the main methods adopted in our 
system is given. Results of our system at this 
bakeoff are reported in section 4. At last, conclu-
sions are derived in section 5. 
2 The Basic Word Segmentation Stage 
In the first stage, the basic word segmentation is 
accomplished. The key issue in this stage is the 
ambiguity problem, which is mainly caused by 
the fact that a Chinese character can occur in dif-
ferent word internal positions in different words 
(Xue, 2003). A lot of machine learning tech-
niques have been applied to resolve this problem, 
the n-gram language model is one of the most 
popular ones among them (Fu and Luke, 2003; 
Li et al, 2005). As such, we also employed n-
gram model in this stage.  
When a sentence is inputted, it is first seg-
mented into a sequence of individual characters 
(e.g. ASCII strings, basic Chinese characters, 
punitions, numerals and so on), marked as C1,n. 
According to the system?s dictionary, several 
word sequences W1,m will be constructed as can-
didates. The function of the n-gram model is to 
find out the best word sequence W* corresponds 
to C1,n, which has the maximum integrated prob-
ability, i.e., 
trigramforWWWP
bigramforWWP
CWPW
m
i
iii
W
m
i
ii
W
nm
W
m
m
m
?
?
=
??
=
?
?
?
=
1
21
1
1
,1,1
*
),|(maxarg
)|(maxarg
)|(maxarg
,1
,1
,1
 
177
The Maximum Likelihood method was used to 
estimate the word n-gram probabilities used in 
our model, and the linear interpolation method 
(Jelinek and Mercer, 1980) was applied to 
smooth these estimated probabilities. 
3 The OOV Word Identification Stage 
The n-gram method is based on the exiting grams 
in the model, so it is good at judging the connect-
ing relationship among known words, but does 
not have the ability to deal with unknown words 
in substance. Therefore, another OOV word 
identification model is required.  
OOV words are regarded as words that do not 
exist in a system?s machine-readable dictionary, 
and a more detailed definition can be found in 
(Wu and Jiang, 2000). In general, Chinese word 
can be created through compounding or abbrevi-
ating of most of existing characters and words. 
Thus, the key to solve the OOV word identifica-
tion lies on whether the new word creation 
mechanisms in Chinese language can be ex-
tracted. Therefore, a POC-NLW language tag-
ging template is introduced to explore such in-
formation on the character-level within words. 
3.1 The POC-NLW Template 
Many character-level based works have been 
done for the Chinese word segmentation, includ-
ing the LMR tagging methods (Xue, 2003; Na-
kagawa. 2004), the IWP mechanism (Wu and 
Jiang, 2000). Based on these previous works, this 
POC-NLW template was derived. Assume that 
the length of a word is the number of component 
characters in it, the template is consist of two 
component: Lmax and a Wl-Pn tag set. Lmax to de-
note the maximum length of a word expressed by 
the template; a Wl-Pn tag denotes that this tag is 
assigned to a character at the n-th position within 
a l-length word, . Apparently, the 
size of this tag set is  
ln ,,2,1 L=
2/)1( maxmax LL ?+
For example, the Chinese word ??? ? is 
tagged as: 
 ? W2P1, ? W2P2 
and ????? is tagged as: 
 ? W3P1, ? W3P2, ? W3P3 
In the example, two words are tagged by the 
template respectively, and the Chinese character 
??? has been assigned two different tags.  
In a sense, the Chinese word creation mecha-
nisms could be extracted through statistics of the 
tags for each character on a certain large corpus.  
On the other hand, while a character sequence 
in a sentence is tagged by this template, the word 
boundaries are obvious. Meanwhile, the word 
segmentation is implemented.  
In addition, in this template, known words and 
unknown words are both regarded as sequences 
of individual characters. Thus, the basic word 
segmentation process, the disambiguation proc-
ess and the OOV word identification process can 
be accomplished in a unified process. Thereby, 
this model can also be used alone to implement 
the word segmentation task. This characteristic 
will make the word segmentation system much 
more efficient. 
3.2 The HMM Tagger 
Form the description of POC-NLW template, it 
can be found that the word segmentation could 
be implemented as POC-NLW tagging, which is 
similar to the so-called part-of-speech (POS) 
tagging problem. In POS tagging, Hidden 
Markov Model (HMM) was applied as one of the 
most significant methods, as described in detail 
in (Brants, 2000). The HMM method can achieve 
high accuracy in tagging with low processing 
costs, so it was adopted in our model. 
According to the definition of POC-NLW 
template, the state set of HMM corresponds to 
the Wl-Pn tag set, and the symbol set is com-
posed of all characters. However, the initial state 
probability matrix and the state transition prob-
ability matrix are not composed of all of the tags 
in the state set. To express more clearly, we de-
fine two subset of the state set: 
? Begin Tag Set (BTS): this set is con-
sisted of tag which can occur in the beg-
ging position in a word. Apparently, these 
tags must have the Wl-P1 form.  
? End Tag Set (ETS): correspond to BTS, 
tags in this set should occur in the end po-
sition, and their form should be like Wl-Pl. 
Apparently, the size of BTS is Lmax as well as 
of ETS. Thus, the initial state probability matrix 
corresponds to BTS instead of the whole state set. 
On the other hand, because of the word internal 
continuity, if the current tag Wl-Pn is not in ETS, 
than the next tag must be Wl-P(n+1). In other 
words, the case in which the transition probabil-
ity is need is that when the current tag is in ETS 
and the next tag belongs to BTS. So, the state 
transition matrix in our model corresponds to 
BTSETS ? . 
178
The probabilities used in HMM were defined 
similarly to those in POS tagging, and were es-
timated using the Maximum Likelihood method 
from the training corpus. 
In the two-stage strategy, the output word se-
quence of the first stage is transferred into the 
second stage. The items in the sequence, includ-
ing individual characters and words, which do 
not have a bigram or trigram relationship with 
the surrounding items, are picked out with its 
surrounding items to compose several sequences 
of items. These item sequences are processed by 
the HMM tagger to form new item sequences. At 
last, these processed items sequences are com-
bined into the whole word sequence as the final 
output. 
4 Results and Analysis 
4.1 System 
The system submitted at this bakeoff was a two-
stage one, as describe at beginning of this paper. 
The model used in the first stage was trigram, 
and the Lmax of the template used in the second 
stage was set to 7. 
In addition to the tags defined in the template 
before, a special tag is introduced into our Wl-Pn 
tag set to indicate all those characters that occur 
after the Lmax-th position in an extremely long 
(longer than Lmax) word., formulized as WLmax-
P(Lmax+1). And then, there are 28 basic tags 
(from W1-P1 to W7-P7) and the special one W7-
P8.  
For instance, using the special tag, the word 
???????????? (form the MSRA 
Corpus ) is tagged as: 
? W7-P1  ? W7-P2  ? W7-P3  ? W7-P4 
? W7-P5  ? W7-P6  ? W7-P7  ? W7-P8 
? W7-P8  ? W7-P8 
4.2 Results at SIGHAN Bakeoff 2006 
Our system participated in the MSRA_Close and 
UPUC_Close track at the SIGHAN Bakeoff 
2006. The test results are as showed in Table 1. 
 
Corpus MSRA UPUC 
F-measure 0.951 0.918 
Recall 0.956 0.932 
Precision 0.947 0.904 
IV Recall 0.972 0.969 
OOV Recall 0.493 0.546 
OOV Precision 0.569 0.757 
Table 1. Results at SIGHAN Bakeoff 2006 
 
The performances of our system on the two 
corpuses can rank in the half-top group among 
the participated systems.  
We notice that the accuracies on known word 
segmentation are relatively better than on OOV 
words segmentation. This appears somewhat un-
expected. In the close experiments we had done 
on the PKU and MSR corpuses of SIGHAN 
Bakeoff 2005, the relative performance of OOV 
Recall was much more outstanding than of the F-
measure. 
We think this is due to the inappropriate pa-
rameters used in n-gram model, which over-
guarantees the performance of basic word seg-
mentation. It can be seen on the IV Recall (high-
est in UPUC_Close track). For only the best out-
put sequence of the n-gram model is transferred 
to the HMM tagger, some potential unknown 
words may be miss-split in the early stage. Thus, 
the OOV Recall is not very good, and this also 
affects the overall performance. 
On the other hand, the performances of OOV 
identification on UPUC are much better than on 
MSRA, while the performances of overall seg-
mentation accuracy on UPUC are worse than on 
MSRA. This phenomenon also happened in our 
experiments on the Bakeoff 2005 corpuses of 
PKU and MSR. In the PKU test data, the rate of 
OOV words according is 0.058 while in MSR is 
0.026. Thus, it can be conclude that the more 
unknown words occur, the more significant abil-
ity of OOV words identification appears.  
In addition, the relative performance of OOV 
Precision are much better. This demonstrates that 
the OOV identification ability of our system is 
appreciable. In other words, the POC-NLW tag-
ging method introduced is effective to some ex-
tent. 
5 CONCLUSION AND FURTHER 
WORK 
In this paper, a POC-NLW template is presented 
for word segmentation, which aims at exploring 
the word creation mechanisms in Chinese lan-
guage by utilizing the character-level informa-
tion to. A two-stage strategy was applied in our 
system to combine the n-gram model based word 
segmentation and OOV word identification im-
plemented by a HMM tagger. Test results show 
that the method achieved high performance on 
word segmentation, especially on unknown 
words identification. Therefore, the method is a 
practical one that can be implemented as an inte-
179
gral component in actual Chinese NLP applica-
tions.  
From the results, it can safely conclude that 
method introduced here does find some charac-
ter-level information, and the information could 
effectively conduct the word segmentation and 
unknown words identification. For this is the first 
time we participate in this bakeoff, and the work 
has been done as a integral part of another sys-
tem during the past two months, the implementa-
tion of the segmentation system we submitted is 
coarse. A lot of improvements, on either theo-
retical methods or implementation techniques, 
are required in our future work, including the 
smoothing techniques in the n-gram model and 
the HMM model, the refine of the features ex-
traction method and the POC-NLW template it-
self, the more harmonious integration strategy 
and so on.  
Acknowledgements 
This work is partially supported by NSFC 
(National Natural Science Foundation of China) 
under Grant No.60475007, Key Project of Chi-
nese Ministry of Education under Grant 
No.02029 and the Foundation of Chinese Minis-
try of Education for Century Spanning Talent. 
References 
Andi Wu, and Zixin Jiang. 2000. Statistically-
enhanced new word identification in a rule-based 
Chinese system. Proceedings of the 2nd Chinese 
Language Processing Workshop, 46-51. 
Frederick Jelinek, and Robert L. Mercer. 1980. Inter-
polated Estimation of Markov Source Parameters 
from Sparse Data. Proceedings of Workshop on 
Pattern Recognition in Practice, Amsterdam, 381-
397. 
Guohong Fu, and Kang-Kwong Luke. 2003. A Two-
stage Statistical Word Segmentation System for 
Chinese. Proceedings of the Second SIGHAN 
Workshop on Chinese Language Processing, 156-
159. 
Heng Li, Yuan Dong, Xinnian Mao, Haila Wang, and 
Wu Liu. 2005. Chinese Word Segmentation in 
FTRD Beijing. Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, 150-
153. 
Nianwen Xue. 2003. Chinese Word Segmentation as 
Character Tagging. International Journal of Com-
putational Linguistics and Chinese Language Pro-
cession, 8(1):29?48. 
Tetsuji Nakagawa. 2004. Chinese and Japanese Word 
Segmentation Using Word-Level and Character-
Level Information. Proceedings of the 20th Inter-
national Conference on Computational Linguistics, 
466?472. 
Thorsten Brants. 2000. TnT ? A Statistical Part-of-
Speech Tagger. Proceedings of the Sixth Confer-
ence on Applied Natural Language Processing 
ANLP-2000, 224?231. 
180
PRIS at Chinese Language Processing  
--Chinese Personal Name Disambiguation 
 
Jiayue Zhang, Yichao Cai, Si Li, Weiran Xu, Jun Guo  
School of Information and Communication Engineering 
Beijing Universit of Posts and Telecommunications 
jyz0706@gmail.com 
 
 
Abstract 
The more Chinese language materials come 
out, the more we have to focus on the ?same 
personal name? problem. In our personal 
name disambiguation system, the hierarchical 
agglomerative clustering is applied, and 
named entity is used as feature for document 
similarity calculation. We propose a two-stage 
strategy in which the first stage involves word 
segmentation and named entity recognition 
(NER) for feature extraction, and the second 
stage focuses on clustering.  
1 Introduction 
World Wide Web (WWW) search engines have 
become widely used in recent years to retrieve 
information about real-world entities such as 
people. Web person search is one of the most 
frequent search types on the web search engine. 
As the sheer amount of web information ex-
pands at an ever more rapid pace, the named-
entity ambiguity problem becomes more and 
more serious in many fields, such as information 
integration, cross-document co-reference, and 
question answering. It is crucial to develop me-
thodologies that can efficiently disambiguate the 
ambiguous names form any given set of data. 
There have been two recent Web People Search 
(WePS) evaluation campaigns [1] on personal 
name disambiguation using data from English 
language web pages. Previous researches on 
name disambiguation mainly employ clustering 
algorithms which disambiguates ambiguous 
names in a given document collection through 
clustering them into different reference entities. 
However, Chinese personal name disambigua-
tion is potentially more challenging due to the 
need for word segmentation, which could intro-
duce errors that can in large part be avoided in 
the English task. 
There are four tasks in Chinese Language 
Processing of the CIPS-SIGHAN Joint Confe-
rence, and we participate in the Chinese Personal 
Name Disambiguation task. To accomplish this 
task, we focused on solving two main problems 
which are word segmentation and duplicate 
names distinguishment. To distinguish duplicate 
names, the system adopts named entity recogni-
tion and clustering strategy. For word segmenta-
tion and NER, we applied a sharing platform 
named LTP designed by Harbin Institute of 
Technology [2].This tagger identifies and labels 
names of locations, organizations, people, time, 
date, numbers and proper nouns in the input text.  
The paper is organized as follows. Section 2 in-
troduces our feature extractions along with their 
corresponding similarity matrix learning. In Sec-
tion 3, we analyze the performance of our sys-
tem. Finally, we draw some conclusions.  
2 Methodology  
Our approach follows a common architecture for 
named-entity disambiguation: the detection of 
ambiguous objects, feature extractions and their 
corresponding similarity matrix learning, and 
clustering. The framework of overall processing 
is shown in Figure 1.  
 
Word Segmentation
Named Entity Recognition
 Building VSM 
HAC
Submitted Results
Corpus
LTP
TF-IDF
K-L 
divergence
weight
similarity
 
 
Figure 1. System Framework 
 
2.1 The detection of ambiguous objects 
 
Since it is common for a single document to 
contain one or more mentions of the ambiguous 
personal name, that is to say, the personal name 
may appear several times in one document, there 
is a need to define the object to be disambi-
guated. Here, we adopt the policy of ?one person 
per document? (all mentions of the ambiguous 
personal name in one document are assumed to 
refer to the same personal entity in reality) as in 
[3] [4] [5]. Therefore, an object is defined as a 
single entity with the ambiguous personal name 
in a given document. This definition of the ob-
ject (document-level object) might be not com-
prehensive, because the mentions of the ambi-
guous personal name in a document may refer to 
multiple entities, but we found that this is a rare 
case (most of those cases occur in genealogy 
web pages). On the other hand, the document-
level object can include much information de-
rived from that document, so that it can be 
represented by features [6]. 
For a given ambiguous personal name, word 
segmentation is applied first. Then we try to ex-
tract all mentions of the ambiguous personal 
name. Take the given personal name ???? for 
example, first, the exact match of the name is 
extracted. Secondly, mentions that are super-
strings of the given name like ?????is also 
extracted . Finally , mentions that contain cha-
racter sequences but not a personal name like 
???????? is ignored.  
Given this definition of an object, we define a 
target entity as an entity that includes a mention 
of the ambiguous personal name. 
 
2.2 Feature extraction and similarity ma-
trix learning 
 
Most of the previous work ([3] [4] [5]) used to-
ken information in the given documents. In this 
paper, we follow and extend their work especial-
ly for a web corpus. Furthermore, compared to a 
token, a phrase contains more information for 
named-entity disambiguation. Therefore, we ex-
plore both token and phrase-based information 
in this paper. Finally, there are two kinds of fea-
ture vectors developed in our system, token-
based and phrase-based. The token-based feature 
vector is composed of tokens, and the phrase-
based feature is composed of phrases. The two 
feature vectors are combined into a unified fea-
ture vector in which tf-idf strategy is used for 
similarity calculation.  
 
2.2.1 Named Entity Features 
 
From the results and papers of various teams 
participating WePS, NEs have been shown to be 
effective features in person name disambigua-
tion, so we used NEs as features in this study. 
Through observation, we found that two differ-
ent individuals can be identified by their corres-
ponding NEs, especially by location, organiza-
tion name and some proper nouns. Hence, in our 
study, we only extracted person, location, organ-
ization name and proper noun as feature from 
the output of LTP, while time, date and numbers 
are discarded. However, location and organiza-
tion name have many proper nouns related 
weakly to a certain person. Therefore, terms 
having high-document-frequency in training data 
sets are removed from test data. 
 
2.2.2 Similarity matrix learning 
 
After NE extraction, we applied the vector space 
model to the calculation of similarities between 
features. In the model, tf-idf is used as the 
weight of the feature, which is defined in Eq. (1).  
iij
ij
ij n
N
MaxFreq
freqwIDFTF log)(: ???
     (1) 
Here, wij is the weight of term (or phrase) ti in 
document dj, freqij is the frequency of ti in dj, 
MaxFreqij is the frequency of the term (or phrase) 
whose frequency is the most in dj, N is the num-
ber of documents under one given name, and ni 
is the number of documents which has term (or 
phrase) ti. 
In this study, the similarities based 
on features described above were calculated us-
ing K-L divergence defined as Eq. (2). 
??
i
KL iQ
iPiPQPD )(
)(log)()||(
        (2) 
P and Q denote the vector of a document respec-
tively. K-L divergence between two vectors 
shows the distance of two related documents. 
The smaller the value of K-L divergence of two 
vectors becomes, the closer the two documents 
are. In order to prevent the zero denominator, we 
applied Dirichlet smoothing, i.e. , the zero ele-
ment in the vector will be replaced by 0.00001.  
 
2.3 Clustering 
 
Clustering is the key part for our personal name 
disambiguation system. This task is viewed as an 
unsupervised hard clustering problem. First, we 
view the problem as unsupervised, using the dis-
tributed training data for parameter validation, to 
optimally tune the parameters in the clustering 
algorithm. Secondly, we observed that the ma-
jority of the input documents reference a single 
individual. Hence, we view the problem as hard 
clustering, assigning input document to exactly 
one individual, so that the produced clusters do 
not overlap. 
In our system, hierarchical agglomerative clus-
tering (HAC) is used as a clustering method. It 
builds up a hierarchy of groups by continuously 
merging the two most similar groups. Each of 
these groups starts as a single item, in this case 
an individual document. In each iteration this 
method calculates the distances between every 
pair of groups, and the closest ones are merged 
together to form a new group. The vector of the 
new group is the average of the original pair.  
This is repeated until there is only one group. 
This process is shown in Fig. 2.  
We used a threshold for selecting cluster. So it is 
not necessary to determine the number of clus-
ters beforehand. W  view the whole group as a 
binary tree, every node which is not a leaf has 
two children, left child and right child, and has a 
record of the distance between the two children. 
We traverse the tree from the root, if the distance 
between the pair of children which form the 
cluster is larger than the threshold, then move 
down to check the distance of its left child, then 
right child. The process will continue until the 
distance between two children is less than the 
threshold. When the process comes to an end, all 
the leaves under the node will be considered to 
be in the same cluster. The selecting process will 
continue until all the leaves are assigned to a 
cluster. The threshold is tuned using the distr i-
buted training data. 
The whole process mainly consists of two 
phases, the first phase is clustering all the single 
items into one group, and the second is selecting 
cluster down along the tree from the root. This 
strategy has a major disadvantage which is the 
new node is the average of its children. Hence, 
with the merger of nodes going on, the distance 
between different groups becomes smaller and 
smaller, which makes the boundaries between 
different clusters blur. This is probably the main 
reason that leads to the unsatisfactory results. 
 
 
Figure 2 visualization of hierarchical clustering  
3 Performance 
Since there is no correct answer of test data 
received, we present the performance of our sys-
tem of training data. There are two results gotten 
from the distributed evaluation in Table 1: one is 
evaluated with B-Cubed, and the other with P_IP. 
Both scores indicate that personal name disam-
biguation needs more effort. 
 
Table 1 The performance of training data 
 prici-
sion 
recall F_sco
re 
B-
Cubed 
71.83 62.88 56.98 
 purity In-
verse 
purity 
F_sco
re 
P_IP 76.43 67.71 62.76 
 
4 Conclusion 
In this report, we describe a system for the Chi-
nese Personal Name Disambiguation task, apply-
ing a two-stage clustering model. Because this is 
our first time attending this kind of task, there 
are many aspects not having been taken into ac-
count. Therefore, improving system performance 
becomes motivation for us to work on it conti-
nuously. In future work, we?ll focus on improv-
ing the clustering algorithm and proper feature 
extraction. 
  
References 
J. Artiles, J. Gonzalo and S. Sekine. WePS 2 Evalua-
tion Campaign: overview of the Web People 
Search Clustering Task. In 2nd Web People Search  
Evaluation Workshop (WePS 2009). In18th  
WWW Conference, 2009.  
http://ir.hit.edu.cn/ 
A. Bagga and B. Baldwin. 1998. Ent ity?based Cross?
document Co?referencing Using the Vector Space 
Model. In 17th COLING. 
C. H. Gooi and J. Allan. 2004.  Cross -Document Co-
reference on a Large Scale Corpus.NAACL 
T. Pedersen, A. Purandare and A. Kulkarn i. 2005. 
Name Discrimination by Clustering Similar Con-
texts. In  Proc. of the Sixth International Confe-
rence on Intelligent Text  Processing and Computa-
tional Linguistics, page 226-237. Mexico City, 
Mexico. 
Y. Chen and J. H. Martin. CU-COMSEM: Exploring  
Rich Features for Unsupervised Web Personal 
Name Disambiguation. In WWW Conference, 
2007. 
M. Ikeda, S. Ono, I. Sato, M. Yoshida and H. Naka-
gawa. Person Name Disambiguation on the Web 
by TwoStage Clustering. In 18th WWW Confe-
rence, 2009. 
E. Elmacioglu, Y. F. Tan, S.Yan, M. Y. Kan and D. 
W. Lee. Web People Name Disambiguation by 
Simple Clustering with Rich Features. In WWW 
Conference, 2007. 
 
