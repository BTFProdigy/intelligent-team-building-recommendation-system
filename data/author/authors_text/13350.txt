Proceedings of the 12th Conference of the European Chapter of the ACL, pages 291?299,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Mildly Non-projective Dependency Structures?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
David Weir and John Carroll
Department of Informatics
University of Sussex, United Kingdom
{davidw,johnca}@sussex.ac.uk
Abstract
We present parsing algorithms for vari-
ous mildly non-projective dependency for-
malisms. In particular, algorithms are pre-
sented for: all well-nested structures of
gap degree at most 1, with the same com-
plexity as the best existing parsers for con-
stituency formalisms of equivalent genera-
tive power; all well-nested structures with
gap degree bounded by any constant k;
and a new class of structures with gap de-
gree up to k that includes some ill-nested
structures. The third case includes all the
gap degree k structures in a number of de-
pendency treebanks.
1 Introduction
Dependency parsers analyse a sentence in terms
of a set of directed links (dependencies) express-
ing the head-modifier and head-complement rela-
tionships which form the basis of predicate argu-
ment structure. We take dependency structures to
be directed trees, where each node corresponds to
a word and the root of the tree marks the syn-
tactic head of the sentence. For reasons of effi-
ciency, many practical implementations of depen-
dency parsing are restricted to projective struc-
tures, in which the subtree rooted at each word
covers a contiguous substring of the sentence.
However, while free word order languages such
as Czech do not satisfy this constraint, parsing
without the projectivity constraint is computation-
ally complex. Although it is possible to parse
non-projective structures in quadratic time under a
model in which each dependency decision is inde-
pendent of all the others (McDonald et al, 2005),
?Partially supported by MEC and FEDER (HUM2007-
66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR,
INCITE08E1R104022ES, INCITE08ENA305025ES, IN-
CITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe
e RI, Bolsas para Estad??as INCITE ? FSE cofinanced).
the problem is intractable in the absence of this as-
sumption (McDonald and Satta, 2007).
Nivre and Nilsson (2005) observe that most
non-projective dependency structures appearing
in practice are ?close? to being projective, since
they contain only a small proportion of non-
projective arcs. This has led to the study of
classes of dependency structures that lie be-
tween projective and unrestricted non-projective
structures (Kuhlmann and Nivre, 2006; Havelka,
2007). Kuhlmann (2007) investigates several such
classes, based on well-nestedness and gap degree
constraints (Bodirsky et al, 2005), relating them
to lexicalised constituency grammar formalisms.
Specifically, he shows that: linear context-free
rewriting systems (LCFRS) with fan-out k (Vijay-
Shanker et al, 1987; Satta, 1992) induce the set
of dependency structures with gap degree at most
k ? 1; coupled context-free grammars in which
the maximal rank of a nonterminal is k (Hotz and
Pitsch, 1996) induce the set of well-nested depen-
dency structures with gap degree at most k ? 1;
and LTAGs (Joshi and Schabes, 1997) induce the
set of well-nested dependency structures with gap
degree at most 1.
These results establish that there must be
polynomial-time dependency parsing algorithms
for well-nested structures with bounded gap de-
gree, since such parsers exist for their correspond-
ing lexicalised constituency-based formalisms.
However, since most of the non-projective struc-
tures in treebanks are well-nested and have a small
gap degree (Kuhlmann and Nivre, 2006), devel-
oping efficient dependency parsing strategies for
these sets of structures has considerable practical
interest, since we would be able to parse directly
with dependencies in a data-driven manner, rather
than indirectly by constructing intermediate con-
stituency grammars and extracting dependencies
from constituency parses.
We address this problem with the following
contributions: (1) we define a parsing algorithm
291
for well-nested dependency structures of gap de-
gree 1, and prove its correctness. The parser runs
in time O(n7), the same complexity as the best
existing algorithms for LTAG (Eisner and Satta,
2000), and can be optimised to O(n6) in the non-
lexicalised case; (2) we generalise the previous al-
gorithm to any well-nested dependency structure
with gap degree at most k in time O(n5+2k); (3)
we generalise the previous parsers to be able to
analyse not only well-nested structures, but also
ill-nested structures with gap degree at most k sat-
isfying certain constraints1, in time O(n4+3k); and
(4) we characterise the set of structures covered by
this parser, which we call mildly ill-nested struc-
tures, and show that it includes all the trees present
in a number of dependency treebanks.
2 Preliminaries
A dependency graph for a string w1 . . . wn is a
graph G = (V,E), where V = {w1, . . . , wn}
and E ? V ? V . We write the edge (wi, wj)
as wi ? wj , meaning that the word wi is a syn-
tactic dependent (or a child) of wj or, conversely,
that wj is the governor (parent) of wi. We write
wi ?? wj to denote that there exists a (possi-
bly empty) path from wi to wj . The projection
of a node wi, denoted bwic, is the set of reflexive-
transitive dependents of wi, that is: bwic = {wj ?
V | wj ?? wi}. An interval (with endpoints i and
j) is a set of the form [i, j] = {wk | i ? k ? j}.
A dependency graph is said to be a tree if it is:
(1) acyclic: wj ? bwic implies wi ? wj 6? E; and
(2) each node has exactly one parent, except for
one node which we call the root or head. A graph
verifying these conditions and having a vertex set
V ? {w1, . . . , wn} is a partial dependency tree.
Given a dependency tree T = (V,E) and a node
u ? V , the subtree induced by the node u is the
graph Tu = (buc, Eu) where Eu = {wi ? wj ?
E | wj ? buc}.
2.1 Properties of dependency trees
We now define the concepts of gap degree and
well-nestedness (Kuhlmann and Nivre, 2006). Let
T be a (possibly partial) dependency tree for
w1 . . . wn: We say that T is projective if bwic is
an interval for every word wi. Thus every node
in the dependency structure must dominate a con-
tiguous substring in the sentence. The gap degree
1Parsing unrestricted ill-nested structures, even when the
gap degree is bounded, is NP-complete: these structures are
equivalent to LCFRS for which the recognition problem is
NP-complete (Satta, 1992).
of a particular node wk in T is the minimum g ? N
such that bwkc can be written as the union of g+1
intervals; that is, the number of discontinuities in
bwkc. The gap degree of the dependency tree T is
the maximum among the gap degrees of its nodes.
Note that T has gap degree 0 if and only if T is
projective. The subtrees induced by nodes wp and
wq are interleaved if bwpc ? bwqc = ? and there
are nodes wi, wj ? bwpc and wk, wl ? bwqc such
that i < k < j < l. A dependency tree T is
well-nested if it does not contain two interleaved
subtrees. A tree that is not well-nested is said to
be ill-nested. Note that projective trees are always
well-nested, but well-nested trees are not always
projective.
2.2 Dependency parsing schemata
The framework of parsing schemata (Sikkel,
1997) provides a uniform way to describe, anal-
yse and compare parsing algorithms. Parsing
schemata were initially defined for constituency-
based grammatical formalisms, but Go?mez-
Rodr??guez et al (2008a) define a variant of the
framework for dependency-based parsers. We
use these dependency parsing schemata to de-
fine parsers and prove their correctness. Due to
space constraints, we only provide brief outlines
of the main concepts behind dependency parsing
schemata.
The parsing schema approach considers pars-
ing as deduction, generating intermediate results
called items. An initial set of items is obtained
from the input sentence, and the parsing process
involves deduction steps which produce new items
from existing ones. Each item contains informa-
tion about the sentence?s structure, and a success-
ful parsing process produces at least one final item
providing a full dependency analysis for the sen-
tence or guaranteeing its existence. In a depen-
dency parsing schema, items are defined as sets of
partial dependency trees2. To define a parser by
means of a schema, we must define an item set
and provide a set of deduction steps that operate
on it. Given an item set I, the set of final items
for strings of length n is the set of items in I that
contain a full dependency tree for some arbitrary
string of length n. A final item containing a de-
pendency tree for a particular string w1 . . . wn is
said to be a correct final item for that string. These
2The formalism allows items to contain forests, and the
dependency structures inside items are defined in a notation
with terminal and preterminal nodes, but these are not needed
here.
292
concepts can be used to prove the correctness of
a parser: for each input string, a parsing schema?s
deduction steps allow us to infer a set of items,
called valid items for that string. A schema is said
to be sound if all valid final items it produces for
any arbitrary string are correct for that string. A
schema is said to be complete if all correct final
items are valid. A correct parsing schema is one
which is both sound and complete.
In constituency-based parsing schemata, deduc-
tion steps usually have grammar rules as side con-
ditions. In the case of dependency parsers it is
also possible to use grammars (Eisner and Satta,
1999), but many algorithms use a data-driven ap-
proach instead, making individual decisions about
which dependencies to create by using probabilis-
tic models (Eisner, 1996) or classifiers (Yamada
and Matsumoto, 2003). To represent these algo-
rithms as deduction systems, we use the notion
of D-rules (Covington, 1990). D-rules take the
form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we ob-
tain a representation of the underlying logic of the
parser while abstracting away from control struc-
tures (the particular model used to create the de-
cisions associated with D-rules). Furthermore, the
choice points in the parsing process and the infor-
mation we can use to make decisions are made ex-
plicit in the steps linked to D-rules.
3 The WG1 parser
3.1 Parsing schema for WG1
We define WG1, a parser for well-nested depen-
dency structures of gap degree ? 1, as follows:
The item set is IWG1 = I1 ? I2, with
I1 = {[i, j, h, , ] | i, j, h ? N, 1 ? h ? n,
1 ? i ? j ? n, h 6= j, h 6= i? 1},
where each item of the form [i, j, h, , ] repre-
sents the set of all well-nested partial dependency
trees3 with gap degree at most 1, rooted at wh, and
such that bwhc = {wh} ? [i, j], and
I2 = {[i, j, h, l, r] | i, j, h, l, r ? N, 1 ? h ? n,
1 ? i < l ? r < j ? n, h 6= j, h 6= i? 1,
h 6= l ? 1, h 6= r}
3In this and subsequent schemata, we use D-rules to ex-
press parsing decisions, so partial dependency trees are as-
sumed to be taken from the set of trees licensed by a set of
D-rules.
where each item of the form [i, j, h, l, r] represents
the set of all well-nested partial dependency trees
rooted at wh such that bwhc = {wh} ? ([i, j] \
[l, r]), and all the nodes (except possibly h) have
gap degree at most 1. We call items of this form
gapped items, and the interval [l, r] the gap of
the item. Note that the constraints h 6= j, h 6=
i + 1, h 6= l ? 1, h 6= r are added to items to
avoid redundancy in the item set. Since the result
of the expression {wh} ? ([i, j] \ [l, r]) for a given
head can be the same for different sets of values of
i, j, l, r, we restrict these values so that we cannot
get two different items representing the same de-
pendency structures. Items ? violating these con-
straints always have an alternative representation
that does not violate them, that we can express
with a normalising function nm(?) as follows:
nm([i, j, j, l, r]) = [i, j ? 1, j, l, r] (if r ? j ? 1 or r = ),
or [i, l ? 1, j, , ] (if r = j ? 1).
nm([i, j, l ? 1, l, r]) = [i, j, l ? 1, l ? 1, r](if l > i + 1),
or [r + 1, j, l ? 1, , ] (if l = i + 1).
nm([i, j, i ? 1, l, r]) = [i ? 1, j, i ? 1, l, r].
nm([i, j, r, l, r]) = [i, j, r, l, r ? 1] (if l < r),
or [i, j, r, , ] (if l = r).
nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.
When defining the deduction steps for this and
other parsers, we assume that they always produce
normalised items. For clarity, we do not explicitly
write this in the deduction steps, writing ? instead
of nm(?) as antecedents and consequents of steps.
The set of initial items is defined as the set
H = {[h, h, h, , ] | h ? N, 1 ? h ? n},
where each item [h, h, h, , ] represents the set
containing the trivial partial dependency tree con-
sisting of a single node wh and no links. This
same set of hypotheses can be used for all the
parsers, so we do not make it explicit for subse-
quent schemata. Note that initial items are sepa-
rate from the item set IWG1 and not subject to its
constraints, so they do not require normalisation.
The set of final items for strings of length n in
WG1 is defined as the set
F = {[1, n, h, , ] | h ? N, 1 ? h ? n},
which is the set of items in IWG1 containing de-
pendency trees for the complete input string (from
position 1 to n), with their head at any word wh.
The deduction steps of the parser can be seen in
Figure 1A.
The WG1 parser proceeds bottom-up, by build-
ing dependency subtrees and joining them to form
larger subtrees, until it finds a complete depen-
dency tree for the input sentence. The logic of
293
A. WG1 parser:
Link Ungapped:
[h1, h1, h1, , ]
[i2, j2, h2, , ]
[i2, j2, h1, , ] wh2 ? wh1
such that wh2 ? [i2, j2] ? wh1 /? [i2, j2],
Link Gapped:
[h1, h1, h1, , ]
[i2, j2, h2, l2, r2]
[i2, j2, h1, l2, r2] wh2 ? wh1
such that wh2 ? [i2, j2] \ [l2, r2] ? wh1 /? [i2, j2] \ [l2, r2],
Combine Ungapped:
[i, j, h, , ] [j + 1, k, h, , ]
[i, k, h, , ]
Combine Opening Gap:
[i, j, h, , ] [k, l, h, , ]
[i, l, h, j + 1, k ? 1]
such that j < k ? 1,
Combine Keeping Gap Left:
[i, j, h, l, r] [j + 1, k, h, , ]
[i, k, h, l, r]
Combine Keeping Gap Right:
[i, j, h, , ] [j + 1, k, h, l, r]
[i, k, h, l, r]
Combine Closing Gap:
[i, j, h, l, r] [l, r, h, , ]
[i, j, h, , ]
Combine Shrinking Gap Left:
[i, j, h, l, r] [l, k, h, , ]
[i, j, h, k + 1, r]
Combine Shrinking Gap Right:
[i, j, h, l, r] [k, r, h, , ]
[i, j, h, l, k ? 1]
Combine Shrinking Gap Centre:
[i, j, h, l, r] [l, r, h, l2, r2]
[i, j, h, l2, r2]
B. WGK parser:
Link:
[h1, h1, h1, []]
[i2, j2, h2, [(l1, r1), . . . , (lg, rg)]]
[i2, j2, h1, [(l1, r1), . . . , (lg, rg)]]
wh2 ? wh1
such that wh2 ? [i2, j2] \
?g
p=1[lp, rp]
?wh1 /? [i2, j2] \
?g
p=1[lp, rp].
Combine Shrinking Gap Right:
[i, j, h, [(l1, r1), . . . , (lq?1, rq?1), (lq, r?), (ls, rs), . . . , (lg, rg)]]
[rq + 1, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Opening Gap:
[i, lq ? 1, h, [(l1, r1), . . . , (lq?1, rq?1)]]
[rq + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k and lq ? rq ,
Combine Shrinking Gap Left:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, rs), (ls+1, rs+1), . . . , (lg, rg)]]
[l?, ls ? 1, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Keeping Gaps:
[i, j, h, [(l1, r1), . . . , (lq, rq)]]
[j + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k,
Combine Shrinking Gap Centre:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, r?), (ls, rs), . . . , (lg, rg)]]
[l?, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
C. Additional steps to turn WG1 into MG1:
Combine Interleaving:
[i, j, h, l, r] [l, k, h, r + 1, j]
[i, k, h, , ]
Combine Interleaving Gap C:
[i, j, h, l, r] [l, k, h, m, j]
[i, k, h, m, r]
such that m < r + 1,
Combine Interleaving Gap L:
[i, j, h, l, r]
[l, k, h, r + 1, u]
[i, k, h, j + 1, u]
such that u > j,
Combine Interleaving Gap R:
[i, j, h, l, r]
[k, m, h, r + 1, j]
[i, m, h, l, k ? 1]
such that k > l.
D. General form of the MGk Combine step:
[ia1 , iap+1 ? 1, h, [(ia1+1, ia2 ? 1), . . . , (iap?1+1, iap ? 1)]]
[ib1 , ibq+1 ? 1, h, [(ib1+1, ib2 ? 1), . . . , (ibq?1+1, ibq ? 1)]]
[imin(a1,b1), imax(ap+1,bq+1) ? 1, h, [(ig1 , ig1+1 ? 1), . . . , (igr , igr+1 ? 1)]]
for each string of length n with a?s located at positions a1 . . . ap(1 ? a1 < . . . < ap ? n), b?s at positions b1 . . . bq(1 ? b1 <
. . . < bq ? n), and g?s at positions g1 . . . gr(2 ? g1 < . . . < gr ? n ? 1), such that 1 ? p ? k, 1 ? q ? k, 0 ? r ? k ? 1,
p + q + r = n, and the string does not contain more than one consecutive appearance of the same symbol.
Figure 1: Deduction steps for the parsers defined in the paper.
the parser can be understood by considering how
it infers the item corresponding to the subtree in-
duced by a particular node, given the items for the
subtrees induced by the direct dependents of that
node. Suppose that, in a complete dependency
analysis for a sentence w1 . . . wn, the word wh
has wd1 . . . wdp as direct dependents (i.e. we have
dependency links wd1 ? wh, . . . , wdp ? wh).
Then, the item corresponding to the subtree in-
duced by wh is obtained from the ones correspond-
ing to the subtrees induced by wd1 . . . wdp by: (1)
applying the Link Ungapped or Link Gapped step
to each of the items corresponding to the subtrees
induced by the direct dependents, and to the hy-
pothesis [h, h, h, , ]. This allows us to infer p
items representing the result of linking each of the
dependent subtrees to the new head wh; (2) ap-
plying the various Combine steps to join all of the
294
items obtained in the previous step into a single
item. The Combine steps perform a union oper-
ation between subtrees. Therefore, the result is a
dependency tree containing all the dependent sub-
trees, and with all of them linked to h: this is
the subtree induced by wh. This process is ap-
plied repeatedly to build larger subtrees, until, if
the parsing process is successful, a final item is
found containing a dependency tree for the com-
plete sentence.
3.2 Proving correctness
The parsing schemata formalism can be used to
prove the correctness of a parsing schema. To
prove that WG1 is correct, we need to prove
its soundness and completeness.4 Soundness is
proven by checking that valid items always con-
tain well-nested trees. Completeness is proven by
induction, taking initial items as the base case and
showing that an item containing a correct subtree
for a string can always be obtained from items
corresponding to smaller subtrees. In order to
prove this induction step, we use the concept of
order annotations (Kuhlmann, 2007; Kuhlmann
and Mo?hl, 2007), which are strings that lexicalise
the precedence relation between the nodes of a de-
pendency tree. Given a correct subtree, we divide
the proof into cases according to the order annota-
tion of its head and we find that, for every possible
form of this order annotation, we can find a se-
quence of Combine steps to infer the relevant item
from smaller correct items.
3.3 Computational complexity
The time complexity of WG1 is O(n7), as the
step Combine Shrinking Gap Centre works with 7
free string positions. This complexity with respect
to the length of the input is as expected for this
set of structures, since Kuhlmann (2007) shows
that they are equivalent to LTAG, and the best ex-
isting parsers for this formalism also perform in
O(n7) (Eisner and Satta, 2000). Note that the
Combine step which is the bottleneck only uses the
7 indexes, and not any other entities like D-rules,
so its O(n7) complexity does not have any addi-
tional factors due to grammar size or other vari-
ables. The space complexity of WG1 is O(n5)
for recognition, due to the 5 indexes in items, and
O(n7) for full parsing.
4Due to space constraints, correctness proofs for the
parsers are not given here. Full proofs are provided in the
extended version of this paper, see (Go?mez-Rodr??guez et al,
2008b).
It is possible to build a variant of this parser
with time complexity O(n6), as with parsers for
unlexicalised TAG, if we work with unlexicalised
D-rules specifying the possibility of dependencies
between pairs of categories instead of pairs of
words. In order to do this, we expand the item set
with unlexicalised items of the form [i, j, C, l, r],
where C is a category, apart from the existing
items [i, j, h, l, r]. Steps in the parser are dupli-
cated, to work both with lexicalised and unlex-
icalised items, except for the Link steps, which
always work with a lexicalised item and an un-
lexicalised hypothesis to produce an unlexicalised
item, and the Combine Shrinking Gap steps, which
can work only with unlexicalised items. Steps are
added to obtain lexicalised items from their unlex-
icalised equivalents by binding the head to partic-
ular string positions. Finally, we need certain vari-
ants of the Combine Shrinking Gap steps that take
2 unlexicalised antecedents and produce a lexi-
calised consequent; an example is the following:
Combine Shrinking Gap Centre L:
[i, j, C, l, r]
[l + 1, r, C, l2, r2]
[i, j, l, l2, r2]
such that cat(wl)=C
Although this version of the algorithm reduces
time complexity with respect to the length of the
input to O(n6), it also adds a factor related to the
number of categories, as well as constant factors
due to using more kinds of items and steps than
the original WG1 algorithm. This, together with
the advantages of lexicalised dependency parsing,
may mean that the original WG1 algorithm is more
practical than this version.
4 The WGk parser
The WG1 parsing schema can be generalised to
obtain a parser for all well-nested dependency
structures with gap degree bounded by a constant
k(k ? 1), which we call WGk parser. In order to
do this, we extend the item set so that it can contain
items with up to k gaps, and modify the deduction
steps to work with these multi-gapped items.
4.1 Parsing schema for WGk
The item set IWGk is the set of all
[i, j, h, [(l1, r1), . . . , (lg, rg)]] where i, j, h, g ? N
, 0 ? g ? k, 1 ? h ? n, 1 ? i ? j ? n , h 6= j,
h 6= i? 1; and for each p ? {1, 2, . . . , g}:
lp, rp ? N, i < lp ? rp < j, rp < lp+1 ? 1,
h 6= lp ? 1, h 6= rp.
An item [i, j, h, [(l1, r1), . . . , (lg, rg)]] repre-
sents the set of all well-nested partial dependency
295
trees rooted at wh such that bwhc = {wh}?([i, j]\
?g
p=1[lp, rp]), where each interval [lp, rp] is called
a gap. The constraints h 6= j, h 6= i + 1, h 6=
lp ? 1, h 6= rp are added to avoid redundancy, and
normalisation is defined as in WG1. The set of fi-
nal items is defined as the set F = {[1, n, h, []] |
h ? N, 1 ? h ? n}. Note that this set is the same
as in WG1, as these are the items that we denoted
[1, n, h, , ] in the previous parser.
The deduction steps can be seen in Figure 1B.
As expected, the WG1 parser corresponds to WGk
when we make k = 1. WGk works in the same
way as WG1, except for the fact that Combine
steps can create items with more than one gap5.
The correctness proof is also analogous to that of
WG1, but we must take into account that the set of
possible order annotations is larger when k > 1,
so more cases arise in the completeness proof.
4.2 Computational complexity
The WGk parser runs in time O(n5+2k): as in
the case of WG1, the deduction step with most
free variables is Combine Shrinking Gap Cen-
tre, and in this case it has 5 + 2k free indexes.
Again, this complexity result is in line with what
could be expected from previous research in con-
stituency parsing: Kuhlmann (2007) shows that
the set of well-nested dependency structures with
gap degree at most k is closely related to cou-
pled context-free grammars in which the maxi-
mal rank of a nonterminal is k + 1; and the con-
stituency parser defined by Hotz and Pitsch (1996)
for these grammars also adds an n2 factor for each
unit increment of k. Note that a small value of
k should be enough to cover the vast majority of
the non-projective sentences found in natural lan-
guage treebanks. For example, the Prague Depen-
dency Treebank contains no structures with gap
degree greater than 4. Therefore, a WG4 parser
would be able to analyse all the well-nested struc-
tures in this treebank, which represent 99.89% of
the total. Increasing k beyond 4 would not pro-
duce further improvements in coverage.
5 Parsing ill-nested structures
The WGk parser analyses dependency structures
with bounded gap degree as long as they are
well-nested. This covers the vast majority of
5In all the parsers in this paper, Combine steps may be
applied in different orders to produce the same result, causing
spurious ambiguity. In WG1 and WGk, this can be avoided
when implementing the schemata, by adding flags to items
so as to impose a particular order.
the structures that occur in natural-language tree-
banks (Kuhlmann and Nivre, 2006), but there is
still a significant minority of sentences that con-
tain ill-nested structures. Unfortunately, the gen-
eral problem of parsing ill-nested structures is NP-
complete, even when the gap degree is bounded:
this set of structures is closely related to LCFRS
with bounded fan-out and unbounded production
length, and parsing in this formalism has been
proven to be NP-complete (Satta, 1992). The
reason for this high complexity is the problem
of unrestricted crossing configurations, appearing
when dependency subtrees are allowed to inter-
leave in every possible way. However, just as
it has been noted that most non-projective struc-
tures appearing in practice are only ?slightly? non-
projective (Nivre and Nilsson, 2005), we charac-
terise a sense in which the structures appearing in
treebanks can be viewed as being only ?slightly?
ill-nested. In this section, we generalise the algo-
rithms WG1 and WGk to parse a proper superset
of the set of well-nested structures in polynomial
time; and give a characterisation of this new set
of structures, which includes all the structures in
several dependency treebanks.
5.1 The MG1 and MGk parsers
The WGk parser presented previously is based on
a bottom-up process, where Link steps are used to
link completed subtrees to a head, and Combine
steps are used to join subtrees governed by a com-
mon head to obtain a larger structure. As WGk is a
parser for well-nested structures of gap degree up
to k, its Combine steps correspond to all the ways
in which we can join two sets of sibling subtrees
meeting these constraints, and having a common
head, into another. Thus, this parser does not use
Combine steps that produce interleaved subtrees,
since these would generate items corresponding to
ill-nested structures.
We obtain a polynomial parser for a wider set of
structures of gap degree at most k, including some
ill-nested ones, by having Combine steps repre-
senting every way in which two sets of sibling sub-
trees of gap degree at most k with a common head
can be joined into another, including those produc-
ing interleaved subtrees, like the steps for gap de-
gree 1 shown in Figure 1C. Note that this does not
mean that we can build every possible ill-nested
structure: some structures with complex crossed
configurations have gap degree k, but cannot be
built by combining two structures of that gap de-
gree. More specifically, our algorithm will be able
296
to parse a dependency structure (well-nested or
not) if there exists a binarisation of that structure
that has gap degree at most k. The parser im-
plicitly works by finding such a binarisation, since
Combine steps are always applied to two items and
no intermediate item generated by them can ex-
ceed gap degree k (not counting the position of
the head in the projection).
More formally, let T be a dependency structure
for the string w1 . . . wn. A binarisation of T is
a dependency tree T ? over a set of nodes, each of
which may be unlabelled or labelled with a word
in {w1 . . . wn}, such that the following conditions
hold: (1) each node has at most two children, and
(2) wi ? wj in T if and only if wi ?? wj in
T ?. A dependency structure is mildly ill-nested
for gap degree k if it has at least one binarisation
of gap degree ? k. Otherwise, we say that it is
strongly ill-nested for gap degree k. It is easy
to prove that the set of mildly ill-nested structures
for gap degree k includes all well-nested structures
with gap degree up to k.
We define MG1, a parser for mildly ill-nested
structures for gap degree 1, as follows: (1) the
item set is the same as that of WG1, except that
items can now contain any mildly ill-nested struc-
tures for gap degree 1, instead of being restricted
to well-nested structures; and (2) deduction steps
are the same as in WG1, plus the additional steps
shown in Figure 1C. These extra Combine steps
allow the parser to combine interleaved subtrees
with simple crossing configurations. The MG1
parser still runs in O(n7), as these new steps do
not use more than 7 string positions.
The proof of correctness for this parser is sim-
ilar to that of WG1. Again, we use the concept
of order annotations. The set of mildly ill-nested
structures for gap degree k can be defined as those
that only contain annotations meeting certain con-
straints. The soundness proof involves showing
that Combine steps always generate items contain-
ing trees with such annotations. Completeness is
proven by induction, by showing that if a subtree
is mildly ill-nested for gap degree k, an item for
it can be obtained from items for smaller subtrees
by applying Combine and Link steps. In the cases
where Combine steps have to be applied, the order
in which they may be used to produce a subtree
can be obtained from its head?s order annotation.
To generalise this algorithm to mildly ill-nested
structures for gap degree k, we need to add a Com-
bine step for every possible way of joining two
structures of gap degree at most k into another.
This can be done systematically by considering a
set of strings over an alphabet of three symbols:
a and b to represent intervals of words in the pro-
jection of each of the structures, and g to repre-
sent intervals that are not in the projection of ei-
ther structure, and will correspond to gaps in the
joined structure. The legal combinations of struc-
tures for gap degree k will correspond to strings
where symbols a and b each appear at most k + 1
times, g appears at most k times and is not the first
or last symbol, and there is no more than one con-
secutive appearance of any symbol. Given a string
of this form, the corresponding Combine step is
given by the expression in Figure 1D. As a particu-
lar example, the Combine Interleaving Gap C step
in Figure 1C is obtained from the string abgab.
Thus, we define the parsing schema for MGk, a
parser for mildly ill-nested structures for gap de-
gree k, as the schema where (1) the item set is
like that of WGk, except that items can now con-
tain any mildly ill-nested structures for gap degree
k, instead of being restricted to well-nested struc-
tures; and (2) the set of deduction steps consists of
a Link step as the one in WGk, plus a set of Com-
bine steps obtained as expressed in Figure 1D.
As the string used to generate a Combine step
can have length at most 3k + 2, and the result-
ing step contains an index for each symbol of the
string plus two extra indexes, the MGk parser has
complexity O(n3k+4). Note that the item and de-
duction step sets of an MGk parser are always su-
persets of those of WGk. In particular, the steps
for WGk are those obtained from strings that do
not contain abab or baba as a scattered substring.
5.2 Mildly ill-nested dependency structures
The MGk algorithm defined in the previous sec-
tion can parse any mildly ill-nested structure for a
given gap degree k in polynomial time. We have
characterised the set of mildly ill-nested structures
for gap degree k as those having a binarisation of
gap degree ? k. Since a binarisation of a depen-
dency structure cannot have lower gap degree than
the original structure, this set only contains struc-
tures with gap degree at most k. Furthermore, by
the relation between MGk and WGk, we know that
it contains all the well-nested structures with gap
degree up to k.
Figure 2 shows an example of a structure that
has gap degree 1, but is strongly ill-nested for gap
degree 1. This is one of the smallest possible such
structures: by generating all the possible trees up
to 10 nodes (without counting a dummy root node
297
Language
Structures
Total
Nonprojective
Total
By gap degree By nestedness
Gap
degree 1
Gap
degree 2
Gap
degree 3
Gap
deg. > 3
Well-
Nested
Mildly
Ill-Nested
Strongly
Ill-Nested
Arabic 2995 205 189 13 2 1 204 1 0
Czech 87889 20353 19989 359 4 1 20257 96 0
Danish 5430 864 854 10 0 0 856 8 0
Dutch 13349 4865 4425 427 13 0 4850 15 0
Latin 3473 1743 1543 188 10 2 1552 191 0
Portuguese 9071 1718 1302 351 51 14 1711 7 0
Slovene 1998 555 443 81 21 10 550 5 0
Swedish 11042 1079 1048 19 7 5 1008 71 0
Turkish 5583 685 656 29 0 0 665 20 0
Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appear-
ing in treebanks for Arabic (Hajic? et al, 2004), Czech (Hajic? et al, 2006), Danish (Kromann, 2003), Dutch (van der Beek et al,
2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al, 2002), Slovene (Dz?eroski et al, 2006), Swedish (Nilsson
et al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).
Figure 2: One of the smallest strongly ill-nested structures.
This dependency structure has gap degree 1, but is only
mildly ill-nested for gap degree ? 2.
located at position 0), it can be shown that all the
structures of any gap degree k with length smaller
than 10 are well-nested or only mildly ill-nested
for that gap degree k.
Even if a structure T is strongly ill-nested for
a given gap degree, there is always some m ? N
such that T is mildly ill-nested for m (since every
dependency structure can be binarised, and binari-
sations have finite gap degree). For example, the
structure in Figure 2 is mildly ill-nested for gap de-
gree 2. Therefore, MGk parsers have the property
of being able to parse any possible dependency
structure as long as we make k large enough.
In practice, structures like the one in Figure 2
do not seem to appear in dependency treebanks.
We have analysed treebanks for nine different lan-
guages, obtaining the data presented in Table 1.
None of these treebanks contain structures that are
strongly ill-nested for their gap degree. There-
fore, in any of these treebanks, the MGk parser can
parse every sentence with gap degree at most k.
6 Conclusions and future work
We have defined a parsing algorithm for well-
nested dependency structures with bounded gap
degree. In terms of computational complexity,
this algorithm is comparable to the best parsers
for related constituency-based formalisms: when
the gap degree is at most 1, it runs in O(n7),
like the fastest known parsers for LTAG, and can
be made O(n6) if we use unlexicalised depen-
dencies. When the gap degree is greater than 1,
the time complexity goes up by a factor of n2
for each extra unit of gap degree, as in parsers
for coupled context-free grammars. Most of the
non-projective sentences appearing in treebanks
are well-nested and have a small gap degree, so
this algorithm directly parses the vast majority of
the non-projective constructions present in natural
languages, without requiring the construction of a
constituency grammar as an intermediate step.
Additionally, we have defined a set of struc-
tures for any gap degree k which we call mildly
ill-nested. This set includes ill-nested structures
verifying certain conditions, and can be parsed in
O(n3k+4) with a variant of the parser for well-
nested structures. The practical interest of mildly
ill-nested structures can be seen in the data ob-
tained from several dependency treebanks, show-
ing that all of the ill-nested structures in them are
mildly ill-nested for their corresponding gap de-
gree. Therefore, our O(n3k+4) parser can analyse
all the gap degree k structures in these treebanks.
The set of mildly ill-nested structures for gap
degree k is defined as the set of structures that have
a binarisation of gap degree at most k. This defini-
tion is directly related to the way the MGk parser
works, since it implicitly finds such a binarisation.
An interesting line of future work would be to find
an equivalent characterisation of mildly ill-nested
structures which is more grammar-oriented and
would provide a more linguistic insight into these
structures. Another research direction, which we
are currently working on, is exploring how vari-
ants of the MGk parser?s strategy can be applied
to the problem of binarising LCFRS (Go?mez-
Rodr??guez et al, 2009).
298
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proc. of LREC 2002, pages
1968?1703, Las Palmas, Spain.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2002.
The annotation process in the Turkish treebank. In
Proc. of EACL Workshop on Linguistically Inter-
preted Corpora - LINC, Budapest, Hungary.
David Bamman and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proc. of
5th Workshop on Treebanks and Linguistic Theories
(TLT2006), pages 67?78.
Manuel Bodirsky, Marco Kuhlmann, and Mathias
Mo?hl. 2005. Well-nested drawings as models
of syntactic structure. Technical Report, Saar-
land University. Electronic version available at:
http://www.ps.uni-sb.de/Papers/.
Michael A. Covington. 1990. A dependency parser
for variable-word-order languages. Technical Re-
port AI-1990-01, Athens, GA.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k ?Zabokrtsky?, and Andreja ?Zele. 2006.
Towards a Slovene dependency treebank. In Proc.
of LREC 2006, pages 1388?1391, Genoa, Italy.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL-99, pages 457?
464, Morristown, NJ. ACL.
Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized tree-adjoining grammars.
In Proc. of 5th Workshop on Tree-Adjoining Gram-
mars and Related Formalisms (TAG+5), pages 14?
19, Paris.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING-96, pages 340?345, Copenhagen.
Carlos Go?mez-Rodr??guez, John Carroll, and David
Weir. 2008a. A deductive approach to dependency
parsing. In Proc. of ACL?08:HLT, pages 968?976,
Columbus, Ohio. ACL.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2008b. Parsing mildly non-projective depen-
dency structures. Technical Report CSRP 600, De-
partment of Informatics, University of Sussex.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proc. of NAACL?09:HLT (to appear).
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan ?Snaidauf,
and Emanuel Bes?ka. 2004. Prague Arabic depen-
dency treebank: Development in data and tools. In
Proc. of NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague depen-
dency treebank 2.0. CDROM CAT: LDC2006T01,
ISBN 1-58563-370-4.
Jir??? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proc. of ACL 2007, Prague,
Czech Republic. ACL.
Gu?nter Hotz and Gisela Pitsch. 1996. On pars-
ing coupled-context-free languages. Theor. Comput.
Sci., 161(1-2):205?233. Elsevier, Essex, UK.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Handbook of for-
mal languages, pages 69?124. Springer-Verlag,
Berlin/Heidelberg/NY.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT2003).
Marco Kuhlmann and Mathias Mo?hl. 2007. Mildly
context-sensitive dependency languages. In Proc. of
ACL 2007, Prague, Czech Republic. ACL.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proc.
of COLING/ACL main conference poster sessions,
pages 507?514, Morristown, NJ, USA. ACL.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral dissertation, Saar-
land University, Saarbru?cken, Germany.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In IWPT 2007: Proc. of the 10th Confer-
ence on Parsing Technologies. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
HLT/EMNLP 2005, pages 523?530, Morristown,
NJ, USA. ACL.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proc. of NODALIDA
2005 Special Session on Treebanks, pages 119?132.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proc. of ACL?05,
pages 99?106, Morristown, NJ, USA. ACL.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In A. Abeille, ed., Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer, Dor-
drecht.
Giorgio Satta. 1992. Recognition of linear context-
free rewriting systems. In Proc. of ACL-92, pages
89?95, Morristown, NJ. ACL.
Klaas Sikkel. 1997. Parsing Schemata ? A Frame-
work for Specification and Analysis of Parsing Al-
gorithms. Springer-Verlag, Berlin/Heidelberg/NY.
L. van der Beek, G. Bouma, R. Malouf, and G. van
Noord. 2002. The Alpino dependency treebank.
In Computational Linguistics in the Netherlands
(CLIN), Twente University.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Proc.
of ACL-87, pages 104?111, Morristown, NJ. ACL.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT 2003), pages 195?206.
299
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 539?547,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Optimal Reduction of Rule Length
in Linear Context-Free Rewriting Systems
Carlos Go?mez-Rodr??guez1, Marco Kuhlmann2, Giorgio Satta3 and David Weir4
1 Departamento de Computacio?n, Universidade da Corun?a, Spain (cgomezr@udc.es)
2 Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se)
3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it)
4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk)
Abstract
Linear Context-free Rewriting Systems
(LCFRS) is an expressive grammar formalism
with applications in syntax-based machine
translation. The parsing complexity of an
LCFRS is exponential in both the rank
of a production, defined as the number of
nonterminals on its right-hand side, and a
measure for the discontinuity of a phrase,
called fan-out. In this paper, we present
an algorithm that transforms an LCFRS
into a strongly equivalent form in which
all productions have rank at most 2, and
has minimal fan-out. Our results generalize
previous work on Synchronous Context-Free
Grammar, and are particularly relevant for
machine translation from or to languages that
require syntactic analyses with discontinuous
constituents.
1 Introduction
There is currently considerable interest in syntax-
based models for statistical machine translation that
are based on the extraction of a synchronous gram-
mar from a corpus of word-aligned parallel texts;
see for instance Chiang (2007) and the references
therein. One practical problem with this approach,
apart from the sheer number of the rules that result
from the extraction procedure, is that the parsing
complexity of all synchronous formalisms that we
are aware of is exponential in the rank of a rule,
defined as the number of nonterminals on the right-
hand side. Therefore, it is important that the rules
of the extracted grammar are transformed so as to
minimise this quantity. Not only is this beneficial in
terms of parsing complexity, but smaller rules can
also improve a translation model?s ability to gener-
alize to new data (Zhang et al, 2006).
Optimal algorithms exist for minimising the size
of rules in a Synchronous Context-Free Gram-
mar (SCFG) (Uno and Yagiura, 2000; Zhang et al,
2008). However, the SCFG formalism is limited
to modelling word-to-word alignments in which a
single continuous phrase in the source language is
aligned with a single continuous phrase in the tar-
get language; as defined below, this amounts to
saying that SCFG have a fan-out of 2. This re-
striction appears to render SCFG empirically inad-
equate. In particular, Wellington et al (2006) find
that the coverage of a translation model can increase
dramatically when one allows a bilingual phrase to
stretch out over three rather than two continuous
substrings. This observation is in line with empir-
ical studies in the context of dependency parsing,
where the need for formalisms with higher fan-out
has been observed even in standard, single language
texts (Kuhlmann and Nivre, 2006).
In this paper, we present an algorithm that com-
putes optimal decompositions of rules in the for-
malism of Linear Context-Free Rewriting Systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS was
originally introduced as a generalization of sev-
eral so-called mildly context-sensitive grammar for-
malisms. In the context of machine translation,
LCFRS is an interesting generalization of SCFG be-
cause it does not restrict the fan-out to 2, allow-
ing productions with arbitrary fan-out (and arbitrary
rank). Given an LCFRS, our algorithm computes a
strongly equivalent grammar with rank 2 and min-
539
imal increase in fan-out.1 In this context, strong
equivalence means that the derivations of the orig-
inal grammar can be reconstructed using some sim-
ple homomorphism (c.f. Nijholt, 1980). Our contri-
bution is significant because the existing algorithms
for decomposing SCFG, based on Uno and Yagiura
(2000), cannot be applied to LCFRS, as they rely
on the crucial property that components of biphrases
are strictly separated in the generated string: Given a
pair of synchronized nonterminal symbols, the ma-
terial derived from the source nonterminal must pre-
cede the material derived from the target nontermi-
nal, or vice versa. The problem that we solve has
been previously addressed by Melamed et al (2004),
but in contrast to our result, their algorithm does not
guarantee an optimal (minimal) increase in the fan-
out of the resulting grammar. However, this is essen-
tial for the practical applicability of the transformed
grammar, as the parsing complexity of LCFRS is ex-
ponential in both the rank and the fan-out.
Structure of the paper The remainder of the pa-
per is structured as follows. Section 2 introduces the
terminology and notation that we use for LCFRS.
In Section 3, we present the technical background
of our algorithm; the algorithm itself is discussed
in Section 4. Section 5 concludes the paper by dis-
cussing related work and open problems.
General notation The set of non-negative integers
is denoted by N. For i, j ? N, we write [i, j] to
denote the interval { k ? N | i ? k ? j }, and use
[i] as a shorthand for [1, i]. Given an alphabet V , we
write V ? for the set of all (finite) strings over V .
2 Preliminaries
We briefly summarize the terminology and notation
that we adopt for LCFRS; for detailed definitions,
see Vijay-Shanker et al (1987).
2.1 Linear, non-erasing functions
Let V be an alphabet. For natural numbers r ? 0
and f, f1, . . . , fr ? 1, a function
g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f
1Rambow and Satta (1999) show that without increasing
fan-out it is not always possible to produce even weakly equiv-
alent grammars.
is called a linear, non-erasing function over V of
type f1 ? ? ? ? ? fr ? f , if it can be defined by an
equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ?g ,
where ?g = ??g,1, . . . , ?g,f ? is an f -tuple of strings
over the variables on the left-hand side of the equa-
tion and symbols in V that contains exactly one oc-
currence of each variable. We call the value r the
rank of g, the value f its fan-out, and write ?(g)
and ?(g), respectively, to denote these quantities.
Note that, if we assume the variables on the left-
hand side of the defining equation of g to be named
according to the specific schema given above, then g
is uniquely determined by ?g.
2.2 Linear context-free rewriting systems
A linear context-free rewriting system (LCFRS)
is a construct G = (VN , VT , P, S), where: VN is
an alphabet of nonterminal symbols in which each
symbol A ? VN is associated with a value ?(A),
called its fan-out; VT is an alphabet of terminal
symbols; S ? N is a distinguished start symbol with
?(S) = 1; and P is a set of productions of the form
p : A? g(B1, B2, . . . , Br) ,
where A,B1, . . . , Br ? VN , and g is a linear, non-
erasing function over the terminal alphabet VT of
type ?(B1) ? ? ? ? ? ?(Br) ? ?(A). In a deriva-
tion of an LCFRS, the production p can be used to
transform a sequence of r tuples of strings, gener-
ated by the nonterminals B1, . . . , Br, into a single
?(A)-tuple of strings, associated with the nonter-
minal A. The values ?(g) and ?(g) are called the
rank and fan-out of p, respectively, and we write
?(p) and ?(p), respectively, to denote these quan-
tities. The rank and fan-out of G, written ?(G)
and ?(G), respectively, are the maximum rank and
fan-out among all of its productions. Given that
?(S) = 1, a derivation will associate S with a set of
one-component tuples of strings over VT ; this forms
the string language generated by G.
Example 1 The following LCFRS generates the
string language { anbncndn | n ? N }. We only
specify the set of productions; the remaining com-
540
ponents of the grammar are obvious from that.
S ? g1(R) g1(?x1,1, x1,2?) = ?x1,1x1,2?
R? g2(R) g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
R? g3 g3 = ??, ??
The functions g1 and g2 have rank 1; the function g3
has rank 0. The functions g2 and g3 have fan-out 2;
the function g1 has fan-out 1. 2
3 Technical background
The general idea behind our algorithm is to replace
each production of an LCFRS with a set of ?shorter?
productions that jointly are equivalent to the original
production. Before formalizing this idea, we first in-
troduce a specialized representation for the produc-
tions of an LCFRS.
We distinguish between occurrences of symbols
within a string by exploiting two different notations.
Let ? = a1a2 ? ? ? an be a string. The occurrence ai
in ? can be denoted by means of its position index
i ? [n], or else by means of its two (left and right)
endpoints, i?1 and i; here, the left (right) endpoint
denotes a boundary between occurrence ai and the
previous (subsequent) occurrence, or the beginning
(end) of the string ?. Similarly, a substring ai ? ? ? aj
of ? with i ? j can be denoted by the positions
i, i+ 1, . . . , j of its occurrences, or else by means of
its left and right endpoints, i? 1 and j.
3.1 Production representation
For the remainder of this section, let us fix an
LCFRS G = (VN , VT , P, S) and a production
p : A ? g(B1, . . . , Br) of G, with g defined as
in Section 2.1. We define
|p| = ?(g) +
?(g)?
i=1
|?g,i|.
Let $ be a fresh symbol that does not occur inG. We
define the characteristic string of the production p
as
?(p) = ?g,1$ ? ? ? $?g,?(g) ,
and the variable string of p as the string ?N (p) ob-
tained from ?(p) by removing all the occurrences of
symbols in VT .
Example 2 We will illustrate the concepts intro-
duced in this section using the concrete production
p0 : A? g(B1, B2, B3), where
?g = ?x1,1ax2,1x1,2, x3,1bx3,2? .
In this case, we have
?(p0) = x1,1ax2,1x1,2$x3,1bx3,2 , and
?N (p0) = x1,1x2,1x1,2$x3,1x3,2 . 2
Let I be an index set, I ? [r]. Consider the set B of
occurrences Bi in the right-hand side of p such that
i ? I .2 We define the position set of B, denoted
by ?B, as the set of all positions 1 ? j ? |?N (p)|
such that the jth symbol in ?N (p) is a variable of the
form xi,h, for i ? I and some h ? 1.
Example 3 Some position sets of p0 are
?{B1} = {1, 3} ,?{B2} = {2} ,?{B3} = {5, 6} .
2
A position set ?B can be uniquely expressed as the
union of f ? 1 intervals [l1 + 1, r1], . . . , [lf + 1, rf ]
such that ri?1 < li for every 1 < i ? f . Thus we
define the set of endpoints of ?B as
?B = { lj | j ? [f ] } ? { rj | j ? [f ] } .
The quantity f is called the fan-out of ?B, writ-
ten ?(?B). Notice that the fan-out of a position set
?{B} does not necessarily coincide with the fan-out
of the non-terminal B in the underlying LCFRS. A
set with 2f endpoints always corresponds to a posi-
tion set of fan-out f .
Example 4 For our running example, we have
?{B1} = {0, 1, 2, 3}, ?{B2} = {1, 2}, ?{B3} =
{4, 6}. Consequently, the fan-out of ?{B1} is 2, and
the fan-out of ?{B2} and ?{B3} is 1. Notice that the
fan-out of the non-terminal B3 is 2. 2
We drop B from ?B and ?B whenever this set is
understood from the context or it is not relevant.
Given a set of endpoints ? = {i1, . . . , i2f} with
i1 < ? ? ? < i2f , we obtain its corresponding position
set by calculating the closure of ?, defined as
[?] = ?fj=1[i2j?1 + 1, i2j ] .
2To avoid clutter in our examples, we abuse the notation by
not making an explicit distinction between nonterminals and oc-
currences of nonterminals in productions.
541
3.2 Reductions
Assume that r > 2. The reduction of p by the non-
terminal occurrencesBr?1, Br is the ordered pair of
productions (p1, p2) that is defined as follows. Let
?1, . . . , ?n be the maximal substrings of ?(p) that
contain only variables xi,j with r ? 1 ? i ? r and
terminal symbols, and at least one variable. Then
p1 : A? g1(B1, . . . , Br?2, X) and
p2 : X ? g2(Br?1, Br) ,
where X is a fresh nonterminal symbol, the char-
acteristic string ?(p1) is the string obtained from
?(p) by replacing each substring ?i by the vari-
able xr?1,i, and the characteristic string ?(p2) is the
string ?1$ ? ? ? $?n.
Note that the defining equations of neither g1
nor g2 are in the specific form discussed in Sec-
tion 2.1; however, they can be brought into this form
by a consistent renaming of the variables. We will
silently assume this renaming to take place.
Example 5 The reduction of p0 by the nonterminal
occurrences B2 and B3 has p1 : A ? g1(B1, X)
and p2 : X ? g2(B2, B3) with
?(p1) = x1,1x2,1x1,2$x2,2
?(p2) = ax2,1$x3,1bx3,2
or, after renaming and in standard notation,
g1(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1x2,1x1,2, x2,2?
g2(?x1,1?, ?x2,1, x2,2?) = ?ax1,1, x2,1bx2,2? .2
It is easy to check that a reduction provides us with a
pair of productions that are equivalent to the original
production p, in terms of generative capacity, since
g1(B1, . . . , Br?2, g2(Br?1, Br)) = g(B1, . . . , Br)
for all tuples of strings generated from the nontermi-
nalsB1, . . . , Br, respectively. Note also that the fan-
out of production p1 equals the fan-out of p. How-
ever, the fan-out of p2 (the value n) may be greater
than the fan-out of p, depending on the way vari-
ables are arranged in ?(p). Thus, a reduction does
not necessarily preserve the fan-out of the original
production. In the worst case, the fan-out of p2 can
be as large as ?(Br?1) + ?(Br).
1: Function NAIVE-BINARIZATION(p)
2: result? ?;
3: currentProd? p;
4: while ?(currentProd) > 2 do
5: (p1, p2)? any reduction of currentProd;
6: result? result ? p2;
7: currentProd? p1;
8: return result ? currentProd;
Figure 1: The naive algorithm
We have defined reductions only for the last two
occurrences of nonterminals in the right-hand side of
a production p. However, it is easy to see that we can
also define the concept for two arbitrary (not neces-
sarily adjacent) occurrences of nonterminals, at the
cost of making the notation more complicated.
4 The algorithm
Let G be an LCFRS with ?(G) = f and ?(G) = r,
and let f ? ? f be a target fan-out. We will now
present an algorithm that computes an equivalent
LCFRS G? of fan-out at most f ? whose rank is at
most 2, if such an LCFRS exists in the first place.
The algorithm works by exhaustively reducing all
productions in G.
4.1 Naive algorithm
Given an LCFRS production p, a naive algorithm
to compute an equivalent set of productions whose
rank is at most 2 is given in Figure 1. By ap-
plying this algorithm to all the productions in the
LCFRSG, we can obtain an equivalent LCFRS with
rank 2. We will call such an LCFRS a binarization
of G.
The fan-out of the obtained LCFRS will depend
on the nonterminals that we choose for the reduc-
tions in line 5. It is not difficult to see that, in the
worst case, the resulting fan-out can be as high as
d r2e ? f . This occurs when we choose d r2e nonter-minals with fan-out f that have associated variables
in the string ?N (p) that do not occur at consecutive
positions.
The algorithm that we develop in Section 4.3 im-
proves on the naive algorithm in that it can be ex-
ploited to find a sequence of reductions that results
in a binarization of G that is optimal, i.e., leads to
542
an LCFRS with minimal fan-out. The algorithm is
based on a technical concept called adjacency.
4.2 Adjacency
Let p be some production in the LCFRS G, and let
?1,?2 be sets of endpoints, associated with some
sets of nonterminal occurrences in p. We say that?1
and ?2 overlap if the intersection of their closures
is nonempty, that is, if [?1]? [?2] 6= ?. Overlapping
holds if and only if the associated sets of nontermi-
nal occurrences are not disjoint. If ?1 and ?2 do
not overlap, we define their merge as
?(?1,?2) = (?1 ??2) \ (?1 ??2) .
It is easy to see that [?(?1,?2)] = [?1] ? [?2].
We say that ?1 and ?2 are adjacent for a given fan-
out f , written ?1 ?f ?2, if ?1 and ?2 do not
overlap, and ?([?(?1,?2)]) ? f .
Example 6 For the production p0 from Example 2,
we have ?(?{B1},?{B2}) = {0, 3}, showing that?{B1} ?1 ?{B2}. Similarly, we have
?(?{B1},?{B3}) = {0, 1, 2, 3, 4, 6} ,
showing that ?{B1} ?3 ?{B3}, but that neither?{B1} ?2 ?{B3} nor ?{B1} ?1 ?{B3} holds. 2
4.3 Bounded binarization algorithm
The adjacency-based binarization algorithm is given
in Figure 2. It starts with a working set contain-
ing the endpoint sets corresponding to each non-
terminal occurrence in the input production p. Re-
ductions of p are only explored for nonterminal oc-
currences whose endpoint sets are adjacent for the
target fan-out f ?, since reductions not meeting this
constraint would produce productions with fan-out
greater than f ?. Each reduction explored by the al-
gorithm produces a new endpoint set, associated to
the fresh nonterminal that it introduces, and this new
endpoint set is added to the working set and poten-
tially used in further reductions.
From the definition of the adjacency relation?f ,
it follows that at lines 9 and 10 of BOUNDED-
BINARIZATION we only pick up reductions for p
that do not exceed the fan-out bound of f ?. This
implies soundness for our algorithm. Completeness
means that the algorithm fails only if there exists no
binarization for p of fan-out not greater than f ?. This
1: Function BOUNDED-BINARIZATION(p, f ?)
2: workingSet? ?;
3: agenda? ?;
4: for all i from 1 to ?(p) do
5: workingSet? workingSet ? {?{Bi}};
6: agenda? agenda ? {?{Bi}};
7: while agenda 6= ? do
8: ?? pop some endpoint set from agenda;
9: for all ?1 ? workingSet with ?1 ?f ? ? do
10: ?2 = ?(?,?1);
11: if ?2 /? workingSet then
12: workingSet? workingSet ? {?2};
13: agenda? agenda ? {?2};
14: if ?{B1,B2,...,B?(p))} ? workingSet then
15: return true;
16: else
17: return false;
Figure 2: Algorithm to compute a bounded binarization
property is intuitive if one observes that our algo-
rithm is a specialization of standard algorithms for
the computation of the closure of binary relations.
A formal proof of this fact is rather long and te-
dious, and will not be reported here. We notice that
there is a very close similarity between algorithm
BOUNDED-BINARIZATION and the deduction pro-
cedure proposed by Shieber et al (1995) for parsing.
We discuss this more at length in Section 5.
Note that we have expressed the algorithm as a
decision function that will return true if there exists
a binarization of p with fan-out not greater than f ?,
and false otherwise. However, the algorithm can
easily be modified to return a reduction producing
such a binarization, by adding to each endpoint set
? ? workingSet two pointers to the adjacent end-
point sets that were used to obtain it. If the algorithm
is successful, the tree obtained by following these
pointers from the final endpoint set ?{B1,...,B?(p)} ?workingSet gives us a tree of reductions that will
produce a binarization of p with fan-out not greater
than f ?, where each node labeled with the set ?{Bi}
corresponds to the nonterminal Bi, and nodes la-
beled with other endpoint sets correspond to the
fresh nonterminals created by the reductions.
543
4.4 Implementation
In order to implement BOUNDED-BINARIZATION,
we can represent endpoint sets in a canonical way
as 2f ?-tuples of integer positions in ascending order,
and with some special null value used to fill posi-
tions for endpoint sets with fan-out strictly smaller
than f ?. We will assume that the concrete null value
is larger than any other integer.
We also need to provide some appropriate repre-
sentation for the set workingSet, in order to guar-
antee efficient performance for the membership test
and the insertion operation. Both operations can be
implemented in constant time if we represent work-
ingSet as an (2?f ?)-dimensional table with Boolean
entries. Each dimension is indexed by values in
[0, n] plus our special null value; here n is the length
of the string ?N (p), and thus n = O(|p|). However,
this has the disadvantage of using space ?(n2f ?),
even in case workingSet is sparse, and is affordable
only for quite small values of f ?. Alternatively, we
can more compactly represent workingSet as a trie
data structure. This representation has size certainly
smaller than 2f ? ? q, where q is the size of the set
workingSet. However, both membership and inser-
tion operations take now an amount of time O(2f ?).
We now analyse the time complexity of algorithm
BOUNDED-BINARIZATION for inputs p and f ?. We
first focus on the while-loop at lines 7 to 13. As
already observed, the number of possible endpoint
sets is bounded by O(n2f ?). Furthermore, because
of the test at line 11, no endpoint set is ever inserted
into the agenda variable more than once in a sin-
gle run of the algorithm. We then conclude that our
while-loop cycles a number of times O(n2f ?).
We now focus on the choice of the endpoint set
?1 in the inner for-loop at lines 9 to 13. Let us fix ?
as in line 8. It is not difficult to see that any ?1 with
?1 ?f ? ? must satisfy
?(?) + ?(?1)? |? ??1| ? f ?. (1)
Let I ? ?, and consider all endpoint sets ?1 with
? ??1 = I . Given (1), we also have
?(?1) ? f ? + |I| ? ?(?). (2)
This means that, for each ? coming out of the
agenda, at line 9 we can choose all endpoint sets ?1
such that ?1 ?f ? ? by performing the following
steps:
? arbitrarily choose a set I ? ?;
? choose endpoints in set ?1\I subject to (2);
? test whether ?1 belongs to workingSet and
whether ?, ?1 do not overlap.
We claim that, in the above steps, the number
of involved endpoints does not exceed 3f ?. To
see this, we observe that from (2) we can derive
|I| ? ?(?) + ?(?1) ? f ?. The total number
of (distinct) endpoints in a single iteration step is
e = 2?(?) + 2?(?1) ? |I|. Combining with the
above inequality we have
e ? 2?(?) + 2?(?1)? ?(?)? ?(?1) + f ?
= ?(?) + ?(?1) + f ? ? 3f ? ,
as claimed. Since each endpoint takes values in
the set [0, n], we have a total of O(n3f ?) different
choices. For each such choice, we need to clas-
sify an endpoint as belonging to either ?\I , ?1\I ,
or I . This amounts to an additional O(33f ?) dif-
ferent choices. Overall, we have a total number of
O((3n)3f ?) different choices. For each such choice,
the test for membership in workingSet for ?1 takes
constant time in case we use a multi-dimensional ta-
ble, or else O(|p|) in case we use a trie. The ad-
jacency test and the merge operations can easily be
carried out in time O(|p|).
Putting all of the above observations together, and
using the already observed fact that n = O(|p|),
we can conclude that the total amount of time re-
quired by the while-loop at lines 7 to 13 is bounded
byO(|p| ? (3|p|)3f ?), both under the assumption that
workingSet is represented as a multi-dimensional ta-
ble or as a trie. This is also a bound on the running
time of the whole algorithm.
4.5 Minimal binarization of a complete LCFRS
The algorithm defined in Section 4.3 can be used
to binarize an LCFRS in such a way that each rule
in the resulting binarization has the minimum pos-
sible fan-out. This can be done by applying the
BOUNDED-BINARIZATION algorithm to each pro-
duction p, until we find the minimum value for the
544
1: Function MINIMAL-BINARIZATION(G)
2: pb = ? {Set of binarized productions}
3: for all production p of G do
4: f ? = fan-out(p);
5: while not BOUNDED-BINARIZATION(p, f ?)
do
6: f ? = f ? + 1;
7: add result of BOUNDED-BINARIZATION(p,
f ?) to pb; {We obtain the tree from
BOUNDED-BINARIZATION as explained in
Section 4.3 and use it to binarize p}
8: return pb;
Figure 3: Minimal binarization by sequential search
bound f ? for which this algorithm finds a binariza-
tion. For a production with rank r and fan-out f ,
we know that this optimal value of f ? must be in
the interval [f, d r2e ? f ] because binarizing a pro-duction cannot reduce its fan-out, and the NAIVE-
BINARIZATION algorithm seen in Section 4.1 can
binarize any production by increasing fan-out to
d r2e ? f in the worst case.
The simplest way of finding out the optimal value
of f ? for each production is by a sequential search
starting with ?(p) and going upwards, as in the algo-
rithm in Figure 3. Note that the upper bound d r2e ? fthat we have given for f ? guarantees that the while-
loop in this algorithm always terminates.
In the worst case, we may need f ? (d r2e ? 1) + 1executions of the BOUNDED-BINARIZATION algo-
rithm to find the optimal binarization of a production
in G. This complexity can be reduced by changing
the strategy to search for the optimal f ?: for exam-
ple, we can perform a binary search within the inter-
val [f, d r2e ? f ], which lets us find the optimal bina-rization in blog(f ? (d r2e?1)+1)c+1 executions ofBOUNDED-BINARIZATION. However, this will not
result in a practical improvement, since BOUNDED-
BINARIZATION is exponential in the value of f ? and
the binary search will require us to run it on val-
ues of f ? larger than the optimal in most cases. An
intermediate strategy between the two is to apply
exponential backoff to try the sequence of values
f?1+2i (for i = 0, 1, 2 . . .). When we find the first
i such that BOUNDED-BINARIZATION does not fail,
if i > 0, we apply the same strategy to the interval
[f?1+2i?1, f?2+2i], and we repeat this method to
shrink the interval until BOUNDED-BINARIZATION
does not fail for i = 0, giving us our optimal f ?.
With this strategy, the amount of executions of the
algorithm that we need in the worst case is
1
2(dlog(?)e+ dlog(?)e
2) + 1 ,
where ? = f ? (d r2e ? 1) + 1, but we avoid usingunnecessarily large values of f ?.
5 Discussion
To conclude this paper, we now discuss a number of
aspects of the results that we have presented, includ-
ing various other pieces of research that are particu-
larly relevant to this paper.
5.1 The tradeoff between rank and fan-out
The algorithm introduced in this paper can be used
to transform an LCFRS into an equivalent form
with rank 2. This will result into a more effi-
ciently parsable LCFRS, since rank exponentially
affects parsing complexity. However, we must take
into account that parsing complexity is also influ-
enced by fan-out. Our algorithm guarantees a min-
imal increase in fan-out. In practical cases it seems
such an increase is quite small. For example, in
the context of dependency parsing, both Go?mez-
Rodr??guez et al (2009) and Kuhlmann and Satta
(2009) show that all the structures in several well-
known non-projective dependency treebanks are bi-
narizable without any increase in their fan-out.
More in general, it has been shown by Seki et al
(1991) that parsing of LCFRS can be carried out in
time O(n|pM |), where n is the length of the input
string and pM is the production in the grammar with
largest size.3 Thus, there may be cases in which one
has to find an optimal tradeoff between rank and fan-
out, in order to minimize the size of pM . This re-
quires some kind of Viterbi search over the space of
all possible binarizations, constructed as described
at the end of Subsection 4.3, for some appropriate
value of the fan-out f ?.
3The result has been shown for the formalism of multiple
context-free grammars (MCFG), but it also applies to LCFRS,
which are a special case of MCFG.
545
5.2 Extension to general LCFRS
This paper has focussed on string-based LCFRS.
As discussed in Vijay-Shanker et al (1987), LCFRS
provide a more general framework where the pro-
ductions are viewed as generating a set of abstract
derivation trees. These trees can be used to specify
how structures other than tuples of strings are com-
posed. For example, LCFRS derivation trees can be
used to specify how the elementary trees of a Tree
Adjoining Grammar can be composed to produced
derived tree. However, the results in this paper also
apply to non-string-based LCFRS, since by limit-
ing attention to the terminal string yield of whatever
structures are under consideration, the composition
operations can be defined using the string-based ver-
sion of LCFRS that is discussed here.
5.3 Similar algorithmic techniques
The NAIVE-BINARIZATION algorithm given in Fig-
ure 1 is not novel to this paper: it is similar to
an algorithm developed in Melamed et al (2004)
for generalized multitext grammars, a formalism
weakly equivalent to LCFRS that has been intro-
duced for syntax-based machine translation. How-
ever, the grammar produced by our algorithm has
optimal (minimal) fan-out. This is an important im-
provement over the result in (Melamed et al, 2004),
as this quantity enters into the parsing complexity
of both multitext grammars and LCFRS as an expo-
nential factor, and therefore must be kept as low as
possible to ensure practically viable parsing.
Rank reduction is also investigated in Nesson
et al (2008) for synchronous tree-adjoining gram-
mars, a synchronous rewriting formalism based on
tree-adjoining grammars Joshi and Schabes (1992).
In this case the search space of possible reductions
is strongly restricted by the tree structures specified
by the formalism, resulting in simplified computa-
tion for the reduction algorithms. This feature is not
present in the case of LCFRS.
There is a close parallel between the technique
used in the MINIMAL-BINARIZATION algorithm
and deductive parsing techniques as proposed by
Shieber et al (1995), that are usually implemented
by means of tabular methods. The idea of exploit-
ing tabular parsing in production factorization was
first expressed in Zhang et al (2006). In fact, the
particular approach presented here has been used
to improve efficiency of parsing algorithms that use
discontinuous syntactic models, in particular, non-
projective dependency grammars, as discussed in
Go?mez-Rodr??guez et al (2009).
5.4 Open problems
The bounded binarization algorithm that we have
presented has exponential run-time in the value of
the input fan-out bound f ?. It remains an open ques-
tion whether the bounded binarization problem for
LCFRS can be solved in deterministic polynomial
time. Even in the restricted case of f ? = ?(p), that
is, when no increase in the fan-out of the input pro-
duction is allowed, we do not know whether p can be
binarized using only deterministic polynomial time
in the value of p?s fan-out. However, our bounded
binarization algorithm shows that the latter problem
can be solved in polynomial time when the fan-out
of the input LCFRS is bounded by some constant.
Whether the bounded binarization problem can
be solved in polynomial time in the value of the
input bound f ? is also an open problem in the re-
stricted case of synchronous context-free grammars,
a special case of an LCFRS of fan-out two with
a strict separation between the two components of
each nonterminal in the right-hand side of a produc-
tion, as discussed in the introduction. An interesting
analysis of this restricted problem can be found in
Gildea and Stefankovic (2007).
Acknowledgements The work of Carlos Go?mez-
Rodr??guez was funded by Ministerio de Educacio?n
y Ciencia and FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, IN-
CITE08E1R104022ES, INCITE08ENA305025ES,
INCITE08PXIB302179PR and Rede Galega de
Procesamento da Linguaxe e Recuperacio?n de Infor-
macio?n). The work of Marco Kuhlmann was funded
by the Swedish Research Council. The work of
Giorgio Satta was supported by MIUR under project
PRIN No. 2007TJNZRE 002. We are grateful to an
anonymous reviewer for a very detailed review with
a number of particularly useful suggestions.
546
References
David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daniel Gildea and Daniel Stefankovic. 2007. Worst-
case synchronous grammar rules. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 147?154. Association
for Computational Linguistics, Rochester, New
York.
Carlos Go?mez-Rodr??guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
A. K. Joshi and Y. Schabes. 1992. Tree adjoining
grammars and lexicalized grammars. In M. Nivat
and A. Podelsky, editors, Tree Automata and Lan-
guages. Elsevier, Amsterdam, The Netherlands.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), Main Conference Poster Sessions, pages
507?514. Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
I. Dan Melamed, Benjamin Wellington, and Gior-
gio Satta. 2004. Generalized multitext gram-
mars. In 42nd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 661?
668. Barcelona, Spain.
Rebecca Nesson, Giorgio Satta, and Stuart M.
Shieber. 2008. Optimal k-arization of syn-
chronous tree-adjoining grammar. In Proceedings
of ACL-08: HLT, pages 604?612. Association for
Computational Linguistics, Columbus, Ohio.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93.
Springer-Verlag, Berlin, Germany.
Owen Rambow and Giorgio Satta. 1999. Indepen-
dent parallelism in finite copying parallel rewrit-
ing systems. Theoretical Computer Science,
223(1?2):87?120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Program-
ming, 24(1?2):3?36.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of
two permutations. Algorithmica, 26(2):290?309.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical for-
malisms. In 25th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
104?111. Stanford, CA, USA.
Benjamin Wellington, Sonjia Waxmonsky, and
I. Dan Melamed. 2006. Empirical lower bounds
on the complexity of translational equivalence. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), pages 977?984. Sydney, Australia.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting synchronous grammar rules
from word-level alignments in linear time. In
22nd International Conference on Computational
Linguistics (Coling), pages 1081?1088. Manch-
ester, England, UK.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 256?263. New York, USA.
547
Proceedings of ACL-08: HLT, pages 968?976,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Deductive Approach to Dependency Parsing?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
John Carroll and David Weir
Department of Informatics
University of Sussex, United Kingdom
{johnca,davidw}@sussex.ac.uk
Abstract
We define a new formalism, based on Sikkel?s
parsing schemata for constituency parsers,
that can be used to describe, analyze and com-
pare dependency parsing algorithms. This
abstraction allows us to establish clear rela-
tions between several existing projective de-
pendency parsers and prove their correctness.
1 Introduction
Dependency parsing consists of finding the structure
of a sentence as expressed by a set of directed links
(dependencies) between words. This is an alterna-
tive to constituency parsing, which tries to find a di-
vision of the sentence into segments (constituents)
which are then broken up into smaller constituents.
Dependency structures directly show head-modifier
and head-complement relationships which form the
basis of predicate argument structure, but are not
represented explicitly in constituency trees, while
providing a representation in which no non-lexical
nodes have to be postulated by the parser. In addi-
tion to this, some dependency parsers are able to rep-
resent non-projective structures, which is an impor-
tant feature when parsing free word order languages
in which discontinuous constituents are common.
The formalism of parsing schemata (Sikkel, 1997)
is a useful tool for the study of constituency parsers
since it provides formal, high-level descriptions
of parsing algorithms that can be used to prove
their formal properties (such as correctness), es-
tablish relations between them, derive new parsers
from existing ones and obtain efficient implementa-
tions automatically (Go?mez-Rodr??guez et al, 2007).
The formalism was initially defined for context-free
grammars and later applied to other constituency-
based formalisms, such as tree-adjoining grammars
?Partially supported by Ministerio de Educacio?n y Ciencia
and FEDER (TIN2004-07246-C03, HUM2007-66607-C04),
Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC-
10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc. da
Linguaxe e RI) and Programa de Becas FPU.
(Alonso et al, 1999). However, since parsing
schemata are defined as deduction systems over sets
of constituency trees, they cannot be used to de-
scribe dependency parsers.
In this paper, we define an analogous formalism
that can be used to define, analyze and compare de-
pendency parsers. We use this framework to provide
uniform, high-level descriptions for a wide range of
well-known algorithms described in the literature,
and we show how they formally relate to each other
and how we can use these relations and the formal-
ism itself to prove their correctness.
1.1 Parsing schemata
Parsing schemata (Sikkel, 1997) provide a formal,
simple and uniform way to describe, analyze and
compare different constituency-based parsers.
The notion of a parsing schema comes from con-
sidering parsing as a deduction process which gener-
ates intermediate results called items. An initial set
of items is directly obtained from the input sentence,
and the parsing process consists of the application of
inference rules (deduction steps) which produce new
items from existing ones. Each item contains a piece
of information about the sentence?s structure, and a
successful parsing process will produce at least one
final item containing a full parse tree for the sentence
or guaranteeing its existence.
Items in parsing schemata are formally defined
as sets of partial parse trees from a set denoted
Trees(G), which is the set of all the possible par-
tial parse trees that do not violate the constraints im-
posed by a grammar G. More formally, an item set
I is defined by Sikkel as a quotient set associated
with an equivalence relation on Trees(G).1
Valid parses for a string are represented by
items containing complete marked parse trees for
that string. Given a context-free grammar G =
1While Shieber et al (1995) also view parsers as deduction
systems, Sikkel formally defines items and related concepts,
providing the mathematical tools to reason about formal prop-
erties of parsers.
968
(N,?, P, S), a marked parse tree for a string
w1 . . . wn is any tree ? ? Trees(G)/root(?) =
S?yield(?) = w1 . . . wn2. An item containing such
a tree for some arbitrary string is called a final item.
An item containing such a tree for a particular string
w1 . . . wn is called a correct final item for that string.
For each input string, a parsing schema?s deduc-
tion steps allow us to infer a set of items, called valid
items for that string. A parsing schema is said to
be sound if all valid final items it produces for any
arbitrary string are correct for that string. A pars-
ing schema is said to be complete if all correct fi-
nal items are valid. A correct parsing schema is one
which is both sound and complete. A correct parsing
schema can be used to obtain a working implemen-
tation of a parser by using deductive engines such
as the ones described by Shieber et al (1995) and
Go?mez-Rodr??guez et al (2007) to obtain all valid fi-
nal items.
2 Dependency parsing schemata
Although parsing schemata were initially defined for
context-free parsers, they can be adapted to different
constituency-based grammar formalisms, by finding
a suitable definition of Trees(G) for each particular
formalism and a way to define deduction steps from
its rules. However, parsing schemata are not directly
applicable to dependency parsing, since their formal
framework is based on constituency trees.
In spite of this problem, many of the dependency
parsers described in the literature are constructive,
in the sense that they proceed by combining smaller
structures to form larger ones until they find a com-
plete parse for the input sentence. Therefore, it
is possible to define a variant of parsing schemata,
where these structures can be defined as items and
the strategies used for combining them can be ex-
pressed as inference rules. However, in order to de-
fine such a formalism we have to tackle some issues
specific to dependency parsers:
? Traditional parsing schemata are used to de-
fine grammar-based parsers, in which the parsing
process is guided by some set of rules which are
used to license deduction steps: for example, an
Earley Predictor step is tied to a particular gram-
mar rule, and can only be executed if such a rule
exists. Some dependency parsers are also grammar-
2wi is shorthand for the marked terminal (wi, i). These are
used by Sikkel (1997) to link terminal symbols to string posi-
tions so that an input sentence can be represented as a set of
trees which are used as initial items (hypotheses) for the de-
duction system. Thus, a sentence w1 . . . wn produces a set of
hypotheses {{w1(w1)}, . . . , {wn(wn)}}.
Figure 1: Representation of a dependency structure with
a tree. The arrows below the words correspond to its as-
sociated dependency graph.
based: for example, those described by Lombardo
and Lesmo (1996), Barbero et al (1998) and Ka-
hane et al (1998) are tied to the formalizations of de-
pendency grammar using context-free like rules de-
scribed by Hays (1964) and Gaifman (1965). How-
ever, many of the most widely used algorithms (Eis-
ner, 1996; Yamada and Matsumoto, 2003) do not use
a formal grammar at all. In these, decisions about
which dependencies to create are taken individually,
using probabilistic models (Eisner, 1996) or classi-
fiers (Yamada and Matsumoto, 2003). To represent
these algorithms as deduction systems, we use the
notion of D-rules (Covington, 1990). D-rules take
the form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we obtain
a representation of the semantics of these parsing
strategies that is independent of the particular model
used to take the decisions associated with each D-
rule.
? The fundamental structures in dependency pars-
ing are dependency graphs. Therefore, as items
for constituency parsers are defined as sets of par-
tial constituency trees, it is tempting to define items
for dependency parsers as sets of partial dependency
graphs. However, predictive grammar-based algo-
rithms such as those of Lombardo and Lesmo (1996)
and Kahane et al (1998) have operations which pos-
tulate rules and cannot be defined in terms of depen-
dency graphs, since they do not do any modifications
to the graph. In order to make the formalism general
enough to include these parsers, we define items in
terms of sets of partial dependency trees as shown in
Figure 1. Note that a dependency graph can always
be extracted from such a tree.
? Some of the most popular dependency parsing
algorithms, like that of Eisner (1996), work by con-
necting spans which can represent disconnected de-
pendency graphs. Such spans cannot be represented
by a single dependency tree. Therefore, our formal-
ism allows items to be sets of forests of partial de-
pendency trees, instead of sets of trees.
969
Taking these considerations into account, we de-
fine the concepts that we need to describe item sets
for dependency parsers:
Let ? be an alphabet of terminal symbols.
Partial dependency trees: We define the set of
partial dependency trees (D-trees) as the set of finite
trees where children of each node have a left-to-right
ordering, each node is labelled with an element of
??(??N), and the following conditions hold:
? All nodes labelled with marked terminals wi ?
(??N) are leaves,
? Nodes labelled with terminals w ? ? do not have
more than one daughter labelled with a marked
terminal, and if they have such a daughter node, it
is labelled wi for some i ? N,
? Left siblings of nodes labelled with a marked ter-
minal wk do not have any daughter labelled wj
with j ? k. Right siblings of nodes labelled with
a marked terminal wk do not have any daughter
labelled wj with j ? k.
We denote the root node of a partial dependency
tree t as root(t). If root(t) has a daughter node la-
belled with a marked terminal wh, we will say that
wh is the head of the tree t, denoted by head(t). If
all nodes labelled with terminals in t have a daughter
labelled with a marked terminal, t is grounded.
Relationship between trees and graphs: Let
t ? D-trees be a partial dependency tree; g(t), its
associated dependency graph, is a graph (V,E)
? V ={wi ? (??N) | wi is the label of a node in
t},
? E ={(wi, wj) ? (??N)2 | C,D are nodes in t
such that D is a daughter of C, wj the label of a
daughter of C, wi the label of a daughter of D}.
Projectivity: A partial dependency tree t ?
D-trees is projective iff yield(t) cannot be written
as . . . wi . . . wj . . . where i ? j.
It is easy to verify that the dependency graph
g(t) is projective with respect to the linear order of
marked terminals wi, according to the usual defi-
nition of projectivity found in the literature (Nivre,
2006), if and only if the tree t is projective.
Parse tree: A partial dependency tree t ?
D-trees is a parse tree for a given string w1 . . . wn
if its yield is a permutation of w1 . . . wn. If its yield
is exactly w1 . . . wn, we will say it is a projective
parse tree for the string.
Item set: Let ? ? D-trees be the set of de-
pendency trees which are acceptable according to a
given grammar G (which may be a grammar of D-
rules or of CFG-like rules, as explained above). We
define an item set for dependency parsing as a set
I ? ?, where ? is a partition of 2?.
Once we have this definition of an item set for
dependency parsing, the remaining definitions are
analogous to those in Sikkel?s theory of constituency
parsing (Sikkel, 1997), so we will not include them
here in full detail. A dependency parsing system is
a deduction system (I, H,D) where I is a depen-
dency item set as defined above, H is a set contain-
ing initial items or hypotheses, and D ? (2(H?I) ?
I) is a set of deduction steps defining an inference
relation `.
Final items in this formalism will be those con-
taining some forest F containing a parse tree for
some arbitrary string. An item containing such a tree
for a particular string w1 . . . wn will be called a cor-
rect final item for that string in the case of nonprojec-
tive parsers. When defining projective parsers, cor-
rect final items will be those containing projective
parse trees for w1 . . . wn. This distinction is relevant
because the concepts of soundness and correctness
of parsing schemata are based on correct final items
(cf. section 1.1), and we expect correct projective
parsers to produce only projective structures, while
nonprojective parsers should find all possible struc-
tures including nonprojective ones.
3 Some practical examples
3.1 Col96 (Collins, 96)
One of the most straightforward projective depen-
dency parsing strategies is the one described by
Collins (1996), directly based on the CYK pars-
ing algorithm. This parser works with dependency
trees which are linked to each other by creating
links between their heads. Its item set is defined as
ICol96 = {[i, j, h] | 1 ? i ? h ? j ? n}, where an
item [i, j, h] is defined as the set of forests containing
a single projective dependency tree t such that t is
grounded, yield(t) = wi . . . wj and head(t) = wh.
For an input string w1 . . . wn, the set of hypothe-
ses is H = {[i, i, i] | 0 ? i ? n + 1}, i.e., the set
of forests containing a single dependency tree of the
form wi(wi). This same set of hypotheses can be
used for all the parsers, so we will not make it ex-
plicit for subsequent schemata.3
The set of final items is {[1, n, h] | 1 ? h ? n}:
these items trivially represent parse trees for the in-
put sentence, where wh is the sentence?s head. The
deduction steps are shown in Figure 2.
3Note that the words w0 and wn+1 used in the definition do
not appear in the input: these are dummy terminals that we will
call beginning of sentence (BOS) and end of sentence (EOS)
marker, respectively; and will be needed by some parsers.
970
Col96 (Collins,96):
R-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h2]
wh1 ? wh2
L-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h1]
wh2 ? wh1
Eis96 (Eisner, 96):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1, F, F ]
R-Link [i, j, F, F ][i, j, T, F ] wi ? wj
L-Link [i, j, F, F ][i, j, F, T ] wj ? wi
CombineSpans
[i, j, b, c]
[j, k, not(c), d]
[i, k, b, d]
ES99 (Eisner and Satta, 99):
R-Link [i, j, i] [j + 1, k, k][i, k, k] wi ? wk
L-Link [i, j, i] [j + 1, k, k][i, k, i] wk ? wi
R-Combiner [i, j, i] [j, k, j][i, k, i]
L-Combiner [i, j, j] [j, k, k][i, k, k]
YM03 (Yamada and Matsumoto, 2003):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1]
R-Link
[i, j]
[j, k]
[i, k] wj ? wk L-Link
[i, j]
[j, k]
[i, k] wj ? wi
LL96 (Lombardo and Lesmo, 96):
Initter [(.S), 1, 0] ?(S)?P Predictor
[A(?.B?), i, j]
[B(.?), j + 1, j] B(?)?P
Scanner [A(?. ? ?), i, h? 1] [h, h, h][A(? ? .?), i, h] wh IS A
Completer [A(?.B?), i, j] [B(?.), j + 1, k][A(?B.?), i, k]
Figure 2: Deduction steps of the parsing schemata for some well-known dependency parsers.
As we can see, we use D-rules as side conditions
for deduction steps, since this parsing strategy is not
grammar-based. Conceptually, the schema we have
just defined describes a recogniser: given a set of D-
rules and an input string wi . . . wn, the sentence can
be parsed (projectively) under those D-rules if and
only if this deduction system can infer a correct final
item. However, when executing this schema with a
deductive engine, we can recover the parse forest by
following back pointers in the same way as is done
with constituency parsers (Billot and Lang, 1989).
Of course, boolean D-rules are of limited interest
in practice. However, this schema provides a formal-
ization of a parsing strategy which is independent
of the way linking decisions are taken in a partic-
ular implementation. In practice, statistical models
can be used to decide whether a step linking words
a and b (i.e., having a ? b as a side condition) is
executed or not, and probabilities can be attached to
items in order to assign different weights to different
analyses of the sentence. The same principle applies
to the rest of D-rule-based parsers described in this
paper.
3.2 Eis96 (Eisner, 96)
By counting the number of free variables used in
each deduction step of Collins? parser, we can con-
clude that it has a time complexity of O(n5). This
complexity arises from the fact that a parentless
word (head) may appear in any position in the par-
tial results generated by the parser; the complexity
can be reduced to O(n3) by ensuring that parentless
words can only appear at the first or last position
of an item. This is the principle behind the parser
defined by Eisner (1996), which is still in wide use
today (Corston-Oliver et al, 2006; McDonald et al,
2005a).
The item set for Eisner?s parsing schema is
IEis96 = {[i, j, T, F ] | 0 ? i ? j ? n} ?
{[i, j, F, T ] | 0 ? i ? j ? n} ? {[i, j, F, F ] |
0 ? i ? j ? n}, where each item [i, j, T, F ] is de-
fined as the item [i, j, j] ? ICol96, each item
[i, j, F, T ] is defined as the item [i, j, i] ? ICol96,
and each item [i, j, F, F ] is defined as the set
of forests of the form {t1, t2} such that t1 and
t2 are grounded, head(t1) = wi, head(t2) = wj ,
and ?k ? N(i ? k < j)/yield(t1) = wi . . . wk ?
yield(t2) = wk+1 . . . wj .
Note that the flags b, c in an item [i, j, b, c] indi-
cate whether the words in positions i and j, respec-
tively, have a parent in the item or not. Items with
one of the flags set to T represent dependency trees
where the word in position i or j is the head, while
items with both flags set to F represent pairs of trees
headed at positions i and j, and therefore correspond
to disconnected dependency graphs.
Deduction steps4 are shown in Figure 2. The
set of final items is {[0, n, F, T ]}. Note that these
items represent dependency trees rooted at the BOS
marker w0, which acts as a ?dummy head? for the
sentence. In order for the algorithm to parse sen-
tences correctly, we will need to define D-rules to
allow w0 to be linked to the real sentence head.
3.3 ES99 (Eisner and Satta, 99)
Eisner and Satta (1999) define an O(n3) parser for
split head automaton grammars that can be used
4Alternatively, we could consider items of the form [i, i +
1, F, F ] to be hypotheses for this parsing schema, so we would
not need an Initter step. However, we have chosen to use a stan-
dard set of hypotheses valid for all parsers because this allows
for more straightforward proofs of relations between schemata.
971
for dependency parsing. This algorithm is con-
ceptually simpler than Eis96, since it only uses
items representing single dependency trees, avoid-
ing items of the form [i, j, F, F ]. Its item set is
IES99 = {[i, j, i] | 0 ? i ? j ? n} ? {[i, j, j] |
0 ? i ? j ? n}, where items are defined as in
Collins? parsing schema.
Deduction steps are shown in Figure 2, and the set
of final items is {[0, n, 0]}. (Parse trees have w0 as
their head, as in the previous algorithm).
Note that, when described for head automaton
grammars as in Eisner and Satta (1999), this algo-
rithm seems more complex to understand and imple-
ment than the previous one, as it requires four differ-
ent kinds of items in order to keep track of the state
of the automata used by the grammars. However,
this abstract representation of its underlying seman-
tics as a dependency parsing schema shows that this
parsing strategy is in fact conceptually simpler for
dependency parsing.
3.4 YM03 (Yamada and Matsumoto, 2003)
Yamada and Matsumoto (2003) define a determinis-
tic, shift-reduce dependency parser guided by sup-
port vector machines, which achieves over 90% de-
pendency accuracy on section 23 of the Penn tree-
bank. Parsing schemata are not suitable for directly
describing deterministic parsers, since they work at
a high abstraction level where a set of operations
are defined without imposing order constraints on
them. However, many deterministic parsers can be
viewed as particular optimisations of more general,
nondeterministic algorithms. In this case, if we rep-
resent the actions of the parser as deduction steps
while abstracting from the deterministic implemen-
tation details, we obtain an interesting nondetermin-
istic parser.
Actions in Yamada and Matsumoto?s parser create
links between two target nodes, which act as heads
of neighbouring dependency trees. One of the ac-
tions creates a link where the left target node be-
comes a child of the right one, and the head of a
tree located directly to the left of the target nodes
becomes the new left target node. The other ac-
tion is symmetric, performing the same operation
with a right-to-left link. An O(n3) nondetermin-
istic parser generalising this behaviour can be de-
fined by using an item set IY M03 = {[i, j] |
0 ? i ? j ? n + 1}, where each item [i, j] is de-
fined as the item [i, j, F, F ] in IEis96; and the de-
duction steps are shown in Figure 2.
The set of final items is {[0, n + 1]}. In order for
this set to be well-defined, the grammar must have
no D-rules of the form wi ? wn+1, i.e., it must not
allow the EOS marker to govern any words. If this
is the case, it is trivial to see that every forest in an
item of the form [0, n + 1] must contain a parse tree
rooted at the BOS marker and with yield w0 . . . wn.
As can be seen from the schema, this algorithm
requires less bookkeeping than any other of the
parsers described here.
3.5 LL96 (Lombardo and Lesmo, 96) and
other Earley-based parsers
The algorithms in the above examples are based on
taking individual decisions about dependency links,
represented by D-rules. Other parsers, such as that
of Lombardo and Lesmo (1996), use grammars with
context-free like rules which encode the preferred
order of dependents for each given governor, as de-
fined by Gaifman (1965). For example, a rule of the
form N(Det ? PP ) is used to allow N to have Det
as left dependent and PP as right dependent.
The algorithm by Lombardo and Lesmo (1996)
is a version of Earley?s context-free grammar parser
(Earley, 1970) using Gaifman?s dependency gram-
mar, and can be written by using an item set
ILomLes = {[A(?.?), i, j] | A(??) ? P ?
1 ? i ? j ? n}, where each item [A(?.?), i, j] rep-
resents the set of partial dependency trees rooted at
A, where the direct children of A are ??, and the
subtrees rooted at ? have yield wi . . . wj . The de-
duction steps for the schema are shown in Figure 2,
and the final item set is {[(S.), 1, n]}.
As we can see, the schema for Lombardo and
Lesmo?s parser resembles the Earley-style parser in
Sikkel (1997), with some changes to adapt it to de-
pendency grammar (for example, the Scanner al-
ways moves the dot over the head symbol ?).
Analogously, other dependency parsing schemata
based on CFG-like rules can be obtained by mod-
ifying context-free grammar parsing schemata of
Sikkel (1997) in a similar way. The algorithm by
Barbero et al (1998) can be obtained from the left-
corner parser, and the one by Courtin and Genthial
(1998) is a variant of the head-corner parser.
3.6 Pseudo-projectivity
Pseudo-projective parsers can generate non-
projective analyses in polynomial time by using
a projective parsing strategy and postprocessing
the results to establish nonprojective links. For
example, the algorithm by Kahane et al (1998) uses
a projective parsing strategy like that of LL96, but
using the following initializer step instead of the
972
Initter and Predictor:5
Initter [A(?), i, i ? 1] A(?) ? P ? 1 ? i ? n
4 Relations between dependency parsers
The framework of parsing schemata can be used to
establish relationships between different parsing al-
gorithms and to obtain new algorithms from existing
ones, or derive formal properties of a parser (such as
soundness or correctness) from the properties of re-
lated algorithms.
Sikkel (1994) defines several kinds of relations
between schemata, which fall into two categories:
generalisation relations, which are used to obtain
more fine-grained versions of parsers, and filtering
relations, which can be seen as the reverse of gener-
alisation and are used to reduce the number of items
and/or steps needed for parsing. He gives a formal
definition of each kind of relation. Informally, a
parsing schema can be generalised from another via
the following transformations:
? Item refinement: We say that P1 ir?? P2 (P2 is an
item refinement of P1) if there is a mapping be-
tween items in both parsers such that single items
in P1 are broken into multiple items in P2 and in-
dividual deductions are preserved.
? Step refinement: We say that P1 sr?? P2 if the
item set of P1 is a subset of that of P2 and every
single deduction step in P1 can be emulated by a
sequence of inferences in P2.
On the other hand, a schema can be obtained from
another by filtering in the following ways:
? Static/dynamic filtering: P1
sf/df???? P2 if the item
set of P2 is a subset of that of P1 and P2 allows a
subset of the direct inferences in P16.
? Item contraction: The inverse of item refinement.
P1 ic?? P2 if P2 ir?? P1.
? Step contraction: The inverse of step refinement.
P1 sc?? P2 if P2 sr?? P1.
All the parsers described in section 3 can be re-
lated via generalisation and filtering, as shown in
Figure 3. For space reasons we cannot show formal
proofs of all the relations, but we sketch the proofs
for some of the more interesting cases:
5The initialization step as reported in Kahane?s paper is dif-
ferent from this one, as it directly consumes a nonterminal from
the input. However, using this step results in an incomplete
algorithm. The problem can be fixed either by using the step
shown here instead (bottom-up Earley strategy) or by adding an
additional step turning it into a bottom-up Left-Corner parser.
6Refer to Sikkel (1994) for the distinction between static and
dynamic filtering, which we will not use here.
4.1 YM03 sr?? Eis96
It is easy to see from the schema definitions that
IY M03 ? IEis96. In order to prove the relation
between these parsers, we need to verify that every
deduction step in YM03 can be emulated by a se-
quence of inferences in Eis96. In the case of the
Initter step this is trivial, since the Initters of both
parsers are equivalent. If we write the R-Link step in
the notation we have used for Eisner items, we have
R-Link [i, j, F, F ] [j, k, F, F ]
[i, k, F, F ] wj ? wk
This can be emulated in Eisner?s parser by an
R-Link step followed by a CombineSpans step:
[j, k, F, F ] ` [j, k, T, F ] (by R-Link),
[j, k, T, F ], [i, j, F, F ] ` [i, k, F, F ] (by CombineSpans).
Symmetrically, the L-Link step in YM03 can be
emulated by an L-Link followed by a CombineSpans
in Eis96.
4.2 ES99 sr?? Eis96
If we write the R-Link step in Eisner and Satta?s
parser in the notation for Eisner items, we have
R-Link [i, j, F, T ] [j + 1, k, T, F ][i, k, T, F ] wi ? wk
This inference can be emulated in Eisner?s parser
as follows:
` [j, j + 1, F, F ] (by Initter),
[i, j, F, T ], [j, j + 1, F, F ] ` [i, j + 1, F, F ] (CombineSpans),
[i, j + 1, F, F ], [j + 1, k, T, F ] ` [i, k, F, F ] (CombineSpans),
[i, k, F, F ] ` [i, k, T, F ] (by R-Link).
The proof corresponding to the L-Link step is sym-
metric. As for the R-Combiner and L-Combiner
steps in ES99, it is easy to see that they are partic-
ular cases of the CombineSpans step in Eis96, and
therefore can be emulated by a single application of
CombineSpans.
Note that, in practice, the relations in sections 4.1
and 4.2 mean that the ES99 and YM03 parsers are
superior to Eis96, since they generate fewer items
and need fewer steps to perform the same deduc-
tions. These two parsers also have the interesting
property that they use disjoint item sets (one uses
items representing trees while the other uses items
representing pairs of trees); and the union of these
disjoint sets is the item set used by Eis96. Also note
that the optimisation in YM03 comes from contract-
ing deductions in Eis96 so that linking operations
are immediately followed by combining operations;
while ES99 does the opposite, forcing combining
operations to be followed by linking operations.
4.3 Other relations
If we generalise the linking steps in ES99 so that the
head of each item can be in any position, we obtain a
973
Figure 3: Formal relations between several well-known dependency parsers. Arrows going upwards correspond to
generalisation relations, while those going downwards correspond to filtering. The specific subtype of relation is
shown in each arrow?s label, following the notation in Section 4.
correct O(n5) parser which can be filtered to Col96
just by eliminating the Combiner steps.
From Col96, we can obtain an O(n5) head-corner
parser based on CFG-like rules by an item refine-
ment in which each Collins item [i, j, h] is split into
a set of items [A(?.?.?), i, j, h]. Of course, the for-
mal refinement relation between these parsers only
holds if the D-rules used for Collins? parser corre-
spond to the CFG rules used for the head-corner
parser: for every D-rule B ? A there must be a
corresponding CFG-like rule A ? . . . B . . . in the
grammar used by the head-corner parser.
Although this parser uses three indices i, j, h, us-
ing CFG-like rules to guide linking decisions makes
the h indices unnecessary, so they can be removed.
This simplification is an item contraction which re-
sults in an O(n3) head-corner parser. From here,
we can follow the procedure in Sikkel (1994) to
relate this head-corner algorithm to parsers analo-
gous to other algorithms for context-free grammars.
In this way, we can refine the head-corner parser
to a variant of de Vreught and Honig?s algorithm
(Sikkel, 1997), and by successive filters we reach a
left-corner parser which is equivalent to the one de-
scribed by Barbero et al (1998), and a step contrac-
tion of the Earley-based dependency parser LL96.
The proofs for these relations are the same as those
described in Sikkel (1994), except that the depen-
dency variants of each algorithm are simpler (due
to the absence of epsilon rules and the fact that the
rules are lexicalised).
5 Proving correctness
Another useful feature of the parsing schemata
framework is that it provides a formal way to de-
fine the correctness of a parser (see last paragraph
of Section 1.1) which we can use to prove that our
parsers are correct. Furthermore, relations between
schemata can be used to derive the correctness of
a schema from that of related ones. In this sec-
tion, we will show how we can prove that the YM03
and ES99 algorithms are correct, and use that fact to
prove the correctness of Eis96.
5.1 ES99 is correct
In order to prove the correctness of a parser, we must
prove its soundness and completeness (see section
1.1). Soundness is generally trivial to verify, since
we only need to check that every individual deduc-
tion step in the parser infers a correct consequent
item when applied to correct antecedents (i.e., in this
case, that steps always generate non-empty items
that conform to the definition in 3.3). The difficulty
is proving completeness, for which we need to prove
that all correct final items are valid (i.e., can be in-
ferred by the schema). To show this, we will prove
the stronger result that all correct items are valid.
We will show this by strong induction on the
length of items, where the length of an item ? =
[i, k, h] is defined as length(?) = k ? i + 1. Cor-
rect items of length 1 are the hypotheses of the
schema (of the form [i, i, i]) which are trivially valid.
We will prove that, if all correct items of length m
are valid for all 1 ? m < l, then items of length l
are also valid.
Let [i, k, i] be an item of length l in IES99 (thus,
l = k? i+1). If this item is correct, then it contains
a grounded dependency tree t such that yield(t) =
wi . . . wk and head(t) = wi.
By construction, the root of t is labelled wi. Let
wj be the rightmost daughter of wi in t. Since t
is projective, we know that the yield of wj must be
of the form wl . . . wk, where i < l ? j ? k. If
l < j, then wl is the leftmost transitive dependent of
wj in t, and if k > j, then we know that wk is the
rightmost transitive dependent of wj in t.
Let tj be the subtree of t rooted at wj . Let t1 be
the tree obtained from removing tj from t. Let t2 be
974
the tree obtained by removing all the children to the
right of wj from tj , and t3 be the tree obtained by re-
moving all the children to the left of wj from tj . By
construction, t1 belongs to a correct item [i, l? 1, i],
t2 belongs to a correct item [l, j, j] and t3 belongs to
a correct item [j, k, j]. Since these three items have
a length strictly less than l, by the inductive hypoth-
esis, they are valid. This allows us to prove that the
item [i, k, i] is also valid, since it can be obtained
from these valid items by the following inferences:
[i, l ? 1, i], [l, j, j] ` [i, j, i] (by the L-Link step),
[i, j, i], [j, k, j] ` [i, k, i] (by the L-Combiner step).
This proves that all correct items of length l which
are of the form [i, k, i] are correct under the induc-
tive hypothesis. The same can be proved for items of
the form [i, k, k] by symmetric reasoning, thus prov-
ing that the ES99 parsing schema is correct.
5.2 YM03 is correct
In order to prove correctness of this parser, we fol-
low the same procedure as above. Soundness is
again trivial to verify. To prove completeness, we
use strong induction on the length of items, where
the length of an item [i, j] is defined as j ? i + 1.
The induction step is proven by considering any
correct item [i, k] of length l > 2 (l = 2 is the base
case here since items of length 2 are generated by
the Initter step) and proving that it can be inferred
from valid antecedents of length less than l, so it is
valid. To show this, we note that, if l > 2, either
wi has at least a right dependent or wk has at least a
left dependent in the item. Supposing that wi has a
right dependent, if t1 and t2 are the trees rooted at wi
and wk in a forest in [i, k], we call wj the rightmost
daughter of wi and consider the following trees:
v = the subtree of t1 rooted at wj , u1 = the tree ob-
tained by removing v from t1, u2 = the tree obtained
by removing all children to the right of wj from v,
u3 = the tree obtained by removing all children to
the left of wj from v.
We observe that the forest {u1, u2} belongs to the
correct item [i, j], while {u3, t2} belongs to the cor-
rect item [j, k]. From these two items, we can obtain
[i, k] by using the L-Link step. Symmetric reason-
ing can be applied if wi has no right dependents but
wk has at least a left dependent, and analogously to
the case of the previous parser, we conclude that the
YM03 parsing schema is correct.
5.3 Eis96 is correct
By using the previous proofs and the relationships
between schemata that we explained earlier, it is
easy to prove that Eis96 is correct: soundness is,
as always, straightforward, and completeness can be
proven by using the properties of other algorithms.
Since the set of final items in Eis96 and ES99 are
the same, and the former is a step refinement of the
latter, the completeness of ES99 directly implies the
completeness of Eis96.
Alternatively, we can use YM03 to prove the cor-
rectness of Eis96 if we redefine the set of final items
in the latter to be of the form [0, n+ 1, F, F ], which
are equally valid as final items since they always
contain parse trees. This idea can be applied to trans-
fer proofs of completeness across any refinement re-
lation.
6 Conclusions
We have defined a variant of Sikkel?s parsing
schemata formalism which allows us to represent
dependency parsing algorithms in a simple, declar-
ative way7. We have clarified relations between
parsers which were originally described very differ-
ently. For example, while Eisner presented his algo-
rithm as a dynamic programming algorithm which
combines spans into larger spans, Yamada and Mat-
sumoto?s works by sequentially executing parsing
actions that move a focus point in the input one po-
sition to the left or right, (possibly) creating a de-
pendency link. However, in the parsing schemata
for these algorithms we can see (and formally prove)
that they are related: one is a refinement of the other.
Parsing schemata are also a formal tool that can be
used to prove the correctness of parsing algorithms.
The relationships between dependency parsers can
be exploited to derive properties of a parser from
those of others, as we have seen in several examples.
Although the examples in this paper are cen-
tered in projective dependency parsing, the formal-
ism does not require projectivity and can be used to
represent nonprojective algorithms as well8. An in-
teresting line for future work is to use relationships
between schemata to find nonprojective parsers that
can be derived from existing projective counterparts.
7An alternative framework that formally describes some de-
pendency parsers is that of transition systems (McDonald and
Nivre, 2007). This model is based on parser configurations and
transitions, and has no clear relationship with the approach de-
scribed here.
8Note that spanning tree parsing algorithms based on edge-
factored models, such as the one by McDonald et al (2005b)
are not constructive in the sense outlined in Section 2, so the
approach described here does not directly apply to them. How-
ever, other nonprojective parsers such as (Attardi, 2006) follow
a constructive approach and can be analysed deductively.
975
References
Miguel A. Alonso, Eric de la Clergerie, David Cabrero,
and Manuel Vilares. 1999. Tabular algorithms for
TAG parsing. In Proc. of the Ninth Conference on Eu-
ropean chapter of the Association for Computational
Linguistics, pages 150?157, Bergen, Norway. ACL.
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc. of
the Tenth Conference on Natural Language Learning
(CoNLL-X), pages 166?170, New York, USA. ACL.
Cristina Barbero, Leonardo Lesmo, Vincenzo Lombarlo,
and Paola Merlo. 1998. Integration of syntactic
and lexical information in a hierarchical dependency
grammar. In Proc. of the Workshop on Dependency
Grammars, pages 58?67, ACL-COLING, Montreal,
Canada.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forest in ambiguous parsing. In Proc. of the
27th Annual Meeting of the Association for Computa-
tional Linguistics, pages 143?151, Vancouver, British
Columbia, Canada, June. ACL.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc. of
the 34th annual meeting on Association for Compu-
tational Linguistics, pages 184?191, Morristown, NJ,
USA. ACL.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proc. of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Association
of Computational Linguistics, pages 160?167, Morris-
town, NJ, USA. ACL.
Jacques Courtin and Damien Genthial. 1998. Parsing
with dependency relations and robust parsing. In Proc.
of the Workshop on Dependency Grammars, pages 88?
94, ACL-COLING, Montreal, Canada.
Michael A. Covington. 1990. A dependency parser for
variable-word-order languages. Technical Report AI-
1990-01, Athens, GA.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 457?464, Mor-
ristown, NJ, USA. ACL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of the
16th International Conference on Computational Lin-
guistics (COLING-96), pages 340?345, Copenhagen,
August.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, 8:304?
337.
Carlos Go?mez-Rodr??guez, Jesu?s Vilares, and Miguel A.
Alonso. 2007. Compiling declarative specifications
of parsing algorithms. In Database and Expert Sys-
tems Applications, volume 4653 of Lecture Notes in
Computer Science, pages 529?538, Springer-Verlag.
David Hays. 1964. Dependency theory: a formalism and
some observations. Language, 40:511?525.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646?652.
Vincenzo Lombardo and Leonardo Lesmo. 1996. An
Earley-type recognizer for dependency grammar. In
Proc. of the 16th conference on Computational linguis-
tics, pages 723?728, Morristown, NJ, USA. ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In ACL ?05: Proc. of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
91?98, Morristown, NJ, USA. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan
Hajic?. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In HLT ?05: Proc. of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proc. of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 122?131.
Joakim Nivre. 2006. Inductive Dependency Parsing
(Text, Speech and Language Technology). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3?
36.
Klaas Sikkel. 1994. How to compare the structure of
parsing algorithms. In G. Pighizzini and P. San Pietro,
editors, Proc. of ASMICS Workshop on Parsing The-
ory. Milano, Italy, Oct 1994, pages 21?39.
Klaas Sikkel. 1997. Parsing Schemata ? A Framework
for Specification and Analysis of Parsing Algorithms.
Texts in Theoretical Computer Science ? An EATCS
Series. Springer-Verlag, Berlin/Heidelberg/New York.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of 8th International Workshop on Parsing Tech-
nologies, pages 195?206.
976
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 985?993,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Optimal-Time Binarization Algorithm
for Linear Context-Free Rewriting Systems with Fan-Out Two
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
Linear context-free rewriting systems
(LCFRSs) are grammar formalisms with
the capability of modeling discontinu-
ous constituents. Many applications use
LCFRSs where the fan-out (a measure of
the discontinuity of phrases) is not allowed
to be greater than 2. We present an ef-
ficient algorithm for transforming LCFRS
with fan-out at most 2 into a binary form,
whenever this is possible. This results
in asymptotical run-time improvement for
known parsing algorithms for this class.
1 Introduction
Since its early years, the computational linguistics
field has devoted much effort to the development
of formal systems for modeling the syntax of nat-
ural language. There has been a considerable in-
terest in rewriting systems that enlarge the generat-
ive power of context-free grammars, still remain-
ing far below the power of the class of context-
sensitive grammars; see (Joshi et al, 1991) for dis-
cussion. Following this line, (Vijay-Shanker et al,
1987) have introduced a formalism called linear
context-free rewriting systems (LCFRSs) that has
received much attention in later years by the com-
munity.
LCFRSs allow the derivation of tuples of
strings,1 i.e., discontinuous phrases, that turn out
to be very useful in modeling languages with rel-
atively free word order. This feature has recently
been used for mapping non-projective depend-
ency grammars into discontinuous phrase struc-
tures (Kuhlmann and Satta, 2009). Furthermore,
LCFRSs also implement so-called synchronous
1In its more general definition, an LCFRS provides a
framework where abstract structures can be generated, as for
instance trees and graphs. Throughout this paper we focus on
so-called string-based LCFRSs, where rewriting is defined
over strings only.
rewriting, up to some bounded degree, and have
recently been exploited, in some syntactic vari-
ant, in syntax-based machine translation (Chiang,
2005; Melamed, 2003) as well as in the modeling
of syntax-semantic interface (Nesson and Shieber,
2006).
The maximum number f of tuple components
that can be generated by an LCFRS G is called
the fan-out of G, and the maximum number r of
nonterminals in the right-hand side of a production
is called the rank of G. As an example, context-
free grammars are LCFRSs with f = 1 and r
given by the maximum length of a production
right-hand side. Tree adjoining grammars (Joshi
and Levy, 1977), or TAG for short, can be viewed
as a special kind of LCFRS with f = 2, since
each elementary tree generates two strings, and r
given by the maximum number of adjunction sites
in an elementary tree.
Several parsing algorithms for LCFRS or equi-
valent formalisms are found in the literature; see
for instance (Seki et al, 1991; Boullier, 2004; Bur-
den and Ljunglo?f, 2005). All of these algorithms
work in time O(|G| ? |w|f ?(r+1)). Parsing time is
then exponential in the input grammar size, since
|G| depends on both f and r. In the develop-
ment of efficient algorithms for parsing based on
LCFRS the crucial goal is therefore to optimize
the term f ? (r + 1).
In practical natural language processing applic-
ations the fan-out of the grammar is typically
bounded by some small number. As an example,
in the case of discontinuous parsing discussed
above, we have f = 2 for most practical cases.
On the contrary, LCFRS productions with a rel-
atively large number of nonterminals are usually
observed in real data. The reduction of the rank of
a LCFRS, called binarization, is a process very
similar to the reduction of a context-free grammar
into Chomsky normal form. While in the special
case of CFG and TAG this can always be achieved,
985
binarization of an LCFRS requires, in the gen-
eral case, an increase in the fan-out of the gram-
mar much larger than the achieved reduction in
the rank. Worst cases and some lower bounds have
been discussed in (Rambow and Satta, 1999; Satta,
1998).
Nonetheless, in many cases of interest binariza-
tion of an LCFRS can be carried out without any
extra increase in the fan-out. As an example, in
the case where f = 2, binarization of a LCFRS
would result in parsing time of O(|G| ? |w|6).
With the motivation of parsing efficiency, much
research has been recently devoted to the design
of efficient algorithms for rank reduction, in cases
in which this can be carried out at no extra increase
in the fan-out. (Go?mez-Rodr??guez et al, 2009) re-
ports a general binarization algorithm for LCFRS.
In the case where f = 2, this algorithm works
in time O(|p|7), where p is the input production.
A more efficient algorithm is presented in (Kuhl-
mann and Satta, 2009), working in time O(|p|) in
case of f = 2. However, this algorithm works
for a restricted typology of productions, and does
not cover all cases in which some binarization is
possible. Other linear time algorithms for rank re-
duction are found in the literature (Zhang et al,
2008), but they are restricted to the case of syn-
chronous context-free grammars, a strict subclass
of the LCFRS with f = 2.
In this paper we focus our attention on LCFRS
with a fan-out of two. We improve upon all
of the above mentioned results, by providing
an algorithm that computes a binarization of an
LCFRS production in all cases in which this is
possible and works in time O(|p|). This is an
optimal result in terms of time complexity, since
?(|p|) is also the size of any output binarization
of an LCFRS production.
2 Linear context-free rewriting systems
We briefly summarize here the terminology and
notation that we adopt for LCFRS; for detailed
definitions, see (Vijay-Shanker et al, 1987). We
denote the set of non-negative integers by N. For
i, j ? N, the interval {k | i ? k ? j} is denoted
by [i, j]. We write [i] as a shorthand for [1, i]. For
an alphabet V , we write V ? for the set of all (fi-
nite) strings over V .
As already mentioned in Section 1, linear
context-free rewriting systems generate tuples of
strings over some finite alphabet. This is done by
associating each production p of a grammar with
a function g that rearranges the string compon-
ents in the tuples generated by the nonterminals
in p?s right-hand side, possibly adding some al-
phabet symbols. Let V be some finite alphabet.
For natural numbers r ? 0 and f, f1, . . . , fr ? 1,
consider a function g : (V ?)f1 ? ? ? ? ? (V ?)fr ?
(V ?)f defined by an equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ~?,
where ~? = ??1, . . . , ?f ? is an f -tuple of strings
over g?s argument variables and symbols in V . We
say that g is linear, non-erasing if ~? contains ex-
actly one occurrence of each argument variable.
We call r and f the rank and the fan-out of g, re-
spectively, and write r(g) and f(g) to denote these
quantities.
A linear context-free rewriting system
(LCFRS) is a tuple G = (VN , VT , P, S), where
VN and VT are finite, disjoint alphabets of nonter-
minal and terminal symbols, respectively. Each
A ? VN is associated with a value f(A), called its
fan-out. The nonterminal S is the start symbol,
with f(S) = 1. Finally, P is a set of productions
of the form
p : A? g(A1, A2, . . . , Ar(g)) ,
where A,A1, . . . , Ar(g) ? VN , and g : (V
?
T )
f(A1)
? ? ? ?? (V ?T )
f(Ar(g)) ? (V ?T )
f(A) is a linear, non-
erasing function.
A production p of G can be used to transform
a sequence of r(g) string tuples generated by the
nonterminals A1, . . . , Ar(g) into a tuple of f(A)
strings generated by A. The values r(g) and f(g)
are called the rank and fan-out of p, respectively,
written r(p) and f(p). The rank and fan-out of G,
written r(G) and f(G), respectively, are the max-
imum rank and fan-out among all of G?s produc-
tions. Given that f(S) = 1, S generates a set of
strings, defining the language of G.
Example 1 Consider the LCFRS G defined by
the productions
p1 : S ? g1(A), g1(?x1,1, x1,2?) = ?x1,1x1,2?
p2 : A? g2(A), g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
p3 : A? g3(), g3() = ??, ??
We have f(S) = 1, f(A) = f(G) = 2, r(p3) = 0
and r(p1) = r(p2) = r(G) = 1. G generates
the string language {anbncndn |n ? N}. For in-
stance, the string a3b3c3d3 is generated by means
986
of the following bottom-up process. First, the
tuple ??, ?? is generated by A through p3. We
then iterate three times the application of p2 to
??, ??, resulting in the tuple ?a3b3, c3d3?. Finally,
the tuple (string) ?a3b3c3d3? is generated by S
through application of p1. 2
3 Position sets and binarizations
Throughout this section we assume an LCFRS
production p : A? g(A1, . . . , Ar) with g defined
through a tuple ~? as in section 2. We also assume
that the fan-out ofA and the fan-out of eachAi are
all bounded by two.
3.1 Production representation
We introduce here a specialized representation for
p. Let $ be a fresh symbol that does not occur
in p. We define the characteristic string of p as
the string
?N (p) = ?
?
1$?
?
2$ ? ? ? $?
?
f(A),
where each ??j is obtained from ?j by removing all
the occurrences of symbols in VT . Consider now
some occurrence Ai of a nonterminal symbol in
the right-hand side of p. We define the position set
of Ai, written XAi , as the set of all non-negative
integers j ? [|?N (p)|] such that the j-th symbol in
?N (p) is a variable of the form xi,h for some h.
Example 2 Let p : A ? g(A1, A2, A3), where
g(?x1,1, x1,2?, ?x2,1?, ?x3,1, x3,2?) = ~? with
~? = ?x1,1ax2,1x1,2, x3,1bx3,2? .
We have ?N (p) = x1,1x2,1x1,2$x3,1x3,2, XA1 =
{1, 3}, XA2 = {2} and XA3 = {5, 6}. 2
Each position set X ? [|?N (p)|] can be repres-
ented by means of non-negative integers i1 < i2 <
? ? ? < i2k satisfying
X =
k?
j=1
[i2j?1 + 1, i2j ].
In other words, we are decomposing X into the
union of k intervals, with k as small as possible.
It is easy to see that this decomposition is always
unique. We call set E = {i1, i2, . . . , i2k} the en-
dpoint set associated with X , and we call k the
fan-out of X , written f(X). Throughout this pa-
per, we will represent p as the collection of all
the position sets associated with the occurrences
of nonterminals in its right-hand side.
Let X1 and X2 be two disjoint position sets
(i.e., X1 ? X2 = ?), with f(X1) = k1 and
f(X2) = k2 and with associated endpoint sets E1
and E2, respectively. We define the merge of X1
and X2 as the set X1 ? X2. We extend the po-
sition set and end-point set terminology to these
merge sets as well. It is easy to check that the en-
dpoint set associated to position set X1 ? X2 is
(E1?E2)\ (E1?E2). We say thatX1 andX2 are
2-combinable if f(X1 ?X2) ? 2. We also say
that X1 and X2 are adjacent, written X1 ? X2,
if f(X1 ?X2) ? max(k1, k2). It is not difficult
to see that X1 ? X2 if and only if X1 and X2 are
disjoint and |E1 ? E2| ? min(k1, k2). Note also
that X1 ? X2 always implies that X1 and X2 are
2-combinable (but not the other way around).
Let X be a collection of mutually disjoint posi-
tion sets. A reduction of X is the process of mer-
ging two position sets X1, X2 ? X , resulting in a
new collectionX ? = (X \{X1, X2})?{X1?X2}.
The reduction is 2-feasible if X1 and X2 are 2-
combinable. A binarization of X is a sequence
of reductions resulting in a new collection with
two or fewer position sets. The binarization is
2-feasible if all of the involved reductions are 2-
feasible. Finally, we say that X is 2-feasible if
there exists at least one 2-feasible binarization for
X .
As an important remark, we observe that when
a collection X represents the position sets of all
the nonterminals in the right-hand side of a pro-
duction p with r(p) > 2, then a 2-feasible reduc-
tion merging XAi , XAj ? X can be interpreted
as follows. We replace p by means of a new pro-
duction p? obtained from p by substituting Ai and
Aj with a fresh nonterminal symbol B, so that
r(p?) = r(p) ? 1. Furthermore, we create a new
production p?? with Ai and Aj in its right-hand
side, such that f(p??) = f(B) ? 2 and r(p??) = 2.
Productions p? and p?? together are equivalent to p,
but we have now achieved a local reduction in rank
of one unit.
Example 3 Let p be defined as in example 2 and
let X = {XA1 , XA2 , XA3}. We have that XA1
and XA2 are 2-combinable, and their merge is the
new position set X = XA1 ? XA2 = {1, 2, 3}.
This merge corresponds to a 2-feasible reduction
of X resulting in X ? = {X,XA3}. Such a re-
duction corresponds to the construction of a new
production p? : A? g?(B,A3) with
g?(?x1,1?, ?x3,1, x3,2?) = ?x1,1, x3,1bx3,2? ;
987
and a new production p?? : B ? g??(A1, A2) with
g??(?x1,1, x1,2?, ?x2,1?) = ?x1,1ax2,1x1,2? . 2
It is easy to see that X is 2-feasible if and only
if there exists a binarization of p that does not in-
crease its fan-out.
Example 4 It has been shown in (Rambow
and Satta, 1999) that binarization of an
LCFRS G with f(G) = 2 and r(G) = 3
is always possible without increasing the
fan-out, and that if r(G) ? 4 then this is
no longer true. Consider the LCFRS pro-
duction p : A ? g(A1, A2, A3, A4), with
g(?x1,1, x1,2?, ?x2,1, x2,2?, ?x3,1, x3,2?, ?x4,1, x4,2?) =
~?, ~? = ?x1,1x2,1x3,1x4,1, x2,2x4,2x1,2x3,2?. It is
not difficult to see that replacing any set of two or
three nonterminals in p?s right-hand side forces
the creation of a fresh nonterminal of fan-out
larger than two. 2
3.2 Greedy decision theorem
The binarization algorithm presented in this paper
proceeds by representing each LCFRS production
p as a collection of disjoint position sets, and then
finding a 2-feasible binarization of p. This binariz-
ation is computed deterministically, by an iterative
process that greedily chooses merges correspond-
ing to pairs of adjacent position sets.
The key idea behind the algorithm is based on a
theorem that guarantees that any merge of adjacent
sets preserves the property of 2-feasibility:
Theorem 1 LetX be a 2-feasible collection of po-
sition sets. The reduction of X by merging any
two adjacent position sets D1, D2 ? X results in
a new collection X ? which is 2-feasible.
To prove Theorem 1 we consider that, sinceX is
2-feasible, there must exist at least one 2-feasible
binarization for X . We can write this binariza-
tion ? as a sequence of reductions, where each re-
duction is characterized by a pair of position sets
(X1, X2) which are merged into X1 ?X2, in such
a way that both each of the initial sets and the res-
ult of the merge have fan-out at most 2.
We will show that, under these conditions, for
every pair of adjacent position sets D1 and D2,
there exists a binarization that starts with the re-
duction merging D1 with D2.
Without loss of generality, we assume that
f(D1) ? f(D2) (if this inequality does not hold
we can always swap the names of the two position
sets, since the merging operation is commutative),
and we define a function hD1?D2 : 2
N ? 2N as
follows:
? hD1?D2(X) = X; if D1 * X ?D2 * X .
? hD1?D2(X) = X; if D1 ? X ?D2 ? X .
? hD1?D2(X) = X ?D1; if D1 * X ?D2 ?
X .
? hD1?D2(X) = X \D1; if D1 ? X ?D2 *
X .
With this, we construct a binarization ?? from ?
as follows:
? The first reduction in ?? merges the pair of
position sets (D1, D2),
? We consider the reductions in ? in or-
der, and for each reduction o merging
(X1, X2), if X1 6= D1 and X2 6=
D1, we append a reduction o? merging
(hD1?D2(X1), hD1?D2(X2)) to ?
?.
We will now prove that, if ? is a 2-feasible bin-
arization, then ?? is also a 2-feasible binarization.
To prove this, it suffices to show the following:2
(i) Every position set merged by a reduction in
?? is either one of the original sets in X , or
the result of a previous merge in ??.
(ii) Every reduction in ?? merges a pair of posi-
tion sets (X1, X2) which are 2-combinable.
To prove (i) we note that by construction of ??,
if an operand of a merging operation in ?? is not
one of the original position sets in X , then it must
be an hD1?D2(X) for some X that appears as an
operand of a merging operation in ?. Since the
binarization ? is itself valid, this X must be either
one of the position sets in X , or the result of a
previous merge in the binarization ?. So we divide
the proof into two cases:
? If X ? X : First of all, we note that X can-
not be D1, since the merging operations of ?
that have D1 as an operand do not produce
2It is also necessary to show that no position set is merged
in two different reductions, but this easily follows from the
fact that hD1?D2(X) = hD1?D2(Y ) if and only if X ?
D1 = Y ?D1. Thus, two reductions in ? can only produce
conflicting reductions in ?? if they merge two position sets
differing only by D1, but in this case, one of the reductions
must merge D1 so it does not produce any reduction in ??.
988
a corresponding operation in ??. If X equals
D2, then hD1?D2(X) is D1 ? D2, which is
the result of the first merging operation in ??.
Finally, if X is one of the position sets in X ,
and not D1 or D2, then hD1?D2(X) = X ,
so our operand is also one of the position sets
in X .
? If X is the result of a previous merging oper-
ation o in binarization ?: Then, hD1?D2(X)
is the result of a previous merging operation
o? in binarization ??, which is obtained by ap-
plying the function hD1?D2 to the operands
and result of o. 3
To prove (ii), we show that, under the assump-
tions of the theorem, the function hD1?D2 pre-
serves 2-combinability. Since two position sets of
fan-out ? 2 are 2-combinable if and only if they
are disjoint and the fan-out of their union is at most
2, it suffices to show that, for everyX,X1, X2 uni-
ons of one or more sets of X , having fan-out ? 2,
such that X1 6= D1, X2 6= D1 and X 6= D1;
(a) The function hD1?D2 preserves disjointness,
that is, if X1 and X2 are disjoint, then
hD1?D2(X1) and hD1?D2(X2) are disjoint.
(b) The function hD1?D2 is distributive with
respect to the union of position sets, that
is, hD1?D2(X1 ? X2) = hD1?D2(X1) ?
hD1?D2(X2).
(c) The function hD1?D2 preserves the property
of having fan-out? 2, that is, ifX has fan-out
? 2, then hD1?D2(X) has fan-out ? 2.
If X1 and X2 do not contain D1 or D2, or if
one of the two unionsX1 orX2 containsD1?D2,
properties (a) and (b) are trivial, since the function
hD1?D2 behaves as the identity function in these
cases.
It remains to show that (a) and (b) are true in the
following cases:
? X1 contains D1 but not D2, and X2 does not
contain D1 or D2:
3Except if one of the operands of the operation o was D1.
But in this case, if we call the other operand Z, then we have
that X = D1 ? Z. If Z contains D2, then X = D1 ?
Z = hD1?D2(X) = hD1?D2(Z), so we apply this same
reasoning with hD1?D2(Z) where we cannot fall into this
case, since there can be only one merge operation in ? that
uses D1 as an operand. If Z does not contain D2, then we
have that hD1?D2(X) = X \D1 = Z = hD1?D2(Z), so
we can do the same.
In this case, ifX1 andX2 are disjoint, we can
writeX1 = Y1?D1, such that Y1, X2, D1 are
pairwise disjoint. By definition, we have that
hD1?D2(X1) = Y1, and hD1?D2(X2) =
X2, which are disjoint, so (a) holds.
Property (b) also holds because, with these
expressions for X1 and X2, we can calcu-
late hD1?D2(X1 ? X2) = Y1 ? X2 =
hD1?D2(X1) ? hD1?D2(X2).
? X1 containsD2 but notD1,X2 does not con-
tain D1 or D2:
In this case, if X1 and X2 are disjoint,
we can write X1 = Y1 ? D2, such that
Y1, X2, D1, D2 are pairwise disjoint. By
definition, hD1?D2(X1) = Y1 ? D2 ? D1,
and hD1?D2(X2) = X2, which are disjoint,
so (a) holds.
Property (b) also holds, since we can check
that hD1?D2(X1 ? X2) = Y1 ? X2 ? D2 ?
D1 = hD1?D2(X1) ? hD1?D2(X2).
? X1 contains D1 but not D2, X2 contains D2
but not D1:
In this case, ifX1 andX2 are disjoint, we can
writeX1 = Y1?D1 andX2 = Y2?D2, such
that Y1, Y2, D1, D2 are pairwise disjoint. By
definition, we know that hD1?D2(X1) = Y1,
and hD1?D2(X2) = Y2 ? D1 ? D2, which
are disjoint, so (a) holds.
Finally, property (b) also holds in this case,
since hD1?D2(X1 ?X2) = Y1 ?X2 ?D2 ?
D1 = hD1?D2(X1) ? hD1?D2(X2).
This concludes the proof of (a) and (b).
To prove (c), we consider a position set X ,
union of one or more sets of X , with fan-out ? 2
and such that X 6= D1. First of all, we observe
that if X does not contain D1 or D2, or if it con-
tains D1 ? D2, (c) is trivial, because the function
hD1?D2 behaves as the identity function in this
case. So it remains to prove (c) in the cases where
X contains D1 but not D2, and where X contains
D2 but not D1. In any of these two cases, if we
call E(Y ) the endpoint set associated with an ar-
bitrary position set Y , we can make the following
observations:
1. Since X has fan-out ? 2, E(X) contains at
most 4 endpoints.
2. SinceD1 has fan-out f(D1),E(D1) contains
at most 2f(D1) endpoints.
989
3. SinceD2 has fan-out f(D2),E(D2) contains
at most 2f(D2) endpoints.
4. Since D1 and D2 are adjacent, we know
that E(D1) ? E(D2) contains at least
min(f(D1), f(D2)) = f(D1) endpoints.
5. Therefore, E(D1) \ (E(D1) ? E(D2)) can
contain at most 2f(D1) ? f(D1) = f(D1)
endpoints.
6. On the other hand, sinceX contains only one
of D1 and D2, we know that the endpoints
where D1 is adjacent to D2 must also be en-
dpoints of X , so that E(D1) ? E(D2) ?
E(X). Therefore, E(X)\(E(D1)?E(D2))
can contain at most 4? f(D1) endpoints.
Now, in the case where X contains D1 but not
D2, we know that hD1?D2(X) = X\D1. We cal-
culate a bound for the fan-out ofX\D1 as follows:
we observe that all the endpoints in E(X \ D1)
must be either endpoints of X or endpoints of
D1, since E(X) = (E(X \ D1) ? E(D1)) \
(E(X \ D1) ? E(D1)), so every position that is
in E(X \D1) but not in E(D1) must be in E(X).
But we also observe that E(X \ D1) cannot con-
tain any of the endpoints where D1 is adjacent to
D2 (i.e., the members of E(D1) ? E(D2)), since
X \D1 does not contain D1 or D2. Thus, we can
say that any endpoint of X \D1 is either a mem-
ber of E(D1) \ (E(D1) ? E(D2)), or a member
of E(X) \ (E(D1) ? E(D2)).
Thus, the number of endpoints in E(X \ D1)
cannot exceed the sum of the number of endpoints
in these two sets, which, according to the reason-
ings above, is at most 4 ? f(D1) + f(D1) = 4.
Since E(X \D1) cannot contain more than 4 en-
dpoints, we conclude that the fan-out of X \ D1
is at most 2, so the function hD1?D2 preserves the
property of position sets having fan-out? 2 in this
case.
In the other case, where X contains D2 but not
D1, we follow a similar reasoning: in this case,
hD1?D2(X) = X ? D1. To bound the fan-out
of X ? D1, we observe that all the endpoints in
E(X ?D1) must be either in E(X) or in E(D1),
since E(X ?D1) = (E(X)?E(D1)) \ (E(X)?
E(D1)). But we also know that E(X ?D1) can-
not contain any of the endpoints where D1 is adja-
cent to D2 (i.e., the members of E(D1)?E(D2)),
since X ?D1 contains both D1 and D2. Thus, we
can say that any endpoint of X ? D1 is either a
1: Function BINARIZATION(p)
2: A ? ?; {working agenda}
3: R ? ??; {empty list of reductions}
4: for all i from 1 to r(p) do
5: A ? A? {XAi};
6: while |A| > 2 and A contains two adjacent
position sets do
7: choose X1, X2 ? A such that X1 ? X2;
8: X ? X1 ?X2;
9: A ? (A \ {X1, X2}) ? {X};
10: append (X1, X2) toR;
11: if |A| = 2 then
12: return R;
13: else
14: return fail;
Figure 1: Binarization algorithm for a production
p : A ? g(A1, . . . , Ar(p)). Result is either a list
of reductions or failure.
member of E(D1)\ (E(D1)?E(D2)), or a mem-
ber of E(X) \ (E(D1) ? E(D2)). Reasoning as
in the previous case, we conclude that the fan-out
of X ? D1 is at most 2, so the function hD1?D2
also preserves the property of position sets having
fan-out ? 2 in this case.
This concludes the proof of Theorem 1.
4 Binarization algorithm
Let p : A ? g(A1, . . . , Ar(p)) be a production
with r(p) > 2 from some LCFRS with fan-out
not greater than 2. Recall from Subsection 3.1 that
each occurrence of nonterminal Ai in the right-
hand side of p is represented as a position setXAi .
The specification of an algorithm for finding a 2-
feasible binarization of p is reported in Figure 1.
The algorithm uses an agenda A as a working
set, where all position sets that still need to be pro-
cessed are stored. A is initialized with the posi-
tion sets XAi , 1 ? i ? r(p). At each step in the
algorithm, the size of A represents the maximum
rank among all productions that can be obtained
from the reductions that have been chosen so far in
the binarization process. The algorithm also uses
a list R, initialized as the empty list, where all re-
ductions that are attempted in the binarization pro-
cess are appended.
At each iteration, the algorithm performs a re-
duction by arbitrarily choosing a pair of adjacent
endpoint sets from the agenda and by merging
them. As already discussed in Subsection 3.1, this
990
corresponds to some specific transformation of the
input production p that preserves its generative ca-
pacity and that decreases its rank by one unit.
We stop the iterations of the algorithm when we
reach a state in which there are no more than two
position sets in the agenda. This means that the
binarization process has come to an end with the
reduction of p to a set of productions equivalent
to p and with rank and fan-out at most 2. This
set of productions can be easily constructed from
the output list R. We also stop the iterations in
case no adjacent pair of position sets can be found
in the agenda. If the agenda has more than two
position sets, this means that no binarization has
been found and the algorithm returns a failure.
4.1 Correctness
To prove the correctness of the algorithm in Fig-
ure 1, we need to show that it produces a 2-feasible
binarization of the given production p whenever
such a binarization exists. This is established by
the following theorem:
Theorem 2 LetX be a 2-feasible collection of po-
sition sets, such that the union of all sets in X is a
position set with fan-out ? 2. The procedure:
while ( X contains any pair of adjacent sets
X1, X2 ) reduce X by merging X1 with X2;
always finds a 2-feasible binarization of X .
In order to prove this, the loop invariant is that
X is a 2-feasible set, and that the union of all po-
sition sets in X has fan-out ? 2: reductions can
never change the union of all sets in X , and The-
orem 1 guarantees us that every change to the state
of X maintains 2-feasibility. We also know that
the algorithm eventually finishes, because every
iteration reduces the amount of position sets in X
by 1; and the looping condition will not hold when
the number of sets gets to be 1.
So it only remains to prove that the loop is only
exited if X contains at most two position sets. If
we show this, we know that the sequence of re-
ductions produced by this procedure is a 2-feasible
binarization. Since the loop is exited when X is 2-
feasible but it contains no pair of adjacent position
sets, it suffices to show the following:
Proposition 1 Let X be a 2-feasible collection of
position sets, such that the union of all the sets in
X is a position set with fan-out? 2. IfX has more
than two elements, then it contains at least a pair
of adjacent position sets. 2
Let X be a 2-feasible collection of more than
two position sets. Since X is 2-feasible, we know
that there must be a 2-feasible binarization of X .
Suppose that ? is such a binarization, and let D1
and D2 be the two position sets that are merged in
the first reduction of ?. Since ? is 2-feasible, D1
and D2 must be 2-combinable.
If D1 and D2 are adjacent, our proposition is
true. If they are not adjacent, then, in order to be 2-
combinable, the fan-out of both position sets must
be 1: if any of them had fan-out 2, their union
would need to have fan-out > 2 for D1 and D2
not to be adjacent, and thus they would not be 2-
combinable. Since D1 and D2 have fan-out 1 and
are not adjacent, their sets of endpoints are of the
form {b1, b2} and {c1, c2}, and they are disjoint.
If we call EX the set of endpoints correspond-
ing to the union of all the position sets in X and
ED1D2 = {b1, b2, c1, c2}, we can show that at
least one of the endpoints in ED1D2 does not ap-
pear in EX , since we know that EX can have at
most 4 elements (as the union has fan-out ? 2)
and that it cannot equalED1D2 because this would
mean that X = {D1, D2}, and by hypothesis X
has more than two position sets. If we call this
endpoint x, this means that there must be a posi-
tion set D3 in X , different from D1 and D2, that
has x as one of its endpoints. Since D1 and D2
have fan-out 1, this implies that D3 must be ad-
jacent either to D1 or to D2, so we conclude the
proof.
4.2 Implementation and complexity
We now turn to the computational analysis of the
algorithm in Figure 1. We define the length of an
LCFRS production p, written |p|, as the sum of
the length of all strings ?j in ~? in the definition
of the linear, non-erasing function associated with
p. Since we are dealing with LCFRS of fan-out at
most two, we easily derive that |p| = O(r(p)).
In the implementation of the algorithm it is con-
venient to represent each position set by means of
the corresponding endpoint set. Since at any time
in the computation we are only processing posi-
tion sets with fan-out not greater than two, each
endpoint set will contain at most four integers.
The for-loop at lines 4 and 5 in the algorithm
can be easily implemented through a left-to-right
scan of the characteristic string ?N (p), detecting
the endpoint sets associated with each position set
XAi . This can be done in constant time for each
991
XAi , and thus in linear time in |p|.
At each iteration of the while-loop at lines 6
to 10 we have that A is reduced in size by one
unit. This means that the number of iterations is
bounded by r(p). We will show below that each
iteration of this loop can be executed in constant
time. We can therefore conclude that our binariz-
ation algorithm runs in optimal time O(|p|).
In order to run in constant time each single it-
eration of the while-loop at lines 6 to 10, we need
to perform some additional bookkeeping. We use
two arrays Ve and Va, whose elements are in-
dexed by the endpoints associated with character-
istic string ?N (p), that is, integers i ? [0, |?N (p)|].
For each endpoint i, Ve[i] stores all the endpoint
sets that share endpoint i. Since each endpoint can
be shared by at most two endpoint sets, such a data
structure has sizeO(|p|). If there exists some posi-
tion setX inAwith leftmost endpoint i, then Va[i]
stores all the position sets (represented as endpoint
sets) that are adjacent to X . Since each position
set can be adjacent to at most four other position
sets, such a data structure has size O(|p|). Finally,
we assume we can go back and forth between po-
sition sets in the agenda and their leftmost end-
points.
We maintain arrays Ve and Va through the fol-
lowing simple procedures.
? Whenever a new position set X is added to
A, for each endpoint i of X we add X to
Ve[i]. We also check whether any position set
in Ve[i] other than X is adjacent to X , and
add these position sets to Va[il], where il is
the leftmost end point of X .
? Whenever some position set X is removed
from A, for each endpoint i of X we remove
X from Ve[i]. We also remove all of the posi-
tion sets in Va[il], where il is the leftmost end
point of X .
It is easy to see that, for any position set X which
is added/removed from A, each of the above pro-
cedures can be executed in constant time.
We maintain a set I of integer numbers i ?
[0, |?N (p)|] such that i ? I if and only if Va[i] is
not empty. Then at each iteration of the while-loop
at lines 6 to 10 we pick up some index in I and re-
trieve at Va[i] some pairX,X ? such thatX ? X ?.
Since X,X ? are represented by means of endpoint
sets, we can compute the endpoint set ofX?X ? in
constant time. Removal of X,X ? and addition of
X?X ? in our data structures Ve and Va is then per-
formed in constant time, as described above. This
proves our claim that each single iteration of the
while loop can be executed in constant time.
5 Discussion
We have presented an algorithm for the binariza-
tion of a LCFRS with fan-out 2 that does not in-
crease the fan-out, and have discussed how this
can be applied to improve parsing efficiency in
several practical applications. In the algorithm of
Figure 1, we can modify line 14 to return R even
in case of failure. If we do this, when a binariza-
tion with fan-out ? 2 does not exist the algorithm
will still provide us with a list of reductions that
can be converted into a set of productions equival-
ent to p with fan-out at most 2 and rank bounded
by some rb, with 2 < rb ? r(p). In case rb <
r(p), we are not guaranteed to have achieved an
optimal reduction in the rank, but we can still ob-
tain an asymptotic improvement in parsing time if
we use the new productions obtained in the trans-
formation.
Our algorithm has optimal time complexity,
since it works in linear time with respect to the
input production length. It still needs to be invest-
igated whether the proposed technique, based on
determinization of the choice of the reduction, can
also be used for finding binarizations for LCFRS
with fan-out larger than two, again without in-
creasing the fan-out. However, it seems unlikely
that this can still be done in linear time, since the
problem of binarization for LCFRS in general, i.e.,
without any bound on the fan-out, might not be
solvable in polynomial time. This is still an open
problem; see (Go?mez-Rodr??guez et al, 2009) for
discussion.
Acknowledgments
The first author has been supported by Ministerio
de Educacio?n y Ciencia and FEDER (HUM2007-
66607-C04) and Xunta de Galicia (PGIDIT-
07SIN005206PR, INCITE08E1R104022ES,
INCITE08ENA305025ES, INCITE08PXIB-
302179PR and Rede Galega de Procesamento
da Linguaxe e Recuperacio?n de Informacio?n).
The second author has been partially supported
by MIUR under project PRIN No. 2007TJN-
ZRE 002.
992
References
Pierre Boullier. 2004. Range concatenation grammars.
In H. Bunt, J. Carroll, and G. Satta, editors, New
Developments in Parsing Technology, volume 23 of
Text, Speech and Language Technology, pages 269?
289. Kluwer Academic Publishers.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing lin-
ear context-free rewriting systems. In IWPT05, 9th
International Workshop on Parsing Technologies.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd ACL, pages 263?270.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proc. of the North American Chapter of the Asso-
ciation for Computational Linguistics - Human Lan-
guage Technologies Conference (NAACL?09:HLT),
Boulder, Colorado. To appear.
Aravind K. Joshi and Leon S. Levy. 1977. Constraints
on local descriptions: Local transformations. SIAM
J. Comput., 6(2):272?284.
Aravind K. Joshi, K. Vijay-Shanker, and David Weir.
1991. The convergence of mildly context-sensitive
grammatical formalisms. In P. Sells, S. Shieber, and
T. Wasow, editors, Foundational Issues in Natural
Language Processing. MIT Press, Cambridge MA.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Proc. of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-09), pages 478?486,
Athens, Greece.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In Proceedings of HLT-NAACL
2003.
Rebecca Nesson and Stuart M. Shieber. 2006. Simpler
TAG semantics through synchronization. In Pro-
ceedings of the 11th Conference on Formal Gram-
mar, Malaga, Spain, 29?30 July.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223:87?120.
Giorgio Satta. 1998. Trading independent for syn-
chronized parallelism in finite copying parallel re-
writing systems. Journal of Computer and System
Sciences, 56(1):27?45.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191?
229.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Pro-
ceedings of the 25th Meeting of the Association for
Computational Linguistics (ACL?87).
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In 22nd Inter-
national Conference on Computational Linguistics
(Coling), pages 1081?1088, Manchester, England,
UK.
993
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 103?108,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating XTAG Parsers from Algebraic Specifications?
Carlos Go?mez-Rodr??guez and Miguel A. Alonso
Departamento de Computacio?n
Universidade da Corun?a
Campus de Elvin?a, s/n
15071 A Corun?a, Spain
{cgomezr, alonso}@udc.es
Manuel Vilares
E. S. de Ingenier??a Informa?tica
Universidad de Vigo
Campus As Lagoas, s/n
32004 Ourense, Spain
vilares@uvigo.es
Abstract
In this paper, a generic system that gener-
ates parsers from parsing schemata is ap-
plied to the particular case of the XTAG
English grammar. In order to be able to
generate XTAG parsers, some transforma-
tions are made to the grammar, and TAG
parsing schemata are extended with fea-
ture structure unification support and a
simple tree filtering mechanism. The gen-
erated implementations allow us to study
the performance of different TAG parsers
when working with a large-scale, wide-
coverage grammar.
1 Introduction
Since Tree Adjoining Grammars (TAG) were in-
troduced, several different parsing algorithms for
these grammars have been developed, each with
its peculiar characteristics. Identifying the advan-
tages and disadvantages of each of them is not
trivial, and there are no comparative studies be-
tween them in the literature that work with real-
life, wide coverage grammars. In this paper, we
use a generic tool based on parsing schemata to
generate implementations of several TAG parsers
and compare them by parsing with the XTAG En-
glish Grammar (XTAG, 2001).
The parsing schemata formalism (Sikkel, 1997)
is a framework that allows us to describe parsers in
a simple and declarative way. A parsing schema
? Partially supported by Ministerio de Educacio?n y Cien-
cia and FEDER (TIN2004-07246-C03-01, TIN2004-07246-
C03-02), Xunta de Galicia (PGIDIT05PXIC30501PN,
PGIDIT05PXIC10501PN, PGIDIT05SIN044E and
PGIDIT05SIN059E), and Programa de becas FPU (Mi-
nisterio de Educacio?n y Ciencia). We are grateful to Eric
Villemonte de la Clergerie and Franc?ois Barthelemy for their
help in converting the XTAG grammar to XML.
is a representation of a parsing algorithm as a
set of inference rules which are used to perform
deductions on intermediate results called items.
These items represent sets of incomplete parse
trees which the algorithm can generate. An input
sentence to be analyzed produces an initial set of
items. Additionally, a parsing schema must de-
fine a criterion to determine which items are final,
i.e. which items correspond to complete parses of
the input sentence. If it is possible to obtain a fi-
nal item from the set of initial items by using the
schema?s inference rules (called deductive steps),
then the input sentence belongs to the language de-
fined by the grammar. The parse forest can then be
retrieved from the intermediate items used to infer
the final items, as in (Billot and Lang, 1989).
As an example, we introduce a CYK-based
algorithm (Vijay-Shanker and Joshi, 1985) for
TAG. Given a tree adjoining grammar G =
(VT , VN , S, I, A)1 and a sentence of length n
which we denote by a1 a2 . . . an2, we de-
note by P (G) the set of productions {N? ?
N?1 N
?
2 . . . N?r } such that N? is an inner node of
a tree ? ? (I ? A), and N?1 N
?
2 . . . N?r is the or-
dered sequence of direct children of N? .
The parsing schema for the TAG CYK-based
algorithm (Alonso et al, 1999) is a function that
maps such a grammar G to a deduction system
whose domain is the set of items
{[N? , i, j, p, q, adj]}
verifying that N? is a tree node in an elementary
1Where VT denotes the set of terminal symbols, VN the
set of nonterminal symbols, S the axiom, I the set of initial
trees and A the set of auxiliary trees.
2From now on, we will follow the usual conventions by
which nonterminal symbols are represented by uppercase let-
ters (A, B . . .), and terminals by lowercase letters (a, b . . .).
Greek letters (?, ?...) will be used to represent trees, N? a
node in the tree ?, and R? the root node of the tree ?.
103
tree ? ? (I ? A), i and j (0 ? i ? j) are string
positions, p and q may be undefined or instanti-
ated to positions i ? p ? q ? j (the latter only
when ? ? A), and adj ? {true, false} indi-
cates whether an adjunction has been performed
on node N? .
The positions i and j indicate that a substring
ai+1 . . . aj of the string is being recognized, and
positions p and q denote the substring dominated
by ??s foot node. The final item set would be
{[R?, 0, n,?,?, adj] | ? ? I}
for the presence of such an item would indicate
that there exists a valid parse tree with yield a1 a2
. . . an and rooted at R?, the root of an initial tree;
and therefore there exists a complete parse tree for
the sentence.
A deductive step ?1...?m? ? allows us to infer
the item specified by its consequent ? from those
in its antecedents ?1 . . . ?m. Side conditions (?)
specify the valid values for the variables appearing
in the antecedents and consequent, and may refer
to grammar rules or specify other constraints that
must be verified in order to infer the consequent.
The deductive steps for our CYK-based parser are
shown in figure 1. The steps DScanCYK and D?CYK are
used to start the bottom-up parsing process by rec-
ognizing a terminal symbol for the input string, or
none if we are using a tree with an epsilon node.
The DBinaryCYK step (where the operation p ? p? re-
turns p if p is defined, and p? otherwise) represents
the bottom-up parsing operation which joins two
subtrees into one, and is analogous to one of the
deductive steps of the CYK parser for CFG. The
DUnaryCYK step is used to handle unary branching pro-
ductions. DFootCYK and D
Adj
CYK implement the adjunc-
tion operation, where a tree ? is adjoined into a
node N? ; their side condition ? ? adj(N?) means
that ? must be adjoinable into the node N? (which
involves checking that N? is an adjunction node,
comparing its label to R??s and verifying that no
adjunction constraint disallows the operation). Fi-
nally, the DSubsCYK step implements the substitution
operation in grammars supporting it.
As can be seen from the example, parsing
schemata are simple, high-level descriptions that
convey the fundamental semantics of parsing algo-
rithms while abstracting implementation details:
they define a set of possible intermediate results
and allowed operations on them, but they don?t
specify data structures for storing the results or an
order for the operations to be executed. This high
abstraction level makes schemata useful for defin-
ing, comparing and analyzing parsers in pencil and
paper without worrying about implementation de-
tails. However, if we want to actually execute
the parsers and analyze their results and perfor-
mance in a computer, they must be implemented
in a programming language, making it necessary
to lose the high level of abstraction in order to ob-
tain functional and efficient implementations.
In order to bridge this gap between theory and
practice, we have designed and implemented a
system able to automatically transform parsing
schemata into efficient Java implementations of
their corresponding algorithms. The input to this
system is a simple and declarative representation
of a parsing schema, which is practically equal to
the formal notation that we used previously. For
example, this is the DBinaryCYK deductive step shown
in figure 1 in a format readable by our compiler:
@step CYKBinary
[ Node1 , i , k , p , q , adj1 ]
[ Node2 , k , j , p? , q? , adj2 ]
-------------------------------- Node3 -> Node1 Node2
[ Node3 , i , j , Union(p;p?) , Union(q;q?) , false ]
The parsing schemata compilation technique
used by our system is based on the following fun-
damental ideas (Go?mez-Rodr??guez et al, 2006a):
? Each deductive step is compiled to a Java class
containing code to match and search for an-
tecedent items and generate the corresponding
conclusions from the consequent.
? The step classes are coordinated by a deduc-
tive parsing engine, as the one described in
(Shieber et al, 1995). This algorithm ensures
a sound and complete deduction process, guar-
anteeing that all items that can be generated
from the initial items will be obtained.
? To attain efficiency, an automatic analysis of
the schema is performed in order to create in-
dexes allowing fast access to items. As each
different parsing schema needs to perform dif-
ferent searches for antecedent items, the index
structures we generate are schema-specific. In
this way, we guarantee constant-time access to
items so that the computational complexity of
our generated implementations is never above
the theoretical complexity of the parsers.
? Since parsing schemata have an open notation,
for any mathematical object can potentially
appear inside items, the system includes an ex-
tensibility mechanism which can be used to
define new kinds of objects to use in schemata.
104
DScanCYK =
[a, i, i + 1]
[N? , i, i + 1 | ?,? | false] a = label(N
?) D?CYK = [N? , i, i | ?,? | false] ? = label(N
?)
DUnaryCYK =
[M? , i, j | p, q | adj]
[N? , i, j | p, q] | false] N
? ? M? ? P(?) DBinaryCYK =
[M? , i, k | p, q | adj1],
[P ? , k, j | p?, q? | adj2]
[N? , i, j | p ? p?, q ? q? | false] N
? ? M?P ? ? P(?)
DFootCYK =
[N? , i, j | p, q | false]
[F? , i, j | i, j | false] ? ? adj(N
?) DAdjCYK =
[R? , i?, j? | i, j | adj],
[N? , i, j | p, q | false]
[N? , i?, j? | p, q | true] ? ? adj(N
?)
DSubsCYK =
[R?, i, j | ?,? | adj]
[N? , i, j | ?,? | false] ? ? subs(N
?)
Figure 1: A CYK-based parser for TAG.
2 Generating parsers for the XTAG
grammar
By using parsing schemata as the ones in (Alonso
et al, 1999; Nederhof, 1999) as input to our sys-
tem, we can easily obtain efficient implementa-
tions of several TAG parsing algorithms. In this
section, we describe how we have dealt with the
particular characteristics of the XTAG grammar
in order to make it compatible with our generic
compilation technique; and we also provide em-
pirical results which allow us to compare the per-
formance of several different TAG parsing algo-
rithms in the practical case of the XTAG gram-
mar. It shall be noted that similar comparisons
have been made with smaller grammars, such as
simplified subsets of the XTAG grammar, but not
with the whole XTAG grammar with all its trees
and feature structures. Therefore, our compari-
son provides valuable information about the be-
havior of various parsers on a complete, large-
scale natural language grammar. This behavior
is very different from the one that can be ob-
served on small grammars, since grammar size be-
comes a dominant factor in computational com-
plexity when large grammars like the XTAG are
used to parse relatively small natural language sen-
tences (Go?mez-Rodr??guez et al, 2006b).
2.1 Grammar conversion
The first step we undertook in order to generate
parsers for the XTAG grammar was a full conver-
sion of the grammar to an XML-based format, a
variant of the TAG markup language (TAGML).
In this way we had the grammar in a well-defined
format, easy to parse and modify. During this con-
version, the trees? anchor nodes were duplicated in
order to make our generic TAG parsers allow ad-
junctions on anchor nodes, which is allowed in the
XTAG grammar.
2.2 Feature structure unification
Two strategies may be used in order to take uni-
fication into account in parsing: feature structures
can be unified after parsing or during parsing. We
have compared the two approaches for the XTAG
grammar (see table 1), and the general conclusion
is that unification during parsing performs better
for most of the sentences, although its runtimes
have a larger variance and it performs much worse
for some particular cases.
In order to implement unification during parsing
in our parsing schemata based system, we must ex-
tend our schemata in order to perform unification.
This can be done in the following way:
? Items are extended so that they will hold a fea-
ture structure in addition to the rest of the infor-
mation they include.
? We need to define two operations on feature
structures: the unification operation and the
?keep variables? operation. The ?keep vari-
ables? operation is a transformation on feature
structures that takes a feature structure as an
argument, which may contain features, values,
symbolic variables and associations between
them, and returns a feature structure contain-
ing only the variable-value associations related
to a given elementary tree, ignoring the vari-
ables and values not associated through these
relations, and completely ignoring features.
? During the process of parsing, feature structures
that refer to the same node, or to nodes that are
taking part in a substitution or adjunction and
105
Strategy Mean T. Mean 10% T. Mean 20% 1st Quart. Median 3rd Quart. Std. Dev. Wilcoxon
During 108,270 12,164 7,812 1,585 4,424 9,671 388,010 0.4545After 412,793 10,710 10,019 2,123 9,043 19,073 14,235
Table 1: Runtimes in ms of an Earley-based parser using two different unification strategies: unification
during and after parsing. The following data are shown: mean, trimmed means (10 and 20%), quartiles,
standard deviation, and p-value for the Wilcoxon paired signed rank test (the p-value of 0.4545 indicates
that no statistically significant difference was found between the medians).
are going to collapse to a single node in the final
parse tree, must be unified. For this to be done,
the test that these nodes must unify is added as
a side condition to the steps that must handle
them, and the unification results are included
in the item generated by the consequent. Of
course, considerations about the different role
of the top and bottom feature structures in ad-
junction and substitution must be taken into ac-
count when determining which feature struc-
tures must be unified.
? Feature structures in items must only hold
variable-value associations for the symbolic
variables appearing in the tree to which the
structures refer, for these relationships hold the
information that we need in order to propa-
gate values according to the rules specified in
the unification equations. Variable-value asso-
ciations referring to different elementary trees
are irrelevant when parsing a given tree, and
feature-value and feature-variable associations
are local to a node and can?t be extrapolated to
other nodes, so we won?t propagate any of this
information in items. However, it must be used
locally for unification. Therefore, steps perform
unification by using the information in their an-
tecedent items and recovering complete feature
structures associated to nodes directly from the
grammar, and then use the ?keep-variables? op-
eration to remove the information that we don?t
need in the consequent item.
? In some algorithms, such as CYK, a single de-
ductive step deals with several different elemen-
tary tree nodes that don?t collapse into one in the
final parse tree. In this case, several ?keep vari-
ables? operations must be performed on each
step execution, one for each of these nodes. If
we just unified the information on all the nodes
and called ?keep variables? at the end, we could
propagate information incorrectly.
? In Earley-type algorithms, we must take a de-
cision about how predictor steps handle fea-
ture structures. Two options are possible: one
is propagating the feature structure in the an-
tecedent item to the consequent, and the other is
discarding the feature structure and generating
a consequent whose associated feature structure
is empty. The first option has the advantage that
violations of unification constraints are detected
earlier, thus avoiding the generation of some
items. However, in scenarios where a predic-
tor is applied to several items differing only in
their associated feature structures, this approach
generates several different items while the dis-
carding approach collapses them into a single
consequent item. Moreover, the propagating
approach favors the appearance of items with
more complex feature structures, thus making
unification operations slower. In practice, for
XTAG we have found that these drawbacks of
propagating the structures overcome the advan-
tages, especially in complex sentences, where
the discarding approach performs much better.
2.3 Tree filtering
The full XTAG English grammar contains thou-
sands of elementary trees, so performance is not
good if we use the whole grammar to parse each
sentence. Tree selection filters (Schabes and Joshi,
1991) are used to select a subset of the grammar,
discarding the trees which are known not to be
useful given the words in the input sentence.
To emulate this functionality in our parsing
schema-based system, we have used its exten-
sibility mechanism to define a function Selects-
tree(a,T) that returns true if the terminal symbol a
selects the tree T. The implementation of this func-
tion is a Java method that looks for this informa-
tion in XTAG?s syntactic database. Then the func-
tion is inserted in a filtering step on our schemata:
106
[a, i, j]
[Selected, ?] alpha ? Trees/SELECTS-TREE(A;?)
The presence of an item of the form
[Selected, ?] indicates that the tree ? has
been selected by the filter and can be used for
parsing. In order for the filter to take effect, we
add [Selected, ?] as an antecedent to every step
in our schemata introducing a new tree ? into the
parse (such as initters, substitution and adjoining
steps). In this way we guarantee that no trees that
don?t pass the filter will be used for parsing.
3 Comparing several parsers for the
XTAG grammar
In this section, we make a comparison of several
different TAG parsing algorithms ? the CYK-
based algorithm described at (Vijay-Shanker
and Joshi, 1985), Earley-based algorithms with
(Alonso et al, 1999) and without (Schabes, 1994)
the valid prefix property (VPP), and Nederhof?s
algorithm (Nederhof, 1999) ? on the XTAG En-
glish grammar (release 2.24.2001), by using our
system and the ideas we have explained. The
schemata for these algorithms without unification
support can be found at (Alonso et al, 1999).
These schemata were extended as described in the
previous sections, and used as input to our sys-
tem which generated their corresponding parsers.
These parsers were then run on the test sentences
shown in table 2, obtaining the performance mea-
sures (in terms of runtime and amount of items
generated) that can be seen in table 3. Note that
the sentences are ordered by minimal runtime.
As we can see, the execution times are not as
good as the ones we would obtain if we used
Sarkar?s XTAG distribution parser written in C
(Sarkar, 2000). This is not surprising, since our
parsers have been generated by a generic tool
without knowledge of the grammar, while the
XTAG parser has been designed specifically for
optimal performance in this grammar and uses ad-
ditional information (such as tree usage frequency
data from several corpora, see (XTAG, 2001)).
However, our comparison allows us to draw
conclusions about which parsing algorithms are
better suited for the XTAG grammar. In terms
of memory usage, CYK is the clear winner, since
it clearly generates less items than the other al-
gorithms, and a CYK item doesn?t take up more
memory than an Earley item.
On the other hand, if we compare execution
times, there is not a single best algorithm, since the
performance results depend on the size and com-
plexity of the sentences. The Earley-based algo-
rithm with the VPP is the fastest for the first, ?eas-
ier? sentences, but CYK gives the best results for
the more complex sentences. In the middle of the
two, there are some sentences where the best per-
formance is achieved by the variant of Earley that
doesn?t verify the valid prefix property. Therefore,
in practical cases, we should take into account the
most likely kind of sentences that will be passed
to the parser in order to select the best algorithm.
Nederhof?s algorithm is always the one with the
slowest execution time, in spite of being an im-
provement of the VPP Earley parser that reduces
worst-case time complexity. This is probably be-
cause, when extending the Nederhof schema in
order to support feature structure unification, we
get a schema that needs more unification opera-
tions than Earley?s and has to use items that store
several feature structures. Nederhof?s algorithm
would probably perform better in relation to the
others if we had used the strategy of parsing with-
out feature structures and then performing unifica-
tion on the output parse forest.
4 Conclusions
A generic system that generates parsers from al-
gebraic specifications (parsing schemata) has been
applied to the particular case of the XTAG gram-
mar. In order to be able to generate XTAG parsers,
some transformations were made to the grammar,
and TAG parsing schemata were extended with
feature structure unification support and a simple
tree filtering mechanism.
The generated implementations allow us to
compare the performance of different TAG parsers
when working with a large-scale grammar, the
XTAG English grammar. In this paper, we have
shown the results for four algorithms: a CYK-
based algorithm, Earley-based algorithms with
and without the VPP, and Nederhof?s algorithm.
The result shows that the CYK-based parser is the
least memory-consuming algorithm. By measur-
ing execution time, we find that CYK is the fastest
algorithm for the most complex sentences, but the
Earley-based algorithm with the VPP is the fastest
for simpler cases. Therefore, when choosing a
parser for a practical application, we should take
107
1. He was a cow 9. He wanted to go to the city
2. He loved himself 10. That woman in the city contributed to this article
3. Go to your room 11. That people are not really amateurs at intelectual duelling
4. He is a real man 12. The index is intended to measure future economic performance
5. He was a real man 13. They expect him to cut costs throughout the organization
6. Who was at the door 14. He will continue to place a huge burden on the city workers
7. He loved all cows 15. He could have been simply being a jerk
8. He called up her 16. A few fast food outlets are giving it a try
Table 2: Test sentences.
Sentence Runtimes in milliseconds Items generatedParser Parser
CYK Ear. no VPP Ear. VPP Neder. CYK Ear. no VPP Ear. VPP Neder.
1 2985 750 750 2719 1341 1463 1162 1249
2 3109 1562 1219 6421 1834 2917 2183 2183
3 4078 1547 1406 6828 2149 2893 2298 2304
4 4266 1563 1407 4703 1864 1979 1534 2085
5 4234 1921 1421 4766 1855 1979 1534 2085
6 4485 1813 1562 7782 2581 3587 2734 2742
7 5469 2359 2344 11469 2658 3937 3311 3409
8 7828 4906 3563 15532 4128 8058 4711 4716
9 10047 4422 4016 18969 4931 6968 5259 5279
10 13641 6515 7172 31828 6087 8828 7734 8344
11 16500 7781 15235 56265 7246 12068 13221 13376
12 16875 17109 9985 39132 7123 10428 9810 10019
13 25859 12000 20828 63641 10408 12852 15417 15094
14 54578 35829 57422 178875 20760 31278 40248 47570
15 62157 113532 109062 133515 22115 37377 38824 59603
16 269187 3122860 3315359 68778 152430 173128
Table 3: Runtimes and amount of items generated by different XTAG parsers on several sentences. The
machine used for all the tests was an Intel Pentium 4 / 3.40 GHz, with 1 GB RAM and Sun Java Hotspot
virtual machine (version 1.4.2 01-b06) running on Windows XP. Best results for each sentence are shown
in boldface.
into account the kinds of sentences most likely to
be used as input in order to select the most suitable
algorithm.
References
M. A. Alonso, D. Cabrero, E. de la Clergerie, and M.
Vilares. 1999. Tabular algorithms for TAG parsing.
Proc. of EACL?99, pp. 150?157, Bergen, Norway.
S. Billot and B. Lang. 1989. The structure of shared
forest in ambiguous parsing. Proc. of ACL?89, pp.
143?151, Vancouver, Canada.
C. Go?mez-Rodr??guez, J. Vilares and M. A.
Alonso. 2006. Automatic Generation of
Natural Language Parsers from Declarative
Specifications. Proc. of STAIRS 2006, Riva
del Garda, Italy. Long version available at
http://www.grupocole.org/GomVilAlo2006a long.pdf
C. Go?mez-Rodr??guez, M. A. Alonso and M. Vilares.
2006. On Theoretical and Practical Complexity of
TAG Parsers. Proc. of Formal Grammars 2006,
Malaga, Spain.
M.-J. Nederhof. 1999. The computational complexity
of the correct-prefix property for TAGs. Computa-
tional Linguistics, 25(3):345?360.
A. Sarkar. 2000. Practical experiments in parsing us-
ing tree adjoining grammars. Proc. of TAG+5, Paris.
Y. Schabes and A. K. Joshi. 1991. Parsing with lexi-
calized tree adjoining grammar. In Masaru Tomita,
editor, Current Issues in Parsing Technologies, pp.
25?47. Kluwer Academic Publishers, Norwell.
Y. Schabes. 1994. Left to right parsing of lexical-
ized tree-adjoining grammars. Computational Intel-
ligence, 10(4):506?515.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36.
K. Sikkel. 1997. Parsing Schemata ? A Frame-
work for Specification and Analysis of Parsing Al-
gorithms. Springer-Verlag, Berlin.
K. Vijay-Shanker and A. K. Joshi. 1985. Some com-
putational properties of tree adjoining grammars.
Proc. of ACL?85, pp. 82?93, Chicago, USA.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for english. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
108
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833?841,
Beijing, August 2010
Evaluation of Dependency Parsers on Unbounded Dependencies
Joakim Nivre Laura Rimell Ryan McDonald Carlos Go?mez-Rodr??guez
Uppsala University Univ. of Cambridge Google Inc. Universidade da Corun?a
joakim.nivre@lingfil.uu.se laura.rimell@cl.cam.ac.uk ryanmcd@google.com cgomezr@udc.es
Abstract
We evaluate two dependency parsers,
MSTParser and MaltParser, with respect
to their capacity to recover unbounded de-
pendencies in English, a type of evalu-
ation that has been applied to grammar-
based parsers and statistical phrase struc-
ture parsers but not to dependency parsers.
The evaluation shows that when combined
with simple post-processing heuristics,
the parsers correctly recall unbounded
dependencies roughly 50% of the time,
which is only slightly worse than two
grammar-based parsers specifically de-
signed to cope with such dependencies.
1 Introduction
Though syntactic parsers for English are re-
ported to have accuracies over 90% on the Wall
Street Journal (WSJ) section of the Penn Tree-
bank (PTB) (McDonald et al, 2005; Sagae and
Lavie, 2006; Huang, 2008; Carreras et al, 2008),
broad-coverage parsing is still far from being a
solved problem. In particular, metrics like attach-
ment score for dependency parsers (Buchholz and
Marsi, 2006) and Parseval for constituency parsers
(Black et al, 1991) suffer from being an aver-
age over a highly skewed distribution of differ-
ent grammatical constructions. As a result, in-
frequent yet semantically important construction
types could be parsed with accuracies far below
what one might expect.
This shortcoming of aggregate parsing met-
rics was highlighted in a recent study by Rimell
et al (2009), introducing a new parser evalua-
tion corpus containing around 700 sentences an-
notated with unbounded dependencies in seven
different grammatical constructions. This corpus
was used to evaluate five state-of-the-art parsers
for English, focusing on grammar-based and sta-
tistical phrase structure parsers. For example, in
the sentence By Monday, they hope to have a
sheaf of documents both sides can trust., parsers
should recognize that there is a dependency be-
tween trust and documents, an instance of object
extraction out of a (reduced) relative clause. In the
evaluation, the recall of state-of-the-art parsers on
this kind of dependency varies from a high of 65%
to a low of 1%. When averaging over the seven
constructions in the corpus, none of the parsers
had an accuracy higher than 61%.
In this paper, we extend the evaluation of
Rimell et al (2009) to two dependency parsers,
MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006a), trained on data from the
PTB, converted to Stanford typed dependencies
(de Marneffe et al, 2006), and combined with a
simple post-processor to extract unbounded de-
pendencies from the basic dependency tree. Ex-
tending the evaluation to dependency parsers is of
interest because it sheds light on whether highly
tuned grammars or computationally expensive
parsing formalisms are necessary for extracting
complex linguistic phenomena in practice. Unlike
the best performing grammar-based parsers stud-
ied in Rimell et al (2009), neither MSTParser nor
MaltParser was developed specifically as a parser
for English, and neither has any special mecha-
nism for dealing with unbounded dependencies.
Dependency parsers are also often asymptotically
faster than grammar-based or constituent parsers,
e.g., MaltParser parses sentences in linear time.
Our evaluation ultimately shows that the re-
call of MSTParser and MaltParser on unbounded
dependencies is much lower than the average
(un)labeled attachment score for each system.
Nevertheless, the two dependency parsers are
found to perform only slightly worse than the best
grammar-based parsers evaluated in Rimell et al
833
Each must match Wisman 's "pie" with the fragment that he carries with him
nsubj dobj
prepaux pobjposs've
prep rcmoddobj
nsubjdet
pobj
poss
dobj
a: ObRC
Five things you can do for $ 15,000  or less
pobjnsubjaux
rcmod
num num
prep cc conj
dobj
b: ObRed
They will remain on a lower-priority list that includes 17 other countries
pobjnsubj
aux
rcmod
nsubj
num
prep amod
det
nsubj
c: SbRC
amod
dobj
What you see are self-help projects
nsubj
csubj cop
amod
dobj
dobj d: Free
What effect does a prism have on light
pobjnsubj
aux
det
det prep
dobj
dobj
e: ObQ
Now he felt ready for the many actions he saw spreading out before him
pobj rcmod
prtmod
detprepacompnsubj
nsubj
amod xcompnsubj
prep
pobj
g: SbEmadv
The men were at first puzz led, then angered by the aimless tacking
pobj
cop conj
advmod
detadvmod
prep
det
prep
amod
pobjnsubjpass
f : RNR
Figure 1: Examples of seven unbounded dependency constructions (a?g). Arcs drawn below each sentence represent the
dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation,
with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets.
(2009) and considerably better than the other sta-
tistical parsers in that evaluation. Interestingly,
though the two systems have similar accuracies
overall, there is a clear distinction between the
kinds of errors each system makes, which we ar-
gue is consistent with observations by McDonald
and Nivre (2007).
2 Unbounded Dependency Evaluation
An unbounded dependency involves a word or
phrase interpreted at a distance from its surface
position, where an unlimited number of clause
boundaries may in principle intervene. The
unbounded dependency corpus of Rimell et al
(2009) includes seven grammatical constructions:
object extraction from a relative clause (ObRC),
object extraction from a reduced relative clause
(ObRed), subject extraction from a relative clause
(SbRC), free relatives (Free), object questions
(ObQ), right node raising (RNR), and subject ex-
traction from an embedded clause (SbEm), all
chosen for being relatively frequent and easy to
identify in PTB trees. Examples of the con-
structions can be seen in Figure 1. The evalu-
ation set contains 80 sentences per construction
(which may translate into more than 80 depen-
dencies, since sentences containing coordinations
may have more than one gold-standard depen-
dency), while the development set contains be-
tween 13 and 37 sentences per construction. The
data for ObQ sentences was obtained from various
years of TREC, and for the rest of the construc-
tions from the WSJ (0-1 and 22-24) and Brown
sections of the PTB.
Each sentence is annotated with one or more
gold-standard dependency relations representing
the relevant unbounded dependency. The gold-
standard dependencies are shown as arcs below
the sentences in Figure 1. The format of the de-
pendencies in the corpus is loosely based on the
Stanford typed dependency scheme, although the
evaluation procedure permits alternative represen-
tations and does not require that the parser out-
put match the gold-standard exactly, as long as the
?spirit? of the construction is correct.
The ability to recover unbounded dependencies
is important because they frequently form part of
the basic predicate-argument structure of a sen-
tence. Subject and object dependencies in par-
ticular are crucial for a number of tasks, includ-
ing information extraction and question answer-
ing. Moreover, Rimell et al (2009) show that,
although individual types of unbounded depen-
dencies may be rare, the unbounded dependency
types in the corpus, considered as a class, occur in
as many as 10% of sentences in the PTB.
In Rimell et al (2009), five state-of-the-art
parsers were evaluated for their recall on the gold-
standard dependencies. Three of the parsers were
based on grammars automatically extracted from
the PTB: the C&C CCG parser (Clark and Curran,
2007), the Enju HPSG parser (Miyao and Tsujii,
2005), and the Stanford parser (Klein and Man-
ning, 2003). The two remaining systems were the
834
RASP parser (Briscoe et al, 2006), using a man-
ually constructed grammar and a statistical parse
selection component, and the DCU post-processor
of PTB parsers (Cahill et al, 2004) using the out-
put of the Charniak and Johnson reranking parser
(Charniak and Johnson, 2005). Because of the
wide variation in parser output representations, a
mostly manual evaluation was performed to en-
sure that each parser got credit for the construc-
tions it recovered correctly. The parsers were run
essentially ?out of the box?, meaning that the de-
velopment set was used to confirm input and out-
put formats, but no real tuning was performed. In
addition, since a separate question model is avail-
able for C&C, this was also evaluated on ObQ
sentences. The best overall performers were C&C
and Enju, which is unsurprising since they are
deep parsers based on grammar formalisms de-
signed to recover just such dependencies. The
DCU post-processor performed somewhat worse
than expected, often identifying the existence of
an unbounded dependency but failing to iden-
tify the grammatical class (subject, object, etc.).
RASP and Stanford, although not designed to re-
cover such dependencies, nevertheless recovered
a subset of them. Performance of the parsers also
varied widely across the different constructions.
3 Dependency Parsers
In this paper we repeat the study of Rimell et al
(2009) for two dependency parsers, with the goal
of evaluating how parsers based on dependency
grammars perform on unbounded dependencies.
MSTParser1 is a freely available implementa-
tion of the parsing models described in McDon-
ald (2006). According to the categorization of
parsers in Ku?bler et al (2008) it is a graph-based
parsing system in that core parsing algorithms can
be equated to finding directed maximum span-
ning trees (either projective or non-projective)
from a dense graph representation of the sentence.
Graph-based parsers typically rely on global train-
ing and inference algorithms, where the goal is to
learn models in which the weight/probability of
correct trees is higher than that of incorrect trees.
At inference time a global search is run to find the
1http://mstparser.sourceforge.net
highest weighted dependency tree. Unfortunately,
global inference and learning for graph-based de-
pendency parsing is typically NP-hard (McDonald
and Satta, 2007). As a result, graph-based parsers
(including MSTParser) often limit the scope of
their features to a small number of adjacent arcs
(usually two) and/or resort to approximate infer-
ence (McDonald and Pereira, 2006).
MaltParser2 is a freely available implementa-
tion of the parsing models described in Nivre et
al. (2006a) and Nivre et al (2006b). MaltParser is
categorized as a transition-based parsing system,
characterized by parsing algorithms that produce
dependency trees by transitioning through abstract
state machines (Ku?bler et al, 2008). Transition-
based parsers learn models that predict the next
state given the current state of the system as well
as features over the history of parsing decisions
and the input sentence. At inference time, the
parser starts in an initial state, then greedily moves
to subsequent states ? based on the predictions of
the model ? until a termination state is reached.
Transition-based parsing is highly efficient, with
run-times often linear in sentence length. Further-
more, transition-based parsers can easily incorpo-
rate arbitrary non-local features, since the current
parse structure is fixed by the state. However, the
greedy nature of these systems can lead to error
propagation if early predictions place the parser
in incorrect states.
McDonald and Nivre (2007) compared the ac-
curacy of MSTParser and MaltParser along a
number of structural and linguistic dimensions.
They observed that, though the two parsers ex-
hibit indistinguishable accuracies overall, MST-
Parser tends to outperform MaltParser on longer
dependencies as well as those dependencies closer
to the root of the tree (e.g., verb, conjunction and
preposition dependencies), whereas MaltParser
performs better on short dependencies and those
further from the root (e.g., pronouns and noun de-
pendencies). Since long dependencies and those
near to the root are typically the last constructed
in transition-based parsing systems, it was con-
cluded that MaltParser does suffer from some
form of error propagation. On the other hand, the
2http://www.maltparser.org
835
richer feature representations of MaltParser led to
improved performance in cases where error prop-
agation has not occurred. However, that study did
not investigate unbounded dependencies.
4 Methodology
In this section, we describe the methodological
setup for the evaluation, including parser training,
post-processing, and evaluation.3
4.1 Parser Training
One important difference between MSTParser and
MaltParser, on the one hand, and the best perform-
ing parsers evaluated in Rimell et al (2009), on
the other, is that the former were never developed
specifically as parsers for English. Instead, they
are best understood as data-driven parser gener-
ators, that is, tools for generating a parser given
a training set of sentences annotated with de-
pendency structures. Over the years, both sys-
tems have been applied to a wide range of lan-
guages (see, e.g., McDonald et al (2006), Mc-
Donald (2006), Nivre et al (2006b), Hall et al
(2007), Nivre et al (2007)), but they come with
no language-specific enhancements and are not
equipped specifically to deal with unbounded de-
pendencies.
Since the dependency representation used in
the evaluation corpus is based on the Stanford
typed dependency scheme (de Marneffe et al,
2006), we opted for using the WSJ section of
the PTB, converted to Stanford dependencies, as
our primary source of training data. Thus, both
parsers were trained on section 2?21 of the WSJ
data, which we converted to Stanford dependen-
cies using the Stanford parser (Klein and Man-
ning, 2003). The Stanford scheme comes in sev-
eral varieties, but because both parsers require the
dependency structure for each sentence to be a
tree, we had to use the so-called basic variety (de
Marneffe et al, 2006).
It is well known that questions are very rare
in the WSJ data, and Rimell et al (2009) found
that parsers trained only on WSJ data generally
performed badly on the questions included in the
3To ensure replicability, we provide all experimental
settings, post-processing scripts and additional information
about the evaluation at http://stp.ling.uu.se/?nivre/exp/.
evaluation corpus, while the C&C parser equipped
with a model trained on a combination of WSJ
and question data had much better performance.
To investigate whether the performance of MST-
Parser and MaltParser on questions could also be
improved by adding more questions to the train-
ing data, we trained one variant of each parser
using data that was extended with 3924 ques-
tions taken from QuestionBank (QB) (Judge et al,
2006).4 Since the QB sentences are annotated in
PTB style, it was possible to use the same conver-
sion procedure as for the WSJ data. However, it is
clear that the conversion did not always produce
adequate dependency structures for the questions,
an observation that we will return to in the error
analysis below.
In comparison to the five parsers evaluated in
Rimell et al (2009), it is worth noting that MST-
Parser and MaltParser were trained on the same
basic data as four of the five, but with a differ-
ent kind of syntactic representation ? dependency
trees instead of phrase structure trees or theory-
specific representations from CCG and HPSG. It
is especially interesting to compare MSTParser
and MaltParser to the Stanford parser, which es-
sentially produces the same kind of dependency
structures as output but uses the original phrase
structure trees from the PTB as input to training.
For our experiments we used MSTParser with
the same parsing algorithms and features as re-
ported in McDonald et al (2006). However, un-
like that work we used an atomic maximum en-
tropy model as the second stage arc predictor as
opposed to the more time consuming sequence la-
beler. McDonald et al (2006) showed that there is
negligible accuracy loss when using atomic rather
than structured labeling. For MaltParser we used
the projective Stack algorithm (Nivre, 2009) with
default settings and a slightly enriched feature
model. All parsing was projective because the
Stanford dependency trees are strictly projective.
4QB contains 4000 questions, but we removed all ques-
tions that also occurred in the test or development set of
Rimell et al (2009), who sampled their questions from the
same TREC QA test sets.
836
4.2 Post-Processing
All the development and test sets in the corpus
of Rimell et al (2009) were parsed using MST-
Parser and MaltParser after part-of-speech tagging
the input using SVMTool (Gime?nez and Ma`rquez,
2004) trained on section 2?21 of the WSJ data in
Stanford basic dependency format. The Stanford
parser has an internal module that converts the
basic dependency representation to the collapsed
representation, which explicitly represents addi-
tional dependencies, including unbounded depen-
dencies, that can be inferred from the basic rep-
resentation (de Marneffe et al, 2006). We per-
formed a similar conversion using our own tool.
Broadly speaking, there are three ways in which
unbounded dependencies can be inferred from the
Stanford basic dependency trees, which we will
refer to as simple, complex, and indirect. In the
simple case, the dependency coincides with a sin-
gle, direct dependency relation in the tree. This
is the case, for example, in Figure 1d?e, where
all that is required is that the parser identifies
the dependency relation from a governor to an
argument (dobj(see, What), dobj(have,
effect)), which we call the Arg relation; no
post-processing is needed.
In the complex case, the dependency is repre-
sented by a path of direct dependencies in the tree,
as exemplified in Figure 1a. In this case, it is
not enough that the parser correctly identifies the
Arg relation dobj(carries, that); it must
also find the dependency rcmod(fragment,
carries). We call this the Link relation, be-
cause it links the argument role inside the relative
clause to an element outside the clause. Other ex-
amples of the complex case are found in Figure 1c
and in Figure 1f.
In the indirect case, finally, the dependency
cannot be defined by a path of labeled depen-
dencies, whether simple or complex, but must
be inferred from a larger context of the tree us-
ing heuristics. Consider Figure 1b, where there
is a Link relation (rcmod(things, do)), but
no corresponding Arg relation inside the relative
clause (because there is no overt relative pro-
noun). However, given the other dependencies,
we can infer with high probability that the im-
plicit relation is dobj. Another example of the
indirect case is in Figure 1g. Our post-processing
tool performs more heuristic inference for the in-
direct case than the Stanford parser does (cf. Sec-
tion 4.3).
In order to handle the complex and indirect
cases, our post-processor is triggered by the oc-
currence of a Link relation (rcmod or conj) and
first tries to add dependencies that are directly im-
plied by a single Arg relation (relations involving
relative pronouns for rcmod, shared heads and
dependents for conj). If there is no overt rela-
tive pronoun, or the function of the relative pro-
noun is underspecified, the post-processor relies
on the obliqueness hierarchy subj < dobj <
pobj and simply picks the first ?missing func-
tion?, unless it finds a clausal complement (indi-
cated by the labels ccomp and xcomp), in which
case it descends to the lower clause and restarts
the search there.
4.3 Parser Evaluation
The evaluation was performed using the same cri-
teria as in Rimell et al (2009). A dependency
was considered correctly recovered if the gold-
standard head and dependent were correct and
the label was an ?acceptable match? to the gold-
standard label, indicating the grammatical func-
tion of the extracted element at least to the level
of subject, passive subject, object, or adjunct.
The evaluation in Rimell et al (2009) took
into account a wide variety of parser output for-
mats, some of which differed significantly from
the gold-standard. Since MSTParser and Malt-
Parser produced Stanford dependencies for this
experiment, evaluation required less manual ex-
amination than for some of the other parsers, as
was also the case for the output of the Stanford
parser in the original evaluation. However, a man-
ual evaluation was still performed in order to re-
solve questionable cases.
5 Results
The results are shown in Table 1, where the ac-
curacy for each construction is the percentage of
gold-standard dependencies recovered correctly.
The Avg column represents a macroaverage, i.e.
the average of the individual scores on the seven
constructions, while the WAvg column represents
837
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
MST 34.1 47.3 78.9 65.5 13.8 45.4 37.6 46.1 63.4
Malt 40.7 50.5 84.2 70.2 16.2 39.7 23.5 46.4 66.9
MST-Q 41.2 50.0
Malt-Q 31.2 48.5
Table 1: Parser accuracy on the unbounded dependency corpus.
Parser ObRC ObRed SbRC Free ObQ RNR SbEm Avg WAvg
C&C 59.3 62.6 80.0 72.6 81.2 49.4 22.4 61.1 69.9
Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.9 70.9
MST 34.1 47.3 78.9 65.5 41.2 45.4 37.6 50.0 63.4
Malt 40.7 50.5 84.2 70.2 31.2 39.7 23.5 48.5 66.9
DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 34.6 47.0
RASP 16.5 1.1 53.7 17.9 27.5 34.5 15.3 23.8 34.1
Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 37.0 50.3
Table 2: Parser accuracy on the unbounded dependency corpus. The ObQ score for C&C, MSTParser, and MaltParser is for
a model trained with additional questions (without this C&C scored 27.5; MSTParser and MaltParser as in Table 1).
a weighted macroaverage, where the construc-
tions are weighted proportionally to their relative
frequency in the PTB. WAvg excludes ObQ sen-
tences, since frequency statistics were not avail-
able for this construction in Rimell et al (2009).
Our first observation is that the accuracies for
both systems are considerably below the ?90%
unlabeled and ?88% labeled attachment scores
for English that have been reported previously
(McDonald and Pereira, 2006; Hall et al, 2006).
Comparing the two parsers, we see that Malt-
Parser is more accurate on dependencies in rela-
tive clause constructions (ObRC, ObRed, SbRC,
and Free), where argument relations tend to be
relatively local, while MSTParser is more accu-
rate on dependencies in RNR and SbEm, which
involve more distant relations. Without the ad-
ditional QB training data, the average scores for
the two parsers are indistinguishable, but MST-
Parser appears to have been better able to take
advantage of the question training, since MST-Q
performs better than Malt-Q on ObQ sentences.
On the weighted average MaltParser scores 3.5
points higher, because the constructions on which
it outperforms MSTParser are more frequent in
the PTB, and because WAvg excludes ObQ, where
MSTParser is more accurate.
Table 2 shows the results for MSTParser and
MaltParser in the context of the other parsers eval-
uated in Rimell et al (2009).5 For the parsers
5The average scores reported differ slightly from those in
which have a model trained on questions, namely
C&C, MSTParser, and MaltParser, the figure
shown for ObQ sentences is that of the question
model. It can be seen that MSTParser and Malt-
Parser perform below C&C and Enju, but above
the other parsers, and that MSTParser achieves the
highest score on SbEm sentences and MaltParser
on SbRC sentences. It should be noted, however,
that Table 2 does not represent a direct compar-
ison across all parsers, since most of the other
parsers would have benefited from heuristic post-
processing of the kind implemented here for MST-
Parser and MaltParser. This is especially true for
RASP, where the grammar explicitly leaves some
types of attachment decisions for post-processing.
For DCU, improved labeling heuristics would sig-
nificantly improve performance. It is instructive to
compare the dependency parsers to the Stanford
parser, which uses the same output representation
and has been used to prepare the training data for
our experiments. Stanford has very low recall on
ObRed and SbEm, the categories where heuristic
inference plays the largest role, but mirrors MST-
Parser for most other categories.
6 Error Analysis
We now proceed to a more detailed error analy-
sis, based on the development sets, and classify
Rimell et al (2009), where a microaverage (i.e., average over
all dependencies in the corpus, regardless of construction)
was reported.
838
the errors made by the parsers into three cate-
gories: A global error is one where the parser
completely fails to build the relevant clausal struc-
ture ? the relative clause in ObRC, ObRed, SbRC,
Free, SbEmb; the interrogative clause in ObQ; and
the clause headed by the higher conjunct in RNR
? often as a result of surrounding parsing errors.
When a global error occurs, it is usually mean-
ingless to further classify the error, which means
that this category excludes the other two. An Arg
error is one where the parser has constructed the
relevant clausal structure but fails to find the Arg
relation ? in the simple and complex cases ? or the
set of surrounding Arg relations needed to infer
an implicit Arg relation ? in the indirect case (cf.
Section 4.2). A Link error is one where the parser
fails to find the crucial Link relation ? rcmod
in ObRC, ObRed, SbRC, SbEmb; conj in RNR
(cf. Section 4.2). Link errors are not relevant for
Free and ObQ, where all the crucial relations are
clause-internal.
Table 3 shows the frequency of different error
types for MSTParser (first) and MaltParser (sec-
ond) in the seven development sets. First of all,
we can see that the overall error distribution is
very similar for the two parsers, which is proba-
bly due to the fact that they have been trained on
exactly the same data with exactly the same an-
notation (unlike the five parsers previously eval-
uated). However, there is a tendency for MST-
Parser to make fewer Link errors, especially in
the relative clause categories ObRC, ObRed and
SbRC, which is compatible with the observation
from the test results that MSTParser does better
on more global dependencies, while MaltParser
has an advantage on more local dependencies, al-
though this is not evident from the statistics from
the relatively small development set.
Comparing the different grammatical construc-
tions, we see that Link errors dominate for the rel-
ative clause categories ObRC, ObRed and SbRC,
where the parsers make very few errors with
respect to the internal structure of the relative
clauses (in fact, no errors at all for MaltParser
on SbRC). This is different for SbEm, where the
analysis of the argument structure is more com-
plex, both because there are (at least) two clauses
involved and because the unbounded dependency
Type Glo
bal
Arg Lin
k
A+
L
Err
ors
# D
eps
ObRC 0/1 1/1 7/11 5/3 13/16 20
ObRed 0/1 0/1 6/7 3/4 9/13 23
SbRC 2/1 1/0 7/13 0/0 10/14 43
Free 2/1 3/5 ? ? 5/6 22
ObQ 4/7 13/13 ? ? 17/20 25
RNR 6/4 4/6 0/0 4/5 14/15 28
SbEm 3/4 3/2 0/0 3/3 9/9 13
Table 3: Distribution of error types in the development
sets; frequencies for MSTParser listed first and MaltParser
second. The columns Arg and Link give frequencies for
Arg/Link errors occurring without the other error type, while
A+L give frequencies for joint Arg and Link errors.
can only be inferred indirectly from the basic de-
pendency representation (cf. Section 4.2). An-
other category where Arg errors are frequent is
RNR, where all such errors consist in attaching
the relevant dependent to the second conjunct in-
stead of to the first.6 Thus, in the example in Fig-
ure 1f, both parsers found the conj relation be-
tween puzzled and angered but attached by to the
second verb.
Global errors are most frequent for RNR, prob-
ably indicating that coordinate structures are diffi-
cult to parse in general, and for ObQ (especially
for MaltParser), probably indicating that ques-
tions are not well represented in the training set
even after the addition of QB data.7 As noted
in Section 4.1, this may be partly due to the fact
that conversion to Stanford dependencies did not
seem to work as well for QB as for the WSJ data.
Another problem is that the part-of-speech tagger
used was trained on WSJ data only and did not
perform as well on the ObQ data. Uses of What as
a determiner were consistently mistagged as pro-
nouns, which led to errors in parsing. Thus, for
the example in Figure 1e, both parsers produced
the correct analysis except that, because of the tag-
ging error, they treated What rather than effect as
the head of the wh-phrase, which counts as an er-
ror in the evaluation.
In order to get a closer look specifically at the
Arg errors, Table 4 gives the confusion matrix
6In the Stanford scheme, an argument or adjunct must be
attached to the first conjunct in a coordination to indicate that
it belongs to both conjuncts.
7Parsers trained without QB had twice as many global
errors.
839
Sb Ob POb EmSb EmOb Other Total
Sb ? 0/0 0/0 0/0 0/0 2/1 2/1
Ob 2/3 ? 0/0 0/1 0/0 4/2 6/6
POb 2/0 7/5 ? 0/0 0/0 5/8 14/13
EmSb 1/1 4/2 0/0 ? 0/0 1/2 6/5
EmOb 0/0 3/1 0/0 0/0 ? 1/6 4/7
Total 5/4 14/8 0/0 0/1 0/0 13/19 32/32
Table 4: Confusion matrix for Arg errors (excluding RNR
and using parsers trained on QB for ObQ); frequencies for
MSTParser listed first and MaltParser second. The column
Other covers errors where the function is left unspecified or
the argument is attached to the wrong head.
for such errors, showing which grammatical func-
tions are mistaken for each other, with an extra
category Other for cases where the function is left
unspecified by the parser or the error is an attach-
ment error rather than a labeling error (and ex-
cluding the RNR category because of the special
nature of the Arg errors in this category). The
results again confirm that the two parsers make
very few errors on subjects and objects clause-
internally. The few cases where an object is
mistaken as a subject occur in ObQ, where both
parsers perform rather poorly in general. By con-
trast, there are many more errors on prepositional
objects and on embedded subjects and objects. We
believe an important part of the explanation for
this pattern is to be found in the Stanford depen-
dency representation, where subjects and objects
are marked as such but all other functions real-
ized by wh elements are left unspecified (using the
generic rel dependency), which means that the re-
covery of these functions currently has to rely on
heuristic rules as described in Section 4.2. Finally,
we think it is possible to observe the tendency for
MaltParser to be more accurate at local labeling
decisions ? reflected in fewer cross-label confu-
sions ? and for MSTParser to perform better on
more distant attachment decisions ? reflected in
fewer errors in the Other category (and in fewer
Link errors).
7 Conclusion
In conclusion, the capacity of MSTParser and
MaltParser to recover unbounded dependencies is
very similar on the macro and weighted macro
level, but there is a clear distinction in their
strengths ? constructions involving more distant
dependencies such as ObQ, RNR and SbEm for
MSTParser and constructions with more locally
defined configurations such as ObRC, ObRed,
SbRC and Free for MaltParser. This is a pattern
that has been observed in previous evaluations of
the parsers and can be explained by the global
learning and inference strategy of MSTParser and
the richer feature space of MaltParser (McDonald
and Nivre, 2007).
Perhaps more interestingly, the accuracies of
MSTParser and MaltParser are only slightly be-
low the best performing systems in Rimell et al
(2009) ? C&C and Enju. This is true even though
MSTParser and MaltParser have not been engi-
neered specifically for English and lack special
mechanisms for handling unbounded dependen-
cies, beyond the simple post-processing heuristic
used to extract them from the output trees. Thus,
it is reasonable to speculate that the addition of
such mechanisms could lead to computationally
lightweight parsers with the ability to extract un-
bounded dependencies with high accuracy.
Acknowledgments
We thank Marie-Catherine de Marneffe for great
help with the Stanford parser and dependency
scheme, Llu??s Ma`rquez and Jesu?s Gime?nez for
great support with SVMTool, Josef van Gen-
abith for sharing the QuestionBank data, and
Stephen Clark and Mark Steedman for helpful
comments on the evaluation process and the pa-
per. Laura Rimell was supported by EPSRC grant
EP/E035698/1 and Carlos Go?mez-Rodr??guez
by MEC/FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus,
Bolsas Estadas INCITE/FSE cofinanced).
References
Black, E., S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, S. Roukos, B. Santorini,
and T. Strzalkowski. 1991. A procedure for quanti-
tatively comparing the syntactic coverage of English
grammars. In Proceedings of 4th DARPAWorkshop,
306?311.
Briscoe, T., J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings
840
of the COLING/ACL 2006 Interactive Presentation
Sessions, 77?80.
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of CoNLL, 149?164.
Cahill, A., M. Burke, R. O?Donovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL, 320?327.
Carreras, X., M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of
CoNLL, 9?16.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL, 173?180.
Clark, S. and J. R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In Proceedings of
LREC.
Gime?nez, J. and L. Ma`rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of LREC.
Hall, J., J. Nivre, and J. Nilsson. 2006. Discriminative
classifiers for deterministic dependency parsing. In
Proceedings of the COLING/ACL 2006 Main Con-
ference Poster Sessions, 316?323.
Hall, J., J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007. Single malt or
blended? A study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task.
Huang, L. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, 586?594.
Judge, J., A. Cahill, and J. van Genabith. 2006. Ques-
tionBank: Creating a corpus of parse-annotated
questions. In Proceedings of COLING-ACL, 497?
504.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of ACL, 423?430.
Ku?bler, S., R. McDonald, and J. Nivre. 2008. Depen-
dency Parsing. Morgan and Claypool.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of EACL, 81?88.
McDonald, R. and G. Satta. 2007. On the complexity
of non-projective data-driven dependency parsing.
In Proceedings of IWPT, 122?131.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL, 91?98.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL, 216?
220.
McDonald, R.. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Miyao, Y. and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proceedings of ACL, 83?90.
Nivre, J., J. Hall, and J. Nilsson. 2006a. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of LREC, 2216?2219.
Nivre, J., J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proceed-
ings of CoNLL, 221?225.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13:95?135.
Nivre, J. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of ACL-
IJCNLP, 351?359.
Rimell, L., S. Clark, and M. Steedman. 2009. Un-
bounded dependency recovery for parser evaluation.
In Proceedings EMNLP, 813?821.
Sagae, K. and A. Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL HLT: Short
Papers, 129?132.
841
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234?1245,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exact Inference for Generative Probabilistic
Non-Projective Dependency Parsing
Shay B. Cohen
School of Computer Science
Carnegie Mellon University, USA
scohen@cs.cmu.edu
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We describe a generative model for non-
projective dependency parsing based on a sim-
plified version of a transition system that has
recently appeared in the literature. We then
develop a dynamic programming parsing al-
gorithm for our model, and derive an inside-
outside algorithm that can be used for unsu-
pervised learning of non-projective depend-
ency trees.
1 Introduction
Dependency grammars have received considerable
attention in the statistical parsing community in
recent years. These grammatical formalisms of-
fer a good balance between structural expressiv-
ity and processing efficiency. Most notably, when
non-projectivity is supported, these formalisms can
model crossing syntactic relations that are typical in
languages with relatively free word order.
Recent work has reduced non-projective parsing
to the identification of a maximum spanning tree in a
graph (McDonald et al, 2005; Koo et al, 2007; Mc-
Donald and Satta, 2007; Smith and Smith, 2007).
An alternative to this approach is to use transition-
based parsing (Yamada and Matsumoto, 2003; Nivre
and Nilsson, 2005; Attardi, 2006; Nivre, 2009;
Go?mez-Rodr??guez and Nivre, 2010), where there is
an incremental processing of a string with a model
that scores transitions between parser states, condi-
tioned on the parse history. This paper focuses on
the latter approach.
The above work on transition-based parsing has
focused on greedy algorithms set in a statistical
framework (Nivre, 2008). More recently, dynamic
programming has been successfully used for pro-
jective parsing (Huang and Sagae, 2010; Kuhlmann
et al, 2011). Dynamic programming algorithms for
parsing (also known as chart-based algorithms) al-
low polynomial space representations of all parse
trees for a given input string, even in cases where
the size of this set is exponential in the length of
the string itself. In combination with appropriate
semirings, these packed representations can be ex-
ploited to compute many values of interest for ma-
chine learning, such as best parses and feature ex-
pectations (Goodman, 1999; Li and Eisner, 2009).
In this paper we move one step forward with re-
spect to Huang and Sagae (2010) and Kuhlmann et
al. (2011) and present a polynomial dynamic pro-
gramming algorithm for non-projective transition-
based parsing. Our algorithm is coupled with a
simplified version of the transition system from At-
tardi (2006), which has high coverage for the type
of non-projective structures that appear in various
treebanks. Instead of an additional transition oper-
ation which permits swapping of two elements in
the stack (Titov et al, 2009; Nivre, 2009), Attardi?s
system allows reduction of elements at non-adjacent
positions in the stack. We also present a generat-
ive probabilistic model for transition-based parsing.
The implication for this, for example, is that one can
now approach the problem of unsupervised learning
of non-projective dependency structures within the
transition-based framework.
Dynamic programming algorithms for non-
projective parsing have been proposed by Kahane et
al. (1998), Go?mez-Rodr??guez et al (2009) and Kuhl-
mann and Satta (2009), but they all run in exponen-
tial time in the ?gap degree? of the parsed structures.
To the best of our knowledge, this paper is the first to
1234
introduce a dynamic programming algorithm for in-
ference with non-projective structures of unbounded
gap degree.
The rest of this paper is organized as follows. In
?2 and ?3 we outline the transition-based model we
use, together with a probabilistic generative inter-
pretation. In ?4 we give the tabular algorithm for
parsing, and in ?5 we discuss statistical inference
using expectation maximization. We then discuss
some other aspects of the work in ?6 and conclude
in ?7.
2 Transition-based Dependency Parsing
In this section we briefly introduce the basic defini-
tions for transition-based dependency parsing. For a
more detailed presentation of this subject, we refer
the reader to Nivre (2008). We then define a spe-
cific transition-based model for non-projective de-
pendency parsing that we investigate in this paper.
2.1 General Transition Systems
Assume an input alphabet ? with a special symbol
$ ? ? , which we use as the root of our parse struc-
tures. Throughout this paper we denote the input
string as w = a0 ? ? ? an?1, n ? 1, where a0 = $ and
ai ? ? \ {$} for each i with 1 ? i ? n? 1.
A dependency tree for w is a directed tree Gw =
(Vw, Aw), where Vw = {0, . . . , n ? 1} is the set of
nodes, and Aw ? Vw ? Vw is the set of arcs. The
root of Gw is the node 0. The intended meaning
is that each node in Vw encodes the position of a
token in w. Furthermore, each arc in Aw encodes a
dependency relation between two tokens. We write
i ? j to denote a directed arc (i, j) ? Aw, where
node i is the head and node j is the dependent.
A transition system (for dependency parsing) is a
tuple S = (C, T, I, Ct), whereC is a set of configur-
ations, defined below, T is a finite set of transitions,
which are partial functions t:C ? C, I is a total
initialization function mapping each input string to
a unique initial configuration, and Ct ? C is a set of
terminal configurations.
A configuration is defined relative to some input
string w, and is a triple (?, ?,A), where ? and ? are
disjoint lists called stack and buffer, respectively,
and A ? Vw ? Vw is a set of arcs. Elements of
? and ? are nodes from Vw and, in the case of the
stack, a special symbol ? that we will use as initial
stack symbol. If t is a transition and c1, c2 are con-
figurations such that t(c1) = c2, we write c1 `t c2,
or simply c1 ` c2 if t is understood from the context.
Given an input string w, a parser based on S in-
crementally processes w from left to right, starting
in the initial configuration I(w). At each step, the
parser nondeterministically applies one transition, or
else it stops if it has reached some terminal config-
uration. The dependency graph defined by the arc
set associated with a terminal configuration is then
returned as one possible analysis for w.
Formally, a computation of S is a sequence ? =
c0, . . . , cm, m ? 1, of configurations such that, for
every iwith 1 ? i ? m, ci?1 `ti ci for some ti ? T .
In other words, each configuration in a computa-
tion is obtained as the value of the preceding con-
figuration under some transition. A computation is
called complete whenever c0 = I(w) for some in-
put string w, and cm ? Ct.
We can view a transition-based dependency
parser as a device mapping strings into graphs (de-
pendency trees). Without any restriction on trans-
ition functions in T , these functions might have an
infinite domain, and could thus encode even non-
recursively enumerable languages. However, in
standard practice for natural language parsing, trans-
itions are always specified by some finite mean. In
particular, the definition of each transition depends
on some finite window at the top of the stack and
some finite window at the beginning of the buffer
in each configuration. In this case, we can view a
transition-based dependency parser as a notational
variant of a push-down transducer (Hopcroft et al,
2000), whose computations output sequences that
directly encode dependency trees. These transducers
are nondeterministic, meaning that several trans-
itions can be applied to some configurations. The
transition systems we investigate in this paper fol-
low these principles.
We close this subsection with some additional
notation. We denote the stack with its topmost ele-
ment to the right and the buffer with its first ele-
ment to the left. We indicate concatenation in the
stack and buffer by a vertical bar. For example, for
k ? Vw, ?|k denotes some stack with topmost ele-
ment k and k|? denotes some buffer with first ele-
ment k. For 0 ? i ? n ? 1, ?i denotes the buffer
1235
[i, i + 1, . . . , n ? 1]; for i ? n, ?i denotes [] (the
empty buffer).
2.2 A Non-projective Transition System
We now turn to give a description of our trans-
ition system for non-projective parsing. While a
projective dependency tree satisfies the requirement
that, for every arc in the tree, there is a direc-
ted path between its headword and each of the
words between the two endpoints of the arc, a non-
projective dependency tree may violate this condi-
tion. Even though some natural languages exhibit
syntactic phenomena which require non-projective
expressive power, most often such a resource is used
in a limited way.
This idea is demonstrated by Attardi (2006), who
proposes a transition system whose individual trans-
itions can deal with non-projective dependencies
only to a limited extent, depending on the distance
in the stack of the nodes involved in the newly con-
structed dependency. The author defines this dis-
tance as the degree of the transition, with transitions
of degree one being able to handle only projective
dependencies. This formulation permits parsing a
subset of the non-projective trees, where this subset
depends on the degree of the transitions. The repor-
ted coverage in Attardi (2006) is already very high
when the system is restricted to transitions of degree
two or three. For instance, on training data for Czech
containing 28,934 non-projective relations, 27,181
can be handled by degree two transitions, and 1,668
additional dependencies can be handled by degree
three transitions. Table 1 gives additional statistics
for treebanks from the CoNLL-X shared task (Buch-
holz and Marsi, 2006).
We now turn to describe our variant of the trans-
ition system of Attardi (2006), which is equivalent to
the original system restricted to transitions of degree
two. Our results are based on such a restriction. It is
not difficult to extend our algorithms (?4) to higher
degree transitions, but this comes at the expense of
higher complexity. See ?6 for more discussion on
this issue.
Let w = a0 ? ? ? an?1 be an input string over ?
defined as in ?2.1, with a0 = $. Our transition sys-
tem for non-projective dependency parsing is
S(np) = (C, T (np), I(np), C(np)t ),
Language Deg. 2 Deg. 3 Deg. 4
Arabic 180 21 7
Bulgarian 961 41 10
Czech 27181 1668 85
Danish 876 136 53
Dutch 9072 2119 171
German 15827 2274 466
Japanese 1484 143 9
Portuguese 3104 424 37
Slovene 601 48 13
Spanish 66 7 0
Swedish 1566 226 79
Turkish 579 185 8
Table 1: The number of non-projective relations of vari-
ous degrees for several treebanks (training sets), as repor-
ted by the parser of Attardi (2006). Deg. stands for ?de-
gree.? The parser did not detect non-projective relations
of degree higher than 4.
where C is the same set of configurations defined
in ?2.1. The initialization function I(np) maps each
string w to the initial configuration ([?], ?0, ?). The
set of terminal configurationsC(np)t contains all con-
figurations of the form ([?, 0], [], A), for any set of
arcs A.
The set of transition functions is defined as
T (np) = {shb | b ? ?} ? {la1, ra1, la2, ra2},
where each transition is specified below. We let vari-
ables i, j, k, l range over Vw, and variable ? is a list
of stack elements from Vw ? {?}:
shb : (?, k|?,A) ` (?|k, ?,A) if ak = b;
la1 : (?|i|j, ?,A) ` (?|j, ?,A ? {j ? i});
ra1 : (?|i|j, ?,A) ` (?|i, ?, A ? {i? j});
la2 : (?|i|j|k, ?,A) ` (?|j|k, ?,A ? {k ? i});
ra2 : (?|i|j|k, ?,A) ` (?|i|j, ?,A ? {i? k}).
Each of the above transitions is undefined on config-
urations that do not match the forms specified above.
As an example, transition la2 is not defined for a
configuration (?, ?,A) with |?| ? 2, and transition
shb is not defined for a configuration (?, k|?,A)
with b 6= ak, or for a configuration (?, [], A).
Transition shb removes the first node from the buf-
fer, in case this node represents symbol b ? ? ,
1236
and pushes it into the stack. These transitions are
called shift transitions. The remaining four trans-
itions are called reduce transitions, i.e., transitions
that consume nodes from the stack. Notice that in
the transition system at hand all the reduce trans-
itions decrease the size of the stack by one ele-
ment. Transition la1 creates a new arc with the top-
most node on the stack as the head and the second-
topmost node as the dependent, and removes the
latter from the stack. Transition ra1 is symmetric
with respect to la1. Transitions la1 and ra1 have
degree one, as already explained. When restricted
to these three transitions, the system is equivalent
to the so-called stack-based arc-standard model of
Nivre (2004). Transition la2 and transition ra2 are
very similar to la1 and ra1, respectively, but with
the difference that they create a new arc between
the topmost node in the stack and a node which is
two positions below the topmost node. Hence, these
transitions have degree two, and are the key com-
ponents in parsing of non-projective dependencies.
We turn next to describe the equivalence between
our system and the system in Attardi (2006). The
transition-based parser presented by Attardi pushes
back into the buffer elements that are in the top pos-
ition of the stack. However, a careful analysis shows
that only the first position in the buffer can be af-
fected by this operation, in the sense that elements
that are pushed back from the stack are never found
in buffer positions other than the first. This means
that we can consider the first element of the buffer
as an additional stack element, always sitting on the
top of the top-most stack symbol.
More formally, we can define a function mc :
C ? C that maps configurations in the original al-
gorithm to those in our variant as follows:
mc((?, k|?,A)) = (?|k, ?,A)
By applying this mapping to the source and target
configuration of each transition in the original sys-
tem, it is easy to check that c1 ` c2 in that parser if
and only if mc(c1) ` mc(c2) in our variant. We ex-
tend this and define an isomorphism between com-
putations in both systems, such that a computation
c0, . . . , cm in the original parser is mapped to a com-
putation mc(c0), . . . ,mc(cm) in the variant, with
both generating the same dependency graph A. This
???
??? 2n2n? 12n? 21 2 3
Figure 1: A dependency structure of arbitrary gap degree
that can be parsed with Attardi?s parser.
proves that our notational variant is in fact equival-
ent to Attardi?s parser.
A relevant property of the set of dependency
structures that can be processed by Attardi?s parser,
even when restricted to transitions of degree two, is
that the number of discontinuities present in each of
their subtrees, defined as the gap degree by Bod-
irsky et al (2005), is not bounded. For example, the
dependency graph in Figure 1 has gap degree n? 1,
and it can be parsed by the algorithm for any arbit-
rary n ? 1 by applying 2n shb transitions to push
all the nodes into the stack, followed by (2n ? 2)
ra2 transitions to create the crossing arcs, and finally
one ra1 transition to create the dependency 1? 2.
As mentioned in ?1, the computational complex-
ity of the dynamic programming algorithm that will
be described in later sections does not depend on the
gap degree, contrary to the non-projective depend-
ency chart parsers presented by Go?mez-Rodr??guez et
al. (2009) and by Kuhlmann and Satta (2009), whose
running time is exponential in the maximum gap de-
gree allowed by the grammar.
3 A Generative Probabilistic Model
In this section we introduce a generative probabil-
istic model based on the transition system of ?2.2.
In formal language theory, there is a standard way
of giving a probabilistic interpretation to a non-
deterministic parser whose computations are based
on sequences of elementary operations such as trans-
itions. The idea is to define conditional probability
distributions over instances of the transition func-
tions, and to ?combine? these probabilities to assign
probabilities to computations and strings.
One difficulty we have to face with when dealing
with transition systems is that the notion of compu-
tation, defined in ?2.1, depends on the input string,
because of the buffer component appearing in each
configuration. This is a pitfall to generative model-
1237
ing, where we are interested in a system whose com-
putations lead to the generation of any string. To
overcome this problem, we observe that each com-
putation, defined as a sequence of stacks and buffers
(the configurations) can equivalently be expressed as
a sequence of stacks and transitions.
More precisely, consider a computation ? =
c0, . . . , cm, m ? 1. Let ?i, be the stack associated
with ci, for each i with 0 ? i ? m. Let alo C? be
the set of all stacks associated with configurations in
C. We can make explicit the transitions that have
been used in the computation by rewriting ? in the
form ?0 `t1 ?1 ? ? ??m?1 `tm ?m. In this way, ?
generates a string that is composed by all symbols
that are pushed into the stack by transitions shb, in
the left to right order.
We can now associate a probability to (our repres-
entation of) sequence ? by setting
p(?) =
m?
i=1
p(ti | ?i?1). (1)
To assign probabilities to complete computations we
should further multiply p(?) by factors ps(?0) and
pe(?m), where ps and pe are start and end probabil-
ity distributions, respectively, both defined over C?.
Note however that, as defined in ?2.2, all initial con-
figurations are associated with stack [?] and all final
configurations are associated with stack [?, 0], thus
ps and pe are deterministic. Note that the Markov
chain represented in Eq. 1 is homogeneous, i.e., the
probabilities of the transition operations do not de-
pend on the time step.
As a second step we observe that, according to the
definition of transition system, each t ? T has an in-
finite domain. A commonly adopted solution is to
introduce a special function, called history function
and denoted by H , defined over the set C? and tak-
ing values over some finite set. For each t ? T and
?, ?? ? C?, we then impose the condition
p(t | ?) = p(t | ??)
whenever H(?) = H(??). Since H is finitely val-
ued, and since T is a finite set, the above condition
guarantees that there will only be a finite number of
parameters p(t | ?) in our model.
So far we have presented a general discussion of
how to turn a transition-based parser into a gener-
ative probabilistic model, and have avoided further
specification of the history function. We now turn
our attention to the non-projective transition system
of ?2.2. To actually transform that system into a
parametrized probabilistic model, and to develop an
associated efficient inference procedure as well, we
need to balance between the amount of information
we put into the history function and the computa-
tional complexity which is required for inference.
We start the discussion with a na??ve model using a
history function defined by a fixed size window over
the topmost portion of the stack. More precisely,
each transition is conditioned on the lexical form of
the three symbols at the top of the stack ?, indic-
ated as b3, b2, b1 ? ? below, with b1 referring to the
topmost symbol. The parameters of the model are
defined as follows.
p(shb | b3, b2, b1) = ?shbb3,b2,b1 , ?b ? ? ,
p(la1 | b3, b2, b1) = ?la1b3,b2,b1 ,
p(ra1 | b3, b2, b1) = ?ra1b3,b2,b1 ,
p(la2 | b3, b2, b1) = ?la2b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?ra2b3,b2,b1 .
The parameters above are subject to the follow-
ing normalization conditions, for every choice of
b3, b2, b1 ? ? :
?la1b3,b2,b1 + ?
ra1
b3,b2,b1 + ?
la2
b3,b2,b1+
?ra2b3,b2,b1 +
?
b??
?shbb3,b2,b1 = 1 .
This na??ve model presents two practical problems.
The first problem relates to the efficiency of an in-
ference algorithm, which has a quite high computa-
tional complexity, as it will be discussed in ?5. A
second problem arises in the probabilistic setting.
Using this model would require estimating many
parameters which are based on trigrams. This leads
to higher sample complexity to avoid sparse counts:
we would need more samples to accurately estimate
the model.
We therefore consider a more elaborated model,
which tackles both of the above problems. Again,
let b3, b2, b1 ? ? indicate the lexical form of the
three symbols at the top of the stack. We define the
1238
distributions p(t | ?) as follows:
p(shb | b1) = ?shbb1 , ?b ? ? ,
p(la1 | b2, b1) = ?rdb1 ? ?
la1
b2,b1 ,
p(ra1 | b2, b1) = ?rdb1 ? ?
ra1
b2,b1 ,
p(la2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1 ,
p(ra2 | b3, b2, b1) = ?rdb1 ? ?
rd2
b2,b1 ? ?
ra2
b3,b2,b1 .
The parameters above are subject to the following
normalization conditions, for every b3, b2, b1 ? ? :
?
b??
?shbb1 + ?
rd
b1 = 1 , (2)
?la1b2,b1 + ?
ra1
b2,b1 + ?
rd2
b2,b1 = 1 , (3)
?la2b3,b2,b1 + ?
ra2
b3,b2,b1 = 1 . (4)
Intuitively, parameter ?rdb denotes the probability
that we perform a reduce transition instead of a shift
transition, given that we have seen lexical form b at
the top of the stack. Similarly, parameter ?rd2b2,b1 de-notes the probability that we perform a reduce trans-
ition of degree 2 (see ?2.2) instead of a reduce trans-
ition of degree 1, given that we have seen lexical
forms b1 and b2 at the top of the stack.
We observe that the above model has a num-
ber of parameters |? | + 4 ? |? |2 + 2 ? |? |3 (not
all independent). This should be contrasted with
the na??ve model, that has a number of parameters
4 ? |? |3 + |? |4.
4 Tabular parsing
We present here a dynamic programming algorithm
for simulating the computations of the system from
?2?3. Given an input string w, our algorithm pro-
duces a compact representation of the set ? (w),
defined as the set of all possible computations of
the model when processing w. In combination with
the appropriate semirings, this method can provide
for instance the highest probability computation in
? (w), or else the probability of w, defined as the
sum of all probabilities of computations in ? (w).
We follow a standard approach in the literature
on dynamic programming simulation of stack-based
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989). More recently, this approach has also
been applied by Huang and Sagae (2010) and by
??????
c 0
c 1
c m
? h 1
h 1? i
? h 2 h 3
minimum
stack
length in
c 1 , . . . , cm
i i + 1
i + 1
buffer size
stack size
st
ac
k
b
u
ff
er
j
Figure 2: Schematic representation of the computations
? associated with item [h1, i, h2h3, j].
Kuhlmann et al (2011) to the simulation of pro-
jective transition-based parsers. The basic idea in
this approach is to decompose computations of the
parser into smaller parts, group them into equival-
ence classes and recombine to obtain larger parts of
computations.
Let w = a0 ? ? ? an?1, Vw and S(np) be defined as
in ?2. We use a structure called item, defined as
[h1, i, h2h3, j],
where 0 ? i < j ? n and h1, h2, h3 ? Vw must
satisfy h1 < i and i ? h2 < h3 < j. The intended
interpretation of an item can be stated as follows; see
also Figure 2.
? There exists a computation ? of S(np) on w hav-
ing the form c0, . . . , cm, m ? 1, with c0 =
(?|h1, ?i, A) and cm = (?|h2|h3, ?j , A?) for
some stack ? and some arc sets A and A?;
? For each iwith 1 ? i < m, the stack ?i associated
with configuration ci has the list ? at the bottom
and satisfies |?i| ? |?|+ 2.
Some comments on the above conditions are in
order here. Let t1, ? ? ? , tm be the sequence of trans-
itions in T (np) associated with computation ?. Then
we have t1 = shai , since |?1| ? |?| + 2. Thus we
conclude that |?1| = |?|+ 2.
The most important consequence of the definition
of item is that each transition ti with 2 ? i ? m
does not depend on the content of the ? portion of
the stack ?i. To see this, consider transition ci?1 `ti
ci. If ti = shai , the content of ? is irrelevant at
1239
this step, since in our model shai is conditioned only
on the topmost stack symbol of ?i?1, and we have
|?i?1| ? |?|+ 2.
Consider now the case of ti = la2. From |?i| ?
|?| + 2 we have that |?i?1| ? |?| + 3. Again, the
content of ? is irrelevant at this step, since in our
model la2 is conditioned only on the three topmost
stack symbols of ?i?1. A similar argument applies
to the cases of ti ? {ra2, la1, ra1}.
From the above, we conclude that if we apply the
transitions t1, . . . , tm to stacks of the form ?|h1, the
resulting computations have all identical probabilit-
ies, independently of the choice of ?.
Each computation satisfying the two conditions
above will be called an I-computation associ-
ated with item [h1, i, h2h3, j]. Notice that an I-
computation has the overall effect of replacing node
h1 sitting above a stack ? with nodes h2 and h3.
This is the key property in the development of our
algorithm below.
We specify our dynamic programming algorithm
as a deduction system (Shieber et al, 1995). The
deduction system starts with axiom [?, 0, ?0, 1], cor-
responding to an initial stack [?] and to the shift of
a0 = $ from the buffer into the stack. The set ? (w)
is non-empty if and only if item [?, 0, ?0, n] can be
derived using the inference rules specified below.
Each inference rule is annotated with the type of
transition it simulates, along with the arc constructed
by the transition itself, if any.
[h1, i, h2h3, j]
[h3, j, h3j, j + 1]
(shaj )
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h5, j]
(la1;h5 ? h4)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra1;h4 ? h5)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h4h5, j]
(la2;h5 ? h2)
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h2h4, j]
(ra2;h2 ? h5)
The above deduction system infers items in a
bottom-up fashion. This means that longer compu-
tations over substrings of w are built by combining
shorter ones. In particular, the inference rule shaj
asserts the existence of I-computations consisting of
a single shaj transition. Such computations are rep-
resented by the consequent item [h3, j, h3j, j + 1],
indicating that the index of the shifted word aj is
added to the stack by pushing it on top of h3.
The remaining four rules implement the reduce
transitions of the model. We have already ob-
served in ?2.2 that all available reduce transitions
shorten the size of the stack by one unit. This al-
lows us to combine pairs of I-computations with
a reduce transition, resulting in a computation that
is again an I-computation. More precisely, if we
concatenate an I-computation asserted by an item
[h1, i, h2h3, k] with an I-computation asserted by an
item [h3, k, h4h5, j], we obtain a computation that
has the overall effect of increasing the size of the
stack by 2, replacing the topmost stack element h1
with stack elements h2, h4 and h5. If we now apply
any of the reduce transitions from the inventory of
the model, we will remove one of these three nodes
from the stack, and the overall result will be again
an I-computation, which can then be asserted by a
certain item. For example, if we apply the reduce
transition la1, the consequent item is [h1, i, h2h5, j],
since an la1 transition removes the second topmost
element from the stack (h4). The other reduce trans-
itions remove a different element, and thus their
rules produce different consequent items.
The above argument shows the soundness of the
deduction system, i.e., an item I = [h1, i, h2h3, j]
is only generated if there exists an I-computation
? = c0, . . . , cm with c0 = (?|h1, ?i, A) and cm =
(?|h2|h3, ?j , A?). To prove completeness, we must
show the converse result, i.e., that the existence of
an I-computation ? implies that item I is inferred.
We first do this under the assumption that the infer-
ence rule for the shift transitions do not have an ante-
cedent, i.e., items [h1, j, h1j, j + 1] are considered
as axioms. We proceed by using strong induction on
the length m of the computation ?.
For m = 1, ? consists of a single transition shaj ,
and the corresponding item I = [h1, j, h1j, j + 1]
is constructed as an axiom. For m > 1, let ? be
as specified above. The transition that produced
1240
cm must have been a reduce transition, otherwise
? would not be an I-computation. Let ck be the
rightmost configuration in c0, . . . , cm?1 whose stack
size is |?| + 2. Then it can be shown that the com-
putations ?1 = c0, . . . , ck and ?2 = ck, . . . , cm?1
are again I-computations. Since ?1 and ?2 have
strictly fewer transitions than ?, by the induction hy-
pothesis, the system constructs items [h1, i, h2h3, k]
and [h3, k, h4h5, j], where h2 and h3 are the stack
elements at the top of ck. Applying to these items
the inference rule corresponding to the reduce trans-
ition at hand, we can construct item I .
When the inference rule for the shift transition has
an antecedent [h1, i, h2h3, j], as indicated above, we
have the overall effect that I-computations consist-
ing of a single transition shifting aj on the top of h3
are simulated only in case there exists a computation
starting with configuration ([?], ?0) and reaching a
configuration of the form (?|h2|h3, ?j). This acts as
a filter on the search space of the algorithm, but does
not invalidate the completeness property. However,
in this case the proof is considerably more involved,
and we do not report it here.
An important property of the deduction system
above, which will be used in the next section, is
that the system is unambiguous, that is, each I-
computation is constructed by the system in a
unique way. This can be seen by observing that, in
the sketch of the completeness proof reported above,
there always is an unique choice of ck that decom-
poses I-computation ? into I-computations ?1 and
?2. In fact, if we choose a configuration ck? other
than ck with stack size |?| + 2, the computation
??2 = ck? , . . . , cm?1 will contain ck as an interme-
diate configuration, which violates the definition of
I-computation because of an intervening stack hav-
ing size not larger than the size of the stack associ-
ated with the initial configuration.
As a final remark, we observe that we can keep
track of all inference rules that have been applied
in the computation of each item by the above al-
gorithm, by encoding each application of a rule as
a reference to the pair of items that were taken as
antecedent in the inference. In this way, we ob-
tain a parse forest structure that can be viewed as a
hypergraph or as a non-recursive context-free gram-
mar, similar to the case of parsing based on context-
free grammars. See for instance Klein and Manning
(2001) or Nederhof (2003). Such a parse forest en-
codes all valid computations in ? (w), as desired.
The algorithm runs in O(n8) time. Using meth-
ods similar to those specified in Eisner and Satta
(1999), we can reduce the running time to O(n7).
However, we do not further pursue this idea here,
and proceed with the discussion of exact inference,
found in the next section.
5 Inference
We turn next to specify exact inference with our
model, for computing feature expectations. Such
inference enables, for example, the derivation of
an expectation-maximization algorithm for unsuper-
vised parsing.
Here, a feature is a function over computations,
providing the count of a pattern related to a para-
meter. We denote by f la2b3,b2,b1(?), for instance,the number of occurrences of transition la2 within
? with topmost stack symbols having word forms
b3, b2, b1 ? ? , with b1 associated with the topmost
stack symbol.
Feature expectations are computed by using an
inside-outside algorithm for the items in the tabu-
lar algorithm. More specifically, given a string w,
we associate each item [h1, i, h2h3, j] defined as in
?4 with two quantities:
I([h1, i, h2h3, j]) =
?
?=([h1],?i),...,([h2,h3],?j)
p(?) ; (5)
O([h1, i, h2h3, j]) =
?
?,?=([?],?0),...,(?|h1,?i)
??=(?|h2|h3,?j),...,([?,0],?n)
p(?) ? p(??) . (6)
I([h1, i, h2h3, j]) and O([h1, i, h2h3, j]) are called
the inside and the outside probabilities, respect-
ively, of item [h1, i, h2h3, j]. The tabular algorithm
of ?4 can be used to compute the inside probabilit-
ies. Using the gradient transformation (Eisner et al,
2005), a technique for deriving outside probabilities
from a set of inference rules, we can also compute
O([h1, i, h2h3, j]). The use of the gradient trans-
formation is valid in our case because the tabular al-
gorithm is unambiguous (see ?4).
Using the inside and outside probabilities, we can
now efficiently compute feature expectations for our
1241
Ep(?|w)[f la2b3,b2,b1(?)] =
?
??? (w)
p(? | w) ? f la2b3,b2,b1(?) =
1
p(w) ?
?
??? (w)
p(?) ? f la2b3,b2,b1(?)
= 1p(w) ?
?
?,i,k,j,
h1,h2,h3,h4,h5,
s.t. ah2=b3,
ah4=b2, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?1=(?|h1,?i),...,(?|h2|h3,?k),
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?1) ? p(?2) ? p(la2 | b3, b2, b1) ? p(?3)
=
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
p(w) ?
?
?,i,j,
h1,h2,h5, s.t.
ah2=b3, ah5=b1
?
?0=([?],?0),...,(?|h1,?i),
?3=(?|h2|h5,?j),...,([?,0],?n)
p(?0) ? p(?3) ?
?
?
k,h3,h4,
s.t. ah4=b2
?
?1=(?|h1,?i),...,(?|h2|h3,?k)
p(?1) ?
?
?2=(?|h2|h3,?k),...,(?|h2|h4|h5,?j)
p(?2)
Figure 3: Decomposition of the feature expectationEp(?|w)[f la2b3,b2,b1(?)] into a finite summation. Quantity p(w) aboveis the sum over all probabilities of computations in ? (w).
model. Figure 3 shows how to express the expect-
ation of feature f la2b3,b2,b1(?) by means of a finitesummation. Using Eq. 5 and 6 and the relation
p(w) = I([?, 0, ?0, n]) we can then write:
Ep(?|w)[f la2b3,b2,b1(?)] =
?rdb1 ? ?
rd2
b2,b1 ? ?
la2
b3,b2,b1
I([?, 0, ?0, n]) ?
?
?
i,j,h1,h4,h5,
s.t. ah4=b2, ah5=b1
O([h1, i, h4h5, j]) ?
?
?
k,h2,h3,
s.t. ah2=b3
I([h1, i, h2h3, k]) ? I([h3, k, h4h5, j]) .
Very similar expressions can be derived for the ex-
pectations for features f ra2b3,b2,b1(?), f la1b2,b1(?), and
f ra1b2,b1(?). As for feature f shbb1 (?), b ? ? , the aboveapproach leads to
Ep(?|w)[f shbb1 (?)] =
=
?shbb1
I([?, 0, ?0, n]) ?
?
?,i,h, s.t.
ah=b1, ai=b
O([h, i, hi, i+ 1]) .
As mentioned above, these expectations can be
used, for example, to derive an EM algorithm for our
model. The EM algorithm in our case is not com-
pletely straightforward because of the way we para-
metrize the model. We give now the re-estimation
steps for such an EM algorithm. We assume that all
expectations below are taken with respect to a set of
parameters ? from iteration s ? 1 of the algorithm,
and we are required to update these ?. To simplify
notation, let us assume that there is only one stringw
in the training corpus. For each b1 ? ? , we define:
Zb1 =
?
b2??
Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
]
+
?
b3,b2??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
;
Zb2,b1 =
?
b3??
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
]
.
We then have, for every b ? ? :
?shbb1 (s)?
Ep(?|w)[f shbb1 (?)]
Zb1 +
?
b??? Ep(?|w)[f
shb?
b1 (?)]
.
1242
Furthermore, we have:
?la1b2,b1(s)?
Ep(?|w)[f la1b2,b1(?)]
Zb2,b1 + Ep(?|w)
[
f la1b2,b1(?) + f
ra1
b2,b1(?)
] ,
and:
?la2b3,b2,b1(s)?
Ep(?|w)[f la2b3,b2,b1(?)]
Ep(?|w)
[
f la2b3,b2,b1(?) + f
ra2
b3,b2,b1(?)
] .
The rest of the parameter updates can easily be de-
rived using the above updates because of the sum-
to-1 constraints in Eq. 2?4.
6 Discussion
We note that our model inherits spurious ambigu-
ity from Attardi?s model. More specifically, we can
have different derivations, corresponding to differ-
ent system computations, that result in identical de-
pendency graphs and strings. While running our
tabular algorithm with the Viterbi semiring effi-
ciently computes the highest probability computa-
tion in ? (w), spurious ambiguity means that find-
ing the highest probability dependency tree is NP-
hard. This latter result can be shown using proof
techniques similar to those developed by Sima?an
(1996). We leave it for future work how to eliminate
spurious ambiguity from the model.
While in the previous sections we have described
a tabular method for the transition system of Attardi
(2006) restricted to transitions of degree up to two, it
is possible to generalize the model to include higher-
degree transitions. In the general formulation of At-
tardi parser, transitions of degree d create links in-
volving nodes located d positions beneath the top-
most position in the stack:
lad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i2| . . . |id+1, ?, A ? {id+1 ? i1});
rad : (?|i1|i2| . . . |id+1, ?, A) `
(?|i1|i2| . . . |id, ?, A ? {i1 ? id+1}).
To define a transition system that supports trans-
itions up to degree D, we use a set of
items of the form [s1 . . . sD?1, i, e1 . . . eD, j], cor-
responding (in the sense of ?4) to compu-
tations of the form c0, . . . , cm, m ? 1,
with c0 = (?|s1| . . . |sD?1, ?i, A) and cm =
(?|e1| . . . |eD, ?j , A?). The deduction steps corres-
ponding to reduce transitions in this general system
have the general form
[s1 . . . sD?1, i, e1m1 . . .mD?1, j]
[m1 . . .mD?1, j, e2 . . . eD+1, w]
[s1 . . . sD?1, i, e1 . . . ec?1ec+1 . . . eD+1, w]
(ep ? ec)
where the values of p and c differ for each transition:
to obtain the inference rule corresponding to a lad
transition, we make p = D + 1 and c = D + 1? d;
and to obtain the rule for a rad transition, we make
p = D + 1? d and c = D + 1. Note that the parser
runs in timeO(n3D+2), whereD stands for the max-
imum transition degree, so each unit increase in the
transition degree adds a cubic factor to the parser?s
polynomial time complexity. This is in contrast to a
previous tabular formulation of the Attardi parser by
Go?mez-Rodr??guez et al (2011), which ran in expo-
nential time.
The model for the transition system we give in this
paper is generative. It is not hard to naturally extend
this model to the discriminative setting. In this case,
we would condition the model on the input string to
get a conditional distribution over derivations. It is
perhaps more natural in this setting to use arbitrary
weights for the parameter values, since the compu-
tation of a normalization constant (the probability of
a string) is required in any case. Arbitrary weights
in the generative setting could be more problematic,
because it would require computing a normalization
constant corresponding to a sum over all strings and
derivations.
7 Conclusion
We presented in this paper a generative probabilistic
model for non-projective parsing, together with the
description of an efficient tabular algorithm for pars-
ing and doing statistical inference with the model.
Acknowledgments
The authors thank Marco Kuhlmann for helpful
comments on an early draft of the paper. The authors
also thank Giuseppe Attardi for the help received to
extract the parsing statistics. The second author has
been partially supported by Ministerio de Ciencia e
Innovacio?n and FEDER (TIN2010-18552-C03-02).
1243
References
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166?170,
New York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143?151,
Vancouver, Canada.
Manuel Bodirsky, Marco Kuhlmann, and Mathias Mo?hl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Gram-
mar and Ninth Meeting on Mathematics of Language,
pages 195?203, Edinburgh, UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
149?164, New York, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 457?464, College Park, MD,
USA.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In Proceedings
of HLT-EMNLP, pages 281?290.
Carlos Go?mez-Rodr??guez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1492?1501, Uppsala, Sweden.
Carlos Go?mez-Rodr??guez, David J. Weir, and John Car-
roll. 2009. Parsing mildly non-projective dependency
structures. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 291?299, Athens, Greece.
Carlos Go?mez-Rodr??guez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics (in press), 37(3).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to Automata Theory.
Addison-Wesley, 2nd edition.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077?1086,
Uppsala, Sweden.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 18th International Conference on Compu-
tational Linguistics (COLING-ACL), pages 646?652,
Montre?al, Canada.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the IWPT, pages
123?134.
Terry Koo, Amir Globerson, Xavier Carreras, and Mi-
chael Collins. 2007. Structured prediction models
via the matrix-tree theorem. In Proceedings of the
EMNLP-CoNLL, pages 141?150.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective depend-
ency parsing. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 478?486, Athens, Greece.
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon,
USA.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbru?cken, July 29?
August 2, 1974, number 14 in Lecture Notes in Com-
puter Science, pages 255?269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40?51, Singa-
pore.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Tenth International Conference on Parsing
Technologies (IWPT), pages 121?132, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Human Language
Technology Conference (HLT) and Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523?530, Vancouver, Canada.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and knuth?s algorithm. Computational Linguistics,
29(1):135?143.
1244
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 99?106, Ann Arbor, USA.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50?57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the 47th An-
nual Meeting of the ACL and the Fourth International
Joint Conference on Natural Language Processing of
the AFNLP, pages 351?359, Singapore.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1?2):3?
36.
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of COLING, pages 1175?
1180.
David A. Smith and Noah A. Smith. 2007. Probab-
ilistic models of nonprojective dependency trees. In
Proceedings of the EMNLP-CoNLL, pages 132?140.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI, pages 281?290.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195?206, Nancy,
France.
1245
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 308?319, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improving Transition-Based Dependency Parsing with Buffer Transitions
Daniel Ferna?ndez-Gonza?lez
Departamento de Informa?tica
Universidade de Vigo
Campus As Lagoas, 32004
Ourense, Spain
danifg@uvigo.es
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a
Campus de Elvin?a, 15071
A Corun?a, Spain
carlos.gomez@udc.es
Abstract
In this paper, we show that significant im-
provements in the accuracy of well-known
transition-based parsers can be obtained, with-
out sacrificing efficiency, by enriching the
parsers with simple transitions that act on
buffer nodes.
First, we show how adding a specific tran-
sition to create either a left or right arc of
length one between the first two buffer nodes
produces improvements in the accuracy of
Nivre?s arc-eager projective parser on a num-
ber of datasets from the CoNLL-X shared
task. Then, we show that accuracy can also be
improved by adding transitions involving the
topmost stack node and the second buffer node
(allowing a limited form of non-projectivity).
None of these transitions has a negative im-
pact on the computational complexity of the
algorithm. Although the experiments in this
paper use the arc-eager parser, the approach is
generic enough to be applicable to any stack-
based dependency parser.
1 Introduction
Dependency parsing has become a very active re-
search area in natural language processing in re-
cent years. The dependency representation of syn-
tax simplifies the syntactic parsing task, since no
non-lexical nodes need to be postulated by the
parsers; while being convenient in practice, since
dependency representations directly show the head-
modifier and head-complement relationships which
form the basis of predicate-argument structure. This
has led to the development of various data-driven
dependency parsers, such as those by Yamada and
Matsumoto (2003), Nivre et al2004), McDonald
et al2005), Martins et al2009), Huang and Sagae
(2010) or Tratz and Hovy (2011), which can be
trained directly from annotated data and produce ac-
curate analyses very efficiently.
Most current data-driven dependency parsers can
be classified into two families, commonly called
graph-based and transition-based parsers (Mc-
Donald and Nivre, 2011). Graph-based parsers (Eis-
ner, 1996; McDonald et al2005) are based on
global optimization of models that work by scoring
subtrees. On the other hand, transition-based parsers
(Yamada and Matsumoto, 2003; Nivre et al2004),
which are the focus of this work, use local training
to make greedy decisions that deterministically se-
lect the next parser state. Among the advantages of
transition-based parsers are the linear time complex-
ity of many of them and the possibility of using rich
feature models (Zhang and Nivre, 2011).
In particular, many transition-based parsers
(Nivre et al2004; Attardi, 2006; Sagae and Tsujii,
2008; Nivre, 2009; Huang and Sagae, 2010; Go?mez-
Rodr??guez and Nivre, 2010) are stack-based (Nivre,
2008), meaning that they keep a stack of partially
processed tokens and an input buffer of unread to-
kens. In this paper, we show how the accuracy of
this kind of parsers can be improved, without com-
promising efficiency, by extending their set of avail-
able transitions with buffer transitions. These are
transitions that create a dependency arc involving
some node in the buffer, which would typically be
considered unavailable for linking by these algo-
308
rithms. The rationale is that buffer transitions con-
struct some ?easy? dependency arcs in advance, be-
fore the involved nodes reach the stack, so that the
classifier?s job when choosing among standard tran-
sitions is simplified.
To test the approach, we use the well-known arc-
eager parser by (Nivre, 2003; Nivre et al2004) as
a baseline, showing improvements in accuracy on
most datasets of the CoNLL-X shared task (Buch-
holz and Marsi, 2006). However, the techniques dis-
cussed in this paper are generic and can also be ap-
plied to other stack-based dependency parsers.
The rest of this paper is structured as follows:
Section 2 is an introduction to transition-based
parsers and the arc-eager parsing algorithm. Section
3 presents the first novel contribution of this paper,
projective buffer transitions, and discusses their
empirical results on CoNLL-X datasets. Section 4
does the same for a more complex set of transitions,
non-projective buffer transitions. Finally, Section
5 discusses related work and Section 6 sums up the
conclusions and points out avenues for future work.
2 Preliminaries
We now briefly present some basic definitions for
transition-based dependency parsing; a more thor-
ough explanation can be found in (Nivre, 2008).
2.1 Dependency graphs
Let w = w1 . . . wn be an input string. A depen-
dency graph forw is a directed graphG = (Vw, A);
where Vw = {0, 1, . . . , n} is a set of nodes, and
A ? Vw ? L ? Vw is a set of labelled arcs. Each
node in Vw encodes the position of a token in w,
where 0 is a dummy node used as artificial root. An
arc (i, l, j) will also be called a dependency link la-
belled l from i to j. We say that i is the syntactic
head of j and, conversely, that j is a dependent of
i. The length of the arc (i, l, j) is the value |j ? i|.
Most dependency representations of syntax do not
allow arbitrary dependency graphs. Instead, they re-
quire dependency graphs to be forests, i.e., acyclic
graphs where each node has at most one head. In this
paper, we will work with parsers that assume depen-
dency graphs G = (Vw, A) to satisfy the following
properties:
? Single-head: every node has at most one in-
coming arc (if (i, l, j) ? A, then for every
k 6= i, (k, l?, j) 6? A).
? Acyclicity: there are no directed cycles in G.
? Node 0 is a root, i.e., there are no arcs of the
form (i, l, 0) in A.
A dependency forest with a single root (i.e., where
all the nodes but one have at least one incoming arc)
is called a tree. Every dependency forest can triv-
ially be represented as a tree by adding arcs from
the dummy root node 0 to every other root node.
For reasons of computational efficiency, many de-
pendency parsers are restricted to work with forests
satisfying an additional restriction called projectiv-
ity. A dependency forest is said to be projective
if the set of nodes reachable by traversing zero or
more arcs from any given node k corresponds to a
continuous substring of the input (i.e., is an interval
{x ? Vw | i ? x ? j}). For trees with a dummy
root node at position 0, this is equivalent to not al-
lowing dependency links to cross when drawn above
the nodes (planarity).
2.2 Transition systems
A transition system is a nondeterministic state ma-
chine that maps input strings to dependency graphs.
In this paper, we will focus on stack-based transi-
tion systems. A stack-based transition system is a
quadruple S = (C, T, cs, Ct) where
? C is a set of parser configurations. Each con-
figuration is of the form c = (?, ?,A) where ?
is a list of nodes of Vw called the stack, ? is a
list of nodes of Vw called the buffer, and A is a
set of dependency arcs,
? T is a finite set of transitions, each of which is
a partial function t : C ? C,
? cs is an initialization function, mapping a sen-
tence w1 . . . wn to an initial configuration
cs = ([0], [1, . . . , n], ?),
? Ct is the set of terminal configurations Ct =
(?, [], A) ? C.
Transition systems are nondeterministic devices,
since several transitions may be applicable to the
same configuration. To obtain a deterministic parser
309
from a transition system, a classifier is trained to
greedily select the best transition at each state. This
training is typically done by using an oracle, which
is a function o : C ? T that selects a single transi-
tion at each configuration, given a tree in the training
set. The classifier is then trained to approximate this
oracle when the target tree is unknown.
2.3 The arc-eager parser
Nivre?s arc-eager dependency parser (Nivre, 2003;
Nivre et al2004) is one of the most widely known
and used transition-based parsers (see for example
(Zhang and Clark, 2008; Zhang and Nivre, 2011)).
This parser works by reading the input sentence
from left to right and creating dependency links as
soon as possible. This means that links are created in
a strict left-to-right order, and implies that while left-
ward links are built in a bottom-up fashion, a right-
ward link a ? b will be created before the node b
has collected its right dependents.
The arc-eager transition system has the following
four transitions (note that, for convenience, we write
a stack with node i on top as ?|i, and a buffer whose
first node is i as i|?):
? SHIFT : (?, i|?,A)? (?|i, ?, A).
? REDUCE : (?|i, ?, A) ? (?, ?,A). Precondi-
tion: ?k, l? | (k, l?, i) ? A.
? LEFT-ARCl : (?|i, j|?,A) ? (?, j|?,A ?
{(j, l, i)}). Preconditions: i 6= 0 and 6 ?k, l? |
(k, l?, i) ? A (single-head)
? RIGHT-ARCl :
(?|i, j|?,A)? (?|i|j, ?,A ? {(i, l, j)}).
The SHIFT transition reads an input word by re-
moving the first node from the buffer and placing it
on top of the stack. The REDUCE transition pops
the stack, and it can only be executed if the topmost
stack node has already been assigned a head. The
LEFT-ARC transition creates an arc from the first
node in the buffer to the node on top of the stack,
and then pops the stack. It can only be executed if
the node on top of the stack does not already have
a head. Finally, the RIGHT-ARC transition creates
an arc from the top of the stack to the first buffer
node, and then removes the latter from the buffer
and moves it to the stack.
The arc-eager parser has linear time complex-
ity. In principle, it is restricted to projective depen-
dency forests, but it can be used in conjunction with
the pseudo-projective transformation (Nivre et al
2006) in order to capture a restricted subset of non-
projective forests. Using this setup, it scored as one
of the top two systems in the CoNLL-X shared task.
3 Projective buffer transitions
In this section, we show that the accuracy of stack-
based transition systems can benefit from adding one
of a pair of new transitions, which we call projective
buffer transitions, to their transition sets.
3.1 The transitions
The two projective buffer transitions are defined as
follows:
? LEFT-BUFFER-ARCl :
(?, i|j|?,A)? (?, j|?,A ? {(j, l, i)}).
? RIGHT-BUFFER-ARCl :
(?, i|j|?,A)? (?, i|?,A ? {(i, l, j)}).
The LEFT-BUFFER-ARC transition creates a left-
ward dependency link from the second node to
the first node in the buffer, and then removes the
first node from the buffer. Conversely, the RIGHT-
BUFFER-ARC transition creates a rightward depen-
dency link from the first node to the second node
in the buffer, and then removes the second node.
We call these transitions projective buffer transitions
because, since they act on contiguous buffer nodes,
they can only create projective arcs.
Adding one (or both) of these transitions to a
projective or non-projective stack-based transition
system does not affect its correctness, as long as
this starting system cannot generate configurations
(?, ?,A) where a buffer node has a head in A1: it
cannot affect completeness because we are not re-
moving existing transitions, and therefore any de-
pendency graph that the original system could build
1Most stack-based transition systems in the literature disal-
low such configurations. However, in parsers that allow them
(such as those defined by Go?mez-Rodr??guez and Nivre (2010)),
projective buffer transitions can still be added without affecting
correctness if we impose explicit single-head and acyclicity pre-
conditions on them. We have not included these preconditions
by default for simplicity of presentation.
310
will still be obtainable by the augmented one; and it
cannot affect soundness (be it for projective depen-
dency forests or for any superset of them) because
the new transitions can only create projective arcs
and cannot violate the single-head or acyclicity con-
straints, given that a buffer node cannot have a head.
The idea behind projective buffer transitions is to
create dependency arcs of length one (i.e., arcs in-
volving contiguous nodes) in advance of the stan-
dard arc-building transitions that need at least one of
the nodes to get to the stack (LEFT-ARC and RIGHT-
ARC in the case of the arc-eager transition system).
Our hypothesis is that, as it is known that
short-distance dependencies are easier to learn for
transition-based parsers than long-distance ones
(McDonald and Nivre, 2007), handling these short
arcs in advance and removing their dependent nodes
will make it easier for the classifier to learn how
to make decisions involving the standard arc tran-
sitions.
Note that the fact that projective buffer transitions
create arcs of length 1 is not explicit in the defini-
tion of the transitions. For instance, if we add the
LEFT-BUFFER-ARCl transition only to the arc-eager
transition system, LEFT-BUFFER-ARCl will only be
able to create arcs of length 1, since it is easy to see
that the first two buffer nodes are contiguous in all
the accessible configurations. However, if we add
RIGHT-BUFFER-ARCl, this transition will have the
potential to create arcs of length greater than 1: for
example, if two consecutive RIGHT-BUFFER-ARCl
transitions are applied starting from a configuration
(?, i|i + 1|i + 2|?,A), the second application will
create an arc i? i+ 2 of length 2.
Although we could have added the length-1 re-
striction to the transition definitions, we have cho-
sen the more generic approach of leaving it to the
oracle instead. While the oracle typically used for
the arc-eager system follows the simple principle of
executing transitions that create an arc as soon as
it has the chance to, adding projective buffer transi-
tions opens up new possibilities: we may now have
several ways of creating an arc, and we have to de-
cide in which cases we train the parser to use one of
the buffer transitions and in which cases we prefer
to train it to ignore the buffer transitions and dele-
gate to the standard ones. Following the hypothe-
sis explained above, our policy has been to train the
parser to use buffer transitions whenever possible for
arcs of length one, and to not use them for arcs of
length larger than one. To test this idea, we also
conducted experiments with the alternative policy
?use buffer transitions whenever possible, regardless
of arc length?: as expected, the obtained accuracies
were (slightly) worse.
The chosen oracle policy is generic and can be
plugged into any stack-based parser: for a given
transition, first check whether it is possible to build a
gold-standard arc of length 1 with a projective buffer
transition.2 If so, choose that transition, and if not,
just delegate to the original parser?s oracle.
3.2 Experiments
To empirically evaluate the effect of projective
buffer transitions on parsing accuracy, we have con-
ducted experiments on eight datasets of the CoNLL-
X shared task (Buchholz and Marsi, 2006): Arabic
(Hajic? et al2004), Chinese (Chen et al2003),
Czech (Hajic? et al2006), Danish (Kromann, 2003),
German (Brants et al2002), Portuguese (Afonso et
al., 2002), Swedish (Nilsson et al2005) and Turk-
ish (Oflazer et al2003; Atalay et al2003).
As our baseline parser, we use the arc-eager pro-
jective transition system by Nivre (2003). Table 1
compares the accuracy obtained by this system alone
with that obtained when the LEFT-BUFFER-ARC
and RIGHT-BUFFER-ARC transitions are added to
it as explained in Section 3.1.
Accuracy is reported in terms of labelled (LAS)
and unlabelled (UAS) attachment score. We used
SVM classifiers from the LIBSVM package (Chang
and Lin, 2001) for all languages except for Chinese,
Czech and German. In these, we used the LIB-
LINEAR package (Fan et al2008) for classifica-
tion, since it reduces training time in these larger
datasets. Feature models for all parsers were specif-
ically tuned for each language.3
2In this context, ?possible? means that we can create the arc
without losing the possibility of creating other gold-standard
arcs. In the case of RIGHT-BUFFER-ARC, this involves check-
ing that the candidate dependent node has no dependents in the
gold-standard tree (if it has any, we cannot remove it from the
stack or it would not be able to collect its dependents, so we do
not use the buffer transition).
3All the experimental settings and feature models used are
included in the supplementary material and also available at
http://www.grupolys.org/?cgomezr/exp/.
311
NE NE+LBA NE+RBA
Language LAS UAS LAS UAS LAS UAS
Arabic 66.43 77.19 67.78 78.26 63.87 74.63
Chinese 86.46 90.18 82.47 86.14 86.62 90.64
Czech 77.24 83.40 78.70 84.24 78.28 83.94
Danish 84.91 89.80 85.21 90.20 82.53 87.35
German 86.18 88.60 84.31 86.50 86.48 88.90
Portug. 86.60 90.20 86.92 90.58 85.55 89.28
Swedish 83.33 88.83 82.81 88.03 81.66 88.03
Turkish 63.77 74.35 57.42 66.24 64.33 74.73
Table 1: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre?s arc-eager parser without modification
(NE), with the LEFT-BUFFER-ARC transition added (NE+LBA) and with the RIGHT-BUFFER-ARC transition added
(NE+RBA). Best results for each language are shown in boldface.
As can be seen in Table 1, adding a projective
buffer transition improves the performance of the
parser in seven out of the eight tested languages. The
improvements in LAS are statistically significant at
the .01 level4 in the Arabic and Czech treebanks.
Note that the decision of which buffer transition
to add strongly depends on the dataset. In the
majority of the treebanks, we can see that when
the LEFT-BUFFER-ARC transition improves perfor-
mance the RIGHT-BUFFER-ARC transition harms it,
and vice versa. The exceptions are Czech, where
both transitions are beneficial, and Swedish, where
both are harmful. Therefore, when using projective
buffer transitions in practice, the language and anno-
tation scheme should be taken into account (or tests
should be made) to decide which one to use.
Table 2 hints at the reason for this treebank-
sensitiveness. By analyzing the relative frequency
of leftward and rightward dependency links (and,
in particular, of leftward and rightward links of
length 1) in the different treebanks, we see a rea-
sonably clear tendency: the LEFT-BUFFER-ARC
transition works better in treebanks that contain a
large proportion of rightward arcs of length 1, and
the RIGHT-BUFFER-ARC transition works better in
treebanks with a large proportion of leftward arcs of
length 1. Note that, while this might seem coun-
terintuitive at a first glance, it is coherent with the
hypothesis that we formulated in Section 3.1: the
4Statistical significance was assessed using Dan Bikel?s ran-
domized parsing evaluation comparator: http://www.cis.
upenn.edu/?dbikel/software.html#comparator
Language L% R% L1% R1% Best PBT
Arabic 12.3 87.7 6.5 55.1 LBA
Chinese 58.4 41.6 35.8 15.1 RBA
Czech 41.4 58.6 22.1 24.9 LBA*
Danish 17.1 82.9 10.9 43.0 LBA
German 39.8 60.2 20.3 19.9 RBA
Portug. 32.6 67.4 22.5 26.9 LBA
Swedish 38.2 61.8 24.1 21.8 LBA*
Turkish 77.8 22.2 47.2 10.4 RBA
Table 2: Analysis of the datasets used in the experiments
in terms of: percentage of leftward and rightward links
(L%, R%), percentage of leftward and rightward links
of length 1 (L1%, R1%), and which projective buffer
transition works better for each dataset according to the
results in Table 1 (LBA = LEFT-BUFFER-ARC, RBA
= RIGHT-BUFFER-ARC). Languages where both tran-
sitions are beneficial (Czech) or harmful (Swedish) are
marked with an asterisk.
advantage of projective buffer transitions is not that
they build arcs more accurately than standard arc-
building transitions (in fact the opposite might be
expected, since they work on nodes while they are
still on the buffer and we have less information about
their surrounding nodes in our feature models), but
that they make it easier for the classifier to decide
among standard transitions. The analysis on Table
2 agrees with that explanation: LEFT-BUFFER-ARC
improves performance in treebanks where it is not
used too often but it can filter out leftward arcs of
length 1, making it easier for the parser to be accu-
rate on rightward arcs of length 1; and the converse
happens for RIGHT-BUFFER-ARC.
312
NE NE+LBA NE+RBA NE+LBA+RBA
Language LA RA LA* RA LBA LA RA* RBA LA* RA* LBA RBA
Arabic 58.28 67.77 42.61 68.65 77.46 55.88 60.63 79.70 37.40 62.28 66.78 75.94
Chinese 85.69 85.79 80.92 84.19 89.00 85.96 84.77 88.01 81.08 79.46 87.72 86.33
Czech 85.73 76.44 80.79 78.34 91.07 86.25 76.62 82.58 79.49 75.98 90.26 81.97
Danish 89.47 83.92 88.65 84.16 91.72 86.27 78.04 92.30 90.23 77.52 88.79 92.10
German 89.15 87.11 83.75 87.23 94.30 89.55 84.38 95.98 79.26 81.60 91.66 90.73
Portuguese 94.77 84.91 90.83 85.11 97.07 93.84 81.86 92.29 88.72 79.86 96.02 89.26
Swedish 87.75 80.74 84.62 81.30 92.83 87.12 74.77 90.73 78.10 72.50 90.86 89.89
Turkish 59.68 74.21 53.02 74.01 72.78 60.23 69.23 73.91 49.34 48.48 65.57 41.94
Table 3: Labelled precision of the arcs built by each transition of Nivre?s arc-eager parser without modification (NE),
with a projective buffer transition added (NE+LBA, NE+RBA) and with both projective buffer transitions added
(NE+LBA+RBA). We mark a standard LEFT-ARC (LA) or RIGHT-ARC (LA) transition with an asterisk (LA*, RA*)
when it is acting only on a ?hard? subset of leftward (rightward) arcs, and thus its precision is not directly comparable
to that of (LA, RA). Best results for each language and transition are shown in boldface.
To further test this idea, we computed the la-
belled precision of each individual transition of the
parsers with and without projective buffer transi-
tions, as shown in Table 3. As we can see, projec-
tive buffer transitions achieve better precision than
standard transitions, but this is not surprising since
they act only on ?easy? arcs of length 1. There-
fore, this high precision does not mean that they ac-
tually build arcs more accurately than the standard
transitions, since it is not measured on the same set
of arcs. Similarly, adding a projective buffer tran-
sition decreases the precision of its corresponding
standard transition, but this is because the standard
transition is then dealing only with ?harder? arcs of
length greather than 1, not because it is making more
errors. A more interesting insight comes from com-
paring transitions that are acting on the same tar-
get set of arcs: we see that, in the languages where
LEFT-BUFFER-ARC is beneficial, the addition of
this transition always improves the precision of the
standard RIGHT-ARC transition; and the converse
happens with RIGHT-BUFFER-ARC with respect to
LEFT-ARC. This further backs the hypothesis that
the filtering of ?easy? links achieved by projective
buffer transitions makes it easier for the classifier to
decide among standard transitions.
We also conducted experiments adding both tran-
sitions at the same time (NE+LBA+RBA), but the
results were worse than adding the suitable transi-
tion for each dataset. Table 3 hints at the reason: the
precision of buffer transitions noticeably decreases
when both of them are added at the same time, pre-
sumably because it is difficult for the classifier to
NE+LBA/RBA NE+PP (CoNLL X)
Language LAS UAS LAS UAS
Arabic 67.78 78.26 66.71 77.52
Chinese 86.62 90.64 86.92 90.54
Czech 78.70 84.24 78.42 84.80
Danish 85.21 90.20 84.77 89.80
German 86.48 88.90 85.82 88.76
Portug. 86.92 90.58 87.60 91.22
Swedish 82.81 88.03 84.58 89.50
Turkish 64.33 74.73 65.68 75.82
Table 4: Comparison of the parsing accuracy (LAS
and UAS, excluding punctuation) of Nivre?s arc-eager
parser with projective buffer transitions (NE+LBA/RBA)
and the parser with the pseudo-projective transformation
(Nivre et al2006)
decide between both with the restricted feature in-
formation available for buffer nodes.
To further put the obtained results into context,
Table 4 compares the performance of the arc-eager
parser with the projective buffer transition most suit-
able for each dataset with the results obtained by the
parser with the pseudo-projective transformation by
Nivre et al2006) in the CoNLL-X shared task, one
of the top two performing systems in that event. The
reader should be aware that the purpose of this ta-
ble is only to provide a broad idea of how our ap-
proach performs with respect to a well-known refer-
ence point, and not to make a detailed comparison,
since the two parsers have not been tuned in homo-
geneous conditions: on the one hand, we had access
to the CoNLL-X test sets which were unavailable
313
System Arabic Danish
Nivre et al2006) 66.71 84.77
McDonald et al2006) 66.91 84.79
Nivre (2009) 67.3 84.7
Go?mez-Rodr??guez and Nivre (2010) N/A 83.81
NE+LBA/RBA 67.78 85.21
Table 5: Comparison of the Arabic and Danish LAS ob-
tained by the arc-eager parser with projective buffer tran-
sitions in comparison to other parsers in the literature that
report results on these datasets.
for the participants in the shared task; on the other
hand, we did not fine-tune the classifier parameters
for each dataset like Nivre et al2006), but used de-
fault values for all languages.
As can be seen in the table, even though the
pseudo-projective parser is able to capture non-
projective syntactic phenomena, the algorithm with
projective buffer transitions (which is strictly pro-
jective) outperforms it in four of the eight treebanks,
including non-projective treebanks such as the Ger-
man one.
Furthermore, to our knowledge, our LAS results
for Arabic and Danish are currently the best pub-
lished results for a single-parser system on these
datasets, not only outperforming the systems partic-
ipating in CoNLL-X but also other parsers tested on
these treebanks in more recent years (see Table 5).
Finally, it is worth noting that adding projective
buffer transitions has no negative impact on effi-
ciency, either in terms of computational complex-
ity or of empirical runtime. Since each projective
buffer transition removes a node from the buffer, no
more than n such transitions can be executed for
a sentence of length n, so adding these transitions
cannot increase the complexity of a transition-based
parser. In the particular case of the arc-eager parser,
using projective buffer transitions reduces the aver-
age number of transitions needed to obtain a given
dependency forest, as some nodes can be dispatched
by a single transition rather than being shifted and
later popped from the stack. In practice, we ob-
served that the training and parsing times of the arc-
eager parser with projective buffer transitions were
slightly faster than without them on the Arabic, Chi-
nese, Swedish and Turkish treebanks, and slightly
slower than without them on the other four tree-
banks, so adding these transitions does not seem to
noticeably degrade (or improve) practical efficiency.
4 Non-projective buffer transitions
We now present a second set of transitions that still
follow the idea of early processing of some depen-
dency arcs, as in Section 3; but which are able to
create arcs skipping over a buffer node, so that they
can create some non-projective arcs. For this reason,
we call them non-projective buffer transitions.
4.1 The transitions
The two non-projective buffer transitions are defined
as follows:
? LEFT-NONPROJ-BUFFER-ARCl :
(?|i, j|k|?,A) ? (?, j|k|?,A ? {(k, l, i)}).
Preconditions: i 6= 0 and 6 ?m, l? | (m, l?, i) ?
A (single-head)
? RIGHT-NONPROJ-BUFFER-ARCl :
(?|i, j|k|?,A)? (?|i, j|?,A ? {(i, l, k)}).
The LEFT-NONPROJ-BUFFER-ARC transition
creates a leftward arc from the second buffer node
to the node on top of the stack, and then pops the
stack. It can only be executed if the node on top of
the stack does not already have a head. The RIGHT-
NONPROJ-BUFFER-ARC transition creates an arc
from the top of the stack to the second node in the
buffer, and then removes the latter from the buffer.
Note that these transitions are analogous to projec-
tive buffer transitions, and they use the second node
in the buffer in the same way, but they create arcs
involving the node on top of the stack rather than
the first buffer node. This change makes the pre-
condition that checks for a head necessary for the
transition LEFT-NONPROJ-BUFFER-ARC to respect
the single-head constraint, since many stack-based
parsers can generate configurations where the node
on top of the stack has a head.
We call these transitions non-projective buffer
transitions because, as they act on non-contiguous
nodes in the stack and buffer, they allow the creation
of a limited set of non-projective dependency arcs.
This means that, when added to a projective parser,
they will increase its coverage.5 On the other hand,
5They may also increase the coverage of parsers allowing
restricted forms of non-projectivity, but that depends on the par-
314
NE NE+LNBA NE+RNBA
Language LAS UAS LAS UAS LAS UAS
Arabic 66.43 77.19 67.13 77.90 67.21 77.92
Chinese 86.46 90.18 87.71 91.39 86.98 90.76
Czech 77.24 83.40 78.88 84.72 78.12 83.78
Danish 84.91 89.80 85.17 90.10 84.25 88.92
German 86.18 88.60 86.96 88.98 85.56 88.30
Portug. 86.60 90.20 86.78 90.34 86.07 89.92
Swedish 83.33 88.83 83.55 89.30 83.17 88.59
Turkish 63.77 74.35 63.04 73.99 65.01 75.70
Table 6: Parsing accuracy (LAS and UAS, excluding punctuation) of Nivre?s arc-eager parser without modifica-
tion (NE), with the LEFT-NONPROJ-BUFFER-ARC transition added (NE+LNBA) and with the RIGHT-NONPROJ-
BUFFER-ARC transition added (NE+RNBA). Best results for each language are shown in boldface.
adding these transitions to a stack-based transition
system does not affect soundness under the same
conditions and for the same reasons explained for
projective buffer transitions in Section 3.1.
Note that the fact that non-projective buffer tran-
sitions are able to create non-projective dependency
arcs does not mean that all the arcs that they build
are non-projective, since an arc on non-contiguous
nodes in the stack and buffer may or may not cross
other arcs. This means that non-projective buffer
transitions serve a dual purpose: not only they
increase coverage, but they also can create some
?easy? dependency links in advance of standard
transitions, just like projective buffer transitions.
Contrary to projective buffer transitions, we do
not impose any arc length restrictions on non-
projective buffer transitions (either as a hard con-
straint in the transitions themselves or as a policy in
the training oracle), since we would like the increase
in coverage to be as large as possible. We wish to
allow the parsers to create non-projective arcs in a
straightforward way and without compromising effi-
ciency. Therefore, to train the parser with these tran-
sitions, we use an oracle that employs them when-
ever possible, and delegates to the original parser?s
oracle otherwise.
4.2 Experiments
We evaluate the impact of non-projective buffer tran-
sitions on parsing accuracy by using the same base-
ticular subset of non-projective structures captured by each such
parser.
line parser, datasets and experimental settings as for
projective buffer transitions in Section 3.2. As can
be seen in Table 6, adding a non-projective buffer
transition to the arc-eager parser improves its per-
formance on all eight datasets. The improvements in
LAS are statistically significant at the .01 level (Dan
Bikel?s comparator) for Chinese, Czech and Turk-
ish. Note that the Chinese treebank is fully projec-
tive, this means that non-projective buffer transitions
are also beneficial when creating projective arcs.
While with projective buffer transitions we ob-
served that each of them was beneficial for about
half of the treebanks, and we related this to the
amount of leftward and rightward links of length 1 in
each; in the case of non-projective buffer transitions
we do not observe this tendency. In this case, LEFT-
NONPROJ-BUFFER-ARC works better than RIGHT-
NONPROJ-BUFFER-ARC in all datasets except for
Turkish and Arabic.
As with the projective transitions, we gathered
data about the individual precision of each of the
transitions. The results were similar to those for
the projective transitions, and show that adding a
non-projective buffer transition improves the preci-
sion of the standard transitions. We also experimen-
tally checked that adding both non-projective buffer
transitions at the same time (NE+LNBA+RNBA)
achieved worse performance than adding only the
most suitable transition for each dataset.
Table 7 compares the performance of the arc-
eager parser with the best non-projective buffer tran-
sition for each dataset with the results obtained by
315
NE+LNBA/RNBA NE+PP (CoNLL X)
Language LAS UAS LAS UAS
Arabic 67.21 77.92 66.71 77.52
Chinese 87.71 91.39 86.92 90.54
Czech 78.88 84.72 78.42 84.80
Danish 85.09 89.98 84.77 89.80
German 86.96 88.98 85.82 88.76
Portug. 86.78 90.34 87.60 91.22
Swedish 83.55 89.30 84.58 89.50
Turkish 65.01 75.70 65.68 75.82
Table 7: Comparison of the parsing accuracy (LAS
and UAS, excluding punctuation) of Nivre?s arc-
eager parser with non-projective buffer transitions
(NE+LNBA/RNBA) and the parser with the pseudo-
projective transformation (Nivre et al2006).
System PP PR NP NR
NE 80.40 80.76 - -
NE+LNBA/RNBA 80.96 81.33 58.87 15.66
NE+PP (CoNLL-X) 80.71 81.00 50.72 29.57
Table 8: Comparison of the precision and recall for pro-
jective (PP, PR) and non-projective (NP, NR) arcs, av-
eraged over all datasets, obtained by Nivre?s arc-eager
parser with and without non-projective buffer transitions
(NE+LNBA/RNBA, NE) and the parser with the pseudo-
projective transformation (Nivre et al2006).
the parser with the pseudo-projective transformation
by Nivre et al2006) in the CoNLL-X shared task.
Note that, like the one in Table 4, this should not
be interpreted as a homogeneous comparison. We
can see that the algorithm with non-projective buffer
transitions obtains better LAS in five out of the eight
treebanks. Precision and recall data on projective
and non-projective arcs (Table 8) show that, while
our parser does not capture as many non-projective
arcs as the pseudo-projective transformation (unsur-
prisingly, as it can only build non-projective arcs in
one direction: that of the particular non-projective
buffer transition used for each dataset); it does so
with greater precision and is more accurate than that
algorithm in projective arcs.
Like projective buffer transitions, non-projective
transitions do not increase the computational com-
plexity of stack-based parsers. The observed train-
ing and parsing times for the arc-eager parser with
non-projective buffer transitions showed a small
overhead with respect to the original arc-eager
(7.1% average increase in training time, 17.0% in
parsing time). For comparison, running the arc-
eager parser with the pseudo-projective transforma-
tion (Nivre et al2006) on the same machine pro-
duced a 23.5% increase in training time and a 87.5%
increase in parsing time.
5 Related work
The approach of adding an extra transition to a
parser to improve its accuracy has been applied in
the past by Choi and Palmer (2011). In that pa-
per, the LEFT-ARC transition from Nivre?s arc-eager
transition system is added to a list-based parser.
However, the goal of that transition is different
from ours (selecting between projective and non-
projective parsing, rather than building some arcs in
advance) and the approach is specific to one algo-
rithm while ours is generic ? for example, the LEFT-
ARC transition cannot be added to the arc-standard
and arc-eager parsers, or to extensions of those like
the ones by Attardi (2006) or Nivre (2009), because
these already have it.
The idea of creating dependency arcs of length 1
in advance to help the classifier has been used by
Cheng et al2006). However, their system creates
such arcs in a separate preprocessing step rather than
dynamically by adding a transition to the parser, and
our approach obtains better LAS and UAS results on
all the tested datasets.
The projective buffer transitions presented here
bear some resemblance to the easy-first parser by
Goldberg and Elhadad (2010), which allows cre-
ation of dependency arcs between any pair of con-
tiguous nodes and is based on the idea of ?easy? de-
pendency links being created first. However, while
the easy-first parser is an entirely new O(n log(n))
algorithm, our approach is a generic extension for
stack-based parsers that does not increase their com-
plexity (so, for example, applying it to the arc-eager
system as in the experiments in this paper yields
O(n) complexity).
Non-projective transitions that create dependency
arcs between non-contiguous nodes have been used
in the transition-based parser by Attardi (2006).
However, the transitions in that parser do not use
the second buffer node, since they are not intended
316
to create some arcs in advance. The non-projective
buffer transitions presented in this paper can also be
added to Attardi?s parser.
6 Discussion
We have presented a set of two transitions, called
projective buffer transitions, and showed that adding
one of them to Nivre?s arc-eager parser improves its
accuracy in seven out of eight tested datasets from
the CoNLL-X shared task. Furthermore, adding one
of a set of non-projective buffer transitions achieves
accuracy improvements in all of the eight datasets.
The obtained improvements are statistically signif-
icant for several of the treebanks, and the parser
with projective buffer transitions surpassed the best
published single-parser LAS results on two of them.
This comes at no cost either on computational com-
plexity or (in the case of projective transitions) on
empirical training and parsing times with respect to
the original parser.
While we have chosen Nivre?s well-known arc-
eager parser as our baseline, we have shown that
these transitions can be added to any stack-based de-
pendency parser, and we are not aware of any spe-
cific property of arc-eager that would make them
work better in practice on this parser than on others.
Therefore, future work will include an evaluation of
the impact of buffer transitions on more transition-
based parsers. Other research directions involve in-
vestigating the set of non-projective arcs allowed
by non-projective buffer transitions, defining dif-
ferent variants of buffer transitions (such as non-
projective buffer transitions that work with nodes lo-
cated deeper in the buffer) or using projective and
non-projective buffer transitions at the same time.
Acknowledgments
This research has been partially funded by the
Spanish Ministry of Economy and Competitive-
ness and FEDER (projects TIN2010-18552-C03-01
and TIN2010-18552-C03-02), Ministry of Educa-
tion (FPU Grant Program) and Xunta de Galicia
(Rede Galega de Recursos Lingu???sticos para unha
Sociedade do Con?ecemento).
References
Susana Afonso, Eckhard Bick, Renato Haber, and Diana
Santos. 2002. ?Floresta sinta?(c)tica?: a treebank for
Portuguese. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 1968?1703, Paris, France. ELRA.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003. The
annotation process in the Turkish treebank. In Pro-
ceedings of EACL Workshop on Linguistically Inter-
preted Corpora (LINC-03), pages 243?246, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the 10th Conference on Computational Natural
Language Learning (CoNLL), pages 166?170.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The tiger treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories, September 20-21, Sozopol, Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A
Library for Support Vector Machines. Software avail-
able at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Anne
Abeille?, editor, Treebanks: Building and Using Parsed
Corpora, chapter 13, pages 231?248. Kluwer.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2006. Multi-lingual dependency parsing at
NAIST. In Proceedings of the Tenth Conference on
Computational Natural Language Learning, CoNLL-
X ?06, pages 191?195, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 687?692, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pages 340?345.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871?1874.
317
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT), pages 742?750.
Carlos Go?mez-Rodr??guez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
?10, pages 1492?1501, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf, and
Emanuel Bes?ka. 2004. Prague Arabic Dependency
Treebank: Development in data and tools. In Proceed-
ings of the NEMLAR International Conference on Ara-
bic Language Resources and Tools.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague Depen-
dency Treebank 2.0. CDROM CAT: LDC2006T01,
ISBN 1-58563-370-4. Linguistic Data Consortium.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 1077?
1086, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In Pro-
ceedings of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 217?220, Va?xjo?, Swe-
den. Va?xjo? University Press.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP (ACL-IJCNLP), pages 342?
350.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 122?131.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Comput. Lin-
guist., 37:197?230.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference and
the Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP), pages 523?530.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
10th Conference on Computational Natural Language
Learning (CoNLL), pages 216?220.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from Antiquity. In Peter Juel Henrichsen, ed-
itor, Proceedings of the NODALIDA Special Session
on Treebanks.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of the 8th Conference on Computational Natural Lan-
guage Learning (CoNLL-2004), pages 49?56, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen Eryig?it,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the 10th Conference on
Computational Natural Language Learning (CoNLL),
pages 221?225.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160. ACL/SIGPARSE.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34(4):513?553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP), pages
351?359.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish treebank.
In Anne Abeille?, editor, Treebanks: Building and Us-
ing Parsed Corpora, pages 261?277. Kluwer.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce de-
pendency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 753?760.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate,
non-projective, semantically-enriched parser. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1257?
1268, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
318
In Proceedings of the 8th International Workshop on
Parsing Technologies (IWPT), pages 195?206.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 562?571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 188?193, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
319
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 66?76,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Dependency Parsing with Undirected Graphs
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a
Campus de Elvin?a, 15071
A Corun?a, Spain
carlos.gomez@udc.es
Daniel Ferna?ndez-Gonza?lez
Departamento de Informa?tica
Universidade de Vigo
Campus As Lagoas, 32004
Ourense, Spain
danifg@uvigo.es
Abstract
We introduce a new approach to transition-
based dependency parsing in which the
parser does not directly construct a depen-
dency structure, but rather an undirected
graph, which is then converted into a di-
rected dependency tree in a post-processing
step. This alleviates error propagation,
since undirected parsers do not need to ob-
serve the single-head constraint.
Undirected parsers can be obtained by sim-
plifying existing transition-based parsers
satisfying certain conditions. We apply this
approach to obtain undirected variants of
the planar and 2-planar parsers and of Cov-
ington?s non-projective parser. We perform
experiments on several datasets from the
CoNLL-X shared task, showing that these
variants outperform the original directed al-
gorithms in most of the cases.
1 Introduction
Dependency parsing has proven to be very use-
ful for natural language processing tasks. Data-
driven dependency parsers such as those by Nivre
et al(2004), McDonald et al(2005), Titov and
Henderson (2007), Martins et al(2009) or Huang
and Sagae (2010) are accurate and efficient, they
can be trained from annotated data without the
need for a grammar, and they provide a simple
representation of syntax that maps to predicate-
argument structure in a straightforward way.
In particular, transition-based dependency
parsers (Nivre, 2008) are a type of dependency
parsing algorithms which use a model that scores
transitions between parser states. Greedy deter-
ministic search can be used to select the transition
to be taken at each state, thus achieving linear or
quadratic time complexity.
0          1          2          3
Figure 1: An example dependency structure where
transition-based parsers enforcing the single-head con-
straint will incur in error propagation if they mistak-
enly build a dependency link 1 ? 2 instead of 2 ? 1
(dependency links are represented as arrows going
from head to dependent).
It has been shown by McDonald and Nivre
(2007) that such parsers suffer from error prop-
agation: an early erroneous choice can place the
parser in an incorrect state that will in turn lead to
more errors. For instance, suppose that a sentence
whose correct analysis is the dependency graph
in Figure 1 is analyzed by any bottom-up or left-
to-right transition-based parser that outputs de-
pendency trees, therefore obeying the single-head
constraint (only one incoming arc is allowed per
node). If the parser chooses an erroneous transi-
tion that leads it to build a dependency link from
1 to 2 instead of the correct link from 2 to 1, this
will lead it to a state where the single-head con-
straint makes it illegal to create the link from 3 to
2. Therefore, a single erroneous choice will cause
two attachment errors in the output tree.
With the goal of minimizing these sources of
errors, we obtain novel undirected variants of
several parsers; namely, of the planar and 2-
planar parsers by Go?mez-Rodr??guez and Nivre
(2010) and the non-projective list-based parser
described by Nivre (2008), which is based on
Covington?s algorithm (Covington, 2001). These
variants work by collapsing the LEFT-ARC and
66
RIGHT-ARC transitions in the original parsers,
which create right-to-left and left-to-right depen-
dency links, into a single ARC transition creating
an undirected link. This has the advantage that
the single-head constraint need not be observed
during the parsing process, since the directed no-
tions of head and dependent are lost in undirected
graphs. This gives the parser more freedom and
can prevent situations where enforcing the con-
straint leads to error propagation, as in Figure 1.
On the other hand, these new algorithms have
the disadvantage that their output is an undirected
graph, which has to be post-processed to recover
the direction of the dependency links and generate
a valid dependency tree. Thus, some complexity
is moved from the parsing process to this post-
processing step; and each undirected parser will
outperform the directed version only if the simpli-
fication of the parsing phase is able to avoid more
errors than are generated by the post-processing.
As will be seen in latter sections, experimental re-
sults indicate that this is in fact the case.
The rest of this paper is organized as follows:
Section 2 introduces some notation and concepts
that we will use throughout the paper. In Sec-
tion 3, we present the undirected versions of the
parsers by Go?mez-Rodr??guez and Nivre (2010)
and Nivre (2008), as well as some considerations
about the feature models suitable to train them. In
Section 4, we discuss post-processing techniques
that can be used to recover dependency trees from
undirected graphs. Section 5 presents an empir-
ical study of the performance obtained by these
parsers, and Section 6 contains a final discussion.
2 Preliminaries
2.1 Dependency Graphs
Let w = w1 . . . wn be an input string. A de-
pendency graph for w is a directed graph G =
(Vw, E), where Vw = {0, . . . , n} is the set of
nodes, and E ? Vw ? Vw is the set of directed
arcs. Each node in Vw encodes the position of
a token in w, and each arc in E encodes a de-
pendency relation between two tokens. We write
i ? j to denote a directed arc (i, j), which will
also be called a dependency link from i to j.1 We
1In practice, dependency links are usually labeled, but
to simplify the presentation we will ignore labels throughout
most of the paper. However, all the results and algorithms
presented can be applied to labeled dependency graphs and
will be so applied in the experimental evaluation.
say that i is the head of j and, conversely, that j
is a syntactic dependent of i.
Given a dependency graph G = (Vw, E), we
write i ?? j ? E if there is a (possibly empty)
directed path from i to j; and i ?? j ? E if
there is a (possibly empty) path between i and j in
the undirected graph underlying G (omitting the
references to E when clear from the context).
Most dependency-based representations of syn-
tax do not allow arbitrary dependency graphs, in-
stead, they are restricted to acyclic graphs that
have at most one head per node. Dependency
graphs satisfying these constraints are called de-
pendency forests.
Definition 1 A dependency graph G is said to be
a forest iff it satisfies:
1. Acyclicity constraint: if i ?? j, then not
j ? i.
2. Single-head constraint: if j ? i, then there
is no k 6= j such that k ? i.
A node that has no head in a dependency for-
est is called a root. Some dependency frame-
works add the additional constraint that depen-
dency forests have only one root (or, equivalently,
that they are connected). Such a forest is called a
dependency tree. A dependency tree can be ob-
tained from any dependency forest by linking all
of its root nodes as dependents of a dummy root
node, conventionally located in position 0 of the
input.
2.2 Transition Systems
In the framework of Nivre (2008), transition-
based parsers are described by means of a non-
deterministic state machine called a transition
system.
Definition 2 A transition system for dependency
parsing is a tuple S = (C, T, cs, Ct), where
1. C is a set of possible parser configurations,
2. T is a finite set of transitions, which are par-
tial functions t : C ? C,
3. cs is a total initialization function mapping
each input string to a unique initial configu-
ration, and
4. Ct ? C is a set of terminal configurations.
To obtain a deterministic parser from a non-
deterministic transition system, an oracle is used
to deterministically select a single transition at
67
each configuration. An oracle for a transition sys-
tem S = (C, T, cs, Ct) is a function o : C ? T .
Suitable oracles can be obtained in practice by
training classifiers on treebank data (Nivre et al
2004).
2.3 The Planar, 2-Planar and Covington
Transition Systems
Our undirected dependency parsers are based
on the planar and 2-planar transition systems
by Go?mez-Rodr??guez and Nivre (2010) and the
version of the Covington (2001) non-projective
parser defined by Nivre (2008). We now outline
these directed parsers briefly, a more detailed de-
scription can be found in the above references.
2.3.1 Planar
The planar transition system by Go?mez-
Rodr??guez and Nivre (2010) is a linear-time
transition-based parser for planar dependency
forests, i.e., forests whose dependency arcs do not
cross when drawn above the words. The set of
planar dependency structures is a very mild ex-
tension of that of projective structures (Kuhlmann
and Nivre, 2006).
Configurations in this system are of the form
c = ??, B,A? where ? and B are disjoint lists of
nodes from Vw (for some input w), and A is a set
of dependency links over Vw. The list B, called
the buffer, holds the input words that are still to
be read. The list ?, called the stack, is initially
empty and is used to hold words that have depen-
dency links pending to be created. The system
is shown at the top in Figure 2, where the nota-
tion ? | i is used for a stack with top i and tail ?,
and we invert the notation for the buffer for clarity
(i.e., i | B as a buffer with top i and tail B).
The system reads the input sentence and creates
links in a left-to-right order by executing its four
transitions, until it gets to a terminal configura-
tion. A SHIFT transition moves the first (leftmost)
node in the buffer to the top of the stack. Transi-
tions LEFT-ARC and RIGHT-ARC create leftward
or rightward link, respectively, involving the first
node in the buffer and the topmost node in the
stack. Finally, REDUCE transition is used to pop
the top word from the stack when we have fin-
ished building arcs to or from it.
2.3.2 2-Planar
The 2-planar transition system by Go?mez-
Rodr??guez and Nivre (2010) is an extension of
the planar system that uses two stacks, allowing
it to recognize 2-planar structures, a larger set
of dependency structures that has been shown to
cover the vast majority of non-projective struc-
tures in a number of treebanks (Go?mez-Rodr??guez
and Nivre, 2010).
This transition system, shown in Figure 2, has
configurations of the form c = ??0,?1, B,A? ,
where we call ?0 the active stack and ?1 the in-
active stack. Its SHIFT, LEFT-ARC, RIGHT-ARC
and REDUCE transitions work similarly to those
in the planar parser, but while SHIFT pushes the
first word in the buffer to both stacks; the other
three transitions only work with the top of the ac-
tive stack, ignoring the inactive one. Finally, a
SWITCH transition is added that makes the active
stack inactive and vice versa.
2.3.3 Covington Non-Projective
Covington (2001) proposes several incremen-
tal parsing strategies for dependency representa-
tions and one of them can recover non-projective
dependency graphs. Nivre (2008) implements a
variant of this strategy as a transition system with
configurations of the form c = ??1, ?2, B,A?,
where ?1 and ?2 are lists containing partially pro-
cessed words and B is the buffer list of unpro-
cessed words.
The Covington non-projective transition sys-
tem is shown at the bottom in Figure 2. At each
configuration c = ??1, ?2, B,A?, the parser has
to consider whether any dependency arc should
be created involving the top of the buffer and the
words in ?1. A LEFT-ARC transition adds a link
from the first node j in the buffer to the node in the
head of the list ?1, which is moved to the list ?2
to signify that we have finished considering it as a
possible head or dependent of j. The RIGHT-ARC
transition does the same manipulation, but creat-
ing the symmetric link. A NO-ARC transition re-
moves the head of the list ?1 and inserts it at the
head of the list ?2 without creating any arcs: this
transition is to be used where there is no depen-
dency relation between the top node in the buffer
and the head of ?1, but we still may want to cre-
ate an arc involving the top of the buffer and other
nodes in ?1. Finally, if we do not want to create
any such arcs at all, we can execute a SHIFT tran-
sition, which advances the parsing process by re-
moving the first node in the bufferB and inserting
it at the head of a list obtained by concatenating
68
?1 and ?2. This list becomes the new ?1, whereas
?2 is empty in the resulting configuration.
Note that the Covington parser has quadratic
complexity with respect to input length, while the
planar and 2-planar parsers run in linear time.
3 The Undirected Parsers
The transition systems defined in Section 2.3
share the common property that their LEFT-ARC
and RIGHT-ARC have exactly the same effects ex-
cept for the direction of the links that they create.
We can take advantage of this property to define
undirected versions of these transition systems, by
transforming them as follows:
? Configurations are changed so that the arc set
A is a set of undirected arcs, instead of di-
rected arcs.
? The LEFT-ARC and RIGHT-ARC transitions
in each parser are collapsed into a single ARC
transition that creates an undirected arc.
? The preconditions of transitions that guaran-
tee the single-head constraint are removed,
since the notions of head and dependent are
lost in undirected graphs.
By performing these transformations and leaving
the systems otherwise unchanged, we obtain the
undirected variants of the planar, 2-planar and
Covington algorithms that are shown in Figure 3.
Note that the transformation can be applied
to any transition system having LEFT-ARC and
RIGHT-ARC transitions that are equal except for
the direction of the created link, and thus col-
lapsable into one. The above three transition sys-
tems fulfill this property, but not every transition
system does. For example, the well-known arc-
eager parser of Nivre (2003) pops a node from the
stack when creating left arcs, and pushes a node
to the stack when creating right arcs, so the trans-
formation cannot be applied to it.2
2One might think that the arc-eager algorithm could still
be transformed by converting each of its arc transitions into
an undirected transition, without collapsing them into one.
However, this would result into a parser that violates the
acyclicity constraint, since the algorithm is designed in such
a way that acyclicity is only guaranteed if the single-head
constraint is kept. It is easy to see that this problem cannot
happen in parsers where LEFT-ARC and RIGHT-ARC transi-
tions have the same effect: in these, if a directed graph is not
parsable in the original algorithm, its underlying undirected
graph cannot not be parsable in the undirected variant.
3.1 Feature models
Some of the features that are typically used to
train transition-based dependency parsers depend
on the direction of the arcs that have been built up
to a certain point. For example, two such features
for the planar parser could be the POS tag associ-
ated with the head of the topmost stack node, or
the label of the arc going from the first node in the
buffer to its leftmost dependent.3
As the notion of head and dependent is lost in
undirected graphs, this kind of features cannot be
used to train undirected parsers. Instead, we use
features based on undirected relations between
nodes. We found that the following kinds of fea-
tures worked well in practice as a replacement for
features depending on arc direction:
? Information about the ith node linked to a
given node (topmost stack node, topmost
buffer node, etc.) on the left or on the right,
and about the associated undirected arc, typi-
cally for i = 1, 2, 3,
? Information about whether two nodes are
linked or not in the undirected graph, and
about the label of the arc between them,
? Information about the first left and right
?undirected siblings? of a given node, i.e., the
first node q located to the left of the given node
p such that p and q are linked to some common
node r located to the right of both, and vice
versa. Note that this notion of undirected sib-
lings does not correspond exclusively to sib-
lings in the directed graph, since it can also
capture other second-order interactions, such
as grandparents.
4 Reconstructing the dependency forest
The modified transition systems presented in the
previous section generate undirected graphs. To
obtain complete dependency parsers that are able
to produce directed dependency forests, we will
need a reconstruction step that will assign a direc-
tion to the arcs in such a way that the single-head
constraint is obeyed. This reconstruction step can
be implemented by building a directed graph with
weighted arcs corresponding to both possible di-
rections of each undirected edge, and then finding
an optimum branching to reduce it to a directed
3These example features are taken from the default model
for the planar parser in version 1.5 of MaltParser (Nivre et
al., 2006).
69
Planar initial/terminal configurations: cs(w1 . . . wn) = ?[], [1 . . . n], ??, Cf = {??, [], A? ? C}
Transitions: SHIFT ??, i|B,A? ? ??|i, B,A?
REDUCE ??|i, B,A? ? ??, B,A?
LEFT-ARC ??|i, j|B,A? ? ??|i, j|B,A ? {(j, i)}?
only if @k | (k, i) ? A (single-head) and i?? j 6? A (acyclicity).
RIGHT-ARC ??|i, j|B,A? ? ??|i, j|B,A ? {(i, j)}?
only if @k | (k, j) ? A (single-head) and i?? j 6? A (acyclicity).
2-Planar initial/terminal configurations: cs(w1 . . . wn) = ?[], [], [1 . . . n], ??, Cf = {??0,?1, [], A? ? C}
Transitions: SHIFT ??0,?1, i|B,A? ? ??0|i,?1|i, B,A?
REDUCE ??0|i,?1, B,A? ? ??0,?1, B,A?
LEFT-ARC ??0|i,?1, j|B,A? ? ??0|i,?1, j|B,A ? {j, i)}?
only if @k | (k, i) ? A (single-head) and i?? j 6? A (acyclicity).
RIGHT-ARC ??0|i,?1, j|B,A? ? ??0|i,?1, j|B,A ? {(i, j)}?
only if @k | (k, j) ? A (single-head) and i?? j 6? A (acyclicity).
SWITCH ??0,?1, B,A? ? ??1,?0, B,A?
Covington initial/term. configurations: cs(w1 . . . wn) = ?[], [], [1 . . . n], ??, Cf = {??1, ?2, [], A? ? C}
Transitions: SHIFT ??1, ?2, i|B,A? ? ??1 ? ?2|i, [], B,A?
NO-ARC ??1|i, ?2, B,A? ? ??1, i|?2, B,A?
LEFT-ARC ??1|i, ?2, j|B,A? ? ??1, i|?2, j|B,A ? {(j, i)}?
only if @k | (k, i) ? A (single-head) and i?? j 6? A (acyclicity).
RIGHT-ARC ??1|i, ?2, j|B,A? ? ??1, i|?2, j|B,A ? {(i, j)}?
only if @k | (k, j) ? A (single-head) and i?? j 6? A (acyclicity).
Figure 2: Transition systems for planar, 2-planar and Covington non-projective dependency parsing.
Undirected Planar initial/term. conf.: cs(w1 . . . wn) = ?[], [1 . . . n], ??, Cf = {??, [], A? ? C}
Transitions: SHIFT ??, i|B,A? ? ??|i, B,A?
REDUCE ??|i, B,A? ? ??, B,A?
ARC ??|i, j|B,A? ? ??|i, j|B,A ? {{i, j}}?
only if i?? j 6? A (acyclicity).
Undirected 2-Planar initial/term. conf.: cs(w1 . . . wn) = ?[], [], [1 . . . n], ??, Cf = {??0,?1, [], A? ? C}
Transitions: SHIFT ??0,?1, i|B,A? ? ??0|i,?1|i, B,A?
REDUCE ??0|i,?1, B,A? ? ??0,?1, B,A?
ARC ??0|i,?1, j|B,A? ? ??0|i,?1, j|B,A ? {{i, j}}?
only if i?? j 6? A (acyclicity).
SWITCH ??0,?1, B,A? ? ??1,?0, B,A?
Undirected Covington init./term. conf.: cs(w1 . . . wn) = ?[], [], [1 . . . n], ??, Cf = {??1, ?2, [], A? ? C}
Transitions: SHIFT ??1, ?2, i|B,A? ? ??1 ? ?2|i, [], B,A?
NO-ARC ??1|i, ?2, B,A? ? ??1, i|?2, B,A?
ARC ??1|i, ?2, j|B,A? ? ??1, i|?2, j|B,A ? {{i, j}}?
only if i?? j 6? A (acyclicity).
Figure 3: Transition systems for undirected planar, 2-planar and Covington non-projective dependency parsing.
70
tree. Different criteria for assigning weights to
arcs provide different variants of the reconstruc-
tion technique.
To describe these variants, we first introduce
preliminary definitions. Let U = (Vw, E) be
an undirected graph produced by an undirected
parser for some string w. We define the follow-
ing sets of arcs:
A1(U) = {(i, j) | j 6= 0 ? {i, j} ? E},
A2(U) = {(0, i) | i ? Vw}.
Note that A1(U) represents the set of arcs ob-
tained from assigning an orientation to an edge
in U , except arcs whose dependent is the dummy
root, which are disallowed. On the other hand,
A2(U) contains all the possible arcs originating
from the dummy root node, regardless of whether
their underlying undirected edges are in U or not;
this is so that reconstructions are allowed to link
unattached tokens to the dummy root.
The reconstruction process consists of finding
a minimum branching (i.e. a directed minimum
spanning tree) for a weighted directed graph ob-
tained from assigning a cost c(i, j) to each arc
(i, j) of the following directed graph:
D(U) = {Vw, A(U) = A1(U) ?A2(U)}.
That is, we will find a dependency tree T =
(Vw, AT ? A(U)) such that the sum of costs of
the arcs in AT is minimal. In general, such a min-
imum branching can be calculated with the Chu-
Liu-Edmonds algorithm (Chu and Liu, 1965; Ed-
monds, 1967). Since the graph D(U) has O(n)
nodes and O(n) arcs for a string of length n, this
can be done in O(n log n) if implemented as de-
scribed by Tarjan (1977).
However, applying these generic techniques is
not necessary in this case: since our graph U is
acyclic, the problem of reconstructing the forest
can be reduced to choosing a root word for each
connected component in the graph, linking it as
a dependent of the dummy root and directing the
other arcs in the component in the (unique) way
that makes them point away from the root.
It remains to see how to assign the costs c(i, j)
to the arcs of D(U): different criteria for assign-
ing scores will lead to different reconstructions.
4.1 Naive reconstruction
A first, very simple reconstruction technique can
be obtained by assigning arc costs to the arcs in
A(U) as follows:
c(i, j)
{
1 if (i, j) ? A1(U),
2 if (i, j) ? A2(U) ? (i, j) 6? A1(U).
This approach gives the same cost to all arcs
obtained from the undirected graph U , while also
allowing (at a higher cost) to attach any node to
the dummy root. To obtain satisfactory results
with this technique, we must train our parser to
explicitly build undirected arcs from the dummy
root node to the root word(s) of each sentence us-
ing arc transitions (note that this implies that we
need to represent forests as trees, in the manner
described at the end of Section 2.1). Under this
assumption, it is easy to see that we can obtain the
correct directed tree T for a sentence if it is pro-
vided with its underlying undirected tree U : the
tree is obtained in O(n) as the unique orientation
of U that makes each of its edges point away from
the dummy root.
This approach to reconstruction has the advan-
tage of being very simple and not adding any com-
plications to the parsing process, while guarantee-
ing that the correct directed tree will be recovered
if the undirected tree for a sentence is generated
correctly. However, it is not very robust, since the
direction of all the arcs in the output depends on
which node is chosen as sentence head and linked
to the dummy root. Therefore, a parsing error af-
fecting the undirected edge involving the dummy
root may result in many dependency links being
erroneous.
4.2 Label-based reconstruction
To achieve a more robust reconstruction, we use
labels to encode a preferred direction for depen-
dency arcs. To do so, for each pre-existing label
X in the training set, we create two labels Xl and
Xr. The parser is then trained on a modified ver-
sion of the training set where leftward links orig-
inally labelled X are labelled Xl, and rightward
links originally labelled X are labelled Xr. Thus,
the output of the parser on a new sentence will be
an undirected graph where each edge has a label
with an annotation indicating whether the recon-
struction process should prefer to link the pair of
nodes with a leftward or a rightward arc. We can
then assign costs to our minimum branching algo-
rithm so that it will return a tree agreeing with as
many such annotations as possible.
71
To do this, we call A1+(U) ? A1(U) the set
of arcs in A1(U) that agree with the annotations,
i.e., arcs (i, j) ? A1(U) where either i < j and
i, j is labelledXr inU , or i > j and i, j is labelled
Xl in U . We callA1?(U) the set of arcs inA1(U)
that disagree with the annotations, i.e.,A1?(U) =
A1(U)\A1+(U). And we assign costs as follows:
c(i, j)
?
?
?
1 if (i, j) ? A1+(U),
2 if (i, j) ? A1?(U),
2n if (i, j) ? A2(U) ? (i, j) 6? A1(U).
where n is the length of the string.
With these costs, the minimum branching algo-
rithm will find a tree which agrees with as many
annotations as possible. Additional arcs from the
root not corresponding to any edge in the output
of the parser (i.e. arcs inA2(U) but not inA1(U))
will be used only if strictly necessary to guarantee
connectedness, this is implemented by the high
cost for these arcs.
While this may be the simplest cost assignment
to implement label-based reconstruction, we have
found that better empirical results are obtained if
we give the algorithm more freedom to create new
arcs from the root, as follows:
c(i, j)
?
?
?
1 if (i, j) ? A1+(U) ? (i, j) 6? A2(U),
2 if (i, j) ? A1?(U) ? (i, j) 6? A2(U),
2n if (i, j) ? A2(U).
While the cost of arcs from the dummy root is
still 2n, this is now so even for arcs that are in the
output of the undirected parser, which had cost 1
before. Informally, this means that with this con-
figuration the postprocessor does not ?trust? the
links from the dummy root created by the parser,
and may choose to change them if it is conve-
nient to get a better agreement with label anno-
tations (see Figure 4 for an example of the dif-
ference between both cost assignments). We be-
lieve that the better accuracy obtained with this
criterion probably stems from the fact that it is bi-
ased towards changing links from the root, which
tend to be more problematic for transition-based
parsers, while respecting the parser output for
links located deeper in the dependency structure,
for which transition-based parsers tend to be more
accurate (McDonald and Nivre, 2007).
Note that both variants of label-based recon-
struction have the property that, if the undirected
parser produces the correct edges and labels for a
0        1       2        3       4        5
R
R L L L
0        1       2        3       4        5
0        1       2        3       4        5
a.
b.
c.
Figure 4: a) An undirected graph obtained by the
parser with the label-based transformation, b) and c)
The dependency graph obtained by each of the variants
of the label-based reconstruction (note how the second
variant moves an arc from the root).
given sentence, then the obtained directed tree is
guaranteed to be correct (as it will simply be the
tree obtained by decoding the label annotations).
5 Experiments
In this section, we evaluate the performance of the
undirected planar, 2-planar and Covington parsers
on eight datasets from the CoNLL-X shared task
(Buchholz and Marsi, 2006).
Tables 1, 2 and 3 compare the accuracy of the
undirected versions with naive and label-based re-
construction to that of the directed versions of
the planar, 2-planar and Covington parsers, re-
spectively. In addition, we provide a comparison
to well-known state-of-the-art projective and non-
projective parsers: the planar parsers are com-
pared to the arc-eager projective parser by Nivre
(2003), which is also restricted to planar struc-
tures; and the 2-planar parsers are compared with
the arc-eager parser with pseudo-projective trans-
formation of Nivre and Nilsson (2005), capable of
handling non-planar dependencies.
We use SVM classifiers from the LIBSVM
package (Chang and Lin, 2001) for all the lan-
guages except Chinese, Czech and German. In
these, we use the LIBLINEAR package (Fan et
al., 2008) for classification, which reduces train-
ing time for these larger datasets; and feature
models adapted to this system which, in the case
of German, result in higher accuracy than pub-
lished results using LIBSVM.
72
The LIBSVM feature models for the arc-eager
projective and pseudo-projective parsers are the
same used by these parsers in the CoNLL-X
shared task, where the pseudo-projective version
of MaltParser was one of the two top performing
systems (Buchholz and Marsi, 2006). For the 2-
planar parser, we took the feature models from
Go?mez-Rodr??guez and Nivre (2010) for the lan-
guages included in that paper. For all the algo-
rithms and datasets, the feature models used for
the undirected parsers were adapted from those of
the directed parsers as described in Section 3.1.4
The results show that the use of undirected
parsing with label-based reconstruction clearly
improves the performance in the vast majority of
the datasets for the planar and Covington algo-
rithms, where in many cases it also improves upon
the corresponding projective and non-projective
state-of-the-art parsers provided for comparison.
In the case of the 2-planar parser the results are
less conclusive, with improvements over the di-
rected versions in five out of the eight languages.
The improvements in LAS obtained with label-
based reconstruction over directed parsing are sta-
tistically significant at the .05 level5 for Danish,
German and Portuguese in the case of the pla-
nar parser; and Czech, Danish and Turkish for
Covington?s parser. No statistically significant de-
crease in accuracy was detected in any of the al-
gorithm/dataset combinations.
As expected, the good results obtained by the
undirected parsers with label-based reconstruc-
tion contrast with those obtained by the variants
with root-based reconstruction, which performed
worse in all the experiments.
6 Discussion
We have presented novel variants of the planar
and 2-planar transition-based parsers by Go?mez-
Rodr??guez and Nivre (2010) and of Covington?s
non-projective parser (Covington, 2001; Nivre,
2008) which ignore the direction of dependency
links, and reconstruction techniques that can be
used to recover the direction of the arcs thus pro-
duced. The results obtained show that this idea
of undirected parsing, together with the label-
4All the experimental settings and feature models used
are included in the supplementary material and also available
at http://www.grupolys.org/?cgomezr/exp/.
5Statistical significance was assessed using Dan Bikel?s
randomized comparator: http://www.cis.upenn.
edu/?dbikel/software.html
based reconstruction technique of Section 4.2, im-
proves parsing accuracy on most of the tested
dataset/algorithm combinations, and it can out-
perform state-of-the-art transition-based parsers.
The accuracy improvements achieved by re-
laxing the single-head constraint to mitigate er-
ror propagation were able to overcome the er-
rors generated in the reconstruction phase, which
were few: we observed empirically that the dif-
ferences between the undirected LAS obtained
from the undirected graph before the reconstruc-
tion and the final directed LAS are typically be-
low 0.20%. This is true both for the naive and
label-based transformations, indicating that both
techniques are able to recover arc directions accu-
rately, and the accuracy differences between them
come mainly from the differences in training (e.g.
having tentative arc direction as part of feature
information in the label-based reconstruction and
not in the naive one) rather than from the differ-
ences in the reconstruction methods themselves.
The reason why we can apply the undirected
simplification to the three parsers that we have
used in this paper is that their LEFT-ARC and
RIGHT-ARC transitions have the same effect ex-
cept for the direction of the links they create.
The same transformation and reconstruction tech-
niques could be applied to any other transition-
based dependency parsers sharing this property.
The reconstruction techniques alone could po-
tentially be applied to any dependency parser
(transition-based or not) as long as it can be some-
how converted to output undirected graphs.
The idea of parsing with undirected relations
between words has been applied before in the
work on Link Grammar (Sleator and Temperley,
1991), but in that case the formalism itself works
with undirected graphs, which are the final out-
put of the parser. To our knowledge, the idea of
using an undirected graph as an intermediate step
towards obtaining a dependency structure has not
been explored before.
Acknowledgments
This research has been partially funded by the Spanish
Ministry of Economy and Competitiveness and FEDER
(projects TIN2010-18552-C03-01 and TIN2010-18552-
C03-02), Ministry of Education (FPU Grant Program) and
Xunta de Galicia (Rede Galega de Recursos Lingu???sticos
para unha Soc. do Con?ec.). The experiments were conducted
with the help of computing resources provided by the Su-
percomputing Center of Galicia (CESGA). We thank Joakim
Nivre for helpful input in the early stages of this work.
73
Planar UPlanarN UPlanarL MaltP
Lang. LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)
Arabic 66.93 (67.34) 77.56 (77.22) 65.91 (66.33) 77.03 (76.75) 66.75 (67.19) 77.45 (77.22) 66.43 (66.74) 77.19 (76.83)
Chinese 84.23 (84.20) 88.37 (88.33) 83.14 (83.10) 87.00 (86.95) 84.51* (84.50*) 88.37 (88.35*) 86.42 (86.39) 90.06 (90.02)
Czech 77.24 (77.70) 83.46 (83.24) 75.08 (75.60) 81.14 (81.14) 77.60* (77.93*) 83.56* (83.41*) 77.24 (77.57) 83.40 (83.19)
Danish 83.31 (82.60) 88.02 (86.64) 82.65 (82.45) 87.58 (86.67*) 83.87* (83.83*) 88.94* (88.17*) 83.31 (82.64) 88.30 (86.91)
German 84.66 (83.60) 87.02 (85.67) 83.33 (82.77) 85.78 (84.93) 86.32* (85.67*) 88.62* (87.69*) 86.12 (85.48) 88.52 (87.58)
Portug. 86.22 (83.82) 89.80 (86.88) 85.89 (83.82) 89.68 (87.06*) 86.52* (84.83*) 90.28* (88.03*) 86.60 (84.66) 90.20 (87.73)
Swedish 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55)
Turkish 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95)
Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL)
postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP)
algorithms, on eight datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Arabic (Hajic? et al
2004), Chinese (Chen et al 2003), Czech (Hajic? et al 2006), Danish (Kromann, 2003), German (Brants et
al., 2002), Portuguese (Afonso et al 2002), Swedish (Nilsson et al 2005) and Turkish (Oflazer et al 2003;
Atalay et al 2003). We show labelled (LAS) and unlabelled (UAS) attachment score excluding and including
punctuation tokens in the scoring (the latter in brackets). Best results for each language are shown in boldface,
and results where the undirected parser outperforms the directed version are marked with an asterisk.
2Planar U2PlanarN U2PlanarL MaltPP
Lang. LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)
Arabic 66.73 (67.19) 77.33 (77.11) 66.37 (66.93) 77.15 (77.09) 66.13 (66.52) 76.97 (76.70) 65.93 (66.02) 76.79 (76.14)
Chinese 84.35 (84.32) 88.31 (88.27) 83.02 (82.98) 86.86 (86.81) 84.45* (84.42*) 88.29 (88.25) 86.42 (86.39) 90.06 (90.02)
Czech 77.72 (77.91) 83.76 (83.32) 74.44 (75.19) 80.68 (80.80) 78.00* (78.59*) 84.22* (84.21*) 78.86 (78.47) 84.54 (83.89)
Danish 83.81 (83.61) 88.50 (87.63) 82.00 (81.63) 86.87 (85.80) 83.75 (83.65*) 88.62* (87.82*) 83.67 (83.54) 88.52 (87.70)
German 86.28 (85.76) 88.68 (87.86) 82.93 (82.53) 85.52 (84.81) 86.52* (85.99*) 88.72* (87.92*) 86.94 (86.62) 89.30 (88.69)
Portug. 87.04 (84.92) 90.82 (88.14) 85.61 (83.45) 89.36 (86.65) 86.70 (84.75) 90.38 (87.88) 87.08 (84.90) 90.66 (87.95)
Swedish 83.13 (82.71) 88.57 (87.59) 81.00 (80.71) 86.54 (85.68) 82.59 (82.25) 88.19 (87.29) 83.39 (82.67) 88.59 (87.38)
Turkish 61.80 (70.09) 72.75 (77.39) 58.10 (67.44) 68.03 (74.06) 61.92* (70.64*) 72.18 (77.46*) 62.80 (71.33) 73.49 (78.44)
Table 2: Parsing accuracy of the undirected 2-planar parser with naive (U2PlanarN) and label-based (U2PlanarL)
postprocessing in comparison to the directed 2-planar (2Planar) and MaltParser arc-eager pseudo-projective
(MaltPP) algorithms. The meaning of the scores shown is as in Table 1.
Covington UCovingtonN UCovingtonL
Lang. LAS(p) UAS(p) LAS(p) UAS(p) LAS(p) UAS(p)
Arabic 65.17 (65.49) 75.99 (75.69) 63.49 (63.93) 74.41 (74.20) 65.61* (65.81*) 76.11* (75.66)
Chinese 85.61 (85.61) 89.64 (89.62) 84.12 (84.02) 87.85 (87.73) 86.28* (86.17*) 90.16* (90.04*)
Czech 78.26 (77.43) 84.04 (83.15) 74.02 (74.78) 79.80 (79.92) 78.42* (78.69*) 84.50* (84.16*)
Danish 83.63 (82.89) 88.50 (87.06) 82.00 (81.61) 86.55 (85.51) 84.27* (83.85*) 88.82* (87.75*)
German 86.70 (85.69) 89.08 (87.78) 84.03 (83.51) 86.16 (85.39) 86.50 (85.90*) 88.84 (87.95*)
Portug. 84.73 (82.56) 89.10 (86.30) 83.83 (81.71) 87.88 (85.17) 84.95* (82.70*) 89.18* (86.31*)
Swedish 83.53 (82.76) 88.91 (87.61) 81.78 (81.47) 86.78 (85.96) 83.09 (82.73) 88.11 (87.23)
Turkish 64.25 (72.70) 74.85 (79.75) 63.51 (72.08) 74.07 (79.10) 64.91* (73.38*) 75.46* (80.40*)
Table 3: Parsing accuracy of the undirected Covington non-projective parser with naive (UCovingtonN) and
label-based (UCovingtonL) postprocessing in comparison to the directed algorithm (Covington). The meaning
of the scores shown is as in Table 1.
74
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC 2002), pages 1968?1703, Paris,
France. ELRA.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank.
In Proceedings of EACL Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), pages 243?
246, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, September 20-21,
Sozopol, Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vec-
tor Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen,
C. Huang, and Z. Gao. 2003. Sinica treebank: De-
sign criteria, representational issues and implemen-
tation. In Anne Abeille?, editor, Treebanks: Building
and Using Parsed Corpora, chapter 13, pages 231?
248. Kluwer.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael A. Covington. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95?102.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Carlos Go?mez-Rodr??guez and Joakim Nivre. 2010.
A transition-based parser for 2-planar dependency
structures. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL ?10, pages 1492?1501, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf,
and Emanuel Bes?ka. 2004. Prague Arabic Depen-
dency Treebank: Development in data and tools. In
Proceedings of the NEMLAR International Confer-
ence on Arabic Language Resources and Tools.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek,
Jir??? Havelka, and Marie Mikulova?. 2006.
Prague Dependency Treebank 2.0. CDROM CAT:
LDC2006T01, ISBN 1-58563-370-4. Linguistic
Data Consortium.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 1077?1086, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT), pages 217?220, Va?xjo?,
Sweden. Va?xjo? University Press.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507?514.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 342?350.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference
and the Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), pages
523?530.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from Antiquity. In Peter Juel Henrichsen,
editor, Proceedings of the NODALIDA Special Ses-
sion on Treebanks.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL-2004), pages 49?
56, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
75
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 2216?2219.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technolo-
gies (IWPT), pages 149?160.
Joakim Nivre. 2008. Algorithms for Deterministic
Incremental Dependency Parsing. Computational
Linguistics, 34(4):513?553.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In Anne Abeille?, editor, Treebanks: Build-
ing and Using Parsed Corpora, pages 261?277.
Kluwer.
Daniel Sleator and Davy Temperley. 1991. Pars-
ing English with a link grammar. Technical Re-
port CMU-CS-91-196, Carnegie Mellon University,
Computer Science.
R. E. Tarjan. 1977. Finding optimum branchings.
Networks, 7:25?35.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference
on Parsing Technologies (IWPT), pages 144?155.
76
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276?284,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Efficient Parsing of Well-Nested Linear Context-Free Rewriting Systems
Carlos G?mez-Rodr?guez1, Marco Kuhlmann2, and Giorgio Satta3
1Departamento de Computaci?n, Universidade da Coru?a, Spain, cgomezr@udc.es
2Department of Linguistics and Philology, Uppsala University, Sweden, marco.kuhlmann@lingfil.uu.se
3Department of Information Engineering, University of Padua, Italy, satta@dei.unipd.it
Abstract
The use of well-nested linear context-free
rewriting systems has been empirically moti-
vated for modeling of the syntax of languages
with discontinuous constituents or relatively
free word order. We present a chart-based pars-
ing algorithm that asymptotically improves the
known running time upper bound for this class
of rewriting systems. Our result is obtained
through a linear space construction of a binary
normal form for the grammar at hand.
1 Introduction
Since its earliest years, one of the main goals of
computational linguistics has been the modeling of
natural language syntax by means of formal gram-
mars. Following results by Huybregts (1984) and
Shieber (1985), special attention has been given to
formalisms that enlarge the generative power of con-
text-free grammars, but still remain below the full
generative power of context-sensitive grammars. On
this line of investigation, mildly context-sensitive
grammar formalisms have been introduced (Joshi,
1985), including, among several others, the tree ad-
joining grammars (TAGs) of Joshi et al (1975).
Linear context-free rewriting system (LCFRS), in-
troduced by Vijay-Shanker et al (1987), is a mildly
context-sensitive formalism that allows the deriva-
tion of tuples of strings, i.e., discontinuous phrases.
This feature has been used to model phrase structure
treebanks with discontinuous constituents (Maier and
S?gaard, 2008), as well as to map non-projective de-
pendency trees into discontinuous phrase structures
(Kuhlmann and Satta, 2009).
Informally, in an LCFRS G, each nonterminal can
generate string tuples with a fixed number of compo-
nents. The fan-out of G is defined as the maximum
number of tuple components generated by G. During
a derivation of an LCFRS, tuple components gener-
ated by the nonterminals in the right-hand side of
a production are concatenated to form new tuples,
possibly adding some terminal symbols. The only re-
striction applying to these generalized concatenation
operations is linearity, that is, components cannot be
duplicated or deleted.
The freedom in the rearrangement of components
has specific consequences in terms of the computa-
tional and descriptional complexity of LCFRS. Even
for grammars with bounded fan-out, the universal
recognition problem is NP-hard (Satta, 1992), and
these systems lack Chomsky-like normal forms for
fixed fan-out (Rambow and Satta, 1999) that are es-
pecially convenient in tabular parsing. This is in con-
trast with other mildly context-sensitive formalisms,
and TAG in particular: TAGs can be parsed in poly-
nomial time both with respect to grammar size and
string size, and they can be cast in normal forms
having binary derivation trees only.
It has recently been argued that LCFRS might be
too powerful for modeling languages with discontin-
uous constituents or with relatively free word order,
and that additional restrictions on the rearrangement
of components might be needed. More specifically,
analyses of both dependency and constituency tree-
banks (Kuhlmann and Nivre, 2006; Havelka, 2007;
Maier and Lichte, 2009) have shown that rearrange-
ments of argument tuples almost always satisfy the
so-called well-nestedness condition, a generalization
276
of the standard condition on balanced brackets. This
condition states that any two components x1, x2 of
some tuple will never be composed with any two
components y1, y2 of some other tuple in such a way
that a ?crossing? configuration is realized.
In this paper, we contribute to a better understand-
ing of the formal properties of well-nested LCFRS.
We show that, when fan-out is bounded by any inte-
ger ? ? 1, these systems can always be transformed,
in an efficient way, into a specific normal form with
no more than two nonterminals in their productions?
right-hand sides. On the basis of this result, we
then develop an efficient parsing algorithm for well-
nested LCFRS, running in timeO(? ? |G| ? |w|2?+2),
where G and w are the input grammar and string,
respectively. Well-nested LCFRS with fan-out ? = 2
are weakly equivalent to TAG, and our complex-
ity result reduces to the well-known upper bound
O(|G| ? |w|6) for this class. For ? > 2, our upper
bound is asymptotically better than the one obtained
from existing parsing algorithms for general LCFRS
or equivalent formalisms (Seki et al, 1991).
Well-nested LCFRS are generatively equivalent
to (among others) coupled context-free grammars
(CCFG), introduced by Hotz and Pitsch (1996).
These authors also provide a normal form and de-
velop a parsing algorithm for CCFGs. One difference
with respect to our result is that the normal form for
CCFGs allows more than two nonterminals to appear
in the right-hand side of a production, even though no
nonterminal may contribute more than two tuple com-
ponents. Also, the construction in (Hotz and Pitsch,
1996) results in a blow-up of the grammar that is ex-
ponential in its fan-out, and the parsing algorithm that
is derived runs in time O(4? ? |G| ? |w|2?+2). Our
result is therefore a considerable asymptotic improve-
ment over the CCFG result, both with respect to the
normal form construction and the parsing efficiency.
Finally, under a practical perspective, our parser is a
simple chart-based algorithm, while the algorithm in
(Hotz and Pitsch, 1996) involves two passes and is
considerably more complex to analyze and to imple-
ment than ours.
Kanazawa and Salvati (2010) mention a normal
form for well-nested multiple context-free grammars.
Structure In Section 2, we introduce LCFRS and
the class of well-nested LCFRS that is the focus of
this paper. In Section 3, we discuss the parsing com-
plexity of LCFRS, and show why grammars using
our normal form can be parsed efficiently. Section 4
presents the transformation of a well-nested LCFRS
into the normal form. Section 5 concludes the paper.
2 Linear Context-Free Rewriting Systems
We write [n] to denote the set of positive integers up
to and including n: [n] = {1, . . . , n}.
2.1 Linear, non-erasing functions
Let ? be an alphabet. For integers m ? 0 and
k1, . . . , km, k ? 1, a total function
f : (??)k1 ? ? ? ? ? (??)km ? (??)k
is called a linear, non-erasing function over ? with
type k1 ? ? ? ? ? km ? k, if it can be defined by an
equation of the form
f(?x1,1, . . . , x1,k1?, . . . , ?xm,1, . . . , xm,km?) = ~? ,
where ~? is a k-tuple of strings over the variables on
the left-hand side of the equation and ? with the
property that each variable occurs in ~? exactly once.
The values m and k are called the rank and the fan-
out of f , and denoted by ?(f) and ?(f).
2.2 Linear Context-Free Rewriting Systems
For the purposes of this paper, a linear context-free
rewriting system, henceforth LCFRS, is a construct
G = (N,T, P, S), where N is an alphabet of nonter-
minal symbols in which each symbol A is associated
with a positive integer ?(A) called its fan-out, T is
an alphabet of terminal symbols, S ? N is a distin-
guished start symbol with ?(S) = 1; and P is a finite
set of productions of the form
p = A? f(A1, . . . , Am) ,
where m ? 0, A,A1, . . . , Am ? N , and f is a linear,
non-erasing function over the terminal alphabet T
with type ?(A1)? ? ? ? ??(Am)? ?(A), called the
composition operation associated with p. The rank
of G and the fan-out of G are defined as the maximal
rank and fan-out of the composition operations of G,
and are denoted by ?(G) and ?(G).
The sets of derivation trees of G are the smallest
indexed family of sets DA, A ? N , such that, if
p = A? f(A1, . . . , Am)
277
N = {S,R} , T = {a, b, c, d} , P = { p1 = S ? f1(R), p2 = R? f2(R), p3 = R? f3 } ,
where: f1(?x1,1, x1,2?) = ?x1,1 x1,2? , f2(?x1,1, x1,2?) = ?a x1,1 b, c x1,2 d? , f3 = ??, ?? .
Figure 1: An LCFRS that generates the string language { anbncndn | n ? 0 }.
is a production of G and ti ? DAi for all i ? [m],
then t = p(t1, . . . , tm) ? DA. By interpreting pro-
ductions as their associated composition operations
in the obvious way, a derivation tree t ? DA evalu-
ates to a ?(A)-tuple of strings over T ; we denote this
tuple by val(t). The string language generated by G,
denoted by L(G), is then defined as
L(G) = {w ? T ? | t ? DS , ?w? = val(t) } .
Two LCFRS are called weakly equivalent, if they
generate the same string language.
Example Figure 1 shows a sample LCFRS G with
?(G) = 1 and ?(G) = 2. The sets of its deriva-
tion trees are DR = { pn2 (p3) | n ? 0 } and
DS = { p1(t) | t ? DR }. The string language
generated by G is { anbncndn | n ? 0 }.
2.3 Characteristic strings
In the remainder of this paper, we use the following
convenient syntax for tuples of strings. Instead of
?v1, . . . , vk? , we write v1 $ ? ? ? $ vk ,
using the $-symbol to mark the component bound-
aries. We call this the characteristic string of the tu-
ple, and an occurrence of the symbol $ a gap marker.
We also use this notation for composition operations.
For example, the characteristic string of the operation
f(?x1,1, x1,2?, ?x2,1?) = ?a x1,1 x2,1, x1,2 b?
is a x1,1 x2,1 $ x1,2 b. If we assume the variables on
the left-hand side of an equation to be named ac-
cording to the schema used in Section 2.1, then the
characteristic string of a composition operation deter-
mines that operation completely. We will therefore
freely identify the two, and write productions as
p = A? [v1 $ ? ? ? $ vk](A1, . . . , Am) ,
where the string inside the brackets is the charac-
teristic string of some composition operation. The
substrings v1, . . . , vk are called the components of
the characteristic string. Note that the character-
istic string of a composition operation with type
k1 ? ? ? ? ? km ? k is a sequence of terminal
symbols, gap markers, and variables from the set
{xi,j | i ? [m], j ? [ki] } in which the number of
gap markers is k?1, and each variable occurs exactly
once. When in the context of such a composition op-
eration we refer to ?a variable of the form xi,j?, then
it will always be the case that i ? [m] and j ? [ki].
The identification of composition operations and
their characteristic strings allows us to construct new
operations by string manipulations: if, for example,
we delete some variables from a characteristic string,
then the resulting string still defines a composition
operation (after a suitable renaming of the remaining
variables, which we leave implicit).
2.4 Canonical LCFRS
To simplify our presentation, we will assume that
LCFRS are given in a certain canonical form. Intu-
itively, this canonical form requires the variables in
the characteristic string of a composition operation
to be ordered in a certain way.
Formally, the defining equation of a composition
operation f with type k1 ? ? ? ? ? km ? k is called
canonical, if (i) the sequence obtained from f by
reading variables of the form xi,1 from left to right
has the form x1,1 ? ? ?xm,1; and (ii) for each i ? [m],
the sequence obtained from f by reading variables
of the form xi,j from left to right has the form
xi,1 ? ? ?xi,ki . An LCFRS is called canonical, if each
of its composition operations is canonical.
We omit the proof that every LCFRS can be trans-
formed into a weakly equivalent canonical LCFRS.
However, we point out that both the normal form and
the parsing algorithm that we present in this paper
can be applied also to general LCFRS. This is in con-
trast to some left-to-right parsers in the literature on
LCFRS and equivalent formalisms (de la Clergerie,
2002; Kallmeyer and Maier, 2009), which actually
depend on productions in canonical form.
2.5 Well-nested LCFRS
We now characterize the class of well-nested LCFRS
that are the focus of this paper. Well-nestedness
was first studied in the context of dependency gram-
mars (Kuhlmann and M?hl, 2007). Kanazawa (2009)
278
defines well-nested multiple context-free grammars,
which are weakly equivalent to well-nested LCFRS.
A composition operation is called well-nested, if it
does not contain a substring of the form
xi,i1 ? ? ?xj,j1 ? ? ?xi,i2 ? ? ?xj,j2 , where i 6= j .
For example, the operation x1,1 x2,1$x2,2 x1,2 is well-
nested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS
is called well-nested, if it contains only well-nested
composition operations.
The class of languages generated by well-nested
LCFRS is properly included in the class of languages
generated by general LCFRS; see Kanazawa and Sal-
vati (2010) for further discussion.
3 Parsing LCFRS
We now discuss the parsing complexity of LCFRS,
and motivate our interest in a normal form for well-
nested LCFRS.
3.1 General parsing schema
A bottom-up, chart-based parsing algorithm for the
class of (not necessarily well-nested) LCFRS can be
defined by using the formalism of parsing schemata
(Sikkel, 1997). The parsing schemata approach con-
siders parsing as a deduction process (as in Shieber
et al (1995)), generating intermediate results called
items. Starting with an initial set of items obtained
from each input sentence, a parsing schema defines
a set of deduction steps that can be used to infer
new items from existing ones. Each item contains
information about the sentence?s structure, and a suc-
cessful parsing process will produce at least one final
item containing a full parse for the input.
The item set used by our bottom-up algorithm to
parse an input string w = a1 ? ? ? an with an LCFRS
G = (N,T, P, S) will be
I = {[A, (l1, r1), . . . , (lk, rk)] | A ? N ?
0 ? li ? ri ? n ?i ? [k]},
where an item [A, (l1, r1), . . . , (lk, rk)] can be inter-
preted as the set of those derivation trees t ? DA
of G for which
val(t) = al1+1 ? ? ? ar1 $ ? ? ? $ alk+1 ? ? ? ark .
The set of final items is thus F = {[S, (0, n)]}, con-
taining full derivation trees that evaluate to w.
For simplicity of definition of the sets of initial
items and deduction steps, let us assume that pro-
ductions of rank > 0 in our grammar do not contain
terminal symbols in their right-hand sides. This can
be easily achieved from a starting grammar by cre-
ating a nonterminal Aa for each terminal a ? T , a
corresponding rank-0 production pa = Aa ? [a](),
and then changing each occurrence of a in the char-
acteristic string of a production to the single variable
associated with the fan-out 1 nonterminal Aa. With
this, our initial item set for a string a1 ? ? ? an will be
H = {[Aai , (i? 1, i)] | i ? [n]} ,
and each production p = A0 ? f(A1, . . . , Am) of
G (excluding the ones we created for the terminals)
will produce a deduction step of the form given in
Figure 2a, where the indexes are subject to the fol-
lowing constraints, imposed by the semantics of f .
1. If the kth component of the characteristic string
of f starts with xi,j , then l0,k = li,j .
2. If the kth component of the characteristic string
of f ends with xi,j , then r0,k = ri,j .
3. If xi,jxi?,j? is an infix of the characteristic string
of f , then ri,j = li?,j? .
4. If the kth component of the characteristic string
of f is the empty string, then l0,k = r0,k.
3.2 General complexity
The time complexity of parsing LCFRS with respect
to the length of the input can be analyzed by counting
the maximum number of indexes that can appear in
an instance of the inference rule above. Although the
total number of indexes is
?m
i=0 2 ? ?(Ai), some of
these indexes are equated by the constraints.
To count the number of independent indexes, con-
sider all the indexes of the form l0,i (corresponding to
the left endpoints of each component of the character-
istic string of f ) and those of the form rj,k for j > 0
(corresponding to the right endpoints of each vari-
able in the characteristic string). By the constraints
above, these indexes are mutually independent, and it
is easy to check that any other index is equated to one
of these: indexes r0,i are equated to the index rj,k
corresponding to the last variable xj,k of the ith com-
ponent of the characteristic string, or to l0,i if there
is no such variable; while indexes lj,k with j > 0
are equated to an index l0,i if the variable xj,k is at
the beginning of a component of the characteristic
string, or to an index rj?,k?(j? > 1) if the variable xj,k
follows another variable xj?,k? .
279
[A1, (l1,1, r1,1), . . . , (l1,?(A1), r1,?(A1))] ? ? ? [Am, (lm,1, rm,1), . . . , (lm,?(Am), rm,?(Am))]
[A0, (l0,1, r0,1), . . . , (l0,?(A0), r0,?(A0))]
(a) The general rule for a parsing schema for LCFRS
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (lm, r
?
1), . . . (l
?
n, r
?
n)]
rm = l?1
(b) Deduction step for concatenation
[B, (l1, r1), . . . , (lm, rm)] [C, (l
?
1, r
?
1), . . . (l
?
n, r
?
n)]
[A, (l1, r1), . . . , (li, r
?
1), . . . (l
?
n, ri+1), . . . , (lm, rm)]
ri = l?1, r
?
n = li+1
(c) Deduction step for wrapping
Figure 2: Deduction steps for parsing LCFRS.
Thus, the parsing complexity (Gildea, 2010) of a
production p = A0 ? f(A1, . . . , Am) is determined
by ?(A0) l-indexes and
?
i?[m] ?(Ai) r-indexes, for
a total complexity of
O(|w|?(A0)+
?
i?[m] ?(Ai))
where |w| is the length of the input string. The pars-
ing complexity of an LCFRS will correspond to the
maximum parsing complexity among its productions.
Note that this general complexity matches the result
given by Seki et al (1991).
In an LCFRS of rank ? and fan-out ?, the maxi-
mum possible parsing complexity is O(|w|?(?+1)),
obtained by applying the above expression to a pro-
duction of rank ? and where each nonterminal has fan-
out ?. The asymptotic time complexity of LCFRS
parsing is therefore exponential both in its rank and
its fan-out. This means that it is interesting to trans-
form LCFRS into equivalent forms that reduce their
rank while preserving the fan-out. For sets of LCFRS
that can be transformed into a binary form (i.e., such
that all its rules have rank at most 2), the ? factor in
the complexity is reduced to a constant, and complex-
ity is improved to O(|w|3?) (see G?mez-Rodr?guez
et al (2009) for further discussion). Unfortunately,
it is known by previous results (Rambow and Satta,
1999) that it is not always possible to convert an
LCFRS into such a binary form without increasing
the fan-out. However, we will show that it is always
possible to build such a binarization for well-nested
LCFRS. Combining this result with the inference
rule and complexity analysis given above, we would
obtain a parser for well-nested LCFRS running in
O(|w|3?) time. But the construction of our binary
normal form additionally restricts binary composition
operations in the binarized LCFRS to be of two spe-
cific forms, concatenation and wrapping, which fur-
ther improves the parsing complexity to O(|w|2?+2),
as we will see below.
3.3 Concatenation and wrapping
A composition operation is called a concatenation
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,m x2,1 $ ? ? ? $ x2,n ,
where m,n ? 1. Intuitively, such an operation corre-
sponds to the bottom-up combination of two adjacent
discontinuous constituents into one. An example of
a concatenation operation is the binary parsing rule
used by the standard CKY parser for context-free
grammars, which combines continuous constituents
(represented as 1-tuples of strings in the LCFRS nota-
tion). In the general case, a concatenation operation
will take an m-tuple and an n-tuple and return an
(m + n ? 1)-tuple, as the joined constituents may
have gaps that will also appear in the resulting tuple.
If we apply the general parsing rule given in Fig-
ure 2a to a production A? conc(B,C), where conc
is a concatenation operation, then we obtain the de-
duction step given in Figure 2b. This step uses 2m
different l- and r-indexes, and 2n? 1 different l?-
and r?-indexes (excluding l?1 which must equal rm),
for a total of 2m+2n?1 = 2(m+n?1)+1 indexes.
Since m+ n? 1 is the fan-out of the nonterminal A,
we conclude that the maximum number of indexes in
the step associated with a concatenation operation in
an LCFRS of fan-out ? is 2?+ 1.
280
before: p
? ? ?
t1 tm
after: p?
q
? ? ?
tq,1 tq,mq
r
? ? ?
tr,1 tr,mr
Figure 3: Transformation of derivation trees
A linear, non-erasing function is called a wrapping
operation, if its characteristic string has the form
x1,1 $ ? ? ? $ x1,i x2,1 $ ? ? ? $ x2,n x1,i+1 $ ? ? ? $ x1,m ,
where m,n ? 1 and i ? [m? 1]. Intuitively, such an
operation wraps the tuple derived from a nontermi-
nal B around the tuple derived from a nonterminal C,
filling the ith gap in the former. An example of a
wrapping operation is the adjunction of an auxiliary
tree in tree-adjoining grammar. In the general case, a
wrapping operation will take an m-tuple and an n-tu-
ple and return an (m + n ? 2)-tuple of strings: the
gaps of the argument tuples appear in the obtained
tuple, except for one gap in the tuple derived from B
which is filled by the tuple derived from C.
By applying the general parsing rule in Figure 2a
to a production A ? wrapi(B,C), where wrapi is
a wrapping operation, then we obtain the deduction
step given in Figure 2c. This step uses 2m different l-
and r-indexes, and 2n? 2 different l?- and r?-indexes
(discounting l?1 and r
?
n which are equal to other in-
dexes), for a total of 2m+2n?2 = 2(m+n?2)+2
indexes. Since the fan-out of A is m + n ? 2, this
means that a wrapping operation needs at most 2?+2
indexes for an LCFRS of fan-out ?.
From this, we conclude that an LCFRS of fan-
out ? in which all composition operations are ei-
ther concatenation operations, wrapping operations,
or operations of rank 0 or 1, can be parsed in time
O(|w|2?+2). In particular, nullary and unary compo-
sition operations do not affect this worst-case com-
plexity, since their associated deduction steps can
never have more than 2? indexes.
4 Transformation
We now show how to transform a well-nested LCFRS
into the normal form that we have just described.
4.1 Informal overview
Consider a production p = A ? f(A1, . . . , Am),
where m ? 2 and f is neither a concatenation nor a
wrapping operation. We will construct new produc-
tions p?, q, r such that every derivation that uses p can
be rewritten into a derivation that uses the new pro-
ductions, and the new productions do not license any
other derivations. Formally, this can be understood as
implementing a tree transformation, where the input
trees are derivations of the original grammar, and the
output trees are derivations of the new grammar. The
situation is illustrated in Figure 3. The tree on top
represents a derivation in the original grammar; this
derivation starts with the rewriting of the nontermi-
nal A using the production p, and continues with the
subderivations t1, . . . , tm. The tree at the bottom rep-
resents a derivation in the transformed grammar. This
derivation starts with the rewriting ofA using the new
production p?, and continues with two independent
subderivations that start with the new productions q
and r, respectively. The sub-derivations t1, . . . , tm
have been partitioned into two sequences
t1,1, . . . , t1,m1 and t2,1, . . . , t2,m2 .
The new production p? will be either a concatenation
or a wrapping operation, and the rank of both q and r
will be strictly smaller than the rank of p. The trans-
formation will continue with q and r, unless these
have rank one. By applying this strategy exhaustively,
we will thus eventually end up with a grammar that
only has productions with rank at most 2, and in
which all productions with rank 2 are either concate-
nation or wrapping operations.
4.2 Constructing the composition operations
To transform the production p, we first factorize the
composition operation f associated with p into three
new composition operations f ?, g, h as follows. Re-
call that we represent composition operations by their
characteristic strings.
In the following, we will assume that no charac-
teristic string starts or ends with a gap marker, or
contains immediate repetitions of gap markers. This
281
property can be ensured, without affecting the asymp-
totic complexity, by adding intermediate steps to the
transformation that we report here; we omit the de-
tails due to space reasons. When this property holds,
we are left with the following two cases. Let us call a
sequence of variables joint, if it contains all and only
variables associated with a given nonterminal.
Case 1 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk f? ,
where k ? 1, x1, . . . , xk are joint variables, and the
suffix f? contains at least one variable. Let
g = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
let h = f?, and let f ? = conc. As f is well-nested,
both g and h define well-nested composition opera-
tions. By the specific segmentation of f , the ranks of
these operations are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 1 .
Case 2 f = x1 f1 x2 ? ? ?xk?1 fk?1 xk ,
where k ? 2, x1, . . . , xk are joint variables, and there
exist at least one i such that the sequence fi contains
at least one variable. Choose an index j as follows:
if there is at least one i such that fi contains at least
one variable and one gap marker, let j be the minimal
such i; otherwise, let j be the minimal i such that fi
contains at least one variable. Now, let
g = x1 f1 x2 ? ? ?xj $ xj+1 ? ? ?xk?1 fk?1 xk ,
let h = fj , and let f ? = wrapj . As in Case 1, both g
and h define well-nested composition operations
whose ranks are strictly smaller than the rank of f .
Furthermore, we have ?(f) = ?(g) + ?(h)? 2 .
Note that at most one of the two cases can apply
to f . Furthermore, since f is well-nested, it is also
true that at least one of the two cases applies. This
is so because for two distinct nonterminals Ai, Ai? ,
either all variables associated with Ai? precede the
leftmost variable associated with Ai, succeed the
rightmost variable associated with Ai, or are placed
between two variables associated with Ai without an-
other variable associated with Ai intervening. (Here,
we have left out the symmetric cases.)
4.3 Constructing the new productions
Based on the composition operations, we now con-
struct three new productions p?, q, r as follows. LetB
and C be two fresh nonterminals with ?(B) = ?(g)
and ?(C) = ?(h), and let p? = A ? f ?(B,C).
The production p? rewrites A into B and C and
combines the two subderivations that originate at
these nonterminals using either a concatenation or a
wrapping operation. Now, let Aq,1, . . . , Aq,mq and
Ar,1, . . . , Ar,mr be the sequences of nonterminals
that are obtained from the sequence A1, . . . , Am by
deleting those nonterminals that are not associated
with any variable in g or h, respectively. Then, let
q = B ? g(Aq,1, . . . , Aq,mq) and
r = C ? h(?Ar,1, . . . , Ar,mr) .
4.4 Example
We now illustrate the transformation using the con-
crete production p = A? f(A1, A2, A3), where
f = x1,1 x2,1 $ x1,2 $ x3,1 .
Note that this operation has rank 3 and fan-out 3.
The composition operations are constructed as fol-
lows. The operation f matches the pattern of Case 1,
and hence induces the operations
g1 = x1,1 x2,1 $ x1,2 , h1 = $ x3,1 , f ?1 = conc .
The productions constructed from these are
p?1 = A? conc(B1, C1) ,
q1 = B1 ? g1(A1, A2) , r1 = C1 ? h1(A3) .
where B1 and C1 are fresh nonterminals with fan-
out 2. The production r1 has rank one, so it does not
require any further transformations. The transforma-
tion thus continues with q1. The operation g1 matches
the pattern of Case 2, and induces the operations
g2 = x1,1 $ x1,2 , h2 = x2,1$ , f ?2 = wrap1 .
The productions constructed from these are
p?2 = B1 ? wrap1(B2, C2) ,
q2 = B2 ? g2(A1) , r2 = C2 ? h2(A2) ,
where B2 and C2 are fresh nonterminals with fan-
out 2. At this point, the transformation terminates.
We can now delete p from the original grammar, and
replace it with the productions {p?1, r1, p
?
2, q2, r2}.
4.5 Correctness
To see that the transformation is correct, we need to
verify that each production of the original grammar
is transformed into a set of equivalent normal-form
productions, and that the fan-out of the new grammar
does not exceed the fan-out of the old grammar.
For the first point, we note that the transformation
preserves well-nestedness, decreases the rank of a
production, and is always applicable as long as the
282
rank of a production is at most 2 and the production
does not use a concatenation or wrapping operation.
That the new productions are equivalent to the old
ones in the sense of Figure 3 can be proved by induc-
tion on the length of a derivation in the original and
the new grammar, respectively.
Let us now convince ourselves that the fan-out of
the new grammar does not exceed the fan-out of the
old grammar. This is clear in Case 1, where
?(f) = ?(g) + ?(h)? 1
implies that both ?(g) ? ?(f) and ?(h) ? ?(f).
For Case 2, we reason as follows. The fan-out of the
operation h, being constructed from an infix of the
characteristic string of the original operation f , is
clearly bounded by the fan-out of f . For g, we have
?(g) = ?(f)? ?(h) + 2 ,
Now suppose that the index j was chosen according
to the first alternative. In this case, ?(h) ? 2, and
?(g) ? ?(f)? 2 + 2 = ?(f) .
For the case where j was chosen according to the
second alternative, ?(f) < k (since there are no
immediate repetitions of gap markers), ?(h) = 1,
and ?(g) ? k. If we assume that each nonterminal
is productive, then this means that the underlying
LCFRS has at least one production with fan-out k or
more; therefore, the fan-out of g does not increase
the fan-out of the original grammar.
4.6 Complexity
To conclude, we now briefly discuss the space com-
plexity of the normal-form transformation. We mea-
sure it in terms of the length of a production, defined
as the length of its string representation, that is, the
string A? [v1 $ ? ? ? $ vk](A1, . . . , Am) .
Looking at Figure 3, we note that the normal-form
transformation of a production p can be understood
as the construction of a (not necessarily complete)
binary-branching tree whose leaves correspond to the
productions obtained by splitting the characteristic
string of p and whose non-leaf nodes are labeled with
concatenation and wrapping operations. By construc-
tion, the sum of the lengths of leaf-node productions
is O(|p|). Since the number of inner nodes of a bi-
nary tree with n leaves is bounded by n ? 1, we
know that the tree hasO(?(p)) inner nodes. As these
nodes correspond to concatenation and wrapping
operations, each inner-node production has length
O(?(p)). Thus, the sum of the lengths of the produc-
tions created from |p| is O(|p|+ ?(p)?(p)). Since
the rank of a production is always smaller than its
length, this is reduced to O(|p|?(p)).
Therefore, the size of the normal-form transfor-
mation of an LCFRS G of fan-out ? is O(?|G|) in
the worst case, and linear space in practice, since
the fan-out is typically bounded by a small integer.
Taking the normal-form transformation into account,
our parser therefore runs in timeO(? ? |G| ? |w|2?+2)
where |G| is the original grammar size.
5 Conclusion
In this paper, we have presented an efficient parsing
algorithm for well-nested linear context-free rewrit-
ing systems, based on a new normal form for this
formalism. The normal form takes up linear space
with respect to grammar size, and the algorithm is
based on a bottom-up process that can be applied
to any LCFRS, achieving O(? ? |G| ? |w|2?+2) time
complexity when applied to LCFRS of fan-out ?
in our normal form. This complexity is an asymp-
totic improvement over existing results for this class,
both from parsers specifically geared to well-nested
LCFRS or equivalent formalisms (Hotz and Pitsch,
1996) and from applying general LCFRS parsing
techniques to the well-nested case (Seki et al, 1991).
The class of well-nested LCFRS is an interest-
ing syntactic formalism for languages with discon-
tinuous constituents, providing a good balance be-
tween coverage of linguistic phenomena in natu-
ral language treebanks (Kuhlmann and Nivre, 2006;
Maier and Lichte, 2009) and desirable formal prop-
erties (Kanazawa, 2009). Our results offer a further
argument in support of well-nested LCFRS: while
the complexity of parsing general LCFRS depends
on two dimensions (rank and fan-out), this bidimen-
sional hierarchy collapses into a single dimension
in the well-nested case, where complexity is only
conditioned by the fan-out.
Acknowledgments G?mez-Rodr?guez has been
supported by MEC/FEDER (HUM2007-66607-C04)
and Xunta de Galicia (PGIDIT07SIN005206PR, Re-
des Galegas de PL e RI e de Ling. de Corpus, Bolsas
Estad?as INCITE/FSE cofinanced). Kuhlmann has
been supported by the Swedish Research Council.
283
References
?ric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata. In
19th International Conference on Computational Lin-
guistics (COLING), pages 1?7, Taipei, Taiwan.
Daniel Gildea. 2010. Optimal parsing strategies for linear
context-free rewriting systems. In Human Language
Technologies: The Eleventh Annual Conference of the
North American Chapter of the Association for Compu-
tational Linguistics, Los Angeles, USA.
Carlos G?mez-Rodr?guez, Marco Kuhlmann, Giorgio
Satta, and David J. Weir. 2009. Optimal reduction
of rule length in linear context-free rewriting systems.
In Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 539?547,
Boulder, CO, USA.
Jir?? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
608?615.
G?nter Hotz and Gisela Pitsch. 1996. On parsing coupled-
context-free languages. Theoretical Computer Science,
161(1?2):205?233.
Riny Huybregts. 1984. The weak inadequacy of context-
free phrase structure grammars. In Ger de Haan, Mieke
Trommelen, and Wim Zonneveld, editors, Van periferie
naar kern, pages 81?99. Foris, Dordrecht, The Nether-
lands.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree Adjunct Grammars. Journal of Computer
and System Sciences, 10(2):136?163.
Aravind K. Joshi. 1985. Tree Adjoining Grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In Natural Language
Parsing, pages 206?250. Cambridge University Press.
Laura Kallmeyer and Wolfgang Maier. 2009. An incre-
mental Earley parser for simple range concatenation
grammar. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT 2009), pages
61?64. Association for Computational Linguistics.
Makoto Kanazawa and Sylvain Salvati. 2010. The copy-
ing power of well-nested multiple context-free gram-
mars. In Fourth International Conference on Language
and Automata Theory and Applications, Trier, Ger-
many.
Makoto Kanazawa. 2009. The pumping lemma for well-
nested multiple context-free languages. In Develop-
ments in Language Theory. 13th International Confer-
ence, DLT 2009, Stuttgart, Germany, June 30?July 3,
2009. Proceedings, volume 5583 of Lecture Notes in
Computer Science, pages 312?325.
Marco Kuhlmann and Mathias M?hl. 2007. Mildly
context-sensitive dependency languages. In 45th An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In 21st International
Conference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational Lin-
guistics (COLING-ACL), Main Conference Poster Ses-
sions, pages 507?514, Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Twelfth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 478?486, Athens, Greece.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In 14th Confer-
ence on Formal Grammar, Bordeaux, France.
Wolfgang Maier and Anders S?gaard. 2008. Treebanks
and mild context-sensitivity. In 13th Conference on
Formal Grammar, pages 61?76, Hamburg, Germany.
Owen Rambow and Giorgio Satta. 1999. Independent
parallelism in finite copying parallel rewriting systems.
Theoretical Computer Science, 223(1?2):87?120.
Giorgio Satta. 1992. Recognition of Linear Context-
Free Rewriting Systems. In 30th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 89?95, Newark, DE, USA.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On Multiple Context-Free Gram-
mars. Theoretical Computer Science, 88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1?2):3?36.
Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Philoso-
phy, 8(3):333?343.
Klaas Sikkel. 1997. Parsing Schemata: A Framework
for Specification and Analysis of Parsing Algorithms.
Springer.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In 25th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 104?111, Stanford, CA, USA.
284
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1492?1501,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Transition-Based Parser for 2-Planar Dependency Structures
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
carlos.gomez@udc.es
Joakim Nivre
Department of Linguistics and Philology
Uppsala University, Sweden
joakim.nivre@lingfil.uu.se
Abstract
Finding a class of structures that is rich
enough for adequate linguistic represen-
tation yet restricted enough for efficient
computational processing is an important
problem for dependency parsing. In this
paper, we present a transition system for
2-planar dependency trees ? trees that can
be decomposed into at most two planar
graphs ? and show that it can be used
to implement a classifier-based parser that
runs in linear time and outperforms a state-
of-the-art transition-based parser on four
data sets from the CoNLL-X shared task.
In addition, we present an efficient method
for determining whether an arbitrary tree
is 2-planar and show that 99% or more of
the trees in existing treebanks are 2-planar.
1 Introduction
Dependency-based syntactic parsing has become
a widely used technique in natural language pro-
cessing, and many different parsing models have
been proposed in recent years (Yamada and Mat-
sumoto, 2003; Nivre et al, 2004; McDonald et al,
2005a; Titov and Henderson, 2007; Martins et al,
2009). One of the unresolved issues in this area
is the proper treatment of non-projective depen-
dency trees, which seem to be required for an ad-
equate representation of predicate-argument struc-
ture, but which undermine the efficiency of depen-
dency parsing (Neuhaus and Bro?ker, 1997; Buch-
Kromann, 2006; McDonald and Satta, 2007).
Caught between the Scylla of linguistically in-
adequate projective trees and the Charybdis of
computationally intractable non-projective trees,
some researchers have sought a middle ground by
exploring classes of mildly non-projective depen-
dency structures that strike a better balance be-
tween expressivity and complexity (Nivre, 2006;
Kuhlmann and Nivre, 2006; Kuhlmann and Mo?hl,
2007; Havelka, 2007). Although these proposals
seem to have a very good fit with linguistic data,
in the sense that they often cover 99% or more of
the structures found in existing treebanks, the de-
velopment of efficient parsing algorithms for these
classes has met with more limited success. For
example, while both Kuhlmann and Satta (2009)
and Go?mez-Rodr??guez et al (2009) have shown
how well-nested dependency trees with bounded
gap degree can be parsed in polynomial time, the
best time complexity for lexicalized parsing of this
class remains a prohibitive O(n7), which makes
the practical usefulness questionable.
In this paper, we explore another characteri-
zation of mildly non-projective dependency trees
based on the notion of multiplanarity. This was
originally proposed by Yli-Jyra? (2003) but has so
far played a marginal role in the dependency pars-
ing literature, because no algorithm was known
for determining whether an arbitrary tree was m-
planar, and no parsing algorithm existed for any
constant value of m. The contribution of this pa-
per is twofold. First, we present a procedure for
determining the minimal number m such that a
dependency tree is m-planar and use it to show
that the overwhelming majority of sentences in de-
pendency treebanks have a tree that is at most 2-
planar. Secondly, we present a transition-based
parsing algorithm for 2-planar dependency trees,
developed in two steps. We begin by showing how
the stack-based algorithm of Nivre (2003) can be
generalized from projective to planar structures.
We then extend the system by adding a second
stack and show that the resulting system captures
exactly the set of 2-planar structures. Although the
contributions of this paper are mainly theoretical,
we also present an empirical evaluation of the 2-
planar parser, showing that it outperforms the pro-
jective parser on four data sets from the CoNLL-X
shared task (Buchholz and Marsi, 2006).
1492
2 Preliminaries
2.1 Dependency Graphs
Let w = w1 . . . wn be an input string.1 An inter-
val (with endpoints i and j) of the string w is a set
of the form [i, j] = {wk | i ? k ? j}.
Definition 1. A dependency graph for w is a di-
rected graph G = (Vw, E), where Vw = [1, n] and
E ? Vw ? Vw.
We call an edge (wi, wj) in a dependency graph G
a dependency link2 from wi to wj . We say that wi
is the parent (or head) of wj and, conversely, that
wj is a syntactic child (or dependent) of wi. For
convenience, we write wi ? wj ? E if the link
(wi, wj) exists; wi ? wj ? E if there is a link
from wi to wj or from wj to wi; wi ?? wj ? E if
there is a (possibly empty) directed path from wi
to wj ; and wi ?? wj ? E if there is a (possibly
empty) path between wi and wj in the undirected
graph underlying G (omitting reference to E when
clear from the context). The projection of a node
wi, denoted bwic, is the set of reflexive-transitive
dependents of wi: bwic = {wj ? V | wi ?? wj}.
Most dependency representations do not allow
arbitrary dependency graphs but typically require
graphs to be acyclic and have at most one head per
node. Such a graph is called a dependency forest.
Definition 2. A dependency graph G for a string
w1 . . . wn is said to be a forest iff it satisfies:
1. Acyclicity: If wi ?? wj , then not wj ? wi.
2. Single-head: If wj ? wi, then not wk ? wi
(for every k 6= j).
Nodes in a forest that do not have a head are called
roots. Some frameworks require that dependency
forests have a unique root (i.e., are connected).
Such a forest is called a dependency tree.
2.2 Projectivity
For reasons of computational efficiency, many de-
pendency parsers are restricted to work with pro-
jective dependency structures, that is, forests in
which the projection of each node corresponds to
a contiguous substring of the input:
1For notational convenience, we will assume throughout
the paper that all symbols in an input string are distinct, i.e.,
i 6= j ? wi 6= wj . This can be guaranteed in practice by
annotating each terminal symbol with its position in the input.
2In practice, dependency links are usually labeled, but to
simplify the presentation we will ignore labels throughout
most of the paper. However, all the results and algorithms
presented can be applied to labeled dependency graphs and
will be so applied in the experimental evaluation.
Definition 3. A dependency forest G for a string
w1 . . . wn is projective iff bwic is an interval for
every word wi ? [1, n].
Projective dependency trees correspond to the set
of structures that can be induced from lexicalised
context-free derivations (Kuhlmann, 2007; Gaif-
man, 1965). Like context-free grammars, projec-
tive dependency trees are not sufficient to repre-
sent all the linguistic phenomena observed in natu-
ral languages, but they have the advantage of being
efficiently parsable: their parsing problem can be
solved in cubic time with chart parsing techniques
(Eisner, 1996; Go?mez-Rodr??guez et al, 2008),
while in the case of general non-projective depen-
dency forests, it is only tractable under strong in-
dependence assumptions (McDonald et al, 2005b;
McDonald and Satta, 2007).
2.3 Planarity
The concept of planarity (Sleator and Temperley,
1993) is closely related to projectivity3 and can be
informally defined as the property of a dependency
forest whose links can be drawn above the words
without crossing.4 To define planarity more for-
mally, we first define crossing links as follows:
let (wi, wk) and (wj , wl) be dependency links in
a dependency graph G. Without loss of general-
ity, we assume that min(i, k) ? min(j, l). Then,
the links are said to be crossing if min(i, k) <
min(j, l) < max (i, k) < max (j, l).
Definition 4. A dependency graph is planar iff it
does not contain a pair of crossing links.
2.4 Multiplanarity
The concept of planarity on its own does not seem
to be very relevant as an extension of projectiv-
ity for practical dependency parsing. According
to the results by Kuhlmann and Nivre (2006), most
non-projective structures in dependency treebanks
are also non-planar, so being able to parse planar
structures will only give us a modest improvement
in coverage with respect to a projective parser.
However, our interest in planarity is motivated by
the fact that it can be generalised to multipla-
narity (Yli-Jyra?, 2003):
3For dependency forests that are extended with a unique
artificial root located at position 0, as is commonly done, the
two notions are equivalent.
4Planarity in the context of dependency structures is not to
be confused with the homonymous concept in graph theory,
which does not restrict links to be drawn above the nodes.
1493
Figure 1: A 2-planar dependency structure with
two different ways of distributing its links into two
planes (represented by solid and dotted lines).
Definition 5. A dependency graph G = (V,E)
is m-planar iff there exist planar dependency
graphs G1 = (V,E1), . . . , Gm = (V,Em) (called
planes) such that E = E1 ? ? ? ? ? Em.
Intuitively, we can associate planes with colours
and say that a dependency graph G is m-planar if it
is possible to assign one of m colours to each of its
links in such a way that links with the same colour
do not cross. Note that there may be multiple
ways of dividing an m-planar graph into planes,
as shown in the example of Figure 1.
3 Determining Multiplanarity
Several constraints on non-projective dependency
structures have been proposed recently that seek a
good balance between parsing efficiency and cov-
erage of non-projective phenomena present in nat-
ural language treebanks. For example, Kuhlmann
and Nivre (2006) and Havelka (2007) have shown
that the vast majority of structures present in exist-
ing treebanks are well-nested and have a small gap
degree (Bodirsky et al, 2005), leading to an inter-
est in parsers for these kinds of structures (Go?mez-
Rodr??guez et al, 2009). No similar analysis has
been performed for m-planar structures, although
Yli-Jyra? (2003) provides evidence that all except
two structures in the Danish dependency treebank
are at most 3-planar. However, his analysis is
based on constraints that restrict the possible ways
of assigning planes to dependency links, and he is
not guaranteed to find the minimal number m for
which a given structure is m-planar.
In this section, we provide a procedure for find-
ing the minimal number m such that a dependency
graph is m-planar and use it to show that the vast
majority of sentences in dependency treebanks are
Figure 2: The crossings graph corresponding to
the dependency structure of Figure 1.
at most 2-planar, with a coverage comparable to
that of well-nestedness. The idea is to reduce
the problem of determining whether a dependency
graph G = (V,E) is m-planar, for a given value
of m, to a standard graph colouring problem. Con-
sider first the following undirected graph:
U(G) = (E,C) where
C = {{ei, ej} | ei, ej are crossing links in G}
This graph, which we call the crossings graph of
G, has one node corresponding to each link in the
dependency graph G, with an undirected link be-
tween two nodes if they correspond to crossing
links in G. Figure 2 shows the crossings graph
of the 2-planar structure in Figure 1.
As noted in Section 2.4, a dependency graph G
is m-planar if each of its links can be assigned
one of m colours in such a way that links with the
same colours do not cross. In terms of the cross-
ings graph, this means that G is m-planar if each
of the nodes of U(G) can be assigned one of m
colours such that no two neighbours have the same
colour. This amounts to solving the well-known k-
colouring problem for U(G), where k = m.
For k = 1 the problem is trivial: a graph is 1-
colourable only if it has no edges. For k = 2, the
problem can be solved in time linear in the size of
the graph by simple breadth-first search. Given a
graph U = (V,E), we pick an arbitrary node v
and give it one of two colours. This forces us to
give the other colour to all its neighbours, the first
colour to the neighbours? neighbours, and so on.
This process continues until we have processed all
the nodes in the connected component of v. If this
has resulted in assigning two different colours to
the same node, the graph is not 2-colourable. Oth-
erwise, we have obtained a 2-colouring of the con-
nected component of U that contains v. If there
are still unprocessed nodes, we repeat the process
by arbitrarily selecting one of them, continue with
the rest of the connected components, and in this
way obtain a 2-colouring of the whole graph if it
1494
Language Structures Non-Projective Not Planar Not 2-Planar Not 3-Pl. Not 4-pl. Ill-nested
Arabic 2995 205 ( 6.84%) 158 ( 5.28%) 0 (0.00%) 0 (0.00%) 0 (0.00%) 1 (0.03%)
Czech 87889 20353 (23.16%) 16660 (18.96%) 82 (0.09%) 0 (0.00%) 0 (0.00%) 96 (0.11%)
Danish 5512 853 (15.48%) 827 (15.00%) 1 (0.02%) 1 (0.02%) 0 (0.00%) 6 (0.11%)
Dutch 13349 4865 (36.44%) 4115 (30.83%) 162 (1.21%) 1 (0.01%) 0 (0.00%) 15 (0.11%)
German 39573 10927 (27.61%) 10908 (27.56%) 671 (1.70%) 0 (0.00%) 0 (0.00%) 419 (1.06%)
Portuguese 9071 1718 (18.94%) 1713 (18.88%) 8 (0.09%) 0 (0.00%) 0 (0.00%) 7 (0.08%)
Swedish 6159 293 ( 4.76%) 280 ( 4.55%) 5 (0.08%) 0 (0.00%) 0 (0.00%) 14 (0.23%)
Turkish 5510 657 (11.92%) 657 (11.92%) 10 (0.18%) 0 (0.00%) 0 (0.00%) 20 (0.36%)
Table 1: Proportion of dependency trees classified by projectivity, planarity, m-planarity and ill-
nestedness in treebanks for Arabic (Hajic? et al, 2004), Czech (Hajic? et al, 2006), Danish (Kromann,
2003), Dutch (van der Beek et al, 2002), German (Brants et al, 2002), Portuguese (Afonso et al, 2002),
Swedish (Nilsson et al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).
exists. Since this process can be completed by vis-
iting each node and edge of the graph U once, its
complexity is O(V + E). The crossings graph of
a dependency graph with n nodes can trivially be
built in time O(n2) by checking each pair of de-
pendency links to determine if they cross, and can-
not contain more than n2 edges, which means that
we can check if the dependency graph for a sen-
tence of length n is 2-planar in O(n2) time.
For k > 2, the k-colouring problem is known
to be NP-complete (Karp, 1972). However, we
have found this not to be a problem when measur-
ing multiplanarity in natural language treebanks,
since the effective problem size can be reduced
by noting that each connected component of the
crossings graph can be treated separately, and that
nodes that are not part of a cycle need not be
considered.5 Given that non-projective sentences
in natural language tend to have a small propor-
tion of non-projective links (Nivre and Nilsson,
2005), the connected components of their cross-
ings graphs are very small, and k-colourings for
them can quickly be found by brute-force search.
By applying these techniques to dependency
treebanks of several languages, we obtain the data
shown in Table 1. As we can see, the coverage
provided by the 2-planarity constraint is compa-
rable to that of well-nestedness. In most of the
treebanks, well over 99% of the sentences are 2-
planar, and 3-planarity has almost total coverage.
As we will see below, the class of 2-planar depen-
dency structures not only has good coverage of lin-
guistic phenomena in existing treebanks but is also
efficiently parsable with transition-based parsing
methods, making it a practically interesting sub-
class of non-projective dependency structures.
5If we have a valid colouring for all the cycles in the
graph, the rest of the nodes can be safely coloured by breadth-
first search as in the k = 2 case.
4 Parsing 1-Planar Structures
In this section, we present a deterministic linear-
time parser for planar dependency structures. The
parser is a variant of Nivre?s arc-eager projec-
tive parser (Nivre, 2003), modified so that it can
also handle graphs that are planar but not projec-
tive. As seen in Table 1, this only gives a modest
improvement in coverage compared to projective
parsing, so the main interest of this algorithm lies
in the fact that it can be generalised to deal with
2-planar structures, as shown in the next section.
4.1 Transition Systems
In the transition-based framework of Nivre (2008),
a deterministic dependency parser is defined by a
non-deterministic transition system, specifying a
set of elementary operations that can be executed
during the parsing process, and an oracle that de-
terministically selects a single transition at each
choice point of the parsing process.
Definition 6. A transition system for dependency
parsing is a quadruple S = (C, T, cs, Ct) where
1. C is a set of possible parser configurations,
2. T is a set of transitions, each of which is a
partial function t : C ? C,
3. cs is a function that maps each input sentence
w to an initial configuration cs(w) ? C,
4. Ct ? C is a set of terminal configurations.
Definition 7. An oracle for a transition system
S = (C, T, cs, Ct) is a function o : C ? T .
An input sentence w can be parsed using a tran-
sition system S = (C, T, cs, Ct) and an oracle o
by starting in the initial configuration cs(w), call-
ing the oracle function on the current configuration
c, and updating the configuration by applying the
transition o(c) returned by the oracle. This pro-
cess is repeated until a terminal configuration is
1495
Initial configuration: cs(w1 . . . wn) = ?[], [w1 . . . wn], ??
Terminal configurations: Cf = {??, [], A? ? C}
Transitions: SHIFT ??, wi|B,A? ? ??|wi, B,A?
REDUCE ??|wi, B,A? ? ??, B,A?
LEFT-ARC ??|wi, wj |B,A? ? ??|wi, wj |B,A ? {(wj , wi)}?
only if 6 ?k|(wk, wi) ? A (single-head) and not wi ?? wj ? A (acyclicity).
RIGHT-ARC ??|wi, wj |B,A? ? ??|wi, wj |B,A ? {(wi, wj)}?
only if 6 ?k|(wk, wj) ? A (single-head) and not wi ?? wj ? A (acyclicity).
Figure 3: Transition system for planar dependency parsing.
reached, and the dependency analysis of the sen-
tence is defined by the terminal configuration.
Each sequence of configurations that the parser
can traverse from an initial configuration to a ter-
minal configuration for some input w is called a
transition sequence. If we associate each config-
uration c of a transition system S = (C, T, cs, Ct)
with a dependency graph g(c), we can say that
S is sound for a class of dependency graphs G
if, for every sentence w and transition sequence
(cs(w), c1, . . . , cf ) of S, g(cf ) is in G, and that S
is complete for G if, for every sentence w and de-
pendency graph G ? G for w, there is a transition
sequence (cs(w), c1, . . . , cf ) such that g(cf ) = G.
A transition system that is sound and complete for
G is said to be correct for G.
Note that, apart from a correct transition system,
a practical parser needs a good oracle to achieve
the desired results, since a transition system only
specifies how to reach all the possible dependency
graphs that could be associated to a sentence, but
not how to select the correct one. Oracles for prac-
tical parsers can be obtained by training classifiers
on treebank data (Nivre et al, 2004).
4.2 A Transition System for Planar
Structures
A correct transition system for the class of planar
dependency forests can be obtained as a variant of
the arc-eager projective system by Nivre (2003).
As in that system, the set of configurations of the
planar transition system is the set of all triples
c = ??, B,A? such that ? and B are disjoint lists
of words from Vw (for some input w), and A is a
set of dependency links over Vw. The list B, called
the buffer, is initialised to the input string and is
used to hold the words that are still to be read from
the input. The list ?, called the stack, is initially
empty and holds words that have dependency links
pending to be created. The system is shown in Fig-
ure 3, where we use the notation ?|wi for a stack
with top wi and tail ?, and we invert the notation
for the buffer for clarity (i.e., wi|B is a buffer with
top wi and tail B).
The system reads the input from left to right and
creates links in a left-to-right order by executing
its four transitions:
1. SHIFT: pops the first (leftmost) word in the
buffer, and pushes it to the stack.
2. LEFT-ARC: adds a link from the first word in
the buffer to the top of the stack.
3. RIGHT-ARC: adds a link from the top of the
stack to the first word in the buffer.
4. REDUCE: pops the top word from the stack,
implying that we have finished building links
to or from it.
Note that the planar parser?s transitions are more
fine-grained than those of the arc-eager projective
parser by Nivre (2003), which pops the stack as
part of its LEFT-ARC transition and shifts a word
as part of its RIGHT-ARC transition. Forcing these
actions after creating dependency links rules out
structures whose root is covered by a dependency
link, which are planar but not projective. In order
to support these structures, we therefore simplify
the ARC transitions (LEFT-ARC and RIGHT-ARC)
so that they only create an arc. For the same rea-
son, we remove the constraint in Nivre?s parser by
which words without a head cannot be reduced.
This has the side effect of making the parser able
to output cyclic graphs. Since we are interested
in planar dependency forests, which do not con-
tain cycles, we only apply ARC transitions after
checking that there is no undirected path between
the nodes to be linked. This check can be done
without affecting the linear-time complexity of the
1496
parser by storing the weakly connected component
of each node in g(c).
The fine-grained transitions used by this parser
have also been used by Sagae and Tsujii (2008)
to parse DAGs. However, the latter parser differs
from ours in the constraints, since it does not allow
the reduction of words without a head (disallowing
forests with covered roots) and does not enforce
the acyclicity constraint (which is guaranteed by
post-processing the graphs to break cycles).
4.3 Correctness and Complexity
For reasons of space, we can only give a sketch
of the correctness proof. We wish to prove that
the planar transition system is sound and com-
plete for the set Fp of all planar dependency
forests. To prove soundness, we have to show
that, for every sentence w and transition sequence
(cs(w), c1, . . . , cf ), the graph g(cf ) associated
with cf is in Fp. We take the graph associated
with a configuration c = (?, B,A) to be g(c) =
(Vw, A). With this, we prove the stronger claim
that g(c) ? Fp for every configuration c that be-
longs to some transition sequence starting with
cs(w). This amounts to showing that in every con-
figuration c reachable from cs(w), g(c) meets the
following three conditions that characterise a pla-
nar dependency forest: (1) g(c) does not contain
nodes with more than one head; (2) g(c) is acyclic;
and (3) g(c) contains no crossing links. (1) is triv-
ially guaranteed by the single-head constraint; (2)
follows from (1) and the acyclicity constraint; and
(3) can be established by proving that there is no
transition sequence that will invoke two ARC tran-
sitions on node pairs that would create crossing
links. At the point when a link from wi to wj is
created, we know that all the words strictly located
between wi and wj are not in the stack or in the
buffer, so no links can be created to or from them.
To prove completeness, we show that every
planar dependency forest G = (V,E) ? Fp
for a sentence w can be produced by apply-
ing the oracle function that maps a configuration
??|wi, wj |B,A? to:
1. LEFT-ARC if wj ? wi ? (E \A),
2. RIGHT-ARC if wi ? wj ? (E \A),
3. REDUCE if ?x[x<i][wx ? wj ? (E \A)],
4. SHIFT otherwise.
We show completeness by setting the following in-
variants on transitions traversed by the application
of the oracle:
1. ?a, b[a,b<j][wa?wb?E ? wa?wb?A]
2. [wi?wj?A?
?k[i<k<j][wk?wj?E ? wk?wj?A]]
3. ?k[k<j][wk 6???
?l[l>k][wk?wl?E ? wk?wl?A]]
We can show that each branch of the oracle func-
tion keeps these invariants true. When we reach a
terminal configuration (which always happens af-
ter a finite number of transitions, since every tran-
sition generating a configuration c = ??, B,A?
decreases the value of the variant function |E| +
|?| + 2|B| ? |A|), it can be deduced from the in-
variant that A = E, which proves completeness.
The worst-case complexity of a deterministic
transition-based parser is given by an upper bound
on transition sequence length (Nivre, 2008). For
the planar system, like its projective counterpart,
the length is clearly O(n) (where n is the number
of input words), since there can be no more than
n SHIFT transitions, n REDUCE transitions, and n
ARC transitions in a transition sequence.
5 Parsing 2-Planar Structures
The planar parser introduced in the previous sec-
tion can be extended to parse all 2-planar depen-
dency structures by adding a second stack to the
system and making REDUCE and ARC transitions
apply to only one of the stacks at a time. This
means that the set of links created in the context
of each individual stack will be planar, but pairs
of links created in different stacks are allowed to
cross. In this way, the parser will build a 2-planar
dependency forest by using each of the stacks to
construct one of its two planes.
The 2-planar transition system, shown in Figure
4, has configurations of the form ??0,?1, B,A?,
where we call ?0 the active stack and ?1 the in-
active stack, and the following transitions:
1. SHIFT: pops the first (leftmost) word in the
buffer, and pushes it to both stacks.
2. LEFT-ARC: adds a link from the first word in
the buffer to the top of the active stack.
3. RIGHT-ARC: adds a link from the top of the
active stack to the first word in the buffer.
4. REDUCE: pops the top word from the active
stack, implying that we have added all links
to or from it on the plane tied to that stack.
5. SWITCH: makes the active stack inactive and
vice versa, changing the plane the parser is
working with.
1497
Initial configuration: cs(w1 . . . wn) = ?[], [], [w1 . . . wn], ??
Terminal configurations: Cf = {??0,?1, [], A? ? C}
Transitions: SHIFT ??0,?1, wi|B,A? ? ??0|wi,?1|wi, B,A?
REDUCE ??0|wi,?1, B,A? ? ??0,?1, B,A?
LEFT-ARC ??0|wi,?1, wj |B,A? ? ??0|wi,?1, wj |B,A ? {(wj , wi)}?
only if 6 ?k | (wk, wi) ? A (single-head) and not wi ?? wj ? A (acyclicity).
RIGHT-ARC ??0|wi,?1, wj |B,A? ? ??0|wi,?1, wj |B,A ? {(wi, wj)}?
only if 6 ?k|(wk, wj) ? A (single-head) and not wi ?? wj ? A (acyclicity).
SWITCH ??0,?1, B,A? ? ??1,?0, B,A?
Figure 4: Transition system for 2-planar dependency parsing.
5.1 Correctness and Complexity
As in the planar case, we provide a brief sketch
of the proof that the transition system in Figure 4
is correct for the set F2p of 2-planar dependency
forests. Soundness follows from a reasoning anal-
ogous to the planar case, but applying the proof
of planarity separately to each stack. In this way,
we prove that the sets of dependency links cre-
ated by linking to or from the top of each of the
two stacks are always planar graphs, and thus their
union (which is the dependency graph stored in A)
is 2-planar. This, together with the single-head and
acyclicity constraints, guarantees that the depen-
dency graphs associated with reachable configura-
tions are always 2-planar dependency forests.
For completeness, we assume an extended form
of the transition system where transitions take the
form ??0,?1, B,A, p?, where p is a flag taking
values in {0, 1} which equals 0 for initial config-
urations and gets flipped by each application of a
SWITCH transition. Then we show that every 2-
planar dependency forest G ? F2p, with planes
G0 = (V,E0) and G1 = (V,E1), can be produced
by this system by applying the oracle function that
maps a configuration ??0|wi,?1, wj |B,A, p? to:
1. LEFT-ARC if wj?wi?(Ep \A),
2. RIGHT-ARC if wi?wj ?(Ep \A),
3. REDUCE if ?x[x<i][wx?wj ?(Ep \A)?
??y[x<y?i][wy?wj ?(Ep \A)]],
4. SWITCH if ?x<j : (wx, wj) or (wj , wx) ? (Ep\A),
5. SHIFT otherwise.
This can be shown by employing invariants analo-
gous to the planar case, with the difference that the
third invariant applies to each stack and its corre-
sponding plane: if ?y is associated with the plane
Ex,6 we have:
3. ?k[k<j][wk 6? ?y]?
?l[l>k][wk?wl?Ex]? [wk?wl?A]
Since the presence of the flag p in configurations
does not affect the set of dependency graphs gen-
erated by the system, the completeness of the sys-
tem extended with the flag p implies that of the
system in Figure 4.
We can show that the complexity of the 2-planar
system is O(n) by the same kind of reasoning as
for the 1-planar system, with the added complica-
tion that we must constrain the system to prevent
two adjacent SWITCH transitions. In fact, without
this restriction, the parser is not even guaranteed
to terminate.
5.2 Implementation
In practical settings, oracles for transition-based
parsers can be approximated by classifiers trained
on treebank data (Nivre, 2008). To do this, we
need an oracle that will generate transition se-
quences for gold-standard dependency graphs. In
the case of the planar parser of Section 4.2, the or-
acle of 4.3 is suitable for this purpose. However,
in the case of the 2-planar parser, the oracle used
for the completeness proof in Section 5.1 cannot
be used directly, since it requires the gold-standard
trees to be divided into two planes in order to gen-
erate a transition sequence.
Of course, it is possible to use the algorithm
presented in Section 3 to obtain a division of sen-
tences into planes. However, for training purposes
and to obtain a robust behaviour if non-2-planar
6The plane corresponding to each stack in a configuration
changes with each SWITCH transition: ?x is associated with
Ex in configurations where p = 0, and with Ex in those
where p = 1.
1498
Czech Danish German Portuguese
Parser LAS UAS NPP NPR LAS UAS NPP NPR LAS UAS NPP NPR LAS UAS NPP NPR
2-planar 79.24 85.30 68.9 60.7 83.81 88.50 66.7 20.0 86.50 88.84 57.1 45.8 87.04 90.82 82.8 33.8
Malt P 78.18 84.12 ? ? 83.31 88.30 ? ? 85.36 88.06 ? ? 86.60 90.20 ? ?
Malt PP 79.80 85.70 76.7 56.1 83.67 88.52 41.7 25.0 85.76 88.66 58.1 40.7 87.08 90.66 83.3 46.2
Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P)
pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score;
NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.
sentences are found, it is more convenient that
the oracle can distribute dependency links into the
planes incrementally, and that it produces a dis-
tribution of links that only uses SWITCH transi-
tions when it is strictly needed to account for non-
planarity. Thus we use a more complex version of
the oracle which performs a search in the crossings
graph to check if a dependency link can be built on
the plane of the active stack, and only performs a
switch when this is not possible. This has proved
to work well in practice, as will be observed in the
results in the next section.
6 Empirical Evaluation
In order to get a first estimate of the empirical ac-
curacy that can be obtained with transition-based
2-planar parsing, we have evaluated the parser
on four data sets from the CoNLL-X shared task
(Buchholz and Marsi, 2006): Czech, Danish, Ger-
man and Portuguese. As our baseline, we take
the strictly projective arc-eager transition system
proposed by Nivre (2003), as implemented in the
freely available MaltParser system (Nivre et al,
2006a), with and without the pseudo-projective
parsing technique for recovering non-projective
dependencies (Nivre and Nilsson, 2005). For the
two baseline systems, we use the parameter set-
tings used by Nivre et al (2006b) in the original
shared task, where the pseudo-projective version
of MaltParser was one of the two top performing
systems (Buchholz and Marsi, 2006). For our 2-
planar parser, we use the same kernelized SVM
classifiers as MaltParser, using the LIBSVM pack-
age (Chang and Lin, 2001), with feature models
that are similar to MaltParser but extended with
features defined over the second stack.7
In Table 2, we report labeled (LAS) and un-
labeled (UAS) attachment score on the four lan-
guages for all three systems. For the two systems
that are capable of recovering non-projective de-
7Complete information about experimental settings can
be found at http://stp.lingfil.uu.se/ nivre/exp/.
pendencies, we also report precision (NPP) and
recall (NPR) specifically on non-projective depen-
dency arcs. The results show that the 2-planar
parser outperforms the strictly projective variant
of MaltParser on all metrics for all languages,
and that it performs on a par with the pseudo-
projective variant with respect to both overall at-
tachment score and precision and recall on non-
projective dependencies. These results look very
promising in view of the fact that very little effort
has been spent on optimizing the training oracle
and feature model for the 2-planar parser so far.
It is worth mentioning that the 2-planar parser
has two advantages over the pseudo-projective
parser. The first is simplicity, given that it is based
on a single transition system and makes a single
pass over the input, whereas the pseudo-projective
parsing technique involves preprocessing of train-
ing data and post-processing of parser output
(Nivre and Nilsson, 2005). The second is the fact
that it parses a well-defined class of dependency
structures, with known coverage8, whereas no for-
mal characterization exists of the class of struc-
tures parsable by the pseudo-projective parser.
7 Conclusion
In this paper, we have presented an efficient algo-
rithm for deciding whether a dependency graph is
2-planar and a transition-based parsing algorithm
that is provably correct for 2-planar dependency
forests, neither of which existed in the literature
before. In addition, we have presented empirical
results showing that the class of 2-planar depen-
dency forests includes the overwhelming majority
of structures found in existing treebanks and that
a deterministic classifier-based implementation of
the 2-planar parser gives state-of-the-art accuracy
on four different languages.
8If more coverage is desired, the 2-planar parser can be
generalised to m-planar structures for larger values of m by
adding additional stacks. However, this comes at the cost of
more complex training models, making the practical interest
of increasing m beyond 2 dubious.
1499
Acknowledgments
The first author has been partially supported by
Ministerio de Educacio?n y Ciencia and FEDER
(HUM2007-66607-C04) and Xunta de Galicia
(PGIDIT07SIN005206PR, Rede Galega de Proce-
samento da Linguaxe e Recuperacio?n de Infor-
macio?n, Rede Galega de Lingu???stica de Corpus,
Bolsas Estad??as INCITE/FSE cofinanced).
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC 2002), pages 1968?1703, Paris,
France. ELRA.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank.
In Proceedings of EACL Workshop on Linguisti-
cally Interpreted Corpora (LINC-03), pages 243?
246, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Leonoor van der Beek, Gosse Bouma, Robert Malouf,
and Gertjan van Noord. 2002. The Alpino depen-
dency treebank. In Language and Computers, Com-
putational Linguistics in the Netherlands 2001. Se-
lected Papers from the Twelfth CLIN Meeting, pages
8?22, Amsterdam, the Netherlands. Rodopi.
Manuel Bodirsky, Marco Kuhlmann, and Mathias
Mo?hl. 2005. Well-nested drawings as models of
syntactic structure. In 10th Conference on Formal
Grammar and 9th Meeting on Mathematics of Lan-
guage, Edinburgh, Scotland, UK.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, September 20-21,
Sozopol, Bulgaria.
Matthias Buch-Kromann. 2006. Discontinuous Gram-
mar: A Model of Human Parsing and Language
Acquisition. Ph.D. thesis, Copenhagen Business
School.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the 10th Conference on Computa-
tional Natural Language Learning (CoNLL), pages
149?164.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: A Library for Support Vec-
tor Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Jason Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, San Francisco, CA, USA, August. ACL /
Morgan Kaufmann.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and Control,
8:304?337.
Carlos Go?mez-Rodr??guez, John Carroll, and David
Weir. 2008. A deductive approach to depen-
dency parsing. In Proceedings of the 46th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL?08:HLT), pages 968?976, Morristown, NJ,
USA. Association for Computational Linguistics.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2009. Parsing mildly non-projective depen-
dency structures. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 291?
299.
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan S?naidauf,
and Emanuel Bes?ka. 2004. Prague Arabic de-
pendency treebank: Development in data and tools.
In Proceedings of the NEMLAR International Con-
ference on Arabic Language Resources and Tools,
pages 110?117.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan S?te?pa?nek,
Jir??? Havelka, and Marie Mikulova?. 2006.
Prague Dependency Treebank 2.0. CDROM
CAT: LDC2006T01, ISBN 1-58563-370-4. Linguis-
tic Data Consortium.
Jiri Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 608?615.
Richard M. Karp. 1972. Reducibility among combi-
natorial problems. In R. Miller and J. Thatcher, ed-
itors, Complexity of Computer Computations, pages
85?103. Plenum Press.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proceedings of the 2nd Workshop on Treebanks and
Linguistic Theories (TLT), pages 217?220, Va?xjo?,
Sweden. Va?xjo? University Press.
Marco Kuhlmann and Mathias Mo?hl. 2007. Mildly
context-sensitive dependency languages. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 160?167.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507?514.
1500
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 478?486.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral dissertation, Saar-
land University, Saarbru?cken, Germany.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP), pages 342?350.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the 10th International
Conference on Parsing Technologies (IWPT), pages
122?131.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91?98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT/EMNLP
2005: Proceedings of the conference on Human
Language Technology and Empirical Methods in
Natural Language Processing, pages 523?530, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate de-
pendency grammars. In Proceedings of the 35th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL) and the 8th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 337?343.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of NODAL-
IDA 2005 Special Session on Treebanks, pages 119?
132. Samfundslitteratur, Frederiksberg, Denmark,
May.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 99?106,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL-2004), pages 49?
56, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 2216?2219.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsen
Eryig?it, and Svetoslav Marinov. 2006b. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the 10th Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 221?225.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency graphs. In Proceedings of the 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 73?
80.
Joakim Nivre. 2008. Algorithms for Deterministic In-
cremental Dependency Parsing. Computational Lin-
guistics, 34(4):513?553.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r,
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In A. Abeille (ed.), Building and Exploiting
Syntactically-annotated Corpora, pages 261?277,
Dordrecht, the Netherlands. Kluwer.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In COLING ?08: Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 753?760, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a Link Grammar. In Proceedings of the
Third International Workshop on Parsing Technolo-
gies (IWPT?93), pages 277?292. ACL/SIGPARSE.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the 10th International Conference on
Parsing Technologies (IWPT), pages 144?155.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
Anssi Mikael Yli-Jyra?. 2003. Multiplanarity ? a
model for dependency structures in treebanks. In
Joakim Nivre and Erhard Hinrichs, editors, TLT
2003. Proceedings of the Second Workshop on Tree-
banks and Linguistic Theories, volume 9 of Mathe-
matical Modelling in Physics, Engineering and Cog-
nitive Sciences, pages 189?200, Va?xjo?, Sweden, 14-
15 November. Va?xjo? University Press.
1501
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 673?682,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Dynamic Programming Algorithms
for Transition-Based Dependency Parsers
Marco Kuhlmann
Dept. of Linguistics and Philology
Uppsala University, Sweden
marco.kuhlmann@lingfil.uu.se
Carlos G?mez-Rodr?guez
Departamento de Computaci?n
Universidade da Coru?a, Spain
cgomezr@udc.es
Giorgio Satta
Dept. of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Abstract
We develop a general dynamic programming
technique for the tabulation of transition-based
dependency parsers, and apply it to obtain
novel, polynomial-time algorithms for parsing
with the arc-standard and arc-eager models. We
also show how to reverse our technique to ob-
tain new transition-based dependency parsers
from existing tabular methods. Additionally,
we provide a detailed discussion of the con-
ditions under which the feature models com-
monly used in transition-based parsing can be
integrated into our algorithms.
1 Introduction
Dynamic programming algorithms, also known as
tabular or chart-based algorithms, are at the core of
many applications in natural language processing.
When applied to formalisms such as context-free
grammar, they provide polynomial-time parsing al-
gorithms and polynomial-space representations of
the resulting parse forests, even in cases where the
size of the search space is exponential in the length
of the input string. In combination with appropri-
ate semirings, these packed representations can be
exploited to compute many values of interest for ma-
chine learning, such as best parses and feature expec-
tations (Goodman, 1999; Li and Eisner, 2009).
In this paper, we follow the line of investigation
started by Huang and Sagae (2010) and apply dy-
namic programming to (projective) transition-based
dependency parsing (Nivre, 2008). The basic idea,
originally developed in the context of push-down
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989), is that while the number of computa-
tions of a transition-based parser may be exponential
in the length of the input string, several portions of
these computations, when appropriately represented,
can be shared. This can be effectively implemented
through dynamic programming, resulting in a packed
representation of the set of all computations.
The contributions of this paper can be summarized
as follows. We provide (declarative specifications of)
novel, polynomial-time algorithms for two widely-
used transition-based parsing models: arc-standard
(Nivre, 2004; Huang and Sagae, 2010) and arc-eager
(Nivre, 2003; Zhang and Clark, 2008). Our algorithm
for the arc-eager model is the first tabular algorithm
for this model that runs in polynomial time. Both
algorithms are derived using the same general tech-
nique; in fact, we show that this technique is applica-
ble to all transition-parsing models whose transitions
can be classified into ?shift? and ?reduce? transitions.
We also show how to reverse the tabulation to de-
rive a new transition system from an existing tabular
algorithm for dependency parsing, originally devel-
oped by G?mez-Rodr?guez et al (2008). Finally, we
discuss in detail the role of feature information in
our algorithms, and in particular the conditions under
which the feature models traditionally used in transi-
tion-based dependency parsing can be integrated into
our framework.
While our general approach is the same as the one
of Huang and Sagae (2010), we depart from their
framework by not representing the computations of
a parser as a graph-structured stack in the sense of
Tomita (1986). We instead simulate computations
as in Lang (1974), which results in simpler algo-
rithm specifications, and also reveals deep similari-
ties between transition-based systems for dependency
parsing and existing tabular methods for lexicalized
context-free grammars.
673
2 Transition-Based Dependency Parsing
We start by briefly introducing the framework of
transition-based dependency parsing; for details, we
refer to Nivre (2008).
2.1 Dependency Graphs
Let w D w0   wn 1 be a string over some fixed
alphabet, where n  1 and w0 is the special token
root. A dependency graph for w is a directed graph
G D .Vw ; A/, where Vw D f0; : : : ; n   1g is the set
of nodes, and A  Vw  Vw is the set of arcs. Each
node in Vw encodes the position of a token in w, and
each arc in A encodes a dependency relation between
two tokens. To denote an arc .i; j / 2 A, we write
i ! j ; here, the node i is the head, and the node j is
the dependent. A sample dependency graph is given
in the left part of Figure 2.
2.2 Transition Systems
A transition system is a structure S D .C; T; I; Ct /,
where C is a set of configurations, T is a finite set
of transitions, which are partial functions t WC * C ,
I is a total initialization function mapping each input
string to a unique initial configuration, and Ct  C
is a set of terminal configurations.
The transition systems that we investigate in this
paper differ from each other only with respect to
their sets of transitions, and are identical in all other
aspects. In each of them, a configuration is de-
fined relative to a string w as above, and is a triple
c D .; ?; A/, where  and ? are disjoint lists of
nodes from Vw , called stack and buffer, respectively,
and A  Vw  Vw is a set of arcs. We denote the
stack, buffer and arc set associated with c by .c/,
?.c/, and A.c/, respectively. We follow a standard
convention and write the stack with its topmost ele-
ment to the right, and the buffer with its first element
to the left; furthermore, we indicate concatenation
in the stack and in the buffer by a vertical bar. The
initialization function maps each string w to the ini-
tial configuration .??; ?0; : : : ; jwj   1?;;/. The set of
terminal configurations contains all configurations of
the form .?0?; ??; A/, where A is some set of arcs.
Given an input string w, a parser based on S pro-
cesses w from left to right, starting in the initial con-
figuration I.w/. At each point, it applies one of
the transitions, until at the end it reaches a terminal
.; i j?;A/ ` . ji; ?; A/ .sh/
. ji jj; ?;A/ ` . jj; ?;A [ fj ! ig/ .la/
. ji jj; ?;A/ ` . ji; ?; A [ fi ! j g/ .ra/
Figure 1: Transitions in the arc-standard model.
configuration; the dependency graph defined by the
arc set associated with that configuration is then re-
turned as the analysis for w. Formally, a computation
of S on w is a sequence  D c0; : : : ; cm, m  0, of
configurations (defined relative to w) in which each
configuration is obtained as the value of the preced-
ing one under some transition. It is called complete
whenever c0 D I.w/, and cm 2 Ct . We note that a
computation can be uniquely specified by its initial
configuration c0 and the sequence of its transitions,
understood as a string over T . Complete computa-
tions, where c0 is fixed, can be specified by their
transition sequences alone.
3 Arc-Standard Model
To introduce the core concepts of the paper, we first
look at a particularly simple model for transition-
based dependency parsing, known as the arc-stan-
dard model. This model has been used, in slightly
different variants, by a number of parsers (Nivre,
2004; Attardi, 2006; Huang and Sagae, 2010).
3.1 Transition System
The arc-standard model uses three types of transi-
tions: Shift (sh) removes the first node in the buffer
and pushes it to the stack. Left-Arc (la) creates a
new arc with the topmost node on the stack as the
head and the second-topmost node as the dependent,
and removes the second-topmost node from the stack.
Right-Arc (ra) is symmetric to Left-Arc in that it
creates an arc with the second-topmost node as the
head and the topmost node as the dependent, and
removes the topmost node.
The three transitions can be formally specified as
in Figure 1. The right half of Figure 2 shows a com-
plete computation of the arc-standard transition sys-
tem, specified by its transition sequence. The picture
also shows the contents of the stack over the course of
the computation; more specifically, column i shows
the stack .ci / associated with the configuration ci .
674
root This news had little effect on the markets
0
1
0 0
1
2
0
2
0
2
3
0
3
0
3
0
3
0
3
54 4
5
0
3
5
0
3
5
0
3
5
6 6 6
77
8
0
3
5
6
8
0
3
5
6
0
3
5
0
3
0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
sh sh sh la sh la sh sh la sh sh sh la ra ra ra ra
1 2
0
Figure 2: A dependency tree (left) and a computation generating this tree in the arc-standard system (right).
3.2 Push Computations
The key to the tabulation of transition-based depen-
dency parsers is to find a way to decompose com-
putations into smaller, shareable parts. For the arc-
standard model, as well as for the other transition
systems that we consider in this paper, we base our
decomposition on the concept of push computations.
By this, we mean computations
 D c0; : : : ; cm ; m  1 ;
on some input string w with the following properties:
(P1) The initial stack .c0/ is not modified during
the computation, and is not even exposed after the
first transition: For every 1  i  m, there exists a
non-empty stack i such that .ci / D .c0/ji .
(P2) The overall effect of the computation is to
push a single node to the stack: The stack .cm/ can
be written as .cm/ D .c0/jh, for some h 2 Vw .
We can verify that the computation in Figure 2 is
a push computation. We can also see that it contains
shorter computations that are push computations; one
example is the computation 0 D c1; : : : ; c16, whose
overall effect is to push the node 3. In Figure 2, this
computation is marked by the zig-zag path traced
in bold. The dashed line delineates the stack .c1/,
which is not modified during 0.
Every computation that consists of a single sh tran-
sition is a push computation. Starting from these
atoms, we can build larger push computations by
means of two (partial) binary operations fla and fra,
defined as follows. Let 1 D c10; : : : ; c1m1 and
2 D c20; : : : ; c2m2 be push computations on the
same input string w such that c1m1 D c20. Then
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c ;
where c is obtained from c2m2 by applying the ra
transition. (The operation fla is defined analogously.)
We can verify that fra.1; 2/ is another push com-
putation. For instance, with respect to Figure 2,
fra.1; 2/ D 0. Conversely, we say that the push
computation 0 can be decomposed into the subcom-
putations 1 and 2, and the operation fra.
3.3 Deduction System
Building on the compositional structure of push com-
putations, we now construct a deduction system (in
the sense of Shieber et al (1995)) that tabulates the
computations of the arc-standard model for a given
input string w D w0   wn 1. For 0  i  n, we
shall write ?i to denote the buffer ?i; : : : ; n 1?. Thus,
?0 denotes the full buffer, associated with the initial
configuration I.w/, and ?n denotes the empty buffer,
associated with a terminal configuration c 2 Ct .
Item form. The items of our deduction system
take the form ?i; h; j ?, where 0  i  h < j  n.
The intended interpretation of an item ?i; h; j ? is:
For every configuration c0 with ?.c0/ D ?i , there
exists a push computation  D c0; : : : ; cm such that
?.cm/ D j? , and .cm/ D .c0/jh.
Goal. The only goal item is ?0; 0; n?, asserting
that there exists a complete computation for w.
Axioms. For every stack  , position i < n and
arc set A, by a single sh transition we obtain the
push computation .; ?i ; A/; . ji; ?iC1; A/. There-
fore we can take the set of all items of the form
?i; i; i C 1? as the axioms of our system.
Inference rules. The inference rules parallel the
composition operations fla and fra. Suppose that
we have deduced the items ?i; h1; k? and ?k; h2; j ?,
where 0  i  h1 < k  h2 < j  n. The
item ?i; h1; k? asserts that for every configuration c10
675
Item form: ?i; h; j ? , 0  i  h < j  jwj Goal: ?0; 0; jwj? Axioms: ?i; i; i C 1?
Inference rules:
?i; h1; k? ?k; h2; j ?
?i; h2; j ?
.laI h2 ! h1/
?i; h1; k? ?k; h2; j ?
?i; h1; j ?
.raI h1 ! h2/
Figure 3: Deduction system for the arc-standard model.
with ?.c10/ D ?i , there exists a push computation
1 D c10; : : : ; c1m1 such that ?.c1m1/ D ?k , and
.c1m1/ D .c10/jh1. Using the item ?k; h2; j ?,
we deduce the existence of a second push compu-
tation 2 D c20; : : : ; c2m2 such that c20 D c1m1 ,
?.c2m2/ D j? , and .c2m2/ D .c10/jh1jh2. By
means of fra, we can then compose 1 and 2 into a
new push computation
fra.1; 2/ D c10; : : : ; c1m1 ; c21; : : : ; c2m2 ; c :
Here, ?.c/ D j? , and .c/ D .c10/jh1. Therefore,
we may generate the item ?i; h1; j ?. The inference
rule for la can be derived analogously.
Figure 3 shows the complete deduction system.
3.4 Completeness and Non-Ambiguity
We have informally argued that our deduction sys-
tem is sound. To show completeness, we prove the
following lemma: For all 0  i  h < j  jwj and
every push computation  D c0; : : : ; cm on w with
?.c0/ D ?i , ?.cm/ D j? and .cm/ D .c0/jh, the
item ?i; h; j ? is generated. The proof is by induction
on m, and there are two cases:
m D 1. In this case,  consists of a single sh transi-
tion, h D i , j D i C 1, and we need to show that the
item ?i; i; i C 1? is generated. This holds because this
item is an axiom.
m  2. In this case,  ends with either a la or a ra
transition. Let c be the rightmost configuration in 
that is different from cm and whose stack size is one
larger than the size of .c0/. The computations
1 D c0; : : : ; c and 2 D c; : : : ; cm 1
are both push computations with strictly fewer tran-
sitions than  . Suppose that the last transition in 
is ra. In this case, ?.c/ D ?k for some i < k < j ,
.c/ D .c0/jh with h < k, ?.cm 1/ D j? , and
.cm 1/ D .c0/jhjh0 for some k  h0 < j . By
induction, we may assume that we have generated
items ?i; h; k? and ?k; h0; j ?. Applying the inference
rule for ra, we deduce the item ?i; h; j ?. An analo-
gous argument can be made for fla.
Apart from being sound and complete, our deduc-
tion system also has the property that it assigns at
most one derivation to a given item. To see this,
note that in the proof of the lemma, the choice of c
is uniquely determined: If we take any other con-
figuration c0 that meets the selection criteria, then
the computation  02 D c
0; : : : ; cm 1 is not a push
computation, as it contains c as an intermediate con-
figuration, and thereby violates property P1.
3.5 Discussion
Let us briefly take stock of what we have achieved
so far. We have provided a deduction system capable
of tabulating the set of all computations of an arc-
standard parser on a given input string, and proved
the correctness of this system relative to an interpre-
tation based on push computations. Inspecting the
system, we can see that its generic implementation
takes space in O.jwj3/ and time in O.jwj5/.
Our deduction system is essentially the same as the
one for the CKY algorithm for bilexicalized context-
free grammar (Collins, 1996; G?mez-Rodr?guez et
al., 2008). This equivalence reveals a deep correspon-
dence between the arc-standard model and bilexical-
ized context-free grammar, and, via results by Eisner
and Satta (1999), to head automata. In particular,
Eisner?s and Satta?s ?hook trick? can be applied to
our tabulation to reduce its runtime to O.jwj4/.
4 Adding Features
The main goal with the tabulation of transition-based
dependency parsers is to obtain a representation
based on which semiring values such as the high-
est-scoring computation for a given input (and with
it, a dependency tree) can be calculated. Such com-
putations involve the use of feature information. In
this section, we discuss how our tabulation of the arc-
standard system can be extended for this purpose.
676
?i; h1; kI hx2; x1i; hx1; x3i? W v1 ?k; h2; j I hx1; x3i; hx3; x4i? W v2
?i; h1; j I hx2; x1i; hx1; x3i? W v1 C v2 C hx3; x4i  E?ra
.ra/
?i; h; j I hx2; x1i; hx1; x3i? W v
?j; j; j C 1I hx1; x3i; hx3; wj i? W hx1; x3i  E?sh
.sh/
Figure 4: Extended inference rules under the feature model ? D hs1:w; s0:wi. The annotations indicate how to calculate
a candidate for an update of the Viterbi score of the conclusion using the Viterbi scores of the premises.
4.1 Scoring Computations
For the sake of concreteness, suppose that we want
to score computations based on the following model,
taken from Zhang and Clark (2008). The score of a
computation  is broken down into a sum of scores
score.t; ct / for combinations of a transition t in the
transition sequence associated with  and the config-
uration ct in which t was taken:
score./ D
X
t2
score.t; ct / (1)
The score score.t; ct / is defined as the dot product of
the feature representation of ct relative to a feature
model ? and a transition-specific weight vector E?t :
score.t; ct / D ?.ct /  E?t
The feature model ? is a vector h1; : : : ; ni of
elementary feature functions, and the feature rep-
resentation ?.c/ of a configuration c is a vector
Ex D h1.c/; : : : ; n.c/i of atomic values. Two ex-
amples of feature functions are the word form associ-
ated with the topmost and second-topmost node on
the stack; adopting the notation of Huang and Sagae
(2010), we will write these functions as s0:w and
s1:w, respectively. Feature functions like these have
been used in several parsers (Nivre, 2006; Zhang and
Clark, 2008; Huang et al, 2009).
4.2 Integration of Feature Models
To integrate feature models into our tabulation of
the arc-standard system, we can use extended items
of the form ?i; h; j I ExL; ExR? with the same intended
interpretation as the old items ?i; h; j ?, except that
the initial configuration of the asserted computations
 D c0; : : : ; cm now is required to have the feature
representation ExL, and the final configuration is re-
quired to have the representation ExR:
?.c0/ D ExL and ?.cm/ D ExR
We shall refer to the vectors ExL and ExR as the left-
context vector and the right-context vector of the
computation  , respectively.
We now need to change the deduction rules so that
they become faithful to the extended interpretation.
Intuitively speaking, we must ensure that the feature
values can be computed along the inference rules.
As a concrete example, consider the feature model
? D hs1:w; s0:wi. In order to integrate this model
into our tabulation, we change the rule for ra as in
Figure 4, where x1; : : : ; x4 range over possible word
forms. The shared variable occurrences in this rule
capture the constraints that hold between the feature
values of the subcomputations 1 and 2 asserted
by the premises, and the computations fra.1; 2/
asserted by the conclusion. To illustrate this, suppose
that 1 and 2 are as in Figure 2. Then the three
occurrences of x3 for instance encode that
?s0:w?.c6/ D ?s1:w?.c15/ D ?s0:w?.c16/ D w3 :
We also need to extend the axioms, which cor-
respond to computations consisting of a single sh
transition. The most conservative way to do this is
to use a generate-and-test technique: Extend the ex-
isting axioms by all valid choices of left-context and
right-context vectors, that is, by all pairs ExL; ExR such
that there exists a configuration c with ?.c/ D ExL
and ?.sh.c// D ExR. The task of filtering out use-
less guesses can then be delegated to the deduction
system.
A more efficient way is to only have one axiom, for
the case where c D I.w/, and to add to the deduction
system a new, unary inference rule for sh as in Fig-
ure 4. This rule only creates items whose left-context
vector is the right-context vector of some other item,
which prevents the generation of useless items. In
the following, we take this second approach, which
is also the approach of Huang and Sagae (2010).
677
?i; h; j I hx2; x1i; hx1; x3i? W .p; v/
?j; j; j C 1I hx1; x3i; hx3; wj i? W .p C ; /
.sh/ , where  D hx1; x3i  E?sh
?i; h1; kI hx2; x1i; hx1; x3i? W .p1; v1/ ?k; h2; j I hx1; x3i; hx3; x4i? W .p2; v2/
?i; h1; j I hx2; x1i; hx1; x3i? W .p1 C v2 C 