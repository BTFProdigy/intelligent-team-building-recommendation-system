Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
SMS based Interface for FAQ Retrieval
Govind Kothari
IBM India Research Lab
gokothar@in.ibm.com
Sumit Negi
IBM India Research Lab
sumitneg@in.ibm.com
Tanveer A. Faruquie
IBM India Research Lab
ftanveer@in.ibm.com
Venkatesan T. Chakaravarthy
IBM India Research Lab
vechakra@in.ibm.com
L. Venkata Subramaniam
IBM India Research Lab
lvsubram@in.ibm.com
Abstract
Short Messaging Service (SMS) is popu-
larly used to provide information access to
people on the move. This has resulted in
the growth of SMS based Question An-
swering (QA) services. However auto-
matically handling SMS questions poses
significant challenges due to the inherent
noise in SMS questions. In this work we
present an automatic FAQ-based question
answering system for SMS users. We han-
dle the noise in a SMS query by formu-
lating the query similarity over FAQ ques-
tions as a combinatorial search problem.
The search space consists of combinations
of all possible dictionary variations of to-
kens in the noisy query. We present an ef-
ficient search algorithm that does not re-
quire any training data or SMS normaliza-
tion and can handle semantic variations in
question formulation. We demonstrate the
effectiveness of our approach on two real-
life datasets.
1 Introduction
The number of mobile users is growing at an
amazing rate. In India alone a few million sub-
scribers are added each month with the total sub-
scriber base now crossing 370 million. The any-
time anywhere access provided by mobile net-
works and portability of handsets coupled with the
strong human urge to quickly find answers has fu-
eled the growth of information based services on
mobile devices. These services can be simple ad-
vertisements, polls, alerts or complex applications
such as browsing, search and e-commerce. The
latest mobile devices come equipped with high
resolution screen space, inbuilt web browsers and
full message keypads, however a majority of the
users still use cheaper models that have limited
screen space and basic keypad. On such devices,
SMS is the only mode of text communication.
This has encouraged service providers to build in-
formation based services around SMS technology.
Today, a majority of SMS based information ser-
vices require users to type specific codes to re-
trieve information. For example to get a duplicate
bill for a specific month, say June, the user has
to type DUPBILLJUN. This unnecessarily con-
straints users who generally find it easy and intu-
itive to type in a ?texting? language.
Some businesses have recently allowed users to
formulate queries in natural language using SMS.
For example, many contact centers now allow cus-
tomers to ?text? their complaints and requests for
information over SMS. This mode of communica-
tion not only makes economic sense but also saves
the customer from the hassle of waiting in a call
queue. Most of these contact center based services
and other regular services like ?AQA 63336?1 by
Issuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-
perts?3 by Number UK Ltd. and ?ChaCha?4 use
human agents to understand the SMS text and re-
spond to these SMS queries. The nature of tex-
ting language, which often as a rule rather than ex-
ception, has misspellings, non-standard abbrevia-
tions, transliterations, phonetic substitutions and
omissions, makes it difficult to build automated
question answering systems around SMS technol-
ogy. This is true even for questions whose answers
are well documented like a FAQ database. Un-
like other automatic question answering systems
that focus on generating or searching answers, in
a FAQ database the question and answers are al-
ready provided by an expert. The task is then
to identify the best matching question-answer pair
for a given query.
In this paper we present a FAQ-based ques-
tion answering system over a SMS interface. Our
1http://www.aqa.63336.com/
2http://www.gtip.co.uk/
3http://www.texperts.com/
4http://www.chacha.com/
852
system allows the user to enter a question in
the SMS texting language. Such questions are
noisy and contain spelling mistakes, abbrevia-
tions, deletions, phonetic spellings, translitera-
tions etc. Since mobile handsets have limited
screen space, it necessitates that the system have
high accuracy. We handle the noise in a SMS
query by formulating the query similarity over
FAQ questions as a combinatorial search prob-
lem. The search space consists of combinations
of all possible dictionary variations of tokens in
the noisy query. The quality of the solution, i.e.
the retrieved questions is formalized using a scor-
ing function. Unlike other SMS processing sys-
tems our model does not require training data or
human intervention. Our system handles not only
the noisy variations of SMS query tokens but also
semantic variations. We demonstrate the effective-
ness of our system on real-world data sets.
The rest of the paper is organized as follows.
Section 2 describes the relevant prior work in this
area and talks about our specific contributions.
In Section 3 we give the problem formulation.
Section 4 describes the Pruning Algorithm which
finds the best matching question for a given SMS
query. Section 5 provides system implementation
details. Section 6 provides details about our exper-
iments. Finally we conclude in Section 7.
2 Prior Work
There has been growing interest in providing ac-
cess to applications, traditionally available on In-
ternet, on mobile devices using SMS. Examples
include Search (Schusteritsch et al, 2005), access
to Yellow Page services (Kopparapu et al, 2007),
Email 5, Blog 6 , FAQ retrieval 7 etc. As high-
lighted earlier, these SMS-based FAQ retrieval ser-
vices use human experts to answer questions.
There are other research and commercial sys-
tems which have been developed for general ques-
tion and answering. These systems generally
adopt one of the following three approaches:
Human intervention based, Information Retrieval
based, or Natural language processing based. Hu-
man intervention based systems exploit human
communities to answer questions. These sys-
tems 8 are interesting because they suggest simi-
lar questions resolved in the past. Other systems
5http://www.sms2email.com/
6http://www.letmeparty.com/
7http://www.chacha.com/
8http://www.answers.yahoo.com/
like Chacha and Askme9 use qualified human ex-
perts to answer questions in a timely manner. The
information retrieval based system treat question
answering as an information retrieval problem.
They search large corpus of text for specific text,
phrases or paragraphs relevant to a given question
(Voorhees, 1999). In FAQ based question answer-
ing, where FAQ provide a ready made database of
question-answer, the main task is to find the clos-
est matching question to retrieve the relevant an-
swer (Sneiders, 1999) (Song et al, 2007). The
natural language processing based system tries to
fully parse a question to discover semantic struc-
ture and then apply logic to formulate the answer
(Molla et al, 2003). In another approach the ques-
tions are converted into a template representation
which is then used to extract answers from some
structured representation (Sneiders, 2002) (Katz et
al., 2002). Except for human intervention based
QA systems most of the other QA systems work
in restricted domains and employ techniques such
as named entity recognition, co-reference resolu-
tion, logic form transformation etc which require
the question to be represented in linguistically cor-
rect format. These methods do not work for SMS
based FAQ answering because of the high level of
noise present in SMS text.
There exists some work to remove noise from
SMS (Choudhury et al, 2007) (Byun et al, 2007)
(Aw et al, 2006) (Kobus et al, 2008). How-
ever, all of these techniques require aligned cor-
pus of SMS and conventional language for train-
ing. Building this aligned corpus is a difficult task
and requires considerable human effort. (Acharya
et al, 2008) propose an unsupervised technique
that maps non-standard words to their correspond-
ing conventional frequent form. Their method can
identify non-standard transliteration of a given to-
ken only if the context surrounding that token is
frequent in the corpus. This might not be true in
all domains.
2.1 Our Contribution
To the best of our knowledge we are the first to
handle issues relating to SMS based automatic
question-answering. We address the challenges
in building a FAQ-based question answering sys-
tem over a SMS interface. Our method is unsu-
pervised and does not require aligned corpus or
explicit SMS normalization to handle noise. We
propose an efficient algorithm that handles noisy
9http://www.askmehelpdesk.com/
853
lexical and semantic variations.
3 Problem Formulation
We view the input SMS S as a sequence of tokens
S = s1, s2, . . . , sn. Let Q denote the set of ques-
tions in the FAQ corpus. Each question Q ? Q
is also viewed as a sequence of terms. Our goal
is to find the question Q? from the corpus Q that
best matches the SMS S. As mentioned in the in-
troduction, the SMS string is bound to have mis-
spellings and other distortions, which needs to be
taken care of while performing the match.
In the preprocessing stage, we develop a Do-
main dictionary D consisting of all the terms that
appear in the corpusQ. For each term t in the dic-
tionary and each SMS token si, we define a simi-
larity measure ?(t, si) that measures how closely
the term t matches the SMS token si. We say that
the term t is a variant of si, if ?(t, si) > 0; this is
denoted as t ? si. Combining the similarity mea-
sure and the inverse document frequency (idf) of t
in the corpus, we define a weight function ?(t, si).
The similarity measure and the weight function are
discussed in detail in Section 5.1.
Based on the weight function, we define a scor-
ing function for assigning a score to each question
in the corpus Q. The score measures how closely
the question matches the SMS string S. Consider
a question Q ? Q. For each token si, the scor-
ing function chooses the term from Q having the
maximum weight; then the weight of the n chosen
terms are summed up to get the score.
Score(Q) =
n?
i=1
[
max
t:t?Q and t?si
?(t, si)
]
(1)
Our goal is to efficiently find the question Q? hav-
ing the maximum score.
4 Pruning Algorithm
We now describe algorithms for computing the
maximum scoring question Q?. For each token
si, we create a list Li consisting of all terms from
the dictionary that are variants of si. Consider a
token si. We collect all the variants of si from the
dictionary and compute their weights. The vari-
ants are then sorted in the descending order of
their weights. At the end of the process we have n
ranked lists. As an illustration, consider an SMS
query ?gud plc buy 10s strng on9?. Here, n = 6
and six lists of variants will be created as shown
Figure 1: Ranked List of Variations
in Figure 1. The process of creating the lists is
speeded up using suitable indices, as explained in
detail in Section 5.
Now, we assume that the lists L1, L2, . . . , Ln
are created and explain the algorithms for com-
puting the maximum scoring question Q?. We de-
scribe two algorithms for accomplishing the above
task. The two algorithms have the same function-
ality i.e. they compute Q?, but the second algo-
rithm called the Pruning algorithm has a better
run time efficiency compared to the first algorithm
called the naive algorithm. Both the algorithms re-
quire an index which takes as input a term t from
the dictionary and returns Qt, the set of all ques-
tions in the corpus that contain the term t. We
call the above process as querying the index on
the term t. The details of the index creation is dis-
cussed in Section 5.2.
Naive Algorithm: In this algorithm, we scan
each list Li and query the index on each term ap-
pearing in Li. The returned questions are added to
a collection C. That is,
C =
n?
i=1
?
?
?
t?Li
Qt
?
?
The collection C is called the candidate set. No-
tice that any question not appearing in the candi-
date set has a score 0 and thus can be ignored. It
follows that the candidate set contains the maxi-
mum scoring question Q?. So, we focus on the
questions in the collection C, compute their scores
and find the maximum scoring question Q?. The
scores of the question appearing in C can be com-
puted using Equation 1.
The main disadvantage with the naive algorithm
is that it queries each term appearing in each list
and hence, suffers from high run time cost. We
next explain the Pruning algorithm that avoids this
pitfall and queries only a substantially small subset
of terms appearing in the lists.
Pruning Algorithm: The pruning algorithm
854
is inspired by the Threshold Algorithm (Fagin et
al., 2001). The Pruning algorithm has the prop-
erty that it queries fewer terms and ends up with
a smaller candidate set as compared to the naive
algorithm. The algorithm maintains a candidate
set C of questions that can potentially be the max-
imum scoring question. The algorithm works in
an iterative manner. In each iteration, it picks
the term that has maximum weight among all the
terms appearing in the lists L1, L2, . . . , Ln. As
the lists are sorted in the descending order of the
weights, this amounts to picking the maximum
weight term amongst the first terms of the n lists.
The chosen term t is queried to find the setQt. The
set Qt is added to the candidate set C. For each
question Q ? Qt, we compute its score Score(Q)
and keep it along with Q. The score can be com-
puted by Equation 1 (For each SMS token si, we
choose the term from Q which is a variant of si
and has the maximum weight. The sum of the
weights of chosen terms yields Score(Q)). Next,
the chosen term t is removed from the list. Each
iteration proceeds as above. We shall now develop
a thresholding condition such that when it is sat-
isfied, the candidate set C is guaranteed to contain
the maximum scoring questionQ?. Thus, once the
condition is met, we stop the above iterative pro-
cess and focus only on the questions in C to find
the maximum scoring question.
Consider end of some iteration in the above pro-
cess. Suppose Q is a question not included in C.
We can upperbound the score achievable by Q, as
follows. At best, Q may include the top-most to-
ken from every list L1, L2, . . . , Ln. Thus, score of
Q is bounded by
Score(Q) ?
n?
i=0
?(Li[1]).
(Since the lists are sorted Li[1] is the term having
the maximum weight in Li). We refer to the RHS
of the above inequality as UB.
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set
C cannot be the maximum scoring question. Thus,
the condition ?Q? ? UB? serves as the termination
condition. At the end of each iteration, we check
if the termination condition is satisfied and if so,
we can stop the iterative process. Then, we simply
pick the question in C having the maximum score
and return it. The algorithm is shown in Figure 2.
In this section, we presented the Pruning algo-
Procedure Pruning Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?.
Begin
Construct lists L1, L2, . . . , Ln //(see Section 5.3).
// Li lists variants of si in descending
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(Li[1])
t? = Lj? [1]
// t? is the term having maximum weight among
// all terms appearing in the n lists.
Delete t? from the list Lj? .
Query the index and fetch Qt?
// Qt? : the set of all questions inQ
//having the term t?
For each Q ? Qt?
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(Li[1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 2: Pruning Algorithm
rithm that efficiently finds the best matching ques-
tion for the given SMS query without the need to
go through all the questions in the FAQ corpus.
The next section describes the system implemen-
tation details of the Pruning Algorithm.
5 System Implementation
In this section we describe the weight function,
the preprocessing step and the creation of lists
L1, L2, . . . , Ln.
5.1 Weight Function
We calculate the weight for a term t in the dic-
tionary w.r.t. a given SMS token si. The weight
function is a combination of similarity measure
between t and si and Inverse Document Frequency
(idf) of t. The next two subsections explain the
calculation of the similarity measure and the idf in
detail.
5.1.1 Similarity Measure
Let D be the dictionary of all the terms in the cor-
pus Q. For term t ? D and token si of the SMS,
the similarity measure ?(t, si) between them is
855
?(t, si) =
?
????
????
LCSRatio(t,si)
EditDistanceSMS(t,si)
if t and si share same
starting character *
0 otherwise
(2)
where LCSRatio(t, si) =
length(LCS(t,si))
length(t) and LCS(t, si) is
the Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, people
typically type the first few characters correctly. Also, this heuristic helps limit
the variants possible for a given token.
The Longest Common Subsequence Ratio
(LCSR) (Melamed, 1999) of two strings is the ra-
tio of the length of their LCS and the length of the
longer string. Since in SMS text, the dictionary
term will always be longer than the SMS token,
the denominator of LCSR is taken as the length of
the dictionary term. We call this modified LCSR
as the LCSRatio.
Procedure EditDistanceSMS
Input: term t, token si
Output: Consonant Skeleton Edit distance
Begin
return LevenshteinDistance(CS(si), CS(t)) + 1
// 1 is added to handle the case where
// Levenshtein Distance is 0
End
Consonant Skeleton Generation (CS)
1. remove consecutive repeated characters
// (call? cal)
2. remove all vowels
//(waiting ? wtng, great? grt)
Figure 3: EditDistanceSMS
The EditDistanceSMS shown in Figure 3
compares the Consonant Skeletons (Prochasson et
al., 2007) of the dictionary term and the SMS to-
ken. If the consonant keys are similar, i.e. the Lev-
enshtein distance between them is less, the simi-
larity measure defined in Equation 2 will be high.
We explain the rationale behind using the
EditDistanceSMS in the similarity measure
?(t, si) through an example. For the SMS
token ?gud? the most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result
the similarity measure between ?gud? and ?good?
will be higher than that of ?gud? and ?guided?.
5.1.2 Inverse Document Frequency
If f number of documents in corpus Q contain a
term t and the total number of documents in Q is
N, the Inverse Document Frequency (idf) of t is
idf(t) = log
N
f
(3)
Combining the similarity measure and the idf
of t in the corpus, we define the weight function
?(t, si) as
?(t, si) = ?(t, si) ? idf(t) (4)
The objective behind the weight function is
1. We prefer terms that have high similarity
measure i.e. terms that are similar to the
SMS token. Higher the LCSRatio and lower
the EditDistanceSMS , higher will be the
similarity measure. Thus for example, for a
given SMS token ?byk?, similarity measure
of word ?bike? is higher than that of ?break?.
2. We prefer words that are highly discrimi-
native i.e. words with a high idf score.
The rationale for this stems from the fact
that queries, in general, are composed of in-
formative words. Thus for example, for a
given SMS token ?byk?, idf of ?bike? will
be more than that of commonly occurring
word ?back?. Thus, even though the similar-
ity measure of ?bike? and ?back? are same
w.r.t. ?byk?, ?bike? will get a higher weight
than ?back? due to its idf.
We combine these two objectives into a single
weight function multiplicatively.
5.2 Preprocessing
Preprocessing involves indexing of the FAQ cor-
pus, formation of Domain and Synonym dictionar-
ies and calculation of the Inverse Document Fre-
quency for each term in the Domain dictionary.
As explained earlier the Pruning algorithm re-
quires retrieval of all questions Qt that contains a
given term t. To do this efficiently we index the
FAQ corpus using Lucene10. Each question in the
FAQ corpus is treated as a Document; it is tok-
enized using whitespace as delimiter and indexed.
10http://lucene.apache.org/java/docs/
856
The Domain dictionaryD is built from all terms
that appear in the corpus Q.
The weight calculation for Pruning algorithm
requires the idf for a given term t. For each term t
in the Domain dictionary, we query the Lucene in-
dexer to get the number of Documents containing
t. Using Equation 3, the idf(t) is calculated. The
idf for each term t is stored in a Hashtable, with t
as the key and idf as its value.
Another key step in the preprocessing stage is
the creation of the Synonym dictionary. The Prun-
ing algorithm uses this dictionary to retrieve se-
mantically similar questions. Details of this step is
further elaborated in the List Creation sub-section.
The Synonym Dictionary creation involves map-
ping each word in the Domain dictionary to it?s
corresponding Synset obtained from WordNet11.
5.3 List Creation
Given a SMS S, it is tokenized using white-spaces
to get a sequence of tokens s1, s2, . . . , sn. Digits
occurring in SMS token (e.g ?10s? , ?4get?) are re-
placed by string based on a manually crafted digit-
to-string mapping (?10? ? ?ten?). A list Li is
setup for each token si using terms in the domain
dictionary. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop
word . A term t from domain dictionary is in-
cluded in Li if its first character is same as that of
the token si and it satisfies the threshold condition
length(LCS(t, si)) > 1.
Each term t that is added to the list is assigned a
weight given by Equation 4.
Terms in the list are ranked in descending or-
der of their weights. Henceforth, the term ?list?
implies a ranked list.
For example the SMS query ?gud plc 2 buy 10s
strng on9? (corresponding question ?Where is a
good place to buy tennis strings online??), is to-
kenized to get a set of tokens {?gud?, ?plc?, ?2?,
?buy?, ?10s?, ?strng?, ?on9?}. Single character to-
kens such as ?2? are neglected as they are most
likely to be stop words. From these tokens cor-
responding lists are setup as shown in Figure 1.
5.3.1 Synonym Dictionary Lookup
To retrieve answers for SMS queries that are
semantically similar but lexically different from
questions in the FAQ corpus we use the Synonym
dictionary described in Section 5.2. Figure 4 illus-
trates some examples of such SMS queries.
11http://wordnet.princeton.edu/
Figure 4: Semantically similar SMS and questions
Figure 5: Synonym Dictionary LookUp
For a given SMS token si, the list of variations
Li is further augmented using this Synonym dic-
tionary. For each token si a fuzzy match is per-
formed between si and the terms in the Synonym
dictionary and the best matching term from the
Synonym dictionary, ? is identified. As the map-
pings between the Synonym and the Domain dic-
tionary terms are maintained, we obtain the corre-
sponding Domain dictionary term ? for the Syn-
onym term ? and add that term to the list Li. ? is
assigned a weight given by
?(?, si) = ?(?, si) ? idf(?) (5)
It should be noted that weight for ? is based on
the similarity measure between Synonym dictio-
nary term ? and SMS token si.
For example, the SMS query ?hw2 countr quik
srv?( corresponding question ?How to return a
very fast serve??) has two terms ?countr? ?
?counter? and ?quik? ? ?quick? belonging to
the Synonym dictionary. Their associated map-
pings in the Domain dictionary are ?return? and
?fast? respectively as shown in Figure 5. During
the list setup process the token ?countr? is looked
857
up in the Domain dictionary. Terms from the Do-
main dictionary that begin with the same character
as that of the token ?countr? and have a LCS > 1
such as ?country?,?count?, etc. are added to the
list and assigned a weight given by Equation 4.
After that, the token ?countr? is looked up in the
Synonym dictionary using Fuzzy match. In this
example the term ?counter? from the Synonym
dictionary fuzzy matches the SMS token. The Do-
main dictionary term corresponding to the Syn-
onym dictionary term ?counter? is looked up and
added to the list. In the current example the cor-
responding Domain dictionary term is ?return?.
This term is assigned a weight given by Equation
5 and is added to the list as shown in Figure 5.
5.4 FAQ retrieval
Once the lists are created, the Pruning Algorithm
as shown in Figure 2 is used to find the FAQ ques-
tionQ? that best matches the SMS query. The cor-
responding answer to Q? from the FAQ corpus is
returned to the user.
The next section describes the experimental
setup and results.
6 Experiments
We validated the effectiveness and usability of
our system by carrying out experiments on two
FAQ data sets. The first FAQ data set, referred
to as the Telecom Data-Set, consists of 1500 fre-
quently asked questions, collected from a Telecom
service provider?s website. The questions in this
data set are related to the Telecom providers prod-
ucts or services. For example queries about call
rates/charges, bill drop locations, how to install
caller tunes, how to activate GPRS etc. The sec-
ond FAQ corpus, referred to as the Yahoo DataSet,
consists of 7500 questions from three Yahoo!
Answers12 categories namely Sports.Swimming,
Sports.Tennis, Sports.Running.
To measure the effectiveness of our system, a
user evaluation study was performed. Ten human
evaluators were asked to choose 10 questions ran-
domly from the FAQ data set. None of the eval-
uators were authors of the paper. They were pro-
vided with a mobile keypad interface and asked to
?text? the selected 10 questions as SMS queries.
Through that exercise 100 relevant SMS queries
per FAQ data set were collected. Figure 6 shows
sample SMS queries. In order to validate that the
system was able to handle queries that were out of
12http://answers.yahoo.com/
Figure 6: Sample SMS queries
Data Set Relevant Queries Irrelevant Queries
Telecom 100 50
Yahoo 100 50
Table 1: SMS Data Set.
the FAQ domain, we collected 5 irrelevant SMS
queries from each of the 10 human-evaluators for
both the data sets. Irrelevant queries were (a)
Queries out of the FAQ domain e.g. queries re-
lated to Cricket, Billiards, activating GPS etc (b)
Absurd queries e.g. ?ama ameyu tuem? (sequence
of meaningless words) and (c) General Queries
e.g. ?what is sports?. Table 1 gives the number
of relevant and irrelevant queries used in our ex-
periments.
The average word length of the collected SMS
messages for Telecom and Yahoo datasets was 4
and 7 respectively. We manually cleaned the SMS
query data word by word to create a clean SMS
test-set. For example, the SMS query ?h2 mke a
pdl bke fstr? was manually cleaned to get ?how
to make pedal bike faster?. In order to quantify
the level of noise in the collected SMS data, we
built a character-level language model(LM)13 us-
ing the questions in the FAQ data-set (vocabulary
size is 44 characters) and computed the perplex-
ity14 of the language model on the noisy and the
cleaned SMS test-set. The perplexity of the LM on
a corpus gives an indication of the average num-
ber of bits needed per n-gram to encode the cor-
pus. Noise will result in the introduction of many
previously unseen n-grams in the corpus. Higher
number of bits are needed to encode these improb-
able n-grams which results in increased perplexity.
From Table 2 we can see the difference in perplex-
ity for noisy and clean SMS data for the Yahoo
and Telecom data-set. The high level of perplexity
in the SMS data set indicates the extent of noise
present in the SMS corpus.
To handle irrelevant queries the algorithm de-
scribed in Section 4 is modified. Only if the
Score(Q?) is above a certain threshold, it?s answer
is returned, else we return ?null?. The threshold
13http://en.wikipedia.org/wiki/Language model
14bits = log2(perplexity)
858
Cleaned SMS Noisy SMS
Yahoo bigram 14.92 74.58trigram 8.11 93.13
Telecom bigram 17.62 59.26trigram 10.27 63.21
Table 2: Perplexity for Cleaned and Noisy SMS
Figure 7: Accuracy on Telecom FAQ Dataset
was determined experimentally.
To retrieve the correct answer for the posed
SMS query, the SMS query is matched against
questions in the FAQ data set and the best match-
ing question(Q?) is identified using the Pruning al-
gorithm. The system then returns the answer to
this best matching question to the human evalua-
tor. The evaluator then scores the response on a bi-
nary scale. A score of 1 is given if the returned an-
swer is the correct response to the SMS query, else
it is assigned 0. The scoring procedure is reversed
for irrelevant queries i.e. a score of 0 is assigned
if the system returns an answer and 1 is assigned
if it returns ?null? for an ?irrelevant? query. The
result of this evaluation on both data-sets is shown
in Figure 7 and 8.
Figure 8: Accuracy on Yahoo FAQ Dataset
In order to compare the performance of our sys-
tem, we benchmark our results against Lucene?s
15 Fuzzy match feature. Lucene supports fuzzy
searches based on the Levenshtein Distance, or
Edit Distance algorithm. To do a fuzzy search
15http://lucene.apache.org
we specify the ? symbol at the end of each to-
ken of the SMS query. For example, the SMS
query ?romg actvt? on the FAQ corpus is refor-
mulated as ?romg? 0.3 actvt? 0.3?. The param-
eter after the ? specifies the required similarity.
The parameter value is between 0 and 1, with a
value closer to 1 only terms with higher similar-
ity will be matched. These queries are run on the
indexed FAQs. The results of this evaluation on
both data-sets is shown in Figure 7 and 8. The
results clearly demonstrate that our method per-
forms 2 to 2.5 times better than Lucene?s Fuzzy
match. It was observed that with higher values
of similarity parameter (? 0.6, ? 0.8), the num-
ber of correctly answered queries was even lower.
In Figure 9 we show the runtime performance of
the Naive vs Pruning algorithm on the Yahoo FAQ
Dataset for 150 SMS queries. It is evident from
Figure 9 that not only does the Pruning Algorithm
outperform the Naive one but also gives a near-
constant runtime performance over all the queries.
The substantially better performance of the Prun-
ing algorithm is due to the fact that it queries much
less number of terms and ends up with a smaller
candidate set compared to the Naive algorithm.
Figure 9: Runtime of Pruning vs Naive Algorithm
for Yahoo FAQ Dataset
7 Conclusion
In recent times there has been a rise in SMS based
QA services. However, automating such services
has been a challenge due to the inherent noise in
SMS language. In this paper we gave an efficient
algorithm for answering FAQ questions over an
SMS interface. Results of applying this on two
different FAQ datasets shows that such a system
can be very effective in automating SMS based
FAQ retrieval.
859
References
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Design-
ing the User Experience for Google SMS. CHI,
Portland, Oregon.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Inter-
face to Yellow Pages Directory, In Proceedings of
the 4th International conference on mobile technol-
ogy, applications, and systems and the 1st Interna-
tional symposium on Computer human interaction
in mobile technology, Singapore.
Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-
jit Jain, and Anupam Basu. 2007. Investigation and
Modeling of the Structure of Texting Language, In
Proceedings of IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, Hyderabad.
E. Voorhees. 1999. The TREC-8 question answering
track report.
D. Molla. 2003. NLP for Answer Extraction in Tech-
nical Domains, In Proceedings of EACL, USA.
E. Sneiders. 2002. Automated question answering
using question templates that cover the conceptual
model of the database, In Proceedings of NLDB,
pages 235?239.
B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.
Marton, and B. Temelkuran. 2002. Omnibase: Uni-
form access to heterogeneous data for question an-
swering, Natural Language Processing and Infor-
mation Systems, pages 230?234.
E. Sneiders. 1999. Automated FAQ Answering: Con-
tinued Experience with Shallow Language Under-
standing, Question Answering Systems. Papers from
the 1999 AAAI Fall Symposium. Technical Report
FS-99?02, November 5?7, North Falmouth, Mas-
sachusetts, USA, AAAI Press, pp.97?107
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007.
Question similarity calculation for FAQ answering,
In Proceeding of SKG 07, pages 298?301.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization, In Proceedings of COLING/ACL, pages
33?40.
Catherine Kobus, Franois Yvon and Graldine Damnati.
2008. Normalizing SMS: are two metaphors bet-
ter than one?, In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 441?448 Manchester.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement, Association for the Ad-
vancement of Artificial Intelligence. AAAI Workshop
on Enhanced Messaging
Ronald Fagin , Amnon Lotem , Moni Naor. 2001.
Optimal aggregation algorithms for middleware, In
Proceedings of the 20th ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database sys-
tems.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition, Computational Linguistics.
E. Prochasson, Christian Viard-Gaudin, Emmanuel
Morin. 2007. Language Models for Handwritten
Short Message Services, In Proceedings of the 9th
International Conference on Document Analysis and
Recognition.
Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,
Shourya Roy. 2008. Unsupervised learning of mul-
tilingual short message service (SMS) dialect from
noisy examples, In Proceedings of the second work-
shop on Analytics for noisy unstructured text data.
860
An Algorithmic Framework for the Decoding Problem in
Statistical Machine Translation
Raghavendra Udupa U Tanveer A Faruquie
IBM India Research Lab
Block-1A, IIT, Hauz Khas
New Delhi - 110 016
India
{uraghave, ftanveer}@in.ibm.com
Hemanta K Maji
Dept. of Computer Science
and Engineering, IIT Kanpur
Kanpur - 208 016
India,
hkmaji@iitk.ac.in
Abstract
The decoding problem in Statistical Ma-
chine Translation (SMT) is a computation-
ally hard combinatorial optimization prob-
lem. In this paper, we propose a new al-
gorithmic framework for solving the decod-
ing problem and demonstrate its utility. In
the new algorithmic framework, the decod-
ing problem can be solved both exactly and
approximately. The key idea behind the
framework is the modeling of the decod-
ing problem as one that involves alternat-
ing maximization of two relatively simpler
subproblems. We show how the subprob-
lems can be solved efficiently and how their
solutions can be combined to arrive at a so-
lution for the decoding problem. A fam-
ily of provably fast decoding algorithms can
be derived from the basic techniques under-
lying the framework and we present a few
illustrations. Our first algorithm is a prov-
ably linear time search algorithm. We use
this algorithm as a subroutine in the other
algorithms. We believe that decoding algo-
rithms derived from our framework can be
of practical significance.
1 Introduction
Decoding is one of the three fundamental prob-
lems in classical SMT (translation model and
language model being the other two) as pro-
posed by IBM in the early 1990?s (Brown et al,
1993). In the decoding problem we are given the
language and translation models and a source
language sentence and are asked to find the
most probable translation for the sentence. De-
coding is a discrete optimization problem whose
search space is prohibitively large. The chal-
lenge is, therefore, in devising a scheme to ef-
ficiently search the solution space for the solu-
tion.
Decoding is known to belong to a class of com-
putational problems popularly known as NP-
hard problems (Knight, 1999). NP-hard prob-
lems are known to be computationally hard and
have eluded polynomial time algorithms (Garey
and Johnson, 1979). The first algorithms for
the decoding problem were based on what is
known among the speech recognition commu-
nity as stack-based search (Jelinek, 1969). The
original IBM solution to the decoding prob-
lem employed a restricted stack-based search
(Berger et al, 1996). This idea was further ex-
plored by Wang and Waibel (Wang and Waibel,
1997) who developed a faster stack-based search
algorithm. In perhaps the first work on the
computational complexity of Decoding, Kevin
Knight showed that the problem is closely re-
lated to the more famous Traveling Salesman
problem (TSP). Independently, Christoph Till-
man adapted the Held-Karp dynamic program-
ming algorithm for TSP (Held and Karp, 1962)
to Decoding (Tillman, 2001). The original Held-
Karp algorithm for TSP is an exponential time
dynamic programming algorithm and Tillman?s
adaptation to Decoding has a prohibitive com-
plexity of O
(
l3m22m
) ? O (m52m) (where m
and l are the lengths of the source and tar-
get sentences respectively). Tillman and Ney
showed how to improve the complexity of the
Held-Karp algorithm for restricted word re-
ordering and gave a O
(
l3m4
) ? O (m7) algo-
rithm for French-English translation (Tillman
and Ney, 2000). An optimal decoder based on
the well-known A? heuristic was implemented
and benchmarked in (Och et al, 2001). Since
optimal solution can not be computed for prac-
tical problem instances in a reasonable amount
of time, much of recent work has focused on
good quality suboptimal solutions. An O
(
m6
)
greedy search algorithm was developed (Ger-
mann et al, 2003) whose complexity was re-
duced further to O
(
m2
)
(Germann, 2003).
In this paper, we propose an algorithmic
framework for solving the decoding problem and
show that several efficient decoding algorithms
can be derived from the techniques developed in
the framework. We model the search problem
as an alternating search problem. The search,
therefore, alternates between two subproblems,
both of which are much easier to solve in prac-
tice. By breaking the decoding problem into
two simpler search problems, we are able to pro-
vide handles for solving the problem efficiently.
The solutions of the subproblems can be com-
bined easily to arrive at a solution for the orig-
inal problem. The first subproblem fixes an
alignment and seeks the best translation with
that alignment. Starting with an initial align-
ment between the source sentence and its trans-
lation, the second subproblem asks for an im-
proved alignment. We show that both of these
problems are easy to solve and provide efficient
solutions for them. In an iterative search for a
local optimal solution, we alternate between the
two algorithms and refine our solution.
The algorithmic framework provides handles
for solving the decoding problem at several lev-
els of complexity. At one extreme, the frame-
work yields an algorithm for solving the decod-
ing problem optimally. At the other extreme, it
yields a provably linear time algorithm for find-
ing suboptimal solutions to the problem. We
show that the algorithmic handles provided by
our framework can be employed to develop a
very fast decoding algorithm which finds good
quality translations. Our fast suboptimal search
algorithms can translate sentences that are 50
words long in about 5 seconds on a simple com-
puting facility.
The rest of the paper is devoted to the devel-
opment and discussion of our framework. We
start with a mathematical formulation of the
decoding problem (Section 2). We then develop
the alternating search paradigm and use it to
develop several decoding algorithms (Section 3).
Next, we demonstrate the practical utility of our
algorithms with the help of results from our ini-
tial experiments (Section 5).
2 Decoding
The decoding problem in SMT is one of finding
the most probable translation e? in the target
language of a given source language sentence f
in accordance with the Fundamental Equation
of SMT (Brown et al, 1993):
e? = argmaxe Pr(f |e)Pr(e). (1)
In the remainder of this paper we will refer
to the search problem specified by Equation 1
as STRICT DECODING.
Rewriting the translation model Pr(f |e) as
?
a Pr(f ,a|e), where a denotes an alignment
between the source sentence and the target sen-
tence, the problem can be restated as:
e? = argmaxe
?
a
Pr(f ,a|e)Pr(e). (2)
Even when the translation model Pr(f |e) is
as simple as IBM Model 1 and the language
model Pr(e) is a bigram language model, the
decoding problem is NP-hard (Knight, 1999).
Unless P = NP, there is no hope of an efficient
algorithm for the decoding problem. Since the
Fundamental Equation of SMT does not yield
an easy handle to design a solution (exact or
even an approximate one) for the problem, most
researchers have instead worked on solving the
following relatively simpler problem (Germann
et al, 2003):
(e?, a?) = argmax(e,a) Pr(f ,a|e)Pr(e). (3)
We call the search problem specified
by Equation 3 as RELAXED DECODING.
Note that RELAXED DECODING relaxes
STRICT DECODING to a joint optimization
problem. The search in RELAXED DECODING
is for a pair (e?, a?). While RELAXED DECODING
is simpler than STRICT DECODING, it is also,
unfortunately, NP hard for even IBM Model
1 and Bigram language model. Therefore, all
practical solutions to RELAXED DECODING
have focused on finding suboptimal solutions.
The challenge is in devising fast search strate-
gies to find good suboptimal solutions. Table 1
lists the combinatorial optimization problems
in the domain of decoding.
In the remainder of the paper,m and l denote
the length of the source language sentence and
its translation respectively.
3 Framework for Decoding
We begin with a couple of useful observations
about the decoding problem. Although decep-
tively simple, these observations are very cru-
cial for developing our framework. They are
the source for algorithmic handles for breaking
the decoding problem into two relatively eas-
ier search problems. The first of these observa-
tions concerns with solving the problem when
we know in advance the mapping between the
source and target sentences. This leads to the
development of an extremely simple algorithm
for decoding when the alignment is known (or
Problem Search
STRICT DECODING e? = argmaxePr(f |e)Pr(e)
RELAXED DECODING (e?, a?) = argmax(e,a)Pr(f ,a|e)Pr(e)
FIXED ALIGNMENT DECODING e? = argmaxePr(f , a?|e)Pr(e)
VITERBI ALIGNMENT a? = argmaxaPr(f ,a|e?)
Table 1: Combinatorial Search Problems in Decoding
can be guessed). Our second observation is on
finding a better alignment between the source
and target sentences starting with an initial
(possibly suboptimal) alignment. The insight
provided by the two observations are employed
in building a powerful algorithmic framework.
3.1 Handles for attacking the Decoding
Problem
Our goal is to arrive at algorithmic handles
for attacking RELAXED DECODING. In this sec-
tion, we make couple of useful observations and
develop algorithmic handles from the insight
provided by them. The first of the two observa-
tions is:
Observation 1 For a given target length l and
a given alignment a? that maps source words to
target positions, it is easy to compute the opti-
mal target sentence e?.
e? = argmaxe Pr(f , a?|e)Pr(e). (4)
Let us call the search problem specified by
Equation 4 as FIXED ALIGNMENT DECODING.
What Observation 1 is saying is that once the
target sentence length and the source to tar-
get mapping is fixed, the optimal target sen-
tence (with the specified target length and
alignment) can be computed efficiently. As
we will show later, the optimal solution for
FIXED ALIGNMENT DECODING can be com-
puted in O (m) time for IBM models 1-5 using
dynamic programming. As we can always guess
an alignment (as is the case with many decoding
algorithms in the literature), the above observa-
tion provides an algorithmic handle for finding
suboptimal solutions for RELAXED DECODING.
Our second observation is on computing the
optimal alignment between the source sentence
and the target sentence.
Observation 2 For a given target sentence e?,
it is easy to compute the optimal alignment a?
that maps the source words to the target words.
a? = argmaxa Pr(f ,a|e?). (5)
It is easy to determine the optimal (Viterbi)
alignment between the source sentence and its
translation. In fact, for IBM models 1 and 2,
the Viterbi alignment can be computed using a
straight forward algorithm in O (ml) time. For
higher models, an approximate Viterbi align-
ment can be computed iteratively by an iter-
ative procedure called local search. In each it-
eration of local search, we look in the neighbor-
hood of the current best alignment for a better
alignment (Brown et al, 1993). The first itera-
tion can start with any arbitrary alignment (say
the Viterbi alignment of Model 2). It is possi-
ble to implement one iteration of local search in
O (ml) time. Typically, the number of iterations
is bounded in practice by O (m), and therefore,
local search takes O
(
m2l
)
time.
Our framework is not strictly dependent on
the computation of an optimal alignment. Any
alignment which is better than the current
alignment is good enough for it to work. It is
straight forward to find one such alignment us-
ing restricted swaps and moves in O (m) time.
In the remainder of this paper, we use the term
Viterbi to denote any linear time algorithm for
computing an improved alignment between the
source sentence and its translation.
3.2 Illustrative Algorithms
In this section, we show how the handles pro-
vided by the above two observations can be em-
ployed to solve RELAXED DECODING. The two
handles are in some sense complementary to
each other. When the alignment is known, we
can efficiently determine the optimal translation
with that alignment. On the other hand, when
the translation is known, we can efficiently de-
termine a better alignment. Therefore, we can
use one to improve the other. We begin with the
following simple linear time decoding algorithm
which is based on the first observation.
Algorithm NaiveDecode
Input: Source language sentence f of length
m > 0.
Optional Inputs: Target sentence length l,
alignment a? between the source words and tar-
get positions.
Output: Target language sentence e? of length
l.
1. If l is not specified, let l = m.
2. If an alignment is not specified, guess some
alignment a?.
3. Compute the optimal translation e? by solv-
ing FIXED ALIGNMENT DECODING,
i.e., e? = argmaxe Pr(f , a?|e)Pr(e).
4. return e?.
When the length of the translation is not
specified, NaiveDecode assumes that the trans-
lation is of the same length as the source sen-
tence. If an alignment that maps the source
words to target positions is not specified, the
algorithm guesses an alignment a? (a? can be the
trivial alignment that maps the source word fj
to target position j, that is, a?j = j, or can
be guessed more intelligently). It then com-
putes the optimal translation for the source
sentence f , with the length of the target sen-
tence and the alignment between the source and
the target sentences kept fixed to l and a? re-
spectively, by maximizing Pr(f , a?|e)Pr(e). As
FIXED ALIGNMENT DECODING can be solved
in O (m) time, NaiveDecode takes only O(m)
time.
The value of NaiveDecode lies not in itself per
se, but in its instrumental role in designing more
superior algorithms. The power of NaiveDecode
can be demonstrated with the following optimal
algorithm for RELAXED DECODING.
Algorithm NaiveOptimalDecode
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e? of length
l, m2 ? l ? 2m.
1. Let e? = null and a? = null.
2. For each l = m2 , . . . , 2m do
3. For each alignment a between the source
words and the target positions do
(a) Let e = NaiveDecode(f , l,a).
(b) If Pr (f , e,a) > Pr (f , e?, a?) then
i. e? = e
ii. a? = a.
4. return (e?, a?).
NaiveOptimalDecode considers various tar-
get lengths and all possible alignments be-
tween the source words and the target posi-
tions. For each target length l and alignment
a it employs NaiveDecode to find the best so-
lution. There are (l + 1)m candidate align-
ments for a target length l and O (m) can-
didate target lengths. Therefore, NaiveOp-
timalDecode explores ? (m(l + 1)m) alignments.
For each of these candidate alignments, it
makes a call to NaiveDecode. The time com-
plexity of NaiveOptimalDecode is, therefore,
O
(
m2(l + 1)m
)
. Although an exponential time
algorithm, it can compute the optimal solution
for RELAXED DECODING.
With NaiveDecode and NaiveOptimalDecode
we have demonstrated the power of the algo-
rithmic handle provided by Observation 1. It
is important to note that these two algorithms
are at the two extremities of the spectrum.
NaiveDecode is a linear time decoding algorithm
that computes a suboptimal solution for RE-
LAXED DECODING while NaiveOptimalDecode
is an exponential time algorithm that computes
the optimal solution. What we want are algo-
rithms that are close to NaiveDecode in com-
plexity and to NaiveOptimalDecode in qual-
ity. It is possible to reduce the complexity of
NaiveOptimalDecode significantly by carefully
reducing the number of alignments that are ex-
amined. Instead of examining all ?(m(l+1)m)
alignments, if we examine only a small num-
ber, say g (m), alignments in NaiveOptimalDe-
code, we can find a solution in O (mg (m)) time.
In the next section, we show how to restrict
the search to only a small number of promis-
ing alignments.
3.3 Alternating Maximization
We now show how to use the two algorithmic
handles to come up with a fast search paradigm.
We alternate between searching the best trans-
lation given an alignment and searching the
best alignment given a translation. Since the
two subproblems are complementary, they can
be used to improve the solution computed by
one another by alternating between the two
problems.
Algorithm AlternatingSearch
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e(o) of
length l (m/2 ? l ? 2m).
1. Let e(o) = null and a(o) = null.
2. For each l = m/2, . . . , 2m do
(a) Let e = null and a = null.
(b) While there is improvement in solution
do
i. Let e = NaiveDecode (f , l,a).
ii. Let a? = V iterbi (f , e).
(c) If Pr (f , e,a) > Pr (f , e(o),a(o)) then
i. e(o) = e
ii. a(o) = a.
3. return e(o).
AlternatingSearch searches for a good trans-
lation by varying the length of the tar-
get sentence. For a sentence length l,
the algorithm finds a translation of length
l and then iteratively improves the trans-
lation. In each iteration it solves two
subproblems: FIXED ALIGNMENT DECODING
and VITERBI ALIGNMENT. The input to each
iteration are the source sentence f , the tar-
get sentence length l, and an alignment a be-
tween the source and target sentences. So, Al-
ternatingSearch finds a better translation e for
f by solving FIXED ALIGNMENT DECODING.
For this purpose it employs NaiveDecode. Hav-
ing computed e, the algorithm computes a bet-
ter alignment (a?) between e and f by solving
VITERBI ALIGNMENT using Viterbi algorithm.
The new alignment thus found is used by the al-
gorithm in the subsequent iteration. At the end
of each iteration the algorithm checks whether
it has made progress. The algorithm returns the
best translation of the source f across a range
of target sentence lengths.
The analysis of AlternatingSearch is compli-
cated by the fact that the number of iterations
(see step 2.b) depends on the input. It is rea-
sonable to assume that the length of the source
sentence (m) is an upper bound on the number
of iterations. In practice, however, the number
of iterations is typically O (1). There are 3m/2
candidate sentence lengths for the translation
(l varies from m/2 to 2m) and both NaiveDe-
code and Viterbi are O (m). Therefore, the time
complexity of AlternatingSearch is O
(
m2
)
.
4 A Linear Time Algorithm for
FIXED ALIGNMENT DECODING
A key component of all our algorithms is
a linear time algorithm for the problem
FIXED ALIGNMENT DECODING. Recall that in
FIXED ALIGNMENT DECODING, we are given
the target length l and a mapping a? from source
words to target positions. The goal is then to
find the optimal translation with a? as the align-
ment. In this section, we give a dynamic pro-
gramming based solution to this problem. Our
solution is based on a new formulation of IBM
translation models. We begin our discussion
with a few technical definitions.
Alignment a? maps each of the source words
fj, j = 1, . . . ,m to a target position in the range
[0, . . . , l]. Define a mapping ? from [0, . . . , l] to
subsets of {1, . . . ,m} as follows:
?(i) = {j : j ? {1, . . . ,m} ? a?j = i} ? i = 0, . . . , l.
?(i) is the set of source positions which are
mapped to the target location i by the align-
ment a? and the fertility of the target position i
is ?i = |?(i)|.
We can rewrite each of the IBM models
Pr (f , a?|e) as follows:
Pr (f , a?|e) = ?
l
?
i=1
TiDiNi.
Table 2 shows the breaking of Pr (f , a?|e) into
the constituents Ti,Di and Ni. As a conse-
quence, we can write Pr (f , a?|e)Pr (e) as:
Pr (f , a?|e)Pr (e) = ??
l
?
i=1
TiDiNiLi
where Li = trigram(ei|ei?2, ei?1) and ? is the
trigram probability of the boundary word.
The above reformulation of the optimiza-
tion function of the decoding problem allows
us to employ Dynamic Programming for solv-
ing FIXED ALIGNMENT DECODING efficiently.
Note that each word ei has only a constant num-
ber of candidates in the vocabulary. Therefore,
the set of words e1, . . . , el that maximizes the
LHS of the above optimization function can be
found in O (m) time using the standard Dy-
namic Programming algorithm (Cormen et al,
2001).
5 Experiments and Results
In this section we describe our experimental
setup and present the initial results. Our goal
Model ? Ti Di Ni
1 ?(m|l)(l+1)m
?
k??(i) t(fk |ei) 1 1
2 ?(m|l) ?k??(i) t(fk |ei)
?
k??(i) a(i|k,m, l) 1
3 n(?0|m)pm?2?00 p?01
?
k??(i) t(fk |ei)
?
k??(i) d(k|i,m, l) ?i! n(?i|ei)
Table 2: Pr (f, a?|e) for IBM Models
was not only to evaluate the performance of our
algorithms on real data, but also to evaluate
how easy it is to code the algorithm and whether
a straightforward implementation of the algo-
rithm with no parameter tuning can give satis-
factory results.
We implemented the algorithms in C++ and
conducted the experiments on an IBM RS-6000
dual processor machine with 1 GB of RAM. We
built a French-English translation model (IBM
Model 3) by training over a corpus of 100 K sen-
tence pairs from the Hansard corpus. The trans-
lation direction was from French to English. We
built an English language model by training
over a corpus consisting of about 800 million
words. We divided the test sentences into sev-
eral classes based on their length. Each length
class consisted of 300 test French sentences.
We implemented four algorithms -1.1 (NaiveDe-
code), 1.2 (Alternating Search with l restricted
to m), 2.1 (NaiveDecode with l varying from
m/2 to 2m) and 2.2 (Alternating Search). In
order to compare the performance of the al-
gorithms proposed in this paper with a previ-
ous decoding algorithm, we also implemented
the dynamic programming based algorithm by
(Tillman, 2001). For each of the algorithms, we
computed the following:
1. Average time taken for translation for
each length class.
2. NIST score of the translations for each
length class.
3. Average value of the optimization
function for the translations for each
length class.
The results of the experiments are summa-
rized in Plots 1, 2 and 3. In all the plots, the
length class is denoted by the x-axis. 11-20 indi-
cates the class with sentences of length between
11 words to 20 words. 51 indicates the group
of sentences with sentence length 51 or more.
Plot 1 shows the average time taken by the al-
gorithms for translating the sentences in each
length class. Time is shown in seconds on a log
scale. Plot 2 shows the NIST score of the trans-
lations for each length class while Plot 3 shows
the average log score of the translations (-ve log
of Pr (f ,a|e)Pr (e)) again for each length class.
It can be seen from Plot 1 that all of our al-
gorithms are indeed very fast in practice. They
are, in fact, an order faster than the Held-Karp
algorithm. Our algorithms are able to trans-
late even long sentences (50+ words) in a few
seconds.
Plot 3 shows that the log scores of the trans-
lations computed by our algorithms are very
close to those computed by the Held-Karp al-
gorithm. Plot 2 compares the NIST scores ob-
tained with each of the algorithm. Among the
four algorithms based on our framework, Al-
gorithm 2.2 gives the best NIST scores as ex-
pected. Although, the log scores of our algo-
rithms are comparable to those of the Held-
Karp algorithm, our NIST scores are lower. It
should be noted that the mathematical quan-
tity that our algorithm tries to optimize is the
log score. Plot 3 shows that our algorithms are
quite good at finding solutions with good scores.
 0.01
 0.1
 1
 10
 100
 1000
 10000
0-10 11-20 21-30 31-40 41-50 51-
Ti
m
e 
in 
se
co
nd
s
Sentence Length
Decoding Time
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 1: Average decoding time
6 Conclusions
The algorithmic framework developed in this
paper is powerful as it yields several decoding
algorithms. At one end of the spectrum is a
provably linear time algorithm for computing
a suboptimal solution and at the other end is
an exponential time algorithm for computing
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
0-10 11-20 21-30 31-40 41-50 51-
NI
ST
 S
co
re
Sentence Length
NIST Scores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 2: NIST scores
 0
 50
 100
 150
 200
 250
 300
 350
 400
0-10 11-20 21-30 31-40 41-50 51-
log
sc
or
e
Sentence Length
Logscores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 3: Log score
the optimal solution. We have also shown that
alternating maximization can be employed to
come up with O
(
m2
)
decoding algorithm. Two
questions in this connection are:
1. Is it possible to reduce the complexity
of AlternatingSearch to O (m)?
2. Instead of exploring each alignment
separately, is it possible to explore a
bunch of alignments in one shot?
Answers to these questions will result in faster
and more efficient decoding algorithms.
7 Acknowledgements
We are grateful to Raghu Krishnapuram for his
insightful comments on an earlier draft of this
paper and Pasumarti Kamesam for his help dur-
ing the course of this work.
References
A. Berger, P. Brown, S. Della Pietra, V. Della
Pietra, A. Kehler, and R. Mercer. 1996. Lan-
guage translation apparatus and method us-
ing context-based translation models. United
States Patent 5,510,981.
P. Brown, S. Della Pietra, V. Della Pietra,
and R. Mercer. 1993. The mathematics of
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
T. H. Cormen, C. E. Leiserson, R. L. Rivest,
and C. Stein. 2001. The MIT Press, Cam-
bridge.
M. R. Garey and D. S. Johnson. 1979. W. H.
Freeman and Company, New York.
U. Germann, M. Jahr, D. Marcu, and K. Ya-
mada. 2003. Fast decoding and optimal de-
coding for machine translation. Artificial In-
telligence.
Ulrich Germann. 2003. Greedy decoding for
statistical machine translation in almost lin-
ear time. In Proceedings of HLT-NAACL
2003. Edmonton, Canada.
M. Held and R. Karp. 1962. A dynamic pro-
gramming approach to sequencing problems.
J. SIAM, 10(1):196?210.
F. Jelinek. 1969. A fast sequential decoding al-
gorithm using a stack. IBM Journal Reseach
and Development, 13:675?685.
Kevin Knight. 1999. Decoding complexity in
word-replacement translation models. Com-
putational Linguistics, 25(4).
F. Och, N. Ueffing, and H. Ney. 2001. An ef-
ficient a* search algorithm for statistical ma-
chine translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in
Machine Translation, pages 55?62. Toulouse,
France.
C. Tillman and H. Ney. 2000. Word reorder-
ing and dp-based search in statistical machine
translation. In Proceedings of the 18th COL-
ING, pages 850?856. Saarbrucken, Germany.
Christoph Tillman. 2001. Word re-ordering
and dynamic programming based search
algorithm for statistical machine transla-
tion. Ph.D. Thesis, University of Technology
Aachen, pages 42?45.
R. Udupa and T. Faruquie. 2004. An english-
hindi statistical machine translation system.
In Proceedings of the 1st IJCNLP, pages 626?
632. Sanya, Hainan Island, China.
Y. Wang and A. Waibel. 1997. Decoding al-
gorithm in statistical machine translation. In
Proceedings of the 35th ACL, pages 366?372.
Madrid, Spain.
Coling 2010: Poster Volume, pages 189?196,
Beijing, August 2010
Unsupervised cleansing of noisy text
Danish Contractor
IBM India Software Labs
dcontrac@in.ibm.com
Tanveer A. Faruquie
IBM Research India
ftanveer@in.ibm.com
L. Venkata Subramaniam
IBM Research India
lvsubram@in.ibm.com
Abstract
In this paper we look at the problem of
cleansing noisy text using a statistical ma-
chine translation model. Noisy text is pro-
duced in informal communications such
as Short Message Service (SMS), Twit-
ter and chat. A typical Statistical Ma-
chine Translation system is trained on par-
allel text comprising noisy and clean sen-
tences. In this paper we propose an un-
supervised method for the translation of
noisy text to clean text. Our method has
two steps. For a given noisy sentence, a
weighted list of possible clean tokens for
each noisy token are obtained. The clean
sentence is then obtained by maximizing
the product of the weighted lists and the
language model scores.
1 Introduction
Noisy unstructured text data is found in informal
settings such as Short Message Service (SMS),
online chat, email, social message boards, news-
group postings, blogs, wikis and web pages. Such
text may contain spelling errors, abbreviations,
non-standard terminology, missing punctuation,
misleading case information, as well as false
starts, repetitions, and special characters.
We define noise in text as any kind of difference
between the surface form of a coded representa-
tion of the text and the correct text. The SMS ?u
kno whn is d last train of delhi metro? is noisy
because several of the words are not spelled cor-
rectly and there are grammar mistakes. Obviously
the person who wrote this message intended to
write exactly what is there in the SMS. But still it
is considered noisy because the message is coded
using non-standard spellings and grammar.
Current statistical machine translation (SMT)
systems rely on large parallel and monolingual
training corpora to produce high quality transla-
tions (Brown et al, 1993). Most of the large paral-
lel corpora available comprise newswire data that
include well formed sentences. Even when web
sources are used to train a SMT system, noisy por-
tions of the corpora are eliminated (Imamura et
al., 2003) (Imamura and Sumita, 2002) (Khadivi
and Ney, 2005). This is because it is known that
noise in parallel corpora results in incorrect train-
ing of models thus degrading the performance.
We are not aware of sufficiently large paral-
lel datasets comprising noisy and clean sentences.
In fact, even dictionaries comprising of noisy to
clean mappings in one language are very limited
in size.
With the increase in noisy text data generated
in various social communication media, cleans-
ing of such text has become necessary. The lack
of noisy parallel datasets means that this prob-
lem cannot be tackled in the traditional SMT way,
where translation models are learned based on the
parallel dataset. Consider the problem of translat-
ing a noisy English sentence e to a clean English
sentence h. SMT imagines that e was originally
conceived in clean English which when transmit-
ted over the noisy channel got corrupted and be-
came a noisy English sentence. The objective of
SMT is to recover the original clean sentence.
189
The goal of this paper is to analyze how noise
can be tackled. We present techniques to trans-
late noisy text sentences e to clean text sentences
h. We show that it is possible to clean noisy text
in an unsupervised fashion by incorporating steps
to construct ranked lists of possible clean English
tokens and then searching for the best clean sen-
tence. Of course as we will show for a given noisy
sentence, several clean sentences are possible. We
exploit the statistical machine learning paradigm
to let the decoder pick the best alternative from
these possible clean options to give the final trans-
lation for a given noisy sentence.
The rest of the paper is organized as follows.
In section 2 we state our contributions and give
an overview of our approach. In Section 3 we
describe the theory behind clean noisy text using
MT. In Section 4 we explain how we use a weigh-
ing function and a plain text dictionary of clean
tokens to guess possible clean English language
tokens. Section 5 describes our system along with
our results. We have given an analysis of the kind
of noise present in our data set in section 5.2
2 Our Approach
In this paper we describe an unsupervised method
to clean noisy text. We formulate the text cleans-
ing problem in the machine translation framework
using translation model 1 (Brown et al, 1993).
We clean the text using a pseudo-translation
model of clean and noisy words along with a lan-
guage model trained using a large monolingual
corpus. We use a decoder to search for the best
clean sentence for a noisy sentence using these
models.
We generate scores for the pseudo translation
model using a weighing function for each token in
an SMS and use these scores along with language
model probabilities to hypothesize the best clean
sentence for a given noisy SMS. Our approach can
be summarized in the following steps:
? Tokenize noisy SMS S into n tokens s1, s2 ...
sn. For each SMS token si create a weighted
list based on a weighing function. These lists
along with their scores corresponds to the
translation probabilities of the SMT transla-
tion model.
? Use the lists generated in the step above
along with clean text language model scores,
in a decoder to hypothesize the best clean
sentence
? At the end of the search choose the highest
scoring sentence as the clean translation of
the noisy sentence
In the above approach we do not learn the trans-
lation model but emulate the translation model
during decoding by analyzing the noise of the to-
kens in the input sentence.
3 Noisy sentence translation
Statistical Translation models were invented by
Brown, et al(Brown et al, 1993) and are based
on the source-channel paradigm of communica-
tion theory. Consider the problem of translating a
noisy sentence e to a clean sentence h. We imag-
ine that e was originally conceived cleanly which
when transmitted over the noisy communication
channel got corrupted and became a noisy sen-
tence. The goal is to get back the original clean
sentence from the noisy sentence. This can be ex-
pressed mathematically as
h? = argmax
h
Pr(h|e)
By Bayes? Theorem
h? = argmax
h
Pr(e|h)Pr(h)
Conceptually, the probability distribution
P (e|h) is a table which associates a probability
score with every possible pair of clean and noisy
sentences (e, h). Every noisy sentence e is a
candidate translation of a given clean sentence h.
The goodness of the translation h? e is given by
the probability score of the pair (e, h). Similarly,
Pr(h) is a table which associates a probability
score with every possible clean sentence h and
measures how well formed the sentence h is.
It is impractical to construct these tables exactly
by examining individual sentences (and sentence
pairs) since the number of conceivable sentences
in any language is countably infinite. Therefore,
the challenge in Statistical Machine Translation
is to construct approximations to the probability
190
distributions P (e|h) and Pr(h) that give an ac-
ceptable quality of translation. In the next section
we describe a model which is used to approximate
P (e|h).
3.1 IBM Translation Model 2
IBM translation model 2 is a generative model,
i.e., it describes how a noisy sentence e could be
stochastically generated given a clean sentence h.
It works as follows:
? Given a clean sentence h of length l, choose
the length (m) for the noisy sentence from a
distribution (m|l).
? For each position j = 1, 2, . . .m in the noisy
string, choose a position aj in the clean string
from a distribution a(aj |j, l,m). The map-
ping a = (a1, a2, . . . , am) is known as align-
ment between the noisy sentence e and the
clean sentence h. An alignment between e
and h tells which word of e is the corrupted
version of the corresponding word of h.
? For each j = 1, 2, . . .m in the noisy string,
choose an noisy word ej according to the dis-
tribution t(ej |haj ).
It follows from the generative model that prob-
ability of generating e = e1e2 . . . em given h =
h1h2 . . . hl with alignment a = (a1, a2, . . . , am)
is
Pr(e, a|h) = (m|l)
m?
j=1
t(ej |haj )a(aj |j,m, l).
It can be easily seen that a sentence e could be
produced from h employing many alignments and
therefore, the probability of generating e given
h is the sum of the probabilities of generating
e given h under all possible alignments a, i.e.,
Pr(e|h) =?a Pr(e, a|h). Therefore,
Pr(e|h) =
(m|l)
l?
a1=0
..
l?
am=0
m?
j=1
t(ej |haj )a(aj |j,m, l).
The above expression can be rewritten as follows:
Pr(e|h) = (m|l)
m?
j=1
l?
i=0
t(ej |hi)a(i|j,m, l).
Typical statistical machine translation systems
use large parallel corpora to learn the transla-
tion probabilities (Brown et al, 1993). Tradi-
tionally such corpora have consisted of news ar-
ticles and other well written articles. Therefore
in theory P (e|h) should be constructed by ex-
amining sentence pairs of clean and noisy sen-
tences. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training.
Aligned parallel corpora for noisy sentence is
difficult to obtain. This lack of data for a lan-
guage and the domain dependence of noise makes
it impractical to construct corpus from which
P (e|h) can be learnt automatically. This leads
to difficulty in learning P (e|h). Fortunately the
alignment between clean and noisy sentences are
monotonic in nature hence we assume a uniform
distribution for a(i|j,m, l) held fixed at (l+1)?1.
This is equivalent to model 1 of IBM translation
model. The translation models t(ej |haj ) can be
thought of as a ranked list of noisy words given
a clean word. In section 4.2 we show how this
ranked list can be constructed in an unsupervised
fashion.
3.2 Language Model
The problem of estimating the sentence forma-
tion distribution Pr(h) is known as the lan-
guage modeling problem. The language mod-
eling problem is well studied in literature par-
ticularly in the context of speech recognition.
Typically, the probability of a n-word sentence
h = h1h2 . . . hn is modeled as Pr(h) =
Pr(h1|H1)Pr(h2|H2) . . . P r(hn|Hn), where Hi
is the history of the ith word hi. One of the most
popular language models is the n-gram model
(Brown et al, 1993) where the history of a word
consists o f the word and the previous n?1 words
in the sentence, i.e., Hi = hihi?1 . . . hi?n+1. In
our application we use a smoothed trigram model.
3.3 Decoding
The problem of searching for a sentence h which
minimizes the product of translation model prob-
191
ability and the language model probability is
known as the decoding problem. The decoding
problem has been proved to be NP-complete even
when the translation model is IBM model 1 and
the language model is bi-gram (K Knight., 1999).
Effective suboptimal search schemes have been
proposed (F. Jelinek, 1969), (C. Tillman et al,
1997).
4 Pseudo Translation Model
In order to be able to exploit the SMT paradigm
we first construct a pseudo translation model. The
first step in this direction is to create noisy token
to clean token mapping. In order to process the
noisy input we first have to map noisy tokens in
noisy sentence, Se, to the possible correct lexical
representations. We use a similarity measure to
map the noisy tokens to their clean lexical repre-
sentations .
4.1 Similarity Measure
For a term te ? De, where De is a dictionary of
possible clean tokens, and token si of the noisy
input Se, the similarity measure ?(te, si) between
them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si) if te and si share
same starting
character
0 otherwise
(1)
where LCSRatio(te, si) = length(LCS(te,si))length(te) and
LCS(te, si) is the Longest common subsequence
between te and si. The intuition behind this mea-
sure is that people typically type the first few char-
acters of a word in an SMS correctly. This way we
limit the possible variants for a particular noisy to-
ken.
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is
the ratio of the length of their LCS and the length
of the longer string. Since in the SMS scenario,
the dictionary term will always be longer than the
SMS token, the denominator of LCSR is taken as
the length of the dictionary term.
The EditDistanceSMS (Figure 1) compares
the Consonant Skeletons (Prochasson et al, 2007)
of the dictionary term and the SMS token. If the
Levenshtein distance between consonant skele-
tons is small then ?(te, si) will be high. The intu-
ition behind using EditDistanceSMS can be ex-
plained through an example. Consider an SMS
token ?gud? whose most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a re-
sult the similarity measure between ?gud? and
?good? will be higher than that of ?gud? and
?guided?. Higher the LCSRatio and lower the
EditDistanceSMS , higher will be the similarity
measure. Hence, for a given SMS token ?byk?,
the similarity measure of word ?bike? is higher
than that of ?break?.
In the next section we show how we use
this similarity measure to construct ranked lists.
Ranked lists of clean tokens have also been used
in FAQ retrieval based on noisy queries (Kothari
et al, 2009).
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 1: EditDistanceSMS
4.2 List Creation
For a given noisy input string Se, we tokenize it
on white space and replace any occurrence of dig-
its to their string based form (e.g. 4get, 2day) to
get a series of n tokens s1, s2, . . . , sn. A list Lei
is created for each token si using terms in a dic-
192
hv u cmplted ure prj rprt
d ddline fr sbmission of d rprt hs bn xtnded
i wil be lte by 20 mns
d docs shd rech u in 2 days
thnk u for cmg 2 d prty
Figure 2: Sample SMS queries
tionary De consisting of clean english words. A
term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (2)
Heuristics are applied to boost scores of some
words based on positional properties of characters
in noisy and clean tokens. The scores of the fol-
lowing types of tokens are boosted:
1. Tokens that are a substring of a dictionary
words from the first character.
2. Tokens having the same first and last charac-
ter as a dictionary word.
3. Token that are dictionary words themselves
(clean text).
The threshold value ? is determined experimen-
tally. Thus we select only the top scoring possible
clean language tokens to construct the sentence.
Once the list are constructed the similarity mea-
sure along with the language model scores is used
by the decoding algorithm to find the best possi-
ble English sentence. It is to be noted that these
lists are constructed at decoding time since they
depend on the noisy surface forms of words in the
input sentence.
5 Experiments
To evaluate our system we used a set of 800 noisy
English SMSes sourced from the publicly avail-
able National University of Singapore SMS cor-
pus1 and a collection of SMSes available from the
Indian Institute of Technology, Kharagpur. The
SMSes are a collection of day-to-day SMS ex-
changes between different users. We manually
1http://wing.comp.nus.edu.sg/downloads/smsCorpus
Figure 3: System implementation
BLEU scores 1-gram 2-gram 3-gram 4-gram
Noisy text 40.96 63.7 45.1 34.5 28.3
Cleaned text 53.90 77.5 58.7 47.4 39.5
Table 1: BLEU scores
generated a cleaned english version of our test set
to use as a reference.
The noisy SMS tokens were used to generate
clean text candidates as described in section 4.2.
The dictionary De used for our experiments was a
plain text list of 25,000 English words. We cre-
ated a tri-gram language model using a collec-
tion of 100,000 clean text documents. The docu-
ments were a collection of articles on news, sport-
ing events, literature, history etc. For decoding
we used Moses2, which is an open source decoder
for SMT (Hoang et al, 2008), (Koehn et al,
2007). The noisy SMS along with clean candi-
date token lists, for each SMS token and language
model probabilities were used by Moses to hy-
pothesize the best clean english output for a given
noisy SMS. The language model and translation
models weights used by Moses during the decod-
ing phase, were adjusted manually after some ex-
perimentation.
We used BLEU (Bilingual evaluation under-
study) and Word error rate (WER) to evaluate the
performance of our system. BLEU is used to
2http://www.statmt.org/moses/
193
Figure 4: Comparison of BLEU scores
establish similarity between a system translated
and human generated reference text. A noisy
SMS ideally has only one possible clean transla-
tion and all human evaluators are likely to provide
the same translation. Thus, BLEU which makes
use of n-gram comparisons between reference and
system generated text, is very useful to measure
the accuracy of our system. As shown in Fig 4
, our system reported significantly higher BLEU
scores than unprocessed noisy text.
The word error rate is defined as
WER = S +D + IN (3)
where S is the number of substitutions, D is the
number of the deletions, I is the number of the in-
sertions and N is the number of words in the refer-
ence The WER can be thought of as an execution
of the Levenstein Edit distance algorithm at the
token level instead of character level.
Fig 5 shows a comparison of the WER. Sen-
tences generated from our system had 10 % lower
WER as compared to the unprocessed noisy sen-
tences. In addition, the sentences generated by our
system match a higher number of tokens (words)
with the reference sentences, as compared to the
noisy sentences.
5.1 System performance
Unlike standard MT system when P (e|h) is pre-
computed during the training time, list generation
in our system is dynamic because it depends on
the noisy words present in the input sentence. In
this section we evaluate the computation time for
list generation along with the decoding time for
finding the best list. We used an Intel Core 2
Duo 2.2 GHz processor with 3 GB DDR2 RAM
Figure 5: Word error rates
Figure 6: Execution time slices
to implement our system. As shown in Fig 6 the
additional computation involving list creation etc
takes up 56% (90 milliseconds) of total translation
time. 43% of the total execution time is taken by
the decoder, while I/O operations take only 1% of
the total execution time. The decoder execution
time slices reported above exclude the time taken
to load the language model. Moses took approxi-
mately 10 seconds to load our language model.
5.2 Measuring noise level in SMS queries
The noise in the collected SMS corpus can be cat-
egorized as follows
1. Removal of characters : The commonly ob-
served patterns include deletion of vowels
(as in ?msg? for ?message?), deletion of re-
peated character (as in ?happy? for ?hapy?)
and truncation (as in ?tue? for ?tuesday?)
Type of Noise % of Total Noisy Tokens
Deletion of Characters 48%
Phonetic Substitution 33%
Abbreviations 5%
Dialectical Usage 4%
Deletion of Words 1.2%
Table 2: Measure of Types of SMS Noise
194
Clean (Reference) text Noisy text Output text
Perplexity 19.61 34.56 21.77
Table 3: Perplexity for Reference, Noisy Cleaned
SMS
2. Phonetic substitution: For example, ?2? for
?to? or ?too?, ?lyf?? for ?life?, ?lite? for
?light? etc.
3. Abbreviation: Some frequently used abbre-
viations are ?tb? for ?text back?, ?lol? for
?laughs out loud?, ?AFAICT? for ?as far as
i can tell? etc.
4. Dialectal and informal usage: Often multiple
words are combined into a single token fol-
lowing certain dialectal conventions. For ex-
ample, ?gonna? is used for ?going to?, ?aint?
is used for ?are not?, etc.
5. Deletion of words: Function words (e.g. ar-
ticles) and pronouns are commonly deleted.
?I am reading the book? for example may be
typed as ?readin bk?.
Table 2 lists statistics on these noise types from
101 SMSes selected at random from our data set.
The average length of these SMSes was 13 words.
Out of the total number of words in the SMSes,
52% were non standard words. Table 2 lists the
statistics for the types of noise present in these non
standard words.
Measuring character level perplexity can be an-
other way of estimating noise in the SMS lan-
guage.The perplexity of a LM on a corpus gives
an indication of the average number of bits needed
per n-gram to encode the corpus. Noise results
in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits
are needed to encode these improbable n-grams
which results in increased perplexity.
We built a character-level language model (LM)
using a document collection (vocabulary size is
20K) and computed the perplexity of the language
model on the noisy and the cleaned SMS test-set
and the SMS reference data.
From Table 3 we can see the difference in per-
plexity for noisy and clean SMS data. Large per-
plexity values for the SMS dataset indicates a high
level of noise. The perplexity evaluation indicates
that our method is able to remove noise from the
input queries as given by the perplexity and is
close to the human correct reference corpus whose
perplexity is 19.61.
6 Conclusion
We have presented an inexpensive, unsupervised
method to clean noisy text. It does not require
the use of a noisy to clean language parallel cor-
pus for training. We show how a simple weigh-
ing function based on observed heuristics and a
vocabulary file can be used to shortlist clean to-
kens. These tokens and their weights are used
along with language model scores, by a decoder
to select the best clean language sentence.
References
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of tex-
ting language. International Journal on Document
Analysis and Recognition.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. In Proceedings of AAAI
Workshop on Enhanced Messaging.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normal-
ization. In Proceedings of COLING-ACL.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS :
Evaluation et bilan quantitatif. In Actes de TALN,
Toulouse, France.
Catherine Kobus, Francois Yvon and Geraldine
Damnati. 2008. Normalizing SMS: Are two
metaphors better than one? In Proceedings of COL-
ING, Manchester.
Sreangsu Acharya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, Evan Herbst 2007.
Moses: Open source toolkit for statistical machine
195
translation. In Proceedings of ACL, Demonstration
Session .
Peter F. Brown, Vincent J.Della Pietra, Stephen A.
Della Pietra, Robert. L. Mercer 1993. The Math-
ematics of Statistical Machine Translation: Parame-
ter Estimation Computational Linguistics.
I. D. Melamed. 1999. Bitext maps and alignment via
pattern recognition. Computational Linguistics.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message
services. In Proceedings of ICDAR.
S. Khadivi and H. Ney. 2005. Automatic filtering of
bilingual corpora for statistical machine translation.
In Proceedings of NLDB, pages 263?274, 2005.
K. Imamura and E. Sumita. 2002. Bilingual corpus
cleaning focusing on translation literality. In In Pro-
ceedings of ICSLP.
K. Imamura, E. Sumita, and Y. Matsumoto. 2003. Au-
tomatic construction of machine translation knowl-
edge using translation literalness. In In Proceedings
of EACL.
K. Knight, 1999. Decoding complexity in word re-
placement translation models. Computational Lin-
guistics.
F. Jelinek, 1969. A fast sequential decoding algorithm
using a stack. IBM Journal of Research and Devel-
opment.
C. Tillman, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in
statistical translation. In Proceedings of ACL.
Hieu Hoang, Philipp Koehn. 2008. Design of the
Moses decoder for statistical machine translation.
In Proceedings of ACL Workshop on Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing.
Govind Kothari, Sumit Negi, Tanveer A. Faruquie,
Venkatesan T. Chakraverthy, L. Venkata Subrama-
niam. 2009. SMS based interface for FAQ retrieval,
In In Proceedings of ACL-IJCNLP
196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Handling Noisy Queries In Cross Language FAQ Retrieval
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
IBM Research India
Vasant Kunj, Institutional Area
New Delhi, India
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
Abstract
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
1 Introduction
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al, 2005), access to Yellow Page
services (Kopparapu et al, 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&A archive (Xue et al,
2008) or the Web (Jijkoun et al, 2005), in a FAQ
database the questions and answers are already pro-
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
87
Figure 1: Sample SMS queries with Hindi FAQs
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al, 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al, 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al, 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al, 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al, 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
?gud? ? > ?god?,?good?,?guide? ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also , naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
2 Problem formulation
Consider an input SMS Se in a source language
e. We view Se as a sequence of n tokens Se =
s1, s2, . . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
88
of questions in the FAQ corpus of a target language
h. Each question Qh ? Qh is also viewed as a se-
quence of tokens. We want to find the question Q?h
from the corpus Qh that best matches the SMS Se.
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure ?(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure ?(th, te, si) > .
We denote this as th ? si.
We define a weight function ?(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpusQh using
the weight function. Consider a question Qh ? Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
Score(Qh) =
n?
i=1
max
th?Qh,te?De & th?si
?(th, te, si)
Our goal is to efficiently find the question Q?h having
the maximum score.
3 Noise removal from queries
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
3.1 Similarity Measure
For a term te ? De and token si of the SMS input
Se, the similarity measure ?(te, si) between them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si)
if te and si share
same starting
character *
0 otherwise
(1)
Where LCSRatio(te, si) =
length(LCS(te,si))
length(te)
and LCS(te, si)
is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al, 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then ?(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token ?gud? whose
most likely correct form is ?good?. The longest
common subsequence for ?good? and ?guided? with
?gud? is ?gd?. Hence the two dictionary terms
?good? and ?guided? have the same LCSRatio of 0.5
w.r.t ?gud?, but the EditDistanceSMS of ?good?
is 1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result the
similarity measure between ?gud? and ?good? will
be higher than that of ?gud? and ?guided?. Higher
the LCSRatio and lower the EditDistanceSMS ,
higher will be the similarity measure. Hence, for
a given SMS token ?byk?, the similarity measure of
word ?bike? is higher than that of ?break?.
4 Cross lingual similarity
Once we have potential candidates which are the
likely disambiguated representations of the noisy
89
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 2: EditDistanceSMS
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
4.1 Statistical Dictionary
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al, 2007). Under IBM model 2 the transla-
tion probability of source language sentence e? =
{t1e, . . . , t
j
e, . . . , t
m
e } and a target language sentence
h? = {t1h, . . . , t
i
h, . . . , t
l
e} is given by
Pr(h?|e?) = ?(l|m)
l?
i=1
m?
j=0
?(tih|t
j
e)a(j|i,m, l).
(2)
Here the word translation model ?(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i,m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, . . . , t
k
h, . . .}
from the target language such that word translation
probability ?(tkh|t
i
e) > ?. We rank these terms ac-
cording to the probability given by the word trans-
lation model ?(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ? D
h.
4.2 Cross lingual similarity measure
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure ?(te, si) > ?. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
?(si, te, th) = ?(te, si).?(th, te) (3)
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
4.3 Cross lingual similarity weight
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
?(th, te, si) as
?(th, te, si) = ?(th, te, si).idf(th) (4)
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token ?bck? if a word translation model produces
a translation output ?wapas? (as in came back) or
?peet? or ?qamar? (as in back pain) then idf will
weigh ?peet? more as it is relatively more discrim-
inative compared to ?wapas? which is used fre-
quently.
5 Pruning and matching
In this section we describe our search algorithm and
the preprocessing needed to find the best question
Q?h for a given SMS query.
5.1 Indexing
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qhth that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
5http://lucene.apache.org/java/docs/
90
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
Tte in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability ?(th|te) > ?, where
th ? Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al, 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al, 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
5.2 List Creation
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, . . . , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (5)
The threshold value ? is determined experimen-
tally. For every te ? Lei we retrieve Tte and then
retrieve the idf scores for every th ? Tte . Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
?(th|te) > 0.1 (6)
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term ?list? im-
plies a sorted list.
For example given a SMS query ?hw mch ds it cst
to stdy in india? as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token ?cst? we get
dictionary words lik ?cost, cast, case, close?. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
5.3 Search algorithm
Given Se containing n tokens, we create n sorted
lists Lh1 , L
h
2 , . . . , L
h
n containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Q?h
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Q?h. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1 , L
h
2 , . . . , L
h
n. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth . The set Qth is added to the candi-
91
Figure 3: List creation
date set C. For each question Q ? Qth , we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question Q?h.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1 [1], L
h
2 [1], . . . , L
h
n[1] from every list. Thus, the
upper bound UB on the score of Q is
Score(Q) ?
n?
i=0
?(Lhi [1]).
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition ?Q? ? UB? serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
Procedure Search Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?h.
Begin
?si, construct Lei for which ?(si, te) > 
// Li lists variants of si
Construct lists Lh1 , L
h
2 , . . . , L
h
n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(L
h
i [1])
t?h = L
h
j? [1]
// t?h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t?h from the list L
h
j? .
Retrieve Qt?h using the index
// Qt?h : the set of all questions in Q
h
//having the term t?h
For each Q ? Qt?h
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(L
h
i [1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 4: Search Algorithm with Pruning
6 Experiments
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al,
92
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
Figure 5: Sample SMS queries
2005) and the rest from the ?out-of-domain? queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
6.1 Translation System
We used the Moses toolkit (Koehn et al, 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold ? de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
6.2 System Evaluation
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
6.2.1 Experiment 1
For Experiment 1 the threshold ? in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
6.2.2 Experiment 2
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it?s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
6.2.3 Experiment 3
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
93
Experiment 1 Experiment 2 Experiment 3
MRR Score 0.41 0.68 0.83
Table 1: MRR Scores
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
Table 2: F1 Measure
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
MRR = 1/|Q|
Q?
i=1
1/ranki (7)
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
6.3 Measuring noise level in SMS queries
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
Cleaned SMS Noisy SMS
English FAQ collection
bigram 16.64 55.19
trigram 9.75 69.41
Table 3: Perplexity for Cleaned and Noisy SMS
94
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. ?hw 2 prvnt ty-
phd? we manually created a clean SMS query ?how
to prevent typhoid?. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
7 Conclusion
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
References
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session .
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
95
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ?98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
96
Proceedings of the ACL 2010 Conference Short Papers, pages 126?131,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatically Generating Term-frequency-induced Taxonomies
Karin Murthy Tanveer A Faruquie L Venkata Subramaniam
K Hima Prasad Mukesh Mohania
IBM Research - India
{karinmur|ftanveer|lvsubram|hkaranam|mkmukesh}@in.ibm.com
Abstract
We propose a novel method to automati-
cally acquire a term-frequency-based tax-
onomy from a corpus using an unsuper-
vised method. A term-frequency-based
taxonomy is useful for application do-
mains where the frequency with which
terms occur on their own and in combi-
nation with other terms imposes a natural
term hierarchy. We highlight an applica-
tion for our approach and demonstrate its
effectiveness and robustness in extracting
knowledge from real-world data.
1 Introduction
Taxonomy deduction is an important task to under-
stand and manage information. However, building
taxonomies manually for specific domains or data
sources is time consuming and expensive. Tech-
niques to automatically deduce a taxonomy in an
unsupervised manner are thus indispensable. Au-
tomatic deduction of taxonomies consist of two
tasks: extracting relevant terms to represent con-
cepts of the taxonomy and discovering relation-
ships between concepts. For unstructured text, the
extraction of relevant terms relies on information
extraction methods (Etzioni et al, 2005).
The relationship extraction task can be classi-
fied into two categories. Approaches in the first
category use lexical-syntactic formulation to de-
fine patterns, either manually (Kozareva et al,
2008) or automatically (Girju et al, 2006), and
apply those patterns to mine instances of the pat-
terns. Though producing accurate results, these
approaches usually have low coverage for many
domains and suffer from the problem of incon-
sistency between terms when connecting the in-
stances as chains to form a taxonomy. The second
category of approaches uses clustering to discover
terms and the relationships between them (Roy
and Subramaniam, 2006), even if those relation-
ships do not explicitly appear in the text. Though
these methods tackle inconsistency by addressing
taxonomy deduction globally, the relationships ex-
tracted are often difficult to interpret by humans.
We show that for certain domains, the frequency
with which terms appear in a corpus on their own
and in conjunction with other terms induces a nat-
ural taxonomy. We formally define the concept
of a term-frequency-based taxonomy and show
its applicability for an example application. We
present an unsupervised method to generate such
a taxonomy from scratch and outline how domain-
specific constraints can easily be integrated into
the generation process. An advantage of the new
method is that it can also be used to extend an ex-
isting taxonomy.
We evaluated our method on a large corpus of
real-life addresses. For addresses from emerging
geographies no standard postal address scheme
exists and our objective was to produce a postal
taxonomy that is useful in standardizing addresses
(Kothari et al, 2010). Specifically, the experi-
ments were designed to investigate the effective-
ness of our approach on noisy terms with lots of
variations. The results show that our method is
able to induce a taxonomy without using any kind
of lexical-semantic patterns.
2 Related Work
One approach for taxonomy deduction is to use
explicit expressions (Iwaska et al, 2000) or lexi-
cal and semantic patterns such as is a (Snow et al,
2004), similar usage (Kozareva et al, 2008), syn-
onyms and antonyms (Lin et al, 2003), purpose
(Cimiano and Wenderoth, 2007), and employed by
(Bunescu and Mooney, 2007) to extract and orga-
nize terms. The quality of extraction is often con-
trolled using statistical measures (Pantel and Pen-
nacchiotti, 2006) and external resources such as
wordnet (Girju et al, 2006). However, there are
126
domains (such as the one introduced in Section
3.2) where the text does not allow the derivation
of linguistic relations.
Supervised methods for taxonomy induction
provide training instances with global seman-
tic information about concepts (Fleischman and
Hovy, 2002) and use bootstrapping to induce new
seeds to extract further patterns (Cimiano et al,
2005). Semi-supervised approaches start with
known terms belonging to a category, construct
context vectors of classified terms, and associate
categories to previously unclassified terms de-
pending on the similarity of their context (Tanev
and Magnini, 2006). However, providing train-
ing data and hand-crafted patterns can be tedious.
Moreover in some domains (such as the one pre-
sented in Section 3.2) it is not possible to construct
a context vector or determine the replacement fit.
Unsupervised methods use clustering of word-
context vectors (Lin, 1998), co-occurrence (Yang
and Callan, 2008), and conjunction features (Cara-
ballo, 1999) to discover implicit relationships.
However, these approaches do not perform well
for small corpora. Also, it is difficult to label the
obtained clusters which poses challenges for eval-
uation. To avoid these problems, incremental clus-
tering approaches have been proposed (Yang and
Callan, 2009). Recently, lexical entailment has
been used where the term is assigned to a cate-
gory if its occurrence in the corpus can be replaced
by the lexicalization of the category (Giuliano and
Gliozzo, 2008). In our method, terms are incre-
mentally added to the taxonomy based on their
support and context.
Association rule mining (Agrawal and Srikant,
1994) discovers interesting relations between
terms, based on the frequency with which terms
appear together. However, the amount of patterns
generated is often huge and constructing a tax-
onomy from all the patterns can be challenging.
In our approach, we employ similar concepts but
make taxonomy construction part of the relation-
ship discovery process.
3 Term-frequency-induced Taxonomies
For some application domains, a taxonomy is in-
duced by the frequency in which terms appear in a
corpus on their own and in combination with other
terms. We first introduce the problem formally and
then motivate it with an example application.
Figure 1: Part of an address taxonomy
3.1 Definition
Let C be a corpus of records r. Each record is
represented as a set of terms t. Let T = {t | t ?
r ? r ? C} be the set of all terms of C. Let f(t)
denote the frequency of term t, that is the number
of records in C that contain t. Let F (t, T+, T?)
denote the frequency of term t given a set of must-
also-appear terms T+ and a set of cannot-also-
appear terms T?. F (t, T+, T?) = | {r ? C |
t ? r? ? t? ? T+ : t? ? r ? ? t? ? T? : t? /? r} |.
A term-frequency-induced taxonomy (TFIT), is
an ordered tree over terms in T . For a node n in
the tree, n.t is the term at n, A(n) the ancestors of
n, and P (n) the predecessors of n.
A TFIT has a root node with the special term ?
and the conditional frequency ?. The following
condition is true for any other node n:
?t ? T, F (n.t, A(n), P (n)) ? F (t, A(n), P (n)).
That is, each node?s term has the highest condi-
tional frequency in the context of the node?s an-
cestors and predecessors. Only terms with a con-
ditional frequency above zero are added to a TFIT.
We show in Section 4 how a TFIT taxonomy
can be automatically induced from a given corpus.
But before that, we show that TFITs are useful in
practice and reflect a natural ordering of terms for
application domains where the concept hierarchy
is expressed through the frequency in which terms
appear.
3.2 Example Domain: Address Data
An address taxonomy is a key enabler for address
standardization. Figure 1 shows part of such an ad-
dress taxonomy where the root contains the most
generic term and leaf-level nodes contain the most
specific terms. For emerging economies building
a standardized address taxonomy is a huge chal-
127
Row Term Part of address Category
1 D-15 house number alphanumerical
2 Rawal building name proper noun
3 Complex building name proper noun
4 Behind landmark marker
5 Hotel landmark marker
6 Ruchira landmark proper noun
7 Katre street proper noun
8 Road street marker
9 Jeevan area proper noun
10 Nagar area marker
11 Andheri city (taluk) proper noun
12 East city (taluk) direction
13 Mumbai district proper noun
14 Maharashtra state proper noun
15 400069 ZIP code 6 digit string
Table 1: Example of a tokenized address
lenge. First, new areas and with it new addresses
constantly emerge. Second, there are very limited
conventions for specifying an address (Faruquie et
al., 2010). However, while many developing coun-
tries do not have a postal taxonomy, there is often
no lack of address data to learn a taxonomy from.
Column 2 of Table 1 shows an example of an
Indian address. Although Indian addresses tend to
follow the general principal that more specific in-
formation is mentioned earlier, there is no fixed or-
der for different elements of an address. For exam-
ple, the ZIP code of an address may be mentioned
before or after the state information and, although
ZIP code information is more specific than city in-
formation, it is generally mentioned later in the
address. Also, while ZIP codes often exist, their
use by people is very limited. Instead, people tend
to mention copious amounts of landmark informa-
tion (see for example rows 4-6 in Table 1).
Taking all this into account, there is often not
enough structure available to automatically infer a
taxonomy purely based on the structural or seman-
tic aspects of an address. However, for address
data, the general-to-specific concept hierarchy is
reflected in the frequency with which terms appear
on their own and together with other terms.
It mostly holds that f(s) > f(d) > f(c) >
f(z) where s is a state name, d is a district name,
c is a city name, and z is a ZIP code. How-
ever, sometimes the name of a large city may be
more frequent than the name of a small state. For
example, in a given corpus, the term ?Houston?
(a populous US city) may appear more frequent
than the term ?Vermont? (a small US state). To
avoid that ?Houston? is picked as a node at the first
level of the taxonomy (which should only contain
states), the conditional-frequency constraint intro-
duced in Section 3.1 is enforced for each node in a
TFIT. ?Houston?s state ?Texas? (which is more fre-
quent) is picked before ?Houston?. After ?Texas? is
picked it appears in the ?cannot-also-appear?? list
for all further siblings on the first level, thus giving
?Houston? has a conditional frequency of zero.
We show in Section 5 that an address taxonomy
can be inferred by generating a TFIT taxonomy.
4 Automatically Generating TFITs
We describe a basic algorithm to generate a TFIT
and then show extensions to adapt to different ap-
plication domains.
4.1 Base Algorithm
Algorithm 1 Algorithm for generating a TFIT.
// For initialization T+, T? are empty
// For initialization l,w are zero
genTFIT(T+, T?, C, l, w)
// select most frequent term
tnext = tj with F (tj , T+, T?) is maximal amongst all
tj ? C;
fnext = F (tnext, T+, T?);
if fnext ? support then
//Output node (tj , l, w)
...
// Generate child node
genTFIT(T+ ? {tnext}, T?, C, l + 1, w)
// Generate sibling node
genTFIT(T+, T? ? {tnext}, C, l, w + 1)
end if
To generate a TFIT taxonomy as defined in Sec-
tion 3.1 we recursively pick the most frequent term
given previously chosen terms. The basic algo-
rithm genTFIT is sketched out in Algorithm 1.
When genTFIT is called the first time, T+ and
T? are empty and both level l and width w are
zero. With each call of genTFIT a new node
n in the taxonomy is created with (t, l, w) where
t is the most frequent term given T+ and T?
and l and w capture the position in the taxonomy.
genTFIT is recursively called to generate a child
of n and a sibling for n.
The only input parameter required by our al-
gorithm is support. Instead of adding all terms
with a conditional frequency above zero, we only
add terms with a conditional frequency equal to or
higher than support. The support parameter con-
trols the precision of the resulting TFIT and also
the runtime of the algorithm. Increasing support
increases the precision but also lowers the recall.
128
4.2 Integrating Constraints
Structural as well as semantic constraints can eas-
ily be integrated into the TFIT generation.
We distinguish between taxonomy-level and
node-level structural constraints. For example,
limiting the depth of the taxonomy by introduc-
ing a maxLevel constraint and checking before
each recursive call if maxLevel is reached, is
a taxonomy-level constraint. A node-level con-
straint applies to each node and affects the way
the frequency of terms is determined.
For our example application, we introduce the
following node-level constraint: at each node we
only count terms that appear at specific positions
in records with respect to the current level of the
node. Specifically, we slide (or incrementally in-
crease) a window over the address records start-
ing from the end. For example, when picking the
term ?Washington? as a state name, occurrences of
?Washington? as city or street name are ignored.
Using a window instead of an exact position ac-
counts for positional variability. Also, to accom-
modate varying amounts of landmark information
we length-normalize the position of terms. That is,
we divide all positions in an address by the average
length of an address (which is 10 for our 40 Mil-
lion addresses). Accordingly, we adjust the size of
the window and use increments of 0.1 for sliding
(or increasing) the window.
In addition to syntactical constraints, semantic
constraints can be integrated by classifying terms
for use when picking the next frequent term. In our
example application, markers tend to appear much
more often than any proper noun. For example,
the term ?Road? appears in almost all addresses,
and might be picked up as the most frequent term
very early in the process. Thus, it is beneficial to
ignore marker terms during taxonomy generation
and adding them as a post-processing step.
4.3 Handling Noise
The approach we propose naturally handles noise
by ignoring it, unless the noise level exceeds the
support threshold. Misspelled terms are generally
infrequent and will as such not become part of
the taxonomy. The same applies to incorrect ad-
dresses. Incomplete addresses partially contribute
to the taxonomy and only cause a problem if the
same information is missing too often. For ex-
ample, if more than support addresses with the
city ?Houston? are missing the state ?Texas?, then
?Houston? may become a node at the first level and
appear to be a state. Generally, such cases only ap-
pear at the far right of the taxonomy.
5 Evaluation
We present an evaluation of our approach for ad-
dress data from an emerging economy. We imple-
mented our algorithm in Java and store the records
in a DB2 database. We rely on the DB2 optimizer
to efficiently retrieve the next frequent term.
5.1 Dataset
The results are based on 40 Million Indian ad-
dresses. Each address record was given to us as
a single string and was first tokenized into a se-
quence of terms as shown in Table 1. In a second
step, we addressed spelling variations. There is no
fixed way of transliterating Indian alphabets to En-
glish and most Indian proper nouns have various
spellings in English. We used tools to detect syn-
onyms with the same context to generate a list of
rules to map terms to a standard form (Lin, 1998).
For example, in Table 1 ?Maharashtra? can also be
spelled ?Maharastra?. We also used a list of key-
words to classify some terms as markers such as
?Road? and ?Nagar? shown in Table 1.
Our evaluation consists of two parts. First, we
show results for constructing a TFIT from scratch.
To evaluate the precision and recall we also re-
trieved post office addresses from India Post1,
cleaned them, and organized them in a tree.
Second, we use our approach to enrich the ex-
isting hierarchy created from post office addresses
with additional area terms. To validate the result,
we also retrieved data about which area names ap-
pear within a ZIP code.2 We also verified whether
Google Maps shows an area on its map.3
5.2 Taxonomy Generation
We generated a taxonomy O using all 40 million
addresses. We compare the terms assigned to
category levels district and taluk4 in O with the
tree P constructed from post office addresses.
Each district and taluk has at least one post office.
Thus P covers all districts and taluks and allows
us to test coverage and precision. We compute the
precision and recall for each category level CL as
1http://www.indiapost.gov.in/Pin/pinsearch.aspx
2http://www.whereincity.com/india/pincode/search
3maps.google.com
4Administrative division in some South-Asian countries.
129
Support Recall % Precision %
100 District 93.9 57.4
Taluk 50.9 60.5
200 District 87.9 64.4
Taluk 49.6 66.1
Table 2: Precision and recall for categorizing
terms belonging to the state Maharashtra
RecallCL = # correct paths from root to CL in O# paths from root to CL in P
PrecisionCL = # correct paths from root to CL in O# paths from root to CL in O
Table 2 shows precision and recall for district
and taluk for the large state Maharashtra. Recall
is good for district. For taluk it is lower because a
major part of the data belongs to urban areas where
taluk information is missing. The precision seems
to be low but it has to be noted that in almost 75%
of the addresses either district or taluk informa-
tion is missing or noisy. Given that, we were able
to recover a significant portion of the knowledge
structure.
We also examined a branch for a smaller state
(Kerala). Again, both districts and taluks appear
at the next level of the taxonomy. For a support
of 200 there are 19 entries in O of which all but
two appear in P as district or taluk. One entry is a
taluk that actually belongs to Maharashtra and one
entry is a name variation of a taluk in P . There
were not enough addresses to get a good coverage
of all districts and taluks.
5.3 Taxonomy Augmentation
We used P and ran our algorithm for each branch
in P to include area information. We focus our
evaluation on the city Mumbai. The recall is low
because many addresses do not mention a ZIP
code or use an incorrect ZIP code. However,
the precision is good implying that our approach
works even in the presence of large amounts of
noise.
Table 3 shows the results for ZIP code 400002
and 400004 for a support of 100. We get simi-
lar results for other ZIP codes. For each detected
area we compared whether the area is also listed
on whereincity.com, part of a post office name
(PO), or shown on google maps. All but four
areas found are confirmed by at least one of the
three external sources. Out of the unconfirmed
terms Fanaswadi and MarineDrive seem to
be genuine area names but we could not confirm
DhakurdwarRoad. The term th is due to our
Area Whereincity PO Google
Bhuleshwar yes no yes
Chira Bazar yes no yes
Dhobi Talao no no yes
Fanaswadi no no no
Kalbadevi Road yes yes yes
Marine Drive no no no
Marine Lines yes yes yes
Princess Street no no yes
th no no no
Thakurdwar Road no no no
Zaveri Bazar yes no yes
Charni Road no yes no
Girgaon yes yes yes
Khadilkar Road yes no yes
Khetwadi Road yes no no
Kumbharwada no no yes
Opera House no yes no
Prathna Samaj yes no no
Table 3: Areas found for ZIP code 400002 (top)
and 400004 (bottom)
tokenization process. 16 correct terms out of 18
terms results in a precision of 89%.
We also ran experiments to measure the cov-
erage of area detection for Mumbai without us-
ing ZIP codes. Initializing our algorithm with
Maharshtra and Mumbai yielded over 100 ar-
eas with a support of 300 and more. However,
again the precision is low because quite a few of
those areas are actually taluk names.
Using a large number of addresses is necessary
to achieve good recall and precision.
6 Conclusion
In this paper, we presented a novel approach to
generate a taxonomy for data where terms ex-
hibit an inherent frequency-based hierarchy. We
showed that term frequency can be used to gener-
ate a meaningful taxonomy from address records.
The presented approach can also be used to extend
an existing taxonomy which is a big advantage
for emerging countries where geographical areas
evolve continuously.
While we have evaluated our approach on ad-
dress data, it is applicable to all data sources where
the inherent hierarchical structure is encoded in
the frequency with which terms appear on their
own and together with other terms. Preliminary
experiments on real-time analyst?s stock market
tips 5 produced a taxonomy of (TV station, An-
alyst, Affiliation) with decent precision and recall.
5See Live Market voices at:
http://money.rediff.com/money/jsp/markets home.jsp
130
References
Rakesh Agrawal and Ramakrishnan Srikant. 1994.
Fast algorithms for mining association rules in large
databases. In Proceedings of the 20th International
Conference on Very Large Data Bases, pages 487?
499.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 576?583.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 120?126.
Philipp Cimiano and Johanna Wenderoth. 2007. Au-
tomatic acquisition of ranked qualia structures from
the web. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 888?895.
Philipp Cimiano, Gu?nter Ladwig, and Steffen Staab.
2005. Gimme? the context: context-driven auto-
matic semantic annotation with c-pankow. In Pro-
ceedings of the 14th International Conference on
World Wide Web, pages 332?341.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134.
Tanveer A. Faruquie, K. Hima Prasad, L. Venkata
Subramaniam, Mukesh K. Mohania, Girish Venkat-
achaliah, Shrinivas Kulkarni, and Pramit Basu.
2010. Data cleansing as a transient service. In
Proceedings of the 26th International Conference on
Data Engineering, pages 1025?1036.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 1?7.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83?135.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 265?272.
Lucja M. Iwaska, Naveen Mata, and Kellyn Kruger.
2000. Fully automatic acquisition of taxonomic
knowledge from large corpora of texts. In Lucja M.
Iwaska and Stuart C. Shapiro, editors, Natural Lan-
guage Processing and Knowledge Representation:
Language for Knowledge and Knowledge for Lan-
guage, pages 335?345.
Govind Kothari, Tanveer A Faruquie, L V Subrama-
niam, K H Prasad, and Mukesh Mohania. 2010.
Transfer of supervision for improved address stan-
dardization. In Proceedings of the 20th Interna-
tional Conference on Pattern Recognition.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1048?1056.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 768?774.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 113?120.
Shourya Roy and L Venkata Subramaniam. 2006. Au-
tomatic generation of domain models for call cen-
ters from noisy transcriptions. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 737?
744.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, pages 1297?1304.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 3?7.
Hui Yang and Jamie Callan. 2008. Learning the dis-
tance metric in a personal ontology. In Proceed-
ing of the 2nd International Workshop on Ontolo-
gies and Information Systems for the Semantic Web,
pages 17?24.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 271?279.
131
