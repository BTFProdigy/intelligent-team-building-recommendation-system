Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 933?939,
Prague, June 2007. c?2007 Association for Computational Linguistics
Single Malt or Blended? A Study in Multilingual Parser Optimization
Johan Hall? Jens Nilsson? Joakim Nivre??
Gu?ls?en Eryig?it? Bea?ta Megyesi? Mattias Nilsson? Markus Saers?
?Va?xjo? University, School of Mathematics and Systems Engineering
E-mail: firstname.lastname@vxu.se
?Uppsala University, Dept. of Linguistics and Philology
E-mail: firstname.lastname@lingfil.uu.se
?Istanbul Technical University, Computer Engineering Dept.
E-mail: gulsen.cebiroglu@itu.edu.tr
Abstract
We describe a two-stage optimization of the
MaltParser system for the ten languages in
the multilingual track of the CoNLL 2007
shared task on dependency parsing. The
first stage consists in tuning a single-parser
system for each language by optimizing pa-
rameters of the parsing algorithm, the fea-
ture model, and the learning algorithm. The
second stage consists in building an ensem-
ble system that combines six different pars-
ing strategies, extrapolating from the opti-
mal parameters settings for each language.
When evaluated on the official test sets, the
ensemble system significantly outperforms
the single-parser system and achieves the
highest average labeled attachment score.
1 Introduction
In the multilingual track of the CoNLL 2007 shared
task on dependency parsing, a single parser must be
trained to handle data from ten different languages:
Arabic (Hajic? et al, 2004), Basque (Aduriz et al,
2003), Catalan, (Mart?? et al, 2007), Chinese (Chen
et al, 2003), Czech (Bo?hmova? et al, 2003), English
(Marcus et al, 1993; Johansson and Nugues, 2007),
Greek (Prokopidis et al, 2005), Hungarian (Csendes
et al, 2005), Italian (Montemagni et al, 2003), and
Turkish (Oflazer et al, 2003).1 Our contribution is
a study in multilingual parser optimization using the
freely available MaltParser system, which performs
1For more information about the task and the data sets, see
Nivre et al (2007).
deterministic, classifier-based parsing with history-
based feature models and discriminative learning,
and which was one of the top performing systems
in the CoNLL 2006 shared task (Nivre et al, 2006).
In order to maximize parsing accuracy, optimiza-
tion has been carried out in two stages, leading to
two different, but related parsers. The first of these is
a single-parser system, similar to the one described
in Nivre et al (2006), which parses a sentence deter-
ministically in a single left-to-right pass, with post-
processing to recover non-projective dependencies,
and where the parameters of the MaltParser system
have been tuned for each language separately. We
call this system Single Malt, to emphasize the fact
that it consists of a single instance of MaltParser.
The second parser is an ensemble system, which
combines the output of six deterministic parsers,
each of which is a variation of the Single Malt parser
with parameter settings extrapolated from the first
stage of optimization. It seems very natural to call
this system Blended.
Section 2 summarizes the work done to optimize
the Single Malt parser, while section 3 explains how
the Blended parser was constructed from the Single
Malt parser. Section 4 gives a brief analysis of the
experimental results, and section 5 concludes.
2 The Single Malt Parser
The parameters available in the MaltParser system
can be divided into three groups: parsing algorithm
parameters, feature model parameters, and learn-
ing algorithm parameters.2 Our overall optimization
2For a complete documentation of these parameters, see
http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
933
strategy for the Single Malt parser was as follows:
1. Define a good baseline system with the same
parameter settings for all languages.
2. Tune parsing algorithm parameters once and
for all for each language (with baseline settings
for feature model and learning algorithm pa-
rameters).
3. Optimize feature model and learning algorithm
parameters in an interleaved fashion for each
language.
We used nine-fold cross-validation on 90% of the
training data for all languages with a training set size
smaller than 300,000 tokens and an 80%?10% train-
devtest split for the remaining languages (Catalan,
Chinese, Czech, English). The remaining 10% of
the data was in both cases saved for a final dry run,
where the parser was trained on 90% of the data for
each language and tested on the remaining (fresh)
10%. We consistently used the labeled attachment
score (LAS) as the single optimization criterion.
Below we describe the most important parameters
in each group, define baseline settings, and report
notable improvements for different languages during
development. The improvements for each language
from step 1 (baseline) to step 2 (parsing algorithm)
and step 3 (feature model and learning algorithm)
can be tracked in table 1.3
2.1 Parsing Algorithm
MaltParser implements several parsing algorithms,
but for the Single Malt system we stick to the one
used by Nivre et al (2006), which performs labeled
projective dependency parsing in linear time, using a
stack to store partially processed tokens and an input
queue of remaining tokens. There are three basic
parameters that can be varied for this algorithm:
1. Arc order: The baseline algorithm is arc-
eager, in the sense that right dependents are
attached to their head as soon as possible, but
there is also an arc-standard version, where the
attachment of right dependents has to be post-
poned until they have found all their own de-
pendents. The arc-standard order was found
3Complete specifications of all parameter settings for all
languages, for both Single Malt and Blended, are available at
http://w3.msi.vxu.se/users/jha/conll07/.
to improve parsing accuracy for Chinese, while
the arc-eager order was maintained for all other
languages.
2. Stack initialization: In the baseline version
the parser is initialized with an artificial root
node (with token id 0) on the stack, so that arcs
originating from the root can be added explic-
itly during parsing. But it is also possible to ini-
tialize the parser with an empty stack, in which
case arcs from the root are only added implic-
itly (to any token that remains a root after pars-
ing is completed). Empty stack initialization
(which reduces the amount of nondeterminism
in parsing) led to improved accuracy for Cata-
lan, Chinese, Hungarian, Italian and Turkish.4
3. Post-processing: The baseline parser performs
a single left-to-right pass over the input, but it
is possible to allow a second pass where only
unattached tokens are processed.5 Such post-
processing was found to improve results for
Basque, Catalan, Czech, Greek and Hungarian.
Since the parsing algorithm only produces projective
dependency graphs, we may use pseudo-projective
parsing to recover non-projective dependencies, i.e.,
projectivize training data and encode information
about these transformations in extended arc labels
to support deprojectivization of the parser output
(Nivre and Nilsson, 2005). Pseudo-projective pars-
ing was found to have a positive effect on over-
all parsing accuracy only for Basque, Czech, Greek
and Turkish. This result can probably be explained
in terms of the frequency of non-projective depen-
dencies in the different languages. For Basque,
Czech, Greek and Turkish, more than 20% of the
sentences have non-projective dependency graphs;
for all the remaining languages the corresponding
4For Arabic, Basque, Czech, and Greek, the lack of im-
provement can be explained by the fact that these data sets allow
more than one label for dependencies from the artificial root.
With empty stack initialization all such dependencies are as-
signed a default label, which leads to a drop in labeled attach-
ment score. For English, however, empty stack initialization did
not improve accuracy despite the fact that dependencies from
the artificial root have a unique label.
5This technique is similar to the one used by Yamada and
Matsumoto (2003), but with only a single post-processing pass
parsing complexity remains linear in string length.
934
Attributes
Tokens FORM LEMMA CPOSTAG POSTAG FEATS DEPREL
S: Top + + + + + +
S: Top?1 +
I: Next + + + + +
I: Next+1 + +
I: Next+2 +
I: Next+3 +
G: Head of Top +
G: Leftmost dependent of Top +
G: Rightmost dependent of Top +
G: Leftmost dependent of Next +
Figure 1: Baseline feature model (S = Stack, I = Input, G = Graph).
figure is 10% or less.6
The cumulative improvement after optimization
of parsing algorithm parameters was a modest 0.32
percentage points on average over all ten languages,
with a minimum of 0.00 (Arabic, English) and a
maximum of 0.83 (Czech) (cf. table 1).
2.2 Feature Model
MaltParser uses a history-based feature model for
predicting the next parsing action. Each feature of
this model is an attribute of a token defined relative
to the current stack S, input queue I, or partially built
dependency graph G, where the attribute can be any
of the symbolic input attributes in the CoNLL for-
mat: FORM, LEMMA, CPOSTAG, POSTAG and
FEATS (split into atomic attributes), as well as the
DEPREL attribute of tokens in the graph G. The
baseline feature model is depicted in figure 1, where
rows denote tokens, columns denote attributes, and
each cell containing a plus sign represents a model
feature.7 This model is an extrapolation from many
previous experiments on different languages and
usually represents a good starting point for further
optimization.
The baseline model was tuned for each of the ten
languages using both forward and backward feature
6In fact, for Arabic, which has about 10% sentences with
non-projective dependencies, it was later found that, with an
optimized feature model, it is beneficial to projectivize the train-
ing data without trying to recover non-projective dependencies
in the parser output. This was also the setting that was used for
Arabic in the dry run and final test.
7The names Top and Next refer to the token on top of the
stack S and the first token in the remaining input I, respectively.
selection. The total number of features in the tuned
models varies from 18 (Turkish) to 56 (Hungarian)
but is typically between 20 and 30. This feature se-
lection process constituted the major development
effort for the Single Malt parser and also gave the
greatest improvements in parsing accuracy, but since
feature selection was to some extent interleaved with
learning algorithm optimization, we only report the
cumulative effect of both together in table 1.
2.3 Learning Algorithm
MaltParser supports several learning algorithms but
the best results have so far been obtained with sup-
port vector machines, using the LIBSVM package
(Chang and Lin, 2001). We use a quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 and LIBSVM?s built-
in one-versus-one strategy for multi-class classifica-
tion, converting symbolic features to numerical ones
using the standard technique of binarization. As our
baseline settings, we used ? = 0.2 and r = 0 for
the kernel parameters, C = 0.5 for the penalty para-
meter, and ? = 1.0 for the termination criterion. In
order to reduce training times during development,
we also split the training data for each language into
smaller sets and trained separate multi-class classi-
fiers for each set, using the POSTAG of Next as the
defining feature for the split.
The time spent on optimizing learning algorithm
parameters varies between languages, mainly due
to lack of time. For Arabic, Basque, and Catalan,
the baseline settings were used also in the dry run
and final test. For Chinese, Greek and Hungarian,
935
Development Dry Run Test Test: UAS
Language Base PA F+L SM B SM B SM B
Arabic 70.31 70.31 71.67 70.93 73.09 74.75 76.52 84.21 85.81
Basque 73.86 74.44 76.99 77.18 80.12 74.97 76.92 80.61 82.84
Catalan 85.43 85.51 86.88 86.65 88.00 87.74 88.70 92.20 93.12
Chinese 83.85 84.39 87.64 87.61 88.61 83.51 84.67 87.60 88.70
Czech 75.00 75.83 77.74 77.91 82.17 77.22 77.98 82.35 83.59
English 85.44 85.44 86.35 86.35 88.74 85.81 88.11 86.77 88.93
Greek 72.67 73.04 74.42 74.89 78.17 74.21 74.65 80.66 81.22
Hungarian 74.62 74.64 77.40 77.81 80.04 78.09 80.27 81.71 83.55
Italian 81.42 81.64 82.50 83.37 85.16 82.48 84.40 86.26 87.77
Turkish 75.12 75.80 76.49 75.87 77.09 79.24 79.79 85.04 85.77
Average 77.78 78.10 79.81 79.86 82.12 79.80 81.20 84.74 86.13
Table 1: Development results for Single Malt (Base = baseline, PA = parsing algorithm, F+L = feature model
and learning algorithm); dry run and test results for Single Malt (SM) and Blended (B) (with corrected test
scores for Blended on Chinese). All scores are labeled attachment scores (LAS) except the last two columns,
which report unlabeled attachment scores (UAS) on the test sets.
slightly better results were obtained by not splitting
the training data into smaller sets; for the remain-
ing languages, accuracy was improved by using the
CPOSTAG of Next as the defining feature for the
split (instead of POSTAG). With respect to the SVM
parameters (?, r, C, and ?), Arabic, Basque, Cata-
lan, Greek and Hungarian retain the baseline set-
tings, while the other languages have slightly dif-
ferent values for some parameters.
The cumulative improvement after optimization
of feature model and learning algorithm parameters
was 1.71 percentage points on average over all ten
languages, with a minimum of 0.69 (Turkish) and a
maximum of 3.25 (Chinese) (cf. table 1).
3 The Blended Parser
The Blended parser is an ensemble system based
on the methodology proposed by Sagae and Lavie
(2006). Given the output dependency graphs Gi
(1 ? i ? m) of m different parsers for an input sen-
tence x, we construct a new graph containing all the
labeled dependency arcs proposed by some parser
and weight each arc a by a score s(a) reflecting its
popularity among the m parsers. The output of the
ensemble system for x is the maximum spanning
tree of this graph (rooted at the node 0), which can
be extracted using the Chu-Liu-Edmonds algorithm,
as shown by McDonald et al (2005). Following
Sagae and Lavie (2006), we let s(a) =
?m
i=1 w
c
iai,
where wci is the average labeled attachment score of
parser i for the word class c8 of the dependent of a,
and ai is 1 if a ? Gi and 0 otherwise.
The Blended parser uses six component parsers,
with three different parsing algorithms, each of
which is used to construct one left-to-right parser
and one right-to-left parser. The parsing algorithms
used are the arc-eager baseline algorithm, the arc-
standard variant of the baseline algorithm, and the
incremental, non-projective parsing algorithm first
described by Covington (2001) and recently used
for deterministic classifier-based parsing by Nivre
(2007), all of which are available in MaltParser.
Thus, the six component parsers for each language
were instances of the following:
1. Arc-eager projective left-to-right
2. Arc-eager projective right-to-left
3. Arc-standard projective left-to-right
4. Arc-standard projective right-to-left
5. Covington non-projective left-to-right
6. Covington non-projective right-to-left
8We use CPOSTAG to determine the part of speech.
936
root 1 2 3?6 7+
Parser R P R P R P R P R P
Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06
Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78.14
Table 2: Recall (R) and precision (P) of Single Malt and Blended for dependencies of different length,
averaged over all languages (root = dependents of root node, regardless of length).
The final Blended parser was constructed by reusing
the tuned Single Malt parser for each language (arc-
standard left-to-right for Chinese, arc-eager left-to-
right for the remaining languages) and training five
additional parsers with the same parameter settings
except for the following mechanical adjustments:
1. Pseudo-projective parsing was not used for the
two non-projective parsers.
2. Feature models were adjusted with respect to
the most obvious differences in parsing strategy
(e.g., by deleting features that could never be
informative for a given parser).
3. Learning algorithm parameters were adjusted
to speed up training (e.g., by always splitting
the training data into smaller sets).
Having trained all parsers on 90% of the training
data for each language, the weights wci for each
parser i and coarse part of speech c was determined
by the labeled attachment score on the remaining
10% of the data. This means that the results obtained
in the dry run were bound to be overly optimistic for
the Blended parser, since it was then evaluated on
the same data set that was used to tune the weights.
Finally, we want to emphasize that the time for
developing the Blended parser was severely limited,
which means that several shortcuts had to be taken,
such as optimizing learning algorithm parameters
for speed rather than accuracy and using extrapo-
lation, rather than proper tuning, for other impor-
tant parameters. This probably means that the per-
formance of the Blended system can be improved
considerably by optimizing parameters for all six
parsers separately.
4 Results and Discussion
Table 1 shows the labeled attachment score results
from our internal dry run (training on 90% of the
training data, testing on the remaining 10%) and the
official test runs for both of our systems. It should
be pointed out that the test score for the Blended
parser on Chinese is different from the official one
(75.82), which was much lower than expected due
to a corrupted specification file required by Malt-
Parser. Restoring this file and rerunning the parser
on the Chinese test set, without retraining the parser
or changing any parameter settings, resulted in the
score reported here. This also improved the aver-
age score from 80.32 to 81.20, the former being the
highest reported official score.
For the Single Malt parser, the test results are on
average very close to the dry run results, indicating
that models have not been overfitted (although there
is considerably variation between languages). For
the Blended parser, there is a drop of almost one
percentage point, which can be explained by the fact
that weights could not be tuned on held-out data for
the dry run (as explained in section 3).
Comparing the results for different languages, we
see a tendency that languages with rich morphology,
usually accompanied by flexible word order, get
lower scores. Thus, the labeled attachment score is
below 80% for Arabic, Basque, Czech, Greek, Hun-
garian, and Turkish. By comparison, the more con-
figurational languages (Catalan, Chinese, English,
and Italian) all have scores above 80%. Linguis-
tic properties thus seem to be more important than,
for example, training set size, which can be seen by
comparing the results for Italian, with one of the
smallest training sets, and Czech, with one of the
largest. The development of parsing methods that
are better suited for morphologically rich languages
with flexible word order appears as one of the most
important goals for future research in this area.
Comparing the results of our two systems, we
see that the Blended parser outperforms the Single
Malt parser for all languages, with an average im-
937
provement of 1.40 percentage points, a minimum of
0.44 (Greek) and a maximum of 2.40 (English). As
shown by McDonald and Nivre (2007), the Single
Malt parser tends to suffer from two problems: error
propagation due to the deterministic parsing strat-
egy, typically affecting long dependencies more than
short ones, and low precision on dependencies orig-
inating in the artificial root node due to fragmented
parses.9 The question is which of these problems is
alleviated by the multiple views given by the compo-
nent parsers in the Blended system. Table 2 throws
some light on this by giving the precision and re-
call for dependencies of different length, treating de-
pendents of the artificial root node as a special case.
As expected, the Single Malt parser has lower preci-
sion than recall for root dependents, but the Blended
parser has even lower precision (and somewhat bet-
ter recall), indicating that the fragmentation is even
more severe in this case.10 By contrast, we see that
precision and recall for other dependencies improve
across the board, especially for longer dependencies,
which probably means that the effect of error propa-
gation is mitigated by the use of an ensemble system,
even if each of the component parsers is determinis-
tic in itself.
5 Conclusion
We have shown that deterministic, classifier-based
dependency parsing, with careful optimization, can
give highly accurate dependency parsing for a wide
range of languages, as illustrated by the performance
of the Single Malt parser. We have also demon-
strated that an ensemble of deterministic, classifier-
based dependency parsers, built on top of a tuned
single-parser system, can give even higher accuracy,
as shown by the results of the Blended parser, which
has the highest labeled attachment score for five lan-
guages (Arabic, Basque, Catalan, Hungarian, and
9A fragmented parse is a dependency forest, rather than a
tree, and is automatically converted to a tree by attaching all
(other) roots to the artificial root node. Hence, children of the
root node in the final output may not have been predicted as
such by the treebank-induced classifier.
10This conclusion is further supported by the observation
that the single most frequent ?frame confusion? of the Blended
parser, over all languages, is to attach two dependents with the
label ROOT to the root node, instead of only one. The frequency
of this error is more than twice as high for the Blended parser
(180) as for the Single Malt parser (83).
Italian), as well as the highest multilingual average
score.
Acknowledgements
We want to thank all treebank providers for making
the data available for the shared task and the (other)
organizers for their efforts in organizing it. Special
thanks to Ryan McDonald, for fruitful discussions
and assistance with the error analysis, and to Kenji
Sagae, for showing us how to produce a good blend.
Thanks also to two reviewers for useful comments.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf., pages 95?102.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
938
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
99?106.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2007. Incremental non-projective dependency
parsing. In Human Language Technologies: The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT),
pages 396?403.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 129?132.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
939
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 93?101,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Where to Look: Modeling Eye Movements in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
We propose a novel machine learning task that
consists in learning to predict which words in
a text are fixated by a reader. In a first pilot
experiment, we show that it is possible to out-
perform a majority baseline using a transition-
based model with a logistic regression classi-
fier and a very limited set of features. We also
show that the model is capable of capturing
frequency effects on eye movements observed
in human readers.
1 Introduction
Any person engaged in normal skilled reading pro-
duces an alternating series of rapid eye movements
and brief fixations that forms a rich and detailed be-
havioral record of the reading process. In the last
few decades a great deal of experimental evidence
has accumulated to suggest that the eye movements
of readers are reflective of ongoing language pro-
cessing and thus provide a useful source of infor-
mation for making inferences about the linguistic
processes involved in reading (Clifton et al, 2007).
In psycholinguistic research, eye movement data is
now commonly used to study how experimental ma-
nipulations of linguistic stimuli manifest themselves
in the eye movement record.
Another related strand of research primarily at-
tempts to understand what determines when and
where the eyes move during reading. This line of
research has led to mathematically well specified ac-
counts of eye movement control in reading being
instantiated as computational models (Legge et al,
1997; Reichle et al, 1998; Salvucci, 2001; Engbert
et al, 2002; McDonald et al, 2005; Feng, 2006;
Reilly and Radach, 2006; Yang, 2006). (For a re-
cent overview, see (Reichle, 2006).) These models
receive text as input and produce predictions for the
location and duration of eye fixations, in approxima-
tion to human reading behavior. Although there are
substantial differences between the various models,
they typically combine both mechanisms of visuo-
motor control and linguistic processing. Two impor-
tant points of divergence concern the extent to which
language processing influences eye movements and
whether readers process information from more than
one word at a time (Starr and Rayner, 2001). More
generally, the models that have emerged to date are
based on different sets of assumptions about the un-
derlying perceptual and cognitive mechanisms that
control eye movements. The most influential model
so far, the E-Z Reader model (Reichle et al, 1998;
Reichle et al, 2003; Pollatsek et al, 2006), rests on
the assumptions that cognitive / lexical processing is
the engine that drives the eyes through the text and
that words are identified serially, one at a time.
Although eye movement models typically have
parameters that are fitted to empirical data sets, they
are not based on machine learning in the standard
sense and their predictions are hardly ever tested on
unseen data. Moreover, their predictions are nor-
mally averaged over a whole group of readers or
words belonging to a given frequency class. In this
study, however, we investigate whether saccadic eye
movements during reading can be modeled using
machine learning. The task we propose is to learn
to predict the eye movements of an individual reader
reading a specific text, using as training data the eye
93
movements recorded for the same person reading
other texts.
Predicting the eye movements of an individual
reader on new texts is arguably a hard problem, and
we therefore restrict the task to predicting word-
based fixations (but not the duration of these fixa-
tions) and focus on a first pilot experiment inves-
tigating whether we can outperform a reasonable
baseline on this task. More precisely, we present ex-
perimental results for a transition-based model, us-
ing a log-linear classifier, and show that the model
significantly outperforms the baseline of always pre-
dicting the most frequent saccade. In addition, we
show that even this simple model is able to capture
frequency effects on eye movements observed in hu-
man readers.
We want to emphasize that the motivation for this
modeling experiment is not to advance the state of
the art in computational modeling of eye movements
during reading. For this our model is far too crude
and limited in scope. The goal is rather to propose a
novel approach to the construction and evaluation of
such models, based on machine learning and model
assessment on unseen data. In doing this, we want
to establish a reasonable baseline for future research
by evaluating a simple model with a restricted set
of features. In future studies, we intend to inves-
tigate how results can be improved by introducing
more complex models as well as a richer feature
space. More generally, the machine learning ap-
proach explored here places emphasis on modeling
eye movement behavior with few a priori assump-
tions about underlying cognitive and physiological
mechanisms.
The rest of the paper is structured as follows. Sec-
tion 2 provides a brief background on basic charac-
teristics of eye movements in reading. The emphasis
is on saccadic eye movements rather than on tempo-
ral aspects of fixations. Section 3 defines the novel
task of learning to predict fixations during reading
and discusses different evaluation metrics for this
task. Section 4 presents a transition-based model
for solving this task, using a log-linear classifier to
predict the most probable transition after each fixa-
tion. Section 5 presents experimental results for the
model using data from the Dundee corpus (Kennedy
and Pynte, 2005), and Section 6 contains conclu-
sions and suggestions for future research.
2 Eye Movements in Reading
Perhaps contrary to intuition, the eyes of readers do
not move smoothly across a line or page of text. It is
a salient fact in reading research that the eyes make
a series of very rapid ballistic movements (called
saccades) from one location to another. In between
saccades, the eyes remain relatively stationary for
brief periods of time (fixations). Most fixations last
about 200-300 ms but there is considerable variabil-
ity, both between and within readers. Thus, some
fixations last under 100 ms while others last over
500 ms (Rayner, 1998). Much of the variability in
fixation durations appears associated to processing
ease or difficulty.
The number of characters that is within the re-
gion of effective vision on any fixation is known as
the perceptual span. For English readers, the per-
ceptual span extends approximately four characters
to the left and fifteen characters to the right of the
fixation. Although readers fixate most words in a
text, many words are also skipped. Approximately
85% of the content words are fixated and 35% of
the function words (Carpenter and Just, 1983). Vari-
ables known to influence the likelihood of skipping
a word are word length, frequency and predictabil-
ity. Thus, more frequent words in the language are
skipped more often than less frequent words. This is
true also when word length is controlled for. Simi-
larly, words that occur in constrained contexts (and
are thus more predictable) are skipped more often
than words in less constrained contexts.
Although the majority of saccades in reading is
relatively local, i.e., target nearby words, more dis-
tant saccades also occur. Most saccades move the
eyes forward approximately 7?9 character spaces.
Approximately 15% of the saccades, however, are
regressions, in which the eyes move back to earlier
parts of the text (Rayner, 1998). It has long been
established that the length of saccades is influenced
by both the length of the fixated word and the word
to the right of the fixation (O?Regan, 1979). Re-
gressions often go back one or two words, but occa-
sionally they stretch further back. Such backward
movements are often thought to reflect linguistic
processing difficulty, e.g., because of syntactic pars-
ing problems. Readers, however, are often unaware
of making regressions, especially shorter ones.
94
3 The Learning Task
We define a text T as a sequence of word tokens
(w1, . . . , wn), and we define a fixation sequence
F for T as a sequence of token positions in T
(i1, . . . , im) (1 < ik < n). The fixation set S(F )
corresponding to F is the set of token positions that
occur in F . For example, the text Mary had a lit-
tle lamb is represented by T = (Mary, had, a, little,
lamb); a reading of this text where the sequence of
fixations is Mary ? little ? Mary ? lamb is repre-
sented by F = (1, 4, 1, 5); and the corresponding
fixation set is S(F ) = {1, 4, 5}.
The task we now want to consider is the one
of predicting the fixation sequence F for a spe-
cific reading event E involving person P reading
text T . The training data consist of fixation se-
quences F1, . . . , Fk for reading events distinct from
E involving the same person P but different texts
T1, . . . , Tk. The performance of a model M is eval-
uated by comparing the predicted fixation sequence
FM to the fixation sequence FO observed in a read-
ing experiment involving P and T . Here are some
of the conceivable metrics for this evaluation:
1. Fixation sequence similarity: How similar
are the sequences FM and FO, as measured, for
example, by some string similarity metric?
2. Fixation accuracy: How large is the agree-
ment between the sets S(FM ) and S(FO), as
measured by 0-1-loss over the entire text, i.e.,
how large is the proportion of positions that are
either in both S(FM ) and S(FO) (fixated to-
kens) or in neither (skipped tokens). This can
also be broken down into precision and recall
for fixated and skipped tokens, respectively.
3. Fixation distributions: Does the model pre-
dict the correct proportion of fixated and
skipped tokens, as measured by the difference
between |S(FM )|/|T | and |S(FO)|/|T |? This
can also be broken down by frequency classes
of words, to see if the model captures frequency
effects reported in the literature.
These evaluation metrics are ordered by an implica-
tional scale from hardest to easiest. Thus, a model
that correctly predicts the exact fixation sequence
also makes correct predictions with respect to the
set of words fixated and the number of words fixated
(but not vice versa). In the same fashion, a model
that correctly predicts which words are fixated (but
not the exact sequence) also correctly predicts the
number of words fixated.
In the experiments reported in Section 5, we will
use variants of the latter two metrics and compare
the performance of our model to the baseline of al-
ways predicting the most frequent type of saccade
for the reader in question. We will report results
both for individual readers and mean scores over all
readers in the test set. The evaluation of fixation se-
quence similarity (the first type of metric) will be
left for future work.
4 A Transition-Based Model
When exploring a new task, we first have to decide
what kind of model to use. As stated in the introduc-
tion, we regard this as a pilot experiment to establish
the feasibility of the task and have therefore chosen
to start with one of the simplest models possible and
see whether we can beat the baseline of always pre-
dicting the most frequent saccade. Since the task
consists in predicting a sequence of different actions,
it is very natural to use a transition-based model,
with configurations representing fixation states and
transitions representing saccadic movements. Given
such a system, we can train a classifier to predict the
next transition given the information in the current
configuration. In order to derive a complete tran-
sition sequence, we start in an initial configuration,
representing the reader?s state before the first fixa-
tion, and repeatedly apply the transition predicted by
the classifier until we reach a terminal state, repre-
senting the reader?s state after having read the entire
text. At an abstract level, this is essentially the same
idea as in transition-based dependency parsing (Ya-
mada and Matsumoto, 2003; Nivre, 2006; Attardi,
2006). In the following subsections, we discuss the
different components of the model in turn, including
the transition system, the classifier used, the features
used to represent data, and the search algorithm used
to derive complete transition sequences.
4.1 Transition System
A transition system is an abstract machine consist-
ing of a set of configurations and transitions between
95
configurations. A configuration in the current sys-
tem is a triple C = (L,R, F ), where
1. L is a list of tokens representing the left con-
text, including the currently fixated token and
all preceding tokens in the text.
2. R is a list of tokens representing the right con-
text, including all tokens following the cur-
rently fixated token in the text.
3. F is a list of token positions, representing the
fixation sequence so far, including the currently
fixated token.
For example, if the text to be read is Mary had a
little lamb, then the configuration
([Mary,had,a,little], [lamb], [1,4])
represents the state of a reader fixating the word little
after first having fixated the word Mary.
For any text T = w1 . . . wn, we define initial and
terminal configurations as follows:
1. Initial: C = ([ ], [w1, . . . , wn], [ ])
2. Terminal: C = ([w1, . . . , wn], [ ], F )
(for any F )
We then define the following transitions:1
1. Progress(n):
([?|wi], [wi+1, . . . , wi+n|?], [?|i])?
([?|wi, wi+1, . . . , wi+n], ?, [?|i, i+n])
2. Regress(n):
([?|wi?n, . . . , wi?1, wi], ?, [?|i])?
([?|wi?n], [wi?n+1, . . . , wi|?], [?|i, i?n])
3. Refixate:
([?|wi], ?, [?|i])? ([?|wi], ?, [?|i, i])
The transition Progress(n) models progressive sac-
cades of length n, which means that the next fixated
word is n positions forward with respect to the cur-
rently fixated word (i.e., n?1 words are skipped).
In a similar fashion, the transition Regress(n) mod-
els regressive saccades of length n. If the parameter
1We use the variables ?, ? and ? for arbitrary sublists of L,
R and F , respectively, and we write the L and F lists with their
tails to the right, to maintain the natural order of words.
n of either Progress(n) or Regress(n) is greater than
the number of words remaining in the relevant di-
rection, then the longest possible movement is made
instead, in which case Regress(n) leads to a terminal
configuration while Progress(n) leads to a configu-
ration that is similar to the initial configuration in
that it has an empty L list. The transition Refixate,
finally, models refixations, that is, cases where the
next word fixated is the same as the current.
To illustrate how this system works, we may con-
sider the transition sequence corresponding to the
reading of the text Mary had a little lamb used as
an example in Section 3:2
Init ? ([ ], [Mary, . . . , lamb], [ ])
P(1) ? ([Mary], [had, . . . , lamb], [1])
P(3) ? ([Mary, . . . , little], [lamb], [1,4])
R(3) ? ([Mary], [had, . . . , lamb], [1,4,1])
P(4) ? ([Mary, . . . , lamb], [ ], [1,4,1,5])
4.2 Learning Transitions
The transition system defined in the previous section
specifies the set of possible saccade transitions that
can be executed during the reading of a text, but it
does not say anything about the probability of dif-
ferent transitions in a given configuration, nor does
it guarantee that a terminal configuration will ever
be reached. The question is now whether we can
learn to predict the most probable transition in such
a way that the generated transition sequences model
the behavior of a given reader. To do this we need
to train a classifier that predicts the next transition
for any configuration, using as training data the ob-
served fixation sequences of a given reader. Before
that, however, we need to decide on a feature repre-
sentation for configurations.
Features used in this study are listed in Table 1.
We use the notation L[i] to refer to the ith token
in the list L and similarly for R and F . The first
two features refer to properties of the currently fix-
ated token. Length is simply the character length
of the word, while frequency class is an index of
the word?s frequency of occurrence in representative
text. Word frequencies are based on occurrences in
the Bristish National Corpus (BNC) and divided into
2We abbreviate Progress(n) and Regress(n) to P(n) and
R(n), respectively.
96
Feature Description
CURRENT.LENGTH The length of the token L[1]
CURRENT.FREQUENCYCLASS The frequency class of the token L[1]
NEXT.LENGTH The length of the token R[1]
NEXT.FREQUENCYCLASS The frequency class of the token R[1]
NEXTPLUSONE.LENGTH The length of the token R[2]
NEXTPLUSTWO.LENGTH The length of the token R[3]
DISTANCE.ONETOTWO The distance, in tokens, between F [1] and F [2]
DISTANCE.TWOTOTHREE The distance, in tokens, between F [2] and F [3]
Table 1: Features defined over fixation configurations. The notation L[i] is used to denote the ith element of list L.
five classes. Frequencies were computed per million
words in the ranges 1?10, 11?100, 101?1000, 1001?
10000, and more than 10000.
The next four features define features of tokens
to the right of the current fixation. For the to-
ken immediately to the right, both length and fre-
quency are recorded whereas only length is con-
sidered for the two following tokens. The last
two features are defined over tokens in the fixa-
tion sequence built thus far and record the history
of the two most recent saccade actions. The first
of these (DISTANCE.ONETOTWO) defines the sac-
cade distance, in number of tokens, that led up
to the token currently being fixated. The second
(DISTANCE.TWOTOTHREE), defines the next most
recent saccade distance, that led up to the previous
fixation. For these two features the following holds.
If the distance is positive, the saccade is progressive,
if the distance is negative, the saccade is regressive,
and if the distance amounts to zero, the saccade is a
refixation.
The small set of features used in the current model
were chosen to reflect experimental evidence on eye
movements in reading. Thus, for example, as noted
in section 2, it is a well-documented fact that short,
frequent and predictable words tend to be skipped.
The last two features are included in the hope of
capturing some of the dynamics in eye movement
behavior, for example, if regressions are more likely
to occur after longer progressive saccades, or if the
next word is skipped more often if the current word
is refixated. Still, it is clear that this is only a tiny
subset of the feature space that might be considered,
and it remains an important topic for future research
to further explore this space and to study the impact
of different features.
Given our feature representation, and given some
training data derived from reading experiments, it
is straightforward to train a classifier for predicting
the most probable transition out of any configura-
tion. There are many learning algorithms that could
be used for this purpose, but in the pilot experiments
we only make use of logistic regression.
4.3 Search Algorithm
Once we have trained a classifier f that predicts the
next transition f(C) out of any configuration C, we
can simulate the eye movement behavior of a person
reading the text T = (w1, . . . , wn) using the follow-
ing simple search algorithm:
1. Initialize C to ([ ], [w1, . . . , wn], [ ]).
2. While C is not terminal, apply f(C) to C.
3. Return F of C.
It is worth noting that search will always terminate
once a terminal configuration has been reached, even
though there is nothing in the transition system that
forbids transitions out of terminal configurations. In
other words, while the model itself allows regres-
sions and refixations after the last word of the text
has been fixated, the search algorithm does not. This
seems like a reasonable approximation for this pilot
study.
5 Experiments
5.1 Experimental Setup
The experiments we report are based on data from
the English section of the Dundee corpus. This sec-
97
Fixation Accuracy Fixations Skips
Reader # sentences Baseline Model Prec Rec F1 Prec Rec F1
a 136 53.3 70.0 69.9 73.8 71.8 69.0 65.8 67.4
b 156 55.7 66.5 65.2 85.8 74.1 70.3 80.4 75.0
c 151 59.9 70.9 72.5 82.8 77.3 67.4 53.1 59.4
d 162 69.0 78.9 84.7 84.8 84.7 66.0 65.8 65.9
e 182 51.7 71.8 69.1 78.4 73.5 75.3 65.2 69.9
f 157 63.5 67.9 70.9 83.7 76.8 58.7 40.2 47.7
g 129 43.3 56.6 49.9 80.8 61.7 72.2 38.1 49.9
h 143 57.6 66.9 69.4 76.3 72.7 62.8 54.3 58.2
i 196 56.4 69.1 69.6 80.3 74.6 68.2 54.7 60.7
j 166 66.1 76.3 82.2 81.9 82.0 65.0 65.4 65.2
Average 157.8 57.7 69.5 70.3 80.9 75.2 67.5 58.3 62.6
Table 2: Fixation and skipping accuracy on test data; Prec = precision, Rec = recall, F1 = balanced F measure.
tion contains the eye tracking record of ten partici-
pants reading editorial texts from The Independent
newspaper. The corpus contains 20 texts, each of
which were read by all participants. Participants also
answered a set of multiple-choice comprehension
questions after having finished reading each text.
The corpus consists of 2379 sentences, 56212 tokens
and 9776 types. The data was recorded using a Dr.
Bouis Oculometer Eyetracker, sampling the position
of the right eye every millisecond (see Kennedy and
Pynte, 2005, for further details).
For the experiments reported here, the corpus was
divided into three data sets: texts 1-16 for training
(1911 sentences), texts 17-18 for development and
validation (237 sentences), and the last two texts 19-
20 for testing (231 sentences).
Since we want to learn to predict the observed
saccade transition for any fixation configuration,
where configurations are represented as feature vec-
tors, it is not possible to use the eye tracking data
directly as training and test data. Instead, we simu-
late the search algorithm on the corpus data of each
reader in order to derive, for each sentence, the fea-
ture vectors over the configurations and the tran-
sitions corresponding to the observed fixation se-
quence. The instances to be classified then consist of
feature representations of configurations while the
classes are the possible transitions.
To somewhat simplify the learning task in this
first study, we removed all instances of non-local
saccades prior to training. Progressions stretching
further than five words ahead of the current fixation
were removed, as were regressions stretching further
back than two words. Refixations were not removed.
Thus we reduced the number of prediction classes to
eight. Removal of the non-local saccade instances
resulted in a 1.72% loss over the total number of in-
stances in the training data for all readers.
We trained one classifier for each reader using lo-
gistic regression, as implemented in Weka (Witten
and Eibe, 2005) and default options. In addition, we
trained majority baseline classifiers for all readers.
These models always predict the most frequent sac-
cadic eye movement for a given reader.
The classifiers were evaluated with respect to the
accuracy achieved when reading previously unseen
text using the search algorithm in 4.3. To ensure
that test data were consistent with training data, sen-
tences including any saccade outside of the local
range were removed prior to test. This resulted
in removal of 18.9% of the total number of sen-
tences in the test data for all readers. Accuracy was
measured in three different ways. First, we com-
puted the fixation accuracy, that is, the proportion
of words that were correctly fixated or skipped by
the model, which we also broke down into precision
and recall for fixations and skips separately.3 Sec-
ondly, we compared the predicted fixation distribu-
3Fixation/skip precision is the proportion of tokens fix-
ated/skipped by the model that were also fixated/skipped by
the reader; fixation/skip recall is the proportion of tokens fix-
ated/skipped by the reader that were also fixated/skipped by the
model.
98
tions to the observed fixation distributions, both over
all words and broken down into the same five fre-
quency classes that were used as features (see Sec-
tion 4). The latter statistics, averaged over all read-
ers, allow us to see whether the model correctly pre-
dicts the frequency effect discussed in section 2.
5.2 Results and Discussion
Table 2 shows the fixation accuracy, and precision,
recall and F1 for fixations and skips, for each of the
ten different models and the average across all mod-
els (bottom row). Fixation accuracy is compared to
the baseline of always predicting the most frequent
saccade type (Progress(2) for readers a and e, and
Progress(1) for the rest).
If we consider the fixation accuracy, we see that
all models improve substantially on the baseline
models. The mean difference between models and
baselines is highly significant (p < .001, paired t-
test). The relative improvement ranges from 4.4 per-
centage points in the worst case (model of reader f )
to 20.1 percentage points in the best case (model of
reader e). The highest scoring model, the model of
reader d, has an accuracy of 78.9%. The lowest scor-
ing model, the model of reader g, has an accuracy
of 56.6%. This is also the reader for whom there
is the smallest number of sentences in the test data
(129), which means that a large number of sentences
were removed prior to testing because of the greater
number of non-local saccades made by this reader.
Thus, this reader has an unusually varied saccadic
behaviour which is particularly hard to model.
Comparing the precision and recall for fixation
and skips, we see that while precision tends to be
about the same for both categories (with a few no-
table exceptions), recall is consistently higher for
fixations than for skips. We believe that this is due
to a tendency of the model to overpredict fixations,
especially for low-frequency words. This has a great
impact on the F1 measure (unweighted harmonic
mean of precision and recall), which is considerably
higher for fixations than for skips.
Figure 1 shows the distributions of fixations
grouped by reader and model. The models appear
reasonably good at adapting to the empirical fixa-
tion distribution of individual readers. However, the
models typically tend to look at more words than the
readers, as noted above. This suggests that the mod-
els lack sufficient information to learn to skip words
more often. This might be overcome by introducing
features that further encourage skipping of words. In
addition to word length and word frequency, that are
already accounted for, n-gram probability could be
included as a measure of predictability, for example.
We also note that there is a strong linear relation
between the capability of fitting the empirical dis-
tribution well and achieving high fixation accuracy
(Pearson?s r: -0.91, as measured by taking the dif-
ferences of each pair of distributions and correlating
them with the fixation accuracy of the models).
Figure 2 shows the mean observed and predicted
fixation and skipping probability as a function of
word frequency class, averaged over all readers. As
seen here, model prediction is responsive to fre-
quency class in a fashion comparable to the read-
ers, although the predictions typically tend to exag-
gerate the observed frequency effect. In the lower
to medium classes (1?3), almost every word is fix-
ated. Then there is a clear drop in fixation proba-
bility for words in frequency class 4 which fits well
with the observed fixation probability. Finally there
is another drop in fixation probability for the most
frequent words (5). The skipping probabilities for
the different classes show the corresponding reverse
trend.
6 Conclusion
In this paper we have defined a new machine learn-
ing task where the goal is to learn the saccadic eye
movement behavior of individual readers in order
to predict the sequence of word fixations for novel
reading events. We have discussed different evalua-
tion metrics for this task, and we have established a
first benchmark by training and evaluating a simple
transition-based model using a log-linear classifier
to predict the next transition. The evaluation shows
that even this simple model, with features limited to
a few relevant properties in a small context window,
outperforms a majority baseline and captures some
of the word frequency effects on eye movements ob-
served in human readers.
This pilot study opens up a number of direc-
tions for future research. With respect to mod-
eling, we need to explore more complex models,
richer feature spaces, and alternative learning algo-
99
a b c d e f g h i j
ReaderModel
Pro
por
tion
0.0
0.2
0.4
0.6
0.8
Figure 1: Proportion of fixated tokens grouped by reader and model
F F
F
F
F
1 2 3 4 5
0.0
0.2
0.4
0.6
0.8
1.0
F F F
F
F
S S
S
S
S
S S
S
S
S
Fix
atio
n p
rob
abi
lity
Frequency class
F
F
S
S
Fixation ? Observed
Fixation ? Predicted
Skipping ? Observed
Skipping ? Predicted
Figure 2: Mean observed and predicted fixation and skipping probability for five frequency classes of words
rithms. For example, given the sequential nature
of the task, it seems natural to explore probabilistic
sequence models such as HMMs (see for example
Feng (2006)). With respect to evaluation, we need
to develop metrics that are sensitive to the sequential
behavior of models, such as the fixation sequence
similarity measure discussed in Section 3, and in-
vestigate to what extent results can be generalized
across readers. With respect to the task itself, we
need to introduce additional aspects of the reading
process, in particular the duration of fixations. By
pursuing these lines of research, we should be able
to gain a better understanding of how machine learn-
ing methods in eye movement modeling can inform
and advance current theories and models in reading
and psycholinguistic research.
100
References
Giuseppe Attardi. 2006. Experiments with a multilan-
guage non-projective dependency parser. In Proceed-
ings of the 10th Conference on Computational Natural
Language Learning (CoNLL), pages 166?170.
Patricia A. Carpenter and Marcel A. Just. 1983. What
your eyes do while your mind is reading. In Keith
Rayner, editor, Eye movements in reading: Perceptual
and language processes, pages 275?307. New York:
Academic Press.
Charles Clifton, Adrian Staub, and Keith Rayner. 2007.
Eye movements in reading words and sentences. In
Roger van Gompel, editor, Eye movements: A window
on mind and brain, pages 341?372. Amsterdam: Else-
vier.
Ralf Engbert, Andr? Longtin, and Reinhold Kliegl. 2002.
A dynamical model of saccade generation in reading
based on spatially distributed lexical processing. Vi-
sion Research, 42:621?636.
Gary Feng. 2006. Eye movements as time-series random
variables: A stochastic model of eye movement con-
trol in reading. Cognitive Systems Research, 7:70?95.
Alan Kennedy and Jo?l Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision research,
45:153?168.
Gordon E. Legge, Timothy S. Klitz, and Bosco S. Tjan.
1997. Mr. Chips: An ideal-observer model of reading.
Psychological Review, 104:524?553.
Scott A. McDonald, R.H.S. Carpenter, and Richard C.
Schillcock. 2005. An anatomically-constrained,
stochastic model of eye movement control in reading.
Psychological Review, 112:814?840.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
J. Kevin O?Regan. 1979. Eye guidance in reading: Evi-
dence for the linguistic control hypothesis. Perception
& Psychophysics, 25:501?509.
Alexander Pollatsek, Erik Reichle, and Keith Rayner.
2006. Tests of the E-Z Reader model: Exploring the
interface between cognition and eye movements.
Keith Rayner. 1998. Eye movements in reading and in-
formation processing: 20 years of research. Psycho-
logical Bulletin, 124:372?422.
Erik Reichle, Alexander Pollatsek, Donald Fisher, and
Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125?157.
Erik Reichle, Keith Rayner, and Alexander Pollatsek.
2003. The E-Z Reader model of eye-movement con-
trol in reading: Comparisons to other models. Behav-
ioral and Brain Sciences, 26:445?476.
Eric Reichle, editor. 2006. Cognitive Systems Research.
7:1?96. Special issue on models of eye-movement
control in reading.
Ronan Reilly and Ralph Radach. 2006. Some empirical
tests of an interactive activation model of eye move-
ment control in reading. Cognitive Systems Research,
7:34?55.
Dario D. Salvucci. 2001. An integrated model of eye
movements and visual encoding. Cognitive Systems
Research, 1:201?220.
Matthew Starr and Keith Rayner. 2001. Eye movements
during reading: some current controversies. Trends in
Cognitive Sciences, 5:156?163.
Ian H. Witten and Frank Eibe. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceedings of the 8th International Workshop on
Parsing Technologies (IWPT), pages 195?206.
Shun-nan Yang. 2006. A oculomotor-based model of
eye movements in reading: The competition/activation
model. Cognitive Systems Research, 7:56?69.
101
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 63?71,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Towards a Data-Driven Model of Eye Movement Control in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
This paper presents a data-driven model
of eye movement control in reading that
builds on earlier work using machine
learning methods to model saccade behav-
ior. We extend previous work by model-
ing the time course of eye movements, in
addition to where the eyes move. In this
model, the initiation of eye movements is
delayed as a function of on-line process-
ing difficulty, and the decision of where to
move the eyes is guided by past reading
experience, approximated using machine
learning methods. In benchmarking the
model against held-out previously unseen
data, we show that it can predict gaze dura-
tions and skipping probabilities with good
accuracy.
1 Introduction
Eye movements during reading proceed as an al-
ternating series of fixations and saccades with con-
siderable variability in fixation times and saccade
lengths. This variation reflects, at least to some
extent, language-related processes during reading.
Much psycholinguistic research, therefore, relies
on measures of eye movements in reading to gain
an understanding of human sentence processing.
Eye tracking recordings are routinely used to study
how readers? eye movements respond to experi-
mental manipulation of linguistic stimuli (Clifton
et al, 2007), and corpus-based analysis of eye-
tracking data has recently emerged as a new way
to evaluate theories of human sentence process-
ing difficulty (Boston et al, 2008; Demberg and
Keller, 2008).
More detailed accounts of the workings of the
eye movement system during reading are offered
by computational models of eye movement con-
trol (see Reichle (2006b), for an overview of re-
cent models). These models receive text as in-
put and produce predictions for the placement
and duration of fixations, in approximation to hu-
man reading behavior. Because eye movements
in reading rely on a coupled cognitive-motor sys-
tem, such models provide detailed accounts for
how eye movements are controlled both by on-line
language processing and lower-level motor con-
trol. Current models such as E-Z Reader (Reichle,
2006a; Pollatsek et al, 2006; Reichle et al, 2009)
and SWIFT (Engbert et al, 2002; Engbert et al,
2005) account for numerous of the known facts
about saccade behavior in reading. This includes
word frequency and predictability effects on fixa-
tion times, word skipping rates, and preview and
spillover effects.
A recent approach to eye-movement model-
ing, less tied to psychophysiological assumptions
about the mechanisms that drive eye movements,
is to build models directly from eye-tracking data
using machine learning techniques inspired by re-
cent work in natural language processing. Thus,
Nilsson and Nivre (2009) show how a classifier
can be trained on authentic eye-tracking data and
then used to predict the saccade behavior of in-
dividual readers on new texts. Methodologically
this differs from the standard approach in compu-
tational modeling of eye movement control, where
model parameters are often fitted to data but model
predictions are not evaluated on unseen data in or-
der to assess the generalization error of these pre-
dictions. Without questioning the validity of the
standard approach, we believe that the strict sep-
aration of training data and test data assumed in
machine learning may provide additional insights
about the properties of these models.
The model of Nilsson and Nivre (2009) is based
on a simple transition system for saccadic move-
ments, a classifier that predicts where to fixate next
and a classifier-guided search algorithm to simu-
late fixation sequences over sentences.
63
One obvious limitation of the model proposed
by Nilsson and Nivre (2009) is that it does not at
all capture the temporal aspects of eye movement
behavior. Thus, for example, it says nothing about
when eye movements are initiated or when the de-
cision of where to fixate next is made during fixa-
tions. In this paper, we try to overcome this limita-
tion by placing the machine-learning approach in
a broader psychological context and detail a model
that also accounts for the timing of fixations. More
precisely, we present a model of the time course of
eye movements, where saccade timing is driven by
on-line language processing and where-decisions
are driven by the experience readers have built up
through years of reading practice.1
It is not our intention in this paper to present
a full-fledged model of eye movement control in
reading. The model is limited in scope and does
not address certain important aspects of eye move-
ment control, such as within-word fixation lo-
cations, refixations and regressions triggered by
higher-order processing. In addition, the linguistic
features influencing timing (when-decisions) and
target selection (where-decisions) are restricted to
the basic variables word length and frequency. In
this way, we hope to provide a baseline against
which richer models of language processing can
be evaluated.
The rest of this paper is structured as follows.
Section 2 provides a brief background on what is
known about the time course of eye movements
during reading. Here we introduce some com-
mon notions that will be used later on. In sec-
tion 3, we first give an overview of the model
and then describe its component processes and
how these processes interrelate. In section 4, we
present an experimental evaluation of the model
using data from the English section of the Dundee
corpus (Kennedy and Pynte, 2005). Section 5 con-
tains our conclusions and suggestions for future
research.
2 The Timing of Eye Movements
The average fixation duration in reading is about
250 ms, and most fixations last between 200-300
ms, although they may range from under 100 ms
to over 500 ms for a given reader (Rayner, 1998).
Because eye movements are a motor response re-
1This view of where-decisions being driven by experience
is similar in spirit to some earlier theories of saccade target
selection in reading, such as the probabilistic account of word
skipping proposed by Brysbaert and Vitu (1998).
quiring preparation before execution, they are ini-
tiated well before the end of the fixation. Hence,
there is a saccade latency of about 150-200 ms
from the time when a saccade is first initiated un-
til the eye movement is actually executed (Becker
and J?rgens, 1979; McPeek et al, 2000). Once
the eye movement is executed, it takes about 25-
45 ms before the eyes are fixated on a new word
again, depending on the length of the movement.
Given an average saccade latency of about 150-
200 ms, and an average fixation duration of 250
ms, it seems clear that eye movements are often
initiated within the first 100 ms of a fixation. How-
ever, as Reichle notes (Reichle et al, 2003), since
the time it takes to identify words is on the order
of 150 - 300 ms, this suggests that there is not
enough time for language processes to have any
direct on-line influence on eye movements. One
key observation to explain language influences on
eye movements, however, is the finding that read-
ers often start processing upcoming words before
they are fixated. Studies on parafoveal preview
show that the amount of time spent fixating a
word depends, among other things, on how much
parafoveal preview of the word is available prior
to the word being fixated (Balota et al, 1985; Pol-
latsek et al, 1992).
A further finding supporting the assumption that
language processes can have an early effect on
eye movements comes from the disappearing text
studies (Rayner et al, 1981; Rayner et al, 2003).
In these studies, words become masked or disap-
pear at a certain point during the fixation. De-
spite this, a word need only be on display for 50-
60 ms in order for reading to proceed quite nor-
mally. More importantly, the time the eyes re-
main fixated after a word disappears depends on
the frequency of the word. Readers remain fix-
ated on low-frequency words longer than on high-
frequency words, even though the word that was
fixated has actually disappeared. In summary,
these studies suggest that there is a robust word
frequency effect in reading as early as 60 ms after
the onset of the fixation.
3 A Model of Eye Movement Control
3.1 General Overview
The model we develop takes the basic time con-
straints associated with language processing and
motor control as a starting point. This means that
our model is driven by estimates of the time it
64
takes to process words, plan an eye movement, ex-
ecute a saccade etc. In line with cognitive con-
trol models of eye movements in reading, such
as E-Z Reader, we assume that the cognitive pro-
cessing of words is the ?engine? that drives eye
movements. That is, eye movements are initiated
in response to on-line language processing. Un-
like E-Z Reader, however, we do not presume a
two-stage lexical process where the completion of
a certain hypothesized first stage triggers an eye
movement.2 Instead, when the eyes move to a new
word, an eye movement is initiated after some de-
lay that is proportional to the amount of cognitive
work left on the word. Furthermore, in contrast
to E-Z Reader we assume that saccade initiation
is decoupled from the decision of where to move
the eyes. In E-Z Reader, the initiation of a saccade
program is in effect a decision to start program-
ming a saccade to the next word. Here, instead,
the target for the next saccade can be any of the
words in the forward perceptual span. Another re-
lated difference, with respect to previous cognitive
control models, is that we assume that the deci-
sion of where to move the eyes is not directly in-
fluenced by on-line language processing. Instead,
this decision is governed by an autonomous rou-
tine, having its own dynamics automated through
years of reading experience. This experience is
approximated using machine learning methods on
authentic eye tracking data.
The model is defined in terms of four processes
that we assume are operative during reading: lex-
ical processing (L), saccade initiation delay (D),
motor programming (M), and saccade execution
(S). These processes are defined in terms of a set
of parameters that determine their duration. Once
an ongoing process ends, a subsequent process is
initiated, for as long as reading continues. As is
commonly assumed in most models of eye move-
ment control, language-related processes and mo-
tor control processes can run in parallel. We will
use the notation wi to refer to the ith word in a text
w1, . . . , wn consisting of n words, and we will use
subscripted symbols Li, Di, Mi and Si to refer to
the lexical processing, the saccade initiation delay,
the motor programming, and the saccade execu-
tion associated with wi.
In the following four subsections, we outline
2In E-Z Reader, the first stage of lexical processing is an
early estimate of the word?s familiarity that provides the sig-
nal to the eye movement system that lexical access is immi-
nent and that a saccade should be planned.
these processes in detail and discuss the general
assumptions underlying them. We then conclude
this section by summarizing how the processes dy-
namically interact to produce eye movement be-
havior.
3.2 Lexical Processing
The time needed to process individual words in
reading is certain to depend on numerous fac-
tors related to a person?s prior reading experi-
ence, word-level properties such as length and fre-
quency, and higher-order language processes such
as syntactic and semantic processing. However,
since our goal in this paper is to validate a sim-
ple model, with as few parameters as possible, we
make the simplifying assumption that the process-
ing time of a word can be approximated by its
length (number of characters) and its frequency of
occurrence in printed text. In particular, we as-
sume that the mean time required for processing a
word wi is a linear function of its length and the
natural logarithm of its frequency:3
t(Li) = b0+b1 length(wi)?b2 ln(freq(wi)) (1)
In equation 1, b0 is the intercept representing the
base time needed to process a word while b1 and
b2 are the respective slopes for the effect of length
and frequency on the base processing time. Again,
we stress that equation 1 is by all accounts an over-
simplification. Thus, for example, it does not take
into account any higher-level top-down influence
on processing time.
Still, we believe equation 1 provides a reason-
able first approximation. A large part of the vari-
ance in measures of reading time can be accounted
for by word frequency and word length. At any
rate, our simple assumption with respect to pro-
cessing time represents a methodological decision
rather than a theoretical one. We want to keep the
model as simple as possible at this stage, and later
explore the effect of including variables related to
higher-order processing.
Once the time interval t(Li) has passed for a
given word wi, lexical processing begins on the
next word. Thus, the completion of t(Li) results
in the initiation of Li+1. Because the processing
of the next word does not start until the processing
of the current word is finished, lexical processing
3We use the logarithm of word frequency because hu-
man response times, in lexical decision tasks for instance, are
linearly related to the natural logarithm of word frequency
(Balota and Chumbley, 1984).
65
proceeds serially and no more than one word is
processed at any given time.
3.3 Saccade Initiation Delay
When the eyes move to a new word wi, a motor
program is initiated after some time. We assume
that the time when a motor program is initiated
depends on the processing difficulty of the fixated
word wi. In particular, the signal to initiate a sac-
cade is deferred in proportion to how much pro-
cessing remains on wi, or put differently, in pro-
portion to how much work remains to be done on
that word. This general routine serves to prevent
the control system from making over-hasty sac-
cades to new words. The length of the saccade ini-
tiation delay t(D) is proportional to the remaining
processing time of word wi at fixation onset:
t(Di) = d (t(Li)? t(Ei)) (2)
where d is a free parameter representing a pro-
portion, t(Li) is the lexical processing time for
the fixated word, and t(Ei) denotes the interval of
time that has elapsed since the initiation of t(Li).
More difficult words are associated with longer
processing times and thus cause later initiation of
saccade programs and therefore also longer fix-
ation durations. The free parameter d defines a
proportion taking values in the range [0, 1]. The
extremes of this range can be interpreted as fol-
lows. If d is set equal to 0, a new saccade program
is initiated immediately upon a new fixation. If
d instead is set equal to 1, the saccade program
starts only after the fixated word has been fully
processed. More generally, a change of the value
of this parameter can be understood as a change of
the amount of cognitive influence on fixation du-
rations. The higher its value, the more cognitive
work must be carried out before a new saccade
program is started. Once the time interval t(D)
has passed, the planning of a new eye movement
starts, i.e., a motor program, M , is initiated.
3.4 Motor Programming
The time needed to plan and initiate an eye move-
ment defines the saccade latency, or motor pro-
gramming time t(M). We assume that the dura-
tion of this period is given by the free parameter
m:
t(Mi) = m (3)
The following is worth noting. Some influential
research suggests that motor programming is com-
pleted in two stages (Becker and J?rgens, 1979).
The first of these being a labile stage during which
a planned saccade can be canceled, e.g., in fa-
vor of another saccade target. The second stage,
closer in time to the execution of the saccade, is
non-labile and once entered, a saccade underway
can no longer be modified or canceled. This divi-
sion between labile and non-labile stages of motor
programming is sometimes implemented in com-
putational models, for example in E-Z Reader and
SWIFT. For now, however, our model does not op-
erationalize the notion of saccade canceling and
thus makes no useful distinction between labile
and non-labile stages of motor programming. Our
only assumption with respect to these different
stages of motor programming is that their respec-
tive durations sum up to m.
An important function of motor programming
in our model, however, is to select a target for the
saccade. Before discussing how this is achieved
we should point out that we make no claim as
to how much time of motor programming is con-
sumed by target selection. It is only presupposed
that saccade target selection, in the normal course
of events, is initiated as soon as there is a decision
to make an eye movement (i.e., when motor pro-
gramming starts), and that, whatever time remains
of motor programming once a target is selected,
this time is spent on preparation of the physical
movement to the selected target. Once motor pro-
gramming is finished, a saccade S is executed to
the target.
Following Nilsson and Nivre (2009), we treat
target selection as a classification task. In prac-
tical terms, this means that we train a classifier
to predict the most likely eye movement follow-
ing any fixation. An instance to be classified con-
sists of a feature vector encoding feature informa-
tion over the current fixated word and words in
the immediate context. Given such feature rep-
resentations and training data obtained from eye-
tracking recordings, essentially any standard ma-
chine learning algorithm can be applied to the
classification task. The type of learning algorithm
that performs best on this task is, however, un-
known. Rather than speculate, we suggest that this
is a question for further research.
The remaining assumptions we make are as fol-
lows. First, because there is a sharp drop-off in
acuity of the human eye around the point of fix-
ation, the number of words that can be discrim-
inated in parafoveal vision on a given fixation is
limited to a few. Therefore, it is reasonable to as-
66
sume that the potential targets for a saccade on
any given fixation are limited to the words avail-
able within the range of effective vision. 4 This
is supported empirically by the fact that the great
majority of outgoing saccades tend to land in one
of the three words that follow the current fixation.
Moreover, we assume that for these potential tar-
gets, only rather coarse, visual information, such
as a gross appreciation of their length, can be ex-
tracted on any given fixation. The reason for this
is that target selection generally occurs relatively
early on in a fixation, at a time when only low-
level visual information can reasonably be gleaned
from the parafovea.
Secondly, we reason that target selection re-
flects an autonomous process that has been au-
tomated, through years of practice, to progress
through the text and select targets in the default
reading direction. Hence, the possible targets for
target selection, as construed here, is limited to the
targets within the forward field of effective vision.
As a consequence, words to the left of the current
fixation are not fixated as a result of target selec-
tion.
Finally, we assume that target selection by de-
fault is a mechanical routine, insensitive to ongo-
ing lexical processing. In the general case, then,
the decision of where to move eyes is made in-
dependently of processing considerations. Mo-
tor programs in general, however, may sometimes
override the default target selection mechanism
and be initiated, not in order to select a new target,
but to correct for situations where motor control
and ongoing language processing are threatening
to desynchronize. Such a corrective program may
be initiated, for instance, if a saccade is executed
to wordi but lexical processing has not yet com-
pleted on wordi?1, and so more lexical process-
ing of wordi?1 is needed before moving on. In
this case, a corrective motor program is initiated
to wordi?1, subsequently resulting in a regression
to that word. In this way, corrective motor pro-
grams serve to synchronize the eyes with the cur-
rent processing stream and for that reason they al-
ways target the word being processed. Moreover,
because corrective saccade programs are launched
with a fixed target, they do not trigger target selec-
tion during motor programming.
4The effective visual field (the perceptual span) extends
about four characters to the left and 15 characters to the right
of the fixation for normal readers of left-to-right orthogra-
phies (Rayner, 1998).
3.5 Saccade Execution
The time to execute a saccade t(S) is determined
by the free parameter s:
t(Si) = s (4)
Once a saccade has been executed, the position of
the eyes shifts to a new word and thus, in the nor-
mal course of events, a new motor program is initi-
ated after t(Di). However, sometimes a saccade is
made ahead of the current processing stream, be-
cause, as noted earlier, a word needs not be fully
processed before a saccade is executed to another
word. Likewise, a saccade may sometimes be ex-
ecuted to a word that has already been fully pro-
cessed, because target selection is an autonomous
process, not influenced by ongoing processing. In
these situations, corrective saccade programs are
initiated. Since corrective saccade programs serve
only to rapidly coordinate the eyes and the cur-
rent processing stream, we assume that they can
be initiated immediately and hence that they are
not subject to saccade initiation delay.
3.6 Eye Movement Control
Having defined the respective component pro-
cesses, we now consider how these processes are
coordinated to model eye movement control. Lex-
ical processing is always running in parallel with
the processes controlling saccade initiation delay,
motor programming and saccade execution, which
are executed in sequence. A simulation of read-
ing is started by initiating lexical processing of the
first word (L1), and the saccade initiation delay
for the first word (D1) (i.e., the first word is fix-
ated). Whenever one of the running processes ter-
minates, new processes are initiated in the follow-
ing way:
? If Li terminates, initiate Li+1.
? If Di terminates, initiate Mi and select new
fixation target wj .
? If Mi terminates, initiate Si.
? If Si terminates and the ongoing lexical pro-
cess is Lj :
? If i = j, initiate Di.
? If i 6= j, initiate Mj and set fixation tar-
get to wj
The simulation terminates when all words have
been lexically processed.
67
4 Experimental Evaluation
4.1 Experimental Setup
In order to estimate the performance of the model
described in the previous section, some experi-
ments were performed using data from the English
section of the Dundee corpus (Kennedy and Pynte,
2005).
In most evaluations of eye movement control
models, the model parameters are fitted against
one and the same corpus by searching the param-
eter space to find the set of parameter values that
best simulates the observed data. This approach
makes it somewhat hard to appreciate how well
a given model generalizes to new, previously un-
seen data. A more stringent evaluation, which af-
fords an assessment of the generalization error of
model predictions, is to set the model parameters
on some portion of the data and then test the model
on another held-out portion. The results we report
in this paper were obtained this way.
The Dundee corpus that was used in these ex-
periments contains the eye tracking records of ten
subjects reading editorials from The Independent,
a UK broadsheet newspaper. The data consist of
20 texts that were read by all subjects, and close to
2400 sentences. We divided these texts into three
sets: the first 16 for training (1911 sentences),
17-18 for model development and validation (237
sentences), and the last two texts, 19-20, for blind
testing of the model (231 sentences). Model pa-
rameters were fitted using only the training and
validation set, prior to evaluating the model on the
held-out test set.
Next we discuss how training was performed,
both in terms of the training of the classifier for
target selection and in terms of the estimation of
the model?s process parameters on the training
data. Before presenting the results, we also discuss
some standard practice in benchmarking models
of eye movement control.
4.2 Training the Classifier
We used the transition-based model outlined by
Nilsson and Nivre (2009) in combination with lo-
gistic regression for training the target selection
classifier. The classifier was trained on a restricted
number of features defined over words in the fixa-
tion context. The feature model we used for these
experiments included information about the word
length of the current fixation and upcoming words,
as well as some historical information about re-
cently made eye movements. The history of pre-
vious eye movements was represented in terms
of the saccade distance (measured in number of
words) that led up to recently made fixations (in-
cluding the current fixation). In this way, the fea-
ture model contained information about, for in-
stance, whether the saccade that led up to the cur-
rent fixation skipped a word or two.
In contrast to Nilsson and Nivre (2009) we did
not train one model for each individual subject in
the corpus. Instead, we trained a single multiple-
subject classifier on all ten readers in the training
set. The performance of this classifier was as-
sessed in terms of how well, on average, it pre-
dicted the observed saccade targets for any given
reader on the development set. Moreover, in line
with the assumption that target selection is re-
stricted to a limited number of candidate words in
the forward visual field, the classifier was trained
to select one of the three words following any fixa-
tion as the target for a saccade. This cross-subject
classifier achieved an average prediction accuracy
of 72% on the development set.
4.3 Estimating Model Parameters
Because the model?s process parameters can not
be directly estimated from eye tracking data they
need to be approximated in other ways. The val-
ues for the intercept and slope parameters for lexi-
cal processing time t(Li) were obtained by fitting
a linear regression of gaze duration on logarithmic
word frequency and word length on the training
data. The assumption that the gaze duration on
a given word reflects the time required to process
the word is necessarily an oversimplification but
is sometimes used in eye movement modeling. A
number of studies indicate that it is indeed a rea-
sonable approximation (Engbert et al, 2002; Pol-
latsek et al, 2006).
The value for the parameter d in the equation for
t(Di) was selected based on a simple parameter
search over the training data. The best fitting value
was assessed by calculating the root mean square
error between predicted and observed values for
gaze durations for different values of d ranging
from 0 to 1 in 0.1 increments, while keeping other
parameter values unchanged. To keep things sim-
ple, the parameters that determine the mean dura-
tion of motor programming, m, and saccade exe-
cution, s, were fixed at 200 ms, and 25 ms, respec-
tively. These values are in good agreement with
68
Parameter Interpretation Value
b0 Intercept: base lexical processing time (ms) 165.5
b1 Slope: effect of length on lexical processing time (ms) 13.5
b2 Slope: effect of frequency on lexical processing time (ms) 3.2
d Proportion of lexical processing time (determines saccade initiation delay) 0.5
m Mean motor programming time (ms) 200
s Mean saccade execution time (ms) 25
Table 1: Model parameters, their interpretations and values, as estimated during training.
estimated values in experimental studies. Table 1
lists the model?s six process parameters and their
values, obtained prior to testing the model.
4.4 Benchmark Evaluation
Models of eye movement control in reading are
typically benchmarked against a set of word-based
dependent eye movement measures which are av-
eraged across subjects. Two such measures are
gaze duration and probability of skipping. Gaze
duration is defined as the sum duration of all fix-
ations on a word prior to any saccade leaving
the word during first-pass reading. Probability of
skipping is simply the mean probability (across
subjects) that a given word is skipped (not fixated)
during first-pass reading.
Because word frequency effects on eye move-
ments during reading are robust and well-
documented, one common benchmark practice is
to evaluate models with respect to their capabil-
ity of reproducing word frequency effects on fix-
ation times and fixation probabilities. Typically,
averages of word-based measures are then broken
down into word-frequency classes. This is a fairly
simple way to see how well a given model can
predict observed means for measures such as gaze
duration and skipping probability for words of dif-
ferent frequency classes. The results we report are
presented this way. We used frequency estimates
based on word occurrences in the written part of
the British National Corpus (BNC). Frequencies
were normalized to occurrences per million words
and then divided into five frequency classes, as
suggested by Reichle et al (1998).
In addition to the model we have outlined so
far, we also present results for two alternative
versions. These models differ from the one we
have discussed only in positing a simpler func-
tion for lexical processing time. The alternative
versions model lexical processing time only as a
linear function of either word length or logarith-
mic word frequency. Hence, we fitted two sepa-
rate simple linear regressions of gaze duration first
on word length, and then on logarithmic word fre-
quency. The regression coefficient and slope were
estimated to 132.5 and 16 for the model based on
word length, and 284 and -11 for the model based
on frequency.
4.5 Results and Discussion
Table 2 shows the observed (empirical) and pre-
dicted (simulated) values of gaze durations and
skipping probabilities for each of the five word fre-
quency classes, both on the development set and
on the held-out test set. M1 and M2 represent the
versions of the model in which lexical processing
time is a linear function of word length, and word
frequency, respectively. M3 represents the version
of the model where lexical processing time is a
linear function of both variables.
The results show that all three models, on the
development set as well as on the test set, are
able to reproduce the most important aspect of
the observed data, namely, that mean gaze du-
rations decrease and mean skipping probabilities
increase with increasing word frequency. Over-
all, M3 performs better than the two other models
in predicting this relationship. The model based
only on word length, M1, performs worse than the
other two models. This is mainly due to the poor
performance of this model in simulating the pro-
portions of skipped words in the upper frequency
classes 4 and 5. In comparison to both M2 and M3,
M1 seriously underestimates the observed skip-
ping probability for words belonging to these fre-
quency classes, on both development and test data.
With respect to gaze duration alone, the three
models perform similarly, although M3 provides
a somewhat better fit on both data sets. The mod-
els generally predict longer gaze durations than the
observed means, except for the most low-frequent
words. In particular, gaze durations for higher-
frequency words (class 4 and 5) are prolonged
compared to the means, giving an overall nar-
69
Gaze duration Probability of skipping
Development Test Development Test
Frequency class Observed M1 M2 M3 Observed M1 M2 M3 Observed M1 M2 M3 Observed M1 M2 M3
1 290 282 280 285 286 278 280 284 0.17 0.15 0.18 0.13 0.16 0.14 0.19 0.14
2 257 271 259 272 261 273 260 275 0.19 0.18 0.20 0.16 0.19 0.15 0.22 0.17
3 229 254 252 249 235 257 254 252 0.24 0.19 0.24 0.20 0.22 0.19 0.25 0.20
4 208 240 238 237 210 244 238 237 0.52 0.23 0.36 0.43 0.53 0.24 0.34 0.40
5 198 238 236 228 195 239 237 230 0.65 0.34 0.51 0.54 0.67 0.32 0.52 0.51
Table 2: Observed and predicted values of Gaze Durations (ms) and Skipping Probabilities on de-
velopment and test set for five frequency classes of words. M1: t(Li) = b0 + b1length(wi), Root
mean square error on development set = 0.48, Root mean square error on test set = 0.52; M2:
t(Li) = b0 ? b1 ln(freq(wi)), Root mean square error on development set = 0.33, Root mean square
error on test set = 0.35; M3: t(Li) = b0 + b1length(wi) ? b2 ln(freq(wi)), Root mean square error on
development set = 0.21, Root mean square error on test set = 0.26; Frequency range: 1:1-10, 2:11-100,
3:101-1000, 4:1001-10000, 5: 10001+
rower range of mean values for the five frequency
classes.
The overall performance of each model, M1,
M2 and M3 was estimated by calculating the root
mean square error (RMSE) between the mean ob-
served and predicted gaze durations and probabil-
ities of skipping. The errors were normalized as
described in Reichle et al (1998). In comparing
the results for both development and test data, the
best overall fit is provided by M3 on the develop-
ment set, giving an RMSE of 0.21 (smaller val-
ues indicate better fit). The fit for the same model
drops to 0.26 when evaluated on the held-out test
data.
To provide some basis for comparison, the ear-
liest version of E-Z Reader (Reichle et al, 1998)
which was fitted to the same dependent measures,
had an RMSE of 0.145. It is important to point
out, however, that this result was based on fitting
the model parameters to a single sentence corpus
of 48 sentences designed for experimental pur-
poses. This corpus contained relatively short (8-
14 words) isolated sentences without any connect-
ing discourse. More generally, as noted by Re-
ichle et al (2009), RMSD values lower than 0.5
provide fits that are reasonably close to the ob-
served means. By this standard, the model M3 per-
forms rather well in simulating the observed data.
Moreover, this version of the model provides the
most realistic estimates of the time it takes to iden-
tify words. Thus, for example, the mean time to
identify the most frequent word in English, ?the?
(frequency class 5), is estimated to be 171 ms,
whereas the mean time to identify the word ?re-
populate?, which is a low-frequency (frequency
class 1) ten-letter word is estimated to be 301 ms.
These estimates are in good agreement with ex-
perimental estimates, which show that word iden-
tification latencies range between 150 and 300 ms
(Rayner and Pollatsek, 1989).
5 Conclusion
In this paper we built on previous work using ma-
chine learning methods to model saccade behavior
in reading and we extended this work by present-
ing a data-driven model of eye movement control
that provides detailed predictions for both when
and where the eyes move during reading. The most
important principles of this model are (i) the initi-
ation of eye movements is delayed as a function
of on-line processing difficulty, and (ii) the deci-
sion of where to move the eyes is driven by an
autonomous routine that has become automated
through years of practice in reading. The model
was trained on eye movements made over a large
corpus of natural text. In benchmarking the model
against held-out data we showed that it is able to
reproduce frequency effects on both gaze dura-
tion and skipping probability with good accuracy
(RMSE = 0.26).
Looking ahead, we plan to extend the model
to account for more empirical data on eye move-
ment behavior in reading. One important step
to meet this goal is to develop a more informed
model of language processing. Current models
of eye movement control in reading generally as-
sume that influences from syntactic and higher-
order processing occur too late in the process-
ing stream to directly influence eye movements.
This is, however, seemingly at odds with recent
70
findings in sentence processing research showing
an influence of syntactic processing difficulty on
both early and late measures of eye movements
in reading (Demberg and Keller, 2008; Boston et
al., 2008). Hence, it is possible that a more ac-
curate model of eye movements in reading will
need to allow for syntactic processing to influ-
ence the early decisions that control the timing of
eye movements. This and other issues will be ad-
dressed in future work.
References
David. A Balota and James. I Chumbley. 1984. Are
lexical decisions a good measure of lexical access?
the role of word frequency in the neglected decision
stage. Journal of Experimental Psychology: Human
perception and Performace, 10:340?357.
David. A. Balota, Alexander Pollatsek, and Keith
Rayner. 1985. The interaction of contextual con-
straints and parafoveal visual information in reading.
Cognitive Psychology, 17:364?390.
W Becker and R J?rgens. 1979. An analysis of the
saccadic system by means of double step stimuli. Vi-
sion Research, 19:967?983.
Marisa F. Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Reasearch, 2:1?12.
Marc Brysbaert and Fran?oise Vitu. 1998. Word skip-
ping: implications for theories of eye movement
control in reading. In Geoffrey Underwood, edi-
tor, Eye guidance in Reading and Scene Perception,
pages 124?147. Elsevier science Ltd.
Charles Clifton, Adrian Staub, and Keith Rayner.
2007. Eye movements in reading words and sen-
tences. In Roger van Gompel, editor, Eye move-
ments: A window on mind and brain, pages 341?
372. Amsterdam: Elsevier.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
Ralf Engbert, Andr? Longtin, and Reinhold Kliegl.
2002. A dynamical model of saccade generation
in reading based on spatially distributed lexical pro-
cessing. Vision Research, 42:621?636.
Ralf Engbert, Antje Nuthmann, Eike Richter, and Rein-
hold Kliegl. 2005. SWIFT: A dynamical model of
saccade generation during reading. Psychological
Review, 112:777?813.
Alan Kennedy and Jo?l Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision research,
45:153?168.
R. M. McPeek, A. A. Skavenski, and K Nakayama.
2000. Concurrent processing of saccades in visual
search. Vision Research, 40:2499?2516.
Mattias Nilsson and Joakim Nivre. 2009. Learn-
ing where to look: Modeling eye movements in
reading. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 93?101.
Alexander Pollatsek, Mary Lesch, Robin K. Morris,
and Keith Rayner. 1992. Phonological codes
are used in integrating information across saccades
in word identification and reading. Experimental
Psychology: Human Perception and Performance,
18:148?162.
Alexander Pollatsek, Erik Reichle, and Keith Rayner.
2006. Tests of the E-Z Reader model: Exploring
the interface between cognition and eye movements.
Cognitive Psychology, 52:1?56.
Keith Rayner and Alexander Pollatsek. 1989. The psy-
chology of reading. Englewood Cliffs, NJ: Prentice
Hall.
Keith Rayner, Albert W. Inhoff, Robert E. Morrison,
Maria L. Slowiaczek, and James H. Bertera. 1981.
Masking of foveal and parafoveal vision during
eye fixations in reading. Journal of Experimental
Psychology: Human Perception and Performance,
7:167?179.
Keith Rayner, Simon P. Liversedge, Sarah J. White, and
Dorine Vergilino-Perez. 2003. Reading disappear-
ing text: cognitive control of eye movements. Psy-
chological science, 14:385?388.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124:372?422.
Erik Reichle, Alexander Pollatsek, Donald Fisher, and
Keith Rayner. 1998. Toward a model of eye move-
ment control in reading. Psychological Review,
105:125?157.
Erik Reichle, Keith Rayner, and Alexander Pollatsek.
2003. The E-Z Reader model of eye-movement con-
trol in reading: Comparisons to other models. Be-
havioral and Brain Sciences, 26:445?476.
Erik Reichle, Tessa Warren, and Kerry McConnell.
2009. Using E-Z Reader to model the effects of
higher-level language processing on eye movements
during reading. Psychonomic Bulletin & Review,
16:1?21.
Eric Reichle, editor. 2006a. Cognitive Systems Re-
search. 7:1?96. Special issue on models of eye-
movement control in reading.
Eric Reichle. 2006b. Computational models of eye
movement control in reading: Theories of the ?eye-
mind" link. Cognitive Systems Research, 7:2?3.
71
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 107?115,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
A Survival Analysis of Fixation Times in Reading
Mattias Nilsson
Department of Linguistics and Philology
Uppsala University
mattias.nilsson@lingfil.uu.se
Joakim Nivre
Department of Linguistics and Philology
Uppsala University
joakim.nivre@lingfil.uu.se
Abstract
Survival analysis is often used in medical and
biological studies to examine the time until
some specified event occurs, such as the time
until death of terminally ill patients. In this
paper, however, we apply survival analysis to
eye movement data in order to model the sur-
vival function of fixation time distributions in
reading. Semiparametric regression modeling
and novel evaluation methods for probabilis-
tic models of eye movements are presented.
Survival models adjusting for the influence of
linguistic and cognitive effects are shown to
reduce prediction error within a critical time
period, roughly between 150 and 250 ms fol-
lowing fixation onset.
1 Introduction
During reading, the eyes move on average four times
per second with substantial variation in individual
fixation times, reflecting, at least in part, momentary
changes in on-line language processing demands.
In psycholinguistics, it is commonly assumed that
derivative measures of fixation times, such as first
fixation duration and gaze duration, reflect cognitive
processes during reading. It is less clear, however,
how the distribution of individual fixation times in
reading is affected by on-line processing activities.
In eye movement oriented research, models that at-
tempt to model the distribution of individual fix-
ation times in reading typically assume that sac-
cadic movements are executed relatively randomly
in time, with cognition only occasionally influenc-
ing the timing of saccades (Feng, 2006; McConkie
et al, 1994; Yang and McConkie, 2001; Yang,
2006). In the model by Yang and McConkie (2001),
for example, it is assumed that cognitive control
can have a direct influence over the timing of sac-
cades only with very long fixations, after the normal
saccade has been canceled due to processing diffi-
culty. Distributional models have often made use
of the hazard function in order to analyze fixation
times in reading (Feng, 2006; Feng, 2009; Yang and
McConkie, 2001). The hazard function, in general
terms, is a function of time representing the instan-
taneous risk that an event (e.g., a saccade) will occur
at some specified time t given that it has not occurred
prior to time t.
In this paper, we model the distribution of fixa-
tion times in terms of a different but related quan-
tity, namely the survival function, which defines the
probability of being alive, i.e., the probability of the
event not having occurred, at some specified time
t. We use semiparametric regression for modeling
the influence of linguistic and cognitive effects on
the survival function, and we assess the results us-
ing survival-based time-dependent evaluation met-
rics. More specifically, our objectives are as follows.
We first estimate the survival functions for ten differ-
ent readers using the Kaplan-Meier method (Kaplan
and Meier, 1958) in order to establish the general
shape of the survival function for reading time data.
Then, we estimate adjusted survival functions using
Cox proportional hazards model (Cox, 1972) in or-
der to examine the influence of stimulus variables
on survival. Finally, we assess the adjusted survival
models both with respect to the estimated effects of
covariates and with respect to the predictive perfor-
107
mance on held out data. The experiments we report
in this paper are based on first fixation data (multi-
ple refixations discarded) from the English section
of the Dundee Corpus of eye movements in reading
(Kennedy and Pynte, 2005).
The remainder of this paper is organized as fol-
lows. Section 2 introduces survival analysis and
further motivates its use for modeling fixation du-
rations in reading. Section 3 introduces and applies
the Kaplan-Meier estimate, to compare the survival
functions for the different readers in the corpus.
Section 4 introduces the Cox proportional hazards
model and section 5 outlines two methods for assess-
ing the performance of survival models on new data.
Section 6 presents the experimental evaluation of us-
ing Cox proportional hazards to model the survival
function and summarize and discuss the results. Sec-
tion 7, finally, concludes this paper.
2 Background
Survival analysis is the study and modeling of the
time it takes for events to occur. Because methods
for survival analysis originally were developed for
studying the lifetime distributions of humans in an
epidemiological context, the prototypical event in
these studies is death and the primary variable of in-
terest thus time until death occurs. The use of sur-
vival analysis, however, reaches beyond the clinical
and medical sciences and survival methods apply to
any study with a naturally identifiable starting point
and a well-defined event of interest as end point. In
non-medical contexts, survival analysis often goes
by other names, such as failure time analysis or re-
liability analysis in engineering applications, event
history analysis in sociology, or simply duration
analysis in yet other contexts.
A defining characteristic of survival analysis is the
ability to deal with censoring in a principled manner.
Censoring is said to occur when only partial infor-
mation about the survival time of an individual (hu-
man or other) is available. The most common type
of censoring is referred to as right-censoring, which
occurs when an individual is not subject to the event
of interest during the course of the observation pe-
riod. In this case, it is only known that the individual
did not experience the event prior to the end of the
study, but may perhaps do so at a later point in time
and this piece of partial information about the cen-
sored survival time is included in the analysis.
There are, however, potentially good reasons for
using survival analysis even in time-to-event studies
that do not necessarily involve censored data, such
as when measuring the brief periods of time elaps-
ing between a stimulus appearance and a button-
press in response-time studies, or when measuring
the time between one saccade and the next during
reading using eye-tracking. Such data is usually not
normally distributed and even in the absence of cen-
soring one may take advantage of the fact that sur-
vival data is almost never assumed to be normally
distributed and the methods of survival analysis are
designed to reflect this. Furthermore, if the correct
parametric model for the data is not known, or one is
not confident enough that a given parametric model
is appropriate, the Cox proportional hazards model
provides a robust1 and widely used semiparametric
regression method for time-to-event data. With re-
spect to eye movement data, the Cox model appears
appealing because, as pointed out by Feng (2006,
2009), several different types of distributions have
been proposed as models of fixation times in reading
at one time or another, suggesting there is indeed lit-
tle agreement with respect to the correct parametric
model.
2.1 Survival and Hazard
Survival data is commonly analyzed and modeled in
terms of the survival and the hazard function.
The survival function describes the probabilistic
relationship between survival and time. Let T be
a random variable denoting an individuals? survival
time (T ? 0). The survival function, S(t), defines
the probability that the individual survives longer
than some specified time t:
S(t) = P (T > t) (1)
The survival function is a monotonically decreas-
ing function heading downward as t increases and
has the following theoretical properties: S(0) = 1,
the probability of surviving past time 0 is 1; and
S(?) = 0, eventually nobody survives and S(t)
1Cox proportional hazards model is ?robust? in the sense
that the results will be reasonably close to those obtained using
the correct parametric model.
108
falls to zero as t tends to infinity. Notice also that
if F (t) is the cumulative distribution function for T ,
the survival function, S(t), is 1? F (t).
In the present study, we let the event of interest
be the occurrence of a saccade following a fixation
period, and the most reasonable starting point for
our measurements, at least in practice, appears to be
the beginning, or the onset, of the fixation period.
We will refer to the period onset-to-saccade inter-
changeably as the fixation time or the survival time.
Thus, in this context, the survival function S(t) sim-
ply expresses the probability that a given fixation
lasts, or survives, longer than some specified time
t.
The hazard function, h(t), gives the instantaneous
potential, per unit time, for an event to occur in some
small time interval after t, given survival up to time
t:
h(t) = lim
?t?0
P (t ? T < t+ ?t | T ? t)
?t
(2)
The conditional probability in the formula for the
hazard function expresses the probability that the
survival time, T , will lie in the time interval between
t and t + ?t, given that the survival time is greater
than or equal to t, where ?t denotes an infinitesi-
mally small interval of time. As already suggested,
in this study the hazard function represents the in-
stantaneous risk, or hazard, of a saccade occurring
following a fixation at some specified time t, given
that it has not yet occurred.
3 Kaplan-Meier Survival Estimate
The survival function for time-to-event data can be
estimated from a sample of survival times, both cen-
sored and uncensored, using the Kaplan-Meier (aka
Product-Limit) method. This is a non-parametric
estimate of the survival function which orders the
survival times, from the shortest to the longest, and
adjusts, for each of the event times, the number of
cases still alive according to the number of cases
that were either subject to the event or censored in
the previous time period.
Let dj be the number of saccades that occur at
time tj , and let nj be the number of fixations for
which no saccade has yet occurred at time tj . The
Kaplan-Meier estimate of the survival function S(t)
is then given by:
S?(t) =
?
t(j)?t
(1?
dj
nj
) (3)
In the absence of censored observations, the Kaplan-
Meier estimate is equivalent to the empirical dis-
tribution, and the cumulative survival probability at
time tj reduces to the number of surviving fixations
at time tj divided by the total number of fixations in
the sample. The value of S?(t) is constant between
event times and the estimated function is therefore a
step function that changes value only at times when
one or more saccades occur.
3.1 Kaplan-Meier Survival of Reading Data
Feng (2009) estimated the hazard function for the
distribution of fixation times for the readers of the
Dundee corpus. Here, we give a complementary
account by estimating the corresponding survival
function for these readers using the Kaplan-Meier
method. Figure 1 shows the survival functions for
each reader plotted against time. Individual differ-
ences in the survival function emerge soon after 50
ms and at 100 ms we can spot different tendencies
with respect to how fast or slow the curves decline.
Overall, however, the behavior of the survival func-
tion appears similar across readers. Typically, the
survival function begins with a slow decline up until
about 150 ms and is then followed by a very rapid
decline during the next 100 ms. Thus, we can see
in figure 1 that the overall survival rates drop from
about 80% to 20% in the time interval 150-250 ms.
Thereafter, the function flattens again and at about
400 ms it appears to be converging between the read-
ers. It is worth noting, however, that the reliability
of the estimate decreases with time since the number
of surviving fixations becomes fewer and fewer.
Median survival time is the point in time when
50% of the total number of fixations has been termi-
nated by a saccade. It is thus read off the plot as the
time where the probability of survival is 0.5. Median
survival time ranges from 168 ms (reader g) to 220
ms (reader b). Mean median survival time across all
ten readers is 191.4 ms with a standard deviation of
14.9 ms.
109
0.0
0.2
0.4
0.6
0.8
1.0
Time (ms) 
Pro
bab
ility
 of 
sur
viv
al
sa   
sb   
sc   
sd   
se   
sf   
sg   
sh   
si   
sj   
0 50 100 150 200 250 300 350 400
Figure 1: Kaplan-Meier curves for fixation durations showing the cumulative survival probability, following fixation
onset, grouped by individual reader (subject a-j).
4 Cox Proportional Hazards Model
This section introduces the Cox proportional haz-
ards model. We will later apply this model in the
experimental part of the paper to obtain adjusted es-
timates of the survival function for the readers in the
Dundee corpus.
The Cox proportional hazards model is a semi-
parametric regression model for survival data relat-
ing survival time to one or more predictors or co-
variates. More precisely, the Cox model regresses
the hazard function on a set of predictors, providing
estimates of their effects in terms of hazard ratios.
The Cox proportional hazards model has the follow-
ing form:
h(t) = h0(t) exp{?1x1 + ?2x2 + . . .+ ?nxn} (4)
where h0(t) is the baseline hazard function at time t,
x1 . . . xn are the set of covariates or predictor vari-
ables, and ?1 . . . ?n are the corresponding coeffi-
cients to be estimated2. Thus, this model gives an
expression for the hazard at time t for a particular
individual with a given set of covariates.
The baseline hazard, h0(t), represents the value
of the hazard function when all covariates are equal
to zero, and in the Cox model this baseline haz-
ard is left unspecified and varies as a function of
time. Since no assumptions are made with respect
2Parameter estimates in the Cox model are obtained by max-
imizing the ?partial? likelihood, as opposed to the (full) likeli-
hood. Details of procedures for parameter estimation can be
found, for example, in Kalbfleisch and Prentice (1980).
to the form or distribution of the baseline hazard,
this can be regarded as the nonparametric part of the
Cox proportional hazards model. However, the Cox
model assumes a parametric form with respect to the
effect of the predictors on the hazard. In particu-
lar, as seen in equation 4, the predictors are assumed
to multiply hazard at any point in time. This is an
important assumption of the Cox model referred to
as the assumption of proportional hazards. It means
that the hazard functions for any two individuals at
any point in time should be proportional. In other
words, if a certain individual has a risk of the event at
some initial point in time that is twice as high as that
of another individual, then, under the proportional
hazards assumption the risk remains twice as high
also at all later times. There are a variety of different
graphical and goodness-of-fit based procedures that
can be used to evaluate the proportional hazards as-
sumption for survival data (see Kleinbaum and Klein
(2005) for an overview.).
The parameter estimates in a fitted Cox model are
commonly interpreted in terms of their hazard ra-
tios. If bi is the value of the coefficient for predictor
xi, the exponentiated coefficient, ebi , gives the esti-
mated hazard ratio for xi. For continuous variables,
the hazard ratio refers to the risk change associated
with one unit increase in xi, controlling for the effect
of the other variables. A hazard ratio above one indi-
cates a raised risk of the event occurring and the pre-
dictor is in this case thus negatively associated with
survival. Correspondingly, a value below one indi-
110
cates a decreased risk and the predictor is thus posi-
tively associated with survival. Lastly, if the hazard
ratio is equal to one, there is no indication of any
associated risk change.
5 Assessment of Survival Models
Accurate prognoses are of critical importance in
many areas where survival analysis apply, for in-
stance in medical contexts where doctors have to es-
timate the expected remaining life time for termi-
nally ill patients. Survival models are thus often as-
sessed with respect to their predictive performance
on novel data, in addition to the statistical signifi-
cance of model covariates. We now briefly review
two of the most commonly used measures for as-
sessing the quality of survival models on indepen-
dent data sets.
5.1 Prediction Error Curves
The prediction error for survival data is defined as a
function of time and can be measured by the Brier
score (Brier, 1950). Intuitively, if an individual is
alive at time t, the predicted survival probability
should be close to 1, and otherwise close to 0. The
prediction error, or Brier score, at time point t is
defined as the mean squared error between the ob-
served survival status Yi(t) for the individual i at
time t, which is equal to 1 if the individual is alive at
t, and 0 otherwise, and the predicted survival proba-
bility for i at time t:
B?S(t) =
1
n
n?
i=1
{Yi(t)? Si(t)}
2 (5)
The lower the Brier score, the lower the predic-
tion error. Various benchmark values for the Brier
score at time t exists. The values 0.25 and 0.33,
for example, correspond to a constant predicted sur-
vival probability of 50% and to a randomly pre-
dicted value between 0 and 1, respectively. Often,
however, the Kaplan-Meier estimate of the survival
function over the training sample is used. In this
case, the benchmark prediction at time point t cor-
responds to the proportion of individuals surviving
past t, thus ignoring all available covariate informa-
tion. By tracking the prediction error over time we
get the prediction error curve (Graf et al, 1999) and
a summary measure of the error for the whole ob-
servation period can be obtained by integrating over
time (the integrated Brier score).
5.2 Concordance Index
The concordance index (Harrell et al, 1982), or C-
index, estimates the probability that a given predic-
tion agrees, or concurs, with the observed outcome.
For uncensored data, the concordance index is given
by the relative frequency of concordant pairs among
all pairs of individuals. A pair is said to be concor-
dant if the individual with the shorter survival time
is also predicted by the model to have the highest
risk of the two. Useful reference values for the con-
cordance index are 0.5 which indicates that the pre-
dictions are no better than chance, and 1 which indi-
cates that the model discriminates the pairs perfectly.
6 Experimental Evaluation
In order to study the influence of cognitive and lin-
guistic effects on the survival function, the following
experiment is performed. First, the Cox proportional
hazards model is used to regress fixation times on
five different stimulus variables associated with the
current fixation, thus providing estimates of the haz-
ard ratios for the effects of each variable adjusted for
the other variables in the model. Second, we obtain
adjusted survival functions, i.e. survival curves that
adjust for the stimulus variables used as covariates,
and we assess these curves with respect to the gen-
eralization error on held-out corpus data.
It is worth pointing out that regression studies on
the Dundee Corpus of eye movements have been
carried out before (e.g., Demberg and Keller, 2008;
Pynte and Kennedy, 2006). Our experiment, how-
ever, differs from previous studies in at least three
ways: (1) our goal is to model the survival func-
tion of fixation time distributions in reading, which
means that we use the survival time of individual fix-
ations as the unit of analysis; (2) we assess the sur-
vival model not only with respect to the estimated
regression coefficients, but also with respect to the
models? predictive performance on unseen data; (3)
we use a semiparametric regression method for sur-
vival data which has not been previously applied,
as far as we know, to reading-time data. It is also
worth pointing out that although we believe that a
111
Table 1: Results of Cox proportional hazards model of fixation times in the Dundee Corpus section 01-16: hazard
ratios (HR) and significance levels (p) for all covariates in the model, and for each individual model of reader a-j.
a b c d e f g h i j
Variable HR p HR p HR p HR p HR p HR p HR p HR p HR p HR p
Word length 1.015 < .001 0.983 < .001 0.979 < .001 0.988 < .001 0.992 < .05 0.992 < .01 0.992 < .01 0.985 < .001 0.990 < .01 0.987 < .001
Word frequency 1.055 < .001 1.042 < .001 1.036 < .001 1.051 < .001 1.051 < .001 1.014 < .001 1.031 < .001 1.028 < .001 1.040 < .001 1.044 < .001
Bigram probability 1.108 < .001 1.196 < .1 1.092 < .05 1.006 < .01 1.013 < .05 1.014 < .001 0.953 1.011 < .001 1.003 1.005 < .05
Surprisal 1.001 0.986 < .001 0.994 < .01 0.984 < .001 0.998 < .01 0.991 < .05 1.002 0.994 0.993 < .05 0.996 < .01
Entropy 0.966 < .001 0.986 < .01 0.980 < .001 0.988 < .01 0.963 < .001 1.002 0.990 < .05 0.992 < .05 0.969 < .001 0.978 < .001
careful comparison of the results obtained using sur-
vival analysis to those reported for other regression
methods would be useful and interesting, it is never-
theless beyond the scope of this paper.
Most of the stimulus variables included in the
analysis have been shown to correlate with reading
times in other regression studies: the number of let-
ters in the word, the logarithm of the word?s rela-
tive frequency (based on occurrences in the British
National Corpus), the logarithm of the conditional
(bigram) probability of the word (based on occur-
rences in the Google Web 1T 5-gram corpus (Brants
and Franz, 2006)), the syntactic surprisal and en-
tropy scores3 (computed here using the probabilis-
tic PCFG parser by Roark et al (2009)). The sur-
prisal (Hale, 2001) at word wi refers to the nega-
tive log probability of wi given the preceding words,
computed using the prefix probabilities of the parser.
A number of studies have previously established a
positive relation between surprisal and word-reading
times (Boston et al, 2008; Demberg and Keller,
2008; Roark et al, 2009). The entropy, as quantified
here, approximates the structural uncertainty associ-
ated with the rest of the sentence, or what is yet to
be computed (Roark et al, 2009).
In this experiment, we use the first 16 texts in
the Dundee corpus for parameter estimation, and the
following two texts, 17 and 18 for model assessment
of the generalization error. To avoid introducing bi-
ases that may result from pooling distributional data
together, we model each of the readers in the cor-
pus separately. Prior to running the experiments,
we also validated the Cox proportional hazards as-
sumption using a goodness-of-fit approach based on
the Schoenfeld residuals (Schoenfeld, 1982). The
outcome of this test indicated a slight violation of
3To ease interpretation of the estimated hazard ratios, no in-
teraction terms were included in this model.
the proportional hazards assumption. However, it
is well-known that a slight violation may occur for
large data samples, given that p-values can be driven
by sample size (Kleinbaum and Klein, 2005).
6.1 Results
6.1.1 Hazard Ratios
Table 1 shows the results of the Cox proportional
hazards regression models. The estimated hazard ra-
tio for each covariate along with the corresponding
significance level is reported for each reader. Recall
that a hazard ratio above one indicates a worse sur-
vival prognosis, i.e. shorter fixation times, while a
hazard ratio below one indicates better survival, i.e.
longer fixation times.
Overall, the effects go in the directions expected
for these variables based on previous research.
There is a significant positive effect of word length
on survival for all but one reader. The hazard ra-
tio for the significant effects ranges between 0.979
and 0.992. Word length thus decreases the hazard by
about 1-2% for each additional letter in a word when
adjusting for the effects of the other covariates in
the model. Word frequency is significantly and neg-
atively related to survival across all readers. More
frequent words have shorter survival times. The av-
erage hazard ratio among the readers is 1.0392 and
the estimated risk of a saccade increases thus on av-
erage by a factor of 1.0392 for each unit increase
in log word frequency. Bigram probability is nega-
tively and significantly related to survival for eight
readers with an average hazard ratio of 1.0569. Sur-
prisal is significantly and positively related to sur-
vival for seven readers. Among these, the hazard
decreases by 1% for each unit increase in surprisal.
Entropy has a significant and positive effect on sur-
vival on all but one readers. The hazard ratios range
between 0.963 and 0.992, corresponding to a de-
112
Brier score t
t.100 t.150 t.200 t.250 t.300
Model Cox KM Cox KM Cox KM Cox KM Cox KM
a 0.05 0.05 0.14 0.15 0.24 0.25 0.14 0.15 0.05 0.06
b 0.05 0.05 0.12 0.13 0.23 0.25 0.21 0.23 0.12 0.13
c 0.13 0.13 0.23 0.24 0.17 0.18 0.06 0.07 0.02 0.02
d 0.07 0.07 0.17 0.18 0.23 0.25 0.15 0.16 0.06 0.06
e 0.05 0.05 0.15 0.15 0.23 0.25 0.14 0.15 0.05 0.05
f 0.09 0.09 0.21 0.21 0.22 0.23 0.12 0.12 0.06 0.06
g 0.16 0.16 0.23 0.23 0.24 0.25 0.12 0.13 0.07 0.07
h 0.07 0.07 0.15 0.15 0.24 0.25 0.20 0.20 0.12 0.12
i 0.04 0.04 0.13 0.13 0.23 0.25 0.10 0.10 0.03 0.03
j 0.06 0.06 0.18 0.19 0.23 0.25 0.12 0.12 0.05 0.05
Avg. 0.077 0.077 0.171 0.176 0.226 0.241 0.136 0.143 0.063 0.065
Table 2: Prediction error on held-out data between the observed survival status and the predicted survival probability
at different times t, for Kaplan-Meier and Cox-model adjusted survival, and for all models of readers a-j.
creased risk by 1-4% per additional unit increase,
after adjusting for the effects of the other predic-
tors. While Frank (2010) recently showed that sen-
tence entropy, i.e. non-structural entropy, accounts
for a significant fraction of the variance in reading
times, our results provide additional support for the
influence of structural sentence entropy on reading
times. Moreover, it is noteworthy that the effect of
entropy appears reliably robust in individual first fix-
ation times, suggesting that the effects of structural
processing demands can be immediate rather than
delayed in the eye movement record.
6.1.2 Adjusted Survival
We summarize the results of the evaluation of the
adjusted survival function on held-out data in table
2 and in table 3. Table 2 shows the Brier score com-
puted at different points in time in the interval 100
to 300 ms. Results are reported both for the Kaplan-
Meier estimate of the survival function and for the
fitted Cox-models. We present the results for each
individual model. The bottom row gives the results
obtained when averaging over all models at the spec-
ified time t.
Recall that the Brier score, or prediction error,
at any specified time t, is computed over all fixa-
tions in the held-out set and gives the average of the
squared distances between the actual survival status
and the predicted survival probability at time t. Al-
though the differences between the Cox-model and
the Kaplan-Meier estimate are small overall, there
are two subtle but notable results. First, the ad-
justed survival model is never underperforming the
Kaplan-Meier survival estimate. The prediction er-
ror of the Cox model is consistently lower or equal to
the Kaplan-Meier prediction error at each time point
and for each reader. Second, in comparison to the
Kaplan-Meier error, the prediction error of the ad-
justed model is systematically lower in the time win-
dow 150-250 ms, but essentially the same prior to,
and after, this time period. This is readily reflected
in the average scores, for example. One interpre-
tation of these small but systematic differences sug-
gests that there is a limited period, approximately no
earlier than 150 ms. and no later than 250 ms. on av-
erage, during which the covariates in the model are
primarily influencing the survival time. Before and
after this period, the stimulus variables of the fixated
word appear to have little or no influence on the time
when saccades are generated. In other words, we ob-
serve an improved agreement to the observed data in
the interval 150-250 ms. under the assumption that
each individual fixation has an independent survival
function whose value at time t is influenced by the
specific values for the stimulus variables of the fix-
ation. Recall that the benchmark, the Kaplan-Meier
estimate, in contrast, assumes one and the same un-
derlying survival function for all fixations, ignoring
all available covariate information. By plotting the
113
Time
Pred
iction
 error
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600
Kaplan.MeierCox
Figure 2: Prediction error curves on held-out data between the observed survival status and the predicted survival
probability, for Kaplan-Meier and Cox-model adjusted survival, for the model of reader d.
Model IBSC C-index
Kaplan-Meier 0.041 0.582
Cox 0.043 0.598
Table 3: Integrated Brier score (IBSC) and Concordance
index (C-index) on held-out data, for Kaplan-Meier and
Cox-model adjusted survival, averaged over the results
obtained for each model of reader a-j.
time-dependent prediction error, subtle differences
in survival over the time course are more easily spot-
ted. Figure 2 shows, as an example, the prediction
error curve for one of the models.
Table 3 gives the integrated brier score, i.e., the
prediction error obtained when integrating over all
event times, and the concordance index C, for
both the Kaplan-Meier estimate and the Cox model.
These results are averaged over the results of the in-
dividual models. The integrated Brier score verifies
that the Cox model fares somewhat better, although
the impact of the model variables appears limited in
time. The C-value for both the Kaplan-Meier and
the Cox model is significantly better than chance
(0.5). A C-value of 0.6 - 0.7 is a common result
for survival data.
7 Conclusion
In this paper we applied survival analysis to model
fixation times in reading. In particular, we modeled
the survival function of fixation time distributions
using the Kaplan-Meier estimate, and the Cox pro-
portional hazards model to adjust for cognitive and
linguistic effects on survival. The adjusted survival
models were assessed with respect to the effect of
covariates on hazard rates, and with respect to their
predictive performance using evaluation metrics that
are novel in the context of eye-movement and psy-
cholinguistic modeling.
The results of the analysis suggests that: (1) struc-
tural sentence entropy influences survival, i.e., in-
creasing structural uncertainty about the rest of the
sentence decreases the risk of moving the eyes; (2)
stimulus variables associated with the current fixa-
tion influence the survival of the fixation in a limited
time frame, roughly between 150 and 250 ms fol-
lowing onset; and (3) linguistic and cognitive effects
may influence the timing of saccades earlier than is
sometimes assumed.
Looking ahead, important topics to be inves-
tigated in the future include frailty models and
competing risks survival analysis. Frailty models
are survival-based regression models with random
effects, designed to account for variance due to
individual-level factors otherwise unaccounted for.
114
Competing risks survival analysis apply to situations
where a finite number of different types of events are
possible, but only one of the events can actually oc-
cur per individual, e.g., dying from either lung can-
cer or stroke. In the current study we did not dif-
ferentiate between different types of events follow-
ing a fixation. A competing risks analysis, however,
would let us differentiate between different types of
saccades and study the influence of predictors on the
survival function based on the type of the saccade
following a fixation, e.g., whether it is a forward-
directed saccade, refixation or regression. These and
other issues will be addressed.
References
Marisa F. Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Reasearch, 2:1?12.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium.
Glenn W. Brier. 1950. Verification of forecasts ex-
pressed in terms of probability. Monthly Weather Re-
view, 78:1?3.
David R. Cox. 1972. Regression models and life-
tables. Journal of the Royal Statistical Society. Series
B (Methodological), 34:187?220.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
Gary Feng. 2006. Eye movements as time-series random
variables: A stochastic model of eye movement con-
trol in reading. Cognitive Systems Research, 7:70?95.
Gary Feng. 2009. Time course and hazard function: A
distributional analysis of fixation duration in reading.
Journal of Eye Movement Research, 3:1?23.
Stefan L. Frank. 2010. Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceedings of
the ACL Workshop on Cognitive Modeling and Com-
putational Linguistics.
Erika Graf, Schmoor Claudia, Sauerbrei Will, and Schu-
macher Martin. 1999. Assessment and comparison
of prognostic classification schemes for survival data.
Statistics in Medicine, 18:2529?2545.
John Hale. 2001. A probabilistic early parser as a
psycholinguistic model. In Proceedings of the sec-
ond conference of the North American chapter of the
Association for Computational Linguistics, volume 2,
pages 159?166.
Frank E. Jr Harrell, Robert M. Califf, David B. Pryor,
Kerry L. Lee, and Rober A. Rosati. 1982. Evaluating
the yield of medical tests. Journal of the American
Medical Association, 247:2543?2546.
John D. Kalbfleisch and Ross L. Prentice. 1980. The
statistical analysis of failure time data. Wiley.
Edward L. Kaplan and Paul Meier. 1958. Nonparametric
estimation from incomplete observations. Journal of
the American statistical association, 53:457?481.
Alan Kennedy and Joel Pynte. 2005. Parafoveal-on-
foveal effects in normal reading. Vision Research,
45:153?168.
David G. Kleinbaum and Mitchell. Klein. 2005. Survival
analysis: A self-learning text. Springer.
George W. McConkie, Paul W. Kerr, and Brian P. Dyre.
1994. What are normal eye movements during read-
ing: Toward a mathematical description. In J. Ygge
and G. Lennerstrand (Eds.), editors, Eye movements
in reading: Perceptual and language processes, pages
315?327. Oxford: Elsevier.
Joel Pynte and Allan Kennedy. 2006. An influence over
eye movements in reading exerted from beyond the
level of the word: Evidence from reading english and
french. Vision Research, 46:3786?3801.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe. Pallier. 2009. Deriving lexical and syn-
tactic expectation-based measures for psycholinguistic
modeling via incremental top-down parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 324?
333.
David Schoenfeld. 1982. Partial residuals for the propor-
tional hazards model. Biometrika, 69:51?55.
Shun-nan Yang and George W. McConkie. 2001. Eye
movements during reading: a theory of saccade initia-
tion times. Vision Research, 41:3567?3585.
Shun-nan Yang. 2006. A oculomotor-based model of eye
movements in reading: The competition/interaction
model. Cognitive Systems Research, 7:56?69.
115
