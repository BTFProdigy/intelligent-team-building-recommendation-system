Distributed Modules for Text Annotation and IE
applied to the Biomedical Domain
Harald Kirsch and Dietrich Rebholz-Schuhmann
EMBL-EBI
Wellcome Trust Genome Campus
Hinxton, Cambridge CB10 1SD, UK
{kirsch,rebholz}@ebi.ac.uk
Abstract
Biological databases contain facts from scien-
tific literature, which have been curated by
hand to ensure high quality. Curation is time-
consuming and can be supported by informa-
tion extraction methods. We present a server
which identifies biological facts in scientific text
and presents the annotation to the curator.
Such facts are: UniProt, UMLS and GO ter-
minology, identification of gene and protein
names, mutations and protein-protein interac-
tions. UniProt, UMLS and GO concepts are
automatically linked to the original source. The
module for mutations is based on syntax pat-
terns and the one for protein-protein interac-
tions on NLP. All modules work independently
of each other in single threads and are combined
in a pipeline to ensure proper meta data inte-
gration. For fast response time the modules are
distributed on a Linux cluster. The server is at
present available to curation teams of biomedi-
cal data and will be opened to the public in the
future.
Contents
1 Introduction
Biologists rely on facts from public databases
like GenBank1, LocusLink2 and UniProt3 and
increasingly integrate facts from scientific liter-
ature into OMIM4, FlyBase5, Gene Ontology
(GO)6 or COSMIC7. Curation of such facts is
time-consuming and costly. It can be supported
by text mining methods (Yeh et al, 2003), but
information extraction is not yet able to fully
1www.ncbi.nlm.nih.gov/Genbank
2www.ncbi.nlm.nih.gov/LocusLink
3www.ebi.ac.uk/uniprot
4www.ncbi.nlm.nih.gov/omim
5www.flybase.org
6www.geneontology.org
7www.sanger.ac.uk/perl/CGP/cosmic
replace curators, since these databases target
100% precision.
Tools to support curation extract facts and
context. Results are then presented to cura-
tors for evaluation and eventually are added to
the database. A supervisor may resolve conflicts
(Albert et al, 2003).
Such information extraction (IE) tools have
to meet a number of demands. First protein
and gene names (PGNs) have to be identified in
the text. Although protein and gene names can
be gathered from public resources like Human
Genome Nomenclature (Hugo)8 and UniProt,
the sets are not complete and do not cover
the full morphological variability encountered
in the literature. Promissing automatic extrac-
tion methods have been reported (Hanisch et
al., 2003), but the BioCreAtIve contest revealed
lower performance9, thus leaving the problem
unsolved. Second, relevant facts associated to
PGNs have to be extracted like disease, tissue
type, species, indication of function and mu-
tations of sequence. Again controlled vocabu-
laries are available, e.g. UMLS10 and GO, but
are also not complete. In the case of muta-
tions, syntax patterns support proper identifica-
tion (Rebholz-Schuhmann et al, 2004). Finally,
identification of relationships between PGNs,
e.g. extraction of protein-protein interactions
(Temkin and Gilder, 2003), is relevant to de-
termine protein function.
Current IE tools like PASTA11 are geared to-
wards special tasks (Gaizauskas et al, 2003).
No IE tool exists that fulfills all above men-
tioned demands for the following reasons. The
complexity of data makes it difficult to provide
fully annotated data sets to train IE techniques
based on machine learning, since annotation is
8www.gene.ucl.ac.uk/hugo
9www.pdg.cnb.uam.es/BioLINK/
BioCreative.eval.html
10www.nlm.nih.gov/research/umls
11www.dcs.shef.ac.uk/nlp/pasta
50
time-consuming and costly. In addition, grow-
ing demands from curation teams and diversity
of their needs require a flexible solution, which
can incrementally be extended by new compo-
nents. To meet this challenge and to provide
an appropriate service, we developed a modular
software environment which tackles basic tasks
for curators.
We implemented a server solution which an-
notates facts identified in biological text and
links them to biomedical databases where pos-
sible. The server is typically accessed via web
browser. Its modular design allows integra-
tion of controlled vocabularies (GO, UniProt,
UMLS), of syntax pattern sets, e.g. for ab-
breviation definitions, and of natural language
processing like the identification of protein-
protein interactions. The server will be avail-
able through EBI?s Web server12 and accepts
text via cut&paste or via a URL.
2 Available Modules
The available modules belong to three cate-
gories: (1) basic NLP modules which mainly
identify syntactical information, (2) modules
which match controlled vocabularies, (3) mod-
ules which match a set of syntax patterns, and
(4) modules for shallow parsing based on cas-
caded patterns. The categories are not inde-
pendent, since named entity (NE) recognition
relies on controlled vocabularies as well as on
patterns for the identification of yet unknown
NEs. Most modules match regular expressions
(REs). These are matched with a finite state au-
tomata (FSA) engine we implemented. It is op-
timized for pipelined execution and huge REs.
Basic NLP modules comprise the sentenciser
and a part-of-speech (PoS) tagger. The senten-
ciser splits text into sentences and wraps them
into a SENT XML element with a unique ID. The
PoS tagger13 was trained on the British national
corpus, but contains lexicon extensions for the
biomedical concepts. Noun phrases (NPs) are
identified with syntax patterns equivalent to
DET (ADJ|ADV) N+.
Controlled vocabularies Identification and
tagging of terminology is a variant of NE recog-
nition. In biology and medicine a large num-
ber of concepts is stored in databases like
UniProt, where roughly 190000 database entries
link PGNs to protein function, species and tis-
sue type. PGNs from UniProt are transformed
12Open to the public after assessment for heavy load
13developed at CIS, www.cis.uni-muenchen.de
into REs which account for morphological vari-
ability. For example col1a1 is transformed into
the pattern (COL1A1|[cC]ol1a1) and IL-1 into
(IL|[Ii]l)[- ]1. The PGNs available from
the database automatically generate a suitable
link from the text to one or more database en-
tries. While adding more dictionaries is techni-
cally trivial, it creates the problem of conflicting
definitions. Already UniProt introduces the up-
percase concept names CAT, NOT and FOR as
PGNs. Disambiguation of such definitions will
be added as soon as available.
Syntax patterns A number of IE tasks are
solved with syntax patterns. The following
modules are integrated into the server: (1)
identification of abbreviations, (2) definitions of
PGNs, and (3) identification of mutations.
Abbreviation extraction is described for ex-
ample in (Chang et al, 2002). In our approach a
variety of patterns equivalent to NP ?(? token
?)? is used, where the token has to be the ab-
breviation of NP. If an abbreviation is found in
the text without its expanded form, however,
it is necessary to decide whether it is indeed an
abbreviation and which expansion applies (work
in progress).
A separate module identifies sentence pieces
where the author explicitely stated the fact
that the concept denotes a PGN. Examples
are The AZ2 protein was ...14 and PMP22
is the crucial gene ...15. Such examples
were translated into the following four patterns:
(1) the X protein, (2) the protein X, (3) T
domain of NP, and (4) NP is a protein. The
X denotes a single token and T represents a se-
lection of concepts which are known to be used
in conjunction with a protein. The tokens the,
is, a and protein again represent sets of equiv-
alent tokens.
Identification of mutations is integrated
as described in (Rebholz-Schuhmann et al,
2004). Integrated patterns identify nomencla-
ture equivalent to AA [0-9]+ AA, where AA de-
notes all variants of an amino acid or nucleic
acid. Apart from the infix representation of the
mutation, any postfix and prefix representation
is covered as well as other syntactical variation.
NLP base IE One component identifies and
highlights protein-protein interactions. It is es-
sential that a phrase describing an interaction
contains a verb or a nominal form describing
an interaction like bind or dimerization. In to-
14PMID 10580148
15PMID 7628084
51
tal, 21 verbs are considered including 10 verbs
which are specific to molecular biology like far-
nesylate. A protein-protein interaction is iden-
tified and tagged, if such a verb phrase connects
two noun phrases and if at least one of the NPs
contains a PGN according to the terminology
tagging.
3 Pipeline of modules shared in
distributed computing
Obviously the presented modules do not work
independently of each other. For example
the protein-protein interaction module uses
NP detection (basic NLP module) which
itself relies on PoS tagging. In addition, NP
detection integrates marked concepts from the
terminology tagging module for the identi-
fication of protein-protein interactions. The
modules form a pipeline equivalent to a UNIX
pipe like "cat input.txt | inputFilter |
sentencise | dictfilter | mutations |
tagger | ...> output.xml". Dependencies
between the modules have to be kept in mind
to determine their correct order. While the
text passes through the pipeline, every filter
picks the XML element it is responsible for and
copies everything else unchanged to the output.
The input filter wraps arbitrary natural lan-
guage text into an XML element describing the
source of the document. Any further module
analyses the text and adds meta data (XML
tags). The following synthesis phase combines
the facts available into larger structures, e.g.
mutations of a gene or protein-protein interac-
tions.
Running the pipeline of modules on a sin-
gle compute node leads to insufficient response
time, since the modules tend to have large mem-
ory footprints. In particular the PoS-tagger as
well as terminology taggers load large dictionar-
ies into memory and therefore have considerable
startup time, whereas steady state operation is
fast. One solution which solves this problem is
to implement each module as a dedicated server
process, which is kept in memory for immediate
response.
REs are applied for processing of data and
meta data. This leads to a special constraint
in the handling of XML tags. It is well known
that REs cannot match recursive parenthesized
structures. As a result, XML elements used
as meta data are not allowed to contain them-
selves. If the XML elements denote parts of
a phrase structure of a natural language sen-
M1 comm Mncomm
data
request
client
. . .
controlling
server
. . .
Figure 1: Processing modules Mi are plugged
into communication components (comm). The
controlling server sends a request to the last
component in the pipe. Each component con-
tacts its predecessor for input and routes it
throught the module. The first component fi-
nally contacts back to the controlling server to
fetch the input and send it down the pipe.
tence, this may in principle be a restriction, but
in practical applications it is not.
We implemented a set of Java classes which
allows to set up distributed pipelined process-
ing. It solves the details of client/server com-
munication to run IE modules in a pipeline and
allows modification to and replacement of mod-
ules through the developer (researcher). As a
result, any class with a method that reads from
an input stream and writes results to an output
stream can serve as a module. In Java terms,
the applied interface is a java.lang.Runnable
calling its methods in void run(). A general
purpose server class is available which, given a
factory method to create the Runnable, handles
all the details of setting up and shutting down
the connections. In particular, connections to
establish a pipeline M1,? . . . ?,Mn, are cre-
ated as follows (fig 1):
The controlling server C generates the
pipeline of modules M1,? . . . ?,Mn (fig. 1).
Typically a component in the web server creates
a reversed list of the modules and adds itself to
the end of the list: Mn, . . . ,M1, C. Then it re-
moves Mn from the list, contacts Mn, sends it
the shortened list Mn?1, . . . ,M1, C and starts
reading input from Mn. Module Mn follows
the same procedure as the server and starts the
Runnable which performs its function receiving
input from the upstream server and writing out-
put to the downstream server. All modules act
the same way and finally M1 contacts the con-
trolling server C to obtain the input data. Ob-
viously C needs to write data to M1 and read
data from Mn in parallel.
52
4 Conclusion
The presented server solution has been set up
to support curators of biomedical facts in their
work. Its modules identify domain knowledge
for molecular biologists and automatically link
into public data resources. We are unaware of
any existing solution like ours, which can inte-
grate modules for information extraction tasks
into a process pipeline based on XML. In col-
laboration with curation teams for UniProt and
COSMIC, the modules will undergo evaluation
for their usefulness in the curation process.
Eventually, information will be automatically
extracted and inserted into public databases.
Every module needs proper evaluation. Mu-
tation extraction already produces reliable data
(Rebholz-Schuhmann et al, 2004), but will
be extended (chromosomal aberrations). The
protein-protein interaction module relies on
chunk parsing and demonstrates how NLP is
integrated as a separate module. Together with
curation teams single modules will be adapted
to their needs. In particular the integration
of controlled vocabularies for species and tissue
types are of strong interest as well as additional
NLP modules, e.g. for the identification of gene
regulation.
A given combination of modules has to con-
sider the dependencies between modules to al-
low efficient handling of information extraction
tasks. When a user requests tagging of UniProt
protein and gene names as well as information
extraction for protein/protein interactions, the
former is actually redundant, because it has to
be run anyway for the latter to work. As a
conclusion the curation teams will propose the
proper combination of modules that they need.
Normalization of identified information is an-
other step. One example is simplication of
acronym definitions, e.g. transformation of
"...androgen receptor (AR) ..." into "<ac
id=?1?>AR</ac>" with meta data accompany-
ing the sentence specifying the expansion "<ex
id=?1?>androgen receptor</ex>". The re-
sult is normalized text which is easier to parse
and thereby leads to better IE results.
The server has been tested on Medline ab-
stracts and on Pdf documents (full papers from
Medline). As (Shah et al, 2003) have shown,
the sections of full text scientific publications
have noticably different information content.
The modular system described allows us to eas-
ily add a module for sectioning of full text pub-
lications.
References
S. Albert, S. Gaudan, H. Knigge, A. Raetsch,
A. Delgado, B. Huhse, H. Kirsch, M. Albers,
D. Rebholz-Schuhmann, and M. Koegl. 2003.
Computer-assisted generation of a protein-
interaction database for nuclear receptors.
Molecular Endocrinology, 17(8):1555?1567.
J.T. Chang, H. Schutze, and R.B. Altman.
2002. Creating an online dictionary of abbre-
viations from medline. Journal of the Amer-
ican Medial Association, 9(6):612?620.
R. Gaizauskas, G. Demetriou, P.J. Artymiuk,
and P. Willett. 2003. Protein structures
and information extraction from biological
texts: The pasta system. Bioinformatics,
19(1):135?143.
D. Hanisch, J. Fluck, H.-T. Mevissen, and
R. Zimmer. 2003. Playing biology?s name
game: identifying protein names in scientific
text. Pacific Symposium on Biocomputing.
D. Rebholz-Schuhmann, S. Marcel, S. Albert,
R. Tolle, G. Casari, and H. Kirsch. 2004.
Automatic extraction of mutations from med-
line and cross-validation with omim. Nucleic
Acids Research, 32(1):135?142.
P.K. Shah, C. Perez-Iratxeta, and M.A. An-
drade. 2003. Information extraction from full
text scientific articles: where are the key-
words? Bioinformatics, 4(20).
J.M. Temkin and M.R. Gilder. 2003. Extrac-
tion of protein interaction information from
unstructured text using a context-free gram-
mar. Bioinformatics, 19(16):2046?2053.
A.S. Yeh, L. Hirschman, and A.A. Morgan.
2003. Evaluation of text data mining for
database curation: lessons learned from the
kdd challenge cup. Bioinformatics, 19:i331?
i339.
53
Annotation and Disambiguation of Semantic Types in Biomedical 
Text: a Cascaded Approach to Named Entity Recognition
Dietrich Rebholz-Schuhmann, Harald Kirsch, 
Sylvain Gaudan, Miguel Arregui
European Bioinformatics Institute (EBI), 
Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{rebholz,kirsch,gaudan,arregui}@ebi.ac.uk
Goran Nenadic
School of Informatics
University of Manchester
Manchester, UK
g.nenadic@manchester.ac.uk
Abstract
Publishers  of  biomedical  journals  in-
creasingly  use  XML  as  the  underlying 
document format.  We present a modular 
text-processing pipeline that inserts XML 
markup  into  such  documents  in  every 
processing step, leading to multi-dimen-
sional markup.  The markup introduced 
is  used  to  identify  and  disambiguate 
named entities of several semantic types 
(protein/gene,  Gene  Ontology  terms, 
drugs  and species)  and to  communicate 
data from one module to the next.  Each 
module  independently  adds,  changes  or 
removes markup, which allows for mod-
ularization  and  a  flexible  setup  of  the 
processing  pipeline.  We  also  describe 
how the cascaded approach is embedded 
in  a  large-scale  XML-based  application 
(EBIMed) used for on-line access to bio-
medical  literature.   We discuss  the  les-
sons  learnt  so  far,  as  well  as  the  open 
problems  that  need  to  be  resolved.   In 
particular,  we  argue  that  the  pragmatic 
and tailored solutions allow for reduction 
in  the need for overlapping annotations 
? although not completely without cost.
1 Introduction
Publishers  of  biomedical  journals  have  widely 
adopted  XML  as  the  underlying  format  from 
which other formats,  such as PDF and HTML, 
are generated.  For example, documents in XML 
format are available from the National Library of 
Medicine1 (Medline abstracts and Pubmed2 Cent-
ral documents), and from BioMed Central3 (full 
text journal articles). Other publishers are head-
ing  into  the  same  direction.   Such  documents 
contain logical markup to organize meta-inform-
1 National Library of Medicine, http://www.nlm.nih.gov/
2 PubMed, http://www.pubmed.org 
3 BioMed Central Ltd, http://www.biomedcentral.com/
ation such as title, author(s), sections, headings, 
citations,  references,  etc.   Inside  the  text  of  a 
document,  XML is  used  for  physical  markup, 
e.g. text in italic or boldface, subscript and super-
script  insertions,  etc.   Manually  generated  se-
mantic markup is available only on the document 
level (e.g. MeSH terms).
One of the most distinguished feature of sci-
entific biomedical literature is that it contains a 
large amount of terms and entities, the majority 
of  which are explained in public electronic data-
bases. Terms (such as names of genes, proteins, 
gene products, organisms, drugs, chemical com-
pounds, etc.) are a key factor for accessing and 
integrating  the  information  stored  in  literature 
(Krauthammer  and  Nenadic,  2004).  Identifica-
tion  and  markup  of  names  and  terms  in  text 
serves several purposes:  
(1) The users profit from highlighted semantic 
types, e.g. protein/gene, drug, species, and from 
links to the defining database for immediate ac-
cess and exploration.
(2) Identified terms facilitate and improve stat-
istical and NLP based text analysis (Hirschman 
et al, 2005; Kirsch et al, 2005).  
In this paper we describe a cascaded approach 
to named-entity recognition (NER) and markup 
in biomedicine that is embedded into EBIMed4, 
an on-line service to access the literature (Reb-
holz-Schuhmann  et  al.,  forthcoming).  EBIMed 
facilitates  both  purposes  mentioned  above.   It 
keeps the annotations provided by publishers and 
inserts  XML  annotations  while  processing  the 
text.  Named entities from different resources are 
identified  in  the  text.  The  individual  modules 
provide annotation of protein names with unique 
identifiers, disambiguation of protein names that 
are  ambiguous  acronyms,  annotation  of  drugs, 
Gene Ontology5 terms and species.  The identi-
fication of protein named entities can be further 
used in an alternative pipeline to identify events 
4 EBIMed, www.ebi.ac.uk/Rebholz-srv/ebimed
5 GO, Gene Ontology, http://geneontology.org, (GO con-
sortium, 2005).
11
such as  protein-protein interactions  and associ-
ations between terms and mutations (Blaschke et 
al., 1999; Rzhetsky et al, 2004; Rebholz-Schuh-
mann  et  al.,  2004;  Nenadic  and  Ananiadou, 
2006).
The rest of the paper is organised as follows. 
In  Section  2  we  briefly  discuss  problems with 
biomedical NER.  The cascaded approach and an 
online text mining system are described in sec-
tions 3 and 4 respectively.  We discuss the les-
sons learnt from the on-line application and re-
mainig open problems in Section 5, while con-
clusions are presented in Section 6.
2 Biomedical Named Entity Recognition 
Terms and named-entities (NEs) are the means 
of scientific communication as they are used to 
identify  the  main  concepts  in  a  domain.  The 
identification  of  terminology in  the  biomedical 
literature is one of the most challenging research 
topics  both  in  the  NLP  and  biomedical  com-
munities (Hirschman et al,  2005; Kirsch et al, 
2005).
Identification  of  named  entities  (NEs)  in  a 
document can be viewed as a three-step proced-
ure (Krauthammer and Nenadic, 2004).  In the 
first step, single or multiple adjacent words that 
indicate the presence of domain concepts are re-
cognised (term recognition). In the second step, 
called  term categorisation, the recognised terms 
are classified into broader domain classes (e.g. as 
genes, proteins, species). The final step is  map-
ping of terms into referential databases. The first 
two steps are commonly referred to as named en-
tity recognition (NER).
One of the main challenges in NER is a huge 
number of new terms and entities that appear in 
the  biomedical  domain.  Further,  terminological 
variation, recognition of boundaries of multiword 
terms, identification of nested terms and ambigu-
ity of terms are the difficult issues when mapping 
terms from the literature to biomedical database 
entries  (Hirschman  et  al.,  2005;  Krauthammer 
and Nenadic, 2004). 
On one hand, NER in the biomedical domain 
(in particular  the recognition part)  profits  from 
large,  freely available terminological  resources, 
which  are  either  provided  as  ontologies  (e.g. 
Gene Ontology, ChEBI6, UMLS7) or result from 
biomedical  databases containing named entities 
(e.g.  UniProt/Swiss-Prot8).  On  the  other  hand, 
combining sets of terms from different termino-
6 ChEBI, Chemical Entities of Biological Interest, 
http://www.ebi.ac.uk/chebi/m
7  UMLS, Unified Medical Language System 
http://www.nlm.nih.gov/research/umls/, (Browne et al, 
2003).
logical resources leads to naming conflicts such 
as homonymous use of names and terminological 
ambiguities. The most obvious problem is when 
the same span of text is assigned to different se-
mantic types (e.g. ?rat? denotes a species and a 
protein). In this case, there are three types of am-
biguities:  
(Amb1) A name is used for different entries in 
the  same database,  e.g.  the  same protein name 
serves  for  a  given  protein  in  different  species 
(Chen et al, 2005).
(Amb2) A name is used for entries in multiple 
databases and thus represents different types, e.g. 
?rat? is a protein and a species.
(Amb3) A name is not only used as a biomed-
ical term but also as part of common English (in 
contrast  to  the  biomedical  terminology),  e.g. 
?who? and  ?how?,  which  are  used  as  protein 
names.
In some cases (i.e. Amb2), broader classifica-
tion can help to disambiguate between different 
entries  (e.g.  differentiate  between  ?CAT? as  a 
protein, animal or medical device). However, it 
is ineffective in situations where names can be 
mapped to several different entries in the same 
data  source.  In  such  situations,  disambiguation 
on the resource level is needed (see, for example, 
(Liu et al, 2002) for disambiguation of terms as-
sociated with several entries in the UMLS Meta-
thesaurus).
In many solutions, the three steps in biomedic-
al NER (namely, recognition, categorisation and 
mapping  to  databases)  are  merged  within  one 
module. For example, using an existing termino-
logical database for recognition of NEs, effect-
ively  leads  to  complete  term  identification  (in 
cases where there are no ambiguities). Some re-
searchers, however, have stressed the advantages 
of tackling each step as a separate task, pointing 
at different sources and methods needed to ac-
complish each of the subtasks (Torii et al, 2003; 
Lee et al, 2003). Also, in the case of modularisa-
tion,  it  is  easier  to  integrate  different  solutions 
for each specific problem. However, it has been 
suggested  that  whether  a  clear  separation  into 
single steps would improve term identification is 
an  open  issue  (Krauthammer  and  Nenadic, 
2004). In this paper we discuss a cascaded, mod-
ular approach to biomedical NER.
3 Biomedical  NER based on XML an-
notation:  Modules in a pipeline
In this Section we present a modular approach to 
identification, disambiguation and annotation of 
8 UniProt, http://www.ebi.uniprot.org/, (Bairoch et al, 
2005); Swiss-Prot, http://ca.expasy.org/sprot/
12
several  biomedical  semantic  types  in  the  text. 
Full identification of NEs and resolving ambigu-
ities in particular, may require a full parse tree of 
a  sentence  in  addition  to  the  analysis  of  local 
context  information.  On  the  other  hand,  full 
parse trees may be only derivable after NEs are 
resolved.  Methods to efficiently overcome these 
problems are not yet available today and in order 
to come up with an applicable solution,  it  was 
necessary to choose a more pragmatic approach.
We  first  discuss  the  basic  principles  and 
design of the processing pipeline, which is based 
on  a  pragmatic  cascade  of  modules,  and  then 
present each of the modules separately.
3.1 Modular  design  of  a  text  processing 
pipeline
Our methodology is based on the idea of separat-
ing the process into clearly defined functions ap-
plied one after  another  to  text,  in  a processing 
pipeline characterized  by  the  following  state-
ments:
(P1)  The complete  text  processing task con-
sists of separate and independent modules.
(P2)  The  task  is  performed  by  running  all 
modules exactly once in a fixed sequence.
(P3) Each module operates continuously on an 
input  stream and  performs  its  function  on 
stretches or  ?windows? of text  that  are usually 
much smaller than the whole input.  As soon as a 
window is  processed,  the module  produces  the 
resulting output.
(P4) After the startup phase, all modules run 
in parallel.  Incoming requests for annotation are 
accepted by a master process that ensures that all 
required modules are approached in the right or-
der.
(P5) Communication of information between 
the modules is strictly downstream and all meta-
information is contained in the data stream itself 
in the form of XML markup.
An instance of a processing pipeline (which is 
actually  embedded in  EBIMed)  is  presented in 
Figure 1. The modules M-1 to M-8 are run in this 
order,  and  no  communication  between them is 
needed apart  from streaming the  text  from the 
output  of  one  module  to  the  input  of  another. 
The text contains the meta-data as XML markup. 
The modules are described below.
Figure 1. A processing pipeline embedded in 
EBIMed
Although  this  is  the  standard  pipeline  for 
EBIMed, it is possible to re-arrange the modules 
to  favour  identification  of  specific  semantic 
types. More precisely, in our modular approach, 
after identification of a term in the text, disam-
biguation only decides whether the term is of that 
type or not. If it is not, the specific annotation is 
removed and left to the downstream modules to 
tag the term differently.  While this  requires  n 
identification steps, adding identification of new 
types is independent of modules already present. 
However, the prioritization of semantic types is 
enforced  by  the  order  of  the  associated  term 
identification modules.  
3.2 Input documents and pre-processing
Input  documents  are  XML-formatted  Medline 
abstracts as provided from the National Library 
of  Medicine  (NLM).   The  XML  structure  of 
Medline abstracts includes meta information at-
tached  to  the  original  document,  such  as  the 
journal, author list, affiliations, publication dates 
as well as annotations inserted by the NLM such 
as  creation  date  of  the  Medline  entry,  list  of 
chemicals associated with the document, as well 
as related MeSH headings. 
The  text  processing  modules  are  only  con-
cerned with the document  parts  that  consist  of 
natural  language  text.   In  Medline  abstracts, 
these stretches of text are marked up as  Article-
Title and AbstractText.  Inside these elements we 
add  another  XML element,  called  text,  to  flag 
natural language text independent of the original 
input  document format  (module  M-1 in  Figure 
1).  Thereby the subsequent text processing mod-
ules become independent of the document struc-
ture: other document types, e.g. BioMed Central 
13
full  text  papers,  can  easily  be  fed  into  the 
pipeline providing a simple adaptation of the in-
put pre-processor. 
As  a  final  pre-processing  step  (M-2),  sen-
tences  are  identified  and  marked  using  the 
<SENT> tag.
3.3 Finding protein names in text
For identification of protein names (M-3 in Fig-
ure  1),  we  use  an  existing  protein  repository. 
UniProt/Swiss-Prot  contains  roughly  190,000 
protein/gene  names  (PGNs)  in  database  entries 
that also annotate proteins with protein function, 
species  and  tissue  type.   PGNs  from 
UniProt/Swiss-Prot are matched with regular ex-
pressions which account for morphological vari-
ability.  These terms are tagged using the <z:uni-
prot> tag (see Figure 2).  The list  of  identifiers 
(ids attribute) contains the accession numbers of 
the mentioned protein in the UniProt/Swiss-Prot 
database.  All  synonyms  from a  database  entry 
are kept,  and in the case of homonymy, where 
one name refers to several  database entries,  all 
accession numbers are stored.  The pair consist-
ing of the database name and the accession num-
ber(s) forms a unique identifier (UID) that rep-
resents  the  semantics  of  the  term  and  can  be 
trivially  rewritten  into  a  URL  pointing  to  the 
database entry. Each entity also contains the at-
tribute  fb which  provides  the  frequency of  the 
term in the British National Corpus (BNC).
3.4 Resolving (some) protein name ambigu-
ities
The approach to finding names that we presented 
can create three types of ambiguities mentioned 
above in Section 2.
In the current implementation,  Amb1 (ambi-
guity  within a  given resource)  is  not  resolved. 
Rather, the links to  all entries in the same data-
base are maintained.  Amb2 and Amb3 are par-
tially  resolved  for  protein/gene  names  as  ex-
plained below (steps M-4 and M-5).  Note that 
Amb2 is  resolved  on  ?first-come  first-serve? 
basis, meaning that an annotation introduced by 
one module is not overwritten by a subsequent 
module.
Many protein names are indeed or at least look 
like abbreviations. It has been proved that ambi-
guities of abbreviations and acronyms found in 
Medline abstracts can be automatically resolved 
with high accuracy  (Yu et  al.,  2002;  Schwartz 
and Hearst, 2003; Gaudan et al, 2005). 
<SENT sid=?2? pm=?.?> Aberrant 
Wnt signaling, which results from 
mutations of either <z:uniprot fb=?0? ids=?P26233,P35222,P35223, 
P35224,Q02248, Q9WU82?>beta-
catenin</z:uniprot> or adenomat-
ous polyposis coli (<z:uniprot fb=?28? ids=?P25054?>APC </z:uni-prot>), renders <z:uniprot fb=?0? ids= ?P26233,P35222, P35223, 
P35224,Q02248, Q9WU82?> beta-
catenin</z:uniprot> resistant to 
degradation, and has been associ-
ated with multiple types of human 
cancers 
</SENT>
Figure 2. XML annotation of UniProt/Swiss-Prot 
proteins  .
In our approach (Gaudan et al, 2005) all ac-
ronyms from Medline have been gathered togeth-
er  
with their expanded forms, called senses.  In ad-
dition all morphological and syntactical variants 
of a known expanded form have been extracted 
from Medline.   Expanded forms were  categor-
ised  into  classes  of  semantically  equivalent 
forms.   Feature  representations  of  Medline  ab-
stracts containing the acronym and the expanded 
form were used to train support vector machines 
(SVMs).   Disambiguation of acronyms to their 
senses in Medline abstracts based on the SVMs 
was achieved at an accuracy of above 98%.  This 
was independent from the presence of the expan-
ded form in the Medline abstract.  This disam-
biguation solution lead to the solution integrated 
into the processing pipeline.  
A potential protein has to be evaluated against 
three possible outcomes: either a name is an ac-
ronym and can be resolved as (a) a protein or (b) 
not a protein, or (c) a name cannot be resolved. 
To  distinguish  cases  (a)  and  (b)  the  document 
content  is  processed  to  identify  the  expanded 
form of the acronym and to check whether the 
expanded form refers to a protein name.  In case 
of (c), the frequency of the name in the  British 
National  Corpus  (BNC)  is  compared  with  a 
threshold.   If  the  frequency is  higher  than  the 
threshold, the name is assumed not to be a pro-
tein name.  The threshold was chosen not to ex-
clude important protein names that have already 
entered common English (such as insulin).
The disambiguation module (M-4) runs on the 
results of the previous module that performs pro-
tein-name  matching  and  indiscriminately  as-
sumes each match to  be  a  protein name.  The 
14
module  M-4 marks  up all  known acronym ex-
pansions in the text and combines the two pieces 
of  information:  a  marked  up  protein  name  is 
looked up in the list of abbreviations.  If the ab-
breviation has an expansion that is marked up in 
the vicinity and denotes a protein name, the ab-
breviation is verified as a protein name (case (a) 
above)  by  adding  an  attribute  with  a  suitable 
value to the protein tag.  The annotation also in-
cludes  the  normalised  form  of  the  acronym, 
which serves as an identifier for further database 
lookups. Similarly, if the expansion is clearly not 
a protein name, the same attribute is used with 
the according value.
Finally, the module M-5 removes the protein 
name markup if the name is either (b) clearly not 
a  protein,  or  in case (c)  has a BNC frequency 
beyond the threshold.
3.5 Finding other names in text
Further modules (M-6, M-7 and M-8 in Fig. 1) 
perform  matching  and  markup  for  drugs  from 
MedlinePlus9,  species  from Entrez  Taxonomy10 
and terms from the Gene Ontology (GO).  As for 
proteins,  the  semantic  type  is  signified  by  the 
element name and a unique ID referencing the 
source database is added as an attribute.  Disam-
biguation for these names and terms is, however, 
not yet available.
Finding GO ontology terms in text can be dif-
ficult, as these names are typically ?descriptions? 
rather than real terms (e.g. GO:0016886,  ligase 
activity,  forming  phosporic  ester  bonds),  and 
therefore  are  not  likely  to  appear  in  text  fre-
quently  (McCray  et  al.,  2002;  Verspoor  et  al., 
2003; Nenadic et al, 2004).
Figure 3 shows an example of a sentence an-
notated for semantic types and POS information 
using the pipeline from the Figure 1.  Note that 
POS tags are inside the type tags although type 
annotation has been performed prior to the POS 
tagging.
3.6 Other modules in the pipeline
The modular text processing pipeline of EBIMed 
is currently being extended to include other mod-
ules. The part-of-speech tagger (POS-tagger) is a 
separate module and combines tokenization and 
POS annotation.  It leaves previously annotated 
entities  as  single  tokens,  even  for  multi-word 
terms,  and  assigns  a  noun  POS  tag  to  every 
named entity.
9 MedlinePlus, National Library of Medicine, http://www.n-
lm.nih.gov/medlineplus/
10 Entrez Taxonomy, National Center for Biotechnology In-
formation, http://www.ncbi.nlm.nih.gov/entrez/
Shallow parsing is introduced as another layer 
in the multidimensional annotation of biomedical 
documents.  After the NER modules, the shallow 
parsing modules extract events of protein-protein 
interactions.  Shallow parsing basically annotates 
noun  phrases  (NP)  and  verb  groups.   Noun 
phrases  that  contain  a  protein  name  receive  a 
modified NP tag (Protein-NP) to simplify finding 
of  protein-protein interaction phrases.   Patterns 
of Protein-NPs in conjunction with selected verb 
groups are annotated as final result.
<abs id='1' db='unknown'>
<text><SENT sid="0" pm="."><tagged>
<tok><sur> </sur><lem cat="bos" 
mor=""></lem></tok><z:uniprot fb="0" ids="P50144,P50145"><tok><sur>Cholecystokinin</sur><lem 
cat="n" mor=":e">cholecystokinin</lem></tok> </z:uniprot>
<tok><sur>and</sur><lem cat="cnj" 
mor=":K">and</lem></tok><z:uniprot fb="4" ids="O02686,P01350"><tok><sur>gastrin</sur><lem cat="n" 
mor=":e">gastrin</lem></tok></z:uniprot>
<tok><sur>differed</sur><lem cat="v" 
mor=":V:P">differ</lem></tok>
<tok><sur>in</sur><lem cat="prep" 
mor="">in</lem></tok>
<tok><sur>stimulatin</sur><lem cat="n" 
mor=":e:m">stimulatin</lem></tok><z:uniprot fb="4" ids="O02686,P01350"><tok><sur>gastrin</sur><lem cat="n" 
mor=":e">gastrin</lem></tok></z:uniprot><z:go ids="GO:0046903" 
onto="biological_process"><tok><sur>secretion</sur><lem cat="n" 
mor=":e">secretion</lem></tok></z:go>
<tok><sur>in</sur><lem cat="prep" 
mor="">in</lem></tok><z:species ids="9986"><tok><sur>rabbit</sur><lem cat="n" 
mor=":e">rabbit</lem></tok></z:species>
<tok><sur>gastric</sur><lem cat="adj" 
mor=":b">gastric</lem></tok>
<tok><sur>glands</sur><lem cat="n" 
mor=":m">gland</lem></tok>
<tok><sur>.</sur><lem cat="eos" 
mor=""></lem></tok>
</tagged></SENT>
</text>
</abs>
Figure 3.  XML annotation of a sentence con-
taining different semantic types and POS tags. 
15
4 EBIMed
This cascaded approach to NER has been incor-
porated into EBIMed, a system for mining bio-
medical literature.
EBIMed is a service that combines document 
retrieval  with  co-occurrence-based  summariza-
tion  of  Medline  abstracts.  Upon  a  keyword 
query, EBIMed retrieves abstracts from EMBL-
EBI?s installation of Medline and filters for bio-
medical terminology.  The final result is organ-
ised in a view displaying pairs of concepts.  Each 
pair co-occurs in at least one sentence in the re-
trieved  abstracts.  The  findings  (e.g. 
UniProt/Swiss-Prot  proteins,  GO  annotations, 
drugs and species) are listed in conjunction with 
the  UniProt/Swiss-Prot  protein  that  appears  in 
the same biological context.  All terms, retrieved 
abstracts and extracted sentences are automatic-
ally linked to contextual information, e.g. entries 
in biomedical databases. 
The annotation modules are also available via 
HTTP  request  that  allows  for  specification  of 
which modules to run (cf. Whatizit11).  Note that 
with  suitable  pre-processing  to  insert  the 
<text> tags, even well formed HTML can be 
processed. 
5 Lessons Learnt so far
Our  text  mining  solution  EBIMed successfully 
applies multi-dimensional markup in a pipeline 
of text processing modules to facilitate online re-
trieval  and mining of  the  biomedical  literature. 
The final goal is semantic annotation of biomed-
ical  terms  with  UID,  and  ? in  the  next  step  ? 
shallow parsing based text  processing for  rela-
tionship  identification.   The  following  lessons 
have been learnt during design, implementation 
and use of our system.  
The end-users expect to see the original docu-
ment at all times and therefore we have to rely on 
proper  formatting  of  the  original  and  the  pro-
cessed text.  Consequently, when adding semant-
ic information, all  other meta-information must 
be  preserved  to  allow  for  proper  rendering  as 
similar  as  possible  to  the  original  document. 
Therefore,  our  approach  does  not  remove  any 
pre-existing annotations supplied by the publish-
er, i.e. the original document could be recovered 
by removing all introduced markup.
All modules only process sections of the docu-
ment containing the natural language text, which 
improves modularisation.  The document struc-
ture is irrelevant to single modules and facilitates 
reading  and  writing  to  the  input  and  output 
11 http://www.ebi.ac.uk/Rebholz-srv/whatizit/pipe
stream, respectively, without taking notice of the 
beginning and/or the end of a single document. 
All  information exchanged between modules is 
contained in the data stream. This facilitates run-
ning all the modules in a given pipeline in paral-
lel, after an initial start-up. Even more, the mod-
ules can be distributed on separate machines with 
no implementation overheads for the communic-
ation over the network.  Adding more modules 
with their own processors does not significantly 
impair overall runtime behaviour for large data-
sets and leads to fast text processing throughput 
combined with a reasonable ? albeit not yet per-
fect ? quality, which allows for new and prac-
tically  useful  text  mining  solutions  such  as 
EBIMed.
Modularisation  of  the  text  processing  tasks 
leads to improved scalability and maintainability 
inherent to all modular software solutions.  In the 
case of the presented solution, the modular ap-
proach allows for a selection of the setup and or-
dering of the modules, leading to a flexible soft-
ware design, which can be adapted to different 
types of documents and which allows for an (in-
cremental)  replacement  of  methods  to  improve 
the quality of the output.  This can also facilitate 
improved  interoperability  of  XML-based  NLP 
tools.
Semantic  annotation  of  named  entities  and 
terms  blends  effectively  with  logical  markup, 
simply because there is no overlap between doc-
ument  structure  and  named  entities  and  terms. 
On the other hand, some physical markup (such 
as <i> in the BMC corpus) is in some documents 
used to highlight names or terms of a semantic 
type, e.g. gene names.  With consistent semantic 
markup, this kind of physical tags could be aban-
doned to be replaced by external style informa-
tion.  However, some semantic annotations still 
must be combined with physical markup as in the 
term B-sup that initially was annotated by a pub-
lisher  as  <b>B</b>-sup and  that  now  (after 
NER)  would  be  marked  as 
<z:uniprot><b>B</b>-sup</z:uniprot>.
Matching  of  names  of  a  semantic  type,  e.g. 
protein/gene,  is  done on a ?longest  of  the left-
most? basis and prioritization of semantic types 
is enforced by the order of the term identification 
modules.   Both  choices  lead  to  the  result  that 
overlapping annotations are preempted and that 
annotations  automatically  endorse  a  link  to  a 
unique identifier, unless there are ambiguity on 
the  level  of  biomedical  resource..  This  type of 
ambiguity is not resolved in our text processing 
solution.  Instead, for a given biomedical term, 
links to all  entries referring to this  term in the 
same database are kept.  
16
One approach to the disambiguation of Amb2 
(multiple resources)  and  Amb3 (common Eng-
lish words) ambiguities would be to integrate all 
terms into  one massive dictionary,  identify  the 
strings in the text and then disambiguate between 
n semantic types.  This would require the disam-
biguation module be trained to distinguish all se-
mantic types. If a new type is added, the disam-
biguation  module  would  need  to  be  retrained, 
which limits the possibilities for expansion and 
tailoring of text mining solutions.
Open Problems: We consider two categories of 
open  problems:  NLP-based  and  XML-based 
problems.
Bio  NLP-based  problems include  challenges 
in recognition and disambiguation of biomedical 
names in text. One of the main issues in our ap-
proach  is  annotation  of  compound  and  nested 
terms.  The  presented  methodology can  lead  to 
the following annotations:
1. the head noun belongs to the same semantic 
type, but is not part of the protein name (as 
represented in the terminological resource): 
<z:uniprot>Wnt-2</z:uniprot> protein
2. the head noun belongs to a different semantic 
type not covered by any of the available ter-
minological resources:
<z:uniprot>WNT8B</z:uniprot> mRNA
3. a compound term consists of terms from dif-
ferent semantic types, but its semantic type is 
not known:
<z:uniprot  fb=?0?  ids=???>beta-
catenin</z:uniprot>  <z:go  ids=??? 
onto= ???>binding </z:go> domain
Therefore,  an important  open problem is the 
annotation of nested terms where an entity name 
is part of a larger term that may or may not be in 
one of the  dictionaries.  Once the  inner term is 
marked up with inline annotation, simple string 
pattern matching (utilised in our approach) can-
not be used easily to find the outer, because the 
XML structure is in the way.  A more effective 
solution could be  a combination of inline  with 
stand-off annotation.
Further, in a more complex case such as in
htr-wnt-<uniprot>A protein</uniprot>
neither wnt nor htr refer to a single protein but 
to a protein family, and whereas A protein is 
a known protein, this is not the case for wnt-A. 
The most obvious annotation <uniprot>htr-
wnt-A protein</uniprot> cannot be re-
solved  by  the  terminology  from  the 
UniProt/Swiss-Prot  database,  as  it  simply  does 
not exist in the database.
More work is also needed on disambiguation 
of  terms  that  correspond  to  common  English 
words.  
Annotation (i.e. XML)-based problems mainly 
relate to an open question whether different tag 
names should be used for various semantic types, 
or semantic types should be represented via at-
tributes  of  a  generalised  named entity  or  term 
tag.  In EBIMed, specific tags are used to denote 
specific semantic types.  A similar challenge is 
how to treat and make use of entities such as in-
line references, citations and formulas (typically 
annotated in journals), which are commonly ig-
nored by NLP modules.
The most important issue, however, is how to 
represent still unresolved ambiguities, so that an-
notations might be modified at a later stage, e.g. 
when POS information or even the full parse tree 
is available. This also includes the issues on kind 
of information that should be made available for 
later  processing.  For  example,  as  (compound) 
term identification is done before POS tagging, 
an  open  question  is  whether  POS  information 
should be assigned to individual components of a 
compound term (in addition to the term itself), 
since this information could be used to complete 
NER or adjust the results in a later stage.  
6 Conclusions 
In  this  paper,  we  have described  a  pipeline  of 
XML-based modules for  identification and dis-
ambiguation  of  several  semantic  types  of  bio-
medical  named entities.  The pipeline  processes 
and semantically enriches documents by adding, 
changing  or  removing  annotations.   More  pre-
cisely, the documents are augmented with UIDs 
referring to referential databases.  In the course 
of the processing, the number of annotated NEs 
increases and the quality of  the annotation im-
proves.  Thus, one of the main issues is to repres-
ent still  unresolved ambiguities consistently, so 
that  the  following  modules  can  perform  both 
identification  and  disambiguation  of  new  se-
mantic types. As subsequent modules try to add 
new  semantic  annotations,  prioritization  of  se-
mantic types is enforced by the order of the term 
identification modules.  
We have shown that such approach can be em-
ployed in a real-world, online information min-
ing  system EBIMed.   The  end-users  expect  to 
view the original layout of the documents at all 
times, and thus the solution needs to provide an 
efficient multidimensional markup that preserves 
and combines existing markup (from publishers) 
with semantic NLP-derived tags.  Since, in the 
biomedical  domain,  it  is  essential  to  provide 
17
links from term and named-entity occurrences to 
referential databases, EBIMed provides identific-
ation and disambiguation of such entities and in-
tegrates text with other knowledge sources.
The existing solution to annotate only longest 
non-overlapped entities is useful for real world 
use scenarios, but we also need ways to improve 
annotations  by  representing  nested  and  over-
lapped terms.
Acknowledgements
The development of EBIMed is supported by the 
Network of Excellence ?Semantic Interoperabil-
ity  and  Data  Mining  in  Biomedicine?  (NoE 
507505).   Medline  abstracts  are provided from 
the National  Library of Medicine (NLM, Beth-
esda,  MD,  USA)  and  PubMed  is  the  premier 
Web portal to access the data.
Sylvain Gaudan is supported by an ?E-STAR? 
fellowship funded by the EC?s FP6 Marie Curie 
Host fellowship for Early Stage Research Train-
ing  under  contract  number  MESTCT-  2004-
504640. Goran Nenadic acknowledges supported 
from the UK BBSRC grant ?Mining Term Asso-
ciations  from Literature  to  Support  Knowledge 
Discovery in Biology? (BB/C007360/1).
EBI thanks IBM for the grant of an IBM eS-
erver BladeCenter for use in its research work.
References 
A. Bairoch, R. Apweiler, C.H. Wu, W.C. Barker, B. 
Boeckmann, S. Ferro, E. Gasteiger, H. Huang, R. 
Lopez, M. Magrane, M.J. Martin, D.A. Natale, C. 
O?Donovan, N. Redaschi and L.S. Yeh.  2005. The 
Universal  Protein  Resource  (UniProt).   Nucleic  
Acids Research, 33(Database issue):D154-9.
C.  Blaschke,  M.A.  Andrade,  C.  Ouzounis  and  A. 
Valencia. 1999. Automatic extraction of biological 
information from scientific text: Protein-protein in-
teractions. Proc. ISMB, 7:60?7.
A.C. Browne, G. Divita, A.R Aronson and A.T. Mc-
Cray. 2003. UMLS language and vocabulary tools. 
AMIA Annual Symposium Proc., p. 798.
L. Chen, H. Liu and C. Friedman. 2005. Gene name 
ambiguity of eukaryotic nomenclature.  Bioinform-
atics, 21(2):248-56
S.  Gaudan,  H.  Kirsch  and  D.  Rebholz-Schuhmann. 
2005.  Resolving abbreviations to their  senses in 
Medline.  Bioinformatics, 21(18):3658-64
GO Consortium.  2006.   The Gene Ontology (GO) 
project  in  2006.  Nucleic  Acids  Research, 
34(suppl_1):D322-D326.
L. Hirschman, A. Yeh, C. Blaschke and A. Valencia. 
2005.  Overview  of  BioCreAtIvE:  critical  assess-
ment of information extraction for biology.  BMC 
Bioinformatics, 6 Suppl 1:S1.
H.  Kirsch,  S.  Gaudan  and  D.  Rebholz-Schuhmann. 
2005.  Distributed modules for text annotation and 
IE applied to the biomedical domain.  Internation-
al  Journal  Medical  Informatics.   (doi:10.1016/ 
j.ijmedinf.2005.06.011)
M. Krauthammer and G. Nenadic. 2004. Term identi-
fication in the biomedical literature.  Journal Bio-
medical Informatics, 37(6):512-26.
K.  Lee,  Y.  Hwang,  and  H.  Rim.  2003.  Two-Phase 
Biomedical  NE  Recognition  based  on  SVMs. 
Proc. of NLP in Biomedicine, ACL 2003. p. 33-40.
H. Liu, S.B. Johnson, and C. Friedman, 2002. Auto-
matic resolution of ambiguous terms based on ma-
chine  learning  and  conceptual  relations  in  the 
UMLS.  J  Am Med  Inform Assoc,  2002.  9(6):  p. 
621-36.
A. McCray, A. Browne and O. Bodenreider O. 2002. 
The  lexical  properties  of  Gene  ontology  (GO). 
Proceedings of AMIA 2002. 2002:504-8.
G. Nenadic, I. Spasic, and S. Ananiadou. 2005. Min-
ing  Biomedical  Abstracts:  What?s  in  a  Term?, 
LNAI Vol. 3248, pp. 797-806, Springer-Verlag
G.  Nenadic  and  S.  Ananiadou.  2006.  Mining  Se-
mantically Related Terms from Biomedical Literat-
ure. ACM Transactions on ALIP, 01/2006 (Special 
Issue Text Mining and Management in  Biomedi-
cine)
xD. Rebholz-Schuhmann, H. Kirsch, M. Arregui,  S. 
Gaudan, M. Rynbeek and P. Stoehr. (forthcoming) 
Identification of proteins and their biological con-
text  from  Medline:   EBI?s  text  mining  service 
EBIMed. 
D.  Rebholz-Schuhmann,  S.  Marcel,  S.  Albert,  R. 
Tolle, G. Casari and H. Kirsch.  2004. Automatic 
extraction of  mutations  from Medline and cross-
validation with OMIM.  Nucleid Acids Research, 
32(1):135?142.
A. Rzhetsky, I. Iossifov, T. Koike, M. Krauthammer, 
P. Kra, et al  2004. GeneWays: A system for ex-
tracting,  analyzing,  visualizing,  and  integrating 
molecular  pathway  data.  Journal  Biomedical  In-
formatics, 37:43?53.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in 
biomedical text. Proceedings of Pac Symp Biocom-
put. 2003. p. 451-62.
M. Torii, S. Kamboj and K. Vijay-Shanker. 2003. An 
Investigation  of  Various  Information  Sources  for 
Classifying  Biological  Names.  Proceedings  of 
NLP in Biomedicine, ACL 2003. p. 113-120
CM Verspoor, C. Joslyn and G. Papcun. 2003. The 
Gene  ontology  as  a  source  of  lexical  semantic 
knowledge for  a  biological  natural  language pro-
cessing  application.  Proc.  of  Workshop  on  Text  
Analysis and Search for Bioinformatics, SIGIR 03
H. Yu, G. Hripcsak and C. Friedman. 2002. Mapping 
abbreviations to full forms in biomedical articles. J 
Am Med Inform Assoc, 2002. 9(3): p. 262-72. 
18
Proceedings of the Workshop on BioNLP, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
How Feasible and Robust is the Automatic Extraction of Gene Regulation
Events ? A Cross-Method Evaluation under Lab and Real-Life Conditions
Udo Hahn1 Katrin Tomanek1 Ekaterina Buyko1 Jung-jae Kim2 Dietrich Rebholz-Schuhmann2
1Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{udo.hahn|katrin.tomanek|ekaterina.buyko}@uni-jena.de
2EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, UK
{kim|rebholz}@ebi.ac.uk
Abstract
We explore a rule system and a machine learn-
ing (ML) approach to automatically harvest
information on gene regulation events (GREs)
from biological documents in two different
evaluation scenarios ? one uses self-supplied
corpora in a clean lab setting, while the other
incorporates a standard reference database of
curated GREs from REGULONDB, real-life
data generated independently from our work.
In the lab condition, we test how feasible
the automatic extraction of GREs really is
and achieve F-scores, under different, not di-
rectly comparable test conditions though, for
the rule and the ML systems which amount
to 34% and 44%, respectively. In the REGU-
LONDB condition, we investigate how robust
both methodologies are by comparing them
with this routinely used database. Here, the
best F-scores for the rule and the ML systems
amount to 34% and 19%, respectively.
1 Introduction
The extraction of binary relations from biomedical
text has caught much attention in the recent years.
Progress on this and other tasks has been monitored
in challenge competitions such as BIOCREATIVE I
and II,1 which dealt with gene/protein names and
and protein-protein interaction.
The BIOCREATIVE challenge and other related
ones have shown at several occasions that partici-
pants continue to use two fundamentally different
1http://biocreative.sourceforge.net/
systems: symbolic pattern-based systems (rule sys-
tems), on the one hand, and feature-based statisti-
cal machine learning (ML) systems, on the other
hand. This has led to some rivalry with regard to the
interpretation of their performance data, the costs
of human efforts still required and their scalability
for the various tasks. While rule systems are of-
ten hand-crafted and fine-tuned to a particular ap-
plication (making a major manual rewrite often nec-
essary when the application area is shifted), ML
systems are trained automatically on manually an-
notated corpora, i.e., without manual intervention,
and thus have the advantage to more easily adapt to
changes in the requested identification tasks. Time
costs (human workload) are thus shifted from rule
design and adaptation to metadata annotation.
Text mining systems as usually delivered by
BioNLP researchers render biologically relevant en-
tities and relations on a limited set of test documents
only. While this might be sufficient for the BioNLP
community, it is certainly insufficient for bioinfor-
maticians and molecular biologists since they re-
quire large-scale data with high coverage and reli-
ability. For our analysis, we have chosen the topic
of gene regulatory events in E. coli, which is a do-
main of very active research and grand challenges.2
Currently the gold standard of the existing body of
knowledge of such events is represented by the fact
database REGULONDB.3 Its content has been man-
2The field of gene regulation is one of the most prominent
topics of research and often mentioned as one of the core fields
of future research in molecular biology (cf, e.g., the Grand
Challenge I-2 described by Collins et al (2003)).
3http://regulondb.ccg.unam.mx/
37
ually gathered from the scientific literature and de-
scribes the curated computational model of mecha-
nisms of transcriptional regulation in E. coli. Having
this gold standard in mind, we face the challenging
task to automatically reproduce this content from the
available literature, to enhance this content with re-
liable additional information and to update this re-
source as part of a regular automatic routine.
Hence, we first explore the feasibility and per-
formance of a rule-based and an ML-based system
against special, independently created corpora that
were generated to enable measurements under clean
experimental lab conditions. This part, due to dif-
ferent experimental settings, is not meant as a com-
parison between both approaches though. We then
move to the even more demanding real-life scenario
where we evaluate and compare these solutions for
the identification of gene regulatory events against
the REGULONDB data resource. This approach tar-
gets the robustness of the proposed text mining so-
lutions from the perspectives of completeness, cor-
rectness and novelty of the generated results.
2 Related Work
Considering relation extraction (RE) in the biomed-
ical domain, there are only few studies which deal
primarily with gene regulation. Yang et al (2008)
focus on the detection of sentences that contain
mentions of transcription factors (proteins regulat-
ing gene expression). They aim at the detection
of new transcription factors, while relations are not
taken into account. In contrast, S?aric? et al (2004)
extract gene regulatory networks and achieve in the
RE task an accuracy of up to 90%. They disregard,
however, ambiguous instances, which may have led
to the low recall around 20%. The Genic Interaction
Extraction Challenge (Ne?dellec, 2005) was orga-
nized to determine the state-of-the-art performance
of systems designed for the detection of gene regula-
tion interactions. The best system achieved a perfor-
mance of about 50% F-score. The results, however,
have to be taken with care as the LLL corpus used in
the challenge is of extremely limited size.
3 Extraction of Gene Regulation Events
Gene regulation is a complex cellular process that
controls the expression of genes. These genes are
then transcribed into their RNA representation and
later translated into proteins, which fulfill various
tasks such as maintaining the cell structure, enabling
the generation of energy and interaction with the en-
vironment.
The analysis of the gene regulatory processes is
ongoing research work in molecular biology and af-
fects a large number of research domains. In par-
ticular the interpretation of gene expression profiles
from microarray analyses could be enhanced using
our understanding of gene regulation events (GREs)
from the literature.
We approach the task of the automatic extraction
of GREs from literature from two different method-
ological angles. On the one hand, we provide a set of
hand-crafted rules ? both for linguistic analysis and
conceptual inference (cf. Section 3.1), the latter be-
ing particularly helpful in unveiling only implicitly
stated biological knowledge. On the other hand, we
supply a machine learning-based system for event
extraction (cf. Section 3.2). No regularities are spec-
ified a priori by a human although, at least in the su-
pervised scenario we have chosen, this approach re-
lies on training data supplied by human (expert) an-
notators who provide sufficiently many instances of
ground truth decisions from which regularities can
automatically be learnt. At the level of system per-
formance, rules tend to foster precision at the cost
of recall and ML systems tend to produce inverse
figures, while there is no conclusive evidence for or
against any of these two approaches.
The extraction of GREs, independent of the ap-
proach one subscribes to, is a complex problem
composed of a series of subtasks. Abstracting away
from lots of clerical and infrastructure services (e.g.,
sentence splitting, tokenization) at the core of any
GRE extraction lie the following basic steps:
? the identification of pairs of gene mentions as
the arguments of a relation ? the well-known
named entity recognition and normalization
task,
? the decision whether the entity pair really con-
stitutes a relation,
? and the identification of the roles of the argu-
ments in the relation which implicitly amounts
to characterize each argument as either agent or
patient.
38
3.1 Rule-based Extraction
The rule-based system extracts GREs from text em-
ploying logical inference. The motivation of using
inference is that the events under scrutiny are often
expressed in text in either a compositional or an in-
complete way. We address this issue by composi-
tionally representing textual semantics and by log-
ically inferring implicit meanings of text over the
compositional representation of textual semantics.
Entity Identification. The system first recognizes
named entities of the types that can be participants of
the target events. We have collected 15,881 E. coli
gene/protein and operon names from REGULONDB
and UNIPROT. Most of the gene/protein names are
associated with UNIPROT identifiers. An operon in
prokaryotes is a DNA sequence with multiple genes
whose expression is controlled by a shared promoter
and which thus express together. We have mapped
the operon names to corresponding gene sets.
Named entity recognition relies on the use of dic-
tionaries. If the system recognizes an operon name,
it then associates the operon with its genes. The
system further recognizes multi-gene object names
(e.g., ?acrAB?), divides them into individual gene
names (e.g., ?acrA?, ?acrB?) and associates the gene
names with the multi-gene object names.
Relation Identification. The system then iden-
tifies syntactic structures of sentences in an in-
put corpus by utilizing the ENJU parser (Sagae et
al., 2007). The ENJU parser generates predicate-
argument structures, and the system converts them
into dependency structures.
The system then analyzes the semantics of the
sentences by matching syntactic-semantic patterns
to the dependency structures. We constructed 1,123
patterns for the event extraction according to the fol-
lowing workflow. We first collected keywords re-
lated to gene regulation, from GENE ONTOLOGY,
INTERPRO, WORDNET, and several papers about
information extraction from biomedical literature
(Hatzivassiloglou and Weng, 2002; Kim and Park,
2004; Huang et al, 2004). Then we collected sub-
categorization frames for each keyword and created
patterns for the frames manually.
Each pattern consists of a syntactic pattern and
a semantic pattern. The syntactic patterns com-
ply with dependency structures. The system tries
to match the syntactic patterns to the dependency
structures of sentences in a bottom-up way, consid-
ering syntactic and semantic restrictions of syntac-
tic patterns. Once a syntactic pattern is successfully
matched to a sub-tree of the available dependency
structure, its corresponding semantic pattern is as-
signed to the sub-tree as one of its semantics. The
semantic patterns are combined according to the de-
pendency structures to form a compositional seman-
tic structure.
The system then performs logical inference over
the semantic structures by using handcrafted infer-
ence rules and extracts target information from the
results of the inference. We have manually created
28 inference rules that reflect the knowledge of the
gene regulation domain. Only relations where the
identified agent is one of those known TFs are kept,
while all others are discarded.
3.2 Generic, ML-based Extraction
Apart from the already mentioned clerical pre-
processing steps, the ML-based extraction of GREs
requires several additional syntactic processing
steps including POS-tagging, chunking, and full
dependency- and constituency-based parsing.4
Entity Identification. To identify gene names in
the documents, we applied GENO, a multi-organism
gene name recognizer and normalizer (Wermter
et al, 2009) which achieved a top-rank perfor-
mance of 86.4% on the gene normalization task
of BIOCREATIVE-II. GENO recognizes gene men-
tions by means of an ML-based named entity tag-
ger trained on publicly available corpora. Subse-
quently, it attempts to map all identified mentions to
organism-specific UNIPROT5 identifiers. Mentions
that cannot be mapped are discarded; only success-
fully mapped mentions are kept. We utilized GENO
in its original version, i.e., without special adjust-
ments to the E. coli organism. However, only those
mentions detected to be genes of E. coli were fed
into the relation extraction component.
4These tasks were performed with the OPENNLP tools
(http://opennlp.sourceforge.net/) and the
MST parser (http://sourceforge.net/projects/
mstparser), both retrained on biomedical corpora.
5http://www.uniprot.de
39
Relation Identification. The ML-based approach
to GRE employs Maximum Entropy models and
constitutes and extension of the system proposed by
Buyko et al (2008) as it also makes use of depen-
dency parse information including dependency tree
level features (Katrenko and Adriaans, 2006) and
shortest dependency path features (Kim et al, 2008).
In short, the feature set consists of:
? word features (covering words before, after and
between both entity mentions);
? entity features (accounting for combinations of
entity types, flags indicating whether mentions
have an overlap, and their mention level);
? chunking and constituency-based parsing fea-
tures (concerned with head words of the
phrases between two entity mentions; this class
of features exploits constituency-based parsing
as well and indicates, e.g., whether mentions
are in the same NP, PP or VP);
? dependency parse features (analyzing both the
dependency levels of the arguments as dis-
cussed by Katrenko and Adriaans (2006) and
dependency path structure between the argu-
ments as described by Kim et al (2008));
? and relational trigger (key)words (accounting
for the connection of trigger words and men-
tions in a full parse tree).
An advantage of ML-based systems is that they
allow for thresholding. To achieve higher recall
values for our system, we may set the confidence
threshold for the negative class (i.e., a pair of en-
tity mentions does not constitute a relation) to values
> 0.5. Clearly, this is at the cost of precision as the
system more readily assigns the positive class.
4 Intrinsic Evaluation of Feasibility
The following two sections aim at evaluating the
rule-based and ML-based GRE extraction systems.
The systems are first ?intrinsically? evaluated, i.e.,
in a cross-validation manner on corpora annotated
with respect to GREs. Second, in a more realistic
scenario, both systems were evaluated against REG-
ULONDB, a database collecting knowledge about
gene regulation in E. coli. This scenario tests which
part of manually accumulated knowledge about gene
regulation in E. coli can automatically be identified
by our systems and at what level of quality.
4.1 Rule-based system
Corpus. For the training and evaluation of the
rule-based system, we annotated 209 MEDLINE ab-
stracts with three types of events: specific events
of gene transcription regulation, general events of
gene expression regulation, and physical events of
binding of transcription factors to gene regulatory
regions. Strictly speaking, only the first type is rele-
vant to REGULONDB. However, biologists often re-
port gene transcription regulation events in the sci-
entific literature as if they are gene expression regu-
lation events, which is a generalization of gene tran-
scription regulation, or the binding event, which it-
self is insufficient evidence for gene transcription
regulation. The two latter types may indicate that
the full-texts contain evidence of the first type.
We asked two curators to annotate the abstracts.
Curator A was trained with example annotations and
interactive discussions. Curator B was trained only
with example annotations and guidelines. For cross-
checking of annotations, we asked them to annotate
an unseen corpus of 97 abstracts and found that Cu-
rator A made 10.8% errors, misjudging three event
additions and, in the other 14 errors, mistaking in
annotating event types, event attributes, and pas-
sage boundaries, while Curator B made 32.4% er-
rors as such. This result indicates that the annotation
of GREs requires intensive and interactive training.
The curators have discussed and agreed on the final
release of the corpora.6
Results. The system has successfully extracted 79
biologically meaningful events among them (21.1%
recall) and incorrectly produced 15 events (84.0%
precision) which constitutes an overall F-score of
33.6%. Among the 79 events, the system has cor-
rectly identified event types of 39 events (49.4% pre-
cision), polarity of 46 events (58.2% precision), and
directness of 51 events (64.6% precision). Note that
the system employed a fully automatic module for
named entity recognition. The event type recogni-
tion is impaired, because it often fails to recognize
6The resultant annotated corpora are available at http://
www.ebi.ac.uk/?kim/eventannotation/.
40
the specific event type of transcription regulation,
but only identifies the general event type of gene ex-
pression regulation due to the lack of identified evi-
dence.
4.2 ML-based system
GeneReg corpus. The GENEREG corpus (Buyko
et al, 2008) constitutes a selection of 314 MED-
LINE abstracts related to gene regulation in E. coli.
These abstracts were randomly drawn from a set of
32,155 selected by MESH term queries from MED-
LINE using keywords such as Escherichia coli, Gene
Expression and Transcription Factors. These 314
abstracts were manually annotated for named enti-
ties involved in gene regulatory processes (such as
transcription factor, including co-factors and regu-
lators, and genes) and pairwise relations between
transcription factors (TFs) and genes, as well as trig-
gers (e.g., clue verbs) essential for the description of
gene regulation relations. As for the relation types,
the GENEREG corpus distinguishes between (a) un-
specified regulation of gene expression, (b) positive,
and (c) negative regulation of gene expression. Out
of the 314 abstracts a set of 65 were randomly se-
lected and annotated by a second annotator to iden-
tify inter-annotator agreement (IAA) values. For the
task of correct identification of the pair of interacting
named entities in gene regulation processes, an IAA
of 78.4% (R), 77.3% (P ), 77.8% (F) was measured ,
while 67% (R), 67.9% (P), 67.4% (F) were achieved
for the identification of interacting pairs plus the 3-
way classification of the interaction relation.
Experimental Setting. The ML-based extraction
system merges all of the above mentioned three
types (unspecific, negative and positive) into one
common type ?relation of gene expression?. So, it
either finds that there is a relation of interest be-
tween a pair of gold entity mentions or not. We
evaluated our system by a 5-fold cross-validation on
the GENEREG corpus. The fold splits were done
on the abstract-level to avoid the otherwise unrealis-
tic scenario where a system is trained on sentences
from an abstract and evaluated on other sentences
but from the same abstract (Pyysalo et al, 2008).
As our focus here is only on the performance of the
GRE extraction component, gold entity mentions as
annotated in the respective corpus were used.
Results. For the experimental settings given
above, the system achieved an F-score of 42% with
a precision of 59% and a recall of 33%. Increasing
the confidence threshold for the negative class in-
creases recall as shown for two different thresholds
in Table 1. As expected this is at the cost of preci-
sion. It shows, that using an extremely high thresh-
old of 0.95 results in a dramatically increased recall
of 73% compared to 33% with the default threshold.
Although at the cost of diminished precision of 32%
compared to originally 59%, the lifted threshold in-
creases the overall F-score (44%) by 2 points.
threshold R P F
default (0.5) 0.33 0.59 0.42
0.80 0.54 0.43 0.48
0.95 0.73 0.32 0.44
Table 1: Different confidence thresholds for the ML-
based system achieved by intrinsic evaluation
5 Extrinsic Evaluation of Robustness
REGULONDB is the primary and largest reference
database providing manually curated knowledge of
the transcriptional regulatory network of E. coli
K12. On K12, approximately for one-third of K12?s
genes, information about their regulation is avail-
able. REGULONDB is updated with content from
recent research papers on this issue. While REG-
ULONDB contains much more, for this paper our
focus was solely on REGULONDB?s information
about gene regulation events in E. coli. In the fol-
lowing, the term REGULONDB refers to this part of
the REGULONDB database. REGULONDB includes
e.g., the following information for each regulation
event: regulatory gene (the ?agent? in such an event,
a transcription factor), the regulated gene (the ?pa-
tient?), the regulatory effect on the regulated gene
(activating, suppression, dual, unknown), and evi-
dence that supports the existence of the regulatory
interaction.
Evaluation against REGULONDB constitutes a
real-life scenario. Thus, the complete extraction sys-
tems were run, including gene name recognition and
normalization as well as relation detection. Hence,
the systems? overall recall values are highly affected
by the gene name identification. REGULONDB is
here taken as a ?true? gold standard and thus as-
41
sumed to be correct and exhaustive with respect to
the GREs contained. As, however, every manu-
ally curated database is likely to be incomplete and
might contain some errors, we supplement our eval-
uation against REGULONDB with a manual analy-
sis of false positives errors caused by our system (cf.
Section 5.4).
5.1 Evaluation Scenario and Experimental
Settings
To evaluate our extraction systems against REG-
ULONDB we first processed a set of input docu-
ments (see below), collected all unique gene reg-
ulation events extracted and compared this set of
events against the full set of known events in REG-
ULONDB. A true positive (TP) hit is obtained, when
an event found automatically corresponds to one in
REGULONDB, i.e., having the same agent and pa-
tient. The type of regulation is not considered. A
false positive (FP) hit is counted, if an event was
found which does not occur in the same way in
REGULONDB, i.e., either patient or agent (or both)
are wrong. False negatives (FN) are those events
covered by REGULONDB but not found by a sys-
tem automatically. From these hit values, standard
precision, recall, and F-score values are calculated.
Of course, the systems? performance largely depend
on the size of the base corpus collection processed.
Thus, for both systems and all three document sets
we got separate performance scores.
Table 2 gives an overview to the document col-
lections used for evaluating the robustness of our
systems: The ?ecoli-tf? variants are documents fil-
tered both with E. coli TF names and with relevance
to E. coli. Abstracts are taken from Medline cita-
tions, while full texts are from a corpus of different
biomedical journals. The third document set, ?regu-
lon ra?, is a set containing abstracts from the REG-
ULONDB references.
name type # documents
ecoli-tf.abstracts abstract 4,347
ecoli-tf.fulltext full texts 1,812
regulon ra abstracts 2,704
Table 2: Document sets for REGULONDB evaluation
5.2 Rule-based-System
Table 3 shows the evaluation results of the rule-
based system against REGULONDB. Though the
system distinguishes the three types of events, we
have considered them all as events of gene tran-
scription regulation for the evaluation. For instance,
the system has extracted 718 unique events with
single-unit participants (i.e., excluding operons), not
considering event types and attributes (e.g., polar-
ity), from the ?ecoli-tf.fulltext? corpus. Among the
events, 347 events are found in Regulon (9.7% re-
call, 48.3% precision). If we only consider the
events that are specifically identified as gene tran-
scription regulation, the system has extracted 379
unique events among which 201 are also found in
Regulon (5.6% recall, 53.0% precision).
participant document set R P F
single-unit ecoli-tf.abstracts 0.09 0.60 0.15
multi-unit ecoli-tf.abstracts 0.24 0.61 0.34
single-unit ecoli-tf.fulltext 0.10 0.48 0.16
multi-unit ecoli-tf.fulltext 0.25 0.49 0.33
single-unit regulon ra 0.07 0.73 0.13
multi-unit regulon ra 0.18 0.70 0.28
Table 3: Results of evaluation against REGULONDB of
rule-based system.
When we split multi-unit participants into individ-
ual genes, the rule-based system shows better per-
formance, as shown in Table 3 with the participant
type ?multi-unit?. This may indicate that the gene
regulatory events of E. coli are often described as
interactions of operons. At best, the system shows
34% F-score with the ?ecoli-tf.abstracts? corpus.
5.3 ML-based System
The ML-based system was designed to recognize
all types of gene regulation events. REGULONDB,
however, contains only the subtype, i.e., regulation
of transcription. Thus, the ML-based system was
evaluated against REGULONDB in two modes: by
default, all events extracted by the systems are con-
sidered; in the ?TF-filtered? mode, only relations
with an agent from the list of all known TFs in E.
coli are considered (as done for the rule-based sys-
tem by default). Thus, comparing to the rule-based
system, only the results obtained in the ?TF-filtered?
mode should be considered.
42
5.3.1 Raw performance scores
The results for the ML-based system are shown in
Table 4. Recall values here range between 7 and
10%, while precision is between 29 and 78% de-
pending on both the document set as well as the
application of the TF filter. The low recall of the
ML-based system is partially due to the fact that the
system does not recognize multi-gene object names
(e.g., ?acrAB?), in this configuration the recall is
similar to the recall of the rule-based system in a
?single-unit modus? (see Table 3).
mode document set R P F
TF-filtered ecoli-tf.abstracts 0.09 0.70 0.16
default ecoli-tf.abstracts 0.09 0.45 0.15
TF-filtered ecoli-relevant.fulltext 0.10 0.54 0.17
default ecoli-relevant.fulltext 0.10 0.29 0.15
TF-filtered regulon ra 0.07 0.78 0.13
default regulon ra 0.07 0.47 0.12
Table 4: Results of evaluation against REGULONDB of
ML-based system
As already shown in the intrinsic evaluation,
application of different confidence thresholds in-
creases the recall of the ML-based system. This was
also done for the evaluation against REGULONDB.
Table 5 shows the impact of increased confidence
thresholds for the negative class on the ?regulon ra?
set for the ?TF-filtered? evaluation mode. Given an
extremely high threshold of 0.95, the recall is in-
creased from 7 to 11% which constitutes a relative
increase of over 60%. Precision obviously drops,
however, the overall F-score has improved from 13
to 19%. These results emphasize that an ML-based
system has an important handle which allows to ad-
just recall according to the application needs.
threshold R P F
default (0.5) 0.07 0.78 0.13
0.8 0.09 0.70 0.16
0.95 0.11 0.63 0.19
Table 5: Different confidence thresholds for the ML-
based system tested on the ?regulon ra? set
5.4 Manual analysis of false positives
REGULONDB was taken as an absolute gold stan-
dard in this evaluation. If a system correctly extracts
an event which is not contained in REGULONDB
for some reason, this constitutes a FP. Moreover, all
kinds of error (e.g., agent and patient mixed up) were
subsumed as FP errors. To analyze the cause and
distribution of FPs in more detail, a manual analysis
of the FP errors was performed and original FP hits
were assigned to one out of four FP error categories:
Cat1: Not a GRE This is really an FP error, as the
extracted relation does not at all constitute a
gene regulation event.
Cat2a: GRE but other than transcription
Unlike REGULONDB which contains only one
subtype of GREs, namely transcriptions, the
ML-based system identifies all kinds of GREs.
Therefore, the ML-based system clearly
identifies events which cannot be contained in
REGULONDB and, therefore, are not really
FPs.
Cat 3: Partially correct transcription event This
category deals with incorrect arguments of
GREs. We distinguish three types of FPs: (a)
the patient and the agent role are interchanged,
(b) the patient is wrong, while the agent is
right, and (c) the agent is wrong, while the
patient is right. In all these three cases, though
errors were committed human curators might
find the partially incorrect information useful
to speed up the curation process.
Cat4: Relation missing in REGULONDB Those
are relations which should be contained in
REGULONDB but are missing for some
reason. The agent is a correctly identified
transcription factor and the sentence contains
a mention of a transcription event. There are
several reasons why this relation was not found
in REGULONDB as we will discuss in the
following.
Table 6 shows the results of the manual FP anal-
ysis of the ML-based system (no TF filter applied)
on the ?ecoli-tf-abstracts? and ?ecoli-tf-fulltexts?.
It shows that the largest source of error is due
to Cat1, i.e., an identified relation is completely
wrong. As fulltext documents are generally more
complex, the relative amount of this kind of errors
is higher here than on abstracts (54.5 % compared
43
category abstracts (%) fulltexts (%)
Cat 1 44.5 54.5
Cat 2 11.2 10.9
Cat 3a 3.8 3.9
Cat 3b 8.5 4.4
Cat 3c 8.2 5.4
Cat 4 23.8 21.0
Table 6: Manual analysis of false positive errors (FP).
Percentages of FPs by category are reported on ?ecoli-tf-
abstracts? and ?ecoli-tf-fulltexts?
to 44.5 %). However, on abstracts and fulltexts, a
bit more than 10 % of the FP are because the sys-
tem found too general GREs which, by definition,
are not contained in REGULONDB (Cat2). Iden-
tified GREs that were partially correct constitute
20.5 % (abstracts) or 13.7 % (fulltexts) of the FP er-
rors (Cat3).
Finally, 23.8% and 21.0% of the FPs for abstracts
and fulltext, respectively, are correct transcription
events but could not be found in REGULONDB
(Cat4). This is due to several reasons. For instance,
identified gene names were incorrectly normalized
so that they could not be found in REGULONDB,
REGULONDB curators have not yet added a relation
or simply overlooked it; relations are correctly iden-
tified as such in the narrow context of a paragraph of
a document but were actually of speculative nature
only (this includes relations whose status is unsure,
often indicated by ?likely? or ?possibly?).
Summarizing, the manual FP analysis shows that
about 50% of all FPs are not completely erroneous.
These numbers must clearly be kept in mind when
interpreting the raw numbers (especially for preci-
sion) reported on in the previous subsection.
5.5 Integration of text mining results
We have integrated the results of the two different
text mining systems and found that both systems are
complementary to each other such that their result
sets do not heavily overlap. For instance, from the
?ecoli-tf.abstract? corpus, the rule-based system ex-
tracts 992 events, while the ML-based system ex-
tracts 705 events. For the integration, we have con-
sidered only the events whose participants are as-
sociated with UNIPROT identifiers. Among the ex-
tracted events, only 285 events are extracted by both
systems. We might speculate that the overlapping
events are more reliable than the rest of the extracted
events. It also leaves 71.3% of the results from
the rule-based system and 59.6% of results from the
ML-based system as unique contributions from each
of the approaches for the integration.
6 Conclusions
We have explored a rule-based and a machine
learning-based approach to the automatic extrac-
tion of gene regulation events. Both approaches
were evaluated under well-defined lab conditions us-
ing self-supplied corpora, and under real-life condi-
tions by comparing our results with REGULONDB,
a well-curated reference data set. While the re-
sults for the first evaluation scenario are state of the
art, performance figures in the real-life scenario are
not so shiny (the best F-scores for the rule-based
and the ML-based system are on the order of 34%
and 19%, respectively). This holds, in particular,
for the comparison with the work of Rodr??guez-
Penagos et al (2007). Still, at least the ML-based
approach is much more general than the very specifi-
cally tuned manual rule set from Rodr??guez-Penagos
et al (2007) and has potential for increases in perfor-
mance. Also, this has been the first extra-mural eval-
uation of automatically generating content for REG-
ULONDB.
Still, the analysis of false positives reveals that
the strict criteria we applied for our evaluation may
appear in another light for human curators. Con-
founded agents and patients (21% on the abstracts,
14% on full texts) and information not contained in
REGULONDB (24% on the abstracts, 21% on full
texts) might be useful from a heuristic perspective to
focus on interesting data during the curation process.
Acknowledgements
This work was funded by the EC within the BOOT-
Strep (FP6-028099) and the CALBC (FP7-231727)
projects. We want to thank Tobias Wagner (Centre
for Molecular Biomedicine, FSU Jena) for perform-
ing the manual FP analysis.
44
References
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the 3rd Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), pages 21?28.
Francis Collins, Eric Green, Alan Guttmacher, and Mark
Guyer. 2003. A vision for the future of genomics re-
search. Nature, 422(6934 (24 Feb)):835?847.
Vasileios Hatzivassiloglou and Wubin Weng. 2002.
Learning anchor verbs for biological interaction pat-
terns from published text articles. International Jour-
nal of Medical Informatics, 67:19?32.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and P. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency trees.
In KDECB 2006 ? Knowledge Discovery and Emer-
gent Complexity in Bioinformatics, pages 61?80.
Jung-jae Kim and Jong C. Park. 2004. BioIE: retar-
getable information extraction and ontological anno-
tation of biological interactions from the literature.
Journal of Bioinformatics and Computational Biology,
2(3):551?568.
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Learning
language in logic - genic interaction extraction LLL?
2005, pages 31?37.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, 9(3), April.
Carlos Rodr??guez-Penagos, Heladia Salgado, Irma
Mart??nez-Flores, and Julio Collado-Vides. 2007. Au-
tomatic reconstruction of a bacterial regulatory net-
work using natural language processing. BMC Bioin-
formatics, 8(293).
Kenji Sagae, Yusuke Miyao, and Junichi Tsujii. 2007.
HPSG parsing with shallow dependency constraints.
In Annual Meeting of Association for Computational
Linguistics, pages 624?631.
Jasmin S?aric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Hui Yang, Goran Nenadic, and John Keane. 2008. Iden-
tification of transcription factor contexts in literature
using machine learning approaches. BMC Bioinfor-
matics, 9(Supplement 3: S11).
45
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747?757,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A discourse-driven content model for summarising scientific articles
evaluated in a complex question answering task
Maria Liakata
University of Warwick/
EMBL-EBI, UK
M.Liakata@warwick.ac.uk
Simon Dobnik
University of Gothenburg, Sweden
simon.dobnik@gu.se
Shyamasree Saha
EMBL-EBI, UK
saha@ebi.ac.uk
Colin Batchelor
Royal Society of Chemistry, UK
batchelorc@rsc.org
Dietrich Rebholz-Schuhmann
University of Zurich, Switzerland/
EMBL-EBI, UK
rebholz@ebi.ac.uk
Abstract
We present a method which exploits auto-
matically generated scientific discourse an-
notations to create a content model for the
summarisation of scientific articles. Full pa-
pers are first automatically annotated using the
CoreSC scheme, which captures 11 content-
based concepts such as Hypothesis, Result,
Conclusion etc at the sentence level. A content
model which follows the sequence of CoreSC
categories observed in abstracts is used to pro-
vide the skeleton of the summary, making a
distinction between dependent and indepen-
dent categories. Summary creation is also
guided by the distribution of CoreSC cate-
gories found in the full articles, in order to
adequately represent the article content. Fi-
nally, we demonstrate the usefulness of the
summaries by evaluating them in a complex
question answering task. Results are very en-
couraging as summaries of papers from auto-
matically obtained CoreSCs enable experts to
answer 66% of complex content-related ques-
tions designed on the basis of paper abstracts.
The questions were answered with a precision
of 75%, where the upper bound for human
summaries (abstracts) was 95%.
1 Introduction
The publication boom of the last few years, espe-
cially in the life sciences, has highlighted the need
to facilitate automatic access to the information con-
tent of articles. Researchers, curators, reviewers all
need to process a continuously expanding flow of
articles whether the purpose is to follow the state of
the art, curate large knowledge bases or have a good
working knowledge of their own and related disci-
plines to assess progress in research. While a lot
of effort has concentrated on information extraction
of particular types of entities and relations from the
scientific literature (Cohen and Hersh, 2005; Kim
et al, 2009; Ananiadou et al, 2010; Kim et al,
2011), with a view to support scientists in obtain-
ing relevant information from scientific articles and
abstracts, less work has focussed on automatically
combining such information in the form of a co-
hesive summary which preserves the context. Re-
searchers rely to a great extent on author-written ab-
stracts, but the latter suffer from a number of prob-
lems; they are less structured, vary significantly in
terms of length, are often not self-contained and
have been written independently of the main doc-
ument (Teufel, 2010, p.83).
Teufel (2001; 2010), (Teufel and Moens, 2002)
identify argumentative zones within scientific arti-
cles and use them to create use-targeted extractive
summaries. Argumentative zones are annotations
which designate the type of knowledge claim and
rhetorical status for a sentence and how these relate
to the communicative function of the entire paper.
A selection of various combinations of argumenta-
tive zones are chosen for the use-targeted extractive
summaries (rhetorical extracts), each of which ful-
fills a different role. For instance, purpose-oriented
extracts less than 10 sentences long are generated
containing a predetermined number of AIM, SOLU-
TION and BACKGROUND zones. As the emphasis
of this approach was the identification of the argu-
mentative zones, less attention was given to the sen-
tence selection criteria for the extractive summaries.
747
The sentences chosen for the rhetorical extracts were
either all sentences of a particular category (in the
case of rare categories) (Teufel and Moens, 2002),
selected according to a classifier trained on a rele-
vance gold standard (Teufel and Moens, 2002), man-
ually or randomly selected (Teufel, 2010, p.60).
More recently Contractor et al (2012) have used
automatically annotated argumentative zones (Guo
et al, 2011) to guide the creation of extractive sum-
maries of scientific articles. Here argumentative
zones are used as features for the summariser, along
with verbs, tf-idf values and sentence location. They
use a standard approach to summarisation, with a bi-
nary classification recognising candidate sentences
which are then fed into a clustering mechanism. Ex-
tracts can be created to summarise the entire paper or
focus on specific user-specified aspects. The num-
ber of sentences to include in the summary is pre-
specified (either directly or using a compression ra-
tio).
Our approach also makes use of the scientific dis-
course for summarisation purposes. We use the sci-
entific discourse to create a content model for ex-
tractive summarisation, with a focus on represent-
ing the content of the full paper, while keeping the
cohesion of the narrative. We first automatically
annotate the articles with a scheme which captures
fine-grained aspects of the content and conceptual
structure of the papers, namely the Core Scientific
Concepts (CoreSC) scheme (Liakata et al, 2010; Li-
akata et al, 2012). The CoreSC scheme is ?uniquely
suited to recovering common types of scientific ar-
guments about hypotheses, explanations, and evi-
dence? (White et al, 2011), which are not read-
ily identifiable by other annotation schemes. Also,
when compared to argumentative zoning and more
specifically its extension for chemistry papers, AZ-
II (Teufel et al, 2009), it was shown to provide a
greater level of detail in terms of categories denot-
ing objectives, methods and outcomes whereas AZ-
II focusses on the attribution of knowledge claims
and the relation with previous work (Liakata et al,
2010).
We then use the distribution of CoreSC categories
observed in abstracts to create a content model
which provides a skeleton for extractive summaries.
The reasoning behind this is to try to preserve co-
hesion within the summaries and we hypothesise
that the sequence of CoreSC categories is a good
proxy for cohesion (see section 3.1). In creating
the summary, instantiating the content model, we
identify independent categories and dependent cate-
gories, and we argue that in order to preserve the co-
hesion of the text the independent categories should
be determined first (see section 3.2). We also pre-
serve in the summary the distribution of CoreSC cat-
egories found in the corresponding full paper.
Finally, we evaluate the extractive summaries in
a complex real world question-answering task, in
which we assess the usefulness of the summaries as
well as to what extent the generated CoreSC sum-
maries represent the content of the original arti-
cle. Experts are presented with different types of
summaries and are asked to answer article-specific
questions on the basis of the summaries (see sec-
tion 4.1). Our results show that automatically gen-
erated CoreSC summaries can answer 66% of com-
plex questions with 75% precision, outperforming
a baseline of microsoft autosummarise summaries
(See section 4.2).
We have also peformed an intrinsic evaluation of
the summaries using ROUGE and automatic mea-
sures for summary informativeness, such as the
Jensen-Shannon divergence, yielding positive re-
sults (See section 4.2). However, as such measures
have not yet reached maturity and are harder to in-
terpret, we consider the user-based evaluation to be
a more reliable measure of summary quality.
Code for generating the summaries can be ob-
tained by contacting the first author and/or visiting
http://www.sapientaproject.com/software.
2 Related work
The Core Scientific Concepts (CoreSC) Scheme:
The CoreSC scheme consists of three layers; the first
layer corresponds to eleven concepts (Background
(BAC), Hypothesis (HYP), Motivation (MOT), Goal
(GOA), Object (OBJ), Method (MET), Model
(MOD), Experiment (EXP), Observation (OBS),
Result (RES) and Conclusion (CON)); the second
layer corresponds to properties of the concepts (e.g.
New/Old) and the third layer provides identifiers
which link instances of the same category. Liakata
et al (2010) created a corpus of 265 full scientific
articles from chemistry and biochemistry annotated
748
with this scheme and trained classifiers using SVMs
and CRFs in (Liakata et al, 2012), with an accu-
racy of >51% across the 11 concepts. Their data
and CoreSC classification system are available on-
line and can provide a good benchmark for com-
parison. Louis & Nenkova (2012) have successfully
used the CoreSC corpus for evaluating syntax-based
coherence models, which indicates the strong con-
nection between coherence and discourse structure.
Summarisation for scientific articles: A lot of
the work on summarising scientific articles has fo-
cussed on citation-based summaries. Qazvinian &
Radev (2008) use sentences from papers citing the
article to be summarised. Sentences are clustered to-
gether creating a topic, with the combination of clus-
ters forming a citation summary network. Qazvinian
& Radev (2010), (Qazvinian et al, 2010) also make
use of citation sentences in other scientific papers to
summarize the contributions of a paper. The draw-
back of citation summaries is that a paper must be
already cited, so this type of summary will not be
useful to a paper reviewer. Also, citations of articles
will have been influenced by other citations rather
than the paper itself.
Document models for summarisation: Our con-
tent model has some similarities with content mod-
elling using global sentence ordering (Barzilay and
Lee, 2004; Chen et al, 2009). In (Barzilay and
Lee, 2004) unsupervised methods are used to cre-
ate HMM topic sequence models for newswire text
articles. Topics are assigned to texts according to
the content model and extracts of fixed length are
created by selecting the topics most likely to occur
in summaries. While we use supervised methods to
annotate papers with a fixed set of topics (CoreSCs)
in scientific papers, our summary content model for
extracts shares similar principles such as global or-
dering of sentences and non-recurrence. However,
their evaluation involved newspaper articles and ex-
tracts which are a lot shorter (15 and 6 sentences,
respectively).
It is not clear whether unsupervised topic mod-
elling such as (Chen et al, 2009) can be applied to
scientific articles (over 100 sentences long), which
by nature include repetition of topics. It would be
interesting to make comparisons with summaries us-
ing content models learnt from our data automati-
cally, following a similar approach to (Sauper et al,
2010) which learns a content model jointly with a
particular supervised task in web-based documents.
3 Extractive Summarisation using
CoreSCs
In this section we describe how we use CoreSC dis-
course categories annotated at the sentence level to
create extractive summaries of full papers, which we
subsequently evaluate in a question answering task
in section 4.
To generate summaries we follow classic text ex-
traction techniques while making use of a document
content model based on CoreSCs. Our aim is for
the content model to reflect both the distribution of
CoreSCs in the paper as well as the discourse model
of human summaries, as the latter is indicated by the
generic ordering of CoreSC categories in abstracts
encountered in a corpus of 265 annotated full pa-
pers (Liakata and Soldatova, 2009; Liakata et al,
2012). While we do not consider abstracts to be ade-
quate summaries, we at least consider them to be co-
herent summaries, which is why the content model
reflects the distribution of CoreSCs in the abstracts.
To create our summaries, we employed automat-
ically generated CoreSC annotations, which are the
output of the classifiers described in (Liakata et al,
2012). These classifiers assign CoreSC categories
to sentences on the basis of features local to a sen-
tence, such as significant n-grams, verbs and word
triples, as well as global features such as the posi-
tion of the sentence within the document and within
a paragraph and section headers. The following sub-
sections give details about the creation of extractive
summaries from CoreSC categories.
3.1 A content model for CoreSC extractive
summaries
Building an extractive summary using a computa-
tional model of document structure is an idea shared
by many previous approaches, whether the model is
hand-crafted, based on rhetorical elements (McKe-
own, 1985; Teufel and Moens, 2002) or rhetorical
relations (Marcu, 1998b; Marcu, 1998a) or whether
it is a content model, learnt automatically from text
as in (Barzilay and Lee, 2004), focussing on the lo-
cal content or a combination of the local content and
global structure (Sauper et al, 2010).
749
Our document content model is primarily based
on the global discourse of the article as provided by
the type and number of CoreSC categories. How-
ever, unlike (Teufel and Moens, 2002), who take a
fixed number of AZ categories of specific type to
create rhetorical extracts, the number of categories
used from each CoreSC category depends on their
distribution in the original article. Any and all types
of CoreSC category could potentially appear in a
summary, as our summaries are meant to be repre-
sentative of the entire content of the paper. Also, the
ordering of the categories in the summary is learnt
to reflect the ordering of categories observed in ab-
stracts of papers from the same domain.
Our model also caters for local discourse depen-
dencies. For example, the selection of a particu-
lar ?Method? sentence for inclusion in the summary
should influence the choice of ?Experiment? sen-
tences, which refers to particular experimental pro-
cedures performed. This is not an issue of concern
to (Teufel and Moens, 2002), but relates to the no-
tion of NUCLEUS and SATELLITE clauses, which
form the foundation of Rhetorical Structure The-
ory (Mann and Thompson, 1998), and guides the
summarisation paradigm of (Marcu, 1998a; Marcu,
1998b). However, the difference here is that we
define a-priori certain categories to be independent
(have the property of playing the role of nucleus in
the discourse) and specify their relation with partic-
ular types of dependent categories. Thus, nuclearity
becomes a property of the CoreSC category, which
is indirectly inherited by the sentence.
Therefore, when creating the CoreSC content
model for summaries we addressed the following is-
sues: (i) summary length; (ii) number of sentences
from each CoreSC, (iii) the ordering in which sen-
tences from each CoreSC category should appear
and (iv) the extraction of sentences according to in-
dependent and dependent categories.
? Summary length: While the literature (Teufel,
2010, p.45) suggests that 20?30% of the original
document is required for an adequately informa-
tive summary, (Teufel, 2010, p.55) assumes this
is too long for scientific papers. For this reason
and to allow better comparison between papers
of varying lengths, we fixed our summary length
to 20 sentences. This is reasonable considering
we have 11 CoreSCs, any and all of which can
appear in both abstracts and full papers.
? Number of sentences from each category: To
reflect the content of the paper, the distribution
of the CoreSC categories in the extract follows
the distribution of CoreSCs in the full paper.
For each CoreSC we determine the number of
sentences to be selected (n(selected(C))) by
multiplying the ratio of that category in the paper
by 20. A difficulty arises if the ratio of a partic-
ular concept in the paper is very low (? 0.05)
in which case we prefer to include one sentence.
If a particular concept is not at all present in the
paper, the number of selected sentences for that
category will be 0.
? Ordering of CoreSC categories in the sum-
mary: According to a study of empirical sum-
maries (Liddy, 1991), sentences of a particular
textual type appear in a particular order. Since
paper abstracts were the closest approximation
of human summaries available to us, CoreSC
category transitions found in abstracts have been
adopted in our content model for extracts. The
transitions were derived semi-empirically. First,
we extracted initial, medium and final bi-grams
of categories from paper abstracts together with
transition probabilities.
Using this information we manually constructed
transitions of the CoreSC categories that best fit
the observed frequencies and our own intuitions.
This gave us the following sequence: MOT >
(HYP) > OBJ > GOA > BAC > MOD > MET
> EXP > OBS > (HYP) > RES > CON. HYP
appears twice in the sequence as annotators had
distinguished two types of hypotheses, global
hypotheses (stated together with other objec-
tives) and hypotheses about particular observa-
tions. The model provides an amalgamated rep-
resentation of CoreSC concepts in abstracts. In-
terestingly, our semi-empirically derived model
closely follows the content model for abstracts
described in (Liddy, 1991). It would be interest-
ing to see how this compares to a Markov model
of CoreSC categories learnt from the annotated
abstracts.
750
3.2 Sentence extraction based on independent
and dependent categories
Sentence extraction involves selecting the most rel-
evant sentences to include in a summary. Typically,
this entails ranking the sentences according to some
measure of salience and selecting the top n-best
sentences. For example, a sentence will be repre-
sented by a number of features associated with it,
such as whether it contains certain high frequency
words or cue phrases, its location in the document,
location in a paragraph (Brandow et al, 1995; Ku-
piec et al, 1995). Other methods include clustering
based on sentence similarity and choosing the cen-
troids (Erkan and Radev, 2004) or choosing the best
connected sentences (Mihalcea and Tarau, 2004).
When sentences are classified according to
CoreSC categories features such as the ones de-
scribed above for text extraction are taken into
account. Liakata et al (2012) report that the
most salient features for classifying CoreSC cat-
egories are overall n-grams, verbs and direct ob-
jects whereas other features such as the location of
the sentence, the neighbouring section headings and
whether a sentence contains citations play an impor-
tant role for some of the categories. Thus, classi-
fication into CoreSC categories already provides a
selection bias for sentence extraction.
As explained in section 3.1, the number of
CoreSC categories in the summaries is determined
according to their distribution in the paper and the
order of the categories is specified in the content
model. Salience for sentence extraction in this case
is determined by the need to select the most repre-
sentative sentences for a category. There isn?t much
point, for example, in identifying that we need to in-
clude a Method sentence (MET) and that this should
be followed by an Experiment sentence (EXP), if we
are not sure that those are indeed the categories of
the sentences we are about to select.
We therefore rank sentences according to the clas-
sifier confidence score (probability) with which they
were assigned a CoreSC category in (Liakata et al,
2012). The intuition behind this is that sentences
with high classifier confidence will be less noisy,
high precision cases and more representative of a
particular category. Indeed, (Liakata et al, 2012)
report statistical significance for the correlation be-
tween high classifier confidence and agreement be-
tween manual and automatic classification
However, as mentioned in section 3.1, there is
inter-dependence between sentences in the text,
which is in turn inherited by the categories assigned
to them. For example, the highest ranking MET
sentence will be related to an Experiment (EXP) or
Background (BAC) sentence, which may not be the
ones with the highest confidence score in their cate-
gory.
In order to preserve discourse cohesion it is im-
portant to select related sentences from different
categories. We resolve this by distinguising the
CoreSCs into independent categories, which by def-
inition are expected to show nucleus behaviour, and
dependent categories. We also specify the rela-
tion between independent and dependent categories.
The independent categories include the categories
with the lowest percentage of sentences in scien-
tific articles as reported in (Liakata et al, 2012),
namely: Motivation (MOT) (1%), Goal (GOA)(1%),
Hypothesis (HYP)(2%), Object (OBJ)(3%), Model
(MOD)(9%), Conclusion (CON)(9%) and Method
(MET)(11%). Categories whose sentence selec-
tion semantically depends on the former are Exper-
iment (EXP)(10%), Background (BAC)(19%), Re-
sult (RES)(21%) and Observation (OBS)(14%). The
independent categories also have higher precision
than recall, in contrast to the dependent categories.
While MET and EXP are almost equally represented
in the CoreSC corpus, EXP by definition provides
the detailed steps of an experimental method and
thus it is semantically dependent on some MET cat-
egory. More specifically, the dependencies are con-
sidered to be as follows: EXP, BAC depend on MET,
RES depends on CON and OBS depends on RES
(OBS is double-dependent).
Sentence extraction is driven by first identifying
the independent categories based on classifier con-
fidence scores and then choosing the corresponding
dependent categories on the basis of both related-
ness to the independent categories and classifier con-
fidence. We use sentence proximity (defined below)
as a measure for relatedness and combine it with
classifier confidence during sentence extraction.
The mechanism to select sentences for inclusion
in the summary, which considers category depen-
dencies, proceeds as follows:
751
? For an independent category CatI, order sentences by
decreasing order of confidence score. The confidence
score is the average confidence score of the SVM and
CRF classifiers reported in (Liakata et al, 2012) for
a sentence.
? For a dependent category Cat, for which we need n
sentences, given the selected sentences m from the
corresponding independent category CatI we do the
following:
? If m = 0, then treat Cat as independent category
for this case.
? Otherwise, for each selected sentence ti in CatI,
calculate its proximity score to every sentence cj
of the dependent category Cat. Proximity is de-
fined as 1?Distance where Distance is an ab-
solute difference in sentence ids between cj and
ti normalised by the maximum absolute distance
found between all cj and ti pairs.
? The classifier prediction score for each cj is mul-
tiplied by the Proximity(cj , ti) score and the
sentences are re-ranked according to the new
scores, where only the n highest ranking cjs are
kept. The last two steps result in an m?n matrix.
? If m = 1, then the choice for the n sentences for
Cat is straightforward.
? Otherwise, we pick the n highest ranking cjs,
proceeding row-wise. Thus, the highest ranking
cjs for the highest ranking independent sentences
ti are given priority and any cj is chosen at most
once.
Once the sentence ids are selected for each inde-
pendent and each dependent category we plug them
into the content model. Sentence order is preserved
within each CoreSC category. For example, if two
Result sentences are selected, the order in which
they appear in the paper will be preserved in the
summary.
4 Summary evaluation via question
answering
4.1 Task Description and experimental setup
We evaluate the extractive CoreSC summaries in
terms of how well they enable 12 chemistry ex-
perts/evaluators (with at least a Masters degree in
chemistry) to answer complex questions about the
papers. Our test corpus consists of 28 papers held
out from the ART/CoreSC corpus, roughly 1/9,
which were annotated automatically with the SVM
and CRF classifiers described in (Liakata et al,
2012) trained on the remaining 8/9 of the corpus.
For each of the 28 papers in the test corpus, we gen-
erated CoreSC summaries automatically using the
method described in section 3. We compare the
performance of the experts on a question answer-
ing (Q-A) task when given the CoreSC summaries
and two other types of summary, amounting to a
total of three experimental conditions (A,B,C). The
other two types of summary are the original paper
abstracts (summaries A), in the absence of human
summaries, and summaries generated by Microsoft
Office Word 2007 AutoSummarize (summaries B).
Microsoft Office Word 2007 AutoSummarize
(MA) is a widely available commercial system with
reportedly good results (Garcia-Hernandez et al,
2009) and performance equivalent to TextRank (Mi-
halcea and Tarau, 2004). MA works by assigning a
score to each word in a sentence depending on its
frequency in the document and sentences are ranked
and extracted according to the combination of scores
of the words they contain. MA therefore follows
classic lexicalised text extraction techniques, is do-
main independent and is completely agnostic of the
discourse. For the latter reason, we considered MA
to be a suitable baseline the comparison with which
would illustrate the effect of using CoreSC cate-
gories on the summary and the merits of having a
discourse based model for summarisation.
Neither the paper title nor section headings were
available to any of the summarising systems as our
extractive system does not make direct use of them
and we were not sure how they would influence MA.
To ensure that each evaluator considered only one
type of summary per paper, so as to avoid bias from
previous stimuli, and to make sure all experts were
exposed to all papers and all types of summary, the
12 experts were assigned to four groups (G1-G4)
and were allocated 28 summaries each according to
the Latin Square design in Table 1.1.
The experimental setup follows the paradigm
of (Teufel, 2001). However, while (Teufel, 2001) de-
veloped a Q-A task to evaluate summaries showing
the contribution of a scientific article in relation to
previous work, the purpose of the Q-A task at hand
1Initially we had four experimental conditions but one was
dropped, so is not presented in this context
752
is to show the usefulness of the extracted summaries
in answering questions on the paper, and how they
compare to a discourse-agnostic baseline. In the
case of (Teufel, 2001) the task consists of a fixed set
of five questions, the same for all articles tuned par-
ticularly to the relation of current and previous work.
By contrast, the current Q-A task aims to show how
well the summaries represent the content of the en-
tire paper, which means that questions are individ-
ual to each paper and required domain knowledge to
create.
Each of the 12 experts answered three content-
based questions per summary, where the questions
were individual to each paper. An example of the
questions and the corresponding answers for a given
paper can be found below.
Example 4.1.1
? Q:What do DNJ imino sugars inhibit the action of?
A: They inhibit glycosidases and ceramide glucosyl-
transferases.
? Q:What methods do the authors use to study the confor-
mation of N-benzyl-DNJ?
A: They use resonant two-photon ionization (R2PI),
ultraviolet?ultraviolet (UV?UV) hole burning, and in-
frared (IR) ion-dip spectroscopies in conjunction with
electronic structure theory calculations.
? Q:What is the conformation of the exocyclic hydrox-
ymethyl group?
A: The exocyclic hydroxymethyl group is axial to the
piperidine ring (gauche- to the ring nitrogen).
As one can see, the questions are complex wh-
questions and correspond to answers with multiple
components. Questions were complex, to minimise
the likelihood of correct random answers. They
were designed by a senior chemistry expert with
knowledge of linguistics, so that they could be an-
swered based on the abstracts (A). For this purpose,
the senior expert chose abstracts that were at least
three sentences long. Ideally, the questions and an-
swers should have been set on the basis of the en-
tire paper, but this was not possible given our time-
frame for the experiment.The underlying assump-
tion is that a good summary should cover most of the
main points of the paper. One of the merits of set-
ting the questions on the basis of the abstracts was
that the answers to be identified were deemed suf-
ficiently important to be expressed in the humanly
created abstract. However, automatic summaries
created in the way proposed here could potentially
answer questions beyond the scope of the abstract
and in cases of very short abstracts be much more
informative.
Experts were told that summaries were automati-
cally generated with no details about different types
of summary; it is assumed that none of them is com-
pletely familiar with the work mentioned in the 28
papers.
On average, it took experts less than 10 minutes to
read a summary and answer the three content-based
questions.
Papers (28)
Evaluator groups 1?7 8?14 15?21 22-28
G1 A B - C
G2 C A B -
G3 - C A B
G4 B - C A
Table 1: Distribution of summaries to evaluators
4.2 Results and Discussion
We compared each evaluator?s answers obtained af-
ter reading a summary against the model answers
set by the senior expert, the author of the questions,
based on the abstract (A) of the corresponding pa-
per. If an evaluator?s answer is identical to a model
answer, then this counts as ?matched?.
For instance in example 4.1.1 above, ?axial to
the piperidine ring?, ?gauche- to the ring nitrogen?
and ?The OH6 group is axial (Gauche) to the ring
nitrogen? were all considered correct, fully matched
answers to the question ?What is the conformation
of the exocyclic hydroxymethyl group??. In the
case of the second question in the same example all
of the following were considered correct and fully
matched: ?Resonant two-photon ionization (R2PI),
UV/UV hole-burn, and IR ion-dip spectroscopies in
conjunction with electronic structure theory calcula-
tions?, ?R2PI UV/UV hole-burn IR ion-dip e- struc-
ture theory calculations? and ?a combination of res-
onant two-photon ionization (R2PI), UV/UV hole-
burn, and IR ion-dip spectroscopies in conjunction
with electronic structure theory calculations?.
If the answer requires listing more than one item
(as is the case with questions one and two of ex-
ample 4.1.1), all of the items have to be matched.
Partially matched answers are counted as ?partially
matched?. Non-matching answers can be of two
753
types. If an un-matched answer coincided with the
answer the senior expert would have given after
reading that particular summary, then it was marked
as ?un-matched:justified?: Such answers were cor-
rect given the particular summary, but are not nec-
essarily correct with respect to the paper and do
not count as alternative answers. If the answer
was un-matched and also unjustified given the con-
tent of the summary, then it was marked as ?un-
matched:unjustified? . These are cases of evalua-
tor error. Similarly, cases where the evaluator gave
?N/A? as an answer were marked as ?justified? or
?unjustified? according to whether the senior expert
could find the answer in the summary or not. The
results from marking answers are shown in Table 2.
Number of A B C
Matched 240 126 135
Partially matched 0 4 3
Un-matched:justified 0 25 15
N/A:justified 0 71 71
Un-matched:unjustified 5 11 17
N/A:unjustified 7 15 11
All answers 252 252 252
Table 2: Matches between summary-based answers and
model answers
Micro-AVG Macro-AVG
S. types R P F R P F
A 1 0.95 0.98 1 0.95 0.97
B 0.64 0.70 0.67 0.64 0.64 0.60
C 0.66 0.75 0.70 0.64 0.70 0.65
Table 3: Precision, Recall and F-score for answering
questions using the four types of summary. A: abstracts,
B: autosummarize, C:automatic CoreSC summaries.
We report Precision, Recall and F-score (P-R-F)
for answering questions given each type of sum-
mary (Table 3). To calculate these we define TP as
matched answers, FN as N/A:justified and FP every-
thing else (partially matched + un-matched:justified
+ un-matched:unjustified + N/A:unjustified). Here,
the standard definition of recall (TP/(TP+FN))
demonstrates how many questions can be answered
using the summary (summary coverage) and Preci-
sion (TP/(TP+FP)) how well the questions are an-
swered (summary clarity).
We consider the F-measure to be an overall indi-
cator of the summary usefulness. Micro-averaging
is obtained by adding all answers from all papers to
calculate TP, FN and FP whereas macro-averaging
calculates P-R-F first per paper and then averages
over all papers.
The rankings remain consistent regardless of the
averaging method. Condition A (abstracts) shows
perfect Recall (the evaluators are able to answer all
the questions) whereas Precision is affected by un-
justified failed matches (Table 2). The perfect recall
is hardly surprising as the questions are designed
on the basis of the abstract but provides a sanity
check for the experiment. The precision sets an up-
per bound for precision with automatic summaries.
Summaries of condition C provide answers to more
questions (Recall) and with greater accuracy (Pre-
cision) than summaries B. When macro-averaging,
the Recall score of summaries C is tied with that for
summaries B but Precision is 6% higher.
To verify the statistical significance for the dif-
ference in precision and recall for summaries B and
C respectively, we performed Monte Carlo sampling
10000 times, for the populations of answers for sum-
maries B and C. During each iteration of sampling,
precision and recall were calculated, creating popu-
lations of 10000 recalls and 10000 precisions propa-
gated to be representative of the original population
of answers. A t-test performed on the population
of precision and the population of recalls showed
statistical significance at 95% in both cases, with
summaries C having a precision of 5% higher and
a recall of 1.4-1.6% higher than summaries B (see
Table 4). Therefore, we can say that CoreSC sum-
maries C are overall better for answering questions
than summaries B.
Comparison between B and C (B-C)
precision recall
t = -105.90 t = -32.52
df = 19959.79 df = 19994.40
p-value < 2.2e-16 p-value < 2.2e-16
alternative hypothesis: true difference in means 6= 0
95% confidence interval: 95% confidence interval:
-0.051 -0.049 -0.016 -0.014
sample estimates: sample estimates:
mean of x mean of y mean of x mean of y
0.696 0.746 0.639 0.655
Table 4: Test for statistical significance betwen sum-
maries B (microsoft) and C (CoreSC)
The difference in precision between summaries
B and C shows the advantage of having a con-
754
tent model: summaries C are significantly clearer.
We had also expected CoreSC summaries to have a
much higher coverage than summaries B, and there-
fore significantly higher recall. However, this dif-
ference was less pronounced perhaps because au-
tosummarize favours shorter sentences, which are
more likely to be found in the abstracts. We expect
that a refinement in the sentence selection criterion,
which would also take sentence length into account,
will help to showcase further the benefits of using a
CoreSC-based content model.
Analysis using ROUGE showed that while sum-
maries C had a slightly higher ROUGE-1 measure
than summaries B (0.75 vs 0.73), with respect to ab-
stracts, ROUGE-L was the same for the two (0.70).
In table 5 we also report measurements on sum-
mary informativeness based on divergence (Kull-
back Leibler (KL) divergence and Jensen Shannon
(JS) divergence), as in (Louis and Nenkova, 2013).
KL divergence is asymmetric and reflects the aver-
age number of bits wasted by coding samples of a
distribution P using another distribution Q. JS diver-
gence is an information-theoretic measure, reflect-
ing the average distance of the KL divergence be-
tween summary and input (the full paper in our case)
from the mean vocabulary distributions. Compared
to other measures, JS divergence has been found
to produce the best predictions of summary qual-
ity (Louis and Nenkova, 2013). In practice, what JS
divergence tells us is how ?different?/divergent the
summary is from the original paper. Low divergence
scores are indicative of greater overlap between the
summaries and the original paper and are considered
positive in terms of the summary information con-
tent.
type KLI-S KLS-I UnJSD SJSD
B 1.66 0.70 0.21 0.19
C 1.40 0.62 0.18 0.17
random 1.61 0.79 0.21 0.19
Table 5: Macro-averaged divergence scores for the 28
test summaries. B: Autosummarize, C: CoreSC, random:
random summaries each 20 sentences long for each paper.
KLI-S: Average Kullback Leibler divergence between in-
put and summary. KLS-I: Kullback Leibler divergence
between summary and input, since KL divergence is not
symmetric. UnJSD: Jensen Shannon divergence between
input and summary. No smoothing. SJSD:A version with
smoothing.
One can see the that CoreSC summaries have con-
sistently lower divergence (both KL and JS) than mi-
crosoft autosummarise summaries and random sum-
maries of the same length. This is a positive out-
come but since such automatic measures of sum-
mary quality have not yet reached maturity and are
harder to interpret, we consider the manual evalua-
tion a more reliable indicator of summary informa-
tiveness and usefulness. Note that it is not appropri-
ate to use divergence to assess the abstracts as this
measure is influenced by the length of a text, which
varies dramatically in the case of abstracts.
5 Conclusions and future work
We have shown how a content model based on
the scientific discourse as annotated by the CoreSC
scheme can be used to produce extractive sum-
maries. These summaries can be generated as al-
ternatives to abstracts. Since they preserve the dis-
tribution of CoreSCs in the paper and are not pro-
duced independently of it, as is the case with many
abstracts, they are potentially more representative of
abstracts than the full article. We have tested the use-
fulness CoreSC based summaries in answering com-
plex questions relating to the content of scientific
papers. Extracts from automated CoreSCs are infor-
mative, outperform microsoft autosummarise sum-
maries, in both intrinsic and extrinsic evaluation, and
enable experts to answer 66% of complex questions
with a precision of 75%.
In the future we would like to experiment further
with refining the sentence selection method so as
to consider criteria for local cohesion, such as lex-
ical chains. We would also like to perform com-
parisons with automatically induced content mod-
els and check their viability for scientific articles.
We also would like to perform a human based eval-
uation of coherence and explore the full potential
of these summaries as alternatives to author-written
abstracts. This work constitutes a very important
step in producing automatic summaries of scientific
papers and enabling experts to extract information
from the papers, a major requirement for resource
curation, which is dependent on constant reviewing
of the literature.
755
Acknowledgements
This work has been funded by an Early Career
Leverhulme Trust Fellowship to Dr Liakata and by
EMBL-EBI, UK. The authors would like to thank
Annie Louis, Yufan Guo, Simone Teufel, Stephen
Clark and the anonymous reviewers for their valu-
able comments. We would also like to thank Mo
Abrahams for the python version of the summarisa-
tion code and the cafe summary toolkit.
References
S. Ananiadou, Pyysalo S., and J. Tsujii. 2010. Event
extraction for systems biology by text mining the liter-
ature. Trends in Biotechnology, 28(7):381?390.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120. Best paper award.
R. Brandow, K. Mitze, and L. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing and Management,
31:675?685.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using latent
permutations. J. Artif. Int. Res., 36:129?163, Septem-
ber.
A. M. Cohen and W. R. Hersh. 2005. A survey of current
work in biomedical text a survey of current work in
biomedical text mining. Briefings in Bioinformatics,
6:57?71.
Danish Contractor, Yufan Guo, and Anna Korhonen.
2012. Using argumentative zones for extractive sum-
marization of scientific articles. In COLING, pages
663?678.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22:2004.
R.A. Garcia-Hernandez, Y. Ledeneva, G.M. Mendoza,
A.H. Dominguez, J. Chavez, A. Gelbukh, and J.L.T.
Fabela. 2009. Comparing commercial tools and state-
of-the-art methods for generating text summaries.
In Artificial Intelligence, 2009. MICAI 2009. Eighth
Mexican International Conference on, pages 92?96,
November.
Yufan Guo, Anna Korhonen, and Thierry Poibeau. 2011.
A weakly-supervised approach to argumentative zon-
ing of scientific documents. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 273?283. Association for
Computational Linguistics.
T. Kim, J.and Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of bionlp?09 shared task on event ex-
traction. In Proceedings of the Workshop on BioNLP:
Shared Task, pages 1?9, Boulder, Colorado.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
756
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
the 18th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?95, pages 68?73, New York, NY, USA. ACM.
M. Liakata and L.N. Soldatova. 2009. The ART Corpus.
Technical report, Aberystwyth University.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor.
2010. Corpora for the conceptualisation and zoning of
scientific papers. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation, Valetta,Malta.
M. Liakata, S. Saha, S. Dobnik, C. Batchelor, and
Rebholz-Schuhmann D. 2012. Automatic recognition
of conceptualisation zones in scientific articles and
two life science applications. Bioinformatics, 28:991?
1000.
Elizabeth DuRoss Liddy. 1991. The discourse-level
structure of empirical abstracts: an exploratory study.
Inf. Process. Manage., 27:55?81, February.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168. Asso-
ciation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically as-
sessing machine summary content without a gold stan-
dard. Computational Linguistics, 39(2):267?300.
W. C. Mann and S. A. Thompson. 1998. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1998a. Improving summarization
through rhetorical parsing tuning. In Proceedings of
The Sixth Workshop on Very Large Corpora, pages
206?215, Montreal,Canada.
Daniel C. Marcu. 1998b. The rhetorical parsing,
summarization, and generation of natural language
texts. Ph.D. thesis, Toronto, Ont., Canada, Canada.
AAINQ35238.
Kathleen R. McKeown. 1985. Text generation: using
discourse strategies and focus constraints to generate
natural language text. Cambridge University Press,
New York, NY, USA.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Texts. In Conference on Empirical
Methods in Natural Language Processing, Barcelona,
Spain.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 689?696, Morristown, NJ, USA.
Association for Computational Linguistics.
Vahed Qazvinian and Dragomir R Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th annual
meeting of the association for computational linguis-
tics, pages 555?564. Association for Computational
Linguistics.
Vahed Qazvinian, Dragomir R Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 895?903. Association for Computational Lin-
guistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In EMNLP?10, pages 377?387.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28:409?445, De-
cember.
Simone Teufel, Advaith Siddharthan, and Colin Batche-
lor. 2009. Towards discipline-independent argumen-
tative zoning: Evidence from chemistry and computa-
tional linguistics. In Proceedings of EMNLP-09, Sin-
gapore.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papersworkshop ?automatic summarization?,
naacl-2001. In NAACL-01 Workshop ?Automatic Text
Summarisation?, Pittsburgh, PA.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. CSLI Studies in Computational Linguistics.
Center for the Study of Language and Information,
Stanford, California.
Elizabeth White, K. Bretonnel Cohen, and Larry Hunter.
2011. Hypothesis and evidence extraction from full-
text scientific journal articles. In Proceedings of
BioNLP 2011 Workshop, pages 134?135, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
757
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 172?175,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Finding small molecule and protein pairs in scientific literature using a
bootstrapping method
Ying Yan, Jee-Hyub Kim, Samuel Croset, Dietrich Rebholz-Schuhmann
European Bioinformatics Institute
Wellcome Trust Genome Campus
Hinxton
Cambridge
UK
{yan, jhkim, croset, rebholz}@ebi.ac.uk
Abstract
The relationship between small molecules
and proteins has attracted attention from the
biomedical research community. In this pa-
per a text mining method of extracting small-
molecule and protein pairs from natural text
is presented, based on a semi-supervised ma-
chine learning approach. The technique has
been applied to the complete collection of
MEDLINE abstracts and pairs were extracted
and evaluated. The results show the feasibility
of the bootstrapping system, which will subse-
quently be further investigated and improved.
1 Introduction
Information extraction has become a major task in
text-mining. A large number of studies have been
carried out with the objective of developing tech-
niques to overcome the highly ambiguous and vari-
able nature of natural language for the extraction of
information from scientific text (Song et al, 2006).
Natural language processing (NLP) of biomedical
text has been initiated and used for different knowl-
edge discovery tasks such as the extraction of rela-
tionships between different types of biological ob-
jects.
Relationships between proteins and small
molecules are of particular concern in the biomed-
ical research domain. The importance of target
specific small molecule research is vital in the
scientific community?s understanding of numerous
biological processes with potential discoveries
yielding various translational benefits and outcomes
to public health and industry. While there has been
a great number of traditional studies already com-
pleted in this field, the underlying difficulty with this
type of research has been trying to understand how
one molecule interacts with a target protein. Given
the biological background, many researchers in
Cheminformatics and Metabolomics are attempting
to find the connections between small molecules
and other biological entities in order to bridge the
chemical and biological domains.
Of the few reported text mining approaches to this
problem, Temkin and Gilder (2003) was concerned
with the extraction of protein and small molecule in-
teraction, and used a rule-based approach utilising
a lexical analyser and context free grammar. Jiao
and Wild (2009) presented a technique for detect-
ing protein and small molecule interaction using a
maximum entropy based learning method; this work
also uses corpus-based machine learning. The main
drawback of both of these studies is that they require
a fully annotated corpus which is difficult to gener-
ate.
1.1 The bootstrapping method
At present a gold standard annotated corpus is not
available, and constructing a reasonable annotated
corpus would require an infeasible amount of man-
ual work. Our proposed solution to this problem
is to develop a semi-supervised machine learning
method. In this paper a bootstrapping algorithm is
presented which requires only unannotated training
texts and a handful of protein small molecule pairs,
known as seeds. The basic work of a bootstrap-
ping system can be presented as an expansion en-
gine which uses the initial seed pairs fed into the
172
system to generate patterns that are used, in turn, to
find more pairs. The operation of the algorithm is
controlled by certain criteria that are delivered from
a measurement of the quality or selectivity of pat-
terns and discovered pairs.
Bootstrapping systems have been maturely used
for information extraction purposes in other research
domains, and it has been empirically shown to be
a powerful method in learning lexico-syntactic pat-
terns for extracting specific relations (Riloff and
Jones, 1999). Bootstrapping systems can operate
with a greatly reduced number of training examples.
A bootstrapping system seems promising for the
purpose of relation extraction, making it a suitable
candidate method for protein and small molecule
pair extraction.
2 Implementation
The typical bootstrapping method was tailored in
order to improve its suitability for our extraction
task, operating in the biomedical literature resource
MEDLINE. The bootstrapping architecture is pre-
sented in Figure 1. The whole collection of MED-
LINE was filtered using a co-occurrence approach
and a named entity recogniser. In this way the
sentences which contained both a protein and a
small molecule were selected. The structure of pat-
terns which are suitable to extract protein and small
molecule pairs from MEDLINE was defined. Each
sentence is tokenized and then normalised based on
the results of syntactic parsing in order to obtain a
more generalised view of the pattern. In the fol-
lowing sections, we describe in more detail these as-
pects.
2.1 Protein and small molecule recognition
Two dictionary-based named entity recognisers
were used to detect the names of proteins and small
molecules in the full collection of MEDLINE ab-
stracts, with the two source dictionaries constructed
using the resources UniProt (Apweiler et al, 2004)
and ChEBI (De Matos et al, 2006) respectively. The
following example shows the two recognisers iden-
tify a chemical object and a protein object in a sen-
tence from a MEDLINE extract:
<chebi>Paracetamol</chebi>, 100 mg/kg, in-
hibited <uniprot>COX-1</uniprot> in stomach
Figure 1: Extraction system architecture
mucosa ex vivo much less effectively than in other
tissues.
2.2 Sentence analysis for normalisation
It was anticipated that variations in tense and other
language characteristics would cause problems in
pattern generation. We therefore applied a list of
normalisation steps for pattern generation. The sur-
rounding context in the biomedical text is not nor-
mally useful and makes it difficult to identify the text
and observe a clear sentence structure. The parsing
result normalises patterns by eliminating non-useful
components in a sentence. The step of normalisation
hence increases the quality of the pattern.
The complete list of normalisation steps is as fol-
lows:
1. Replaced the representation of measurement
units, such as mg/L and ml/day.
2. Employed the part-of-speech (POS) tagger GE-
NIA (Tsuruoka et al, 2005) to analyse each to-
ken, and the tokens which are weakly related to
the sentence structure were removed. So that,
the only remaining tokens are the head noun of
a noun phrase (NP), the verb phrase, and prepo-
sitional phrase chunks.
3. Finally a simple rule to identify the head noun
was defined. In a general case, for a NP se-
quence, the last token is considered as the head
noun. When the last token is a single character,
the second last token is considered as the head
noun.
173
Table 1: An example of a generated pattern
Seed tuple: Paracetamol, COX-1
Found string: ?CHEBI, UNIT, inhibit UNIPROT
in mucosa than in tissue.?
Pattern: NP List1, UNIT, inhibit NP List2
Constraints: NP List1=?CHEBI*?
NP List2=?UNIPROT*?
Keywords: ?,UNIT,inhibit?
The above example after these normalisation
steps becomes:
CHEBI*, UNIT, inhibit UNIPROT* in mucosa
than in tissue.
where CHEBI* and UNIPROT* are the seeds in
context.
2.3 Bootstrapping
The bootstrapping system is applied to the nor-
malised sentences. The process starts with 100
high precision protein small molecule pairs col-
lected from the ChEBI ontology. These pairs were
retrieved by querying the ChEBI sub-ontology for
the relation ?has role?. From the resulting data we
extracted small molecules that are enzyme inhibitors
together with the name of the enzyme.
2.3.1 Pattern generation and pair extraction
The concept of a bootstrapping system is that us-
ing a high precision seed pair to start the extrac-
tion engine, the system can effectively learn the pat-
tern construction rule and the pattern constraints.
Searching for the seed pairs in the corpus returns
strings which are candidate extraction patterns for
other pairs. The candidate patterns are made up of
?slots? and ?context strings?, where the slots are ei-
ther of type small-molecule or protein, and context
is the text connecting the slots and the words imme-
diately before and after the pair. By analysing the
surrounding context of the slots new elements of the
pattern are discovered, which can subsequently be
used to search for new small-molecule protein pairs.
The process of deriving a pattern from the above ex-
ample is shown in Table 1.
The generated pattern can then be used to search
the corpus and find other matching contexts. New
pairs are retrieved from the matching context by
simply locating the protein and small molecule
names from the same positions as they are in the pat-
tern.
For instance, the pattern produced in Table 1 is
matched against a normalised sentence ?data sug-
gest CHEBI, UNIT, inhibit UNIPROT?, extracting
the new pair <trifluoperazine, CaMKII>.
2.3.2 Evaluating seeds and patterns
The quality of the pattern is critical since pat-
terns that generate a bad pair can introduce more
false positive seeds. Therefore, within a bootstrap-
ping system it is necessary to have a stage of pattern
evaluation. Estimations of the confidence score of
a pattern can be used as one of the stopping criteria.
We implemented an evaluation step for both patterns
and pairs based on an evaluation method developed
by Agichtein and Gravano (2000). Adapting the ap-
proach to this work, if patterni predicts tuple t =
<chemical, protein>, and there is already a tuple
t? = <chemical, protein?> with high confidence,
and chemical from t is same as chemical from t?,
then we could define this as a positive match of pat-
tern (Ppositive), otherwise the pattern is considered
as a negative match (Pnegative). So that the confi-
dence score of pattern (P ) is estimated as:
Conf(P ) =
Ppositive
Ppositive + Pnegative
(1)
To evaluate the pairs we again employ the method
described by Agichtein and Gravano (2000). The
confidence of a particular pair is a function of the
number of patterns that generate it. Equation 2
shows how to calculate a confidence score for tuple
T , where P is the set of patterns that derive T . Ci is
the context that also contains T , Match(Ci, Pi) is
the degree of match of Ci and Pi.
Conf(T ) = 1?
?|P |
I=0 (1? (Conf (Pi) ? Match (Ci, Pi)))
(2)
3 Results and discussion
Table 2 shows the top 10 generated patterns ranked
by the frequency that they appear in MEDLINE. As
can be seen the patterns all have very simple struc-
tures. Simple patterns are more likely to be produc-
tive, i.e the simpler the structure of the pattern, the
more pairs it generates. However, simple structures
are also likely to generate more false negative pairs.
174
The pairs produced by these top 10 patterns were
collected, and the confidence score then calculated
using equation 1. The result implies that the confi-
dence score of a pattern, and in turn the selectivity
and productivity of the pattern, are strongly associ-
ated with the pattern?s structure.
Table 2: The top 10 comment patterns
Frequency Pattern Confidence
68 UNIPROT* CHEBI* CHEBI 0.16
61 CHEBI* UNIPROT* UNIPROT 0.15
51 CHEBI* UNIPROT* be 0.10
49 CHEBI* UNIPROT* CHEBI 0.10
41 UNIPROT* CHEBI* be 0.21
40 CHEBI* UNIPROT* 0.08
38 UNIPROT* CHEBI* UNIPROT 0.16
37 UNIPROT* CHEBI* 0.30
26 be CHEBI* UNIPROT* 0.26
24 UNIPROT* CHEBI CHEBI* CHEBI 0.17
3.1 Quality of the extracted pairs
One hundred pairs extracted by first and second gen-
eration patterns were randomly selected for manual
inspection by a domain expert curator. It was found
that over 60% were valid pairs. From further exami-
nation of the cases together with their extraction pat-
terns, it can be seen that the patterns have a high con-
fidence score, ensuring the quality of the extracted
pair. For instance, from the original text Paraceta-
mol, 100 mg/kg, inhibited COX-1 in stomach mucosa
ex vivo much less effectively than in other tissues, the
pattern ?CHEBI*, UNIT, inhibit UNIPROT*? with
0.62 confidence score derives a correct pair <Parac-
etamol, COX-1>.
Generally speaking, simple patterns are more
likely to have lower confidence scores. However it
was also found that the pattern quality heavily de-
pends on the quality and reliability of the name en-
tity recognition (NE) system.
4 Conclusions and future work
We have presented a method of detecting small
molecule and protein pairs in MEDLINE abstracts.
It employs semi-supervised machine learning meth-
ods to enable patterns to be automatically generated,
rather than requiring human input. The approach can
be used for high throughput text mining applications
where manual curation is unrealistic.
The first and second iteration of results are
promising and show that the approach enables many
useful small molecule protein pairs to be extracted
from MEDLINE using just a small number of seed
pairs as input. The approach makes use of a rigor-
ous method of evaluating the quality of generated
patterns and extracted pairs. Manual inspection has
been used to validate these preliminary results and
has shown that approximately half of the discovered
pairs represent valid small molecule protein relation-
ships, and we expect to improve this significantly.
In future we will develop the method further
and analyse the results after further algorithm iter-
ations, enabling discovery of new patterns and con-
sequently new pairs of proteins and small molecules
that are currently undetected.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Proceedings of the fifth ACM conference on Digital li-
braries, pages 85?94. ACM.
R. Apweiler, A. Bairoch, C.H. Wu, W.C. Barker,
B. Boeckmann, S. Ferro, E. Gasteiger, H. Huang,
R. Lopez, M. Magrane, et al 2004. UniProt: the uni-
versal protein knowledgebase. Nucleic acids research,
32(suppl 1):D115?D119.
P. De Matos, M. Ennis, M. Darsow, M. Guedj, K. Degt-
yarenko, and R. Apweiler. 2006. ChEBI-chemical en-
tities of biological interest. Nucleic Acids Research,
Database Summary: 646.
D. Jiao and D.J. Wild. 2009. Extraction of CYP chemi-
cal interactions from biomedical literature using natu-
ral language processing methods. Journal of chemical
information and modeling, 49(2):263?269.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the National Conference on Artifi-
cial Intelligence, pages 474?479. John Wiley & Sons
Ltd.
M. Song, I.Y. Song, X. Hu, and H. Han. 2006. Infor-
mation extraction in biomedical literature. In J. Wang,
editor, Encyclopedia of Data Warehousing and Data
Mining, pages 615?620. Information Science Refer-
ence.
J.M. Temkin and M.R. Gilder. 2003. Extraction
of protein interaction information from unstructured
text using a context-free grammar. Bioinformatics,
19(16):2046?2053.
Y. Tsuruoka, Y. Tateishi, J.D. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Ad-
vances in informatics, pages 382?392.
175
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 50?57,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
GRO Task: Populating the Gene Regulation Ontology with events and 
relations 
 
 
Jung-jae Kim, 
Xu Han 
School of Computer Engineering 
Nanyang Technological University 
Nanyang Avenue, Singapore 
jungjae.kim@ntu.edu.sg, 
HANX0017@e.ntu.edu.sg 
Vivian Lee 
European Bioinfor-
matics Institute 
Wellcome Trust Ge-
nome Campus 
Hinxton, Cambridge, 
UK 
vivian_clee@ 
yahoo.com 
Dietrich Rebholz-
Schuhmann 
Institute of Computational 
Linguistics 
University of Zurich  
Binzm?hlestrasse 14  
Zurich, Switzerland  
rebholz@cl.uzh.ch 
 
  
 
Abstract 
Semantic querying over the biomedical litera-
ture has gained popularity, where a semantic 
representation of biomedical documents is re-
quired. Previous BioNLP Shared Tasks exer-
cised semantic event extraction with a small 
number of pre-defined event concepts. The 
GRO task of the BioNLP?13-ST imposes the 
challenge of dealing with over 100 GRO con-
cepts. Its annotated corpus consists of 300 
MEDLINE abstracts, and an analysis of inter-
annotator agreement on the annotations by two 
experts shows Kappa values between 43% and 
56%. The results from the only participant are 
promising with F-scores 22% (events) and 
63% (relations), and also lead us to open is-
sues such as the need to consider the ontology 
structure. 
1 Background 
As semantic resources in the biomedical domain, 
including ontologies and linked data, increase, 
there is a demand for semantic querying over the 
biomedical literature, instead of the keyword 
searching supported by conventional search en-
gines (e.g. PubMed). The semantic search re-
quires adapting Semantic Web technologies to 
the literature, to analyze the complex semantics 
described in biomedical documents and to repre-
sent them with ontology concepts and relations. 
The ontology-based formal semantics will then 
form a Semantic Web. The GRO task of the 
BioNLP Shared Tasks 2013 is to provide a plat-
form to develop and evaluate systems for identi-
fying complex semantic representation of bio-
medical documents in the domain of gene regula-
tion. 
   There are solutions for servicing the ontology 
concepts recognized in the biomedical literature, 
including TextPresso (M?ller et al, 2004) and 
GoPubMed (Doms and Schroeder, 2005). They 
utilize term recognition methods to locate the 
occurrences of ontology terms, together with 
terminological variations. Systems like EBIMed 
(Rebholz-Schuhmann et al, 2007) and FACTA 
(Tsuruoka et al, 2008) go further to collect and 
display co-occurrences of ontology terms. How-
ever, they do not extract events and relations of 
the semantic types defined in ontologies. 
The annotation of those ontology event and re-
lation instances described in text was initiated in 
the biomedical domain by the GENIA corpus 
(Kim et al, 2003), and the tasks of the BioNLP 
Shared Tasks 2009 and 2011 aimed at automati-
cally identifying such ontological annotations. 
However, the tasks dealt only with a small num-
ber of ontology concepts (less than 20 unique 
concepts in total), considering the thousands of 
concepts defined in standard biomedical ontolo-
gies (e.g. Gene Ontology, anatomy ontologies). 
The goal of the Gene Regulation Ontology 
(GRO) task is to confirm if text mining tech-
niques can be scaled up to cover hundreds of 
(and eventually thousands of) concepts, and 
thereby to address the complex semantic repre-
sentation of biomedical documents. 
The GRO task is to automatically annotate bi-
omedical documents with the Gene Regulation 
Ontology (Beisswanger et al, 2008). GRO is a 
50
conceptual model of gene regulation and in-
cludes 507 concepts, which are cross-linked to 
such standard ontologies as Gene Ontology and 
Sequence Ontology and are integrated into a 
deep hierarchical structure via is-a and part-of 
relations. Note that many of the GRO concepts 
are more specific than those used in the previous 
BioNLP Shared Tasks. The GRO is one of the 
first ontological resources that bring together 
different types of ontology concepts and relations 
in a coherent structure. It has two top-level cate-
gories of concepts, Continuant and Occurrent, 
where the Occurrent branch has concepts for 
processes that are related to the regulation of 
gene expression (e.g. Transcription, 
RegulatoryProcess), and the Continuant branch 
has concepts mainly for physical entities that are 
involved in those processes (e.g. Gene, Protein, 
Cell). It also defines semantic relations (e.g. 
hasAgent, locatedIn) that link the instances of the 
concepts. The GRO task in the BioNLP Shared 
Task (ST) 2013 assumes that the instances of 
Continuant concepts are provided and focuses on 
extracting the instances of the events and rela-
tions defined in the GRO. 
This paper is organized as follows: We de-
scribe the manual construction of the training 
and test datasets for the task in Section 2 and ex-
plain the evaluation criteria and the results in 
Section 3. 
2 Corpus annotation 
2.1 Annotation elements 
The BioNLP?13-ST GRO task follows the repre-
sentation and task setting of the ST?09 and ST?11 
main tasks. The representation involves three 
primary categories of annotation elements: enti-
ties (i.e. the instances of Continuant concepts), 
events (i.e. those of Occurrent concepts) and re-
lations. Mentions of entities in text can be either 
contiguous or discontinuous spans that are as-
signed the most specific and appropriate Contin-
uant concepts (e.g. TranscriptionFactor, 
CellularComponent). The event annotation is 
associated with the mention of a contiguous span 
in text (called event trigger) that explicitly sug-
gests the annotated event type (e.g. ?controls? - 
RegulatoryProcess). If a participant of an event, 
either an entity or another event, can be explicit-
ly identified with a specific mention in text, the 
participant is annotated with its role in the event. 
In this task, we consider only two types of roles 
(i.e. hasAgent, hasPatient), where an agent of an 
event is the entity that causes or initiates the 
event (e.g. a protein that causes a regulation 
event), and a patient of an event is the entity up-
on which the event is carried out (e.g. the gene 
that is expressed in a gene expression event) 
(Dowty, 1991). The semantic relation annotation 
is to annotate other semantic relations (e.g. 
locatedIn, fromSpecies) between entities and/or 
events, without event triggers. Figure 1 illustrates 
some of the annotations.  
2.2 Document selection  
The corpus texts are selected based on the rele-
vance to the topic of gene regulation in humans. 
Specifically, we first obtained a list of human 
transcription factors (TFs) and then used Pub-
Med to collect a set of candidate documents. A 
random subset of 300 documents was then se-
lected for the GRO task from the collection. We 
annotated entities, events, and relations in them, 
and divided them into three subsets of 150 (train-
ing), 50 (development), and 100 (test) docu-
ments. In fact, 100 out of the 200 documents for 
training and development are from Kim et al 
(2011a), though we revised and updated their 
annotations based on new annotation guidelines, 
some of which are explained below.  
2.3 Annotation guidelines 
The first step of annotating ontology concepts in 
the text is the recognition of a word or a phrase 
that refers to a concept of the GRO. Such a word 
or phrase, called mention, is one of the names of 
the concept, its synonyms, or expressions that are 
semantically equivalent to or subsumed by the 
concept. For each mention, we annotate it with 
the single, most specific and appropriate concept, 
but not with any general concept. For example, if 
Figure 1. Example annotations of the GRO corpus 
51
a protein is clearly mentioned as a transcription 
factor in the text, we annotate it with the GRO 
concept TranscriptionFactor, not with Protein. 
There are many issues in the annotation, and 
we here introduce our guidelines on two of them 
about complex noun phrases and overlapping 
concepts.  
1) If a noun phrase refers to an event that cor-
responds to an Occurrent concept and includes 
mentions of other concepts, we consider sepa-
rately annotating the multiple mentions in the 
phrase with concepts and relations. For example 
in the phrase ?nephric duct formation?, we anno-
tate it as follows:  
? ?formation?:CellularProcess hasPatient 
?nephric duct?:Cell 
This means that the phrase indicates an individu-
al of CellularProcess, which is an event of form-
ing an entity of Cell, which is nephric duct. An-
other example noun phrase that involves multiple 
mentions is ?Sim-2 mRNA expression?, which is 
annotated as follows: 
? ?expression?:GeneExpression hasPatient 
(?mRNA?:MessangerRNA encodes 
?Sim-2?:Gene) 
However, we do not allow such multi-mention 
annotation on e.g.  
? ?mRNA expression?, because this phrase 
is too generic and frequent so that a mul-
ti-mention annotation for it, ?expres-
sion?:GeneExpression hasPatient 
?mRNA?:MessangerRNA, does not en-
code any ?useful? information 
?  ?nuclear factor?, because this factor is 
not always located in nucleus. 
Therefore, we decided that, in general, we avoid 
annotation of generic information, but consider a 
thread of information specific only if it involves 
specific entities like individual gene/protein and 
cell (e.g. Sim-2, nephric duct). Also, we did not 
divide a noun phrase to multiple mentions if the 
relation between the mentions is not always true 
(cf. ?nuclear factor? ? ?factor?:Protein locatedIn 
?nuclear?:Nucleus). 
2) As some GRO concepts are overlapping, we 
made the following guidelines: 
(a) When there is ambiguity between Increase 
(Decrease), Activation (Inhibition), and 
PositiveRegulation (NegativeRegulation), we 
annotate 
o binary relations with PositiveRegulation, 
ignoring Activation 
(e.g., ?augment?:PositiveRegulation hasAgent 
?Nmi?:Protein hasPatient (?recruit-
ment?:Transport hasPatient ?coactivator pro-
tein?: TranscriptionCoactivator)) 
o unary relations with Increase 
(e.g., ?enhance?:Increase hasPatient ?transcrip-
tion?:Transcription) 
   Note that we cannot exchange the two concepts 
of PositiveRegulation and Increase in the two 
examples due to the arity restriction. 
(b) Binding concepts are ambiguous. We anno-
tate as follows: 
o For such a GRO concept as "Binding of 
A to B", A should be the agent and B the 
patient. 
(For example, when we annotate 
BindingOfProteinToDNA and 
BindingOfTFToTFBindingSiteOfProtein, Protein 
and TF will be agents, and DNA and 
BindingSiteOfProtein will be patients, respec-
tively.) 
o For such a GRO concept as "Binding to 
A" for binary relation between two enti-
ties of the same type, both entities should 
be patients. 
(For example, in the events of binding between 
proteins with BindingToProtein and of binding 
between RNAs with BindingToRNA, the pro-
teins and the RNAs, respectively, will all be pa-
tients.) 
Other annotation guidelines can be found at the 
task homepage1. 
2.4 Annotation 
Two annotators with biology background anno-
tated the documents with GRO entities, events 
and relations. They used the Web-based annota-
tion tool brat (Stenetorp et al, 2012) for the an-
notation. Annotator A is the one who annotated 
the earlier version of the corpus (Kim et al, 
2011a). He first revised the earlier version of 100 
abstracts (named Set 1) and drafted the new an-
notation guidelines. Annotator B studied the 
drafted annotations and guidelines and then fur-
ther revised them, and the two annotators togeth-
er updated and made agreements on final ver-
sions of the annotations and guidelines. They 
selected two more sets of 100 abstracts each 
(named Sets 2 and 3), where Set 2 was combined 
with Set 1 to become the training and develop-
ment datasets, and Set 3 became the test dataset. 
They updated the guidelines after annotating Sets 
2 and 3 independently and together combining 
their annotations. 
                                                 
1 http://nlp.sce.ntu.edu.sg/wiki/projects/bionlpst13grotask/ 
52
We estimated the inter-annotator agreement 
(IAA) between the two annotators for Sets 2 and 
3 with Kappa measures as shown in Table 1. The 
Kappa values between 43% and 56% are moder-
ately acceptable, though not substantial, which is 
expected with the high degree of the ontology?s 
complexity and also with the high number of 
mentions (56 per abstract; see Table 2). Note that 
the agreement is met, only when the two annota-
tors annotate the same concept on the same men-
tion with the same boundaries and, if any, the 
same roles/arguments, not considering the gener-
alization criteria used for evaluation (see Section 
3 for details). If we relax the boundary restriction 
(i.e. approximate span matching of (Kim et al, 
2009)), the Kappa values for events slightly in-
crease to 47% (Set 2) and 45% (Set 3). Also note 
that the agreement on relations is higher than 
those on entities and events.  
We analyzed the different annotations by the 
two annotators as follows: As for the entity anno-
tations, 84% of the differences are boundary 
mismatches, while the rest are due to mismatch 
of entity types and to missing by either of the 
annotators. As for the event annotations, 56% of 
the differences are also boundary mismatches, 
and 31% are missed by either of the annotators. 
The majority (71%) of the differences in relation 
annotations are due to missing by either annota-
tor, while the rest are mostly due to the differ-
ences in the entity annotations. 
One negative finding is that the agreement did 
not always increase from Set 2 to Set 3, which 
means the two annotators did not improve the 
alignment of their understanding about the anno-
tation even after making agreements on Set 2 
annotations. It may be too early to conclude, and 
the Kappa value might increase as the annotators 
examine more examples, since the annotation 
corpus size in total (Sets 1,2,3 together) is still 
small compared to the total number of GRO con-
cepts. After examining the IAA, we integrated 
the independently annotated sets and released the 
final versions of the three datasets at the task 
homepage. 
 
Table 1. Inter-annotator agreement re-
sults 
 Set 2 Set 3 
Entities  44.6% 43.8% 
Events  45.8% 43.2% 
Relations  54.7% 55.9% 
All 46.2% 45.3% 
 
2.5 Statistics 
Table 2 shows the number of MEDLINE ab-
stracts in each of the three datasets: training, de-
velopment, and test datasets. It also shows the 
number of instances for each of the following 
annotation types: entities (i.e. instances of Con-
tinuant concepts), event mentions (i.e. event trig-
gers), event instances (i.e. instances of Occurrent 
concepts), and relation instances. Note that rela-
tion instances are not associated with mentions 
like event instances. It also shows the number of 
unique entity/event types (i.e. unique GRO con-
cepts) used in the annotation of each dataset. The 
total number of unique entity types in the three 
datasets is 174, and that of unique event types is 
126. 
 
Table 2. Number of annotation elements 
 Train Dev. Test 
No. of documents 150 50 100
No. of entity mentions 5902 1910 4007
No. of event mentions 2005 668 2164
No. of event instances 2175 747 2319
No. of event instances 
with agents 
693 251 625
No. of event instances 
with patients 
1214 451 1467
No. of relation instances 1964 581 1287
No. of unique entity types 128 94 147
No. of unique event types 98 72 100
 
  Note that the frequency of event instances in the 
test dataset (23.2 per document) is much higher 
than those in the training and development da-
tasets (14.5 and 14.9 per document, respective-
ly). We compared the three datasets and ob-
served that several event types (e.g. 
GeneticModification), which are popular in the 
test dataset (e.g. GeneticModification is the 12th 
frequent type (2.3%)), seldom appear in the other 
two datasets. It may indicate that the annotators 
were getting aware of (or familiar with) more 
GRO concepts as they annotate more documents, 
where the test dataset is the last annotated. This 
sudden increase of frequency did not happen for 
the entity annotations, possibly because the two 
annotators were provided with candidate entity 
annotations, though of low quality, from a pre-
liminary dictionary-based entity recognition 
method and modified them. 
  Table 3 shows the number of mentions for the 
most frequent top-level Continuant concepts 
such as InformationBiopolymer, whose sub-
concepts include Gene and Protein, Cell, and 
53
ExperimentalMethod. Please note that these fre-
quent concepts are closely related to the topic of 
gene regulation, and that this distribution may 
reflect to some degree the distribution of terms in 
the sub-domain of gene regulation, but not that in 
the whole MEDLINE. If you like to see the de-
scendant concepts of those top-level concepts, 
please refer to the latest version of the GRO2.  
 
Table 3. Number of mentions for frequent 
top-level Continuant concepts 
Level 2 Level 3 Level 4 Count 
Continuant/PhysicalContinuant 3647 
 MolecularEntity 2805 
 InformationBiopo
lymer 
2508 
 ComplexMolecula
rEntity 
140 
 Chemical 127 
 Ligand 27 
 LivingEntity 584 
 Cell 306 
 Organism 268 
 Tissue 170 
 CellComponent 77 
Continuant/NonPhysicalContinuant 359 
 ExperimentalMethod 123 
 Function 111 
 MolecularStructure 66 
 Locus 25 
 Phenotype 11 
 
  Table 4 shows the number of event instances 
for the most frequent top-level Occurrent con-
cepts. Table 5 shows the number of instances for 
each relation. 
 
Table 4. Number of event instances for 
frequent top-level Occurrent concepts 
Level 3 Level 4 Count 
Occurrent/Process/RegulatoryProcess 782 
 PositiveRegulation 217 
 NegativeRegulation 186 
Occurrent/Process/MolecularProcess 422 
 IntraCellularProcess 189 
Occurrent/Process/PhysiologicalProcess 418 
 OrganismalProcess 143 
Occurrent/Process/PhysicalInteraction 312 
 Binding 296 
Occurrent/Process/Mutation 82 
Occurrent/Process/Localization 77 
                                                 
2 http://www.ebi.ac.uk/Rebholz-srv/GRO/GRO.html 
 Transport 16 
Occurrent/Process/Decrease 73 
Occurrent/Process/Affecting 64 
 Maintenance 20 
Occurrent/Process/ExperimentalInterve
ntion 
54 
 GeneticModification 54 
Occurrent/Process/Increase 49 
Occurrent/Process/ResponseProcess 38 
 ResponseToChemicalStimul
us 
13 
 
Table 5. Number of relation instances 
Relation Count Relation Count 
locatedIn 405 hasPart 403 
fromSpecies 274 hasFunction 82 
resultsIn 56 encodes 49 
precedes 17 hasQuality 1 
 
3 Evaluation 
There was one submission for the GRO task of 
the BioNLP?13-ST, designated as ?TEES-2.1? 
(Bj?rne and Salakoski, 2013). For comparison 
purposes, the GRO task organizers produced re-
sults with a preliminary system by adapting our 
existing system, designated as OSEE (Kim and 
Rebholz-Schuhmann, 2011b), for event extrac-
tion and developing a simple machine learning 
model for relation identification. We describe 
these two systems briefly and compare their re-
sults with several criteria. 
3.1 System descriptions 
TEES-2.1 is based on multi-step SVM classifica-
tion, which automatically learns event annotation 
rules to train SVM classifiers and applies the 
classifiers for 1) locating triggers, 2) identifying 
event arguments, and 3) selecting candidate 
events.  
OSEE is a pattern matching system that learns 
language patterns for event extraction from the 
training dataset and applies them to the test da-
taset. It performs the three steps of TEES-2.1 in a 
single step of pattern matching, thus requiring a 
huge amount of patterns (eventually, a pattern for 
each combination of the features from the three 
steps) and failing to consider that many features 
of a step are independent from other steeps and 
also from event types and can thus be general-
ized.  
We added a simple Na?ve Bayes model to the 
system for identifying (binary) semantic relations 
between entities, which utilizes such features as 
54
entity strings, the distance between them, and the 
shortest path between the two entities in the de-
pendency structure of the source sentence, which 
is identified by Enju parser (Sagae et al, 2007). 
3.2 Evaluation criteria 
The GRO task follows some of the evaluation 
criteria of the Genia Event Extraction (GE) task 
of BioNLP-ST 2009 (Kim et al, 2009), includ-
ing strict and approximate matching, and also 
introduce new criteria that consider 1) the hierar-
chical structure of the GRO and 2) parent and/or 
grandparent of answer concept. We here explain 
these new criteria in detail. 
1) In this scheme of evaluation, the event re-
sults of a participant are classified into the GRO 
concepts at the third level (see Table 4 for exam-
ples), which are ancestors of their labeled clas-
ses, and the evaluation results are accumulated 
for each of those concepts at the third level. This 
scheme may give us insights on which categories 
the participant system shows strength or weak-
ness. 
2) This scheme is to deal with such a case that 
the answer class is "GeneExpression", but a par-
ticipant gives "IntraCellularProcess" or 
"MolecularProcess", which are the parent and 
grandparent of the answer class, thus not entirely 
wrong nor too generic. For example, the scheme 
"Allowing parents" allows "IntraCellularProcess" 
to be a correct match to the answer class 
"GeneExpression", as well as the answer class 
itself. "Allowing grandparents" accepts the 
grandparents of answer classes as well as the 
parents. 
3.3 Evaluation results 
Table 6 shows the evaluation results of the two 
systems. Note that all the evaluation results in 
terms of precision, recall, and F-score in all the 
tables are percentages. The performance of the 
TEES-2.1 systems, which is clearly better than 
the OSEE system, is lower than its performance 
for other tasks of the BioNLP?13-ST, which is 
understandable, considering 1) the higher num-
ber of GRO concepts than those for the other 
tasks and 2) the low Kappa value of the inter-
annotator agreement. 
It also shows that the evaluation scheme that al-
lows the parents/grandparents of answer con-
cepts for acceptance does not greatly help in-
creasing the performance, which may mean that 
the systems are designed to aim individual con-
cepts, not considering the ontology structure. 
This issue of considering the structure of the on-
tology in event extraction can be an interesting 
future work. 
 
Table 6. Evaluation results (percentage) 
Evaluation 
scheme 
TEES-2.1 OSEE 
R P F R P F 
Strict match-
ing 
15 37 22 10 18 13 
Approximate 
boundary 
matching 
16 39 23 10 20 14 
Approximate 
recursive 
matching 
16 39 23 12 20 15 
Allowing par-
ents 
16 38 23 10 19 13 
Allowing 
grandparents 
16 38 23 10 19 13 
 
Table 7 shows the performance of the systems 
for different event categories in the third level of 
the GRO. It shows that the systems are good at 
extracting events of the categories of 
MolecularProcess (e.g. GeneExpression) and 
Localization (e.g. Transport), but are, expectedly, 
poor at extracting events of the categories with 
small number of training data (e.g. Decrease, 
ResponseProcess). 
 
Table 7. Evaluation results grouped into 
3rd-level GRO concepts (%) 
3rd-level con-
cept 
TEES-2.1 OSEE 
R P F R P F 
RegulatoryPr
ocess 
12 24 16 10 11 11
MolecularPro
cess 
30 60 40 23 51 31
Physiological
Process 
9 78 17 6 25 9
PhysicalIntera
ction 
18 33 24 3 6 4
Mutation 16 39 23 1 8 2
Localization 21 62 31 16 55 24
Decrease 3 12 4 0 0 0
Affecting 2 50 3 0 0 0
Increase 8 8 8 0 0 0
ResponseProc
ess 
3 8 4 5 50 10
 
  Table 8 shows the performance of the systems 
for the most frequent concepts and also for some 
selected infrequent concepts. From the results, 
we observe that the system performance for an 
event class does not reflect the number of train-
55
ing data of the class, and that the performance of 
the syntactic pattern matching system OSEE is 
high for the event classes, for which the machine 
learning system TEES-2.1 also performs well. 
These observations may indicate that the current 
approaches to event extraction deal with event 
types independently, not considering the hierar-
chical (or semantic) relations between the event 
types nor relations between entity types. 
 
Table 8. Evaluation results for frequent 
and infrequent individual concepts (%) 
Event class 
(Count) 
TEES-2.1 OSEE 
R P F R P F 
RegulatoryPr
ocess (224) 
18 23 20 13 13 13
PositiveRegul
ation (217) 
11 22 15 11 9 9
NegativeRegu
lation (186) 
12 23 16 14 10 12
GeneExpressi
on (160) 
59 72 65 46 67 55
Disease (143) 0 0 0 1 100 3
Decrease (73) 3 12 4 0 0 0
Localization 
(61) 
16 71 27 20 60 30
Development
alProcess (61) 
23 82 36 23 78 35
BindingOfPro
teinToDNA 
(55) 
13 15 14 0 0 0
GeneticModif
ication (54) 
0 0 0 0 0 0
 
  Table 9 shows the performance of the systems 
for the GRO relations. These results of TEES in 
the relation identification of the GRO task (F-
scores between 50% and 87%) are much higher 
than the best results of relation identification 
(40% F-score) in the Bacteria Biotopes (BB) task 
(N?dellec et al, 2013), which is to extract rela-
tions of localization and part-of. Though the two 
relation identification tasks of GRO and BB can-
not be directly compared due to many differ-
ences (e.g. entity types, relation types, corpus 
sources), it may indicate that the GRO task cor-
pus has been annotated consistently enough to 
train a model with such high performance and 
that the low performance of event extraction 
compared to relation identification may be due to 
the big number of event types and would be re-
solved as the corpus size increases. 
 
Table 9. Evaluation results for relations 
(%) 
Relation TEES-2.1 OSEE 
R P F R P F 
locatedIn 45 83 58 66 38 48
hasPart 45 81 58 76 22 34
fromSpecies 80 96 87 89 41 56
hasFunction 38 73 50 62 20 30
encodes 49 89 63 45 2 5
Total 49 86 63 72 23 35
 
4 Conclusion 
The main challenge in this task is the increased 
size of the underlying ontology (i.e. GRO) and 
the more complex semantic representation in 
GRO compared to those in other ontologies used 
for ontology-based event extraction. The com-
plex structure of the GRO enables us to evaluate 
participant systems at different abstrac-
tion/generalization levels. The evaluation results 
of the participant are quite promising, leading us 
to open issues in this direction, including the in-
corporation of ontology structure in event extrac-
tion. We plan to extend the corpus semi-
automatically by incrementally updating the 
event extraction system with more training data. 
References  
E. Beisswanger, V. Lee, J.-J. Kim, D. Rebholz-
Schuhmann, A. Splendiani, O. Dameron, S. 
Schulz, and U. Hahn, ?Gene Regulation Ontology 
(GRO): design principles and use cases,? Stud 
Health Technol Inform, vol. 136, pp. 9?14, 2008. 
Jari Bj?rne, Tapio Salakoski. TEES 2.1: Automated 
annotation scheme learning in the BioNLP 2013 
Shared Task. In proceedings of the workshop of 
BioNLP 2013 Shared Task, 2013. (submitted) 
A. Doms, M. Schroeder. GoPubMed: exploring Pub-
Med with the gene ontology. Nucleic Acids Res 
2005; 33:W783?6. 
D. Dowty. Thematic Proto-Roles and Argument Se-
lection. Language 67(3):547-619, 1991. 
J.D. Kim, T. Ohta, Y. Tateisi et al GENIA corpus - a 
semantically annotated corpus for bio-text mining. 
Bioinformatics 19:i180-i182, 2003. 
J.D. Kim, T. Ohta, S. Pyysalo et al Overview of 
BioNLP'09 shared task on event extraction. In Pro-
ceedings of the Workshop on Current Trends in Bi-
omedical Natural Language Processing: Shared 
Task, Association for Computational Linguistics, 
pp. 1-9, 2009. 
56
Jung-Jae Kim, Xu Han and WatsonWei Khong Chua. 
Annotation of biomedical text with Gene Regula-
tion Ontology:Towards Semantic Web for biomed-
ical literature. In Proceedings of LBM 2011, 
pp.63?70, 2011a. 
Jung-jae Kim, Dietrich Rebholz-Schuhmann. Improv-
ing the extraction of complex regulatory events 
from scientific text by using ontology-based infer-
ence. Journal of Biomedical Semantics 2(Suppl 
5):S3, 2011b. 
H.M. M?ller, E.E. Kenny, P.W. Sternberg. 
Textpresso: an ontology-based information retriev-
al and extraction system for biological literature. 
PLoS Biol 2:e309, 2004. 
Claire N?dellec, Robert Bossy, Jin-Dong Kim, Jung-
jae Kim, Tomoko Ohta, Sampo Pyysalo, Pierre 
Zweigenbaum. Overview of BioNLP Shared Task 
2013. Proc Workshop BioNLP Shared Task 2013, 
ACL 2013, 2013. (to appear) 
D. Rebholz-Schuhmann, H. Kirsch, M. Arregui, et al 
EBIMed: text crunching to gather facts for proteins 
from Medline. Bioinformatics 23:e237?44, 2007. 
Kenji Sagae, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG Parsing with Shallow Dependency 
Constraints. In Proceedings of ACL 2007, 2007. 
P. Stenetorp, S. Pyysalo, G. Topic, T. Ohta, S. 
Ananiadou, and J. ichi Tsujii, ?brat: a Web-based 
Tool for NLP-Assisted Text Annotation,? EACL. 
The Association for Computer Linguistics, pp. 
102?107, 2012.  
Yoshimasa Tsuruoka, Jun'ichi Tsujii, and Sophia 
Ananiadou. FACTA: a text search engine for find-
ing associated biomedical concepts. Bioinformatics 
24(21):2559-2560, 2008.  
 
57
