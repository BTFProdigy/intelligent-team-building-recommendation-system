Proceedings of the ACL 2007 Demo and Poster Sessions, pages 73?76,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Feature Based Approach to Leveraging Context for Classifying 
Newsgroup Style Discussion Segments 
Yi-Chia Wang, Mahesh Joshi 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{yichiaw,maheshj}@cs.cmu.edu 
Carolyn Penstein Ros? 
Language Technologies Institute/  
Human-Computer Interaction Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213 
cprose@cs.cmu.edu 
 
 
Abstract 
On a multi-dimensional text categorization 
task, we compare the effectiveness of a fea-
ture based approach with the use of a state-
of-the-art sequential learning technique that 
has proven successful for tasks such as 
?email act classification?.  Our evaluation 
demonstrates for the three separate dimen-
sions of a well established annotation 
scheme that novel thread based features 
have a greater and more consistent impact 
on classification performance.  
1 Introduction 
The problem of information overload in personal 
communication media such as email, instant mes-
saging, and on-line discussion boards is a well 
documented phenomenon (Bellotti, 2005).  Be-
cause of this, conversation summarization is an 
area with a great potential impact (Zechner, 2001). 
What is strikingly different about this form of 
summarization from summarization of expository 
text is that the summary may include more than 
just the content, such as the style and structure of 
the conversation (Roman et al, 2006).  In this pa-
per we focus on a classification task that will even-
tually be used to enable this form of conversation 
summarization by providing indicators of the qual-
ity of group functioning and argumentation. 
Lacson and colleagues (2006) describe a form of 
conversation summarization where a classification 
approach is first applied to segments of a conversa-
tion in order to identify regions of the conversation 
related to different types of information.  This aids 
in structuring a useful summary.  In this paper, we 
describe work in progress towards a different form 
of conversation summarization that similarly lev-
erages a text classification approach.  We focus on 
newsgroup style interactions.  The goal of assess-
ing the quality of interactions in that context is to 
enable the quality and nature of discussions that 
occur within an on-line discussion board to be 
communicated in a summary to a potential new-
comer or group moderators.   
We propose to adopt an approach developed in 
the computer supported collaborative learning 
(CSCL) community for measuring the quality of 
interactions in a threaded, online discussion forum 
using a multi-dimensional annotation scheme 
(Weinberger & Fischer, 2006).  Using this annota-
tion scheme, messages are segmented into idea 
units and then coded with several independent di-
mensions, three of which are relevant for our work, 
namely micro-argumentation, macro-
argumentation, and social modes of co-
construction, which categorizes spans of text as 
belonging to one of five consensus building cate-
gories.  By coding segments with this annotation 
scheme, it is possible to measure the extent to 
which group members? arguments are well formed 
or the extent to which they are engaging in func-
tional or dysfunctional consensus building behav-
ior. 
This work can be seen as analogous to work on 
?email act classification? (Carvalho & Cohen, 
2005).  However, while in some ways the structure 
of newsgroup style interaction is more straightfor-
ward than email based interaction because of the 
unambiguous thread structure (Carvalho & Cohen, 
2005), what makes this particularly challenging 
73
from a technical standpoint is that the structure of 
this type of conversation is multi-leveled, as we 
describe in greater depth below.   
We investigate the use of state-of-the-art se-
quential learning techniques that have proven suc-
cessful for email act classification in comparison 
with a feature based approach.  Our evaluation 
demonstrates for the three separate dimensions of a 
context oriented annotation scheme that novel 
thread based features have a greater and more con-
sistent impact on classification performance.  
2 Data and Coding 
We make use of an available annotated corpus of 
discussion data where groups of three students dis-
cuss case studies in an on-line, newsgroup style 
discussion environment (Weinberger & Fischer, 
2006).  This corpus is structurally more complex 
than the data sets used previously to demonstrate 
the advantages of using sequential learning tech-
niques for identifying email acts (Carvalho & 
Cohen, 2005).  In the email act corpus, each mes-
sage as a whole is assigned one or more codes.  
Thus, the history of a span of text is defined in 
terms of the thread structure of an email conversa-
tion. However, in the Weinberger and Fischer cor-
pus, each message is segmented into idea units.  
Thus, a span of text has a context within a message, 
defined by the sequence of text spans within that 
message, as well as a context from the larger 
thread structure.  
The Weinberger and Fischer annotation scheme 
has seven dimensions, three of which are relevant 
for our work.  
1. Micro-level of argumentation [4 categories] 
How an individual argument consists of a 
claim which can be supported by a ground 
with warrant and/or specified by a qualifier  
2. Macro-level of argumentation [6 categories] 
Argumentation sequences are examined in 
terms of how learners connect individual ar-
guments to create a more complex argument 
(for example, consisting of an argument, a 
counter-argument, and integration)  
3. Social Modes of Co-Construction [6 catego-
ries] To what degree or in what ways learn-
ers refer to the contributions of their learn-
ing partners, including externalizations, 
elicitations, quick consensus building, inte-
gration oriented consensus building, or con-
flict oriented consensus building, or other. 
For the two argumentation dimensions, the most 
natural application of sequential learning tech-
niques is by defining the history of a span of text in 
terms of the sequence of spans of text within a 
message, since although arguments may build on 
previous messages, there is also a structure to the 
argument within a single message.  For the Social 
Modes of Co-construction dimension, it is less 
clear.  However, we have experimented with both 
ways of defining the history and have not observed 
any benefit of sequential learning techniques by 
defining the history for sequential learning in terms 
of previous messages.  Thus, for all three dimen-
sions, we report results for histories defined within 
a single message in our evaluation below. 
3 Feature Based Approach 
In previous text classification research, more atten-
tion to the selection of predictive features has been 
done for text classification problems where very 
subtle distinctions must be made or where the size 
of spans of text being classified is relatively small.  
Both of these are true of our work. For the base 
features, we began with typical text features ex-
tracted from the raw text, including unstemmed uni-
grams and punctuation.  We did not remove stop 
words, although we did remove features that occured 
less than 5 times in the corpus.  We also included a 
feature that indicated the number of words in the 
segment. 
 
Thread Structure Features. The simplest context-
oriented feature we can add based on the threaded 
structure is a number indicating the depth in the 
thread where a message appears.  We refer to this 
feature as deep.  This is expected to improve per-
formance to the extent that thread initial messages 
may be rhetorically distinct from messages that 
occur further down in the thread.  The other con-
text oriented feature related to the thread structure 
is derived from relationships between spans of text 
appearing in the parent and child messages.  This 
feature is meant to indicate how semantically re-
lated a span of text is to the spans of text in the 
parent message.  This is computed using the mini-
mum of all cosine distance measures between the 
vector representation of the span of text and that of 
each of the spans of text in all parent messages, 
74
which is a typical shallow measure of semantic 
similarity.  The smallest such distance measure is 
included as a feature indicating how related the 
current span of text is to a parent message.  
 
Sequence-Oriented Features. We hypothesized that 
the sequence of codes within a message follows a 
semi-regular structure.  In particular, the discussion 
environment used to collect the Weinberger and 
Fischer corpus inserts prompts into the message 
buffers before messages are composed in order to 
structure the interaction.  Users fill in text under-
neath these prompts.  Sometimes they quote mate-
rial from a previous message before inserting their 
own comments.  We hypothesized that whether or 
not a piece of quoted material appears before a 
span of text might influence which code is appro-
priate.  Thus, we constructed the fsm feature, 
which indicates the state of a simple finite-state 
automaton that only has two states. The automaton 
is set to initial state (q0) at the top of a message. It 
makes a transition to state (q1) when it encounters a 
quoted span of text.  Once in state (q1), the automa-
ton remains in this state until it encounters a 
prompt. On encountering a prompt it makes a tran-
sition back to the initial state (q0).  The purpose is 
to indicate places where users are likely to make a 
comment in reference to something another par-
ticipant in the conversation has already contributed. 
4 Evaluation 
The purpose of our evaluation is to contrast our 
proposed feature based approach with a state-of-
the-art sequential learning technique (Collins, 
2002).  Both approaches are designed to leverage 
context for the purpose of increasing classification 
accuracy on a classification task where the codes 
refer to the role a span of text plays in context.   
We evaluate these two approaches alone and in 
combination over the same data but with three dif-
ferent sets of codes, namely the three relevant di-
mensions of the Weinberger and Fischer annota-
tion scheme.  In all cases, we employ a 10-fold 
cross-validation methodology, where we apply a 
feature selection wrapper in such as way as to se-
lect the 100 best features over the training set on 
each fold, and then to apply this feature space and 
the trained model to the test set.  The complete 
corpus comprises about 250 discussions of the par-
ticipants.  From this we have run our experiments 
with a subset of this data, using altogether 1250 
annotated text segments. Trained coders catego-
rized each segment using this multi-dimensional 
annotation scheme, in each case achieving a level 
of agreement exceeding .7 Kappa both for segmen-
tation and coding of all dimensions as previously 
published (Weinberger & Fischer, 2006). 
For each dimension, we first evaluate alternative 
combinations of features using SMO, Weka?s im-
plementation of Support Vector Machines (Witten 
& Frank, 2005).  For a sequential learning algo-
rithm, we make use of the Collins Perceptron 
Learner (Collins, 2002).  When using the Collins 
Perceptron Learner, in all cases we evaluate com-
binations of alternative history sizes (0 and 1) and 
alternative feature sets (base and base+AllContext).  
In our experimentation we have evaluated larger 
history sizes as well, but the performance was con-
sistently worse as the history size grew larger than 
1. Thus, we only report results for history sizes of 
0 and 1. 
Our evaluation demonstrates that we achieve a 
much greater impact on performance with carefully 
designed, automatically extractable context ori-
ented features.  In all cases we are able to achieve a 
statistically significant improvement by adding 
context oriented features, and only achieve a statis-
tically significant improvement using sequential 
learning for one dimension, and only in the ab-
sence of context oriented features. 
4.1 Feature Based Approach 
0.61
0.71
0.52
0.62
0.73
0.67
0.61
0.70
0.66
0.61
0.73
0.69
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base Base+Thread Base+Seq Base+AllContext
 
Figure 1. Results with alternative features 
sets 
 
75
We first evaluated the feature based approach 
across all three dimensions and demonstrate that 
statistically significant improvements are achieved 
on all dimensions by adding context oriented fea-
tures.  The most dramatic results are achieved on 
the Social Modes of Co-Construction dimension 
(See Figure 1). All pairwise contrasts between al-
ternative feature sets within this dimension are sta-
tistically significant.  In the other dimensions, 
while Base+Thread is a significant improvement 
over Base, there is no significant difference be-
tween Base+Thread and Base+AllContext.   
4.2 Sequential Learning 
0.54
0.63
0.43
0.56
0.64
0.52
0.56
0.63
0.59
0.56
0.65
0.61
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Social Macro Micro
Dimension
Ka
pp
a 
fro
m
 
10
-
fo
ld
 
CV
Base / 0 Base /  1 Base+AllContext / 0 Base+AllContext / 1
 
Figure 2. Results with Sequential Learning 
 
The results for sequential learning are weaker than 
for the feature based (See Figure 2). While the 
Collins Perceptron learner possesses the capability 
of modeling sequential dependencies between 
codes, which SMO does not possess, it is not nec-
essarily a more powerful learner.  On this data set, 
the Collins Perceptron learner consistently per-
forms worse that SMO.  Even restricting our 
evaluation of sequential learning to a comparison 
between the Collins Perceptron learner with a his-
tory of 0 (i.e., no history) with the same learner 
using a history of 1, we only see a statistically sig-
nificant improvement on the Social Modes of Co-
Construction dimension.  This is when only using 
base features, although the trend was consistently 
in favor of a history of 1 over 0. Note that the stan-
dard deviation in the performance across folds was 
much higher with the Collins Perceptron learner, 
so that a much greater difference in average would 
be required in order to achieve statistical signifi-
cance.  Performance over a validation set was al-
ways worse with larger history sizes than 1.   
5 Conclusions  
We have described work towards an approach to 
conversation summarization where an assessment 
of conversational quality along multiple process 
dimensions is reported.  We make use of a well-
established annotation scheme developed in the 
CSCL community.  Our evaluation demonstrates 
that thread based features have a greater and more 
consistent impact on performance with this data. 
 
This work was supported by the National Sci-
ence Foundation grant number SBE0354420, and 
Office of Naval Research, Cognitive and Neural Sci-
ences Division Grant N00014-05-1-0043. 
References 
Bellotti, V., Ducheneaut, N., Howard, M. Smith, I., 
Grinter, R. (2005). Quality versus Quantity: Email-
centric task management and its relation with over-
load. Human-Computer Interaction, 2005, vol. 20 
Carvalho, V. & Cohen, W. (2005). On the Collective 
Classification of Email ?Speech Acts?, Proceedings 
of SIGIR ?2005. 
Collins, M (2002). Discriminative Training Methods for 
Hidden Markov Models: Theory and Experiments 
with Perceptron Algorithms. In Proceedings of 
EMNLP 2002.  
Lacson, R., Barzilay, R., & Long, W. (2006). Automatic 
analysis of medical dialogue in the homehemodialy-
sis domain: structure induction and summarization, 
Journal of Biomedical Informatics 39(5), pp541-555. 
Roman, N., Piwek, P., & Carvalho, A. (2006).  Polite-
ness and Bias in Dialogue Summarization : Two Ex-
ploratory Studies, in J. Shanahan, Y. Qu, & J. Wiebe 
(Eds.) Computing Attitude and Affect in Text: Theory 
and Applications, the Information Retrieval Series.   
Weinberger, A., & Fischer, F. (2006). A framework to 
analyze argumentative knowledge construction in 
computer-supported collaborative learning. Com-
puters & Education, 46, 71-95. 
Witten, I. H. & Frank, E. (2005).  Data Mining: Practi-
cal Machine Learning Tools and Techniques, sec-
ond edition, Elsevier: San Francisco. 
Zechner, K. (2001). Automatic Generation of Concise 
Summaries of Spoken Dialogues in Unrestricted 
Domains. Proceedings of ACM SIG-IR 2001. 
76
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 519 ? 529, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Web-Based Unsupervised Learning for Query 
Formulation in Question Answering 
Yi-Chia Wang1, Jian-Cheng Wu2, Tyne Liang1, and Jason S. Chang2 
1
 Dep. of Computer and Information Science, National Chiao Tung University,  
1001 Ta Hsueh Rd., Hsinchu, Taiwan 300, R.O.C. 
rhyme.cis92g@nctu.edu.tw, tliang@cis.nctu.edu.tw 
2
 Dep. of Computer Science, National Tsing Hua University,  
101, Section 2 Kuang Fu Road, Hsinchu, Taiwan 300, R.O.C. 
d928322@oz.nthu.edu.tw, jschang@cs.nthu.edu.tw 
Abstract. Converting questions to effective queries is crucial to open-domain 
question answering systems. In this paper, we present a web-based 
unsupervised learning approach for transforming a given natural-language 
question to an effective query. The method involves querying a search engine 
for Web passages that contain the answer to the question, extracting patterns 
that characterize fine-grained classification for answers, and linking these 
patterns with n-grams in answer passages. Independent evaluation on a set of 
questions shows that the proposed approach outperforms a naive keyword-
based approach in terms of mean reciprocal rank and human effort. 
1   Introduction 
An automated question answering (QA) system receives a user?s natural-language 
question and returns exact answers by analyzing the question and consulting a large 
text collection [1, 2]. As Moldovan et al [3] pointed out, over 60% of the QA errors 
can be attributed to ineffective question processing, including query formulation and 
query expansion.  
A naive solution to query formulation is using the keywords in an input question as 
the query to a search engine. However, it is possible that the keywords may not appear 
in those answer passages which contain answers to the given question. For example, 
submitting the keywords in ?Who invented washing machine?? to a search engine like 
Google may not lead to retrieval of answer passages like ?The inventor of the automatic 
washer was John Chamberlain.? In fact, by expanding the keyword set (?invented?, 
?washing?, ?machine?) with ?inventor of,? the query to a search engine is effective in 
retrieving such answer passages as the top-ranking pages. Hence, if we can learn how to 
associate a set of questions (e.g. (?who invented ???) with effective keywords or 
phrases (e.g. ?inventor of?) which are likely to appear in answer passages, the search 
engine will have a better chance of retrieving pages containing the answer. 
In this paper, we present a novel Web-based unsupervised learning approach to 
handling question analysis for QA systems. In our approach, training-data questions 
are first analyzed and classified into a set of fine-grained categories of question 
520 Y.-C. Wang et al 
patterns. Then, the relationships between the question patterns and n-grams in answer 
passages are discovered by employing a word alignment technique. Finally, the best 
query transforms are derived by ranking the n-grams which are associated with a 
specific question pattern. At runtime, the keywords in a given question are extracted 
and the question is categorized. Then the keywords are expanded according the 
category of the question. The expanded query is the submitted to a search engine in 
order to bias the search engine to return passages that are more likely to contain 
answers to the question. Experimental results indicate the expanded query indeed 
outperforms the approach of directly using the keywords in the question. 
2   Related Work 
Recent work in Question Answering has attempted to convert the original input 
question into a query that is more likely to retrieve the answers. Hovy et al [2] utilized 
WordNet hypernyms and synonyms to expand queries to increase recall. Hildebrandt et 
al. [4] looked up in a pre-compiled knowledge base and a dictionary to expand a 
definition question. However, blindly expanding a word using its synonyms or 
dictionary gloss may cause undesirable effects. Furthermore, it is difficult to determine 
which of many related word senses should be considered when expanding the query.  
Radev et al [5] proposed a probabilistic algorithm called QASM that learns the best 
query expansion from a natural language question. The query expansion takes the 
form of a series of operators, including INSERT, DELETE, REPLACE, etc., to 
paraphrase a factual question into the best search engine query by applying 
Expectation Maximization algorithm. On the other hand, Hermjakob et al [6] 
described an experiment to observe and learn from human subjects who were given a 
question and asked to write queries which are most effective in retrieving the answer 
to the question. First, several randomly selected questions are given to users to 
?manually? generate effective queries that can bias Web search engines to return 
answers. The questions, queries, and search results are then examined to derive seven 
query reformulation techniques that can be used to produce queries similar to the ones 
issued by human subjects. 
In a study closely related to our work, Agichtein et al [7] presented Tritus system 
that automatically learns transforms of wh-phrases (e.g. expanding ?what is? to 
?refers to?) by using FAQ data. The wh-phrases are restricted to sequences of 
function word beginning with an interrogative, (i.e. who, what, when, where, why, 
and how).  These wh-phrases tend to coarsely classify questions into a few types. 
Tritus uses heuristic rules and thresholds of term frequencies to learn transforms. 
In contrast to previous work, we rely on a mathematical model trained on a set of 
questions and answers to learn how to transform the question into an effective query. 
Transformations are learned based on a more fine-grained question classification 
involving the interrogative and one or more content words. 
3   Transforming Question to Query 
The method is aimed at automatically learning of the best transforms that turn a given 
natural language question into an effective query by using the Web as corpus. To that 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 521 
end, we first automatically obtain a collection of answer passages (APs) as the 
training corpus from the Web by using a set of (Q, A) pairs. Then we identify the 
question pattern for each Q by using statistical and linguistic information. Here, a 
question pattern Qp is defined as a question word plus one or two keywords that are 
related to the question word. Qp represents the question intention and it can be treated 
as a preference indicative for fine-grained type of named entities. Finally, we decide 
the transforms Ts for each Qp by choosing those phrases in the APs that are 
statistically associated with Qp and adjacent to the answer A. 
Table 1. An example of converting a question (Q) with its answer (A) to a SE query and 
retrieving answer passages (AP) 
(Q, A) AP 
Bungalow For Rent in Islamabad, Capital 
Pakistan. Beautiful Big House For ? What is the capital of Pakistan?  Answer:( Islamabad) 
(k1, k2,?, kn, A) 
Islamabad is the capital of Pakistan. Current 
time, ? 
capital, Pakistan, Islamabad ?the airport which serves Pakistan's capital Islamabad, ? 
3.1   Search the Web for Relevant Answer Passages 
For training purpose, a large amount of question/answer passage pairs are mined from 
the Web by using a set of question/answer pairs as seeds.  
More formally, we attempt to retrieve a set of (Q, AP) pairs on the Web for training 
purpose, where Q stands for a natural language question, and AP is a passage 
containing at least one keyword in Q and A (the answer to Q). The seed data (Q, A) 
pairs can be acquired from many sources, including trivia game Websites, TREC QA 
Track benchmarks, and files of Frequently Asked Questions (FAQ). The output of 
this training-data gathering process is a large collection of (Q, AP) pairs. We describe 
the procedure in details as follows: 
1. For each (Q, A) pair, the keywords k1, k2,?, kn are extracted from Q by removing 
stopwords. 
2. Submit (k1, k2,?, kn, A) as a query to a search engine SE. 
3. Download the top n summaries returned by SE. 
4. Separate sentences in the summaries, and remove HTML tags, URL, special 
character references (e.g., ?&lt;?). 
5. Retain only those sentences which contain A and some ki. 
Consider the example of gathering answer passages from the Web for the (Q, A) 
pair where Q = ?What is the capital of Pakistan?? and A = ?Islamabad.? See Table 1 
for the query submitted to a search engine and potential answer passages returned. 
3.2   Question Analysis 
This subsection describes the presented identification of the so-called ?question 
pattern? which is critical in categorizing a given question and transforming the 
question into a query. 
522 Y.-C. Wang et al 
Formally, a ?question pattern? for any question is defined as following form: 
question-word  head-word+ 
where ?question-word? is one of the interrogatives (Who/What/Where/When/How) 
and the ?head-word? represents the headwords in the subsequent chunks that tend to 
reflect the intended answer more precisely. If the first headword is a light verb, an 
additional headword is needed. For instance, ?who had hit? is a reasonable question 
pattern for ?Who had a number one hit in 1984 with ?Hello???, while ?who had? 
seems to be too coarse. 
In order to determine the appropriate question pattern for each question, we 
examined and analyzed a set of questions which are part-of-speech (POS) tagged and 
phrase-chunked. With the help of a set of simple heuristic rules based on POS and 
chunk information, fine-grained classification of questions can be carried out 
effectively. 
Question Pattern Extraction 
After analyzing recurring patterns and regularity in quizzes on the Web, we designed 
a simple procedure to recognize question patterns. The procedure is based on a small 
set of prioritized rules. 
The question word which is one of the wh-words (?who,? ?what,? ?when,? 
?where,? ?how,? or ?why?) tagged as determiner or adverbial question word. 
According to the result of POS tagging and phrase chunking, we further decide the 
main verb and the voice of the question. Then, we apply the following expanded rules 
to extract words to form question patterns: 
Rule 1: Question word in a chunk of length more than one (see Example (1) in Table 2). 
Qp = question word + headword in the same chunk 
Rule 2: Question word followed by a light verb and Noun Phrase(NP) or 
Prepositional Phrase(PP) chunk (Example (2)). 
Qp = question word + light verb +headword in the following NP or PP chunk 
Rule 3: Question word followed immediately by a verb (Example (3)).  
Qp = question word + headword in the following Verb Phrase(VP) or NP chunk 
Rule 4: Question word followed by a passive VP (Example (4)).  
Qp = Question word + ?to be? + headword in the passive VP chunk 
Rule 5: Question word followed by the copulate ?to be? and an NP (Example (5)).  
Qp = Question word + ?to be? + headword in the next NP chunk 
Rule 6: If none of the above rules are applicable, the question pattern is the question 
word. 
By exploiting linguistic information of POS and chunks, we can easily form the 
question pattern. These heuristic rules are intuitive and easy to understand. Moreover, 
the fact that these patterns which tend to recur imply that they are general and it is 
easy to gather training data accordingly. These question patterns also indicate a 
preference for the answer to be classified with a fine-grained type of proper nouns. In 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 523 
the next section, we describe how we exploit these patterns to learn the best question-
to-query transforms. 
Table 2. Example questions and question patterns (of words shown in bold) 
(1) Which female singer performed the first song on Top of the Pops? 
(2) Who in 1961 made the first space flight? 
(3) Who painted ?The Laughing Cavalier?? 
(4) What is a group of geese called? 
(5) What is the second longest river in the world? 
3.3   Learning Best Transforms 
This section describes the procedure for learning transforms Ts which convert the 
question pattern Qp into bigrams in relevant APs. 
Word Alignment Across Q and AP 
We use word alignment techniques developed for statistical machine translation to 
find out the association between question patterns in Q and bigrams in AP. The reason 
why we use bigrams in APs instead of unigrams is that bigrams tend to have more 
unique meaning than single words and are more effective in retrieving relevant 
passages. 
We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs. The 
method involves preprocessing steps for each (Q, AP) pair so as to filter useless 
information: 
1. Perform part-of-speech tagging on Q and AP. 
2. Replace all instances of A with the tag <ANS> in APs to indicate the location of 
the answers. 
3. Identify the question pattern, Qp and keywords which are not a named entity. We 
denote the question pattern and keywords as q1, q2, ..., qn. 
4. Convert AP into bigrams and eliminate bigrams with low term frequency (tf) or 
high document frequency (df). Bigrams composed of two function words are also 
removed, resulting in bigrams a1, a2, ..., am. 
We then align q?s and a?s via Competitive Linking Algorithm (CLA) procedure as 
follows: 
Input: A collection C of (Q; A) pairs, where (Q; A) = (q1 = Qp , q2, q3, ..., qn ; a1, a2, 
..., am) 
Output: Best alignment counterpart a?s for all q?s in C 
1. For each pair of (Q; A) in C and for all qi and aj in each pair of C, calculate LLR(qi, 
aj), logarithmic likelihood ratio (LLR) between qi and aj, which reflects their 
statistical association. 
2. Discard (q, a) pairs with a LLR value lower than a threshold. 
524 Y.-C. Wang et al 
3. For each pair of (Q; A) in C and for all qi and aj therein, carry out Steps 4-7: 
4. Sort list of (qi, aj) in each pair of (Q ; A) by decreasing LLR value. 
5. Go down the list and select a pair if it does not conflict with previous selection. 
6. Stop when running out of pairs in the list. 
7. Produce the list of aligned pairs for all Qs and APs. 
8. Tally the counts of aligning (q, a). 
9. Select top k bigrams, t1, t2, ..., tk, for every question pattern or keyword q. 
The LLR statistics is generally effective in distinguishing related terms from 
unrelated ones. However, if two terms occur frequently in questions, their alignment 
counterparts will also occur frequently, leading to erroneous alignment due to indirect 
association. CLA is designed to tackle the problem caused by indirect association. 
Therefore, if we only make use of the alignment counterpart of the question pattern, 
we can keep the question keywords in Q so as to reduce the errors caused by indirect 
association. For instance, the question ?How old was Bruce Lee when he died?? Our 
goal is to learn the best transforms for the question pattern ?how old.? In other words, 
we want to find out what terms are associated with ?how old? in the answer passages. 
However, if we consider the alignment counterparts of ?how old? without considering 
those keyword like ?died,? we run the risk of getting ?died in? or ?is dead? rather than 
?years old? and ?age of.? If we have sufficient data for a specific question pattern like 
?how long,? we will have more chances to obtain alignment counterparts that are 
effective terms for query expansion. 
Distance Constraint and Proximity Ranks 
In addition to the association strength implied with alignment counts and co-
occurrence, the distance of the bigrams to the answer should also be considered. We 
observe that terms in the answer passages close to the answers intuitively tend to be 
useful in retrieving answers. Thus, we calculate the bigrams appearing in a window of 
three words appearing on both sides of the answers to provide additional constraints 
for query expansion. 
Combing Alignment and Proximity Ranks 
The selection of the best bigrams as the transforms for a specific question pattern is 
based on a combined rank of alignment count and proximity count. It takes the 
average of these two counts to re-rank bigrams. The average rank of a bigram b is  
Rankavg (b) = (Rankalign (b)+ Rankprox (b))/2, 
where Rankalign (b) is the rank of b?s alignment count and Rankprox (b) is the rank of 
b?s proximity count. The n top-ranking bigrams  for a specific type of question will be 
chosen to transform the question pattern into query terms. For the question pattern 
?how old,? the candidate bigrams with alignment ranks, co-occurring ranks, and 
average ranks are shown in Table 3. 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 525 
Table 3. Average rank calculated from for the bigram counterparts of ?how old? 
Bigrams Alignment Rank Proximity Rank Avg. Rank Final Rank 
age of 1 1 1 1 
years old 2 2 2 2 
ascend the 3 - - - 
throne in 4 3 3.5 3 
the youngest 3 - - - 
? ? ? ? ? 
3.4   Runtime Transformation of Questions 
At runtime, a given question Q submitted by a user is converted into one or more 
keywords and a question pattern, which is subsequently expanded in to a sequence of 
query terms based on the transforms obtained at training. 
We follow the common practice of keyword selection in formulating Q into a 
query: 
? Function words are identified and discarded. 
? Proper nouns that are capitalized or quoted are treated as a single search term with 
quotes. 
Additionally, we expand the question patterns based on alignment and proximity 
considerations: 
? The question pattern Qp is identified according to the rules (in Section 3.2) and is 
expanded to be a disjunction (sequence of ORs) of Qp?s headword and n top-
ranking bigrams (in section 3.3) 
? The query will be a conjunction (sequence of ANDs) of expanded Qp, proper 
names, and remaining keywords. Except for the expanded Qp, all other proper 
names and keywords will be in the original order in the given question for the best 
results. 
Table 4. An example of transformation from question into query 
Question 
How old was Bruce Lee when he died? 
Question pattern Proper noun Keyword 
how old 
Transformation 
age of, years old 
?Bruce Lee? died 
Expanded query 
Boolean query: ( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died? 
Equivalent Google query: (old || ?age of? || ?years old?) ?Bruce Lee? died 
526 Y.-C. Wang et al 
For example, formulating a query for the question ?How old was Bruce Lee when 
he died?? will result in a question pattern ?how old.? Because there is a proper noun 
?Bruce Lee? in the question and a remaining keyword ?died,? the query becomes  
?( ?old? OR ?age of? OR ?years old? ) AND ?Bruce Lee? AND ?died.?? Table 4 lists the 
query formulating for the example question.  
4   Experiments and Evaluation 
The proposed method is implemented by using the Web search engine, Google, as the 
underlying information retrieval system. The experimental results are also justified 
with assessing the effectiveness of question classification and query expansion. 
We used a POS tagger and chunker to perform shallow parsing of the questions 
and answer passages. The tagger was developed using the Brown corpus and 
WordNet. The chunker is built from the shared CoNLL-2000 data provided by 
CoNLL-2000. The shared task CoNLL-2000 provides a set of training and test data 
for chunks. The chunker we used produces chunks with an average precision rate of 
about 94%. 
4.1   Evaluation of Question Patterns 
The 200 questions from TREC-8 QA Track provide an independent evaluation of how 
well the proposed method works for question pattern extraction works. We will also 
give an error analysis. 
Table 5. Evaluation results of question pattern extraction 
 Two ?good? labels At least one ?good? label 
Precision (%) 86 96 
Table 6. The first five questions with question patterns and judgment 
Question Question pattern Judgment 
Who is the author of the book, "The Iron 
Lady: A Biography of Margaret Thatcher"? Who-author good 
What was the monetary value of the Nobel 
Peace Prize in 1989? What value good  
What does the Peugeot company manufacture? What do 
manufacture good 
How much did Mercury spend on advertising 
in 1993?      How much good 
What is the name of the managing director of 
Apricot Computer? What name bad 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 527 
Two human judges both majoring in Foreign Languages were asked to assess the 
results of question pattern extraction and give a label to each extracted question 
pattern. A pattern will be judged as ?good? if it clearly expresses the answer 
preference of the question; otherwise, it is tagged as ?bad.? The precision rate of 
extraction for these 200 questions is shown in Table 5. The second column indicates 
the precision rate when both of two judges agree that an extracted question pattern is 
?good.? In addition, the third column indicates the rate of those question patterns that 
are found to be ?good? by either judge. The results imply that the proposed pattern 
extraction rules are general, since they are effective even for questions independent of 
the training and development data. Table 6 shows evaluation results for ?two ?good? 
labels? of the first five questions. 
We summarize the reasons behind these bad patterns: 
? Incorrect part-of-speech tagging and chunking 
? Imperative questions such as ?Name the first private citizen to fly in space.? 
? Question patterns that are not specific enough 
For instance, the system produces ?what name? for ?What is the name of the 
chronic neurological autoimmune disease which ? ??, while the judges suggested 
that ?what disease.?. Indeed, some of the patterns extracted can be modified to meet 
the goal of being more fine-grained and indicative of a preference to a specific type of 
proper nouns or terminology. 
4.2   Evaluation of Query Expansion 
We implemented a prototype of the proposed method called Atlas (Automatic 
Transform Learning by Aligning Sentences of question and answer). To develop the 
system of Atlas, we gathered seed training data of questions and answers from a trivia 
game website, called QuizZone1. We collected the questions posted in June, 2004 on 
QuizZone and obtained 3,851 distinct question-answer pairs. We set aside the first 45 
questions for testing and used the rest for training. For each question, we form a query 
with question keywords and the answer and submitted the query to Google to retrieve 
top 100 summaries as the answer passages. In all, we collected 95,926 answer passages.  
At training time, we extracted a total of 338 distinct question patterns from 3,806 
questions. We aligned these patterns and keywords with bigrams in the 95,926 answer 
passages, identified the locations of the answers, and obtained the bigrams appearing 
within a distance of 3 of the answers. At runtime, we use the top-ranking bigram to 
expand each question pattern. If no such bigrams are found, we use only the keyword 
in the question patterns. The expanded terms for question pattern are placed at the 
beginning of the query.  
We submitted forty-five keyword queries and the same number of expanded 
queries generated by Atlas for the test questions to Google and obtained ten returned 
summaries for evaluation. For the evaluation, we use three indicators to measure the 
performance. The first indicator is the mean reciprocal rank (MRR) of the first 
relevant document (or summary) returned. If the r-th document (summary) returned is 
the one with the answer, then the reciprocal rank of the document (summary) is 1/r. 
                                                          
1
 QuizZone (http://www.quiz-zone.co.uk) 
528 Y.-C. Wang et al 
The mean reciprocal rank is the average reciprocal rank of all test questions. The 
second indicator of effective query is the recall at R document retrieved (Recall at R). 
The last indicator measures the human effort (HE) in finding the answer. HE is 
defined as the least number of passages needed to be viewed for covering all the 
answers to be returned from the system. 
The average length of these test questions is short. We believe the proposed 
question expansion scheme helps those short sentences, which tend to be less 
effective in retrieving answers. We evaluated the expanded queries against the same 
measures for summaries returned by simple keyword queries. Both batches of 
returned summaries for the forty-five questions were verified by two human judges. 
As shown in Table 7, the MRR produced by keyword-based scheme is slightly lower 
than the one yielded by the presented query expansion scheme. Nevertheless, such 
improvement is encouraging by indicating the effectiveness of the proposed method. 
Table 8 lists the comparisons in more details. It is found that our method is 
effective in bringing the answers to the top 1 and top 2 summaries as indicated by the 
high Recall of 0.8 at R = 2. In addition, Table 8 also shows that less user?s efforts are 
needed by using our approach. That is, for each question, the average of summaries 
required to be viewed by human beings goes down from 2.7 to 2.3. 
In the end, we found that those bigrams containing a content word and a function 
word  turn out to be very effective. For instance, our method tends to favor transforms 
Table 7. Evaluation results of MRR 
Performances MRR 
GO (Direct keyword query for Google) 0.64 
AT+GO (Atlas expanded query for Google) 0.69 
Table 8. Evaluation Result of Recall at R and Human Effort 
Rank count Recall at R Rank GO AT+GO GO AT+GO 
1 25 26 0.56 0.58 
2 6 10 0.69 0.80 
3 5 3 0.80 0.87 
4 0 1 0.80 0.89 
5 1 1 0.82 0.91 
6 2 0 0.87 0.91 
7 1 0 0.89 0.91 
8 2 0 0.93 0.91 
9 0 1 0.93 0.93 
10 0 0 0.93 0.93 
No answers 3 3 
Human Effort 122 105 
 
# of questions 45 45 
HE per question 2.7 2.3 
 
 Web-Based Unsupervised Learning for Query Formulation in Question Answering 529 
such as ?who invented? to bigrams such as ?invented by,? ?invent the,? and ?inventor 
of.? This contrasts to conventional wisdom of using a stoplist of mostly function 
words and excluding them from consideration in a query. Our experiment also shows 
a function word as part of a phrasal term seems to be very effective, for it indicate an 
implied relation with the answer.  
5   Conclusion and Future Work 
In this paper, we introduce a method for learning query transformations that improves 
the ability to retrieve passages with answers using the Web as corpus. The method 
involves question classification and query transformations using a learning-based 
approach. We also describe the experiment with over 3,000 questions indicates that 
satisfactory results were achieved. The experimental results show that the proposed 
method provides effective query expansion that potentially can lead to performance 
improvement for a question answering system. 
A number of future directions present themselves. First, the patterns learned from 
answer passages acquired on the Web can be refined and clustered to derive a 
hierarchical classification of questions for more effective question classification. Second, 
different question patterns, like ?who wrote? and ?which author?, should be treated as the 
same in order to cope with data sparseness and improve system performance. On the 
other hand, an interesting direction is the generating pattern transformations that contain 
the answer extraction patterns for different types of questions.  
References 
1. Ittycheriah, A., Franz, M., Zhu, W.-J., and Rathaparkhi, A. 2000. IBM?s statistical 
question answering system. In Proceedings of the TREC-9 Question Answering Track, 
Gaithersburg, Maryland. 
2. Hovy, E., Gerber, L., Hermjakob, U., Junk, M., and Lin, C.-Y. 2000. Question answering 
in Webclopedia. In Proceedings of the TREC-9 Question Answering Track, Gaithersburg, 
Maryland. 
3. Moldovan D., Pasca M., Harabagiu S., & Surdeanu M. 2002. Performance Issues and error 
Analysis in an Open-Domain Question Answering System. In Proceedings of the 40th 
Annual Meeting of ACL, Philadelphia, Pennsylvania. 
4. Hildebrandt, W., Katz, B., & Lin, J. 2004. Answering definition questions with multiple 
knowledge sources. In Proceedings of the 2004 Human Language Technology Conference 
and the North American Chapter of the Association for Computational. 
5. Radev, D. R., Qi, H., Zheng, Z., Blair-Goldensohn, S., Fan, Z. Z. W., and Prager, J. M. 
2001. Mining the web for answers to natural language questions. In Proceedings of the 
International Conference on Knowledge Management (CIKM-2001), Atlanta, Georgia. 
6. Hermjakob, U., Echihabi, A., and Marcu, D. 2002. Natural Language Based 
Reformulation Resource and Web Exploitation for Question Answering. In Proceeding of 
TREC-2002, Gaithersburg, Maryland. 
7. Agichtein, E., Lawrence, S., and Gravano, L. Learning to find answers to questions on the 
Web. 2003. In ACM Transactions on Internet Technology (TOIT), 4(2):129-162. 
8. Melamed, I. D. 1997. A Word-to-Word Model of Translational Equivalence. In 
Proceedings of the 35st Annual Meeting of ACL, Madrid, Spain. 
9. Yi-Chia Wang, Jian-Cheng Wu, Tyne Liang, and Jason S. Chang. 2004. Using the Web as 
Corpus for Un-supervised Learning in Question Answering, Proceedings of Rocling 2004, 
Taiwan. 
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 673?676,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Making Conversational Structure Explicit:  
Identification of Initiation-response Pairs within Online Discussions 
 
Yi-Chia Wang Carolyn P. Ros? 
Language Technologies Institute  Language Technologies Institute  
Carnegie Mellon University Carnegie Mellon University 
Pittsburgh, PA 15213, USA Pittsburgh, PA 15213, USA 
yichiaw@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
In this paper we investigate how to identify 
initiation-response pairs in asynchronous, 
multi-threaded, multi-party conversations.  
We formulate the task of identifying initia-
tion-response pairs as a pairwise ranking 
problem. A novel variant of Latent Semantic 
Analysis (LSA) is proposed to overcome a li-
mitation of standard LSA models, namely that 
uncommon words, which are critical for sig-
naling initiation-response links, tend to be 
deemphasized as it is the more frequent terms 
that end up closer to the latent factors selected 
through singular value decomposition. We 
present experimental results demonstrating 
significantly better performance of the novel 
variant of LSA over standard LSA.  
1 Introduction 
In recent years, research in the analysis of social 
media (e.g., weblogs, discussion boards, and mes-
sengers) has grown in popularity. Unlike exposito-
ry text, the data produced through use of social 
media is often conversational, multi-threaded, and 
more complex because of the involvement of nu-
merous participants who are distributed both across 
time and across space. Recovering the multi-
threaded structure is an active area of research. 
In this paper, we form the foundation for a 
broader study of this type of data by investigating 
the basic unit of interaction, referred to as an initi-
ation-response pair (Schegloff, 2007). Initiation-
response pairs are pairs of utterances that are typi-
cally contributed by different participants, and 
where the first pair part sets up an expectation for 
the second pair part. Types of common initiation-
response pairs include question-answer, assess-
ment-agreement, blame-denial, etc. Note that al-
though sometimes discussion forum interfaces 
make the thread structure of the interaction expli-
cit, these affordances are not always present. And 
even in forums that have these affordances, the 
apparent structure of the discourse as represented 
through the interface may not capture all of the 
contingencies between contributions in the unfold-
ing conversation. Thus, the goal of this investiga-
tion is to investigate approaches for automatically 
identifying initiation-response pairs in conversa-
tions.   
One of the challenges in identifying initiation-
response pairs is that the related messages are not 
necessarily adjacent to each other in the stream of 
contributed messages, especially within the asyn-
chronous environment of social media. Further-
more, individual differences related to writing style 
or creative expression of self may also complicate 
the identification of the intended connections be-
tween contributions. Identification of initiation-
response pairs is an important step towards auto-
matic processing of conversational data. One po-
tential application of this work is conversation 
summarization. A summary should include both 
the initiation and response as a coherent unit or it 
may fail to capture the intended meaning. 
We formulate the task of identifying initiation-
response pairs as a pairwise ranking problem. The 
goal is to distinguish message pairs that constitute 
an initiation-response pair from those that do not. 
We believe a ranking approach, where the degree 
of relatedness between a message pair can be con-
sidered in light of the relatedness between each of 
them and the surrounding messages within the 
same thread, is a more suitable paradigm for this 
task than a discrete classification-based paradigm.    
 Previous work on recovering conversational 
structure has relied on simple lexical cohesion 
673
measures (i.e., cosine similarity), temporal infor-
mation (Lewis and Knowles, 1997; Wang et al, 
2008), and meta-data (Minkov et al, 2006). How-
ever, relatively little work has investigated the im-
portance of specifically in-focus connections 
between initiation-response pairs and utilized them 
as clues for the task. Consider, for example, the 
following excerpt discussing whether congress 
should pass a bill requiring the use of smaller cars 
to save the environment:   
a) Regressing to smaller vehicles would discourage 
business from producing more pollution. 
b) If CO2 emissions are lowered, wouldn't tax revenues 
be lowered as well? Are the democrats going to wil-
lingly give up Medicaid and social security? 
Although segment (b) is a reply to segment (a), the 
amount of word overlap is minimal. Nonetheless, 
we can determine that (b) is a response to (a) by 
recognizing the in-focus connections, such as "ve-
hicles-CO2" and "pollution-CO2." To properly 
account for connections between initiations and 
responses, we introduce a novel variant of Latent 
Semantic Analysis (LSA) into our ranking model.  
In section 2, we describe the Usenet data and 
how we extract a large corpus of initiation-
response pairs from it. Section 3 explains our rank-
ing model as well as the proposed novel LSA vari-
ation. The experimental results and discussion are 
detailed in Section 4 and Section 5, respectively. 
2 Usenet and Generation of Data 
The experiment for this paper was conducted using 
data crawled from the alt.politics.usa Usenet (User 
Network) discussion forum, including all posts 
from the period between June 2003 and June 2008.  
The resulting set contains 784,708 posts. The posts 
in this dataset alo contain meta-data that makes 
parent-child relationships explicit (i.e., through the 
References field). Thus, we know 625,116 of the 
posts are explicit responses to others posts. The 
messages are organized into a total of 77,985 dis-
cussion threads, each of which has 2 or more posts.   
In order to evaluate the quality of using the ex-
plicit reply structure as our gold standard for initia-
tion-response links, we asked human judges to 
annotate the response structure of a random-
selected medium-length discussion (19 posts) 
where we had removed the meta-data that indi-
cated the initiation-reply structure. The result 
shows the accuracy of our gold standard is 0.89. 
To set up the data as a pairwise ranking prob-
lem, we arranged the posts in the corpus into in-
stances containing three messages each, one of 
which is a response message, one of which is the 
actual initiating message, and the other of which is 
a foil selected from the same thread. The idea is 
that the ranking model will be trained to prefer the 
actual initiating message in contrast to the foil.   
The grain size of our examples is finer than 
whole messages. More specifically, positive exam-
ples are pairs of spans of text that have an initia-
tion-reply relationship. We began the process with 
pairs of messages where the meta-data indicates 
that an initiation-reply relationship exits, but we 
didn?t stop there. For our task it is important to 
narrow down to the specific spans of text that have 
the initiation-response relation. For this, we used 
the indication of quoted material within a message. 
We observed that when users explicitly quote a 
portion of a previously posted message, the portion 
of text immediately following the quoted material 
tends to have an explicit discourse connection with 
it. Consider the following example:  
>> Why is the quality of life of the child, mother,  
>> and society at large, more important than the 
>> sanctity of life? 
> Because in the case of anencephaly at least, 
> the life is ended before it begins. 
We disagree on this point. Why do you refuse to 
provide your very own positive definition of life?  
Do you believe life begins before birth?  At birth?  
After birth?  Never? 
In this thread, the reply expresses an opinion 
against the first level quote, but not the second lev-
el quote. Thus, we used segments of text with sin-
gle quotes as an initiation and the immediately 
following non-quoted text as the response. We ex-
tracted positive examples by scanning each post to 
locate the first level quote that is immediately fol-
lowed by unquoted content. If such quoted material 
was found, the quoted material and the unquoted 
response were both extracted to form a positive 
example. Otherwise, the message was discarded. 
For each post P where we extracted a positive 
example, we also extracted a negative example by 
picking a random post R from the same thread as 
P. We selected the negative example in such a way 
to make the task difficult in a realistic way. Choos-
ing R from other threads would make the task too 
easy because the topics of P and R would most 
likely be different. We also stipulated that R cannot 
be the parent, grandparent, sibling, or child of P. 
674
Together the non-quoted text of P and R forms a 
negative instance. Thus, the final dataset consists 
of pairs of message pairs ((pi, pj), (pi, pk)), where 
they have the same reply message pi, and pj is the 
correct quote message of pi, but pk is not. In other 
words, (pi, pj) is considered as a positive example; 
(pi, pk) is a negative example. We constructed a 
total of 100,028 instances for our dataset, 10,000 
(~10%) of which were used for testing, and 90,028 
(~90%) of which were the learning set used to con-
struct the LSA space described in the next section. 
3 Ranking Models for Identification of  
Initiation-Response Pairs 
Our pairwise ranking model1 takes as input an or-
dered pair of message pairs ((pi, pj), (pi, pk)) and 
computes their relatedness using a similarity func-
tion sim. Specifically, 
( xij, xik ) = ( sim (pi, pj), sim (pi, pk) ) 
where xij is the similarity value between post pi and 
pj; xik is the similarity value between post pi and pk.   
To determine which of the two message pairs ranks 
higher regarding initiation-response relatedness, 
we use the following scoring function to compare 
their corresponding similarity values: 
score (xij, xik) = xij ? xik  
If the score is positive, the model ranks (pi, pj) 
higher than (pi, pk) and vice versa. A message pair 
ranked higher means it has more evidence of being 
an initiation-reply link, compared to the other pair. 
3.1 Alternative Similarity Functions 
We introduce and motivate 3 alternative similarity 
functions, where the first two are considered as 
baseline approaches and the third one is a novel 
variation of LSA. We argue that the proposed LSA 
variation is an appropriate semantic similarity 
measurement for identifying topic continuation and 
initiation-reply pairs in online discussions.  
Cosine Similarity (cossim).  We choose an ap-
proach that uses only lexical cohesion as our base-
line. Previous work (Lewis and Knowles, 1997; 
Wang et al, 2008) has verified its usefulness for 
the thread identification task. In this case, 
                                                          
1 We cast the problem as a pairwise ranking problem in order 
to focus specifically on the issue of characterizing how initia-
tion-response links are encoded in language through lexical 
choice.  Note that once trained, pairwise ranking models can 
be used to rank multiple instances. 
sim(pi,pj) = cossim(pi,pj)  
where cossim(pi,pj) computes the cosine of the an-
gle between two posts pi and pj while they are 
represented as term vectors. 
LSA Average Similarity (lsaavg).  LSA is a well-
known method for grouping semantically related 
words (Landauer et al, 1998). It represents word 
meanings in a concept space with dimensionality k. 
Before we describe how to compute average simi-
larity given an LSA space, we explain how the 
LSA space was constructed in our work. First, we 
construct a term-by-document matrix, where we 
use the 90,028 message learning set mentioned at 
the end of Section 2. Next, LSA applies singular 
value decomposition to the matrix, and reduces the 
dimensionality of the feature space to a k dimen-
sional concept space. This generated LSA space is 
used by both lsaavg and lsacart later. 
For lsaavg, we follow Foltz et al (1998):  
 
 
 
 
The meaning of each post is represented as a vec-
tor in the LSA space by averaging across the LSA 
representations for each of its words. The similari-
ty between the two posts is then determined by 
computing the cosine value of their LSA vectors.  
This is the typical method for using LSA in text 
similarity comparisons. However, note that not all 
words carry equal weight within the vector that 
results from this averaging process. Words that are 
closer to the "semantic prototypes" represented by 
each of the k dimensions of the reduced vector 
space will have vectors with longer lengths than 
words that are less prototypical. Thus, those words 
that are closer to those prototypes will have a larg-
er effect on the direction of the resulting vector and 
therefore on the comparison with other texts. An 
important consideration is whether this is a desira-
ble effect. It would lead to deemphasizing those 
unusual types of information that might be being 
discussed as part of a post. However, one might 
expect that those things that are unusual types of 
information might actually be more likely to be the 
in-focus information within an initiation that res-
ponses may be likely to refer to. In that case, for 
our purposes, we would not expect this typical me-
thod for applying LSA to work well. 
LSA Cartesian Similarity (lsacart).  To properly 
account for connections between initiations and 
( ) ( )
??
?
?
?
?
??
?
?
?
?
==
??
??
j
pt
b
i
pt
a
jiji p
t
p
t
pplsaavgppsim jbia ,cos,,
675
responses that include unusual words, we introduce 
the following similarity function:  
 
 
 
 
where we take the mean of the cosine values for all 
the word pairs in the Cartesian product of posts pi 
and pj. Note that in this formulation, all words have 
an equal chance to affect the overall similarity be-
tween vectors since it is the angle represented by 
each word in a pair that comes to play when cosine 
distance is applied to a word pair. Length is no 
longer a factor. Moreover, the averaging is across 
cosine similarity scores rather than LSA vectors. 
4 Experimental Results 
The results are found in Table 1. For comparison, 
we also report the random baseline (0.50).  
 
  Random 
Baseline 
Cos- 
Similarity 
LSA- 
Average 
LSA- 
Cart 
Accuracy 0.50 0.66 0.60 0.71 
Table 1. Overview of results 
Besides the random baseline, LSA-Average per-
forms the worst (0.60), with simple Cosine similar-
ity (0.66) in the middle, and LSA-Cart (0.71) the 
best, with each of the pairwise contrasts being sta-
tistically significant. We believe the reason why 
LSA-Average performs so poorly on this task is 
precisely because, as discussed in last section, it 
deemphasizes those words that contribute the most 
unusual content. LSA-Cart addresses this issue. 
To further understand this effect, we conducted 
an error analysis. We divided the instances into 4 
sets based on the lexical cohesion between the re-
sponse and the true initiation and between the re-
sponse and the foil, by taking the median split on 
the distributions of these two cohesion scores.  Our 
finding is that model performances vary by subset. 
In particular, we find that it is only in cases where 
the positive example has low lexical cohesion (e.g. 
our "vehicles-CO2" and "pollution-CO2" example 
from the earlier section), that we see the benefit of 
the LSA-Cart approach.  In other cases, where the 
cohesion between the reply and the true initiation 
is high, Cos-Similarity performs best. 
5 Discussion and Conclusion 
We have argued why the task of detecting initia-
tion-response pairs in multi-party discussions is 
important and challenging. We proposed a method 
for acquiring a large corpus for use to identify init-
iation-response pairs. In our experiments, we have 
shown that the ranking model using a variant of 
LSA performs best, which affirms our hypothesis 
that unusual information and uncommon words 
tends to be the focus of ongoing discussions and 
therefore to be the key in identifying initiation-
response links. 
In future work, we plan to further investigate the 
connection between an initiation-response pairs 
from multiple dimensions, such as topical cohe-
rence, semantic relatedness, conversation acts, etc. 
One important current direction is to develop a 
richer operationalization of the interaction that ac-
counts for the way posts sometimes respond to a 
user, a collection of users, or a user?s posting histo-
ry, rather than specific posts per se. 
Acknowledgments 
We thank Mary McGlohon for sharing her data 
with us.  This research was funded through NSF 
grant DRL-0835426. 
References  
David D. Lewis and Kimberly A. Knowles. 1997. 
Threading electronic mail: A preliminary study. In-
formation Processing and Management, 33(2), 209?
217. 
Einat Minkov, William W. Cohen, Andrew Y. Ng. 
2006. Contextual Search and Name Disambiguation 
in Email using Graphs. In Proceedings of the Inter-
national ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR), pages 35?
42. ACM Press, 2006. 
Peter W. Foltz, Walter Kintsch, Thomas K. Landauer. 
1998. Textual coherence using latent semantic analy-
sis. Discourse Processes, 25, 285?307. 
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis. 
Discourse Processes, 25, 259-284.  
Schegloff, E. 2007.  Sequence Organization in Interac-
tion: A Primer in Conversation Analysis, Cambridge 
University Press. 
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, Ca-
rolyn P. Ros?. 2008. Recovering Implicit Thread 
Structure in Newsgroup Style Conversations. In Pro-
ceedings of the 2nd International Conference on 
Weblogs and Social Media (ICWSM II), Seattle, 
USA. 
( ) ( )
( )
ji
pptt
ba
jiji pp
tt
pplsacartppsim jiba
 
,cos
,,
),(
?
??
==
676
