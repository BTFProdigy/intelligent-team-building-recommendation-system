Knowledge-Free Induction of Inflectional Morphologies
Patrick SCHONE                                 Daniel JURAFSKY
University of Colorado at Boulder           University of Colorado at Boulder
Boulder, Colorado 80309                        Boulder, Colorado 80309
schone@cs.colorado.edu                        jurafsky@cs.colorado.edu
Abstract
We propose an algorithm  to automatically induce
the morphology of inflectional languages using only
text corpora and no human input.  Our algorithm
combines cues from orthography, semantics, and
syntactic distributions to induce morphological
relationships in German, Dutch, and English. Using
CELEX as a gold standard for evaluation, we show
our algorithm to be an improvement over any
knowledge-free algorithm yet proposed.
1       Introduction
Many NLP tasks, such as building machine-readable
dictionaries, are dependent on the results of
morphological analysis.  While morphological
analyzers have existed since the early 1960s, current
algorithms require human labor to build rules for
morphological structure.  In an attempt to avoid this
labor-intensive process, recent work has focused on
machine-learning approaches to induce
morphological structure using large corpora. 
In this paper, we propose a knowledge-free
algorithm to automatically induce the morphology
structures of a language.  Our algorithm takes as
input a large corpus and  produces as output a set of
conflation sets indicating the various inflected and
derived forms for each word in the language.  As an
example, the conflation set of the word ?abuse?
would contain ?abuse?,  ?abused?, ?abuses?,
?abusive?, ?abusively?, and so forth. Our algorithm
extends earlier approaches to morphology induction
by combining various induced information sources:
the semantic relatedness of the affixed forms using
a Latent Semantic Analysis approach to corpus-
based semantics (Schone and Jurafsky, 2000), affix
frequency, syntactic context, and transitive closure.
Using the hand-labeled CELEX lexicon  (Baayen, et
al., 1993) as our gold standard, the current version
of our algorithm achieves an F-score of 88.1% on
the task of identifying conflation sets in English,
outperforming earlier algorithms.  Our algorithm is
also applied to German and Dutch and evaluated on
its ability to find  prefixes, suffixes, and circumfixes
in these languages.  To our knowledge, this serves
as the first evaluation of complete regular
morphological induction of German or Dutch
(although researchers such as Nakisa and Hahn
(1996) have evaluated induction algorithms on
morphological sub-problems in German).
2 Previous Approaches
Previous morphology induction approaches have
fallen into three categories.  These categories differ
depending on whether human input is provided and
on whether the goal is to obtain affixes or complete
morphological analysis.  We here briefly describe
work in each category.
2.1 Using a Knowledge Source to Bootstrap
Some researchers begin with some initial human-
labeled source from which they induce other
morphological components. In particular, Xu and
Croft (1998) use word context derived from  a
corpus to refine Porter stemmer output. Gaussier
(1999) induces derivational morphology using an
inflectional lexicon which includes part of speech
information.  Grabar and Zweigenbaum (1999) use
the SNOMED corpus of semantically-arranged
medical terms to find semantically-motivated
morphological relationships. Also, Yarowsky and
Wicentowski (2000) obtained outstanding results at
inducing English past tense after beginning with a
list of the open class roots in the language, a table of
a language?s inflectional parts of speech, and the
canonical suffixes for each part of speech.
2.2 Affix Inventories
A second, knowledge-free category of research has
focused on obtaining affix inventories.  Brent, et al
(1995) used minimum description length (MDL) to
find the most data-compressing suffixes. Kazakov
(1997) does something akin to this using MDL as a
fitness metric for evolutionary computing. D?Jean
(1998) uses a strategy similar to that of Harris
(1951). He declares that a stem has ended when the
number of characters following it exceed some
given threshold and identifies any residual following semantic relations, we identified those word pairs
the stems as suffixes.  that have strong semantic correlations as being
2.3 Complete morphological analysis
Due to the existence of morphological ambiguity
(such as with the word ?caring? whose stem is
?care? rather than ?car?), finding affixes alone does
not constitute a complete morphological analysis.
Hence, the last category of research is also
knowledge-free but attempts to induce, for each
word of a corpus, a complete analysis.  Since our Most of the existing algorithms described focus on
approach falls into this category (expanding upon suffixing in inflectional languages (though
our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and D?Jean describe work on prefixes).
we describe work in this area in more detail. None of these algorithms consider the general
2.3.1 Jacquemin?s multiword approach
Jacquemin (1997) deems pairs of word n-grams as
morphologically related if two words in the first n-
gram have the same first few letters (or stem) as two
words in the second n-gram and if there is a suffix
for each stem whose length is less than k. He also
clusters groups of words having the same kinds of
word endings, which gives an added performance
boost.  He applies his algorithm to a French term list
and scores based on sampled, by-hand evaluation. 
2.3.2. Goldsmith: EM and MDLs
Goldsmith (1997/2000) tries to automatically sever
each word in exactly one place in order to establish
a potential set of stems and suffixes.  He uses the
expectation-maximization algorithm (EM) and MDL
as well as some triage procedures to help eliminate
inappropriate parses for every word in a corpus.  He
collects the possible suffixes for each stem and calls
these signatures which give clues about word
classes. With the exceptions of capitalization
removal and some word segmentation, Goldsmith's
algorithm is otherwise knowledge-free. His
algorithm, Linguistica, is freely available on the
Internet.  Goldsmith applies his algorithm to various
languages but evaluates in English and French.
2.3.3  Schone and Jurafsky: induced semantics
In our earlier work, we (Schone and Jurafsky
(2000)) generated a list of N candidate suffixes and
used this list to identify word pairs which share the
same stem but conclude with distinct candidate
suffixes.  We then applied  Latent Semantic
Analysis (Deerwester, et al, 1990) as a method of
automatically determining semantic relatedness
between word pairs.  Using statistics from the
morphological variants of each other.  With the
exception of word segmentation, we provided  no
human information to our system.  We applied our
system to an English corpus and evaluated by
comparing each word?s conflation set as produced
by our algorithm to those derivable from CELEX.
2.4 Problems with earlier approaches 
conditions of circumfixing or infixing, nor are they
applicable to other language types such as
agglutinative languages (Sproat, 1992).
Additionally, most approaches have centered
around statistics of orthographic properties.  We had
noted previously (Schone and Jurafsky, 2000),
however, that errors can arise from strictly
orthographic systems.  We had observed in other
systems such errors as inappropriate removal of
valid affixes (?ally?<?all?), failure to resolve
morphological ambiguities (?hated?<?hat?), and
pruning of semi-productive affixes (?dirty?h?dirt?).
Yet we illustrated that induced semantics can help
overcome some of these errors.
However, we have since observed that induced
semantics can give rise to different kinds of
problems.  For instance, morphological variants may
be semantically opaque such that the meaning of
one variant cannot be readily determined by the
other (?reusability?h?use?).  Additionally,  high-
frequency function words may be conflated due to
having weak semantic information (?as?<?a?).
Coupling  semantic and orthographic statistics, as
well as introducing induced syntactic information
and relational transitivity can help in overcoming
these problems.  Therefore, we begin with an
approach similar to our previous algorithm.  Yet we
build upon this algorithm in several ways in that we:
[1] consider circumfixes, [2] automatically identify
capitalizations by treating them similar to prefixes
[3] incorporate frequency information, [4] use
distributional information to help identify syntactic
properties, and [5] use transitive closure to help find
variants that may not have been found to be
semantically related but which are related to mutual
variants.  We then apply these strategies to English,
Figure 1: Strategy and evaluation
Figure 2: Inserting the residual lexicon into a trie
German, and Dutch.  We evaluate our algorithm Figure 2). Yet using this approach, there may be
against the human-labeled CELEX lexicon in all circumfixes whose endings will be overlooked in
three languages and compare our results to those the search for suffixes unless we first remove all
that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes.  Therefore, we build a lexicon
would have obtained on our same data. We show consisting of all words in our corpus and identify all
how each of our additions result in progressively word beginnings with frequencies in excess of some
better overall solutions. threshold (T ). We call these pseudo-prefixes. We
3  Current Approach
3.1 Finding Candidate Circumfix Pairings
As in our earlier approach (Schone and Jurafsky,
2000), we begin by generating, from an untagged
corpus, a list of word pairs that might be
morphological variants.  Our algorithm has changed
somewhat, though, since we previously sought word
pairs that vary only by a prefix or a suffix, yet we
now wish to generalize to those with circumfixing
differences.  We use ?circumfix? to mean true
circumfixes like the German ge-/-t as well as
combinations of prefixes and suffixes. It should be
mentioned also that we assume the existence of
languages having valid circumfixes that are not
composed merely of a prefix  and a suffix that
appear independently elsewhere.  
To find potential morphological variants, our first
goal is to find word endings which could serve as
suffixes. We had shown in our earlier work how one
might do this using a character tree, or  trie  (as in
1
strip all pseudo-prefixes from each word in our
lexicon and add the word residuals back into the
lexicon as if they were also words.  Using this final
lexicon, we can now seek for suffixes in a manner
equivalent to what we had done before (Schone and
Jurafsky, 2000).  
To demonstrate how this is done, suppose our
initial  lexicon / contained the words ?align,?
?real,? ?aligns,? ?realign?, ?realigned?,  ?react?,
?reacts,? and ?reacted.? Due to the high frequency
occurrence of ?re-? suppose it is identified as a
pseudo-prefix.  If we strip off ?re-? from all words,
and add all residuals to a trie, the branch of the trie
of words beginning with ?a? is depicted in Figure 2.
In our earlier work, we showed that a majority of
the regular suffixes in the corpus can be found by
identifying trie branches that appear repetitively.
By ?branch? we mean those places in the trie where
some splitting occurs.  In the case of Figure 2, for
example, the branches  NULL (empty circle), ?-s?
and ?-ed? each appear twice.  We assemble a list of
all trie branches that occur some minimum number
of times (T ) and refer to such as potential suffixes.2
Given this list, we can now  find potential prefixes
using a similar strategy. Using our original lexicon,
we can now strip off all potential suffixes from each
word and form a new augmented lexicon.  Then, (as
we had proposed before) if we reverse the ordering
on the words and insert them into a trie, the
branches that are formed will be potential prefixes
(in reverse order).
Before describing the last steps of this procedure,
it is beneficial to define a few terms (some of which
appeared in our previous work):
[a] potential circumfix: A pair B/E where B and E
occur respectively in potential prefix and suffix lists
[b] pseudo-stem: the residue of a word after its
potential circumfix is removed
[c] candidate circumfix: a potential circumfix which
appears affixed to at least T  pseudo-stems that are3
shared by other potential circumfixes
[d] rule: a pair of candidate circumfixes sharing at
least T  pseudo-stems4
[e] pair of potential morphological variants
(PPMV): two words sharing the same rule but
distinct candidate circumfixes
[f] ruleset: the set of all PPMVs for a common rule
Our final goal in this first stage of induction is to
find all of the possible rules and their corresponding
rulesets. We therefore re-evaluate each word in the
original lexicon to identify all potential circumfixes
that could have been valid for the word.  For
example, suppose that the lists of potential suffixes
and prefixes contained ?-ed? and  ?re-? respectively.
Note also that NULL exists by default in both lists
as well.  If we consider the word ?realigned? from
our lexicon /, we would find that its potential
circumfixes would be NULL/ed, re/NULL, and
re/ed and the corresponding pseudo-stems would be
?realign,? ?aligned,? and ?align,? respectively,
From /, we also note that circumfixes re/ed and
NULL/ing share the pseudo-stems ?us,? ?align,? and
?view? so a rule could be created: re/ed<NULL/ing.
This means that word pairs such as ?reused/using?
and ?realigned/aligning? would be deemed PPMVs.
Although the choices in T  through T  is1 4
somewhat arbitrary, we chose  T =T =T =10 and1 2 3
T =3. In English, for example, this yielded 305354
possible rules. Table 1 gives a sampling of these
potential rules in each of the three languages in
terms of frequency-sorted rank.  Notice that several
?rules? are quite valid, such as the indication of an
English suffix -s. There are also valid circumfixes
like the ge-/-t circumfix of German. Capitalization
also appears (as a ?prefix?), such as C< c in English,
D<d in German, and V<v in Dutch. Likewise,there
are also some rules that may only be true in certain
circumstances, such as -d<-r in English (such as
worked/worker, but certainly not for steed/steer.)
However, there are some rules that are
Table 1: Outputs of the trie stage: potential rules
Rank ENGLISH GERMAN DUTCH
1 -s< L -n< L -en< L
2 -ed< -ing -en< L -e< L
4 -ing< L -s< L -n< L
8 -ly< L -en< -t de-< L
12 C-< c- -en< -te -er< L
16 re-< L 1-< L -r< L
20 -ers< -ing er-< L V-< v-
24 1-< L 1-< 2- -ingen < -e
28 -d< -r ge-/-t < -en ge-< -e
32 s-< L D-< d- -n< -rs
 
wrong: the potential ?s-? prefix of English  is never
valid although word combinations like stick/tick
spark/park, and slap/lap happen frequently in
English. Incorporating semantics can help determine
the validity of each rule.
3.2 Computing Semantics
Deerwester, et al (1990) introduced an algorithm
called Latent Semantic Analysis (LSA) which
showed that valid semantic relationships between
words and documents in a corpus can be induced
with virtually no human intervention. To do this,
one typically begins by applying singular value
decomposition (SVD) to a matrix, M, whose entries
M(i,j) contains the frequency of word i as seen in
document j of the corpus. The SVD decomposes M
into the product of three matrices, U, D, and V  suchT
that U and V  are orthogonal matrices and D is aT
diagonal matrix whose entries are the singular
values of M.  The LSA approach then zeros out all
but the top k singular values of the SVD, which has
the effect of projecting vectors into an optimal k-
dimensional subspace. This methodology is
well-described in the literature (Landauer, et al,
1998; Manning and Sch?tze, 1999). 
In order to obtain semantic representations of each
word, we apply our previous strategy (Schone and
Jurafsky (2000)). Rather than using a term-
document matrix, we had followed an approach akin
to that of Sch?tze (1993), who performed SVD on
a Nx2N  term-term matrix.  The N here represents
the N-1 most-frequent words as well as a glob
position to account for all other words not in the top
N-1.  The matrix  is structured such that for a given
word w?s row, the first N columns denote words that
-NCS (?,1) 

P

NCS
exp[	((x	?)/1)2]dx
NCS(w1,w2 ) 

min
k(1,2)
cos(
w1 ,w2)	?k
1k
(1)
Pr(NCS)


nT-NCS(?T,1T)
(nR	nT)-NCS(0,1)  nT-NCS(?T,1T)
.
precede w by up to 50 words, and the second N
columns represent those words that follow by up to
50 words.  Since SVDs are more designed to work then, if there were n  items in the ruleset, the
with  normally-distributed data (Manning and probability that a NCS is non-random is
Sch?tze, 1999, p. 565), we fill each entry with a
normalized count (or Z-score) rather than straight
frequency.   We then compute the SVD and keep the
top 300 singular values to form semantic vectors for We define Pr (w <w )=Pr(NCS(w ,w )). We
each word.  Word w would be assigned the semantic choose to accept as valid relationships only those
vector  U D , where U  represents the row ofW= w k w
U corresponding to w and D  indicates that only thek
top k diagonal entries of D have been preserved. 
As a last comment, one would like to be able to
obtain a separate semantic vector for every word
(not just those in the top N).  SVD computations can
be expensive and impractical for large values of N.
Yet due to the fact that U and V  are orthogonalT
matrices, we can start with a matrix of reasonable-
sized N and ?fold in? the remaining terms, which is
the approach we have followed.  For details about
folding in terms, the reader is referred to Manning
and Sch?tze (1999, p. 563).
3.3 Correlating Semantic Vectors
To correlate these semantic vectors, we use
normalized cosine scores (NCSs) as we had
illustrated before (Schone and Jurafsky (2000)).
The normalized cosine score between two words w1
and w  is determined by first computing cosine2
values between each word?s semantic vector and
200 other randomly selected semantic vectors.  This
provides a mean (?) and variance (1 ) of correlation2
for each word.  The NCS is given to be 
We had previously illustrated NCS values on
various PPMVs and showed that this type of score
seems to be appropriately identifying semantic
relationships. (For example, the PPMVs of car/cars
and ally/allies had NCS values of 5.6 and 6.5
respectively, whereas car/cares and ally/all had
scored only -0.14 and -1.3.)  Further, we showed
that by performing this normalizing process, one can
estimate the probability that an NCS is random or
not.  We expect that random NCSs will be
approximately normally distributed according to
N(0,1). We can also estimate the distribution
N(? ,1 ) of true correlations and number of  termsT T2
in that distribution (n ).  If we define  a functionT
R
sem 1 2 1 2
PPMVs with Pr T , where T  is an acceptancesem 5 5
threshold. We showed in our earlier work that
T =85% affords high overall precision while still5
identifying most valid morphological relationships.
 3.4 Augmenting with Affix Frequencies
The first major change to our previous algorithm is
an attempt to overcome some of the weaknesses of
purely semantic-based morphology induction by
incorporating information about affix frequencies.
As validated by Kazakov (1997), high frequency
word endings and beginnings in inflectional
languages are very likely to be legitimate affixes.  In
English, for example, the highest frequency rule is
-s<L. CELEX suggests that 99.7% of our PPMVs
for this rule would be true. However, since the
purely semantic-based approach tends to select only
relationships with contextually similar meanings,
only 92% of the PPMVs are retained.  This suggests
that one might improve the analysis by
supplementing semantic probabilities with
orthographic-based probabilities (Pr ). orth
Our approach to obtaining Pr   is motivated byorth
an appeal to minimum edit distance (MED). MED
has been applied to the morphology induction
problem by other researchers (such as Yarowsky
and Wicentowski, 2000).  MED determines the
minimum-weighted set of insertions, substitutions,
and deletions required to transform one word into
another. For example, only a single deletion is
required to transform ?rates? into ?rate? whereas
two substitutions and an insertion are required to
transform it into ?rating.? Effectively, if Cost(&) is
transforming cost, Cost(rates<rate) = Cost(s<L)
whereas Cost(rates<rating)=Cost(es<ing). More
generally, suppose word X has circumfix C =B /E1 1 1
and pseudo-stem -S-, and word Y has circumfix
C =B /E  also with pseudo-stem -S-. Then,2 2 2
Cost(X<Y)=Cost(B SE <B SE )=Cost(C <C ).1 1 2 2 1 2
Since we are free to choose whatever cost function
we desire, we can equally choose one whose range
Cost(C1<C2)
1	
2 . f (C1<C2 )
max f (C1<Z) 
~Z
max f (W<C2)
~W
lies in the interval of [0,1]. Hence, we can assign Consider Table 2 which is a sample of PPMVs
Pr (X<Y) = 1-Cost(X<Y). This calculation implies from the ruleset for ?-s<L? along with theirorth
that the orthographic probability that X and Y are probabilities of validity.  A validity threshold (T ) of
morphological variants is directly derivable from the 85% would mean that the four bottom PPMVs
cost of transforming C  into C . would be deemed invalid.  Yet if we find that the1 2
The only question remaining is how to determine local contexts of these low-scoring word pairs
Cost(C <C ). This cost should depend on a number match the contexts of other PPMVs having high1 2
of factors: the frequency of the rule f(C <C ),  the scores (i.e., those whose scores exceed T ), then1 2
reliability of the metric in comparison to that of their probabilities of validity should increase.  If we
semantics (., where .  [0,1]), and the frequencies could compute a syntax-based probability for these
of other rules involving C  and C .  We define the words, namely Pr , then assuming independence1 2
orthographic probability of validity as we would have:
  Figure 3 describes the pseudo-code for an
We suppose that orthographic information is less (L) and right-hand (R) sides of each valid PPMV of
reliable than semantic information, so we arbitrarily a given ruleset, try to find a collection of words
set .=0.5.  Now since Pr (X<Y)=1-Cost(C <C ), from the corpus that are collocated with L and R butorth 1 2
we can readily combine it with Pr  if we assume which occur statistically too many or too few timessem
independence using the ?noisy or? formulation: in these collocations.  Such word sets form
  Pr (valid) = Pr  +Pr  - (Pr  Pr ).  (2) signatures.  Then, determine similar signatures fors-o sem orth sem orth  
By using this formula, we obtain 3% (absolute)
more of the correct PPMVs than semantics alone
had provided for the -s<L rule and, as will be
shown later, gives reasonable improvements overall.
3.5 Local Syntactic Context
Since a primary role of morphology ? inflectional
morphology in particular ?  is to convey syntactic
information, there is no guarantee that two words
that are morphological variants need to share similar
semantic properties.  This suggests that performance
could improve if the induction process took
advantage of  local, syntactic contexts around words
in addition to the more global, large-window
contexts used in semantic processing.  
Table 2: Sample probabilities for ?-s<L?
Word+s Word Pr Word+s Word Pr
agendas agenda .968 legends legend .981
ideas idea .974 militias militia 1.00
pleas plea 1.00 guerrillas guerrilla 1.00
seas sea 1.00 formulas formula 1.00
areas area 1.00 railroads railroad 1.00
Areas Area .721 pads pad .731
Vegas Vega .641 feeds feed .543
5
5
syntax
Pr (valid) = Pr  +Pr  - (Pr  Pr )
  s-o syntax s-o syntax
algorithm to compute Pr .  Essentially, thesyntax
algorithm has two major components.  First, for left
a randomly-chosen set of words from the corpus as
well as for each of the PPMVs of the ruleset that are
not yet validated.  Lastly, compute the NCS and
their corresponding probabilities (see equation 1)
between the ruleset?s signatures and those of the to-
be-validated PPMVs to see if they can be validated.
Table 3 gives an example of the kinds of
contextual words one might expect for the ?-s<L?
rule. In fact, the syntactic signature for ?-s<L? does
indeed include such words as are, other, these, two,
were, and have as indicators of words that occur on
the left-hand side of the ruleset, and a, an, this, is,
has, and A as indicators of the right-hand side.
These terms help distinguish plurals from singulars.
Table 3: Examples of ?-s<L? contexts
Context for L Context for R
agendas are seas were a legend this formula
two red pads pleas have militia is an area
these ideas other areas railroad has A guerrilla
There is an added benefit from following this
approach: it can also be used to find rules that,
though different, seem to convey similar
information . Table 4 illustrates a number of such
agreements.  We have yet to take advantage of this
feature, but it clearly could be of use for part-of-
speech induction.
procedure SyntaxProb(ruleset,corpus)
    leftSig  =GetSignature(ruleset,corpus,left)
    rightSig=GetSignature(ruleset,corpus,right)
         =Concatenate(leftSig, rightSig)ruleset
    (? ,1 )=ComparetoRandom( )ruleset ruleset ruleset
    foreach PPMV in ruleset
       if   (Pr (PPMV)  T  )   continueS-O 5
       wLSig=GetSignature(PPMV,corpus,left)
       wRSig=GetSignature(PPMV,corpus,right)
         =Concatenate(wLSig, wRSig)PPMV
       (? ,1 )=ComparetoRandom( )PPMV PPMV PPMV
       prob[PPMV]=Pr(NCS(PPMV,ruleset))
end procedure
function GetSignature(ruleset,corpus,side)
    foreach PPMV in ruleset
        if   (Pr (PPMV) < T  )   continueS-O 5
        if  (side=left) X = LeftWordOf(PPMV)
        else  X = RightWordOf(PPMV)
        CountNeighbors(corpus,colloc,X)
    colloc  =SortWordsByFreq(colloc)
    for i = 1 to 100 signature[i]=colloc[i]
    return signature
end function
procedure CountNeighbors(corpus,colloc,X)
   foreach W in Corpus
       push(lexicon,W)
       if (PositionalDistanceBetween(X,W)2)
          count[W] = count[W]+1
    foreach W in lexicon
       if ( Zscore(count[W]) 3.0   or
             Zscore(count[W]) -3.0)
           colloc[W]=colloc[W]+1
end procedure
Figure 3: Pseudo-code to find Probability  syntax Figure 4: Semantic strengths
Table 4: Relations amongst rules
Rule Relative Cos Rule Relative Cos
-s<L -ies<y 83.8 -ed<L -d<L 95.5
-s<L -es<L 79.5 -ing<L -e<L 94.3
-ed<L -ied<y 81.9 -ing<L -ting<L 70.7
3.6 Branching Transitive Closure
Despite the semantic, orthographic, and syntactic
components of the algorithm, there are still valid
PPMVs, (X<Y), that may seem unrelated due to
corpus choice or weak distributional properties.
However, X and Y may appear as members of other
valid PPMVs such as (X<Z) and (Z<Y) containing
variants (Z, in this case) which are either
semantically or syntactically related to both of the
other words.  Figure 4 demonstrates this property in
greater detail.  The words conveyed in Figure 4 are
all words from the corpus that have potential
relationships between variants of the word ?abuse.?
Links between two words, such as ?abuse? and
?Abuse,? are labeled with a weight which is the
semantic correlation derived by LSA. Solid lines
represent valid relationships with Pr 0.85 andsem
dashed lines indicate relationships with lower-than-
threshold scores. The absence of a link suggests that
either the potential relationship was never identified
or discarded at an earlier stage.  Self loops are
assumed for each node since clearly each word
should be related morphologically to itself. Since
there are seven words that are valid morphological
relationships of ?abuse,? we would like to see a
complete graph containing 21 solid edges.  Yet, only
eight connections can be found by semantics alone
(Abuse<abuse, abusers<abusing, etc.).  
However, note that there is a path that can be
followed along solid edges from every correct word
to every other correct variant.  This suggests that
taking into consideration link transitivity (i.e., if
X<Y , Y <Y , Y <Y ,... and Y<Z, then X<Z)1 1 2 2 3 t
may drastically reduce the number of deletions. 
There are two caveats that need to be considered
for transitivity to be properly pursued.  The first
caveat: if no rule exists that would transform X into
Z, we will assume that despite the fact that there
may be a probabilistic path between the two, we
Pr
? i 
 
t
N
t
j
0 pj.
function BranchProbBetween(X,Z)
prob=0
foreach independent path ?j
    prob = prob+Pr (X<Z) - (prob*Pr (X<Z) )
?j ?j
return prob
Figure 5: Pseudocode for Branching Probability
Figure 6: Morphologic relations of ?conduct?
will disregard such a path. The second caveat is that  the algorithms we test against.  Furthermore, since
we will say that paths can only consist of solid CELEX has limited coverage, many of these lower-
edges, namely each Pr(Y<Y ) on every path must frequency words could not be scored anyway.  Thisi i+1
exceed  the specified  threshold. cut-off also helps each of the algorithms to obtain
Given these constraints, suppose now there is a stronger statistical information on the words they do
transitive relation from X to Z by way of some process which means that any observed failures
intermediate path ?={Y Y Y }.  That is, assume cannot be attributed to weak statistics.i 1, 2,.. t
there is a path X<Y  Y <Y ,...,Y<Z.  Suppose Morphological relationships can be represented as1, 1 2 t
also that the probabilities of these relationships are directed graphs.  Figure 6, for instance, illustrates
respectively p , p , p ,...,p .  If  is a decay factor in the directed graph, according to CELEX, of words0 1 2 t
the unit interval accounting for the number of link associated with ?conduct.?  We will call the words
separations, then we will say that the Pr(X<Z) of such a directed graph the conflation set for any of
along path ?  has probability                       .       We the words in the graph.   Due to the difficulty ini
combine the probabilities of all independent paths developing a scoring algorithm to compare directed
between X and Z according to Figure 5: graphs, we will follow our earlier approach and only
If the returned probability exceeds T , we declare X5
and Z to be morphological variants of each other.
4 Evaluation
We compare this improved algorithm to our former
algorithm (Schone and Jurafsky (2000)) as well as
to Goldsmith's Linguistica (2000).  We use as input
to our system 6.7 million words of English
newswire, 2.3 million of German, and 6.7 million of
Dutch.  Our gold standards are the hand-tagged
morphologically-analyzed CELEX lexicon in each
of these languages (Baayen, et al, 1993). We apply
the algorithms only to those words of our corpora
with frequencies of 10 or more.   Obviously this cut-
off slightly limits the generality of our results, but
it also greatly decreases processing time for all of
compare induced conflation sets to those of
CELEX. To evaluate, we compute the number of
correct (&), inserted (,), and deleted (') words each
algorithm predicts for each hypothesized conflation
set.  If X  represents word w's conflation setw
according to an algorithm, and if Y   represents itsw
CELEX-based conflation set, then, 
& = ~w(|X Y |/|Y |), w w w
' = ~w(|Y -(X Y )|/|Y |), andw w w w
,  = ~w (|X -(X Y )|/|Y |),w w w w
In making these computations, we disregard any
CELEX words absent from our data set and vice
versa. Most capital words are not in CELEX so this
process also discards them. Hence, we also make an
augmented CELEX to incorporate capitalized forms.
Table 5 uses the above scoring mechanism to
compare the F-Scores (product of precision and
recall divided by average of the two ) of our system
at a cutoff threshold of 85% to those of our earlier
algorithm (?S/J2000?) at the same threshold;
Goldsmith; and a baseline system which performs
no analysis (claiming that for any word, its
conflation set only consists of itself). The ?S? and
?C? columns respectively indicate performance of
systems when scoring for suffixing and
circumfixing (using the unaugmented CELEX). The
?A? column shows circumfixing performance using
the augmented CELEX. Space limitations required
that we illustrate ?A? scores for one language only,
but performance in the other two language is
similarly degraded. Boxes are shaded out for
algorithms not designed to produce circumfixes. 
Note that each of our additions resulted in an
overall improvement which held true across each of
the three languages.  Furthermore, using ten-fold
cross validation on the English data, we find that F-
score differences of the S column are each
statistically significant at least at the 95% level.
Table 5: Computation of F-Scores
Algorithms English German Dutch
S C A S C S C
None 62.8 59.9 51.7 75.8 63.0 74.2 70.0
Goldsmith 81.8 84.0 75.8
S/J2000 85.2 88.3 82.2
+orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9
+ syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4
+ transitive 84.5 79.7 78.9 79.688.1 92.3 85.8
5 Conclusions
We have illustrated three extensions to our earlier
morphology induction work (Schone and Jurafsky
(2000)). In addition to induced semantics, we
incorporated induced orthographic, syntactic, and
transitive information resulting in almost a 20%
relative reduction in overall  induction error.  We
have also extended the work by illustrating
performance in German and Dutch where, to our
knowledge, complete morphology induction
performance measures have not previously been
obtained.  Lastly, we showed a mechanism whereby
circumfixes as well as combinations of prefixing
and suffixing can be induced in lieu of the suffix-
only strategies prevailing in most previous research.
For the future, we expect improvements could be
derived by coupling this work, which focuses
primarily on inducing regular morphology, with that
of Yarowsky and Wicentowski (2000), who assume
some information about regular morphology in order
to induce irregular morphology. We also believe
that some findings of this work can benefit other
areas of linguistic induction, such as part of speech.
Acknowledgments
The authors wish to thank the anonymous reviewers
for their thorough review and insightful comments.
References
Baayen, R.H., R. Piepenbrock, and H. van Rijn. (1993)
The CELEX lexical database (CD-ROM), LDC, Univ.
of Pennsylvania, Philadelphia, PA.
Brent, M., S. K. Murthy, A. Lundberg. (1995).
Discovering morphemic suffixes: A case study in
MDL induction. Proc. Of 5  Int?l Workshop onth
Artificial Intelligence and Statistics
D?Jean, H. (1998) Morphemes as necessary concepts for
structures: Discovery from untagged corpora.
Workshop on paradigms and Grounding in Natural
Language Learning, pp. 295-299.Adelaide, Australia
Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. (1990) Indexing by
Latent Semantic Analysis. Journal of the American
Society of Information Science, Vol. 41, pp.391-407.
Gaussier, ?. (1999) Unsupervised learning of derivational
morphology from inflectional lexicons. ACL '99
Workshop: Unsupervised Learning in Natural
Language Processing, Univ. of Maryland.
Goldsmith, J. (1997/2000) Unsupervised learning of the
morphology of a natural language. Univ. of Chicago.
http://humanities.uchicago.edu/faculty/goldsmith.
Grabar, N. and P. Zweigenbaum. (1999) Acquisition
automatique de connaissances morphologiques sur le
vocabulaire  m?dical, TALN, Carg?se, France.
Harris, Z.  (1951) Structural Linguistics. University of
Chicago Press.
Jacquemin, C. (1997) Guessing morphology from terms
and corpora. SIGIR'97, pp. 156-167, Philadelphia, PA.
Kazakov, D. (1997) Unsupervised learning of na?ve
morphology with genetic algorithms. In W.
Daelemans, et al, eds., ECML/Mlnet Workshop on
Empirical Learning of Natural Language Processing
Tasks, Prague, pp. 105-111.
Landauer, T.K., P.W. Foltz, and D. Laham. (1998)
Introduction to Latent Semantic Analysis. Discourse
Processes. Vol. 25, pp. 259-284.
Manning, C.D. and H. Sch?tze. (1999) Foundations of
Statistical Natural Language Processing, MIT Press,
Cambridge, MA.
Nakisa, R.C., U.Hahn. (1996) Where defaults don't help:
the case of the German plural system.  Proc. of the
18th Conference of the Cognitive Science Society.
Schone, P. and D. Jurafsky. (2000) Knowledge-free
induction of morphology using latent semantic
analysis.  Proc. of the Computational Natural
Language Learning Conference, Lisbon, pp. 67-72.
Sch?tze, H. (1993) Distributed syntactic representations
with an application to part-of-speech tagging.
Proceedings of the IEEE International Conference on
Neural Networks, pp. 1504-1509.
Sproat, R. (1992) Morphology and Computation. MIT
Press, Cambridge, MA.
Xu, J., B.W. Croft. (1998) Corpus-based stemming using
co-occurrence of word variants. ACM Transactions on
Information Systems, 16 (1), pp. 61-81.
Yarowsky, D. and R. Wicentowski. (2000) Minimally
supervised morphological analysis by multimodal
alignment.  Proc. of the ACL 2000, Hong Kong.
In: Proceedings of CoNLL-2000 and LLL-2000, pages 67-72, Lisbon, Portugal, 2000. 
Knowledge-Free Induction of Morphology 
Using Latent Semantic Analysis 
Pat r i ck  Schone and Dan ie l  Ju ra fsky  
University of Colorado 
Boulder, Colorado 80309 
{schone, jurafsky}@cs.colorado.edu 
Abst ract  
Morphology induction is a subproblem of 
important tasks like automatic learning of 
machine-readable dictionaries and grammar in- 
duction. Previous morphology induction ap- 
proaches have relied solely on statistics of hy- 
pothesized stems and affixes to choose which 
affixes to consider legitimate. Relying on stem- 
and-affix statistics rather than semantic knowl- 
edge leads to a number of problems, such as the 
inappropriate use of valid affixes ("ally" stem- 
ming to "all"). We introduce a semantic-based 
algorithm for learning morphology which only 
proposes affixes when the stem and stem-plus- 
affix are sufficiently similar semantically. We 
implement our approach using Latent Seman- 
tic Analysis and show that our semantics-only 
approach provides morphology induction results 
that rival a current state-of-the-art system. 
1 In t roduct ion  
Computational morphological analyzers have 
existed in various languages for years and it has 
been said that "the quest for an efficient method 
for the analysis and generation of word-forms is 
no longer an academic research topic" (Karlsson 
and Karttunen, 1997). However, development 
of these analyzers typically begins with human 
intervention requiring time spans from days to 
weeks. If it were possible to build such ana- 
lyzers automatically without human knowledge, 
significant development time could be saved. 
On a larger scale, consider the task 
of inducing machine-readable dictionaries 
(MRDs) using no human-provided information 
("knowledge-free"). In building an MRD, 
"simply expanding the dictionary to encompass 
every word one is ever likely to encounter...fails 
to take advantage of regularities" (Sproat, 
1992, p. xiii). Hence, automatic morphological 
analysis is also critical for selecting appropriate 
and non-redundant MRD headwords. 
For the reasons expressed above, we are in- 
terested in knowledge-free morphology induc- 
tion. Thus, in this paper, we show how to au- 
tomatically induce morphological relationships 
between words. 
Previous morphology induction approaches 
(Goldsmith, 1997, 2000; D4Jean, 1998; Gauss- 
ier, 1999) have focused on inflectional languages 
and have used statistics of hypothesized stems 
and affixes to choose which affixes to consider 
legitimate. Several problems can arise using 
only stem-and-affix statistics: (1) valid affixes 
may be applied inappropriately ("ally" stem- 
ming to "all"), (2) morphological ambiguity 
may arise ("rating" conflating with "rat" in- 
stead of "rate"), and (3) non-productive affixes 
may get accidentally pruned (the relationship 
between "dirty" and "dirt" may be lost)3 
Some of these problems could be resolved 
if one could incorporate word semantics. For 
instance, "all" is not semantically similar to 
"ally," so with knowledge of semantics, an algo- 
rithm could avoid conflating these two words. 
To maintain the "knowledge-free" paradigm, 
such semantics would need to be automati- 
cally induced. Latent Semantic Analysis (LSA) 
(Deerwester, et al, 1990); Landauer, et al, 
1998) is a technique which automatically iden- 
tifies semantic information from a corpus. We 
here show that incorporating LSA-based seman- 
tics alone into the morphology-induction pro- 
cess can provide results that rival a state-oh 
the-art system based on stem-and-affix statis- 
tics (Goldsmith's Linguistica). 
1Error examples are from Goldsmith's Linguistica 
67 
Our algorithm automatically extracts poten- 
tial affixes from an untagged corpus, identifies 
word pairs sharing the same proposed stem but 
having different affixes, and uses LSA to judge 
semantic relatedness between word pairs. This 
process erves to identify valid morphological re- 
lations. Though our algorithm could be applied 
to any inflectional language, we here restrict 
it to English in order to perform evaluations 
against the human-labeled CELEX database 
(Baayen, et al, 1993). 
2 P rev ious  work  
Existing induction algorithms all focus on iden- 
tifying prefixes, suffixes, and word stems in in- 
flectional languages (avoiding infixes and other 
language types like concatenative or aggluti- 
native languages (Sproat, 1992)). They also 
observe high frequency occurrences of some 
word endings or beginnings, perform statistics 
thereon, and propose that some of these ap- 
pendages are valid morphemes. 
However, these algorithms differ in specifics. 
D~Jean (1998) uses an approach derived from 
Harris (1951) where word-splitting occurs if the 
number of distinct letters that follows a given 
sequence of characters urpasses a threshoid. 
He uses these hypothesized affixes to resegment 
words and thereby identify additional affixes 
that were initially overlooked. His overall goal is 
different from ours: he primarily seeks an affix 
inventory. 
Goldsmith (1997) tries cutting each word 
in exactly one place based on probability and 
lengths of hypothesized stems and affixes. He 
applies the EM algorithm to eliminate inappro- 
priate parses. He collects the possible suffixes 
for each stem calling these a signature which 
aid in determining word classes. Goldsmith 
(2000) later incorporates minimum description 
length to identify stemming characteristics that 
most compress the data, but his algorithm oth- 
erwise remains similar in nature. Goldsmith's 
algorithm is practically knowledge-free, though 
he incorporates capitalization removal and some 
word segmentation. 
Gaussier (1999) begins with an inflectional 
lexicon and seeks to find derivational morphol- 
ogy. The words and parts of speech from his 
inflectional lexicon serve for building relational 
families of words and identifying sets of word 
pairs and suffixes therefrom. Gaussier splits 
words based on p-similarity - words that agree 
in exactly the first p characters. He also builds 
a probabilistic model which indicates that the 
probability of two words being morphological 
variants is based upon the probability of their 
respective changes in orthography and morpho- 
syntactics. 
3 Cur rent  approach  
Our algorithm also focuses on inflectional lan- 
guages. However, with the exception of word 
segmentation, we provide it no human informa- 
tion and we consider only the impact of seman- 
tics. Our approach (see Figure 1) can be de- 
composed into four components: (1) initially 
selecting candidate affixes, (2) identifying af- 
fixes which are potential morphological vari- 
ants of each other, (3) computing semantic vec- 
tors for words possessing these candidate affixes, 
and (4) selecting as valid morphological variants 
those words with similar semantic vectors. 
Figure 1: Processing Architecture 
Stage 1 Stage 2 Stage 3 Stage 4 
Identify I\[ paa~~ l~ I\[ semantic II variants 
potential \[lare pos lmell vectors II that have 
affixes I I morplm- I I for I I slmuar 
........ ) ( logical \]( words \] ( semantic 
3.1 Hypothes iz ing  affixes 
To select candidate affixes, we, like Gaussier, 
identify p-similar words. We insert words into a 
trie (Figure 2) and extract potential affixes by 
observing those places in the trie where branch- 
ing occurs. Figure 2's hypothesized suffixes are 
NULL, "s," "ed," "es," "ing," "e," and "eful." 
We retain only the K most-frequent candidate 
affixes for subsequent processing. The value for 
K needs to be large enough to account for the 
number of expected regular affixes in any given 
language as well as some of the more frequent 
irregular affixes. We arbitrarily chose K to be 
200 in our system. (It should also be mentioned 
that we can identify potential prefixes by insert- 
ing words into the trie in reversed order. This 
prefix mode can additionally serve for identify- 
ing capitalization.) 
68 
F igure  2: Trie structure 
( 
0 0 
() 
( ) 
3.2 Morpho log ica l  variants 
We next identify pairs of candidate affixes that 
descend from a common ancestor node in the 
trie. For example, ("s", NULL) constitutes such 
a pair from Figure 2. We call these pairs rules. 
Two words sharing the same root and the 
same affix rule, such as "cars" and "car," form 
what we call a pair of potential morphological 
variants (PPMVs). We define the ruleset of a 
given rule to be the set of all PPMVs that have 
that rule in common. For instance, from Figure 
2, the ruleset for ("s", NULL) would be the pairs 
"cars/car" and "cares/care." Our algorithm es- 
tablishes a list which identifies the rulesets for 
every hypothesized rule extracted from the data 
and then it must proceed to determine which 
rulesets or PPMVs describe true morphological 
relationships. 
3.3 Computing Semantic Vectors 
Deerwester, et al (1990) showed that it is 
possible to find significant semantic relation- 
ships between words and documents in a corpus 
with virtually no human intervention (with the 
possible exception of a human-built stop word 
list). This is typically done by applying singu- 
lar value decomposition (SVD) to a matrix, M, 
where each entry M(i,j) contains the frequency 
of word i as seen in document j of the corpus. 
This methodology is referred to as Latent Se- 
mantic Analysis (LSA) and is well-described in
the literature (Landauer, et al, 1998; Manning 
and Schfitze, 1999). 
SVDs seek to decompose a matrix A into the 
product of three matrices U, D, and V T where 
U and V T are  orthogonal matrices and D is 
a diagonal matrix containing the singular val- 
ues (squared eigenvalues) of A. Since SVD's 
can be performed which identify singular val- 
ues by descending order of size (Berry, et al, 
1993), LSA truncates after finding the k largest 
singular values. This corresponds to projecting 
the vector representation of each word into a 
k-dimensional subspace whose axes form k (la- 
tent) semantic directions. These projections are 
precisely the rows of the matrix product UkDk. 
A typical k is 300, which is the value we used. 
However, we have altered the algorithm some- 
what to fit our needs. First, to stay as close to 
the knowledge-free scenario as possible, we nei- 
ther apply a stopword list nor remove capitaliza- 
tion. Secondly, since SVDs are more designed 
to work on normally-distributed data (Manning 
and Schiitze, 1999, p. 565), we operate on Z- 
scores rather than counts. Lastly, instead of 
generating a term-document matrix, we build a 
term-term atrix. 
Schiitze (1993) achieved excellent perfor- 
mance at classifying words into quasi-part- 
of-speech classes by building and perform- 
ing an SVD on an Nx4N term-term matrix, 
M(i,Np+j). The indices i and j represent the 
top N highest frequency words. The p values 
range from 0 to 3 representing whether the word 
indexed by j is positionally offset from the word 
indexed by i by -2, -1, +1, or +2, respectively. 
For example, if "the" and "people" were re- 
spectively the 1st and 100th highest frequency 
words, then upon seeing the phrase "the peo- 
ple," Schfitze's approach would increment the 
counts of M(1,2N+100) and M(100,N+i).  
We used Schfitze's general framework but tai- 
lored it to identify local semantic information. 
We built an Nx2N matrix and our p values cor- 
respond to those words whose offsets from word 
i are in the intervals \[-50,-1\] and \[1,501, respec- 
tively. We also reserve the Nth position as a 
catch-all position to account for all words that 
are not in the top (N-l). An important issue to 
resolve is how large should N be. We would like 
69 
to be able to incorporate semantics for an arbi- 
trarily large number of words and LSA quickly 
becomes impractical on large sets. Fortunately, 
it is possible to build a matrix with a smaller 
value of N (say, 2500), perform an SVD thereon, 
and then fold in remaining terms (Manning and 
Schfitze, 1999, p. 563). Since the U and V ma- 
trices of an SVD are orthogonal matrices, then 
uuT:vvT : I .  This implies that AV=UD.  
This means that for a new word, w, one can 
build a vector ~T which identifies how w relates 
to the top N words according to the p different 
conditions described above. For example, if w 
were one of the top N words, then ~w T would 
simply represent w's particular ow from the A 
matrix. The product f~w = ~wTVk is the projec- 
tion of ~T into the k-dimensional latent seman- 
tic space. By storing an index to the words of 
the corpus as well as a sorted list of these words, 
one can efficiently build a set of semantic vec- 
tors which includes each word of interest. 
3.4 Stat i s t i ca l  Computat ions  
Morphologically-related words frequently share 
similar semantics, so we want to see how well se- 
mantic vectors of PPMVs correlate. If we know 
how PPMVs correlate in comparison to other 
word pairs from their same rulesets, we can ac- 
tually determine the semantic-based probability 
that the variants are legitimate. In this section, 
we identify a measure for correlating PPMVs 
and illustrate how ruleset-based statistics help 
identify legitimate PPMVs. 
3.4.1 Semant ic  Cor re la t ion  of  Words  
The cosine of the angle between two vectors v l  
and v2 is given by, 
cos(v l ,v2) -  v l -v2  
II v l  llll v2 H" 
We want to determine the correlation between 
each of the words of every PPMV. We use what 
we call a normalized cosine score (NCS) as a cor- 
relation. To obtain a NCS, we first calculate the 
cosine between each semantic vector, nw, and 
the semantic vectors from 200 randomly chosen 
words. By this means we obtain w's correlation 
mean (#w) and standard deviation (aw). If v 
is one of w's variants, then we define the NCS 
between ~w and nv  to be 
cos(nw, nv)  - #y ). min ( 
ye{w,v} ay 
Table 1 provides normalized cosine scores for 
several PPMVs from Figure 2 and from among 
words listed originally as errors in other sys- 
tems. (NCSs are effectively Z-scores.) 
Table  1: Normalized Cosines for various PPMVs 
PPMVs I NCSs PPMVs NCSs I 
car/cars 5.6 ally/allies 6.5 
car/caring -0.71 ally/all -1.3 
car/cares -0.14 dirty/dirt  2.4 
car/cared i -0.96 rat ing/rate 0.97 
3.4.2 Ru leset - leve l  S ta t i s t i cs  
By considering NCSs for all word pairs cou- 
pled under a particular rule, we can deter- 
mine semantic-based probabilities that indicate 
which PPMVs are legitimate. We expect ran- 
dom NCSs to be normally-distributed accord- 
ing to Af(0,1). Given that a particular uleset 
contains nR PPMVs, we can therefore approx- 
imate the number (nT), mean (#T) and stan- 
dard deviation (aT) of true correlations. If we 
_C .~___~ 2 .  define ~z(#,a)  to be fee  " - J ax, then we 
can compute the probability that the particular 
correlation is legitimate: 
Pr( true) = nT ~ Z(~T ,aT) 
(nR--nT ~z(O, 1) +nT~Z(~T, aT)" 
3.4.3 Subru les  
It is possible that a rule can be hypothesized 
at the trie stage that is true under only certain 
conditions. A prime example of such a rule is 
("es", NULL). Observe from Table 1 that the 
word "cares" poorly correlates with "car." Yet, 
it is true that "-es" is a valid suffix for the words 
"flashes," "catches," "kisses," and many other 
words where the "-es" is preceded by a voiceless 
sibilant. 
Hence, there is merit to considering subrules 
that arise while performing analysis on a par- 
ticular rule. For instance, while evaluating the 
("es", NULL) rule, it is desirable to also con- 
sider potential subrules such as ("ches", "ch") 
and ("tes", "t"). One might expect hat the av- 
erage NCS for the ("ches", "ch") subrule might 
be higher than the overall rule ("es", NULL) 
whereas the opposite will likely be true for 
("tes', "t"). Table 2 confirms this. 
70 
Table 2: Analysis of subrules 
Rule/Subrule I Average StDev t#instances 
("es", NULL) 1.62 
("ches", "ch" ) 2.20 
("shes", "sh") 2.39 
("res", "r") -0.69 
("tes","t") -0.58 
2.43 173 
1.66 32 
1.52 15 
0.47 6 
0.93 11 
4 Resu l t s  
We compare our algorithm to Goldsmith's Lin- 
guistica (2000) by using CELEX's (Baayen, 
et al, 1993) suffixes as a gold standard. 
CELEX is a hand-tagged, morphologically- 
analyzed atabase of English words. CELEX 
has limited coverage of the words from our data 
set (where our data consists of over eight mil- 
lion words from random subcollections of TREC 
data (Voorhees, et a1,1997/8)), so we only con- 
sidered words with frequencies of 10 or more. 
F igure 3: Morphological directed graphs 
(b) (f) 
concerned concerted 
(a) / (c) (g) k 
concerns concerts , ~eoncer~ conceri~ (e) . 
\ conc(edr)ning con~eh!ting / 
(i) (j) 
concerto ~-- concertos 
Morphological relationships can be represented 
graphically as directed graphs (see Figure 3, 
where three separate graphs are depicted). De- 
veloping a scoring algorithm to compare di- 
rected graphs is likely to be prone to disagree- 
ments. Therefore, we score only the vertex sets 
of directed graphs. We will refer to these ver- 
tex sets as conflation sets. For example, con- 
cern's conflation set contains itself as well as 
"concerned," "concerns," and "concerning" (or, 
in shorthand notation, the set is {a,b,c,d}). 
To evaluate an algorithm, we sum the num- 
ber of correct (C), inserted (Z), and deleted (D) 
words it predicts for each hypothesized confla- 
tion set. If Xw represents word w's conflation 
set according to the algorithm, and if Yw repre- 
sents its CELEX-based conflation set, then 
C = Ew( Ixw AYwl/lYwl), 
= Evw(lYw - (Xw NYw)I/IYwl), and 
z = Ew( lx~ - (xw AY~)I/IYwl). 
However, in making these computations, we dis- 
regard any CELEX words that are not in the 
algorithm's data set and vice versa. 
For example, suppose two algorithms were be- 
ing compared on a data set where all the words 
from Figure 3 were available except "concert- 
ing" and "concertos." Suppose further that one 
algorithm proposed that {a,b,c,d,e,f,g,i} formed 
a single conflation set whereas the other algo- 
rithm proposed the three sets {a,b,c,d},{e,g,i}, 
and {f}. Then Table 3 illustrates how the two 
algorithms would be scored. 
Table 3: Example of scoring 
I II a I b I c I d I e I f I g I i IITotalt 
C1 4/4 4/4 4/4 4/4 3/3 3/3 3/3 1/1 8 
D1 0/4 0/4 0/4 0/4 0/3 0/3 0/3 0/1 0 
Zl 4/4 4/4 4/4 4/4 5/3 5/3 5/3 7/1 16 
C2 4/4 4/4 4/4 4/4 2/3 2/3 1/3 1/1 20/3 
D2 0/4 0/4 0/4 0/4 1/3 1/3 2/3 0/1 4/3 
Z2 0/4i0/4 0/4 0/4 1/3 1/3 0/3 2/1 8/3 
To explain Table 3, consider algorithm one's 
entries for 'a.' Algorithm one had pro- 
posed that Xa={a,b,c,d,e,f,g,i} when in reality, 
Ya={a,b,c,d}. Since IXa NYal = 4 and IYal=4, 
then CA=4/4. The remaining values of the table 
can be computed accordingly. 
Using the values from Table 3, we can 
also compute precision, recall, and F-Score. 
Precision is defined to be C/(C+Z), recall is 
C/(C+D), and F-Score is the product of pre- 
cision and recall divided by the average of the 
two. For the first algorithm, the precision, re- 
call, and F-Score would have respectively been 
1/3, 1, and 1/2. In the second algorithm, these 
numbers would have been 5/7, 5/6, and 10/13. 
Table 4 uses the above scoring mechanism to 
compare between Linguistica nd our system (at 
various probability thresholds). Note that since 
Linguistica removes capitalization, it will have 
a different otal word count than our system. 
71 
Table 4: Performance on English CELEX 
Algorithm Linguistica 
LSA- LSA- LSA- 
based based based 
pr_> 0.5 pr_> 0.7 pr> 0.85 
#Correct 10515 10529 10203 9863 
#Inserts 2157 1852 1138 783 
#Deletes 2571 2341 i 2667 3007 
Precision 83.0% 85.0% 90.0% 92.6% 
Recall 80.4% 81.8% 79.3% 76.6% 
F-Score 81.6% 83.4% 84.3% 83.9% 
5 Conc lus ions  
These results suggest hat semantics and LSA 
can play a key part in knowledge-free mor- 
phology induction. Semantics alone worked at 
least as well as Goldsmith's frequency-based ap- 
proach. Yet we believe that semantics-based 
and frequency-based approaches play comple- 
mentary roles. In current work, we are examin- 
ing how to combine these two approaches. 
Re ferences  
Albright, A. and B. P. Hayes. 1999. An au- 
tomated learner for phonology and mor- 
phology. Dept. of Linguistics, UCLA. At 
http: //www.humnet.ucla.edu/humnet/linguis- 
tics/people/hayes/learning/learner.pdf. 
Baayen, R. H., R. Piepenbrock, and H. van Rijn. 
1993. The CELEX lexical database (CD-ROM), 
Linguistic Data Consortium, University of Penn- 
sylvania, Philadelphia, PA. 
Berry, M., T. Do, G. O'Brien, V. Krishna, and 
S. Varadhan. 1993. SVDPACKC user's guide. CS- 
93-194, University of Tennessee. 
D~jean, H. 1998. Morphemes as necessary con- 
cepts for structures: Discovery from untagged 
corpora. University of Caen-Basse Normandie. 
http://www.info.unicaen.fr/~ DeJean/travail/ar - 
ticles/pgl 1.htm. 
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. Journal of the Ameri- 
can Society for Information Science. 
Gaussier, l~. 1999. Unsupervised learning of deriva- 
tional morphology from inflectional lexicons. A CL 
'99 Workshop Proceedings: Unsupervised Learn- 
ing in Natural Language Processing, University of 
Maryland. 
Goldsmith, J. 1997. Unsupervised learning of the 
morphology of a natural anguage. University of 
Chicago. 
Goldsmith, J. 2000. Unsupervised learning of 
the morphology of a natural language. Uni- 
versity of Chicago. http://humanities.uchi- 
cago.edu/faculty/goldsmith. 
Harris, Z. 1951. Structural Linguistics. University of 
Chicago Press. 
Hull, D. A. and G. Grefenstette. 1996. A de- 
tailed analysis of English stemming algorithms. 
XEROX Technical Report, http://www.xrce.xe- 
rox.com/publis/mltt/mltt-023.ps. 
Krovetz, R. 1993. Viewing morphology as an infer- 
ence process. Proceedings of the 16thA CM/SIGIR 
Conference, pp. 191-202. 
Jurafsky, D. S. and J. H. Martin. 2000. Speech and 
Language Processing. Prentice Hall, Inc., Engle- 
wood, N.J. 
Karlsson, F. and L. Karttunen,. 1997. "Sub- 
sentencial Processing." In Survey of the State of 
the Art in Human Language Technology, R. Cole, 
Ed., Giardini Editori e Stampatori, Italy. 
Koskenniemi, K. 1983. Two-level Morphology: a 
General Computational Model for Word-Form 
Recognition and Production. Ph.D. thesis, Univer- 
sity of Helsinki. 
Landauer,T. K., P. W. Foltz, and D. Laham. 1998. 
Introduction to Latent Semantic Analysis. Dis- 
course Processes. Vol. 25, pp. 259-284. 
Lovins, J. 1968. Development of a stemming al- 
gorithm. Mechanical Translation and Computa- 
tional Linguistics, Vol. 11, pp.22-31 
Manning, C. D. and H. Schfitze. 1999. Foundations 
of Statistical Natural Language Processing, MIT 
Press, Cambridge, MA. 
Porter, M. 1980. An algorithm for suffix stripping. 
Program, Vol. 14(3), pp.130-137. 
Ritchie, G. and G. J. Russell. 1992. Computational 
morphology: Practical Mechanisms for the En- 
glish Lexicon. MIT. 
Schfitze, H. 1993. Distributed syntactic representa- 
tions with an application to part-of-speech tag- 
ging. Proceedings of the IEEE International Con- 
ference on Neural Networks, pp. 1504-1509. 
Scott, D. 1992. Multivariate Density Estimation: 
Theory, Practice, and Visualization. John Wiley 
& Sons, New York. 
Sproat, R. 1992. Morphology and Computation. MIT 
Press, Cambridge, MA. 
Van den Bosch, A. and W. Daelemans. 1999. 
Memory-based morphological nalysis. Proc. of 
the 37th Annual Meeting of the ACL, University 
of Maryland, pp. 285-292. 
Voorhees, E., D. Hoffman, and C. Barnes. 1996-7. 
TREC Information Retrieval: Text Research Col- 
lection, Vols. 4-5 (CD-ROM), National Institute 
of Standards and Technology. 
Woods, W. 2000. Aggressive morphology for robust 
lexical coverage. Proceedings of the 6th ANLP/lst 
NAACL, Seattle, WA. 
72 
Is Knowledge-Free Induction of Multiword Unit 
Dictionary Headwords a Solved Problem?
Patrick Schone and Daniel Jurafsky
University of Colorado, Boulder CO 80309
{schone, jurafsky}@cs.colorado.edu
Abstract
We seek a knowledge-free method for inducing
multiword units from text corpora for use as
machine-readable dictionary headwords.  We
provide two major evaluations of nine existing
collocation-finders and  illustrate the continuing
need for improvement.  We use Latent Semantic
Analysis to make modest gains in performance, but
we show the significant challenges encountered  in
trying this approach.
1 Introduction
A multiword unit (MWU) is a connected
collocation: a sequence of neighboring words
?whose exact and unambiguous meaning or
connotation cannot be derived from the meaning or
connotation of its components? (Choueka, 1988).  In
other words, MWUs are typically non-compositional
at some linguistic level. For example, phonological
non-compositionality has been observed (Finke &
Weibel, 1997; Gregory, et al 1999) where words
like ?got? [g<t] and ?to? [tu] change phonetically to
?gotta? [g<rF] when combined.  We have interest in
inducing headwords for machine-readable
dictionaries (MRDs), so our interest is in semantic
rather than phonological non-compositionality.  As
an example of semantic non-compositionality,
consider ?compact disk?: one could not deduce that
it was a music medium by only considering the
semantics of ?compact? and ?disk.?
MWUs may also be non-substitutable and/or
non-modifiable (Manning and Sch?tze, 1999).  Non-
substitutability implies that substituting a word of
the MWU with its synonym should no longer
convey the same original content: ?compact disk?
does not readily imply ?densely-packed disk.? Non-
modifiability, on the other hand, suggests one
cannot modify the MWU?s structure and still convey
the same content: ?compact disk? does not signify
?disk that is compact.?
MWU dictionary headwords generally satisfy at
least one of these constraints. For example, a
compositional phrase would typically be excluded
from a hard-copy dictionary since its constituent
words would already be listed. These strategies
allow hard-copy dictionaries to remain compact.  
As mentioned, we wish to find MWU headwords
for machine-readable dictionaries (MRDs).
Although space is not an issue in MRDs, we desire
to follow the lexicographic practice of reducing
redundancy.  As Sproat indicated, "simply
expanding the dictionary to encompass every word
one is ever likely to encounter is wrong: it fails to
take advantage of regularities" (1992, p. xiii).  Our
goal is to identify an automatic, knowledge-free
algorithm that finds all and only those collocations
where it is necessary to supply a definition.
?Knowledge-free? means that the process should
proceed without human input (other than, perhaps,
indicating whitespace and punctuation).
This seems like a solved problem.  Many
collocation-finders exist, so one might suspect that
most could suffice for finding MWU dictionary
headwords.  To verify this, we evaluate nine
existing collocation-finders to see which best
identifies valid headwords.  We evaluate using two
completely separate gold standards: (1) WordNet
and (2) a compendium of Internet dictionaries.
Although web-based resources are dynamic and
have better coverage than WordNet (especially for
acronyms and names), we show that WordNet-based
scores are comparable to those using Internet
MRDs. Yet the evaluations indicate that significant
improvement is still needed in MWU-induction.
As an attempt to improve MWU headword
induction, we introduce several algorithms using
Latent Semantic Analysis (LSA). LSA is a
technique which automatically induces semantic
relationships between words.  We use LSA to try to
eliminate proposed MWUs which are semantically
compositional.  Unfortunately, this does not help.
Yet when we use LSA to identify substitutable delimiters.  This suggests that in a language with
MWUs, we do show modest performance gains. whitespace, one might prefer to begin at the word
2 Previous Approaches
For decades, researchers have explored various
techniques for identifying interesting collocations.
There have essentially been three separate kinds of
approaches for accomplishing this task.  These
approaches could be broadly classified into (1)
segmentation-based, (2) word-based and knowledge-
driven, or (3) word-based and probabilistic. We will
illustrate strategies that have been attempted in each
of the approaches. Since we assume knowledge of
whitespace, and since many of the first and all of the
second categories rely upon human input, we will be
most interested in the third category.
2.1 Segmentation-driven Strategies
Some researchers view MWU-finding as a natural
by-product of segmentation. One can regard text as
a stream of symbols and segmentation as a means of
placing delimiters in that stream so as to separate
logical groupings of symbols from one another.  A
segmentation process may find that a symbol stream
should not be delimited even though subcomponents
of the stream have been seen elsewhere.  In such
cases, these larger units may be MWUs.
The principal work on segmentation has focused
either on identifying words in phonetic streams
(Saffran, et. al, 1996; Brent, 1996; de Marcken,
1996) or on tokenizing Asian and Indian languages
that do not normally include word delimiters in their
orthography (Sproat, et al 1996; Ponte and Croft
1996; Shimohata, 1997; Teahan, et al, 2000; and
many others). Such efforts have employed various
strategies for segmentation, including the use of
hidden Markov models, minimum description
length, dictionary-based approaches, probabilistic
automata, transformation-based learning, and text
compression. Some of these approaches require
significant sources of human knowledge, though
others, especially those that follow data
compression or HMM schemes, do not.  
These approaches could be applied to languages
where word delimiters exist (such as in European
languages delimited by the space character).
However, in such languages, it seems more prudent
to simply take advantage of delimiters rather than
introducing potential errors by trying to find word
boundaries while ignoring knowledge of the
level and identify appropriate word combinations.
2.2 Word-based, knowledge-driven Strategies
Some researchers start with words and propose
MWU induction methods that make use of parts of
speech, lexicons, syntax or other linguistic structure
(Justeson and Katz, 1995; Jacquemin, et al, 1997;
Daille, 1996). For example, Justeson and Katz
indicated that the patterns NOUN  NOUN and ADJ
NOUN are very typical of MWUs.  Daille also
suggests that in French, technical MWUs follow
patterns such as ?NOUN de NOUN" (1996, p. 50).
To find word combinations that satisfy such patterns
in both of these situations necessitates the use of a
lexicon equipped with part of speech tags. Since we
are interested in knowledge-free induction of
MWUs, these approaches are less directly related to
our work. Furthermore, we are not really interested
in identifying constructs such as general noun
phrases as the above rules might generate, but
rather, in finding only those collocations that one
would typically need to define.  
2.3 Word-based, Probabilistic Approaches
The third category assumes at most whitespace
and punctuation knowledge and attempts to infer
MWUs using word combination probabilities.
Table 1 (see next page) shows nine commonly-used
probabilistic MWU-induction approaches.  In the
table,  f  and P  signify frequency and probabilityX X
of a word X. A variable XY indicates a word bigram
and   indicates its expected frequency at random.XY
An overbar signifies a variable?s complement. For
more details, one can consult the original sources as
well as Ferreira and Pereira (1999) and Manning
and Sch?tze (1999).
3 Lexical Access
Prior to applying the algorithms, we lemmatize
using a weakly-informed tokenizer that knows only
that whitespace and punctuation separate words.
Punctuation can either be discarded or treated as
words.  Since we are equally interested in finding
units like ?Dr.? and ?U. S.,? we opt to treat
punctuation as words.
Once we tokenize, we use Church?s (1995) suffix
array approach to identify word n-grams that occur
at least T times (for T=10).   We then rank-order the
PX|YMIXY
MZ PrZ|YMIZY
	2log
[PX PYPX PY]
fY
[PXYPXY]
fXY [PXYPXY]
fXY
M
i{X,X}
j{Y,Y}
(fij 	 ij )2
ij
fXY 	 XY
XY (1	(XY/N))
fXY 	 XY
fXY (1	(fXY/N))
Table 1: Probabilistic Approaches
METHOD FORMULA
Frequency
(Guiliano, 1964)
fXY
Pointwise Mutual
Information (MI)
(Fano, 1961;
Church and Hanks,
1990)
log  (P  / P P )2 XY X Y
Selectional
Association
(Resnik, 1996)
Symmetric
Conditional
Probability
(Ferreira and
Pereira, 1999)
P  / P PXY X Y2
Dice Formula
(Dice, 1945) 2 f / (f +f )XY X Y
Log-likelihood
(Dunning, 1993; (Daille, 1996). Since we need knowledge-poor
Daille, 1996) induction, we cannot use human-suggested filtering
Chi-squared ($ )2
(Church and Gale,
1991)
Z-Score
(Smadja, 1993;
Fontenelle, et al,
1994)
Student?s t-Score
(Church and
Hanks, 1990)
n-gram list in accordance to each probabilistic
algorithm.  This task is non-trivial since most
algorithms were originally suited for finding two-
word collocations.  We must therefore decide how
to expand the algorithms to identify general n-grams
(say, C=w w ...w ).  We can either generalize or1 2 n
approximate. Since generalizing requires
exponential compute time and memory for several
of the algorithms, approximation is an attractive
alternative. 
One approximation redefines X and Y to be,
respectively, the word sequences w w ...w  and1 2 i
w w ...w  where i is chosen to maximize P P .i+1 i+2 n, X Y
This has a natural interpretation of being the
expected probability of concatenating the two most
probable substrings in order to form the larger unit.
Since it can be computed rapidly with low memory
costs, we use this approximation.
Two additional issues need addressing before
evaluation.  The first regards document sourcing. If
an n-gram appears in multiple sources (eg.,
Congressional Record versus Associated Press),  its
likelihood of accuracy should increase. This is
particularly true if we are looking for MWU
headwords for a general versus specialized
dictionary.  Phrases that appear in one source may
in fact be general MWUs, but frequently, they are
text-specific units. Hence, precision gained by
excluding single-source n-grams may be worth
losses in recall.  We will measure this trade-off.
Second, evaluating with punctuation as words and
applying no filtering mechanism may unfairly bias
against some algorithms.  Pre- or post-processing of
n-grams with a linguistic filter has shown to
improve some induction algorithms? performance
rules as in Section 2.2.  Yet we can filter by pruning
n-grams whose beginning or ending word is among
the top N most frequent words.  This unfortunately
eliminates acronyms like ?U.  S.? and phrasal verbs
like ?throw up.? However, discarding some words
may be worthwhile if the final list of n-grams is
richer in terms of MRD headwords. We therefore
evaluate with such an automatic filter, arbitrarily
(and without optimization) choosing  N=75.
4 Evaluating Performance
A natural scoring standard is to select a language
and evaluate against headwords from existing
dictionaries in that language.  Others have used
similar standards (Daille, 1996), but to our
knowledge, none to the extent described here.  We
evaluate thousands of hypothesized units from an
unconstrained corpus. Furthermore, we use two
separate evaluation gold standards: (1) WordNet
(Miller, et al 1990) and (2) a collection of Internet
MRDs.  Using  two gold standards helps valid
MWUs.  It also provides evaluation using both static
and dynamic resources.  We choose to evaluate in
English  due  to the wealth of  linguistic resources.
Rank ZScore $2 SCP Dice MutualInfo.
Select
Assoc.
Log
Like. TScore Freq
1 Iwo
Jima
Buenos
Aires
Buenos
Aires
Buenos
Aires
Iwo
Jima
United
States
United
States
United
States
United
States
2
bona
fide
Iwo
Jima
Iwo
Jima
Iwo
Jima
bona
fide
House
of
Repre-
sentatives
Los
Angeles
Los
Angeles
Los
Angeles
4 Burkina
Faso Suu Kyi Suu Kyi Suu Kyi
Wounded 
Knee
Los
Angeles
New
York
New
York
New
York
8 Satanic
Verses Sault Ste Sault Ste Sault Ste
Hubble
Space
Telescope
my
colleagues
Soviet
Union
my
colleagues
my
colleagues
16 Ku 
Klux
Ku 
Klux
Ku 
Klux
Ku 
Klux
alma
mater
H . R SocialSecurity
High
School
High
School
32
Pledge of
Allegiance
Pledge of
Allegiance
Pledge of
Allegiance
Pledge of
Allegiance
Coca -
Cola War II
House of
Repre-
sentatives
Wednesday
* * * *
64 Telephone
& amp ;
Telegraph
Telephone
& amp ;
Telegraph
Telephone
& amp ;
Telegraph
Internal
Revenue
Planned
Parent-
hood
Prime
Minister * * *
real
estate
New
Jersey
128 Prime
Minister
Prime
Minister
Prime
Minister
Salman
Rushdie
Sault Ste
. Marie
both  
sides
At the 
same time
Wall
Street
term
care
256 Lehman
Hutton
Lehman
Hutton
Lehman
Hutton
tongue -
in -
cheek
o ? clock At the
same
del Mar all
over
grand
jury
512
La Habra La Habra La Habra
compens-
atory and
punitive
20th -
Century
Monday
night
days
later
80
percent
Great
Northern
1024 telephone
interview
telephone
interview
telephone
interview
Food and
Agriculture
Sheriff ?s
deputies
South
Dakota
County
Jail
where
you
300
million
  Table 2: Outputs from each algorithm at different sorted ranks
The ?* *? and ?* * *? are actual units.
In particular, we use a randomly-selected corpus the first five columns as ?information-like.?
consisting of a 6.7 million word subset of  the TREC Similarly, since the last four columns share
databases (DARPA, 1993-1997). properties of the frequency approach, we will refer
Table 2 illustrates a sample of rank-ordered output to them as ?frequency-like.? 
from each of the different algorithms (following the One?s application may dictate which set of
cross-source, filtered paradigm described in section algorithms to use.  Our gold standard selection
3).  Note that algorithms in the first four columns reflects our interest in general word dictionaries, so
produce results that are similar to each other as do results we obtain may differ from results we might
those in the last four columns. Although the mutual have obtained using terminology lexicons.
information results seem to be almost in a class of If our gold standard contains K MWUs with
their own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), our
first four sets of results; therefore, we will refer to figure of merit (FOM) is given by
1
K M
K
i
1 Pi ,
          little or even negative impact.  On the other hand,
where P  (precision at i) equals i/H , and H  is thei i i
number of hypothesized MWUs required to find the
i  correct MWU. This FOM corresponds to areath
under a precision-recall curve.
4.1 WordNet-based Evaluation
WordNet has definite advantages as an evaluation
resource. It has in excess of 50,000 MWUs, is freely
accessible, widely used, and is in electronic form.
Yet, it obviously cannot contain every MWU.  For
instance, our corpus contains 177,331 n-grams (for
2n10) satisfying T10, but WordNet contains
only 2610 of these. It is unclear, therefore, if
algorithms are wrong when they propose  MWUs
that are not in WordNet. We will assume they are
wrong but with a special caveat for proper nouns.
WordNet includes few proper noun MWUs.  Yet
several algorithms produce large numbers of proper
nouns. This biases against them.  One could contend
that all proper nouns MWUs are valid,  but we
disagree.  Although such may be MWUs, they are
not necessarily MRD headwords; one would not
include every proper noun  in a dictionary, but
rather, those needing definitions.  To overcome this,
we will have two scoring modes.  The first, ?S?
mode (standing for some) discards any proposed
capitalized n-gram whose uncapitalized version is
not in WordNet.  The second mode ?N? (for none)
disregards all capitalized n-grams. 
Table 3 illustrates algorithmic performance as
compared to the 2610 MWUs from WordNet.  The
first double column illustrates ?out-of-the-box?
performance on all 177,331 possible n-grams. The
second double column shows cross-sourcing: only
hypothesizing MWUs that appear in at least two
separate datasets  (124,952 in all), but being
evaluated against all of the 2610 valid units.  Double
columns 3 and 4 show effects from high-frequency
filtering the n-grams of the first and second columns
(reporting only 29,716 and 17,720 n-grams)
respectively.  
As Table 3 suggests, for every condition, the
information-like algorithms seem to perform best at
identifying valid, general MWU headwords.
Moreover, they are enhanced when cross-sourcing
is considered; but since much of their strength
comes from identifying proper nouns, filtering has
the frequency-like approaches are independent of
data source.  They also improve significantly with
filtering. Overall, though, after the algorithms are
judged, even the best score of 0.265 is far short of
the maximum possible, namely 1.0.
Table 3: WordNet-based scores
Prob (1) (2) (3) (4)
algo- WordNet WordNet WordNet WordNet
rithm cross- +Filter cross-
source source
+Filter
S N S N S N S N
Zscore .222 .146 .263 .193 .220 .129 .265 .173
SCP .221 .145 .262 .192 .220 .129 .265 .173
Chi-sqr .222 .146 .263 .193 .220 .129 .265 .173
Dice .242 .167 .265 .199 .230 .142 .256 .172
MI .191 .122 .245 .169 .185 .111 .233 .151
SA .057 .051 .058 .053 .182 .125 .202 .143
Loglike .049 .050 .068 .064 .118 .095 .177 .129
T-score .050 .051 .050 .052 .150 .109 .160 .118
Freq .035 .037 .034 .037 .144 .105 .152 .112
4.2   Web-based Evaluation
Since WordNet is static and cannot report on all of
a corpus? n-grams, one may expect different
performance by using a more all-encompassing,
dynamic resource. The Internet houses dynamic
resources which can judge practically every induced
n-gram.  With permission and sufficient time, one
can repeatedly query websites that host large
collections of MRDs and evaluate each n-gram. 
Having approval, we queried: (1) onelook.com,
(2) acronymfinder.com, and (3) infoplease.com. The
first website interfaces with over 600 electronic
dictionaries.  The second is devoted to identifying
proper acronyms.  The third focuses on world facts
such as historical figures and organization names. 
To minimize disruption to websites by reducing
the total number of queries needed for evaluation,
we use an evaluation approach from the information
retrieval community (Sparck-Jones and van
Rijsbergen, 1975). Each algorithm reports its top
5000 MWU choices and  the union of these choices
(45192 possible n-grams) is looked up on the
Internet.  Valid MWUs identified at any website are
assumed to be the only valid units in the data.
{Xi}ni
1 {Xi
}n
i
1
cos(X,Y) 
 X #Y||X|| ||Y|| .
Algorithms are then evaluated based on this showed how one could compute latent semantic
collection.  Although this strategy for evaluation is vectors for any word in a corpus (Schone and
not flawless, it is reasonable and makes dynamic Jurafsky, 2000).  Using the same approach, we
evaluation tractable. Table 4 shows the algorithms? compute semantic vectors for every proposed word
performance (including proper nouns). n-gram C=X X ...X   Since LSA involves word
Though Internet dictionaries and WordNet are counts, we can also compute semantic vectors
completely separate ?gold standards,? results are
surprisingly consistent.   One can conclude that
WordNet may safely be used as a gold standard in
future MWU headword evaluations. Also,
Table 4 Performance on Internet data
Prob (1) (2) (3) (4)
algorithm Internet Internet Internet Internet
cross- +Filter cross-
source source
+Filter
Z-Score .165 .260 .169 .269
SCP .166 .259 .170 .270
Chi-sqr .166 .260 .170 .270
Dice .183 .258 .187 .267
MI .139 .234 .140 .234
SA .027 .033 .107 .194
Log Like .023 .043 .087 .162
T-score .025 .027 .110 .142
Freq .016 .017 .104 .134
one can see that Z-scores, $ , and2
SCP have virtually identical results and seem to best
identify MWU headwords (particularly if proper
nouns are desired).  Yet there is still significant
room for improvement.
5 Improvement strategies
Can performance be improved?  Numerous
strategies could be explored. An idea we discuss
here tries using induced semantics to rescore the
output of the best algorithm (filtered, cross-sourced
Zscore) and eliminate semantically compositional or
modifiable MWU hypotheses.
Deerwester, et al(1990) introduced Latent
Semantic Analysis (LSA) as a computational
technique for inducing semantic relationships
between words and documents.  It forms high-
dimensional vectors using word counts and uses
singular value decomposition to project those
vectors into an optimal k-dimensional, ?semantic?
subspace (see Landauer, et al 1998).  
Following an approach from Sch?tze (1993), we
1 2 n.
(denoted by ) for C?s subcomponents.  These can
either  include  (           ) or exclude (             )  C?s
counts. We seek to see if induced semantics can
help eliminate incorrectly-chosen MWUs.  As will
be shown, the effort using semantics in this nature
has a very small payoff for the expended cost.
5.1    Non-compositionality
Non-compositionality is a key component of valid
MWUs, so we may desire to emphasize n-grams that
are semantically non-compositional.   Suppose we
wanted to determine if C (defined above) were non-
compositional.  Then given some meaning function,
, C should satisfy an equation like:  
g(  (C) , h( (X ),...,(X ) )  )0,           (1)1 n
where h combines the semantics of C?s
subcomponents and g measures semantic
differences.  If C were a bigram, then if g(a,b) is
defined to be |a-b|, if h(c,d) is the sum of c and d,
and if (e) is set to -log P , then equation (1) woulde
become the pointwise mutual information of the
bigram. If g(a,b) were defined to be (a-b)/b , and if?
h(a,b)=ab/N and  (X)=f  , we essentially get Z-X
scores.  These formulations suggest that several of
the probabilistic algorithms we have seen include
non-compositionality measures already.  However,
since the probabilistic algorithms rely only on
distributional information obtained by considering
juxtaposed words,  they tend to incorporate a
significant amount of non-semantic information
such as syntax. Can semantic-only rescoring help?
To find out, we must select g, h, and .  Since we
want to eliminate MWUs that are compositional, we
want h?s output to correlate well with C when there
is compositionality and correlate poorly otherwise.
Frequently, LSA vectors are correlated using the
cosine between them:
A large cosine indicates strong correlation, so large
values for g(a,b)=1-|cos(a,b)| should signal weak
correlation or non-compositionality. h could
Mn
i
1 wi ai
cos
cos(Xi,Y) 

min
k{Xi,Y}
cos(Xi ,Y)	?k
1k
.
represent a weighted vector sum of the components? required for this task.  This seems to be a significant
semantic vectors with weights (w ) set to either 1.0 component.  Yet there is still another: maybei
or the reciprocal of the words? frequencies. semantic compositionality is not always bad.
Table 5 indicates several results using these Interestingly, this is often the case.  Consider
settings.  As the first four rows indicate and as vice_president, organized crime, and
desired, non-compositionality is more apparent for Marine_Corps. Although these are MWUs, one
 * (i.e., the vectors derived from excluding C?sX
counts) than for  .  Yet, performance overall isX
horrible, particularly considering we are rescoring
Z-score output whose score was 0.269.  Rescoring
caused five-fold degradation!
Table 5: Equation 1 settings
g(a,b) h(a) (X) w Score oni
Internet
1-|cos(a,b)|
X 1 0.0517
1/fi 0.0473
 *X 1 0.0598
1/fi* 0.0523
|cos(a,b)|
X 1 0.174
1/fi 0.169
 *X 1 0.131
1/fi* 0.128
What happens if we instead emphasize
compositionality?  Rows 5-8 illustrate the effect:
there is a significant recovery in performance.  The
most reasonable explanation for this is that if
MWUs and their components are strongly
correlated, the components may rarely occur except
in context with the MWU.  It takes about 20 hours
to compute the  * for each possible n-gramX
combination. Since the probabilistic algorithms
already identify n-grams that share strong
distributional properties with their components, it
seems imprudent to exhaust resources on this LSA-
based strategy for non-compositionality.
These findings warrant some discussion.  Why did
non-compositionality fail?  Certainly there is the
possibility that better choices for g, h, and  could
yield improvements.  We actually spent months
trying to find an optimal combination as well as a
strategy for coupling LSA-based scores with the Z-
scores, but without avail. Another possibility:
although LSA can find semantic relationships, it
may not make semantic decisions at the level
would still expect that the first is related to
president, the second relates to crime, and the last
relates to Marine.  Similarly, tokens such as
Johns_Hopkins and Elvis are anaphors for
Johns_Hopkins_University and Elvis_Presley, so
they should have similar meanings.
This begs the question: can induced semantics
help at all?  The answer is ?yes.? The key is using
LSA where it does best: finding things that are
similar ? or substitutable. 
5.2 Non-substitutivity
For every collocation C=X X ..X X X ..X , we1 2 i-1 i+1 ni
attempt to find other similar patterns in the data,
X X ..X YX ..X .  If X  and Y are semantically1 2 i-1 i+1 n i
related, chances are that C is substitutable.
Since LSA excels at finding semantic correlations,
we can compare   and   to see if C isXi Y 
substitutable.  We use our earlier approach (Schone
and Jurafsky, 2000) for performing the comparison;
namely, for every word W, we compute cos(  )w, R
for 200 randomly chosen words, R. This allows for
computation of a correlaton mean (? ) and standardW
deviation (1 ) between W  and other words.   AsW  
before, we then compute a normalized cosine score
(      ) between words of interest, defined by
With this set-up, we now look for substitutivity.
Note that phrases may be substitutable and still be
headword if their substitute phrases are themselves
MWUs.  For example, dioxide in carbon_dioxide is
semantically similar to monoxide in
carbon_monoxide.   Moreover, there are other
important instances of valid substitutivity: 
& Abbreviations 
AlAlbert   <   Al_GoreAlbert_Gore
& Morphological similarities
RicoRican <  Puerto_RicoPuerto_Rican
& Taxonomic relationships 
bachelormaster<  
bachelor_?_s_degreemaster_?_s_degree. 
Figure 1: Precision-recall curve for rescoringHowever, guilty and innocent are semantically
related, but pleaded_guilty and pleaded_innocent
are not MWUs.  We would like to emphasize only n-
grams whose substitutes are valid MWUs.
To show how we do this using LSA, suppose we
want to rescore a list L whose entries are potential
MWUs.  For every entry X in L, we seek out all
other entries whose sorted order is less than some
maximum value (such as 5000) that have all but one
word in common.  For example, suppose X is
?bachelor_?_s_degree.?  The only other entry that
matches in all but one word is ?master_?_s_degree.?
If the semantic vectors for ?bachelor? and ?master?
have a normalized cosine score greater than a
threshold of 2.0, we then say that the two MWUs
are in each others substitution set.  To rescore, we
assign a new score to each entry in substitution set.
Each element in the substitution set gets the same
score.  The score is derived using a combination of
the previous Z-scores for each element in the
substitution set.  The combining function may be an
averaging, or a computation of the median, the
maximum, or something else.  The maximum
outperforms the average and the median on our data.
By applying in to our data, we observe a small but
visible improvement of 1.3% absolute to .282 (see
Fig. 1). It is also possible that other improvements
could be gained using other combining strategies.  
6 Conclusions
This paper identifies several new results in the area
of MWU-finding.  We saw that MWU headword
evaluations using WordNet provide similar results
to those obtained from far more extensive web-
based resources. Thus, one could safely use
WordNet as a gold standard for future evaluations.
We also noted that information-like algorithms,
particularly Z-scores, SCP, and $2, seem to perform
best at finding MRD headwords regardless of
filtering mechanism, but that improvements are still
needed. We proposed two new LSA-based
approaches which attempted to address issues of
non-compositionality and non-substitutivity.
Apparently,  either current algorithms already
capture much non-compositionality or LSA-based
models of non-compositionality are of little help.
LSA does help somewhat as a model of
substitutivity. However, LSA-based gains are small
compared to the effort required to obtain them.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments and insights.
References
AcronymFinder.com(2000-1). http://www.acronymfinder.
com.  Searches between March 2000 and April 2001.
Brent, M.R. and Cartwright, T.A. (1996). Distributional
regularity and phonotactic constraints are useful for
segmentation.  Cognition, 61, 93-125.
Choueka, Y. (1988).  Looking for needles in a haystack
or locating interesting collocation expressions in large
textual databases. Proceedings of the RIAO, pp. 38-43.
Church, K.W. (1995). N-grams.  Tutorial at ACL, ?95.
MIT, Cambridge, MA.
Church, K.W., & Gale, W.A. (1991). Concordances for
parallel text.  Proc. of the 7  Annual Conference of theth
UW Center for ITE New OED & Text Research, pp.
40-62, Oxford.
Church, K.W., & Hanks, P. (1990). Word association
norms, mutual information and lexicography.
Computational Linguistics, Vol. 16, No. 1, pp. 22-29.
Daille, B. (1996). ?Study and Implementation of
Combined Techniques from Automatic Extraction of
Terminology? Chap. 3 of "The Balancing Act":
Combining Symbolic and Statistical Approaches to
Language (Klavans, J., Resnik, P. (eds.)), pp. 49-66
DARPA (1993-1997). DARPA text collections: A.P.
Material, 1988-1990, Ziff Communications Corpus,
1989, Congressional Record of the 103  Congress,rd
and Los Angeles Times.
Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. (1990) Indexing by
Latent Semantic Analysis. Journal of the American
Society of Information Science, Vol. 41
de Marcken, C. (1996) Unsupervised Language
Acquisition, Ph.D., MIT Manning, C.D., Sch?tze, H. (1999) Foundations of
Dias, G., S. Guillor?, J.G. Pereira Lopes (1999). Statistical Natural Language Processing, MIT Press,
Language independent automatic acquisition of rigid Cambridge, MA, 1999.
multiword units from unrestricted text corpora. TALN, Mikheev, A., Finch, S. (1997). Collocation lattices and
Carg?se. maximum entropy models. WVLC, Hong Kong.
Dice, L.R. (1945). Measures of the amount of ecologic
 associations between species. Journal of Ecology, 26,
1945. 
Dunning, T (1993). Accurate methods for the statistics of
surprise and coincidence.  Computational Linguistics.
Vol. 19, No. 1.
Fano, R. (1961).  Transmission of Information.  MIT
Press,   Cambridge, MA.
Finke, M. and Weibel, A. (1997) Speaking mode
dependent pronunciation modeling in large vocabulary
conversational speech recognition.  Eurospeech-97.
Ferreira da Silva, J., Pereira Lopes, G. (1999). A local
maxima method and a fair dispersion normalization for
extracting multi-word units from corpora. Sixth
Meeting on Mathematics of Language, pp. 369-381.
Fontenelle, T., Br?ls, W., Thomas, L., Vanallemeersch,
T., Jansen, J. (1994). DECIDE, MLAP-Project 93-19,
deliverable D-1a: Survey of collocation extraction
tools.  Tech. Report, Univ. of Liege, Liege, Belgium.
Giuliano, V. E. (1964) "The interpretation of word
associations." In M.E. Stevens et al (Eds.) Statistical
association methods for mechanized documentation,
pp. 25-32. National Bureau of Standards
Miscellaneous Publication 269, Dec. 15, 1965.
Gregory, M. L., Raymond, W.D., Bell, A., Fosler-
Lussier, E., Jurafsky, D. (1999). The effects of
collocational strength and contextual predictability in
lexical production. CLS99, University of Chicago.
Heid, U. (1994). On ways words work together. Euralex-
99.
Hindle, D. (1990). Noun classification from predicate-
argument structures.  Proceedings of the Annual
Meeting of the ACL, pp. 268-275.
InfoPlease.com (2000-1). http://www.infoplease.com.
Searches between March 2000 and April 2001.
Jacquemin, C., Klavans, J.L., & Tzoukermann, E. (1997).
Expansion of multi-word terms for indexing and
retrieval using morphology and syntax.  Proc. of ACL
1997, Madrid, pp. 24-31.
Justeson, J.S. and S.M.Katz (1995).  Technical
terminology: some linguistic properties and an
algorithm for identification in text. Natural Language
Engineering 1:9-27.
Kilgariff, A., & Rose, T. (1998).  Metrics for corpus
similarity & homogeneity. Manuscript, ITRI,
University of Brighton.
Landauer, T.K., P.W. Foltz, and D. Laham. (1998)
Introduction to Latent Semantic Analysis. Discourse
Processes. Vol. 25, pp. 259-284.
Miller, G. (1990).?WordNet: An on-line lexical
database,? International Journal of Lexicography, 3(4).
OneLook.com (2000-1). http://www.onelook.com.
Searches between March 2000 and April 2001.
Ponte, J.M., Croft, B.W. (1996). Useg: A Retargetable
word segmentation procedure for information retrieval.
Symposium on Document Analysis and Information
Retrieval ?96.  Technical Report TR96-2, University of
Massachusetts.
Resnik, P. (1996). Selectional constraints: an
information-theoretic model and its computational
realization. Cognition. Vol. 61, pp. 127-159.
Saffran, J.R., Newport, E.L., and Aslin, R.N. (1996).
Word segmentation: the role of distributional cues.
Journal of Memory and Language, Vol. 25, pp. 606-
621.
Schone, P. and D. Jurafsky. (2000) Knowledge-free
induction of morphology using latent semantic
analysis.  Proc. of the Computational Natural
Language Learning Conference, Lisbon, pp. 67-72.
Sch?tze, H. (1993) Distributed syntactic representations
with an application to part-of-speech tagging.
Proceedings of the IEEE International Conference on
Neural Networks, pp. 1504-1509.
Shimohata, S., Sugio, T., Nagata, J. (1997). Retrieving
collocations by co-occurrences and word order
constraints.  Proceedings of the 35  Annual Mtg. of theth
Assoc. for Computational Linguistics.  Madrid.
Morgan-Kauffman Publishers, San Francisco.  Pp.
476-481.
Smadja, F. (1993).  Retrieving collocations from text:
Xtract.  Computational Linguistics, 19:143-177.
Sparck-Jones, K., C. van Rijsbergen (1975) Report on the
need for and provision of an ?ideal? information
retrieval text collection, British Library Research and
Development Report, 5266, Computer Laboratory,
University of Cambridge.
Sproat R, Shih, C. (1990) A statistical method for finding
word boundaries in Chinese text.  Computer
Processing of Chinese & Oriental Languages, Vol. 4,
No. 4.
Sproat, R. (1992) Morphology and Computation. MIT
Press, Cambridge, MA.
Sproat, R.W., Shih, C., Gale, W., Chang, N. (1996) A
stochastic finite-state word segmentation algorithm for
Chinese. Computational Linguistics, Vol. 22, #3.
Teahan, W.J., Yingyin, W. McNab, R, Witten, I.H.
(2000). A Compression-based algorithm for Chinese
word segmentation. ACL Vol. 26, No. 3, pp. 375-394.
	

		
	
Learning Named Entity Hyponyms for Question Answering
Paul McNamee
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
paul.mcnamee@jhuapl.edu
Rion Snow
Stanford AI Laboratory
Stanford University
Stanford, CA 94305, USA
rion@cs.stanford.edu
Patrick Schone
Department of Defense
Fort George G. Meade, MD 20755-6000
pjschon@tycho.ncsc.mil
James Mayfield
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723-6099, USA
james.mayfield@jhuapl.edu
Abstract
Lexical mismatch is a problem that con-
founds automatic question answering sys-
tems. While existing lexical ontologies such
as WordNet have been successfully used to
match verbal synonyms (e.g., beat and de-
feat) and common nouns (tennis is-a sport),
their coverage of proper nouns is less ex-
tensive. Question answering depends sub-
stantially on processing named entities, and
thus it would be of significant benefit if
lexical ontologies could be enhanced with
additional hypernymic (i.e., is-a) relations
that include proper nouns, such as Edward
Teach is-a pirate. We demonstrate how a re-
cently developed statistical approach to min-
ing such relations can be tailored to iden-
tify named entity hyponyms, and how as a
result, superior question answering perfor-
mance can be obtained. We ranked candi-
date hyponyms on 75 categories of named
entities and attained 53% mean average pre-
cision. On TREC QA data our method pro-
duces a 9% improvement in performance.
1 Introduction
To correctly extract answers, modern question an-
swering systems depend on matching words be-
tween questions and retrieved passages containing
answers. We are interested in learning hypernymic
(i.e., is-a) relations involving named entities because
we believe these can be exploited to improve a sig-
nificant class of questions.
For example, consider the following questions:
? What island produces Blue Mountain coffee?
? In which game show do participants compete
based on their knowledge of consumer prices?
? What villain is the nemesis of Dudley Do-
Right?
Knowledge that Jamaica is an island, that The Price
is Right is a game show, and that Snidely Whiplash
is a villain, is crucial to answering these questions.
Sometimes these relations are evident in the same
context as answers to questions, for example, in
?The island of Jamaica is the only producer of Blue
Mountain coffee?; however, ?Jamaica is the only
producer of Blue Mountain coffee? should be suf-
ficient, despite the fact that Jamaica is an island is
not observable from the sentence.
The dynamic nature of named entities (NEs)
makes it difficult to enumerate all of their evolv-
ing properties; thus manual creation and curation
of this information in a lexical resource such as
WordNet (Fellbaum, 1998) is problematic. Pasca
and Harabagiu discuss how insufficient coverage of
named entities impairs QA (2001). They write:
?Because WordNet was not designed
as an encyclopedia, the hyponyms of con-
cepts such as composer or poet are illus-
trations rather than an exhaustive list of
instances. For example, only twelve com-
poser names specialize the concept com-
poser ... Consequently, the enhancement
of WordNet with NE information could
help QA.?
799
The chief contribution of this study is demonstrat-
ing that an automatically mined knowledge base,
which naturally contains errors as well as correctly
distilled knowledge, can be used to improve QA per-
formance. In Section 2 we discuss prior work in
identifying hypernymic relations. We then explain
our methods for improved NE hyponym learning
and its evaluation (Section 3) and apply the relations
that are discovered to enhance question answering
(Section 4). Finally we discuss our results (Section
5) and present our conclusions (Section 6).
2 Hyponym Induction
We review several approaches to learning is-a rela-
tions.
2.1 Hearst Patterns
The seminal work in the field of hypernym learn-
ing was done by Hearst (1992). Her approach was
to identify discriminating lexico-syntactic patterns
that suggest hypernymic relations. For example, ?X,
such as Y?, as in ?elements, such as chlorine and
fluorine?.
2.2 KnowItAll
Etzioni et al developed a system, KnowItAll, that
does not require training examples and is broadly
applicable to a variety of classes (2005). Starting
with seed examples generated from high precision
generic patterns, the system identifies class-specific
lexical and part-of-speech patterns and builds a
Bayesian classifier for each category. KnowItAll
was used to learn hundreds of thousands of class
instances and clearly has potential for improving
QA; however, it would be difficult to reproduce the
approach because of information required for each
class (i.e., specifying synonyms such as town and
village for city) and because it relies on submitting a
large number of queries to a web search engine.
2.3 Query Logs
Pasca and Van Durme looked at learning entity class
membership for five high frequency classes (com-
pany, country, city, drug, and painter), using search
engine query logs (2007). They reported precision
at 50 instances between 0.50 and 0.82.
2.4 Dependency Patterns
Snow et al have described an approach with several
desirable properties: (1) it is weakly-supervised and
only requires examples of hypernym/hyponym rela-
tions and unannotated text; (2) the method is suit-
able for both common and rare categories; and, (3)
it achieves good performance without post filtering
using the Web (2005; 2006). Their method relies
on dependency parsing, a form of shallow parsing
where each word modifies a single parent word.
Hypernym/hyponym word pairs where the words1
belong to a single WordNet synset were identified
and served to generate training data in the follow-
ing way: making the assumption that when the two
words co-occur, evidence for the is-a relation is
present, sentences containing both terms were ex-
tracted from unlabeled text. The sentences were
parsed and paths between the nouns in the depen-
dency trees were calculated and used as features in a
supervised classifier for hypernymy.
3 Learning Named Entity Hyponyms
The present work follows the technique described
by Snow et al; however, we tailor the approach in
several ways. First, we replace the logistic regres-
sion model with a support vector machine (SVM-
Light). Second, we significantly increase the size
of training corpora to increase coverage. This ben-
eficially increases the density of training and test
vectors. Third, we include additional features not
based on dependency parses (e.g., morphology and
capitalization). Fourth, because we are specifically
interested in hypernymic relations involving named
entities, we use a bootstrapping phase where train-
ing data consisting primarily of common nouns are
used to make predictions and we then manually ex-
tract named entity hyponyms to augment the train-
ing data. A second learner is then trained using the
entity-enriched data.
3.1 Data
We rely on large amounts of text; in all our exper-
iments we worked with a corpus from the sources
given in Table 1. Sentences that presented difficul-
ties in parsing were removed and those remaining
1Throughout the paper, use of the term word is intended to
include named entities and other multiword expressions.
800
Table 1: Sources used for training and learning.
Size Sentences Genre
TREC Disks 4,5 81 MB 0.70 M Newswire
AQUAINT 1464 MB 12.17 M Newswire
Wikipedia (4/04) 357 MB 3.27 M Encyclopedia
Table 2: Characteristics of training sets.
Pos. Pairs Neg. Pairs Total Features
Baseline 7975 63093 162528
+NE 9331 63093 164298
+Feat 7975 63093 162804
were parsed with MINIPAR (Lin, 1998). We ex-
tracted 17.3 million noun pairs that co-occurred in
at least one sentence. All pairs were viewed as po-
tential hyper/hyponyms.
Our three experimental conditions are summa-
rized in Table 2. The baseline model used 71068
pairs as training data; it is comparable to the
weakly-supervised hypernym classifier of Snow et
al. (2005), which used only dependency parse fea-
tures, although here the corpus is larger. The entity-
enriched data extended the baseline training set by
adding positive examples. The +Feat model uses ad-
ditional features besides dependency paths.
3.2 Bootstrapping
Our synthetic data relies on hyper/hyponym pairs
drawn from WordNet, which is generally rich in
common nouns and lacking in proper nouns. But
certain lexical and syntactic features are more likely
to be predictive for NE hyponyms. For example, it
is uncommon to precede a named entity with an in-
definite article, and certain superlative adjectives are
more likely to be used to modify classes of entities
(e.g., ?the youngest coach?, ?the highest peak?). Ac-
cordingly we wanted to enrich our training data with
NE exemplars.
By manually reviewing highly ranked predictions
of the baseline system, we identified 1356 additional
pairs to augment the training data. This annotation
took about a person-day. We then rescanned the cor-
pus to build training vectors for these co-occurring
nouns to produce the +NE model vectors.
Table 3: Features considered for +Feat model.
Feature Comment
Hypernym con-
tained in hyponym
Sands Hotel is-a hotel
Length in chars /
words
Chars: 1-4, 5-8, 9-16, 17+
Words: 1, 2, 3, 4, 5, 6, 7+
Has preposition Treaty of Paris; Statue of Liberty
Common suffixes -ation, -ment, -ology, etc...
Figurative term Such as goal, basis, or problem
Abstract category Like person, location, amount
Contains digits Usually not a good hyponym
Day of week;
month of year
Indiscriminately co-occurs with
many nouns.
Presence and depth
in WordNet graph
Shallow hypernyms are unlikely to
have entity hyponyms. Presence in
WN suggests word is not an entity.
Lexname of 1st
synset in WordNet
Root classes like person, location,
quantity, and process.
Capitalization Helps identify entities.
Binned document
frequency
Partitioned by base 10 logs
3.3 Additional Features
The +Feat model incorporated an additional 276 bi-
nary features which are listed in Table 3. We consid-
ered other features such as the frequency of patterns
on the Web, but with over 17 million noun pairs this
was computationally infeasible.
3.4 Evaluation
To compare our different models we created a test
set of 75 categories. The classes are diverse and
include personal, corporate, geographic, political,
artistic, abstract, and consumer product entities.
From the top 100 responses of the different learn-
ers, a pool of candidate hyponyms was created, ran-
domly reordered, and judged by one of the authors.
To assess the quality of purported hyponyms we
used average precision, a measure in ranked infor-
mation retrieval evaluation, which combines preci-
sion and recall.
Table 4 gives average precision values for the
three models on 15 classes of mixed difficulty2. Per-
formance varies considerably based on the hyper-
nym category, and for a given category, by classifier.
N is the number of known correct instances found in
the pool that belong to a given category.
Aggregate performance, as mean average preci-
sion, was computed over all 75 categories and is
2These are not the highest performing classes
801
Table 4: Average precision on 15 categories.
N Baseline +NE +Feat
chemical element 78 0.9096 0.9781 0.8057
african country 48 0.8581 0.8521 0.4294
prep school 26 0.6990 0.7098 0.7924
oil company 132 0.6406 0.6342 0.7808
boxer 109 0.6249 0.6487 0.6773
sculptor 95 0.6108 0.6375 0.8634
cartoonist 58 0.5988 0.6109 0.7097
volcano 119 0.5687 0.5516 0.7722
horse race 23 0.4837 0.4962 0.7322
musical 80 0.4827 0.4270 0.3690
astronaut 114 0.4723 0.5912 0.5738
word processor 26 0.4437 0.4426 0.6207
chief justice 115 0.4029 0.4630 0.5955
perfume 43 0.2482 0.2400 0.5231
pirate 10 0.1885 0.3070 0.2282
Table 5: Mean average precision over 75 categories.
Baseline +NE +Feat
MAP 0.4801 0.5001 (+4.2%) 0.5320 (+10.8%)
given in Table 5. Both the +NE and +Feat models
yielded improvements that were statistically signif-
icant at a 99% confidence level. The +Feat model
gained 11% over the baseline condition. The maxi-
mum F-score for +Feat is 0.55 at 70% recall.
Mean average precision emphasizes precision at
low ranks, so to capture the error characteristics at
multiple operating points we present a precision-
recall graph in Figure 1. The +NE and +Feat models
both attain superior performance at all but the lowest
recall levels. For question answering this is impor-
tant because it is not known which entities will be
the focus of a question, so the ability to deeply mine
various entity classes is important.
Table 6 lists top responses for four categories.
3.5 Discussion
53% mean average precision seems good, but is it
good enough? For automated taxonomy construc-
tion precision of extracted hyponyms is critically
important; however, because we want to improve
question answering we prefer high recall and can
tolerate some mistakes. This is because only a small
set of passages that are likely to contain an answer
are examined in detail, and only from this subset
of passages do we need to reason about potential
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Level
P
r
e
c
i
s
i
o
n
Feat
Ent
Baseline
Figure 1: Precision-recall graph for three classifiers.
hyponyms. In the next section we describe an ex-
periment which confirms that our learned entity hy-
ponyms are beneficial.
4 QA Experiments
4.1 QACTIS
To evaluate the usefulness of our learned NE hy-
ponyms for question answering, we used the QAC-
TIS system (Schone et al, 2005). QACTIS was
fielded at the 2004-2006 TREC QA evaluations and
placed fifth at the 2005 workshop. We worked with
a version of the software from July 2005.
QACTIS uses WordNet to improve matching of
question and document words, and a resource, the
Semantic Forest Dictionary (SFD), which contains
many hypernym/hyponym pairs. The SFD was pop-
ulated through both automatic and manual means
(Schone et al, 2005), and was updated based on
questions asked in TREC evaluations through 2004.
4.2 Experimental Setup
We used factoid questions from the TREC 2005-
2006 QA evaluations (Voorhees and Dang, 2005)
and measured performance with mean reciprocal
rank (MRR) and percent correct at rank 1.
All runs made use of WordNet 2.0, and we ex-
amined several other sources of hypernym knowl-
802
Table 6: Top responses for four categories using the +Feat model. Starred entries were judged incorrect.
Sculptor Horse Race Astronaut Perfume
1 Evelyn Beatrice Longman Tevis Cup Mark L Polansky * Avishag
2 Nancy Schon Kenilworth Park Gold Cup Richard O Covey Ptisenbon
3 Phidias Cox Plate George D Nelson Poeme
4 Stanley Brandon Kearl Grosser Bugatti Preis Guion Bluford Jr Parfums International
5 Andy Galsworthy Melbourne Cup Stephen S Oswald Topper Schroeder
6 Alexander Collin * Great Budda Hall Eileen Collins * Baccarin
7 Rachel Feinstein Travers Stakes Leopold Eyharts Pink Lady
8 Zurab K Tsereteli English Derby Daniel M Tani Blue Waltz
9 Bertel Thorvaldsen * Contrade Ronald Grabe WCW Nitro
10 Cildo Meireles Palio * Frank Poole Jicky
Table 7: Additional knowledge sources by size.
Classes Class Instances
Baseline 76 11,066
SFD 1,140 75,647
SWN 7,327 458,370
+Feat 44,703 1,868,393
edge. The baseline condition added a small subset
of the Semantic Forest Dictionary consisting of 76
classes seen in earlier TREC test sets (e.g., nation-
alities, occupations, presidents). We also tested: (1)
the full SFD; (2) a database from the Stanford Word-
net (SWN) project (Snow et al, 2006); and, (3) the
+Feat model discussed in Section 3. The number of
classes and entries of each is given in Table 7.
4.3 Results
We observed that each source of knowledge benefit-
ted questions that were incorrectly answered in the
baseline condition. Examples include learning a me-
teorite (Q84.1), a university (Q93.3), a chief oper-
ating officer (Q108.3), a political party (Q183.3), a
pyramid (Q186.4), and a movie (Q211.5).
In Table 8 we compare performance on questions
from the 2005 and 2006 test sets. We assessed
performance primarily on test questions that were
deemed likely to benefit from hyponym knowledge
? questions that had a readily discernible category
(e.g., ?What film ...?, ?In what country ...?) ? but we
also give results on the entire test set.
The WordNet-only run suffers a large decrease
compared to the baseline. This is expected because
WordNet lacks coverage of entities and the baseline
condition specifically populates common categories
of entities that have been observed in prior TREC
evaluations. Nonetheless, WordNet is useful to the
system because it addresses lexical mismatch that
does not involve entities.
The full SFD, the SWN, and the +Feat model
achieved 17%, 2%, and 9% improvements in answer
correctness, respectively. While no model had ex-
posure to the 2005-2006 TREC questions, the SFD
database was manually updated based on training
on the TREC-8 through TREC-2004 data sets. It
approximates an upper bound on gains attributable
to addition of hyponym knowledge: it has an un-
fair advantage over the other models because recent
question sets use similar categories to those in ear-
lier TRECs. Our +Feat model, which has no bias
towards TREC questions, realizes larger gains than
the SWN. This is probably at least in part because it
produced a more diverse set of classes and a signif-
icantly larger number of class instances. Compared
to the baseline condition the +Feat model sees a 7%
improvement in mean reciprocal rank and a 9% im-
provement in correct first answers; both results rep-
resent a doubling of performance compared to the
use of WordNet alne. We believe that these results
illustrate clear improvement attributable to automat-
ically learned hyponyms.
The rightmost columns in Table 8 reveal that the
magnitude of improvements, when measured over
all questions, is less. But the drop off is consistent
with the fact that only one third of questions have
clear need for entity knowledge.
5 Discussion
Although there is a significant body of work in auto-
mated ontology construction, few researchers have
examined the relationship between their methods
803
Table 8: QA Performance on TREC 2005 & 2006 Data
Hyponym-Relevant Subset (242) All Questions (734)
MRR % Correct MRR % Correct
WN-alone 0.189 (-45.6%) 12.8 (-51.6%) 0.243 (-29.0%) 18.26 (-30.9%)
Baseline 0.348 26.4 0.342 26.4
SFD 0.405 (+16.5%) 31.0 (+17.2%) 0.362 (+5.6%) 27.9 (+5.7%)
SWN 0.351 (+1.0%) 26.9 (+1.6%) 0.343 (+0.3%) 26.6 (+0.5%)
Feat 0.373 (+7.4%) 28.9 (+9.4%) 0.351 (+2.5%) 27.3 (+3.1%)
for knowledge discovery and improved question-
answering performance. One notable study was con-
ducted by Mann (2002). Our work differs in two
ways: (1) his method for identifying hyponyms was
based on a single syntactic pattern, and (2) he looked
at a comparatively simple task ? given a question
and one answer sentence containing the answer, ex-
tract the correct named entity answer.
Other attempts to deal with lexical mismatch in
automated QA include rescoring based on syntactic
variation (Cui et al, 2005) and identification of ver-
bal paraphrases (Lin and Pantel, 2001).
The main contribution of this paper is showing
that large-scale, weakly-supervised hyponym learn-
ing is capable of producing improvements in an end-
to-end QA system. In contrast, previous studies have
generally presented algorithmic advances and show-
cased sample results, but failed to demonstrate gains
in a realistic application. While the hypothesis that
discovering is-a relations for entities would improve
factoid QA is intuitive, we believe these experiments
are important because they show that automatically
distilled knowledge, even when containing errors
that would not be introduced by human ontologists,
is effective in question answering systems.
6 Conclusion
We have shown that highly accurate statistical learn-
ing of named entity hyponyms is feasible and that
bootstrapping and feature augmentation can signif-
icantly improve classifier accuracy. Mean aver-
age precision of 53% was attained on a set of 75
categories that included many fine-grained entity
classes. We also demonstrated that mining knowl-
edge about entities can be directly applied to ques-
tion answering, and we measured the benefit on
TREC QA data. On a subset of questions for
which NE hyponyms are likely to help we found that
learned hyponyms generated a 9% improvement in
performance compared to a strong baseline.
References
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng
Chua. 2005. Question answering passage retrieval using
dependency relations. In SIGIR 2005, pages 400?407.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana M.
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised Named-Entity
Extraction from the Web: An Experimental Study. Artificial
Intelligence, 165(1):191?134.
Christine Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In ACL 1992, pages 539?545.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference
rules for question-answering. Natural Language Engineer-
ing, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of minipar.
In Workshop on the Evaluation of Parsing Systems.
Gideon S. Mann. 2002. Fine-grained proper noun ontolo-
gies for question answering. In COLING-02 on SEMANET,
pages 1?7.
Marius Pasca and Benjamin Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from query
logs. In IJCAI-07, pages 2832?2837.
Marius Pasca and Sanda M. Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on WordNet and
Other Lexical Resources.
Patrick Schone, Gary Ciany, Paul McNamee, James Mayfield,
and Thomas Smith. 2005. QACTIS-based Question An-
swering at TREC 2005. In Proceedings of the 14th Text RE-
trieval Conference.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learn-
ing syntactic patterns for automatic hypernym discovery. In
NIPS 17.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In ACL
2006, pages 801?808.
804
Proceedings of ACL-08: HLT, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Wiki Resources for Multilingual Named Entity Recognition 
 
Alexander E. Richman Patrick Schone 
Department of Defense Department of Defense 
Washington, DC 20310 Fort George G. Meade, MD 20755 
arichman@psualum.com pjschon@tycho.ncsc.mil 
 
 
 
 
Abstract 
In this paper, we describe a system by which 
the multilingual characteristics of Wikipedia 
can be utilized to annotate a large corpus of 
text with Named Entity Recognition (NER) 
tags requiring minimal human intervention 
and no linguistic expertise.  This process, 
though of value in languages for which 
resources exist, is particularly useful for less 
commonly taught languages.  We show how 
the Wikipedia format can be used to identify 
possible named entities and discuss in detail 
the process by which we use the Category 
structure inherent to Wikipedia to determine 
the named entity type of a proposed entity.  
We further describe the methods by which 
English language data can be used to 
bootstrap the NER process in other languages.  
We demonstrate the system by using the 
generated corpus as training sets for a variant 
of BBN's Identifinder in French, Ukrainian, 
Spanish, Polish, Russian, and Portuguese, 
achieving overall F-scores as high as 84.7% 
on independent, human-annotated corpora, 
comparable to a system trained on up to 
40,000 words of human-annotated newswire. 
1 Introduction 
Named Entity Recognition (NER) has long been a 
major task of natural language processing.  Most of 
the research in the field has been restricted to a few 
languages and almost all methods require substan-
tial linguistic expertise, whether creating a rule-
based technique specific to a language or manually 
annotating a body of text to be used as a training 
set for a statistical engine or machine learning. 
 In this paper, we focus on using the multilingual 
Wikipedia (wikipedia.org) to automatically create 
an annotated corpus of text in any given language, 
with no linguistic expertise required on the part of 
the user at run-time (and only English knowledge 
required during development).  The expectation is 
that for any language in which Wikipedia is 
sufficiently well-developed, a usable set of training 
data can be obtained with minimal human 
intervention.  As Wikipedia is constantly 
expanding, it follows that the derived models are 
continually improved and that increasingly many 
languages can be usefully modeled by this method.   
 In order to make sure that the process is as 
language-independent as possible, we declined to 
make use of any non-English linguistic resources 
outside of the Wikimedia domain (specifically, 
Wikipedia and the English language Wiktionary  
(en.wiktionary.org)).  In particular, we did not use 
any semantic resources such as WordNet or part of 
speech taggers.  We used our automatically anno-
tated corpus along with an internally modified 
variant of BBN's IdentiFinder (Bikel et al, 1999), 
specifically modified to emphasize fast text 
processing,  called ?PhoenixIDF,? to create several 
language models that could be tested outside of the 
Wikipedia framework.  We built on top of an 
existing system, and left existing lists and tables 
intact.  Depending on language, we evaluated our 
derived models against human or machine 
annotated data sets to test the system. 
2 Wikipedia 
2.1  Structure 
Wikipedia is a multilingual, collaborative encyclo-
pedia on the Web which is freely available for re-
search purposes.  As of October 2007, there were 
over 2 million articles in English, with versions 
available in 250 languages. This includes 30 lan-
guages with at least 50,000 articles and another 40 
with at least 10,000 articles.  Each language is 
available for download (download.wikimedia.org) 
in a text format suitable for inclusion in a database.  
For the remainder of this paper, we refer to this 
format. 
1
 Within Wikipedia, we take advantage of five 
major features: 
? Article links, links from one article to another 
of the same language; 
? Category links, links from an article to special 
?Category? pages; 
? Interwiki links, links from an article to a 
presumably equivalent, article in another 
language; 
? Redirect pages, short pages which often 
provide equivalent names for an entity; and 
? Disambiguation pages, a page with little 
content that links to multiple similarly named 
articles. 
The first three types are collectively referred to as 
wikilinks. 
 A typical sentence in the database format looks 
like the following: 
 
?Nescopeck Creek is a [[tributary]] of the [[North 
Branch Susquehanna River]] in [[Luzerne County, 
Pennsylvania|Luzerne County]].? 
 
The double bracket is used to signify wikilinks.  In 
this snippet, there are three articles links to English 
language Wikipedia pages, titled ?Tributary,? 
?North Branch Susquehanna River,? and ?Luzerne 
County, Pennsylvania.?  Notice that in the last link, 
the phrase preceding the vertical bar is the name of 
the article, while the following phrase is what is 
actually displayed to a visitor of the webpage. 
 Near the end of the same article, we find the 
following representations of Category links: 
[[Category:Luzerne County, Pennsylvania]], 
[[Category:Rivers of Pennsylvania]], {{Pennsyl-
vania-geo-stub}}.  The first two are direct links to 
Category pages.  The third is a link to a Template, 
which (among other things) links the article to 
?Category:Pennsylvania geography stubs?.  We 
will typically say that a given entity belongs to 
those categories to which it is linked in these ways. 
 The last major type of wikilink is the link be-
tween different languages.  For example, in the 
Turkish language article ?Kanuni Sultan S?ley-
man? one finds a set of links including [[en:Sulei-
man the Magnificent]] and [[ru:???????? I]].  
These represent links to the English language 
article ?Suleiman the Magnificent? and the Russian 
language article ????????? I.?  In almost all 
cases, the articles linked in this manner represent 
articles on the same subject. 
 A redirect page is a short entry whose sole pur-
pose is to direct a query to the proper page.  There 
are a few reasons that redirect pages exist, but the 
primary purpose is exemplified by the fact that 
?USA? is an entry which redirects the user to the 
page entitled ?United States.?   That is, in the vast 
majority of cases, redirect pages provide another 
name for an entity. 
 A disambiguation page is a special article 
which contains little content but typically lists a 
number of entries which might be what the user 
was seeking.  For instance, the page ?Franklin? 
contains 70 links, including the singer ?Aretha 
Franklin,? the town ?Franklin, Virginia,? the 
?Franklin River? in Tasmania, and the cartoon 
character ?Franklin (Peanuts).?  Most disambigua-
tion pages are in Category:Disambiguation or one 
of its subcategories. 
2.2 Related Studies 
Wikipedia has been the subject of a considerable 
amount of research in recent years including 
Gabrilovich and Markovitch (2007), Strube and 
Ponzetto (2006), Milne et al (2006), Zesch et al 
(2007), and Weale (2007).  The most relevant to 
our work are Kazama and Torisawa (2007), Toral 
and Mu?oz (2006), and Cucerzan (2007).  More 
details follow, but it is worth noting that all known 
prior results are fundamentally monolingual, often 
developing algorithms that can be adapted to other 
languages pending availability of the appropriate 
semantic resource.  In this paper, we emphasize the 
use of links between articles of different languages, 
specifically between English (the largest and best 
linked Wikipedia) and other languages. 
 Toral and Mu?oz (2006) used Wikipedia to cre-
ate lists of named entities.  They used the first 
sentence of Wikipedia articles as likely definitions 
of the article titles, and used them to attempt to 
classify the titles as people, locations, organiza-
tions, or none.  Unlike the method presented in this 
paper, their algorithm relied on WordNet (or an 
equivalent resource in another language).  The au-
thors noted that their results would need to pass a 
manual supervision step before being useful for the 
NER task, and thus did not evaluate their results in 
the context of a full NER system. 
 Similarly, Kazama and Torisawa (2007) used 
Wikipedia, particularly the first sentence of each 
article, to create lists of entities.  Rather than 
building entity dictionaries associating words and 
2
phrases to the classical NER tags (PERSON, LO-
CATION, etc.) they used a noun phrase following 
forms of the verb ?to be? to derive a label.  For ex-
ample, they used the sentence ?Franz Fischler ... is 
an Austrian politician? to associate the label ?poli-
tician? to the surface form ?Franz Fischler.?   They 
proceeded to show that the dictionaries generated 
by their method are useful when integrated into an 
NER system.  We note that their technique relies 
upon a part of speech tagger, and thus was not ap-
propriate for inclusion as part of our non-English 
system.  
Cucerzan (2007), by contrast to the above, 
used Wikipedia primarily for Named Entity Dis-
ambiguation, following the path of Bunescu and 
Pa?ca (2006).  As in this paper, and unlike the 
above mentioned works, Cucerzan made use of the 
explicit Category information found within Wiki-
pedia.  In particular, Category and related list-
derived data were key pieces of information used 
to differentiate between various meanings of an 
ambiguous surface form.  Unlike in this paper, 
Cucerzan did not make use of the Category infor-
mation to identify a given entity as a member of 
any particular class.  We also note that the NER 
component was not the focus of the research, and 
was specific to the English language. 
 
3 Training Data Generation 
3.1   Initial Set-up and Overview 
Our approach to multilingual NER is to pull back 
the decision-making process to English whenever 
possible, so that we could apply some level of lin-
guistic expertise.  In particular, by focusing on 
only one language, we could take maximum ad-
vantage of the Category structure, something very 
difficult to do in the general multilingual case. 
 For computational feasibility, we downloaded 
various language Wikipedias and the English lan-
guage Wiktionary   in their text (.xml) format and 
stored each language as a table within a single 
MySQL database.  We only stored the title, id 
number, and body (the portion between the 
<TEXT> and </TEXT> tags) of each article.   
 We elected to use the ACE Named Entity types 
PERSON, GPE (Geo-Political Entities), OR-
GANIZATION, VEHICLE, WEAPON, LOCA-
TION, FACILITY, DATE, TIME, MONEY, and 
PERCENT.  Of course, if some of these types were 
not marked in an existing corpus or not needed for 
a given purpose, the system can easily be adapted. 
  Our goal was to automatically annotate the text 
portion of a large number of non-English articles 
with tags like <ENAMEX TYPE=?GPE?>Place 
Name</ENAMEX> as used in MUC (Message 
Understanding Conference).  In order to do so, our 
system first identifies words and phrases within the 
text that might represent entities, primarily through 
the use of wikilinks.  The system then uses catego-
ry links and/or interwiki links to associate that 
phrase with an English language phrase or set of 
Categories.  Finally, it determines the appropriate 
type of the English language data and assumes that 
the original phrase is of the same type. 
 In practice, the English language categorization 
should be treated as one-time work, since it is 
identical regardless of the language model being 
built.  It is also the only stage of development at 
which we apply substantial linguistic knowledge, 
even of English.   
 In the sections that follow, we begin by show-
ing how the English language categorization is 
done.  We go on to describe how individual non-
English phrases are associated with English lan-
guage information.  Next, we explain how possible 
entities are initially selected.  Finally, we discuss 
some optional steps as well as how and why they 
could be used. 
3.2   English Language Categorization  
For each article title of interest (specifically ex-
cluding Template pages, Wikipedia admistrative 
pages, and articles whose title begins with ?List 
of?), we extracted the categories to which that en-
try was assigned.  Certainly, some of these cate-
gory assignments are much more useful than others 
 For instance, we would expect that any entry in 
?Category:Living People? or ?Category:British 
Lawyers? will refer to a person while any entry in 
?Category:Cities in Norway? will refer to a GPE.  
On the other hand, some are entirely unhelpful, 
such as ?Category:1912 Establishments? which 
includes articles on Fenway Park (a facility), the 
Republic of China (a GPE), and the Better 
Business Bureau (an organization). Other catego-
ries can reliably be used to determine that the 
article does not refer to a named entity, such as 
?Category:Endangered species.?  We manually 
derived a relatively small set of key phrases, the 
most important of which are shown in Table 1. 
3
Table 1: Some Useful Key Category Phrases 
 
PERSON ?People by?, ?People in?, ?People from?, 
?Living people?, ?births?,  ?deaths?,  ?by 
occupation?, ?Surname?, ?Given names?, 
?Biography stub?, ?human names? 
ORG ?Companies?, ?Teams?, ?Organizations?, 
?Businesses?, ?Media by?, ?Political 
parties?, ?Clubs?, ?Advocacy groups?, 
?Unions?, ?Corporations?, ?Newspapers?, 
?Agencies?, ?Colleges?, ?Universities? ,  
?Legislatures?, ?Company stub?, ?Team 
stub?, ?University stub?, ?Club stub? 
GPE ?Cities?, ?Countries?, ?Territories?, 
?Counties?, ?Villages?, ?Municipalities?, 
?States? (not part of ?United States?), 
?Republics?, ?Regions?, ?Settlements? 
DATE ?Days?, ?Months?, ?Years?, ?Centuries? 
NONE ?Lists?, ?List of?, ?Wars?, ?Incidents? 
 
For each article, we searched the category 
hierarchy until a threshold of reliability was passed 
or we had reached a preset limit on how far we 
would search.   
 For example, when the system tries to classify 
?Jacqueline Bhabha,? it extracts the categories 
?British Lawyers,? ?Jewish American Writers,? 
and ?Indian Jews.?  Though easily identifiable to a 
human, none of these matched any of our key 
phrases, so the system proceeded to extract the 
second order categories ?Lawyers by nationality,? 
?British legal professionals,? ?American writers by 
ethnicity,?  ?Jewish writers,? ?Indian people by 
religion,? and ?Indian people by ethnic or national 
origin? among others.  ?People by? is on our key 
phrase list, and the two occurrences passed our 
threshold, and she was then correctly identified. 
 If an article is not classified by this method, we 
check whether it is a disambiguation page (which 
often are members solely of ?Category:Disam-
biguation?).  If it is, the links within are checked to 
see whether there is a dominant type.  For instance, 
the page ?Amanda Foreman? is a disambiguation 
page, with each link on the page leading to an 
easily classifiable article. 
 Finally, we use Wiktionary, an online colla-
borative dictionary, to eliminate some common 
nouns.  For example, ?Tributary? is an entry in 
Wikipedia which would be classified as a Location 
if viewed solely by Category structure.  However, 
it is found as a common noun in Wiktionary, over-
ruling the category based result. 
3.3 Multilingual Categorization  
When attempting to categorize a non-English term 
that has an entry in its language?s Wikipedia, we 
use two techniques to make a decision based on 
English language information.  First, whenever 
possible, we find the title of an associated English 
language article by searching for a wikilink 
beginning with ?en:?.  If such a title is found, then 
we categorize the English article as shown in 
Section 3.2, and decide that the non-English title is 
of the same type as its English counterpart.  We 
note that links to/from English are the most 
common interlingual wikilinks. 
 Of course, not all articles worldwide have Eng-
lish equivalents (or are linked to such even if they 
do exist). In this case, we attempt to make a deci-
sion based on Category information, associating 
the categories with their English equivalents, when 
possible.  Fortunately, many of the most useful 
categories have equivalents in many languages. 
 For example, the Breton town of Erquy has a 
substantial article in the French language Wikipe-
dia, but no article in English.  The system proceeds 
by determining that Erquy belongs to four French 
language categories:  ?Cat?gorie:Commune des 
C?tes-d'Armor,? ?Cat?gorie:Ville portuaire de 
France,? ?Cat?gorie:Port de plaisance,? and 
?Cat?gorie:Station baln?aire.?  The system pro-
ceeds to associate these, respectively, with ?Cate-
gory:Communes of C?tes-d'Armor,? UNKNOWN, 
?Category:Marinas,? and ?Category:Seaside re-
sorts? by looking in the French language pages of 
each for wikilinks of the form [[en:...]].   
 The first is a subcategory of ?Category:Cities, 
towns and villages in France? and is thus easily 
identified by the system as a category consisting of 
entities of type GPE. The other two are ambiguous 
categories (facility and organization elements in 
addition to GPE).   Erquy is then determined to be 
a GPE by majority vote of useful categories.     
 We note that the second French category actu-
ally has a perfectly good English equivalent (Cate-
gory:Port cities and towns in France), but no one 
has linked them as of this writing.   We also note 
that the ambiguous categories are much more 
GPE-oriented in French.  The system still makes 
the correct decision despite these factors. 
 We do not go beyond the first level categories 
or do any disambiguation in the non-English case.  
Both are avenues for future improvement. 
4
3.4 The Full System  
To generate a set of training data in a given lan-
guage, we select a large number of articles from its 
Wikipedia (50,000 or more is recommended, when 
possible).  We prepare the text by removing exter-
nal links, links to images, category and interlingual 
links, as well as some formatting.  The main pro-
cessing of each article takes place in several stages, 
whose primary purposes are as follows: 
? The first pass uses the explicit article links 
within the text. 
? We then search an associated English language 
article, if available, for additional information. 
? A second pass checks for multi-word phrases 
that exist as titles of Wikipedia articles. 
? We look for certain types of person and 
organization instances. 
? We perform additional processing for 
alphabetic or space-separated languages, 
including a third pass looking for single word 
Wikipedia titles. 
? We use regular expressions to locate additional 
entities such as numeric dates.  
 In the first pass, we attempt to replace all wiki-
links with appropriate entity tags.  We assume at 
this stage that any phrase identified as an entity at 
some point in the article will be an entity of the 
same type throughout the article, since it is com-
mon for contributors to make the explicit link only 
on the first occasion that it occurs.  We also as-
sume that a phrase in a bold font within the first 
100 characters is an equivalent form of the title of 
the article as in this start of the article on Erquy: 
?Erquy (Erge-ar-Mor en breton, Erqi en gallo)?.  
The parenthetical notation gives alternate names in 
the Breton and Gallo languages.  (In Wiki database 
format, bold font is indicated by three apostrophes 
in succession.)   
 If the article has an English equivalent, we 
search that article for wikilinked phrases as well, 
on the assumption that both articles will refer to 
many of the same entities.  As the English lan-
guage Wikipedia is the largest, it frequently con-
tains explicit references to and articles on 
secondary people and places mentioned, but not 
linked, within a given non-English article.  After 
this point, the text to be annotated contains no 
Wikipedia specific information or formatting. 
 In the second pass, we look for strings of 2 to 4 
words which were not wikilinked but which have 
Wikipedia entries of their own or are partial 
matches to known people and organizations (i.e. 
?Mary Washington? in an article that contains 
?University of Mary Washington?).  We require 
that each such string contains something other than 
a lower case letter (when a language does not use 
capitalization, nothing in that writing system is 
considered to be lower case for this purpose).   
When a word is in more than one such phrase, the 
longest match is used. 
 We then do some special case processing.  
When an organization is followed by something in 
parentheses such as <ENAMEX TYPE=?ORGAN-
IZATION?>Maktab al-Khadam?t</ENAMEX> 
(MAK), we hypothesize that the text in the 
parentheses is an alternate name of the organiza-
tion.   We also looked for unmarked strings of the 
form X.X. followed by a capitalized word, where 
X represents any capital letter, and marked each 
occurrence as a PERSON. 
 For space-separated or alphabetic languages, 
we did some additional processing at this stage to 
attempt to identify more names of people.  Using a 
list of names derived from Wiktionary (Appen-
dix:Names) and optionally a list derived from 
Wikipedia (see Section 3.5.1), we mark possible 
parts of names.  When two or more are adjacent, 
we mark the sequence as a PERSON.  Also, we fill 
in partial lists of names by assuming single non-
lower case words between marked names are actu-
ally parts of names themselves.  That is, we would 
replace <ENAMEX TYPE=?PERSON?>Fred 
Smith</ENAMEX>, Somename <ENAMEX 
TYPE=?PERSON?>Jones </ENAMEX> with 
<ENAMEX TYPE=?PERSON?> Fred Smith</E-
NAMEX>, <ENAMEX TYPE= ?PERSON?> 
Somename Jones</ENAMEX>. At this point, we 
performed a third pass through the article.  We 
marked all non-lower case single words which had 
their own Wikipedia entry, were part of a known 
person's name, or were part of a known 
organization's name. 
 Afterwards, we used a series of simple, lan-
guage-neutral regular expressions to find addi-
tional TIME, PERCENT, and DATE entities such 
as ?05:30? and ?12-07-05?.  We also executed 
code that included quantities of money within a 
NUMEX tag, as in converting 500 <NUMEX 
TYPE=?MONEY?>USD</NUMEX> into <NU-
MEX TYPE=?MONEY?>500 USD</NUMEX>. 
5
3.5 Optional Processing 
3.5.1 Recommended Additions 
All of the above could be run with almost no un-
derstanding of the language being modeled 
(knowing whether the language was space-sepa-
rated and whether it was alphabetic or character-
based were the only things used).  However, for 
most languages, we spent a small amount of time 
(less than one hour) browsing Wikipedia pages to 
improve performance in some areas.    
 We suggest compiling a small list of stop 
words.  For our purposes, the determiners and the 
most common prepositions are sufficient, though a 
longer list could be used for the purpose of com-
putational efficiency.   
 We also recommend compiling a list of number 
words as well as compiling a list of currencies, 
since they are not capitalized in many languages, 
and may not be explicitly linked either.  Many lan-
guages have a page on ISO 4217 which contains 
all of the currency information, but the format 
varies sufficiently from language to language to 
make automatic extraction difficult.  Together, 
these allow phrases like this (taken from the 
French Wikipedia) to be correctly marked in its 
entirety as an entity of type MONEY: ?25 millions 
de dollars.? 
 If a language routinely uses honorifics such as 
Mr. and Mrs., that information can also be found 
quickly. Their use can lead to significant im-
provements in PERSON recognition. 
 During preprocessing, we typically collected a 
list of people names automatically, using the entity 
identification methods appropriate to titles of 
Wikipedia articles.  We then used these names 
along with the Wiktionary derived list of names 
during the main processing.  This does introduce 
some noise as the person identification is not per-
fect, but it ordinarily increases recall by more than 
it reduces precision. 
3.5.2 Language Dependent Additions 
Our usual, language-neutral processing only 
considers wikilinks within a single article when 
determining the type of unlinked words and 
phrases.  For example, if an article included the 
sentence ?The [[Delaware River|Delaware]] forms 
the boundary between [[Pennsylvania]] and [[New 
Jersey]]?, our system makes the assumption that 
every occurrence of the unlinked word ?Delaware? 
appearing in the same article is also referring to the 
river and thus mark it as a LOCATION.  
 For some languages, we preferred an alternate 
approach, best illustrated by an example:  The 
word ?Washington? without context could refer to 
(among others) a person, a GPE, or an organiza-
tion.  We could work through all of the explicit 
wikilinks in all articles (as a preprocessing step) 
whose surface form is Washington and count the 
number pointing to each.  We could then decide 
that every time the word Washington appears 
without an explicit link, it should be marked as its 
most common type.  This is useful for the Slavic 
languages, where the nominative form is typically 
used as the title of Wikipedia articles, while other 
cases appear frequently (and are rarely wikilinked). 
 At the same time, we can do a second type of 
preprocessing which allows more surface forms to 
be categorized.  For instance, imagine that we were 
in a Wikipedia with no article or redirect associ-
ated to ?District of Columbia? but that someone 
had made a wikilink of the form [[Washing-
ton|District of Columbia]].  We would then make 
the assumption that for all articles, District of Co-
lumbia is of the same type as Washington.   
 For less developed wikipedias, this can be 
helpful.  For languages that have reasonably well 
developed Wikipedias and where entities rarely, if 
ever, change form for grammatical reasons (such 
as French), this type of preprocessing is virtually 
irrelevant.  Worse, this processing is definitely not 
recommended for languages that do not use capi-
talization because it is not unheard of for people to 
include sections like: ?The [[Union Station|train 
station]] is located at ...? which would cause the 
phrase ?train station? to be marked as a FACILITY 
each time it occurred.  Of course, even in lan-
guages with capitalization, ?train station? would be 
marked incorrectly in the article in which the 
above was located, but the mistake would be iso-
lated, and should have minimal impact overall. 
4 Evaluation and Results 
After each data set was generated, we used the text 
as a training set for input to PhoenixIDF.  We had 
three human annotated test sets, Spanish, French 
and Ukrainian, consisting of newswire.  When 
human annotated sets were not available, we held 
out more than 100,000 words of text generated by 
our wiki-mining process to use as a test set. For the 
above languages, we included wiki test sets for 
6
comparison purposes. We will give our results as 
F-scores in the Overall, DATE, GPE, 
ORGANIZATION, and PERSON categories using 
the scoring metric in (Bikel et. al, 1999).  The 
other ACE categories are much less common, and 
contribute little to the overall score. 
4.1 Spanish Language Evaluation  
The Spanish Wikipedia is a substantial, well-de-
veloped Wikipedia, consisting of more than 
290,000 articles as of October 2007.  We used two 
test sets for comparison purposes.  The first con-
sists of 25,000 words of human annotated news-
wire derived from the ACE 2007 test set, manually 
modified to conform to our extended MUC-style 
standards.  The second consists of 335,000 words 
of data generated by the Wiki process held-out 
during training.  
 
Table 2: Spanish Results 
 
F (prec. / recall) Newswire Wiki test set 
ALL .827 (.851 / .805) .846 (.843 / .848) 
DATE .912 (.861 / .970) .925 (.918 / .932) 
GPE .877 (.914 / .843) .877 (.886 / .868) 
ORG .629 (.681 / .585) .701 (.703 / .698) 
PERSON .906 (.921 / .892) .821 (.810 / .833) 
 
 There are a few particularly interesting results 
to note.  First, because of the optional processing, 
recall was boosted in the PERSON category at the 
expense of precision.  The fact that this category 
scores higher against newswire than against the 
wiki data suggests that the not-uncommon, but 
isolated, occurrences of non-entities being marked 
as PERSONs in training have little effect on the 
overall system.  Contrarily, we note that deletions 
are the dominant source of error in the ORGANI-
ZATION category, as seen by the lower recall.  
The better performance on the wiki set seems to 
suggest that either Wikipedia is relatively poor in 
Organizations or that PhoenixIDF underperforms 
when identifying Organizations relative to other 
categories or a combination. 
 An important question remains: ?How do these 
results compare to other methodologies??  In par-
ticular, while we can get these results for free, how 
much work would traditional methods require to 
achieve comparable results? 
 To attempt to answer this question, we trained 
PhoenixIDF on additional ACE 2007 Spanish lan-
guage data converted to MUC-style tags, and 
scored its performance using the same set of 
newswire.  Evidently, comparable performance to 
our Wikipedia derived system requires between 
20,000 and 40,000 words of human-annotated 
newswire.  It is worth noting that Wikipedia itself 
is not newswire, so we do not have a perfect com-
parison.   
 
Table 3: Traditional Training 
 
~ Words of Training  Overall F-score 
3500 .746 
10,000 .760 
20,000 .807 
40,000 .847 
 
4.2  French Language Evaluation 
The French Wikipedia is one of the largest 
Wikipedias, containing more than 570,000 articles 
as of October 2007.  For this evaluation, we have 
25,000 words of human annotated newswire 
(Agence France Presse, 30 April and 1 May 1997) 
covering diverse topics.  We used 920,000 words 
of Wiki-derived data for the second test.   
 
Table 4: French Results 
 
F (prec. / recall) Newswire Wiki test set 
ALL .847 (.877 / .819) .844 (.847 / .840)  
DATE .921 (.897 / .947) .910 (.888 / .934) 
GPE .907 (.933 / .882) .868 (.889 / .849) 
ORG .700 (.794 / .625) .718 (.747 / .691) 
PERSON .880 (.874 / .885) .823 (.818 / .827) 
 
The overall results seem comparable to the Span-
ish, with the slightly better overall performance 
likely correlated to the somewhat more developed 
Wikipedia. We did not have sufficient quantities of 
annotated data to run a test of the traditional meth-
ods, but Spanish and French are sufficiently similar 
languages that we expect this model is comparable 
to one created with about 40,000 words of human-
annotated data. 
7
4.3 Ukrainian Language Evaluation 
 
The Ukrainian Wikipedia is a medium-sized 
Wikipedia with 74,000 articles as of October 2007. 
Also, the typical article is shorter and less well-
linked to other articles than in the French or Span-
ish versions.  Moreover, entities tend to appear in 
many surface forms depending on case, leading us 
to expect somewhat worse results.  In the Ukrain-
ian case, the newswire consisted of approximately 
25,000 words from various online news sites cov-
ering primarily political topics.  We also held out 
around 395,000 words for testing. We were also 
able to run a comparison test as in Spanish.   
 
Table 5: Ukrainian Results 
 
F (prec. / recall) Newswire Wiki test set 
ALL .747 (.863 / .649) .807 (.809 / .806) 
DATE .780 (.759 / .803) .848 (.842 / .854) 
GPE .837 (.833 / .841) .887 (.901 / .874) 
ORG .585 (.800 / .462) .657 (.678 / .637) 
PERSON .764 (.899 / .664) .690 (.675 / .706) 
 
Table 6: Traditional Training 
 
~ Words of Training  Overall F-score 
5000 .662 
10,000 .692 
15,000 .740 
20,000 .761 
 
The Ukrainian newswire contained a much higher 
proportion of organizations than the French or 
Spanish versions, contributing to the overall lower 
score. The Ukrainian language Wikipedia itself 
contains very few articles on organizations relative 
to other types, so the distribution of entities of the 
two test sets are quite different.  We also see that 
the Wiki-derived model performs comparably to a 
model trained on 15-20,000 words of human-
annotated text.   
 
4.4 Other Languages 
 
For Portuguese, Russian, and Polish, we did not 
have human annotated corpora available for test-
ing.  In each case, at least 100,000 words were held 
out from training to be used as a test set.  It seems 
safe to suppose that if suitable human-annotated 
sets were available for testing, the PERSON score 
would likely be higher, and the ORGANIZATION 
score would likely be lower, while the DATE and 
GPE scores would probably be comparable. 
 
Table 7: Other Language Results 
 
F-score Polish Portuguese Russian 
ALL .859 .804 .802 
DATE .891 .861 .822 
GPE .916 .826 .867 
ORG .785 .706 .712 
PERSON .836 .802 .751 
 
5 Conclusions 
In conclusion, we have demonstrated that Wikipe-
dia can be used to create a Named Entity Recogni-
tion system with performance comparable to one 
developed from 15-40,000 words of human-anno-
tated newswire, while not requiring any linguistic 
expertise on the part of the user.  This level of per-
formance, usable on its own for many purposes, 
can likely be obtained currently in 20-40 lan-
guages, with the expectation that more languages 
will become available, and that better models can 
be developed, as Wikipedia grows.   
 Moreover, it seems clear that a Wikipedia-de-
rived system could be used as a supplement to 
other systems for many more languages.  In par-
ticular, we have, for all practical purposes, embed-
ded in our system an automatically generated 
entity dictionary. 
 In the future, we would like to find a way to 
automatically generate the list of key words and 
phrases for useful English language categories. 
This could implement the work of Kazama and 
Torisawa, in particular. We also believe perform-
ance could be improved by using higher order non-
English categories and better disambiguation.  We 
could also experiment with introducing automati-
cally generated lists of entities into PhoenixIDF 
directly.  Lists of organizations might be parti-
cularly useful, and ?List of? pages are common in 
many languages. 
8
References  
Bikel, D., R. Schwartz, and R. Weischedel. 1999. 
An algorithm that learns what's in a name. Ma-
chine  Learning, 211-31.  
 
Bunescu, R and M. Pa?ca. 2006. Using Encyclope-
dic knowledge for named entity disambigua-
tion. In  Proceedings of EACL, 9-16. 
 
Cucerzan, S. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In  Pro-
ceedings of EMNLP/CoNLL, 708-16. 
 
Gabrilovitch, E. and S. Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceed-
ings of IJCAI, 1606-11. 
 
Gabrilovitch, E. and S. Markovitch. 2006. Over-
coming the brittleness bottleneck using 
Wikipedia: enhancing text categorization with 
encyclopedic knowledge. In Proceedings of 
AAAI, 1301-06. 
 
Gabrilovitch, E. and S. Markovitch. 2005. Feature 
generation for text categorization using world 
knowledge. In Proceedings of IJCAI, 1048-53. 
 
Kazama, J. and K. Torisawa. 2007. Exploiting 
Wikipedia as external knowledge for named 
entity recognition. In Proceedings of 
EMNLP/CoNLL, 698-707. 
 
Milne, D., O. Medelyan and I. Witten. 2006. Min-
ing domain-specific thesauri from Wikipedia: a 
case study. Web Intelligence 2006, 442-48 
 
Strube, M. and S. P. Ponzeto. 2006. WikiRelate! 
Computing  semantic relatedness using 
Wikipedia. In Proceedings of AAAI, 1419-24. 
 
Toral, A. and R.  Mu?oz. 2006. A proposal to 
automatically build and maintain gazetteers for 
named entity recognition by using Wikipedia. 
In Proceedings of  EACL, 56-61. 
 
Weale, T. 2006. Using Wikipedia categories for 
document classification. Ohio St. University, 
preprint. 
 
Zesch, T., I. Gurevych and M. M?hlh?user. 2007. 
Analyzing and accessing Wikipedia as a lexical 
semantic resource. In Proceedings of GLDV, 
213-21. 
 
9
