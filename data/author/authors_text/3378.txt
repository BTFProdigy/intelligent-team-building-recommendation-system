Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 745?752
Manchester, August 2008
Classifying chart cells for quadratic complexity context-free inference
Brian Roark and Kristy Hollingshead
Center for Spoken Language Understanding
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{roark,hollingk}@cslu.ogi.edu
Abstract
In this paper, we consider classifying word
positions by whether or not they can either
start or end multi-word constituents. This
provides a mechanism for ?closing? chart
cells during context-free inference, which
is demonstrated to improve efficiency and
accuracy when used to constrain the well-
known Charniak parser. Additionally, we
present a method for ?closing? a sufficient
number of chart cells to ensure quadratic
worst-case complexity of context-free in-
ference. Empirical results show that this
O(n
2
) bound can be achieved without im-
pacting parsing accuracy.
1 Introduction
While there have been great advances in the statis-
tical modeling of hierarchical syntactic structure in
the past 15 years, exact inference with such mod-
els remains very costly, so that most rich syntac-
tic modeling approaches involve heavy pruning,
pipelining or both. Pipeline systems make use of
simpler models with more efficient inference to re-
duce the search space of the full model. For ex-
ample, the well-known Ratnaparkhi (1999) parser
used a POS-tagger and a finite-state NP chunker as
initial stages of a multi-stage Maximum Entropy
parser. The Charniak (2000) parser uses a simple
PCFG to prune the chart for a richer model; and
Charniak and Johnson (2005) added a discrimina-
tively trained reranker to the end of that pipeline.
Recent results making use of finite-state chun-
kers early in a syntactic parsing pipeline have
shown both an efficiency (Glaysher andMoldovan,
2006) and an accuracy (Hollingshead and Roark,
2007) benefit to the use of such constraints in a
parsing system. Glaysher and Moldovan (2006)
demonstrated an efficiency gain by explicitly dis-
allowing entries in chart cells that would result in
constituents that cross chunk boundaries. Holling-
shead and Roark (2007) demonstrated that high
precision constraints on early stages of the Char-
niak and Johnson (2005) pipeline?in the form of
base phrase constraints derived either from a chun-
ker or from later stages of an earlier iteration of the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
same pipeline?achieved significant accuracy im-
provements, by moving the pipeline search away
from unlikely areas of the search space. Both
of these approaches (as with Ratnaparkhi earlier)
achieve their improvements by ruling out parts of
the search space for downstream processes, and the
gain can either be realized in efficiency (same ac-
curacy, less time) or accuracy (same time, greater
accuracy). Parts of the search space are ruled out
precisely when they are inconsistent with the gen-
erally reliable output of the chunker, i.e., the con-
straints are a by-product of chunking.
In this paper, we consider building classifiers
that more directly address the problem of ?closing?
chart cells to entries, rather than extracting this in-
formation from taggers or chunkers built for a dif-
ferent purpose. We build two classifiers, which tag
each word in the sequence with a binary class la-
bel. The first classifier decides if the word can be-
gin a constituent of span greater than one word; the
second classifier decides if the word can end a con-
stituent of span greater than 1. Given a chart cell
(i, j) with start word w
i
and end word w
j
, where
j>i, that cell can be ?closed? to entries if the first
classifier decides thatw
i
cannot be the first word of
a multi-word constituent or if the second classifier
decides that w
j
cannot be the last word in a multi-
word constituent. In such a way, we can optimize
classifiers specifically for the task of constrain-
ing chart parsers. Note that such classifier output
would be relatively straightforward to incorporate
into most existing context-free constituent parsers.
We demonstrate the baseline accuracies of such
classifiers, and their impact when the constraints
are placed on the Charniak and Johnson (2005)
parsing pipeline. Various ways of using classifier
output are investigated, including one method for
guaranteeing quadratic complexity of the context-
free parser. A proof of the quadratic complexity is
included, along with a detailed performance evalu-
ation when constraining the Charniak parser to be
worst-case quadratic.
2 Background
Dynamic programming for context-free inference
generally makes use of a chart structure, as shown
in Fig. 1. Each cell in the chart represents a pos-
sible constituent spanning a substring, which is
identified by the indices of the first and last words
of the substring. Thus, the cell identified with
745
i, j
i, j?1 i+1, j
i, j?2 i+1, j?1 i+2, j
i, j?3 i+1, j?2 i+2, j?1 i+3, j
i, j?4 i+1, j?3 i+2, j?2 i+3, j?1 i+4, j
Figure 1: Fragment of a chart structure. Each cell is indexed
with start and end word indices.
i, j will contain possible constituents spanning the
substring w
i
. . . w
j
. Context-free inference has cu-
bic complexity in the length of the string n, due
to the O(n
2
) chart cells and O(n) possible child
configurations at each cell. For example, the CYK
algorithm, which assumes a grammar in Chomsky
Normal Form (hence exactly 2 non-terminal chil-
dren for each constituent of span greater than 1),
must consider the O(n) possible midpoints for the
two children of constituents at each cell.
In a parsing pipeline, some decisions about the
hidden structure are made at an earlier stage. For
example, base phrase chunking involves identify-
ing a span as a base phrase of some category,
often NP. A base phrase constituent has no chil-
dren other than pre-terminal POS-tags, which all
have a single terminal child, i.e., there is no in-
ternal structure in the base phrase involving non-
POS non-terminals. This has a number of implica-
tions for the context-free parser. First, there is no
need to build internal structure within the identi-
fied base phrase constituent. Second, constituents
which cross brackets with the base phrase can-
not be part of the final tree structure. This sec-
ond constraint on possible trees can be thought
of as a constraint on chart cells, as pointed out
in Glaysher and Moldovan (2006): no multi-word
spanning constituent can begin at a word falling
within a base-phrase chunk, other than the first
word of that chunk. Similarly, no multi-word span-
ning constituent can end at a word falling within a
base-phrase chunk, other than the last word of that
chunk. These constraints rule out many possible
structures that the full context-free parser would
have to otherwise consider.
These start and end constraints can be extracted
from the output of the chunker, but the chunker is
not trained to optimize the accuracy (or the pre-
cision) of these particular constraints, rather typi-
cally to optimize chunking accuracy. Further, these
constraints can apply even for words which fall
outside of typical chunks. For example, in En-
glish, verbs and prepositions tend to occur before
their arguments, hence are often unlikely to end
constituents, despite not being inside a typically
defined base phrase. If we can build a classifier
specifically for this task (determining whether a
Strings in corpus 39832
Word tokens in corpus 950028
Tokens neither first nor last in string 870399
Word tokens in S
1
439558 50.5%
Word tokens in E
1
646855 74.3%
Table 1: Statistics on word classes from sections 2-21 of the
Penn Wall St. Journal Treebank
word can start or end a multi-word constituent),
we can more directly optimize the classifier for use
within a pipeline.
3 Starting and ending constituents
To better understand the particular task that we
propose, and its likely utility, we first look at the
distribution of classes and our ability to build sim-
ple classifiers to predict these classes. First, let
us introduce notation. Given a string of n words
w
1
. . . w
n
, we will say that a word w
i
(1<i<n) is
in the class S
>1
if there is a constituent spanning
w
i
. . . w
j
for some j>i; and w
i
? S
1
otherwise.
Similarly, we will say that a word w
j
(1<j<n) is
in the class E
>1
if there is a constituent spanning
w
i
. . . w
j
for some i<j; and w
j
? E
1
otherwise.
These are two separate binary classification tasks.
Note that the first word w
1
and the last word
w
n
are unambiguous in terms of whether they start
or end constituents of length greater than 1. The
first word w
1
must start a constituent spanning the
whole string, and the last word w
n
must end that
same constituent. The first word w
1
cannot end a
constituent of length greater than 1; similarly, the
last word w
n
cannot start a constituent of length
greater than 1. Hence our classifier evaluation
omits those two word positions, leading to n?2
classifications for a string of length n.
Table 1 shows statistics from sections 2-21 of
the Penn WSJ Treebank (Marcus et al, 1993).
From the nearly 1 million words in approximately
40 thousand sentences, just over 870 thousand are
neither the first nor the last word in the string,
hence possible members of the sets S
1
or E
1
, i.e.,
not beginning a multi-word constituent (S
1
) or not
ending a multi-word constituent (E
1
). Of these,
over half (50.5%) do not begin multi-word con-
stituents, and nearly three quarters (74.3%) do not
end multi-word constituents. This high latter per-
centage reflects English right-branching structure.
How well can we perform these binary classifi-
cation tasks, using simple (linear complexity) clas-
sifiers? To investigate this question, we used sec-
tions 2-21 of the Penn WSJ Treebank as training
data, section 00 as heldout, and section 24 as de-
velopment. Word classes are straightforwardly ex-
tracted from the treebank trees, by measuring the
span of constituents starting and ending at each
word position. We trained log linear models with
the perceptron algorithm (Collins, 2002) using fea-
746
Markov order
Classification Task 0 1 2
S
1
(no multi-word constituent start) 96.7 96.9 96.9
E
1
(no multi-word constituent end) 97.3 97.3 97.3
Table 2: Classification accuracy on development set for bi-
nary classes S
1
and E
1
, for various Markov orders.
tures similar to those used for NP chunking in Sha
and Pereira (2003), including surrounding POS-
tags (provided by a separately trained log linear
POS-tagger) and surrounding words, up to 2 be-
fore and 2 after the current word position.
Table 2 presents classification accuracy on the
development set for both of these classification
tasks. We trained models with Markov order 0
(each word classified independently), order 1 (fea-
tures with class pairs) and order 2 (features with
class triples). This did not change performance
for the E
1
classification, but Markov order 1 was
slightly (but significantly) better than order 0 for
S
1
classification. Hence, from this point forward,
all classification will be Markov order 1.
We can see from these results that simple classi-
fication approaches yield very high classification
accuracy. The question now becomes, how can
classifier output be used to constrain a context-free
parser, and what is the impact on parser perfor-
mance of using such a classifier in the pipeline.
4 Closing chart cells
Before moving on to an empirical investigation of
constraining context-free parsers with the methods
we propose, we first need to take a fairly detailed
look at representations internal to these parsers. In
particular, while we can rule out multi-word con-
stituents with particular start and end positions,
there may be intermediate or incomplete structures
within the parser that should not be ruled out at
these same start and end positions. Hence the no-
tion of ?closing? a chart cell is slightly more com-
plicated than it may seem initially.
Consider the chart representation in Fig. 1. Sup-
pose that w
i
is in class S
1
and w
j
is in class E
1
,
for i<j. We can ?close? all cells (i, k) such that
i<k and all cells (l, j) such that l<j, based on
the fact that multi-word constituents cannot begin
with word w
i
and cannot end with w
j
. A closed
cell will not take complete entries, and, depending
on the constraint used to close the cell, will have
restrictions on incomplete entries. To make this
more explicit, let us precisely define complete and
incomplete entries.
Context-free inference using dynamic program-
ming over a chart structure builds longer-span con-
stituents by combining smaller span constituents,
guided by rules in a context-free grammar. A
context-free grammar G = (V, T, S
?
, P ) consists
of: a set of non-terminal symbols V , including a
special start symbol S
?
; a set of terminal symbols
T ; and a set of rule productions P of the form
A ? ? for A ? V and ? ? (V ? T )
?
, i.e.,
a single non-terminal on the left-hand side of the
rule production, and a sequence of 0 or more ter-
minals or non-terminals on the right-hand side of
the rule. If we have a rule production A ? B C
in P , a completed B entry in chart cell (i, j) and
a completed C entry in chart cell (j, k), then we
can place a completed A entry in chart cell (i, k),
typically with some indication that the A was built
from the B and C entries. Such a chart cell entry
is sometimes called an ?edge?.
The issue with incomplete edges arises when
there are rule productions in P with more than two
children on the right-hand side of the rule. Rather
than trying to combine an arbitrarily large num-
ber of smaller cell entries, a more efficient ap-
proach, which exploits shared structure between
rules, is to only perform pairwise combination,
and store incomplete edges to represent combina-
tions that require further combination to achieve
a complete edge. This can either be performed
in advance, e.g., by factoring a grammar to be in
Chomsky Normal Form, as required by the CYK
algorithm (Cocke and Schwartz, 1970; Younger,
1967; Kasami, 1965), resulting in ?incomplete?
non-terminals created by the factorization; or in-
complete edges can be represented through so-
called dotted rules, as with the Earley (1970) al-
gorithm, in which factorization is essentially per-
formed on the fly. For example, if we have a rule
production A ? B C D in P , a completed B en-
try in chart cell (i, j) and a completed C entry in
chart cell (j, k), then we can place an incomplete
edgeA ? B C ?D in chart cell (i, k). The dot sig-
nifies the division between what has already been
combined (to the left of the dot), and what remains
to be combined.
1
Then, if we have an incomplete
edge A ? B C ?D in chart cell (i, k) and a com-
plete D in cell (k, l), we can place a completed A
entry in chart cell (i, l).
If a chart cell (i, j) has been ?closed? due to
constraints limiting multi-word constituents with
that span ? either w
i
? S
1
or w
j
? E
1
(and i<j) ?
then it is clear that ?complete? edges should not be
entered in the cell, since these represent precisely
the multi-word constituents that are being ruled
out. How about incomplete edges? To the extent
that an incomplete edge can be extended to a valid
complete edge, it should be allowed. There are
two cases. If w
i
? S
1
, then under the assumption
that incomplete edges are extended from left-to-
right (see footnote 1), the incomplete edge should
1
Without loss of generality, we will assume that edges are
extended from left-to-right.
747
Parsing accuracy % of Cells
Parsing constraints LR LP F Closed
None (baseline) 88.6 89.2 88.9 ?
S
1
positions 87.6 89.1 88.3 44.6
E
1
positions 87.4 88.5 87.9 66.4
Both S
1
and E
1
86.5 88.6 87.4 80.3
Table 3: Charniak parsing accuracy on section 24 under var-
ious constraint conditions, using word labels extracted using
Markov order 1 model.
be discarded, because any completed edges that
could result from extending that incomplete edge
would have the same start position, i.e., the chart
cell would be (i, k) for some k>i, which is closed
to the completed edge. However, if w
i
6? S
1
, then
w
j
? E
1
. A complete edge achieved by extending
the incomplete edge will end at w
k
for k>j, and
cell (i, k) may be open, hence the incomplete edge
should be allowed in cell (i, j). See ?6 for limita-
tions on how such incomplete edges arise in closed
cells, which has consequences for the worst-case
complexity under certain conditions.
5 Constraining the Charniak parser
5.1 Parser overview and constraint methods
The Charniak (2000) parser is a multi-stage,
agenda-driven, edge-based parser, that can be con-
strained by precluding edges from being placed on
the agenda. Here we will briefly describe the over-
all architecture of that parser, and our method for
constraining its search.
The first stage of the Charniak parser uses an
agenda and a simple PCFG to build a sparse chart,
which is used in later stages with the full model.
We will focus on this first stage, since it is here
that we will be constraining the parser. The edges
on the agenda and in the chart are dotted rules, as
described in ?4. When edges are created, they are
pushed onto the agenda. Edges that are popped
from the agenda are placed in the chart, and then
combined with other chart entries to create new
edges that are pushed onto the agenda. When a
complete edge spanning the whole string is placed
in the chart, at least one full solution exists in the
chart. After this happens, the parser continues
adding edges to the chart and agenda until reaching
some parameterized target number of additional
edges in the chart, at which point the next stage
of the pipeline receives the chart as input and any
edges remaining on the agenda are discarded.
We constrain the first stage of the Charniak
parser as follows. Using classifiers, a subset of
word positions are assigned to class S
1
, and a sub-
set are assigned to class E
1
. (Words can be as-
signed to both.) When an edge is created for cell
(i, j), where i < j, it is not placed on the agenda
if either of the following two conditions hold: 1)
w
i
? S
1
; or 2) the edge is complete and w
j
? E
1
.
0.5 0.6 0.7 0.8 0.9 10.95
0.96
0.97
0.98
0.99
1
Recall
Pre
cis
ion
Start classification
End classification
Figure 2: Precision/recall tradeoff of S
1
and E
1
tags on the
development set.
Of course, the output of our classifier is not per-
fect, hence imposing these constraints will some-
times rule out the true parse, and parser accuracy
may degrade. Furthermore, because of the agenda-
based heuristic search, the efficiency of search may
not be impacted as straightforwardly as one might
expect for an exact inference algorithm. For these
reasons, we have performed extensive empirical
trials under a variety of conditions to try to clearly
understand the best practices for using these sorts
of constraints for this sort of parser.
5.2 Experimental trials
We begin by simply taking the output of the
Markov order 1 taggers, whose accuracies are re-
ported in Table 2, and using word positions labeled
as S
1
or E
1
to ?close? cells in the Charniak parser,
as described above. Table 3 presents parser accu-
racy on the development set (section 24) under four
conditions: the unconstrained baseline; using just
S
1
words to close cells; using just E
1
word posi-
tions to close cells; and using both S
1
and E
1
po-
sitions to close cells. As can be seen from these
results, all of these trials result in a decrease in
accuracy from the baseline, with larger decreases
associated with higher percentages of closed cells.
These results indicate that, despite the relatively
high accuracy of classification, the precision of our
classifier in producing the S
1
and E
1
tags is too
low. To remedy this, we traded some recall for pre-
cision as follows. We used the forward-backward
algorithm with our Markov order 1 tagging model
to assign a conditional probability at each word po-
sition of the tags S
1
and E
1
given the string. At
each word position w
i
for 1<i<n, we took the log
likelihood ratio of tag S
1
as follows:
LLR(w
i
? S
1
) = log
P(w
i
? S
1
| w
1
. . . w
n
)
P(w
i
6? S
1
| w
1
. . . w
n
)
(1)
and the same for tag E
1
. A default classification
threshold is to label S
1
or E
1
if the above log like-
lihood is greater than zero, i.e., if the S
1
tag is more
likely than not. To improve the precision, we can
move this threshold to some greater value.
748
0 0.2 0.4 0.6 0.8 187.5
88
88.5
89
89.5
Fraction of constraints preserved
Ch
arn
iak
 pa
rse
r F
?m
eas
ure
Start position constraints
End position constraints
Baseline performance
Figure 3: Charniak parser F-measure at various operating
points of the fraction c of total constraints kept.
Each word position in a string was ranked with
respect to these log likelihood ratios for each
tag.
2
If the total number of words w
i
with
LLR(w
i
? S
1
) > 0 is k, then we defined multi-
ple operating points by setting the threshold such
that ck words remained above threshold, for some
constant c between 0 and 1. Fig. 2 shows the pre-
cision/recall tradeoff at these operating points for
both S
1
and E
1
tags. Note that for both tags, we
can achieve over 99% precision with recall above
70%, and for theE
1
tag (a more frequent class than
S
1
) that level of precision is achieved with recall
greater than 90%.
Constraints were derived at each of these oper-
ating points and used within the Charniak parsing
pipeline. Fig. 3 shows the F-measure parsing per-
formance using either S
1
or E
1
constraints at vari-
ous values of c for preserving ck of the original k
constraints. As can be seen from that graph, with
improved precision both types of constraints have
operating points that achieve accuracy improve-
ments over the baseline parser on the dev set under
default parser settings.
This accuracy improvement is similar to results
obtained in Hollingshead and Roark (2007), where
base phrase constraints from a finite-state chun-
ker were used to achieve improved parse accuracy.
Their explanation for the accuracy improvement,
which seems to apply in this case as well, is that
the first stage of the Charniak parser is still pass-
ing the same number of edges in the chart to the
second stage, but that the edges now come from
more promising parts of the search space, i.e., the
parser does a better job of exploring good parts of
the search space. Hence the constraints seem to be
doing what they should do, which is constrain the
search without unduly excluding good solutions.
Note that these results are all achieved with
the default parsing parameterizations, so that ac-
curacy gains are achieved, but not necessarily ef-
ficiency gains. The Charniak parser allows for
2
Perceptron weights were interpreted in the log domain
and conditionally normalized appropriately.
0 200 400 600 800 1000 120086
87
88
89
90
Seconds to parse development set
F?m
eas
ure
 pa
rse
 ac
cur
acy
Constrained parser
Unconstrained parser
Figure 4: Speed/accuracy tradeoff for both the uncon-
strained Charniak parser and when constrained with high pre-
cision start/end constraints.
narrow search parameterizations, whereby fewer
edges are added to the chart in the initial stage.
Given the improved search using these constraints,
high accuracy may be achieved at far narrower
search parameterizations than the default setting of
the parser. To look at potential efficiency gains
to be had from these constraints, we chose the
most constrained operating points for both start
and end constraints that do not hurt accuracy rel-
ative to the baseline parser (c = 0.7 for S
1
and
c = 0.8 for E
1
) and used both kinds of constraints
in the parser. We then ran the Charniak parser with
varying search parameters, to observe performance
when search is narrower than the default. Fig. 4
presents F-measure accuracy for both constrained
and unconstrained parser configurations at various
search parameterizations. The times for the con-
strained parser configurations include the approx-
imately 20 seconds required for POS-tagging and
word-boundary classification of the dev set.
These results demonstrate a sharper knee of the
curve for the constrained runs, with parser accu-
racy that is above that achieved by the uncon-
strained parser under the default search parameter-
ization, even after a nearly 5 times speedup.
5.3 Analysis of constraints on 1-best parses
There are two ways in which the constraints could
be improving parser performance: by helping the
parser to find higher probability parses that it was
formerly losing because of search errors; or by
not allowing the parser to select high probability
parses that violate the constraints. To get a sense
of whether the constraints on the parser are sim-
ply fixing search errors or are imposing constraints
on the model itself, we examined the 1-best parses
from both constrained and unconstrained scenar-
ios. First, we calculated the geometric mean of
the 1-best parse probabilities under both scenarios,
which were (in logs) ?207.99 for unconstrained
and ?208.09 for constrained. Thus, the con-
strained 1-best parses had very slightly less proba-
bility than the unconstrained parses, indicating that
the constraints were not simply fixing search er-
749
rors, but also eliminated some MAP parses.
To get a sense of how often search errors
were corrected versus ruling out of MAP parses,
we compared the constrained and unconstrained
parses at each string, and tallied when the uncon-
strained parse probabilities were greater (or less)
than the constrained parse probabilities, as well as
when they were equal. At the default search pa-
rameterization (210), 84.8 percent of the strings
had the same parses; in 9.2 percent of the cases the
unconstrained parses had higher probability; and
in 5.9 percent of the cases the constrained parses
had higher probability. The narrower search pa-
rameterization at the knee of the curve in Fig. 4 had
similar results: 84.6 percent were the same; in 8.6
percent of the cases the unconstrained probability
was higher; and in 6.8 percent of the cases the con-
strained probability was higher. Hence, when the
1-best parse differs, the parse found via constraints
has a higher probability in approximately 40 per-
cent of the cases.
6 O(n
2
) complexity context-free parsing
Using sufficient S
1
and E
1
constraints of the sort
we have been investigating, we can achieve worst-
case quadratic (instead of cubic) complexity. A
proof, based on the CYK algorithm, is given in
Appendix A, but we can make the key points here.
First, cubic complexity of context-free inference
is due to O(n
2
) chart cells and O(n) possible
child configurations per cell. If we ?close? all but
O(n) cells, the ?open? cells will be processed with
worst-case quadratic complexity (O(n) cells with
O(n) possible child configurations per cell). If we
can show that the remaining O(n
2
) ?closed? cells
each can be processed within constant time, then
the overall complexity is quadratic. The proof in
Appendix A shows that this is the case if closing a
cell is such that: when a cell (i, j) is closed, then
either all cells (i, k) for k>i are closed or all cells
(k, j) for k<j are closed. These conditions are
achieved when we select sets S
1
and E
1
and close
cells accordingly.
Just as we were able to order word position log
likelihood scores for classes S
1
and E
1
to im-
prove precision in the previous section, here we
will order them so that we can continue select-
ing positions until we have guaranteed less than
some threshold of ?open? cells. If the threshold
is linear in the length of the string, we will be
able to parse the string with worst-case quadratic
complexity, as shown in Appendix A. We will set
our threshold to kn for some constant k (in our
experiments, k ranges from 2 to 10). Table 4
presents the percentage of cells closed, class (S
1
and E
1
) precision and parser accuracy when the
number of ?open? cells is bounded to be less than
Open % cells Class Parse accuracy
cells closed Prec LR LP F
all ? ? 88.6 89.2 88.9
10n 39.1 99.9 88.6 89.2 88.9
8n 50.4 99.9 88.6 89.2 88.9
6n 62.8 99.9 88.6 89.2 88.9
4n 75.7 99.8 88.8 89.4 89.1
2n 88.8 99.8 88.8 89.5 89.1
Table 4: Varying constant k for kn ?open? cells, yielding
O(n
2
) parsing complexity guarantees
the threshold. These results clearly demonstrate
that such constraints can be placed on real context-
free parsing problems without significant impact to
accuracy?in fact, with small improvements.
We were quite surprised by these trials, fully ex-
pecting these limits to negatively impact accuracy.
The likely explanation is that the existing Char-
niak search strategy itself is bounding processing
in such a way that the additional constraints placed
on the process do not interfere with standard pro-
cessing. Note that our approach closes a higher
percentage of cells in longer strings, which the
Charniak pipeline already more severely prunes
than shorter strings. Further, this approach appears
to be relying very heavily on E
1
constraints, hence
has very high precision of classification.
While the Charniak parser may not be the ideal
framework within which to illustrate these worst-
case complexity improvements, the lack of impair-
ment to the parser provides strong evidence that
other parsers could make use of the resulting charts
to achieve significant efficiency gains.
7 Conclusion & Future Work
In this paper, we have presented a very simple ap-
proach to constraining context-free chart parsing
pipelines that has several nice properties. First,
it is based on a simple classification task that
can achieve very high accuracy using very sim-
ple models. Second, the classifier output can
be straightforwardly used to constrain any chart-
based context-free parser. Finally, we have shown
(in Appendix A) that ?closing? sufficient cells
with these techniques leads to quadratic worst-case
complexity bounds. Our empirical results with the
Charniak parser demonstrated that our classifiers
were sufficiently accurate to allow for such bounds
to be placed on the parser without hurting parsing
accuracy.
Future work in this direction will involve trying
different methods for defining effective operating
points, such as more heavily constraining longer
strings, in an attempt to further improve the search
in the Charniak parser. We would also like to in-
vestigate performance when using other chart pars-
ing strategies, such as when using cell pruning in-
stead of an agenda.
750
CYK(w
1
. . . w
n
, G = (V, T, S
?
, P, ?))  PCFG G must be in CNF
1 for t = 1 to n do  scan in words/POS-tags (span=1)
2 for j = 1 to |V | do
3 ?
j
(t, t)? P(A
j
? w
t
)
4 for s = 2 to n do  all spans > 1
5 for t = 1 to n?s+1 do
6 e? t+s?1  end word position for this span
7 for i = 1 to |V | do
8
?
i
(t, e)? argmax
t<m?e
?
argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
9
?
i
(t, e)? max
t<m?e
?
max
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
Figure 5: Pseudocode of a basic CYK algorithm for PCFG in Chomsky Normal Form (CNF).
References
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 173?180.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the Association for Computational Linguistics,
pages 132?139.
Cocke, J. and J.T. Schwartz. 1970. Programming languages
and their compilers: Preliminary notes. Technical report,
Courant Institute of Mathematical Sciences, NYU.
Collins, M.J. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing
(EMNLP), pages 1?8.
Earley, J. 1970. An efficient context-free parsing algorithm.
Communications of the ACM, 6(8):451?455.
Glaysher, E. and D. Moldovan. 2006. Speeding up full syn-
tactic parsing by leveraging partial parsing decisions. In
Proceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 295?300.
Hollingshead, K. and B. Roark. 2007. Pipeline iteration. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 952?959.
Kasami, T. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Technical
report, AFCRL-65-758, Air Force Cambridge Research
Lab., Bedford, MA.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini. 1993.
Building a large annotated corpus of English: The Penn
treebank. Computational Linguistics, 19:313?330.
Ratnaparkhi, A. 1999. Learning to parse natural language
with maximum entropy models. Machine Learning, 34(1-
3):151?175.
Sha, F. and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of HLT-NAACL,
pages 134?141.
Younger, D.H. 1967. Recognition and parsing of context-
free languages in time n
3
. Information and Control,
10(2):189?208.
Appendix A Proof of quadratic
complexity parsing with constraints
For this proof, we will use the well-known CYK
parsing algorithm, which makes use of grammars
in Chomsky Normal Form (CNF). To achieve
CNF, among other things, rules with more than 2
children on the right-hand side must be factored
into multiple binary rules. To do this, compos-
ite non-terminals are created in the factorizations,
which represent incomplete constituents, i.e., those
edges that require further combination to be made
complete.
3
For example, if we have a rule pro-
duction A ? B C D in the context-free grammar
G, then a new composite non-terminal would be
created, e.g., B-C, and two binary rules would re-
place the previous ternary rule: A ? B-C D and
B-C ? B C. The B-C non-terminal represents
part of a rule expansion that needs to be combined
with something else to produce a complete non-
terminal from the original set of non-terminals.
Let V
?
be the set of non-terminals that are cre-
ated through factorization, which hence represent
incomplete edges.
Fig. 5 shows pseudocode of a basic CYK algo-
rithm for use with a probabilistic CFG in CNF,
G = (V, T, S
?
, P, ?). The function ? maps from
rules in P to probabilities. Lines 1-3 of the algo-
rithm in Fig. 5 initialize the span 1 cells. Lines 4-9
are where the cubic complexity comes in: O(n)
loops in line 4, each of which include O(n) loops
in line 5, each of which requires finding a max-
imum over O(n) midpoints m in lines 8-9. For
each non-terminal A
i
? V at each cell (t, e), the
algorithm stores a backpointer ?
i
(t, e) in line 8, for
efficiently extracting the maximum likelihood so-
lution at the end of inference; and maximum prob-
abilities ?
i
(t, e) in line 9, for use in the dynamic
program.
Given a set of word positions in the classes S
1
and E
1
, as defined in the main part of this paper,
we can designate all cells (i, j) in the chart where
either w
i
? S
1
or w
j
? E
1
to be ?closed?. Chart
cells that are not closed will be called ?open?. The
total number of cells in the chart is (n
2
+ n)/2,
and if we set a threshold on the maximum number
of open cells to be kn, the number of closed cells
must be at least (n
2
+n)/2?kn. Given an ordering
of words (see ?6 for one approach), we can add
words to these sets one word at a time and close the
3
As before, we assume that edges are extended from left-
to-right, which requires a left-factorization of the grammar.
751
QUADCYK(w
1
. . . w
n
, G = (V, T, S
?
, P, ?), V
?
, S
1
, E
1
)  PCFG G must be in CNF
1 for t = 1 to n do  scan in words/POS-tags (span=1)
2 for j = 1 to |V | do
3 ?
j
(t, t)? P(A
j
? w
t
)
4 for s = 2 to n do  all spans > 1
5 for t = 1 to n?s+1 do
6 e? t+s?1  end word position for this span
7 if w
t
? S
1
CONTINUE  start position t ?closed?
8 else if w
e
? E
1
 end position e ?closed?
9 for i = 1 to |V | do
10 if A
i
6? V
?
CONTINUE  only ?incomplete? factored non-terminals (V
?
)
11
?
i
(t, e)? argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t, e? 1) ?
k
(e, e)
12
?
i
(t, e)? max
j,k
P(A
i
? A
j
A
k
) ?
j
(t, e? 1) ?
k
(e, e)
13 else  chart cell (t, e) ?open?
14 for i = 1 to |V | do
15
?
i
(t, e)? argmax
t<m?e
?
argmax
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
16
?
i
(t, e)? max
t<m?e
?
max
j,k
P(A
i
? A
j
A
k
) ?
j
(t,m? 1) ?
k
(m, e)
?
Figure 6: Pseudocode of a modified CYK algorithm, with quadratic worst case complexity with O(n) ?open? cells. In
addition to string and grammar, it requires specification of factored non-terminal set V
?
and position constraints (S
1
, E
1
).
related cells, until the requisite number of closures
are achieved. Then the resulting sets of S
1
word
positions andE
1
word positions can be provided to
the parsing algorithm, in addition to the grammar
G and the set of factored non-terminals V
?
.
Fig. 6 shows pseudocode of a modified CYK al-
gorithm that takes into account S
1
and E
1
word
classes. Lines 1-6 of the algorithm in Fig. 6 are
identical to those in the algorithm in Fig. 5. At
line 7, we have identified the chart cell being pro-
cessed, which is (t, e). If w
t
? S
1
then the cell is
completely closed, and there is nothing to do. Oth-
erwise, if w
e
? E
1
(lines 8-12), then factored non-
terminals from V
?
can be created in that cell by
finding legal combinations of children categories.
If neither of these conditions hold, then the cell is
open (lines 13-16) and processing occurs as in the
standard CYK algorithm (lines 14-16 of the algo-
rithm in Fig. 6 are identical to lines 7-9 in Fig. 5).
If the number of ?open? cells is less than kn for
some constant k, then we can prove that the algo-
rithm in Fig. 6 is O(n
2
) when given a left-factored
grammar in CNF. A key part of the proof rests on
two lemmas:
Lemma 1: Let V
?
be the set of composite non-
terminals created when left-factoring a CFG
to be in CNF, as described earlier. Then, for
any production A
i
? A
j
A
k
in the grammar,
A
k
6? V
?
.
Proof: With left-factoring, any k-ary production
A ? A
1
. . . A
k?1
A
k
results in new non-terminals
that concatenate the first k ? 1 non-terminals on
the right-hand side. These factored non-terminals
are always the leftmost child in the new produc-
tion, hence no second child in the resulting CNF
grammar can be a factored non-terminal.2
Lemma 2: For a cell (t, e) in the chart, if
w
e
? E
1
, then the only possible midpoint m
for creating an entry in the cell is e.
Proof: Placing an entry in cell (t, e) requires a rule
A
i
? A
j
A
k
, an A
j
entry in cell (t,m?1) and an
A
k
entry in cell (m, e). Suppose there is an A
k
en-
try in cell (m, e) for m < e. Recall that w
e
? E
1
,
hence the cell (m, e) is closed to non-terminals not
in V
?
. By Lemma 1, A
k
6? V
?
, therefore the cell
(m, e) is closed to A
k
entries. This is a contradic-
tion. Therefore, the lemma is proved.2
Theorem: Let O be the set of cells (t, e) such
that w
t
6? S
1
and w
e
6? E
1
(?open? cells).
If |O| < kn for some constant k, where n is
the length of the string, then the algorithm in
Fig. 6 has worst case complexity O(n
2
).
Proof: Lines 4 and 5 of the algorithm in Fig. 6
loop throughO(n
2
) cells (t, e), for which there are
three cases: w
t
? S
1
(line 7 of Fig. 6); w
e
? E
1
(lines 8-12); and (t, e) ? O (lines 13-16).
Case 1: w
t
? S
1
. No further work to be done.
Case 2: w
e
? E
1
. There is a constant amount of
work to be done, for the reason that there is only
one possible midpoint m for binary children com-
binations (namely e, as proved in Lemma 2), hence
no need to perform the maximization over O(n)
midpoints.
Case 3: (t, e) ? O. As with standard CYK pro-
cessing, there are O(n) possible midpoints m over
which to maximize, hence O(n) work required.
Only O(n) cells fall in case 3, hence the to-
tal amount of work associated with the cells in O
is O(n
2
). There are O(n
2
) cells associated with
cases 1 and 2, each of which has a total amount
of work bounded by a constant, hence the total
amount of work associated with the cells not in
O is also O(n
2
). Therefore the overall worst-case
complexity of the algorithm under these conditions
is O(n
2
). 2
752
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 787?794, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Comparing and Combining Finite-State and Context-Free Parsers
Kristy Hollingshead and Seeger Fisher and Brian Roark
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006
{hollingk,fishers,roark}@cslu.ogi.edu
Abstract
In this paper, we look at comparing high-
accuracy context-free parsers with high-
accuracy finite-state (shallow) parsers on
several shallow parsing tasks. We
show that previously reported compar-
isons greatly under-estimated the perfor-
mance of context-free parsers for these
tasks. We also demonstrate that context-
free parsers can train effectively on rel-
atively little training data, and are more
robust to domain shift for shallow pars-
ing tasks than has been previously re-
ported. Finally, we establish that combin-
ing the output of context-free and finite-
state parsers gives much higher results
than the previous-best published results,
on several common tasks. While the
efficiency benefit of finite-state models
is inarguable, the results presented here
show that the corresponding cost in accu-
racy is higher than previously thought.
1 Introduction
Finite-state parsing (also called chunking or shallow
parsing) has typically been motivated as a fast first-
pass for ? or approximation to ? more expensive
context-free parsing (Abney, 1991; Ramshaw and
Marcus, 1995; Abney, 1996). For many very-large-
scale natural language processing tasks (e.g. open-
domain question answering from the web), context-
free parsing may be too expensive, whereas finite-
state parsing is many orders of magnitude faster and
can also provide very useful syntactic annotations
for large amounts of text. For this reason, finite-state
parsing (hereafter referred to as shallow parsing) has
received increasing attention in recent years.
In addition to the clear efficiency benefit of
shallow parsing, Li and Roth (2001) have further
claimed both an accuracy and a robustness benefit
versus context-free parsing. The output of a context-
free parser, such as that of Collins (1997) or Char-
niak (2000), can be transformed into a sequence of
shallow constituents for comparison with the output
of a shallow parser. Li and Roth demonstrated that
their shallow parser, trained to label shallow con-
stituents along the lines of the well-known CoNLL-
2000 task (Sang and Buchholz, 2000), outperformed
the Collins parser in correctly identifying these con-
stituents in the Penn Wall Street Journal (WSJ) Tree-
bank (Marcus et al, 1993). They argued that their
superior performance was due to optimizing directly
for the local sequence labeling objective, rather than
for obtaining a hierarchical analysis over the entire
string. They further showed that their shallow parser
trained on the Penn WSJ Treebank did a far better
job of annotating out-of-domain sentences (e.g. con-
versational speech) than the Collins parser.
This paper re-examines the comparison of shal-
low parsers with context-free parsers, beginning
with a critical examination of how their outputs
are compared. We demonstrate that changes to the
conversion routine, which take into account differ-
ences between the original treebank trees and the
trees output by context-free parsers, eliminate the
previously-reported accuracy differences. Second,
we show that a convention that is widely accepted
for evaluation of context-free parses ? ignoring
punctuation when setting the span of a constituent ?
results in improved shallow parsing performance by
certain context-free parsers across a variety of shal-
low parsing tasks. We also demonstrate that context-
free parsers perform competitively when applied to
out-of-domain data. Finally, we show that large im-
provements can be obtained in several shallow pars-
ing tasks by using simple strategies to incorporate
context-free parser output into shallow parsing mod-
els. Our results demonstrate that a rich context-free
787
parsing model is, time permitting, worth applying,
even if only shallow parsing output is needed. In
addition, our best results, which greatly improve on
the previous-best published results on several tasks,
shed light on how much accuracy is sacrificed in
shallow parsing to get finite-state efficiency.
2 Evaluating Heterogeneous Parser Output
Two commonly reported shallow parsing tasks are
Noun-Phrase (NP) Chunking (Ramshaw and Mar-
cus, 1995) and the CoNLL-2000 Chunking task
(Sang and Buchholz, 2000), which extends the NP-
Chunking task to recognition of 11 phrase types1
annotated in the Penn Treebank. Reference shal-
low parses for this latter task were derived from
treebank trees via a conversion script known as
chunklink2. We follow Li and Roth (2001) in
using chunklink to also convert trees output by a
context-free parser into a flat representation of shal-
low constituents. Figure 1(a) shows a Penn Tree-
bank tree and Figure 1(c) its corresponding shallow
parse constituents, according to the CoNLL-2000
guidelines. Note that consecutive verb phrase (VP)
nodes result in a single VP shallow constituent.
Just as the original treebank trees are converted
for training shallow parsers, they are also typ-
ically modified for training context-free parsers.
This modification includes removal of empty nodes
(nodes tagged with ?-NONE-? in the treebank), and
removal of function tags on non-terminals; e.g., NP-
SBJ (subject NP) and NP-TMP (temporal NP) are
both mapped to NP. The output of the context-free
parser is, of course, in the same format as the train-
ing input, so empty nodes and function tags are not
present. This type of modified tree is what is shown
in Figure 1(b); note that the original treebank tree,
shown in Figure 1(a), had an empty subject NP in
the embedded clause which has been removed for
the modified tree.
To compare the output of their shallow parser with
the output of the well-known Collins (1997) parser,
Li and Roth applied the chunklink conversion
script to extract the shallow constituents from the
output of the Collins parser on WSJ section 00. Un-
1These include: ADJP, ADVP, CONJP, INTJ, LST, NP, PP,
PRT, SBAR, UCP and VP. Anything not in one of these base
phrases is designated as ?outside?.
2Downloaded from http://ilk.kub.nl/?sabine/chunklink/.
(a) S


H
HH
NP-SBJ-1
They
VP
 HH
are VP
 HH
starting S


HH
H
NP-SBJ
-NONE-
*-1
VP


H
HH
to VP


HH
H
buy NP
 HH
growth stocks
(b) S


HH
H
NP
They
VP


HH
H
are VP


HH
H
starting S
VP


HH
H
to VP


H
HH
buy NP
 HH
growth stocks
(c) [NP They] [VP are starting to buy] [NP growth stocks]
Figure 1: (a) Penn WSJ treebank tree, (b) modified treebank
tree, and (c) CoNLL-2000 style shallow bracketing, all of the
same string.
fortunately, the script was built to be applied to the
original treebank trees, complete with empty nodes,
which are not present in the output of the Collins
parser, or any well-known context-free parser. The
chunklink script searches for empty nodes in the
parse tree to perform some of its operations. In par-
ticular, any S node that contains an empty subject
NP and a VP is reduced to just a VP node, and
then combined with any immediately-preceding VP
nodes to create a single VP constituent. If the S
node does not contain an empty subject NP, as in
Figure 1(b), the chunklink script creates two VP
constituents: [VP are starting] [VP to buy], which
in this case results in a bracketing error. However,
it is a simple matter to insert an empty subject NP
into unary S?VP productions so that these nodes
are processed correctly by the script.
Various conventions have become standard in
evaluating parser output over the past decade. Per-
haps the most widely accepted convention is that
of ignoring punctuation for the purposes of assign-
ing constituent span, under the perspective that, fun-
788
Phrase Evaluation Scenario
System Type (a) (b) (c)
?Modified? All 98.37 99.72 99.72
Truth VP 92.14 98.70 98.70
Li and Roth All 94.64 - -
(2001) VP 95.28 - -
Collins (1997) All 92.16 93.42 94.28
VP 88.15 94.31 94.42
Charniak All 93.88 95.15 95.32
(2000) VP 88.92 95.11 95.19
Table 1: F-measure shallow bracketing accuracy under three
different evaluation scenarios: (a) baseline, used in Li and Roth
(2001), with original chunklink script converting treebank
trees and context-free parser output; (b) same as (a), except that
empty subject NPs are inserted into every unary S?VP produc-
tion; and (c) same as (b), except that punctuation is ignored for
setting constituent span. Results for Li and Roth are reported
from their paper. The Collins parser is provided with part-of-
speech tags output by the Brill tagger (Brill, 1995).
damentally, constituents are groupings of words.
Interestingly, this convention was not followed in
the CoNLL-2000 task (Sang and Buchholz, 2000),
which as we will see has a variable effect on context-
free parsers, presumably depending on the degree to
which punctuation is moved in training.
2.1 Evaluation Analysis
To determine the effects of the conversion routine
and different evaluation conventions, we compare
the performance of several different models on one
of the tasks presented in Li and Roth (2001). For
this task, which we label the Li & Roth task, sec-
tions 2-21 of the Penn WSJ Treebank are used as
training data, section 24 is held out, and section 00
is for evaluation.
For all trials in this paper, we report F-measure
labeled bracketing accuracy, which is the harmonic
mean of the labeled precision (P ) and labeled recall
(R), as they are defined in the widely used PARSE-
VAL metrics; i.e. the F-measure accuracy is 2PRP+R .
Table 1 shows baseline results for the Li and
Roth3 shallow parser, two well-known, high-
accuracy context-free parsers, and the reference
(true) parses after being modified as described
3We were unable to obtain the exact model used in Li and
Roth (2001), and so we use their reported results here. Note
that they used reference part-of-speech (POS) tags for their re-
sults on this task. All other results reported in this paper, unless
otherwise noted, were obtained using Brill-tagger POS tags.
above (by removing empty nodes and function
tags). Evaluation scenario (a) in Table 1 corre-
sponds to what was used in Li and Roth (2001) fol-
lowing CoNLL-2000 guidelines, with the original
chunklink script used to transform the context-
free parser output into shallow constituents. We
can see from the performance of the modified truth
in this scenario that there are serious problems
with this conversion, due to the way in which
it handles unary S?VP productions. If we de-
terministically insert empty subject NP nodes for
all such unary productions prior to the use of the
chunklink script, which we do in evaluation sce-
nario (b) of Table 1, this repairs the bulk of the
errors. Some small number of errors remain, due
largely to the fact that if the S node has been an-
notated with a function tag (e.g. S-PRP, S-PRD, S-
CLR), then chunklink will not perform its re-
duction operation on that node. However, for our
purposes, this insertion repair sufficiently corrects
the error to perform meaningful comparisons. Fi-
nally, evaluation scenario (c) follows the context-
free parsing evaluation convention of ignoring punc-
tuation when assigning constituent span. This af-
fects some parsers more than others, depending on
how the parser treats punctuation internally; for
example, Bikel (2004) documents that the Collins
parser raises punctuation nodes within the parse
tree. Since ignoring punctuation cannot hurt perfor-
mance, only improve it, even the smallest of these
differences are statistically significant.
Note that after inserting empty nodes and ignor-
ing punctuation, the accuracy advantage of Li and
Roth over Collins is reduced to a dead heat. Of
the two parsers we evaluated, the Charniak (2000)
parser gave the best performance, which is consis-
tent with its higher reported performance on the
context-free parsing task versus other context-free
parsers. Collins (2000) reported a reranking model
that improved his parser output to roughly the same
level of accuracy as Charniak (2000), and Charniak
and Johnson (2005) report an improvement using
reranking over Charniak (2000). For the purposes
of this paper, we needed an available parser that
was (a) trainable on different subsets of the data to
be applied to various tasks; and (b) capable of pro-
ducing n-best candidates, for potential combination
with a shallow parser. Both the Bikel (2004) imple-
789
System NP-Chunking CoNLL-2000 Li & Roth task
SPRep averaged perceptron 94.21 93.54 95.12
Kudo and Matsumoto (2001) 94.22 93.91 -
Sha and Pereira (2003) CRF 94.38 - -
Voted perceptron 94.09 - -
Zhang et al (2002) - 94.17 -
Li and Roth (2001) - 93.02 94.64
Table 2: Baseline results on three shallow parsing tasks: the NP-Chunking task (Ramshaw and Marcus, 1995); the CoNLL-2000
Chunking task (Sang and Buchholz, 2000); and the Li & Roth task (Li and Roth, 2001), which is the same as CoNLL-2000 but
with more training data and a different test section. The results reported in this table include the best published results on each of
these tasks.
mentation of the Collins parser and the n-best ver-
sion of the Charniak (2000) parser, documented in
Charniak and Johnson (2005), fit the requirements.
Since we observed higher accuracy from the Char-
niak parser, from this point forward we report just
Charniak parser results4.
2.2 Shallow Parser
In addition to the trainable n-best context-free parser
from Charniak (2000), we needed a trainable shal-
low parser to apply to the variety of tasks we were
interested in investigating. To this end, we repli-
cated the NP-chunker described in Sha and Pereira
(2003) and trained it as either an NP-chunker or with
the tagset extended to classify all 11 phrase types
included in the CoNLL-2000 task (Sang and Buch-
holz, 2000). Our shallow parser uses exactly the fea-
ture set delineated by Sha and Pereira, and performs
the decoding process using a Viterbi search with a
second-order Markov assumption as they described.
These features include unigram and bigram words
up to two positions to either side of the current word;
unigram, bigram, and trigram part-of-speech (POS)
tags up to two positions to either side of the current
word; and unigram, bigram, and trigram shallow
constituent tags. We use the averaged perceptron al-
gorithm, as presented in Collins (2002), to train the
parser. See (Sha and Pereira, 2003) for more details
on this approach.
To demonstrate the competitiveness of our base-
line shallow parser, which we label the SPRep av-
eraged perceptron, Table 2 shows results on three
different shallow parsing tasks. The NP-Chunking
4The parser is available for research purposes at
ftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in n-
best mode. The one-best performance of the parser is the same
as what was presented in Charniak (2000).
task, originally introduced in Ramshaw and Marcus
(1995) and also described in (Collins, 2002; Sha and
Pereira, 2003), brackets just base NP constituents5.
The CoNLL-2000 task, introduced as a shared task
at the CoNLL workshop in 2000 (Sang and Buch-
holz, 2000), extends the NP-Chunking task to label
11 different base phrase constituents. For both of
these tasks, the training set was sections 15-18 of
the Penn WSJ Treebank and the test set was section
20. We follow Collins (2002) and Sha and Pereira
(2003) in using section 21 as a heldout set. The third
task, introduced by Li and Roth (2001), performs the
same labeling as in the CoNLL-2000 task, but with
more training data and different testing sets: training
was WSJ sections 2-21 and test was section 00. We
used section 24 as a heldout set; this section is often
used as heldout for training context-free parsers.
Training and testing data for the CoNLL-2000
task is available online6. For the heldout sets for
each of these tasks, as well as for all data sets
needed for the Li & Roth task, reference shallow
parses were generated using the chunklink script
on the original treebank trees. All data was tagged
with the Brill POS tagger (Brill, 1995) after the
chunklink conversion. We verified that using
this method on the original treebank trees in sections
15-18 and 20 generated data that is identical to the
CoNLL-2000 data sets online. Replacing the POS
tags in the input text with Brill POS tags before the
5We follow Sha and Pereira (2003) in deriving the NP con-
stituents from the CoNLL-2000 data sets, by replacing all non-
NP shallow tags with the ?outside? (?O?) tag. They mention
that the resulting shallow parse tags are somewhat different than
those used by Ramshaw and Marcus (1995), but that they found
no significant accuracy differences in training on either set.
6Downloaded from the CoNLL-2000 Shared Task website
http://www.cnts.ua.ac.be/conll2000/chunking/.
790
chunklink conversion results in slightly different
shallow parses.
From Table 2 we can see that our shallow parser
is competitive on all three tasks7. Sha and Pereira
(2003) noted that the difference between their per-
ceptron and CRF results was not significant, and
our performance falls between the two, thus repli-
cating their result within noise. Our performance
falls 0.6 percentage points below the best published
result on the CoNLL-2000 task, and 0.5 percentage
points above the performance by Li and Roth (2001)
on their task. Overall, ours is a competitive approach
for shallow parsing.
3 Experimental Results
3.1 Comparing Finite-State and
Context-Free Parsers
The first two rows of Table 3 present a comparison
between the SPRep shallow parser and the Charniak
(2000) context-free parser detailed in Charniak and
Johnson (2005). We can see that the performance
of the two models is virtually indistinguishable for
all three of these tasks, with or without ignoring of
punctuation. As mentioned earlier, we used the ver-
sion of this parser with improved n-best extraction,
as documented in Charniak and Johnson (2005), al-
though without the reranking of the candidates that
they also report in that paper. For these trials, we
used just the one-best output of that model, which is
the same as in Charniak (2000).
Note that the standard training set for context-free
parsing (sections 2-21) is only used for the Li &
Roth task; for the other two tasks, both the SPRep
and the Charniak parsers were trained on sections
15-18, with section 21 as heldout. This demonstrates
that the context-free parser, even when trained on a
small fraction of the total treebank, is able to learn a
competitive model for this task.
3.2 Combining Finite-State and
Context-Free Parsers
It is likely true that a context-free parser which has
been optimized for global parse accuracy will, on
occasion, lose some shallow parse accuracy to sat-
isfy global structure constraints that do not constrain
7Sha and Pereira (2003) reported the Kudo and Matsumoto
(2001) performance on the NP-Chunking task to be 94.39 and
to be the best reported result on this task. In the cited paper,
however, the result is as reported in our table.
a shallow parser. However, it is also likely true
that these longer distance constraints will on occa-
sion enable the context-free parser to better identify
the shallow constituent structure. In other words,
despite having very similar performance, our shal-
low parser and the Charniak context-free parser are
likely making complementary predictions about the
shallow structure that can be exploited for further
improvements. In this section, we explore two sim-
ple methods for combining the system outputs.
The first combination of the system outputs,
which we call unweighted intersection, is the sim-
plest kind of ?rovered? system, which restricts the
set of shallow parse candidates to the intersection
of the sets output by each system, but does not
combine the scores. Since the Viterbi search of
the SPRep model provides a score for all possi-
ble shallow parses, the intersection of the two sets
is simply the set of shallow-parse sequences in the
50-best candidates output by the Charniak parser.
We then use the SPRep perceptron-model scores to
choose from among just these candidates. We con-
verted the 50-best lists returned by the Charniak
parser into k-best lists of shallow parses by using
chunklink to convert each candidate context-free
parse into a shallow parse. Many of the context-free
parses map to the same shallow parse, so the size of
this list is typically much less than 50, with an aver-
age of around 7. Each of the unique shallow-parse
candidates is given a score by the SPRep percep-
tron, and the best-scoring candidate is selected. Ef-
fectively, we used the Charniak parser?s k-best shal-
low parses to limit the search space for our shallow
parser.
The second combination of the system outputs,
which we call weighted intersection, extends the un-
weighted intersection by including the scores from
the Charniak parser, which are log probabilities.
The score for a shallow parse output by the Char-
niak parser is the log of the sum of the probabili-
ties of all context-free parses mapping to that shal-
low parse. We normalize across all candidates for
a given string, hence these are conditional log prob-
abilities. We multiply these conditional log proba-
bilities by a scaling factor ? before adding them to
the SPRep perceptron score for a particular candi-
date. Again, the best-scoring candidate using this
composite score is selected from among the shallow
791
NP-Chunking CoNLL-2000 Li & Roth task
Punctuation Punctuation Punctuation
System Leave Ignore Leave Ignore Leave Ignore
SPRep averaged perceptron 94.21 94.25 93.54 93.70 95.12 95.27
Charniak (2000) 94.17 94.20 93.77 93.92 95.15 95.32
Unweighted intersection 95.13 95.16 94.52 94.64 95.77 95.92
Weighted intersection 95.57 95.58 95.03 95.16 96.20 96.33
Table 3: F-measure shallow bracketing accuracy on three shallow parsing tasks, for the SPRep perceptron shallow parser, the
Charniak (2000) context-free parser, and for systems combining the SPRep and Charniak system outputs.
parse candidates output by the Charniak parser. We
used the heldout data to empirically estimate an op-
timal scaling factor for the Charniak scores, which
is 15 for all trials reported here. This factor com-
pensates for differences in the dynamic range of the
scores of the two parsers.
Both of these intersections are done at test-time,
i.e. the models are trained independently. To remain
consistent with task-specific training and testing sec-
tion conventions, the individual models were always
trained on the appropriate sections for the given task,
i.e. WSJ sections 15-18 for NP-Chunking and the
CoNLL-2000 tasks, and sections 2-21 for the Li &
Roth task.
Results from these methods of combination are
shown in the bottom two rows of Table 3. Even
the simple unweighted intersection gives quite large
improvements over each of the independent systems
for all three tasks. All of these improvements are
significant at p < 0.001 using the Matched Pair
Sentence Segment test (Gillick and Cox, 1989). The
weighted intersection gives further improvements
over the unweighted intersection for all tasks, and
this improvement is also significant at p < 0.001,
using the same test.
3.3 Robustness to Domain Shift
Our final shallow parsing task was also proposed in
Li and Roth (2001). The purpose of this task was
to examine the degradation in performance when
parsers, trained on one relatively clean domain such
as WSJ, are tested on another, mismatched domain
such as Switchboard. The systems that are reported
in this section are trained on sections 2-21 of the
WSJ Treebank, with section 24 as heldout, and
tested on section 4 of the Switchboard Treebank.
Note that the systems used here are exactly the ones
presented for the original Li & Roth task, in Sec-
Punctuation
System Leave Ignore
Li & Roth (reference tags) 88.47 -
SPRep avg perceptron
Reference tags 91.37 91.86
Brill tags 87.94 88.42
Charniak (2000) 87.94 88.44
Unweighted intersection 88.66 89.16
Weighted intersection 89.22 89.69
Table 4: Shallow bracketing accuracy of several different sys-
tems, trained on sections 2-21 of WSJ Treebank and applied
to section 4 of the Switchboard Treebank. Li and Roth (2001)
results are as reported in their paper, with reference POS tags
rather than Brill-tagger POS tags.
tions 3.1 and 3.2; only the test set has changed, train-
ing and heldout sets remain exactly the same, as do
the mixing parameters for the weighted intersection.
In the trials reported in Li and Roth (2001), both of
the evaluated systems were provided with reference
POS tags from the Switchboard Treebank. In the
current results, we show our SPRep averaged per-
ceptron system provided both with reference POS
tags for comparison with the Li and Roth results,
and provided with Brill-tagger POS tags for com-
parison with other systems. Table 4 shows our re-
sults for this task. Whereas Li and Roth reported
a more marked degradation in performance when
using a context-free parser as compared to a shal-
low parser, we again show virtually indistinguish-
able performance between our SPRep shallow parser
and the Charniak context-free parser. Again, using a
weighted combined model gave us large improve-
ments over each independent model, even in this
mismatched domain.
3.4 Reranked n-best List
Just prior to the publication of this paper, we were
able to obtain the trained reranker from Charniak
792
WSJ Sect. 00 SWBD Sect. 4
Punctuation Punctuation
System Leave Ignore Leave Ignore
SPRep 95.12 95.27 87.94 88.43
C & J one-best 95.15 95.32 87.94 88.44
(2005) reranked 95.81 96.04 88.64 89.17
Weighted intersection 96.32 96.47 89.32 89.80
Table 5: F-measure shallow bracketing accuracy when trained
on WSJ sections 2-21 and applied to either WSJ section 00 or
SWBD section 4. Systems include our shallow parser (SPRep);
the Charniak and Johnson (2005) system (C & J), both initial
one-best and reranked-best; and the weighted intersection be-
tween the reranked 50-best list and the SPRep system.
and Johnson (2005), which allows a comparison of
the shallow parsing gains that they obtain from that
system with those documented here. The reranker is
a discriminatively trained Maximum Entropy model
with an F-measure parsing accuracy objective. It
uses a large number of features, and is applied to the
50-best output from the generative Charniak parsing
model. The reranking model was trained on sections
2-21, with section 24 used as heldout. This allows us
to compare its shallow parsing accuracy with other
systems on the tasks that use this training setup: the
Li & Roth task (testing on WSJ section 00) and the
domain shift task (testing on Switchboard section
4). Table 5 shows two new trials making use of this
reranking model.
The Charniak and Johnson (2005) system out-
put (denoted C & J in the table) before rerank-
ing (denoted one-best) is identical to the Charniak
(2000) results that have been reported in the other
tables. After reranking (denoted reranked), the per-
formance improves by roughly 0.7 percentage points
for both tasks, nearly reaching the performance
that we obtained with weighted intersection of the
SPRep model and the n-best list before reranking.
Weighted intersection between the reranked list and
the shallow parser as described earlier, with a newly
estimated scaling factor (?=30), provides a roughly
0.5 percentage point increase over the result ob-
tained by the reranker. The difference between the
Charniak output before and after reranking is statis-
tically significant at p < 0.001, as is the difference
between the reranked output and the weighted inter-
section, using the same test reported earlier.
3.5 Discussion
While it may be seen to be overkill to apply a
context-free parser for these shallow parsing tasks,
we feel that these results are very interesting for
a couple of reasons. First, they go some way to-
ward correcting the misperception that context-free
parsers are less applicable in real-world scenarios
than finite-state sequence models. Finite-state mod-
els are undeniably more efficient; however, it is
important to have a clear idea of how much ac-
curacy is being sacrificed to reach that efficiency.
Any given application will need to examine the ef-
ficiency/accuracy trade-off with different objectives
for optimality. For those willing to trade efficiency
for accuracy, it is worthwhile knowing that it is pos-
sible to do much better on these tasks than what has
been reported in the past.
4 Conclusion and Future Work
In summary, we have demonstrated in this paper that
there is no accuracy or robustness benefit to shal-
low parsing with finite-state models over using high-
accuracy context-free models. Even more, there is a
large benefit to be had in combining the output of
high-accuracy context-free parsers with the output
of shallow parsers. We have demonstrated a large
improvement over the previous-best reported re-
sults on several tasks, including the well-known NP-
Chunking and CoNLL-2000 shallow parsing tasks.
Part of the misperception of the relative benefits
of finite-state and context-free models is due to dif-
ficulty evaluating across these differing annotation
styles. Mapping from context-free parser output
to the shallow constituents defined in the CoNLL-
2000 task depends on many construction-specific
operations that have unfairly penalized context-free
parsers in previous comparisons.
While the results of combining system outputs
show one benefit of combining systems, as presented
in this paper, they hardly exhaust the possibilities
of exploiting the differences between these models.
Making use of the scores for the shallow parses out-
put by the Charniak parser is a demonstrably ef-
fective way to improve performance. Yet there are
other possible features explicit in the context-free
parse candidates, such as head-to-head dependen-
cies, which might be exploited to further improve
performance. We intend to explore including fea-
tures from the context-free parser output in our per-
ceptron model to improve shallow parsing accuracy.
Another possibility is to look at improving
793
context-free parsing accuracy. Within a multi-pass
parsing strategy, the high-accuracy shallow parses
that result from system combination could be used
to restrict the search within yet another pass of a
context-free parser. That parser could then search
for the best global analysis from within just the
space of parses consistent with the provided shallow
parse. Also, features of the sort used in our shallow
parser could be included in a reranker, such as that
in Charniak and Johnson (2005), with a context-free
parsing accuracy objective.
A third possibility is to optimize the definition of
the shallow-parse phrase types themselves, for use
in other applications. The composition of the set of
phrase types put forth by Sang and Buchholz (2000)
may not be optimal for certain applications. One
such application is discourse parsing, which relies
on accurate detection of clausal boundaries. Shal-
low parsing could provide reliable information on
the location of these boundaries, but the current set
of phrase types may be too general for such use. For
example, consider infinitival verb phrases, which of-
ten indicate the start of a clause whereas other types
of verb phrases do not. Unfortunately, with only one
VP category in the CoNLL-2000 set of phrase types,
this distinction is lost. Expanding the defined set of
phrase types could benefit many applications.
Future work will also include continued explo-
ration of possible features that can be of use for ei-
ther shallow parsing models or context-free parsing
models. In addition, we intend to investigate ways
in which to encode approximations to context-free
parser derived features that can be used within finite-
state models, thus perhaps preserving finite-state ef-
ficiency while capturing at least some of the accu-
racy gain that was observed in this paper.
Acknowledgments
We would like to thank Eugene Charniak and Mark
Johnson for help with the parser and reranker doc-
umented in their paper. The first author of this pa-
per was supported under an NSF Graduate Research
Fellowship. In addition, this research was supported
in part by NSF Grant #IIS-0447214. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
Steven Abney. 1991. Parsing by chunks. In Robert Berwick,
Steven Abney, and Carol Tenny, editors, Principle-Based
Parsing. Kluwer Academic Publishers, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cascades.
Natural Language Engineering, 2(4):337?344.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4).
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of NAACL, pages
132?139.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of the 35th Annual
Meeting of ACL, pages 16?23.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th ICML Confer-
ence.
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the Conference on
EMNLP, pages 1?8.
L. Gillick and S. Cox. 1989. Some statistical issues in the com-
parison of speech recognition algorithms. In Proceedings of
ICASSP, pages 532?535.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In Proceedings of the 2nd Annual Meeting
of NAACL.
Xin Li and Dan Roth. 2001. Exploring evidence for shallow
parsing. In Proceedings of the 5th Conference on CoNLL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceedings of
the 3rd Workshop on Very Large Corpora, pages 82?94.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of the 4th Conference on CoNLL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the HLT-NAACL
Annual Meeting.
Tong Zhang, Fred Damerau, and David Johnson. 2002. Text
chunking based on a generalization of Winnow. Journal of
Machine Learning Research, 2:615?637.
794
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 647?655,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Linear Complexity Context-Free Parsing Pipelines via Chart Constraints
Brian Roark and Kristy Hollingshead
Center for Spoken Language Understanding
Division of Biomedical Computer Science
Oregon Health & Science University
{roark,hollingk}@cslu.ogi.edu
Abstract
In this paper, we extend methods from Roark
and Hollingshead (2008) for reducing the
worst-case complexity of a context-free pars-
ing pipeline via hard constraints derived from
finite-state tagging pre-processing. Methods
from our previous paper achieved quadratic
worst-case complexity. We prove here that al-
ternate methods for choosing constraints can
achieve either linear orO(N log2N) complex-
ity. These worst-case bounds on processing
are demonstrated to be achieved without re-
ducing the parsing accuracy, in fact in some
cases improving the accuracy. The new meth-
ods achieve observed performance compara-
ble to the previously published quadratic com-
plexity method. Finally, we demonstrate im-
proved performance by combining complexity
bounding methods with additional high preci-
sion constraints.
1 Introduction
Finite-state pre-processing for context-free parsing
is very common as a means of reducing the amount
of search required in the later stage. For ex-
ample, the well-known Ratnaparkhi parser (Ratna-
parkhi, 1999) used a finite-state POS-tagger and NP-
chunker to reduce the search space for his Maxi-
mum Entropy parsing model, and achieved linear
observed-time performance. Other recent examples
of the utility of finite-state constraints for parsing
pipelines include Glaysher and Moldovan (2006),
Djordjevic et al (2007), Hollingshead and Roark
(2007), and Roark and Hollingshead (2008). Note
that by making use of constraints derived from pre-
processing, they are no longer performing full exact
inference?these are approximate inference meth-
ods, as are the methods presented in this paper. Most
of these parsing pipeline papers show empirically
that these techniques can improve pipeline efficiency
for well-known parsing tasks. In contrast, in Roark
and Hollingshead (2008), we derived and applied the
finite-state constraints so as to guarantee a reduc-
tion in the worst-case complexity of the context-free
parsing pipeline from O(N3) in the length of the
string N to O(N2) by closing chart cells to entries.
We demonstrated the application of such constraints
to the well-known Charniak parsing pipeline (Char-
niak, 2000), which resulted in no accuracy loss when
the constraints were applied.
While it is important to demonstrate that these
sorts of complexity-reducing chart constraints do not
interfere with the operation of high-accuracy, state-
of-the-art parsing approaches, existing pruning tech-
niques used within such parsers can obscure the im-
pact of these constraints on search. For example, us-
ing the default search parameterization of the Char-
niak parser, the Roark and Hollingshead (2008) re-
sults demonstrated no parser speedup using the tech-
niques, rather an accuracy improvement, which we
attributed to a better use of the amount of search per-
mitted by that default parameterization. We only
demonstrated efficiency improvements by reducing
the amount of search via the Charniak search param-
eterization. There we showed a nice speedup of the
parser versus the default, while maintaining accu-
racy levels. However, internal heuristics of the Char-
niak search, such as attention shifting (Blaheta and
Charniak, 1999; Hall and Johnson, 2004), can make
this accuracy/efficiency tradeoff somewhat difficult
to interpret.
Furthermore, one might ask whetherO(N2) com-
plexity is as good as can be achieved through the
paradigm of using finite-state constraints to close
chart cells. What methods of constraint would be
required to achieve O(N logN) or linear complex-
647
ity? Would such constraints degrade performance,
or can the finite-state models be applied with suffi-
cient precision to allow for such constraints without
significant loss of accuracy?
In this paper, we adopt the same paradigm pur-
sued in Roark and Hollingshead (2008), but apply
it to an exact inference CYK parser (Cocke and
Schwartz, 1970; Younger, 1967; Kasami, 1965). We
demonstrate that imposing constraints sufficient to
achieve quadratic complexity in fact yields observed
linear parsing time, suggesting that tighter complex-
ity bounds are possible. We prove that a differ-
ent method of imposing constraints on words be-
ginning or ending multi-word constituents can give
O(N log2N) or O(N) worst-case complexity, and
we empirically evaluate the impact of such an ap-
proach.
The rest of the paper is structured as follows. We
begin with a summary of the chart cell constraint
techniques from Roark and Hollingshead (2008),
and some initial empirical trials applying these tech-
niques to an exact inference CYK parser. Complex-
ity bounding approaches are contrasted (and com-
bined) with high precision constraint selection meth-
ods from that paper. We then present a new approach
to making use of the same sort of finite-state tag-
ger output to achieve linear or N log2N complexity.
This is followed with an empirical validation of the
new approach.
2 Background: Chart Cell Constraints
The basic algorithm from Roark and Hollingshead
(2008) is as follows. Let B be the set of words in a
string w1 . . . wk that begin a multi-word constituent,
and let E be the set of words in the string that end a
multi-word constituent. For chart parsing with, say,
the CYK algorithm, cells in the chart represent sub-
strings wi . . . wj of the string, and can be indexed
with (i, j), the beginning and ending words of the
substring. If wi 6? B, then we can close any cell
(i, j) where i < j, i.e., no complete constituents
need be stored in that cell. Similarly, if wj 6? E,
then we can close any cell (i, j) where i < j. A dis-
criminatively trained finite-state tagger can be used
to classify words as being in or out of these sets
with relatively high tagging accuracy, around 97%
for both sets (B and E). The output of the tagger is
then used to close cells, thus reducing the work for
the chart parser.
An important caveat must be made about these
closed cells, related to incomplete constituents. For
simplicity of exposition, we will describe incom-
plete constituents in terms of factored categories in
a Chomsky Normal Form grammar, e.g., the new
non-terminal Z:X+W that results when the ternary
rule production Z ? Y X W is factored into
the two binary productions Z ? Y Z:X+W and
Z:X+W ? X W . A factored category such
as Z:X+W should be permitted in cell (i, j) if
wj ? E, even if wi 6? B, because the category could
subsequently combine with an Y category to create
a Z constituent that begins at some word wp ? B.
Hence there are three possible conditions for cell
(i, j) in the chart:
1. wj 6? E: closing the cell affects all con-
stituents, both complete and incomplete
2. wi 6? B and wj ? E: closing the cell affects
only complete constituents
3. wi ? B and wj ? E: cell is not closed, i.e., it
is ?open?
In Roark and Hollingshead (2008), we proved
that, for the CYK algorithm, there is no work neces-
sary for case 1 cells, a constant amount of work for
case 2 cells, and a linear amount of work for case
3 cells. Therefore, if the number of cells allowed
to fall in case 3 is linear, the overall complexity of
search is O(N2).
The amount of work for each case is related
to how the CYK algorithm performs its search.
Each cell in the chart (i, j) represents a substring
wi . . . wj , and building non-terminal categories in
that cell involves combining non-terminal categories
(via rules in the context-free grammar) found in cells
of adjacent substrings wi . . . wm and wm+1 . . . wj .
The length of substrings can be up to order N
(length of the whole string), hence there are O(N)
midpoint words wm in the standard algorithm, and
in the case 3 cells above. This accounts for the lin-
ear amount of work for those cells. Case 2 cells
have constant work because there is only one pos-
sible midpoint, and that is wi, i.e., the first child of
any incomplete constituent placed in a case 2 cell
must be span 1, since wi 6? B. This is a very con-
cise recap of the proof, and we refer the reader to
our previous paper for more details.
648
3 Constraining Exact-Inference CYK
Despite referring to the CYK algorithm in the proof,
in Roark and Hollingshead (2008) we demonstrated
our approach by constraining the Charniak parser
(Charniak, 2000), and achieved an improvement in
the accuracy/efficiency tradeoff curve. However, as
mentioned earlier, the existing complicated system
of search heuristics in the Charniak parser makes in-
terpretation of the results more difficult. What can
be said from the previous results is that constraining
parsers in this way can improve performance of even
the highest accuracy parsers. Yet those results do not
provide much of an indication of how performance
is impacted for general context-free inference.
For this paper, we use an exact inference (exhaus-
tive search) CYK parser, using a simple probabilis-
tic context-free grammar (PCFG) induced from the
Penn WSJ Treebank (Marcus et al, 1993). The
PCFG is transformed to Chomsky Normal Form
through right-factorization, and is smoothed with a
Markov (order-2) transform. Thus a production such
as Z ? Y X W V becomes three rules: (1)
Z ? Y Z:X+W ; (2) Z:X+W ? X Z:W+V ;
and (3) Z:W+V ? W V . Note that only two child
categories are encoded within the new factored cate-
gories, instead of all of the remaining children as in
our previous factorization example. This so-called
?Markov? grammar provides some smoothing of the
PCFG; the resulting grammar is also smoothed us-
ing lower order Markov grammars.
We trained on sections 2-21 of the treebank, and
all results except for the final table are on the devel-
opment section (24). The final table is on the test
section (23). All results report F-measure labeled
bracketing accuracy for all sentences in the section.
To close cells, we use a discriminatively trained
finite-state tagger to tag words as being either in B
or not, and also (in a separate pass) either in E or
not. Note that the reference tags for each word can
be derived directly from the treebank, based on the
spans of constituents beginning (or ending) at each
word. Note also that these reference tags are based
on a non-factored grammar.
For example, consider the chart in Figure 1 for the
five symbol string ?abcde?. Each cell in the chart is
labeled with the substring that the cell spans, along
with the begin and end indices of the substring, e.g.,
(3, 5) spans the third symbol to the fifth symbol:
abcde
(1, 5)
abcd
(1, 4)
bcde
(2, 5)
abc
(1, 3)
bcd
(2, 4)
cde
(3, 5)
ab
(1, 2)
bc
(2, 3)
cd
(3, 4)
de
(4, 5)
a
(1, 1)
b
(2, 2)
c
(3, 3)
d
(4, 4)
e
(5, 5)
Figure 1: Fragment of a chart structure. Each cell is labeled
with the substring spanned by that cell, along with the start and
end word indices. Cell shading reflects b 6? E and d 6? E
constraints: black denotes ?closed? cells; white and gray are
?open?; gray cells have ?closed? children cells, reducing the
number of midpoints requiring processing.
cde. If our tagger output is such that b 6? E and
d 6? E, then four cells will be closed: (1, 2), (1, 4),
(2, 4) and (3, 4). The gray shaded cells in the figure
have some midpoints that require no work, because
they involve closed children cells.
4 Constraint Selection
4.1 High Precision vs Complexity Bounding
The chart constraints that are extracted from the
finite-state tagger come in the form of set exclu-
sions, e.g., d 6? E. Rather than selecting constraints
from the single, best-scoring tag sequence output by
the tagger, we instead rely on the whole distribu-
tion over possible tag strings to select constraints.
We have two separate tagging tasks, each with two
possible tags of each word wi in each string: (1) B
or ?B; and (2) E or ?E, where ?X signifies that
wi 6? X for X ? {B,E}. The tagger (Holling-
shead et al, 2005) uses log linear models trained
with the perceptron algorithm, and derives, via the
forward-backward algorithm, the posterior probabil-
ity of each of the two tags at each word, so that
Pr(B) + Pr(?B) = 1. Then, for every word wi
in the string, the tags B and E are associated with a
posterior probability that gives us a score forwi ? B
and wi ? E. All possible set memberships wi ? X
in the string can be ranked by this score. From this
ranking, a decision boundary can be set, such that
all word/set pairs wi ? B or wj ? E with above-
threshold probability are accepted, and all pairs be-
low threshold are excluded from the set.
The default decision boundary for this tagging
649
task is 0.5 posterior probability (more likely than
not), and tagging performance at that threshold is
good (around 97% accuracy, as mentioned previ-
ously). However, since this is a pre-processing step,
we may want to reduce possible cascading errors by
allowing more words into the sets B and E. In
other words, we may want more precision in our
set exclusion constraints. One method for this is to
count the number c of word/set pairs below poste-
rior probability of 0.5, then set the threshold so that
only kc word/set pairs fall below threshold, where
0 < k ? 1. Note that the closer the parameter k
is to 0, the fewer constraints will be applied to the
chart. We refer to the resulting constraints as ?high
precision?, since the selected constraints (set exclu-
sions) have high precision. This technique was also
used in the previous paper.
We also make use of the ranked list of word/set
pairs to impose quadratic bounds on context-free
parsing. Starting from the top of the list (high-
est posterior probability for set inclusion), word/set
pairs are selected and the number of open cells (case
3 in Section 2) calculated. When the accumulated
number of open cells reaches kN for sentence length
N , the decision threshold is set. In such a way, there
are only a linear number of open, case 3 cells, hence
the parsing has quadratic worst-case complexity.
For both of these methods, the parameter k can
vary, allowing for more or less set inclusion. Fig-
ure 2 shows parse time versus F-measure parse ac-
curacy on the development set for the baseline (un-
constrained) exact-inference CYK parser, and for
various parameterizations of both the high preci-
sion constraints and the quadratic bound constraints.
Note that accuracy actually improves with the im-
position of these constraints. This is not surpris-
ing, since the finite-state tagger deriving the con-
straints made use of lexical information that the sim-
ple PCFG did not, hence there is complementary in-
formation improving the model. The best operating
points?fast parsing and relatively high accuracy?
are achieved with 90% of the high precision con-
straints, and 5N cells left open. These achieve a
roughly 20 times speedup over the baseline uncon-
strained parser and achieve between 1.5 and 3 per-
cent accuracy gains over the baseline.
We can get a better picture of what is going on by
considering the scatter plots in Figure 3, which plot
0 500 1000 1500 2000 2500 3000 350065
70
75
80
Seconds to parse section
F?m
easu
re a
ccur
acy
 
 
Baseline exact inferenceHigh precision constraintsO(N2) complexity bounds
Figure 2: Time to parse (seconds) versus accuracy (F-measure)
for the baseline of exact inference (no constraints) versus
two methods of imposing constraints with varying parameters:
(1) High precision constraints; (2) Sufficient constraints to im-
pose O(N2) complexity (the number of open cells ? kN ).
each sentence according to its length versus the pars-
ing time for that sentence at three operating points:
baseline (unconstrained); high precision at 90%; and
quadratic with 5N open cells. The top plot shows up
to 120 words in the sentence, and up to 5 seconds of
parsing time. The middle graph zooms in to under
1 second and up to 60 words; and the lowest graph
zooms in further to under 0.1 seconds and up to 20
words. It can be seen in each graph that the uncon-
strained CYK parsing quickly leaves the graph via a
steep cubic curve.
Three points can be taken away from these plots.
First, the high precision constraints are better for
the shorter strings than the quadratic bound con-
straints (see bottom plot); yet with the longer strings,
the quadratic constraints better control parsing time
than the high precision constraints (see top plot).
Second, the quadratic bound constraints appear to
actually result in roughly linear parsing time, not
quadratic. Finally, at the ?crossover? point, where
quadratic constraints start out-performing the high
precision constraints (roughly 40-60 words, see mid-
dle plot), there is quite high variance in high preci-
sion constraints versus the quadratic bounds: some
sentences process more quickly than the quadratic
bounds, some quite a bit worse. This illustrates
the difference between the two methods of select-
ing constraints: the high precision constraints can
provide very strong gains, but there is no guarantee
for the worst case. In such a way, the high preci-
sion constraints are similar to other tagging-derived
650
0 20 40 60 80 100 1200
1
2
3
4
5
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
0 10 20 30 40 50 600
0.2
0.4
0.6
0.8
1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
0 5 10 15 200
0.02
0.04
0.06
0.08
0.1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsHigh precision constraints (90%)O(N2) parsing (open cells ? 5N)
Figure 3: Scatter plots of sentence length versus parsing time
for (1) baseline exact inference (no constraints); (2) high pre-
cision begin- and end-constituent constraints (90% level); and
(3) O(N2) constraints (5N open cells).
constraints like POS-tags or chunks.
4.2 Combining Constraints
Depending on the length of the string, the quadratic
constraints may close more or fewer chart cells
than the high precision constraints?more for long
strings, fewer for short strings. We can achieve
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 74.1 3646
High Precision (90%) 77.0 181
Quadratic (5N ) 75.7 317
Quad (5N ) + HiPrec (90%) 76.9 166
Table 1: Speed and accuracy of exact-inference CYK parser
on WSJ section 24 under various constraint conditions, includ-
ing combining quadratic bound constraints and high precision
constraints.
worst-case bounds, along with superior typical case
speedups, by combining both methods as follows:
first apply the quadratic bounds; then, if there are
any high precision constraints that remain unap-
plied, add them. Table 1 shows F-measure accuracy
and parsing time (in seconds) for four trials on the
development set: the baseline CYK with no con-
straints; high precision constraints at the 90% level;
quadratic bound constraints at the 5N level; and a
combination of the quadratic bound and high preci-
sion constraints. We can see that, indeed, the com-
bination of the two yield speedups over both inde-
pendently, with no significant drop in accuracy from
the high precision constraints alone. Further results
with worst-case complexity bounds will be com-
bined with high precision constraints in this way.
The observed linear parsing time in Figure 3 with
the quadratic constraints raises the following ques-
tion: can we apply these constraints in a way that
guarantees linear complexity? The answer is yes,
and this is the subject of the next section.
5 Linear andN log2N Complexity Bounds
Given the two setsB and E, recall the three cases of
chart cells (i, j) presented in Section 2: 1) wj 6? E
(cell completely closed); 2) wj ? E and wi 6? B
(cell open only for incomplete constituents); and 3)
wi ? B and wj ? E (cell open for all constituents).
Quadratic worst-case complexity is achieved with
these sets by limiting case 3 to hold for only O(N)
cells?each with linear work?and the remaining
O(N2) cells (cases 1 and 2) have none or constant
work, hence overall quadratic (Roark and Holling-
shead, 2008).
One might ask: why would imposing constraints
to achieve a quadratic bound give us linear observed
parsing time? One possibility is that the linear num-
ber of case 3 cells don?t have a linear amount of
work, but rather a constant bounded amount of work.
651
If there were a constant bounded number of mid-
points, then the amount of work associated with case
3 would be linear. Note that a linear complexity
bound would have to guarantee a linear number of
case 2 cells as well since there is a constant amount
of work associated with case 2 cells.
To provide some intuition as to why the quadratic
bound method resulted in linear observed parsing
time, consider again the chart structure in Figure 1.
The black cells in the chart represent the cells that
have been closed when wj 6? E (case 1 cells). In
our example, w2 6? E caused the cell spanning ab
to be closed, and w4 6? E caused the cells span-
ning abcd, bcd and cd to be closed. Since there is
no work required for these cells, the amount of work
required to parse the sentence is reduced. However,
the quadratic bound does not include any potential
reduced work in the remaining open cells. The gray
cells in the chart are cells with a reduced number of
possible midpoints, as effected by the closed cells
in the chart. For example, categories populating the
cell spanning abc in position (1, 3) can be built in
two ways: either by combining entries in cell (1, 1)
with entries in (2, 3) at midpoint m = 1; or by com-
bining entries in (1, 2) and (3, 3) at midpointm = 2.
However, cell (1, 2) is closed, hence there is only
one midpoint at which (1, 3) can be built (m = 1).
Thus the amount of work to parse the sentence will
be less than the worst-case quadratic bound based on
this processing savings in open cells.
While imposition of the quadratic bound may
have resulted (fortuitously) in constant bounded
work for case 3 cells and a linear number of case
2 cells, there is no guarantee that this will be the
case. One method to guarantee that both conditions
are met is the following: if |E| ? k for some con-
stant k, then both conditions will be met and parsing
complexity will be linear. We prove here that con-
straining E to contain a constant number of words
results in linear complexity.
Lemma 1: If |E| ? k for some k, then the
amount of work for any cell is bounded by ck
for some constant c (grammar constant).
Proof: Recall from Section 2 that for each cell
(i, j), there are j?i midpoints m that require com-
bining entries in cells (i,m) and (m+1, j) to create
entries in cell (i, j). If m > i, then cell (i,m) is
empty unless wm ? E. If cell (i,m) is empty, there
is no work to be done at that midpoint. If |E| ? k,
then there are a maximum of k midpoints for any
cell, hence the amount of work is bounded by ck for
some constant c.2
Lemma 2: If |E| ? k for some k, then the num-
ber of cells (i, j) such that wj ? E is no more
than kN where N is the length of the string.
Proof: For a string of length N , each word wj in
the string has at most N cells such that wj is the
last word in the substring spanned by that cell, since
each such cell must begin with a distinct word wi in
the string where i ? j, of which there are at mostN .
Therefore, if |E| ? k for some k, then the number
of cells (i, j) such that wj ? E would be no more
than kN .2
Theorem: If |E| ? k, then the parsing complex-
ity is O(k2N).
Proof: As stated earlier, each cell (i, j) falls in one
of three cases: 1) wj 6? E; 2) wj ? E and wi 6? B;
and 3) wi ? B and wj ? E. Case 1 cells are com-
pletely closed, there is no work to be done in those
cells. By Lemma 2, there are at maximum kN cells
that fall in either case 2 or case 3. By Lemma 1, the
amount of work for each of these cells is bounded
by ck for some constant c. Therefore, the theorem is
proved.2
If |E| ? k for a constant k, the theorem proves
the complexity will be O(N). If |E| ? k logN ,
then parsing complexity will be O(N log2N). Fig-
ure 4 shows sentence length versus parsing time
under three different conditions1: baseline (uncon-
strained); O(N log2N) at |E| ? 3 logN ; and linear
at |E| ? 16. The bottom graph zooms in to demon-
strate that the O(N log2N) constraints can outper-
form the linear constraints for shorter strings (see
around 20 words). As the length of the string in-
creases, though, the performance lines cross, and the
linear constraints demonstrate higher efficiency for
the longer strings, as expected.
Unlike the method for imposing quadratic
bounds, this method only makes use of set E, not
B. To select the constraints, we rank the word/E
posterior probabilities, and choose the top k (either
constant or scaled with a logN factor); the rest of
the words fall outside of the set. In this approach,
1Selection of these particular operating points for the
N log2N and linear methods is discussed in Section 6.
652
0 20 40 60 80 100 1200
2
4
6
8
10
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)
0 10 20 30 400
0.2
0.4
0.6
0.8
1
Sentence length in words
Pars
ing t
ime 
in se
cond
s
 
 No constraintsO(Nlog2N) parsing (k=3)O(N) parsing (k=16)
Figure 4: Scatter plots of sentence length versus pars-
ing time for (1) baseline exact inference (no constraints);
(2) O(N log2N) constraints; and (3) O(N) constraints.
every word falls in the B set, hence no constraints
on words beginning multi-word constituents are im-
posed.
6 Results
Figure 5 plots F-measure accuracy versus time to
parse the development set for four methods of
imposing constraints: the previously plotted high
precision and quadratic bound constraints, along
with O(N log2N) and linear bound constraints us-
ing methods described in this paper. All meth-
ods are employed at various parameterizations, from
very lightly constrained to very heavily constrained.
The complexity-bound constraints are not combined
with the high-precision constraints for this plot.
As can be seen from the plot, the linear and
O(N log2N) methods do not, as applied, achieve as
favorable of an accuracy/efficiency tradeoff curve as
the quadratic bound method. This is not surprising,
0 200 400 600 80055
60
65
70
75
80
Seconds to parse section
F?m
easu
re a
ccur
acy
 
 
High precision constraintsO(N2) complexity boundsO(Nlog2N) complexity boundsO(N) complexity bounds
Figure 5: Time to parse (seconds) versus accuracy (F-
measure) for high precision constraints of various thresholds
versus three methods of imposing constraints with complexity
bounds: (1) O(N2) complexity (number of open cells ? kN );
(2) O(N log2N) complexity (|E| ? k logN ); and (3) linear
complexity (|E| ? k).
given that no words are excluded from the set B
for these methods, hence far fewer constraints over-
all are applied with the new method than with the
quadratic bound method.
Of course, the high precision constraints can be
applied together with the complexity bound con-
straints, as described in Section 4.2. For combining
complexity-bound constraints with high-precision
constraints, we first chose operating points for both
the linear and O(N log2N) complexity bound meth-
ods at the points before accuracy begins to de-
grade with over-constraint. For the linear complex-
ity method, the operating point is to constrain the
set size of E to a maximum of 16 members, i.e.,
|E| ? 16. For the N log2N complexity method,
|E| ? 3 logN .
Table 2 presents results for these operating points
used in conjunction with the 90% high precision
constraints. For these methods, this combination
is particularly important, since it includes all of the
high precision constraints from the set B, which are
completely ignored by both of the new methods. We
can see from the results in the table that the com-
bination brings the new constraint methods to very
similar accuracy levels as the quadratic constraints,
yet with the guarantee of scaling linearly to longer
and longer sentences.
The efficiency benefits of combining constraints,
shown in Table 2, are relatively small here because
the dataset contains mostly shorter sentences. Space
653
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 74.1 3646
High Precision (90%) 77.0 181
Quad (5N ) + HiPrec (90%) 76.9 166
N log2N (3logN ) + HP (90) 76.9 170
Linear (16) + HiPrec (90%) 76.8 167
Table 2: Speed and accuracy of exact-inference CYK parser
on WSJ section 24 under various constraint conditions, includ-
ing combining various complexity bound constraints and high
precision constraints.
limitations prevent us from including scatter plots
similar to those in Figure 3 for the constraint combi-
nation trials, which show that the observed parsing
time of shorter sentences is typically identical under
each constraint set, while the parsing time of longer
sentences tends to differ more under each condition
and exhibit characteristics of the complexity bounds.
Thus by combining high-precision and complexity
constraints, we combine typical-case efficiency ben-
efits with worst-case complexity bounds.
Note that these speedups are achieved with no
additional techniques for speeding up search, i.e.,
modulo the cell closing mechanism, the CYK pars-
ing is exhaustive?it explores all possible category
combinations from the open cells. Techniques such
as coarse-to-fine or A? parsing, the use of an agenda,
or setting of probability thresholds on entries in
cells?these are all orthogonal to the current ap-
proach, and could be applied together with them
to achieve additional speedups. However, none
of these other techniques provide what the current
methods do: a complexity bound that will hold even
in the worst case.
To validate the selected operating points on a dif-
ferent section, Table 3 presents speed and accuracy
results on the test set (WSJ section 23) for the exact-
inference CYK parser.
We also conducted similar preliminary trials for
parsing the Penn Chinese Treebank (Xue et al,
2004), which contains longer sentences and differ-
ent branching characteristics in the induced gram-
mar. Results are similar to those shown here, with
chart constraints providing both efficiency and ac-
curacy gains.
7 Conclusion
We have presented a method for constraining a
context-free parsing pipeline that provably achieves
F-measure time
Constraints accuracy (seconds)
None (baseline CYK) 73.8 5122
High Precision (90%) 76.8 272
Quad (5N ) + HiPrec (90%) 76.8 263
N log2N (3logN ) + HP (90) 76.8 266
Linear (16) + HiPrec (90%) 76.8 264
Table 3: Speed and accuracy of exact-inference CYK parser
on WSJ section 23 under various constraint conditions, includ-
ing combining various complexity bound constraints and high
precision constraints.
linear worst case complexity. Our method achieves
comparable observed performance to the quadratic
complexity method previously published in Roark
and Hollingshead (2008). We were motivated to
pursue this method by the observed linear parsing
time achieved with the quadratic bound constraints,
which suggested that a tighter complexity bound
could be achieved without hurting performance.
We have also shown that combining methods for
achieving complexity bounds?which are of pri-
mary utility for longer strings?with methods for
achieving strong observed typical case speedups can
be profitable, even for shorter strings. The result-
ing combination achieves both typical speedups and
worst-case bounds on processing.
The presented methods may not be the only way
to achieve these bounds using tagger pre-processing
of this sort, though they do have the virtue of
very simple constraint selection. More complicated
methods that track, in fine detail, how many cells
are open versus closed, run the risk of a constraint
selection process that is itself quadratic in the length
of the string, given that there are a quadratic number
of chart cells. Even so, the presented methods criti-
cally control midpoints for all cells only via the set
E (words that can end a multi-word constituent) and
ignoreB. More complicated methods for using both
sets that also achieve linear complexity (perhaps
with a smaller constant), or that achieve O(N logN)
complexity rather than O(N log2N), may exist.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0447214 and DARPA grant #HR0011-08-1-
0016. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
654
References
D. Blaheta and E. Charniak. 1999. Automatic compen-
sation for parser figure-of-merit flaws. In Proceedings
of the 37th annual meeting of the Association for Com-
putational Linguistics (ACL), pages 513?518.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 132?139.
J. Cocke and J.T. Schwartz. 1970. Programming lan-
guages and their compilers: Preliminary notes. Tech-
nical report, Courant Institute of Mathematical Sci-
ences, NYU.
B. Djordjevic, J.R. Curran, and S. Clark. 2007. Im-
proving the efficiency of a wide-coverage CCG parser.
In Proceedings of the 10th International Workshop on
Parsing Technologies (IWPT), pages 39?47.
E. Glaysher and D. Moldovan. 2006. Speeding up full
syntactic parsing by leveraging partial parsing deci-
sions. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 295?300.
K. Hall and M. Johnson. 2004. Attention shifting for
parsing speech. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 40?46.
K. Hollingshead and B. Roark. 2007. Pipeline iteration.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
952?959.
K. Hollingshead, S. Fisher, and B. Roark. 2005.
Comparing and combining finite-state and context-
free parsers. In Proceedings of the Human Lan-
guage Technology Conference and the Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 787?794.
T. Kasami. 1965. An efficient recognition and syntax
analysis algorithm for context-free languages. Techni-
cal report, AFCRL-65-758, Air Force Cambridge Re-
search Lab., Bedford, MA.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING), pages 745?
752.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2004. The
Penn Chinese treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
10(4):1?30.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
655
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 952?959,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Pipeline Iteration
Kristy Hollingshead and Brian Roark
Center for Spoken Language Understanding, OGI School of Science & Engineering
Oregon Health & Science University, Beaverton, Oregon, 97006 USA
{hollingk,roark}@cslu.ogi.edu
Abstract
This paper presents pipeline iteration, an ap-
proach that uses output from later stages
of a pipeline to constrain earlier stages of
the same pipeline. We demonstrate sig-
nificant improvements in a state-of-the-art
PCFG parsing pipeline using base-phrase
constraints, derived either from later stages
of the parsing pipeline or from a finite-
state shallow parser. The best performance
is achieved by reranking the union of un-
constrained parses and relatively heavily-
constrained parses.
1 Introduction
A ?pipeline? system consists of a sequence of pro-
cessing stages such that the output from one stage
provides the input to the next. Each stage in such a
pipeline identifies a subset of the possible solutions,
and later stages are constrained to find solutions
within that subset. For example, a part-of-speech
tagger could constrain a ?base phrase? chunker (Rat-
naparkhi, 1999), or the n-best output of a parser
could constrain a reranker (Charniak and Johnson,
2005). A pipeline is typically used to reduce search
complexity for rich models used in later stages, usu-
ally at the risk that the best solutions may be pruned
in early stages.
Pipeline systems are ubiquitous in natural lan-
guage processing, used not only in parsing (Rat-
naparkhi, 1999; Charniak, 2000), but also machine
translation (Och and Ney, 2003) and speech recogni-
tion (Fiscus, 1997; Goel et al, 2000), among others.
Despite the widespread use of pipelines, they have
been understudied, with very little work on gen-
eral techniques for designing and improving pipeline
systems (although cf. Finkel et al (2006)). This pa-
per presents one such general technique, here ap-
plied to stochastic parsing, whereby output from
later stages of a pipeline is used to constrain earlier
stages of the same pipeline. To our knowledge, this
is the first time such a pipeline architecture has been
proposed.
It may seem surprising that later stages of a
pipeline, already constrained to be consistent with
the output of earlier stages, can profitably inform
the earlier stages in a second pass. However, the
richer models used in later stages of a pipeline pro-
vide a better distribution over the subset of possible
solutions produced by the early stages, effectively
resolving some of the ambiguities that account for
much of the original variation. If an earlier stage is
then constrained in a second pass not to vary with re-
spect to these resolved ambiguities, it will be forced
to find other variations, which may include better so-
lutions than were originally provided.
To give a rough illustration, consider the Venn di-
agram in Fig. 1(i). Set A represents the original sub-
set of possible solutions passed along by the earlier
stage, and the dark shaded region represents high-
probability solutions according to later stages. If
some constraints are then extracted from these high-
probability solutions, defining a subset of solutions
(S) that rule out some of A, the early stage will be
forced to produce a different set (B). Constraints
derived from later stages of the pipeline focus the
search in an area believed to contain high-quality
candidates.
Another scenario is to use a different model al-
together to constrain the pipeline. In this scenario,
(i) (ii)
A
B
S A
B
S
Figure 1: Two Venn diagrams, representing (i) constraints
derived from later stages of an iterated pipelined system; and
(ii) constraints derived from a different model.
952
represented in Fig. 1(ii), the other model constrains
the early stage to be consistent with some subset of
solutions (S), which may be largely or completely
disjoint from the original set A. Again, a different set
(B) results, which may include better results than A.
Whereas when iterating we are guaranteed that the
new subset S will overlap at least partially with the
original subset A, that is not the case when making
use of constraints from a separately trained model.
In this paper, we investigate pipeline iteration
within the context of the Charniak and Johnson
(2005) parsing pipeline, by constraining parses to
be consistent with a base-phrase tree. We derive
these base-phrase constraints from three sources: the
reranking stage of the parsing pipeline; a finite-state
shallow parser (Hollingshead et al, 2005); and a
combination of the output from these two sources.
We compare the relative performance of these three
sources and find the best performance improvements
using constraints derived from a weighted combina-
tion of shallow parser output and reranker output.
The Charniak parsing pipeline has been exten-
sively studied over the past decade, with a num-
ber of papers focused on improving early stages of
the pipeline (Charniak et al, 1998; Caraballo and
Charniak, 1998; Blaheta and Charniak, 1999; Hall
and Johnson, 2004; Charniak et al, 2006) as well
as many focused on optimizing final parse accuracy
(Charniak, 2000; Charniak and Johnson, 2005; Mc-
Closky et al, 2006). This focus on optimization has
made system improvements very difficult to achieve;
yet our relatively simple architecture yields statisti-
cally significant improvements, making pipeline it-
eration a promising approach for other tasks.
2 Approach
Our approach uses the Charniak state-of-the-art
parsing pipeline. The well-known Charniak (2000)
coarse-to-fine parser is a two-stage parsing pipeline,
in which the first stage uses a vanilla PCFG to pop-
ulate a chart of parse constituents. The second
stage, constrained to only those items in the first-
stage chart, uses a refined grammar to generate an
n-best list of parse candidates. Charniak and John-
son (2005) extended this pipeline with a discrimina-
tive maximum entropy model to rerank the n-best
parse candidates, deriving a significant benefit from
the richer model employed by the reranker.
For our experiments, we modified the parser1 to
1ftp://ftp.cs.brown.edu/pub/nlparser/
Base Shallow
Parser Phrases Phrases
Charniak parser-best 91.9 94.4
reranker-best 92.8 94.8
Finite-state shallow parser 91.7 94.3
Table 1: F-scores on WSJ section 24 of output from two
parsers on the similar tasks of base-phrase parsing and shallow-
phrase parsing. For evaluation, base and shallow phrases are
extracted from the Charniak/Johnson full-parse output.
allow us to optionally provide base-phrase trees to
constrain the first stage of parsing.
2.1 Base Phrases
Following Ratnaparkhi (1999), we define a base
phrase as any parse node with only preterminal chil-
dren. Unlike the shallow phrases defined for the
CoNLL-2000 Shared Task (Tjong Kim Sang and
Buchholz, 2000), base phrases correspond directly
to constituents that appear in full parses, and hence
can provide a straightforward constraint on edges
within a chart parser. In contrast, shallow phrases
collapse certain non-constituents?such as auxiliary
chains?into a single phrase, and hence are not di-
rectly applicable as constraints on a chart parser.
We have two methods for deriving base-phrase
annotations for a string. First, we trained a finite-
state shallow parser on base phrases extracted from
the Penn Wall St. Journal (WSJ) Treebank (Marcus
et al, 1993). The treebank trees are pre-processed
identically to the procedure for training the Charniak
parser, e.g., empty nodes and function tags are re-
moved. The shallow parser is trained using the per-
ceptron algorithm, with a feature set nearly identical
to that from Sha and Pereira (2003), and achieves
comparable performance to that paper. See Holling-
shead et al (2005) for more details. Second, base
phrases can be extracted from the full-parse output
of the Charniak and Johnson (2005) reranker, via a
simple script to extract nodes with only preterminal
children.
Table 1 shows these systems? bracketing accu-
racy on both the base-phrase and shallow parsing
tasks for WSJ section 24; each system was trained
on WSJ sections 02-21. From this table we can
see that base phrases are substantially more difficult
than shallow phrases to annotate. Output from the
finite-state shallow parser is roughly as accurate as
output extracted from the Charniak parser-best trees,
though a fair amount below output extracted from
the reranker-best trees.
In addition to using base phrase constraints from
these two sources independently, we also looked at
953
combining the predictions of both to obtain more re-
liable constraints. We next present a method of com-
bining output from multiple parsers based on com-
bined precision and recall optimization.
2.2 Combining Parser n-best Lists
In order to select high-likelihood constraints for the
pipeline, we may want to extract annotations with
high levels of agreement (?consensus hypotheses?)
between candidates. In addition, we may want to
favor precision over recall to avoid erroneous con-
straints within the pipeline as much as possible.
Here we discuss how a technique presented in Good-
man?s thesis (1998) can be applied to do this.
We will first present this within a general chart
parsing approach, then move to how we use it for n-
best lists. Let T be the set of trees for a particular
input, and let a parse T ? T be considered as a set
of labeled spans. Then, for all labeled spans X ? T ,
we can calculate the posterior probability ?(X) as
follows:
?(X) =
?
T?T
P(T )JX ? T K
?
T ??T P(T ?)
(1)
where JX ? T K =
{
1 if X ? T
0 otherwise.
Goodman (1996; 1998) presents a method for us-
ing the posterior probability of constituents to maxi-
mize the expected labeled recall of binary branching
trees, as follows:
T? = argmax
T?T
?
X?T
?(X) (2)
Essentially, find the tree with the maximum sum of
the posterior probabilities of its constituents. This
is done by computing the posterior probabilities
of constituents in a chart, typically via the Inside-
Outside algorithm (Baker, 1979; Lari and Young,
1990), followed by a final CYK-like pass to find the
tree maximizing the sum.
For non-binary branching trees, where precision
and recall may differ, Goodman (1998, Ch.3) pro-
poses the following combined metric for balancing
precision and recall:
T? = argmax
T?T
?
X?T
(?(X)? ?) (3)
where ? ranges from 0 to 1. Setting ?=0 is equiv-
alent to Eq. 2 and thus optimizes recall, and setting
?=1 optimizes precision; Appendix 5 at the end of
this paper presents brief derivations of these met-
rics.2 Thus, ? functions as a mixing factor to balance
recall and precision.
This approach also gives us a straightforward way
to combine n-best outputs of multiple systems. To
do this, we construct a chart of the constituents in the
trees from the n-best lists, and allow any combina-
tion of constituents that results in a tree ? even one
with no internal structure. In such a way, we can
produce trees that only include a small number of
high-certainty constituents, and leave the remainder
of the string unconstrained, even if such trees were
not candidates in the original n-best lists.
For simplicity, we will here discuss the combina-
tion of two n-best lists, though it generalizes in the
obvious way to an arbitrary number of lists. Let T
be the union of the two n-best lists. For all trees
T ? T , let P1(T ) be the probability of T in the first
n-best list, andP2(T ) the probability of T in the sec-
ond n-best list. Then, we define P(T ) as follows:
P(T ) = ?
P1(T )
?
T ??T P1(T ?)
+
P2(T )
?
T ??T P2(T ?)
(4)
where the parameter ? dictates the relative weight of
P1 versus P2 in the combination.3
For this paper, we combined two n-best lists of
base-phrase trees. Although there is no hierarchi-
cal structure in base-phrase annotations, they can be
represented as flat trees, as shown in Fig. 2(a). We
constructed a chart from the two lists being com-
bined, using Eq. 4 to define P(T ) in Eq. 1. We wish
to consider every possible combination of the base
phrases, so for the final CYK-like pass to find the
argmax tree, we included rules for attaching each
preterminal directly to the root of the tree, in addi-
tion to rules permitting any combination of hypoth-
esized base phrases.
Consider the trees in Fig. 2. Figure 2(a) is a
shallow parse with three NP base phrases; Figure
2(b) is the same parse where the ROOT produc-
tion has been binarized for the final CYK-like pass,
which requires binary productions. If we include
productions of the form ?ROOT ? X ROOT? and
?ROOT ? X Y? for all non-terminals X and Y (in-
cluding POS tags), then any tree-structured com-
bination of base phrases hypothesized in either n-
2Our notation differs slightly from that in Goodman (1998),
though the approaches are formally equivalent.
3Note that P1 and P2 are normalized in eq. 4, and thus are
not required to be true probabilities. In turn, P is normalized
when used in eq. 1, such that the posterior probability ? is a
true probability. Hence P need not be normalized in eq. 4.
954
(a)
ROOT




 
 
@
@
PP
PP
PP
P
NP
 HH
DT
the
NN
broker
VBD
sold
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
(b)
ROOT


H
HH
NP
 HH
DT
the
NN
broker
ROOT
 HH
VBD
sold
ROOT

 H
H
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
(c)
S



H
H
HH
NP
 HH
DT
the
NN
broker
VP




H
H
H
H
VBD
sold
NP
 HH
DT
the
NNS
stocks
NP
NN
yesterday
Figure 2: Base-phrase trees (a) as produced for an n-best list and (b) after root-binarization for n-best list combination. Full-parse
tree (c) consistent with constraining base-phrase tree (a).
87 88 89 90 91 92 93 94 95 96 9786
87
88
89
90
91
92
93
94
95
96
precision
reca
ll
Charniak ? reranked (solid viterbi)
Finite?state shallow parser (solid viterbi)Charniak reranked + Finite?state
Figure 3: The tradeoff between recall and precision using a
range of ? values (Eq. 3) to select high-probability annotations
from an n-best list. Results are shown on 50-best lists of base-
phrase parses from two parsers, and on the combination of the
two lists.
best list is allowed, including the one with no base
phrases at all. Note that, for the purpose of finding
the argmax tree in Eq. 3, we only sum the posterior
probabilities of base-phrase constituents, and not the
ROOT symbol or POS tags.
Figure 3 shows the results of performing this com-
bined precision/recall optimization on three separate
n-best lists: the 50-best list of base-phrase trees ex-
tracted from the full-parse output of the Charniak
and Johnson (2005) reranker; the 50-best list output
by the Hollingshead et al (2005) finite-state shallow
parser; and the weighted combination of the two lists
at various values of ? in Eq. 3. For the combination,
we set ?=2 in Eq. 4, with the Charniak and Johnson
(2005) reranker providing P1, effectively giving the
reranker twice the weight of the shallow parser in
determining the posteriors. The shallow parser has
perceptron scores as weights, and the distribution of
these scores after a softmax normalization was too
peaked to be of utility, so we used the normalized
reciprocal rank of each candidate as P2 in Eq. 4.
We point out several details in these results.
First, using this method does not result in an F-
measure improvement over the Viterbi-best base-
phrase parses (shown as solid symbols in the graph)
for either the reranker or shallow parser. Also, us-
ing this model effects a greater improvement in pre-
cision than in recall, which is unsurprising with
these non-hierarchical annotations; unlike full pars-
ing (where long sequences of unary productions can
improve recall arbitrarily), in base-phrase parsing,
any given span can have only one non-terminal. Fi-
nally, we see that the combination of the two n-best
lists improves over either list in isolation.
3 Experimental Setup
For our experiments we constructed a simple parsing
pipeline, shown in Fig. 4. At the core of the pipeline
is the Charniak and Johnson (2005) coarse-to-fine
parser and MaxEnt reranker, described in Sec. 2.
The parser constitutes the first and second stages of
our pipeline, and the reranker the final stage. Fol-
lowing Charniak and Johnson (2005), we set the
parser to output 50-best parses for all experiments
described here. We constrain only the first stage of
the parser: during chart construction, we disallow
any constituents that conflict with the constraints, as
described in detail in the next section.
3.1 Parser Constraints
We use base phrases, as defined in Sec. 2.1, to con-
strain the first stage of our parsing pipeline. Under
these constraints, full parses must be consistent with
the base-phrase tree provided as input to the parser,
i.e., any valid parse must contain all of the base-
phrase constituents in the constraining tree. The
full-parse tree in Fig. 2(c), for example, is consis-
tent with the base-phrase tree in Fig. 2(a).
Implementing these constraints in a parser is
straightforward, one of the advantages of using base
phrases as constraints. Since the internal structure
of base phrases is, by definition, limited to preter-
minal children, we can constrain the entire parse by
constraining the parents of the appropriate pretermi-
nal nodes. For any preterminal that occurs within
the span of a constraining base phrase, the only
valid parent is a node matching both the span (start
and end points) and the label of the provided base
955
A3
Shallow
Parser
Coarse
Parser
Fine
Parser Reranker
D
CB
extracted
base phrases
A1
A2
+
Figure 4: The iterated parsing pipeline. In the first iteration,
the coarse parser may be either unconstrained, or constrained
by base phrases from the shallow parser (A1). In the second
iteration, base phrase constraints may be extracted either from
reranker output (A2) or from a weighted combination of shal-
low parser output and reranker output (A3). Multiple sets of
n-best parses, as output by the coarse-to-fine parser under dif-
ferent constraint conditions, may be joined in a set union (C).
phrase. All other proposed parent-nodes are re-
jected. In such a way, for any parse to cover the
entire string, it would have to be consistent with the
constraining base-phrase tree.
Words that fall outside of any base-phrase con-
straint are unconstrained in how they attach within
the parse; hence, a base-phrase tree with few words
covered by base-phrase constraints will result in a
larger search space than one with many words cov-
ered by base phrases. We also put no restrictions on
the preterminal labels, even within the base phrases.
We normalized for punctuation. If the parser fails to
find a valid parse with the constraints, then we lift
the constraints and allow any parse constituent orig-
inally proposed by the first-stage parser.
3.2 Experimental Conditions
Our experiments will demonstrate the effects of con-
straining the Charniak parser under several differ-
ent conditions. The baseline system places no con-
straints on the parser. The remaining experimen-
tal conditions each consider one of three possible
sources of the base phrase constraints: (1) the base
phrases output by the finite-state shallow parser;
(2) the base phrases extracted from output of the
reranker; and (3) a combination of the output from
the shallow parser and the reranker, which is pro-
duced using the techniques outlined in Sec. 2.2.
Constraints are enforced as described in Sec. 3.1.
Unconstrained For our baseline system, we
run the Charniak and Johnson (2005) parser and
reranker with default parameters. The parser is pro-
vided with treebank-tokenized text and, as men-
tioned previously, outputs 50-best parse candidates
to the reranker.
FS-constrained The FS-constrained condition
provides a comparison point of non-iterated con-
straints. Under this condition, the one-best base-
System LR LP F
Finite-state shallow parser 91.3 92.0 91.7
Charniak reranker-best 92.2 93.3 92.8
Combination (?=0.5) 92.2 94.1 93.2
Combination (?=0.9) 81.0 97.4 88.4
Table 2: Labeled recall (LR), precision (LP), and F-scores
on WSJ section 24 of base-phrase trees produced by the three
possible sources of constraints.
phrase tree output by the finite-state shallow parser
is input as a constraint to the Charniak parser. We
run the parser and reranker as before, under con-
straints from the shallow parser. The accuracy of
the constraints used under this condition is shown in
the first row of Table 2. Note that this condition is
not an instance of pipeline iteration, but is included
to show the performance levels that can be achieved
without iteration.
Reranker-constrained We will use the
reranker-constrained condition to examine the ef-
fects of pipeline iteration, with no input from other
models outside the pipeline. We take the reranker-
best full-parse output under the condition of uncon-
strained search, and extract the corresponding base-
phrase tree. We run the parser and reranker as be-
fore, now with constraints from the reranker. The
accuracy of the constraints used under this condition
is shown in the second row of Table 2.
Combo-constrained The combo-constrained
conditions are designed to compare the effects of
generating constraints with different combination
parameterizations, i.e., different ? parameters in Eq.
3. For this experimental condition, we extract base-
phrase trees from the n-best full-parse trees output
by the reranker. We combine this list with the n-best
list output by the finite-state shallow parser, exactly
as described in Sec. 2.2, again with the reranker pro-
viding P1 and ?=2 in Eq. 4. We examined a range
of operating points from ?=0.4 to ?=0.9, and re-
port two points here (?=0.5 and ?=0.9), which rep-
resent the highest overall accuracy and the highest
precision, respectively, as shown in Table 2.
Constrained and Unconstrained Union When
iterating this pipeline, the original n-best list of full
parses output from the unconstrained parser is avail-
able at no additional cost, and our final set of ex-
perimental conditions investigate taking the union
of constrained and unconstrained n-best lists. The
imposed constraints can result in candidate sets that
are largely (or completely) disjoint from the uncon-
strained sets, and it may be that the unconstrained
set is in many cases superior to the constrained set.
956
Constraints Parser-best Reranker-best Oracle-best # Candidates
Baseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9
FS-constrained 88.44 89.50 94.10 46.2
Reranker-constrained 89.60 90.46 95.07 46.9
Combo-constrained (?=0.5) 89.81 90.74 95.41 46.3
Combo-constrained (?=0.9) 89.34 90.43 95.91 47.5
Table 3: Full-parse F-scores on WSJ section 24. The unconstrained search (first row) provides a baseline comparison for the
effects of constraining the search space. The last four rows demonstrate the effect of various constraint conditions.
Even our high-precision constraints did not reach
100% precision, attesting to the fact that there was
some error in all constrained conditions. By con-
structing the union of the two n-best lists, we can
take advantage of the new constrained candidate set
without running the risk that the constraints have re-
sulted in a worse n-best list. Note that the parser
probabilities are produced from the same model in
both passes, and are hence directly comparable.
The output of the second pass of the pipeline
could be used to constrain a third pass, for multiple
pipeline iterations. However, we found that further
iterations provided no additional improvements.
3.3 Data
Unless stated otherwise, all reported results will be
F-scores on WSJ section 24 of the Penn WSJ Tree-
bank, which was our development set. Training data
was WSJ sections 02-21, with section 00 as held-
out data. Crossfold validation (20-fold with 2,000
sentences per fold) was used to train the reranker
for every condition. Evaluation was performed us-
ing evalb under standard parameterizations. WSJ
section 23 was used only for final testing.
4 Results & Discussion
We evaluate the one-best parse candidates before
and after reranking (parser-best and reranker-best,
respectively). We additionally provide the best-
possible F-score in the n-best list (oracle-best) and
the number of unique candidates in the list.
Table 3 presents trials showing the effect of con-
straining the parser under various conditions. Con-
straining the parser to the base phrases produced
by the finite-state shallow parser (FS-constrained)
hurts performance by half a point. Constraining the
parser to the base phrases produced by the reranker,
however, provides a 0.7 percent improvement in the
parser-best accuracy, and a 0.2 percent improvement
after reranking. Combining the two base-phrase n-
best lists to derive the constraints provides further
improvements when ?=0.5, to a total improvement
of 0.9 and 0.5 percent over parser-best and reranker-
best accuracy, respectively. Performance degrades
at ?=0.9 relative to ?=0.5, indicating that, even at
a lower precision, more constraints are beneficial.
The oracle rate decreases under all of the con-
strained conditions as compared to the baseline,
demonstrating that the parser was prevented from
finding some of the best solutions that were orig-
inally found. However, the improvement in F-
score shows that the constraints assisted the parser
in achieving high-quality solutions despite this de-
graded oracle accuracy of the lists.
Table 4 shows the results when taking the union
of the constrained and unconstrained lists prior to
reranking. Several interesting points can be noted
in this table. First, despite the fact that the FS-
constrained condition hurts performance in Table
3, the union provides a 0.5 percent improvement
over the baseline in the parser-best performance.
This indicates that, in some cases, the Charniak
parser is scoring parses in the constrained set higher
than in the unconstrained set, which is evidence of
search errors in the unconstrained condition. One
can see from the number of candidates that the FS-
constrained condition provides the set of candidates
most disjoint from the original unconstrained parser,
leading to the largest number of candidates in the
union. Surprisingly, even though this set provided
the highest parser-best F-score of all of the union
sets, it did not lead to significant overall improve-
ments after reranking.
In all other conditions, taking the union de-
creases the parser-best accuracy when compared to
the corresponding constrained output, but improves
the reranker-best accuracy in all but the combo-
constrained ?=0.9 condition. One explanation for
the lower performance at ?=0.9 versus ?=0.5 is
seen in the number of candidates, about 7.5 fewer
than in the ?=0.5 condition. There are fewer con-
straints in the high-precision condition, so the re-
sulting n-best lists do not diverge as much from the
original lists, leading to less diversity in their union.
The gains in performance should not be attributed
to increasing the number of candidates nor to allow-
957
Constraints Parser-best Reranker-best Oracle-best # Candidates
Baseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9
Unconstrained ? FS-constrained 89.39 90.27 96.61 74.9
Unconstrained ? Reranker-constrained 89.23 90.59 96.48 70.3
Unconstrained ? Combo (?=0.5) 89.28 90.78 96.53 69.7
Unconstrained ? Combo (?=0.9) 89.03 90.44 96.40 62.1
Unconstrained (100-best) 88.82 90.13 96.38 95.2
Unconstrained (50-best, beam?2) 89.01 90.45 96.13 48.1
Table 4: Full-parse F-scores on WSJ section 24 after taking the set union of unconstrained and constrained parser output under
the 4 different constraint conditions. Also, F-score for 100-best parses, and 50-best parses with an increased beam threshold, output
by the Charniak parser under the unconstrained condition.
Constraints F-score
Baseline (Unconstrained, 50-best) 91.06
Unconstrained ? Combo (?=0.5) 91.48
Table 5: Full-parse F-scores on WSJ section 23 for our best-
performing system on WSJ section 24. The 0.4 percent F-score
improvement is significant at p < 0.001.
ing the parser more time to generate the parses. The
penultimate row in Table 4 shows the results with
100-best lists output in the unconstrained condition,
which does not improve upon the 50-best perfor-
mance, despite an improved oracle F-score. Since
the second iteration through the parsing pipeline
clearly increases the overall processing time by a
factor of two, we also compare against output ob-
tained by doubling the coarse-parser?s beam thresh-
old. The last row in Table 4 shows that the increased
threshold yields an insignificant improvement over
the baseline, despite a very large processing burden.
We applied our best-performing model (Uncon-
strained ? Combo, ?=0.5) to the test set, WSJ sec-
tion 23, for comparison against the baseline system.
Table 5 shows a 0.4 percent F-score improvement
over the baseline for that section, which is statisti-
cally significant at p < 0.001, using the stratified
shuffling test (Yeh, 2000).
5 Conclusion & Future Work
In summary, we have demonstrated that pipeline it-
eration can be useful in improving system perfor-
mance, by constraining early stages of the pipeline
with output derived from later stages. While the
current work made use of a particular kind of
constraint?base phrases?many others could be ex-
tracted as well. Preliminary results extending the
work presented in this paper show parser accuracy
improvements from pipeline iteration when using
constraints based on an unlabeled partial bracketing
of the string. Higher-level phrase segmentations or
fully specified trees over portions of the string might
also prove to be effective constraints. The tech-
niques shown here are by no means limited to pars-
ing pipelines, and could easily be applied to other
tasks making use of pipeline architectures.
Acknowledgments
Thanks to Martin Jansche for useful discussions on
topics related to this paper. The first author of this
paper was supported under an NSF Graduate Re-
search Fellowship. In addition, this research was
supported in part by NSF Grant #IIS-0447214. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not necessarily reflect the views of the NSF.
References
J.K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication papers for the 97th Meeting of the
Acoustical Society of America.
D. Blaheta and E. Charniak. 1999. Automatic compensation
for parser figure-of-merit flaws. In Proceedings of the 37th
Annual Meeting of ACL, pages 513?518.
S. Caraballo and E. Charniak. 1998. New figures of merit
for best-first probabilistic chart parsing. Computational Lin-
guistics, 24(2):275?298.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proceedings of
the 43rd Annual Meeting of ACL, pages 173?180.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-based
best-first chart parsing. In Proceedings of the 6th Workshop
for Very Large Corpora, pages 127?133.
E. Charniak, M. Johnson, M. Elsner, J.L. Austerweil, D. Ellis,
S.R. Iyangar, J. Moore, M.T. Pozar, C. Hill, T.Q. Vu, and
I. Haxton. 2006. Multi-level course-to-fine PCFG parsing.
In Proceedings of the HLT-NAACL Annual Meeting, pages
168?175.
E. Charniak. 2000. A Maximum-Entropy-inspired parser. In
Proceedings of the 1st Annual Meeting of NAACL and 6th
Conference on ANLP, pages 132?139.
J.R. Finkel, C.D. Manning, and A.Y. Ng. 2006. Solving the
problem of cascading errors: Approximate Bayesian infer-
ence for linguistic annotation pipelines. In Proceedings of
EMNLP, pages 618?626.
J. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer output voting error reduction
(ROVER). In Proceedings of the IEEE Workshop on Auto-
matic Speech Recognition and Understanding.
V. Goel, S. Kumar, and W. Byrne. 2000. Segmental minimum
Bayes-risk ASR voting strategies. In Proceedings of ICSLP,
pages 139?142.
958
J. Goodman. 1996. Parsing algorithms and metrics. In Pro-
ceedings of the 34th Annual Meeting of ACL, pages 177?183.
J. Goodman. 1998. Parsing inside-out. Ph.D. thesis, Harvard
University.
K. Hall and M. Johnson. 2004. Attention shifting for parsing
speech. In Proceedings of the 42nd Annual Meeting of ACL,
pages 40?46.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Comparing
and combining finite-state and context-free parsers. In Pro-
ceedings of HLT-EMNLP, pages 787?794.
K. Lari and S.J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4(1):35?56.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993.
Building a large annotated corpus of English: The Penn tree-
bank. Computational Linguistics, 19:314?330.
D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking
and self-training for parser adaptation. In Proceedings of
COLING-ACL, pages 337?344.
F.J. Och and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29.
A. Ratnaparkhi. 1999. Learning to parse natural language with
maximum entropy models. Machine Learning, 34(1-3):151?
175.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proceedings of the HLT-NAACL Annual
Meeting, pages 134?141.
E.F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proceedings of
CoNLL, pages 127?132.
A. Yeh. 2000. More accurate tests for the statistical signifi-
cance of result differences. In Proceedings of the 18th Inter-
national COLING, pages 947?953.
Appendix A Combined Precision/Recall
Decoding
Recall that T is the set of trees for a particular input,
and each T ? T is considered as a set of labeled
spans. For all labeled spans X ? T , we can calcu-
late the posterior probability ?(X) as follows:
?(X) =
?
T?T
P(T )JX ? T K
?
T ??T P(T ?)
where JX ? T K =
{
1 if X ? T
0 otherwise.
If ? is the reference tree, the labeled precision
(LP) and labeled recall (LR) of a T relative to ? are
defined as
LP =
|T ? ? |
|T |
LR =
|T ? ? |
|? |
where |T | denotes the size of the set T .
A metric very close to LR is |T ? ? |, the number
of nodes in common between the tree and the ref-
erence tree. To maximize the expected value (E) of
this metric, we want to find the tree T? as follows:
T? = argmax
T?T
E
[
|T
?
? |
]
= argmax
T?T
?
T ??T
P(T ?)
[
|T
?
T ?|
]
?
T ???T P(T
??)
= argmax
T?T
?
T ??T
P(T ?)
?
X?T JX ? T
?K
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?
T ??T
P(T ?)JX ? T ?K
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?(X) (5)
This exactly maximizes the expected LR in the
case of binary branching trees, and is closely re-
lated to LR for non-binary branching trees. Simi-
lar to maximizing the expected number of match-
ing nodes, we can minimize the expected number of
non-matching nodes, for a metric related to LP:
T? = argmin
T?T
E
[
|T | ? |T
?
? |
]
= argmax
T?T
E
[
|T
?
? | ? |T |
]
= argmax
T?T
?
T ??T
P(T ?)
[
|T
?
T ?| ? |T |
]
?
T ???T P(T
??)
= argmax
T?T
?
T ??T
P(T ?)
?
X?T (JX ? T
?K ? 1)
?
T ???T P(T
??)
= argmax
T?T
?
X?T
?
T ??T
P(T ?)(JX ? T ?K ? 1)
?
T ???T P(T
??)
= argmax
T?T
?
X?T
(?(X)? 1) (6)
Finally, we can combine these two metrics in a
linear combination. Let ? be a mixing factor from 0
to 1. Then we can optimize the weighted sum:
T? = argmax
T?T
E
[
(1? ?)|T
?
? |+ ?(|T
?
? | ? |T |)
]
= argmax
T?T
(1? ?)E
[
|T
?
? |
]
+ ?E
[
|T
?
? | ? |T |
]
= argmax
T?T
[
(1? ?)
?
X?T
?(X)
]
+
[
?
?
X?T
(?(X)? 1)
]
= argmax
T?T
?
X?T
(?(X)? ?) (7)
The result is a combined metric for balancing preci-
sion and recall. Note that, if ?=0, Eq. 7 is the same
as Eq. 5 and thus maximizes the LR metric; and if
?=1, Eq. 7 is the same as Eq. 6 and thus maximizes
the LP metric.
959
BioNLP 2007: Biological, translational, and clinical language processing, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
Syntactic complexity measures for detecting Mild Cognitive Impairment
Brian Roark, Margaret Mitchell and Kristy Hollingshead
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton, Oregon, 97006 USA
{roark,meg.mitchell,hollingk}@cslu.ogi.edu
Abstract
We consider the diagnostic utility of vari-
ous syntactic complexity measures when ex-
tracted from spoken language samples of
healthy and cognitively impaired subjects.
We examine measures calculated from man-
ually built parse trees, as well as the same
measures calculated from automatic parses.
We show statistically significant differences
between clinical subject groups for a num-
ber of syntactic complexity measures, and
these differences are preserved with auto-
matic parsing. Different measures show dif-
ferent patterns for our data set, indicating
that using multiple, complementary mea-
sures is important for such an application.
1 Introduction
Natural language processing (NLP) techniques are
often applied to electronic health records and other
clinical datasets. Another potential clinical use of
NLP is for processing patient language samples,
which can be used to assess language development
(Sagae et al, 2005) or the impact of neurodegenera-
tive impairments on speech and language (Roark et
al., 2007). In this paper, we present methods for au-
tomatically measuring syntactic complexity of spo-
ken language samples elicited during neuropsycho-
logical exams of elderly subjects, and examine the
utility of these measures for discriminating between
clinically defined groups.
Mild Cognitive Impairment (MCI), and in par-
ticular amnestic MCI, the earliest clinically de-
fined stage of Alzheimer?s-related dementia, often
goes undiagnosed due to the inadequacy of com-
mon screening tests such as the Mini-Mental State
Examination (MMSE) for reliably detecting rela-
tively subtle impairments. Linguistic memory tests,
such as word list and narrative recall, are more ef-
fective than the MMSE in detecting MCI, yet are
still individually insufficient for adequate discrimi-
nation between healthy and impaired subjects. Be-
cause of this, a battery of examinations is typically
used to improve psychometric classification. Yet the
summary recall scores derived from these linguistic
memory tests (total correctly recalled) ignore poten-
tially useful information in the characteristics of the
spoken language itself.
Narrative retellings provide a natural, conversa-
tional speech sample that can be analyzed for many
of the characteristics of speech and language that
have been shown to discriminate between healthy
and impaired subjects, including syntactic complex-
ity (Kemper et al, 1993; Lyons et al, 1994) and
mean pause duration (Singh et al, 2001). These
measures go beyond simply measuring fidelity to
the narrative, thus providing key additional dimen-
sions for improved diagnosis of impairment. Recent
work (Roark et al, 2007) has shown significant dif-
ferences between healthy and MCI groups for both
pause related and syntactic complexity measures de-
rived from transcripts and audio of narrative recall
tests. In this paper, we look more closely at syntac-
tic complexity measures.
There are two key considerations when choos-
ing how to measure syntactic complexity of spoken
language samples for the purpose of psychometric
evaluation. First and most importantly, the syntactic
complexity measures will be used for discrimination
between groups, hence high discriminative utility is
desired. It has been demonstrated in past studies
(Cheung and Kemper, 1992) that many competing
measures are in fact very highly correlated, so it may
be the case that many measures are equally discrimi-
native. For this reason, previous results (Roark et al,
2007) have focused on a single syntactic complexity
metric, that of Yngve (1960).
A second key consideration, however, is the fi-
delity of the measure when derived from transcripts
via automatic parsing. Different syntactic complex-
ity measures rely on varying levels of detail from
1
the parse tree. Some syntactic complexity measures,
such as that of Yngve (1960), make use of unla-
beled tree structures to derive their scores; others,
such as that of Frazier (1985), rely on labels within
the tree, in addition to the tree structure, to pro-
vide the scores. Given these different uses of detail,
some measures may be less reliable with automa-
tion, hence dis-preferred in the context of automated
evaluation. Ideally, simple, easy-to-automate mea-
sures with high discriminative utility are preferred.
In the current paper, we demonstrate that various
syntactic complexity measures capture complemen-
tary systematic differences between subject groups,
suggesting that the best approach to discriminating
between healthy and impaired subjects is to collect
various measures, as a way of capturing language
?signatures? of the impairment.
For many measures of syntactic complexity, the
nature of the syntactic annotation is critical ? differ-
ent conventions of structural annotation will yield
different scores. We will thus spend the next sec-
tion briefly detailing the syntactic annotation con-
ventions that were followed for this work. This is
followed by a section describing a range of complex-
ity measures to be derived from these annotations.
Finally, we present empirical results on the samples
of spoken narrative retellings.
2 Syntactic annotation
For manual syntactic annotation of collected data
(see Section 4), we followed the syntactic annota-
tion conventions of the well-known Penn Treebank
(Marcus et al, 1993). This provides several key ben-
efits. First, there is an extensive annotation guide
that has been developed, not just for written but also
for spoken language, so that consistent annotation
was facilitated. Second, the large out-of-domain
corpora, in particular the 1 million words of syn-
tactically annotated Switchboard telephone conver-
sations, provide a good starting point for training
domain adapted parsing models. Finally, we can use
multiple domains for evaluating the correlations be-
tween various syntactic complexity measures.
There are characteristics of Penn Treebank anno-
tation that can impact syntactic complexity scoring.
First, prenominal modifiers are typically grouped in
a flat constituent with no internal structure. This an-
notation choice can result in very long noun phrases
(NPs) which pose very little difficulty in terms of
human processing performance, but can inflate com-
plexity measures that measure deviation from right-
branching structures, such as that of Yngve (1960).
Second, in spoken language annotations, a reparan-
dum1 is denoted with a special non-terminal cate-
gory EDITED. For this paper, we remove from the
tree these non-terminals, and the structures under-
neath them, prior to evaluating syntactic complexity.
3 Syntactic complexity
There is no single agreed-upon measurement of
syntactic complexity. A range of measures have
been proposed, with different primary considera-
tions driving the notion of complexity for each.
Many measures focus on the order in which vari-
ous constructions are acquired by children learning
the syntax of their native language ? later acquisi-
tions being taken as higher complexity. Examples
of this sort of complexity measure are: mean length
of utterance (MLU), which is typically measured
in morphemes (Miller and Chapman, 1981); the
Index of Productive Syntax (Scarborough, 1990),
a multi-point scale which has recently been auto-
mated for child-language transcript analysis (Sagae
et al, 2005); and Developmental Level (Rosenberg
and Abbeduto, 1987), a 7-point scale of complex-
ity based on the presence of specific grammatical
constructions. Other approaches have relied upon
the right-branching nature of English syntactic trees
(Yngve, 1960; Frazier, 1985), under the assump-
tion that deviations from that correspond to more
complexity in the language. Finally, there are ap-
proaches focused on the memory demands imposed
by ?distance? between dependent words (Lin, 1996;
Gibson, 1998).
3.1 Yngve scoring
The scoring approach taken in Yngve (1960) is re-
lated to the size of a ?first in/last out? stack at each
word in a top-down, left-to-right parse derivation.
Consider the tree in Figure 1. If we knew exactly
which productions to use, the parse would begin
with an S category on the stack and advance as
follows: pop the S and push VP and NP onto the
stack; pop NP and push PRP onto the stack; pop
PRP from the stack; pop VP from the stack and
push NP and VBD onto the stack; and so on. At
the point when the word ?she? is encountered, only
VP remains on the stack of the parser. When ?was?
1A reparandum is a sequence of words that are aborted by
the speaker, then repaired within the same utterance.
2
S
1 0
NP
0
VP
1 0
PRP VBD NP
1 0
NP
1 0
PP
1 0
DT NN IN NP
2 1 0
DT NN NN
Yngve score:she1 was1 a2 cook1 in1 a2 school1 cafeteria0
1
Figure 1: Parse tree with branch scores for Yngve scoring.
is reached, just NP is on the stack. Thus, the Yn-
gve score for these two words is 1. When the next
word ?a? is reached, however, there are two cate-
gories on the stack: PP and NN, so this word re-
ceives an Yngve score of 2. Stack size has been re-
lated by some (Resnik, 1992) to working memory
demands, although it most directly measures devia-
tion from right-branching trees.
To calculate the size of the stack at each word,
we can use the following simple algorithm. At each
node in the tree, label the branches from that node
to each of its children, beginning with zero at the
rightmost child and continuing to the leftmost child,
incrementing the score by one for each child. Hence,
each rightmost branch in the tree of Figure 1 is la-
beled with 0, the leftmost branch in all binary nodes
is labeled with 1, and the leftmost branch in the
ternary node is labeled with 2. Then the score for
each word is the sum of the branch scores from the
root of the tree to the word.
Given the score for each word, we can then de-
rive an overall complexity score by summing them
or taking the maximum or mean. For this paper,
we report mean scores for this and other word-based
measures, since we have found these means to pro-
vide better performing scores than either total sum
or maximum. For the tree in Figure 1, the maximum
is 2, the total is 9 and the mean over 8 words is 118 .
3.2 Frazier scoring
Frazier (1985) proposed an approach to scoring syn-
tactic complexity that traces a path from a word up
the tree until reaching either the root of the tree or
the lowest node which is not the leftmost child of its
parent.2 For example, Figure 2 shows the tree from
2An exception is made for empty subject NPs, in which case
S
1.5
NP
1
VP
1
PRP VBD NP
1
NP
1
PP
1
DT NN IN NP
1
DT NN NN
Frazier score:
she
2.5
was
1
a
2
cook
0
in
1
a
1
school
0
cafeteria
0
1
Figure 2: Parse tree fragments with scores for Frazier scoring.
Figure 1 broken into distinct paths for each word
in the string. The first word has a path up to the
root, while the second word just up to the VP, since
the VP has an NP sibling to its left. The word is
then scored, as in the Yngve measure, by summing
the scores on the links along the path. Each non-
terminal node in the path contributes a score of 1,
except for sentence nodes and sentence-complement
nodes,3 which score 1.5 rather than 1. Thus em-
bedded clauses contribute more to the complexity
measure than other embedded categories, as an ex-
plicit acknowledgment of sentence embeddings as a
source of syntactic complexity.
As with the Yngve score, we can calculate the
total and the mean of these word scores. In con-
trast to the maximum score calculated for the Yngve
measure, Frazier proposed summing the word scores
for each 3-word sequence in the sentence, then tak-
ing the maximum of these sums as a measure of
highly-localized concentrations of grammatical con-
stituents. For the example in Figure 2, the maximum
is 2.5, the maximum 3-word sum is 5.5, and the total
is 7.5, yielding a mean of 1516 .
3.3 Dependency distance
Rather than examining the tree structure itself, one
might also extract measures from lexical depen-
dency structures. These dependencies can be de-
rived from the tree using standard rules for estab-
lishing head children for constituents, originally at-
the succeeding verb receives an additional score of 1 (for the
deleted NP), and its path continues up the tree. Empty NPs are
annotated in our manual parse trees but not in the automatic
parses, which may result in a small disagreement in the Frazier
scores for manual and automatic trees.
3Every non-terminal node beginning with an S, including
SQ and SINV, were counted as sentence nodes. Sequences of
sentence nodes, i.e. an SBAR appearing directly under an S
node, were only counted as a single sentence node and thus only
contributed to the score once.
3
she was a cook in a school cafeteria
1
Figure 3: Dependency graph for the example string.
tributed to Magerman (1995), to percolate lexical
heads up the tree. Figure 3 shows the dependency
graph that results from this head percolation ap-
proach, where each link in the graph represents a de-
pendency relation from the modifier to the head. For
example, conventional head percolation rules spec-
ify the VP as the head of the S, so ?was?, as the head
of the VP, is thus the lexical head of the entire sen-
tence. The lexical heads of the other children of the
S node are called modifiers of the head of the S node;
thus, since ?she? is the head of the subject NP, there
is a dependency relation between ?she? and ?was?.
Lin (1996) argued for the use of this sort of depen-
dency structure to measure the difficulty in process-
ing, given the memory overhead of very long dis-
tance dependencies. Both Lin (1996) and Gibson
(1998) showed that human performance on sentence
processing tasks could be predicted with measures
of this sort. While details may differ ? e.g., how
to measure distance, what counts as a dependency ?
we can make use of the general approach given Tree-
bank style parses and head percolation, resulting in
graphs of the sort in Figure 3. For the current paper,
we count the distance between words for each de-
pendency link. For Figure 3, there are 7 dependency
links, a distance total of 11, and a mean of 147 .
3.4 Developmental level (D-Level)
D-Level defines eight levels of sentence complex-
ity, from 0-7, based on the development of complex
sentences in normal-development children. Each
level is defined by the presence of specific grammat-
ical constructions (Rosenberg and Abbeduto, 1987);
we follow Cheung and Kemper (1992) in assigning
scores equivalent to the defined level of complex-
ity. A score of zero corresponds to simple, single-
clause sentences; embedded infinitival clauses get
a score of 1 (She needs to pay the rent); conjoined
clauses (She worked all day and worried all night),
compound subjects (The woman and her four chil-
dren had not eaten for two days), and wh-predicate
complements score 2. Object noun phrase rela-
tive clauses or complements score 3 (The police
caught the man who robbed the woman), whereas
the same constructs in subject noun phrases score
5 (The woman who worked in the cafeteria was
robbed). Gerundive complements and comparatives
(They were hungrier than her) receive a score of 4;
subordinating conjunctions (if, before, as soon as)
score 6. Finally, a score of 7 is used as a catch-all
category for sentences containing more than one of
any of these grammatical constructions.
3.5 POS-tag sequence cross entropy
One possible approach for detecting rich syntactic
structure is to look for infrequent or surprising com-
binations of parts-of-speech (POS). We can measure
this over an utterance by building a simple bi-gram
model over POS tags, then measuring the cross en-
tropy of each utterance.4
Given a bi-gram model over POS-tags, we can
calculate the probability of the sequence as a whole.
Let ?i be the POS-tag of word wi in a sequence of
wordsw1 . . . wn, and assume that ?0 is a special start
symbol, and that ?n+1 is a special stop symbol. Then
the probability of the POS-tag sequence is
P(?1 . . . ?n) =
n+1?
i=1
P(?i | ?i?1) (1)
The cross entropy is then calculated as
H(?1 . . . ?n) = ?
1
n
log P(?1 . . . ?n) (2)
With this formulation, this basically boils down to
the mean negative log probability of each tag given
the previous tag.
4 Data
4.1 Subjects
We collected audio recordings of 55 neuropsycho-
logical examinations administered at the Layton Ag-
ing & Alzheimer?s Disease Center, an NIA-funded
Alzheimer?s center for research at OHSU. For this
study, we partitioned subjects into two groups: those
who were assigned a Clinical Dementia Rating
(CDR) of 0 (healthy) and those who were assigned
a CDR of 0.5 (Mild Cognitive Impairment; MCI).
The CDR (Morris, 1993) is assigned with access to
clinical and cognitive test information, independent
of performance on the battery of neuropsychologi-
cal tests used for this research study, and has been
shown to have high expert inter-annotator reliability
(Morris et al, 1997).
4For each test domain, we used cross-validation techniques
to build POS-tag bi-gram models and evaluate with them in that
domain.
4
CDR = 0 CDR = 0.5
(n=29) (n=18)
Measure M SD M SD t(45)
Age 88.1 9.0 91.9 4.4 ?1.65
Education (Y) 15.0 2.2 14.3 2.8 1.04
MMSE 28.4 1.4 25.9 2.6 4.29***
Word List (A) 20.0 4.0 15.4 3.3 4.06***
Word List (R) 6.8 2.0 3.9 1.7 5.12***
Wechsler LM I 17.2 4.0 10.9 4.2 5.20***
Wechsler LM II 15.8 4.3 9.5 5.4 4.45***
Cat.Fluency (A) 17.2 4.1 13.9 4.2 2.59*
Cat.Fluency (V) 12.8 4.5 9.6 3.6 2.57*
Digits (F) 6.6 1.4 6.1 1.2 1.11
Digits (B) 4.7 1.0 4.7 1.1 ?0.04
Table 1: Neuropsychological test results for subjects.
***p < 0.001; **p < 0.01 ; *p < 0.05
Of the collected recordings, three subjects were
recorded twice; for the current study only one
recording was used for each subject. Three subjects
were assigned a CDR of 1.0 and were excluded from
the study; two further subjects were excluded for er-
rors in the recording that resulted in missing audio.
Of the remaining 47 subjects, 29 had CDR = 0, and
18 had CDR = 0.5.
4.2 Neuropsychological tests
Table 1 presents means and standard deviations for
age, years of education and the manually-calculated
scores of a number of standard neuropsychological
tests that were administered during the recorded ses-
sion. These tests include: the Mini Mental State Ex-
amination (MMSE); the CERAD Word List Acqui-
sition (A) and Recall (R) tests; the Wechsler Logical
Memory (LM) I (immediate) and II (delayed) narra-
tive recall tests; Category Fluency, Animals (A) and
Vegetables (V); and Digit Span (WAIS-R) forward
(F) and backward (B).
The Wechsler Logical Memory I/II tests are the
basis of our study on syntactic complexity measures.
The original narrative is a short, 3 sentence story:
Anna Thompson of South Boston, employed as a cook in a
school cafeteria, reported at the police station that she had
been held up on State Street the night before and robbed of
fifty-six dollars. She had four small children, the rent was
due, and they had not eaten for two days. The police, touched
by the woman?s story, took up a collection for her.
Subjects are asked to re-tell this story immediately
after it is told to them (LM I), as well as after ap-
proximately 30 minutes of unrelated activities (LM
II). We transcribed each retelling, and manually an-
notated syntactic parse trees according to the Penn
Treebank annotation guidelines. Algorithms for au-
tomatically extracting syntactic complexity markers
from parse trees were written to accept either man-
System LR LP F-measure
Out-of-domain (WSJ) 77.7 80.1 78.9
Out-of-domain (SWBD) 84.0 86.2 85.1
Domain adapted from SWBD 87.9 88.3 88.1
Table 2: Parser accuracy on Wechsler Logical Memory re-
sponses using just out-of-domain data (either from the Wall St.
Journal (WSJ) or Switchboard (SWBD) treebanks) versus using
a domain adapted system.
ually annotated trees or trees output from an auto-
matic parser, to demonstrate the plausibility of using
automatically generated parse trees.
4.3 Parsing
For automatic parsing, we made use of the well-
known Charniak parser (Charniak, 2000). Following
best practices (Charniak and Johnson, 2001), we re-
moved sequences covered by EDITED nodes in the
tree from the strings prior to parsing. For this pa-
per, EDITED nodes were identified from the manual
parse, not automatically. Table 2 shows parsing ac-
curacy of our annotated retellings under three pars-
ing model training conditions: 1) trained on approx-
imately 1 million words of Wall St. Journal (WSJ)
text; 2) trained on approximately 1 million words
of Switchboard (SWBD) corpus telephone conver-
sations; and 3) using domain adaptation techniques
starting from the SWBD Treebank. The SWBD out-
of-domain system reaches quite respectable accura-
cies, and domain adaptation achieves 3 percent ab-
solute improvement over that.
For domain adaptation, we used MAP adapta-
tion techniques (Bacchiani et al, 2006) via cross-
validation over the entire set of retellings. For
each subject, we trained a model using the SWBD
treebank as the out-of-domain treebank, and the
retellings of the other 46 subjects as in-domain train-
ing. We used a count merging approach, with the
in-domain counts scaled by 1000 relative to the out-
of-domain counts. See Bacchiani et al (2006) for
more information on stochastic grammar adaptation
using these techniques.
5 Experimental results
5.1 Correlations
Our first set of experimental results regard correla-
tions between measures. Table 3 shows results for
five of our measures over all three treebanks that we
have been considering: Penn WSJ Treebank, Penn
SWBD Treebank, and the Wechsler LM retellings.
The correlations along the diagonal are between the
same measure when extracted from manually an-
notated trees and when extracted from automatic
5
Penn WSJ Treebank Penn SWBD Treebank Wechsler LM Retellings
(a) (b) (c) (d) (e) (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)
(a) Frazier 0.89 0.96 0.94
(b) Yngve -0.31 0.96 -0.72 0.96 -0.69 0.95
(c) Tree nodes 0.91 -0.16 0.92 0.58 -0.06 0.93 0.93 -0.48 0.85
(d) Dep len -0.29 0.75 -0.13 0.93 -0.74 0.97 -0.08 0.96 -0.72 0.96 -0.51 0.96
(e) Cross Ent 0.17 0.18 0.15 0.19 0.93 -0.55 0.76 0.09 0.76 0.98 -0.13 0.45 0.05 0.41 0.97
Table 3: Correlation matrices for several measures on an utterance-by-utterance basis. Correlations along the diagonal are between
the manual measures and the measures when automatically parsed. All other correlations are between measures when derived from
manual parse trees.
parses. All other correlations are between mea-
sures derived from manual trees. All correlations
are taken per utterance.
From this table, we can see that all of the mea-
sures derived from automatic parses have a high
correlation with the manually derived measures, in-
dicating that they may preserve any discriminative
utility of these markers. Interestingly, the num-
ber of nodes in the tree per word tends to corre-
late well with the Frazier score, while the depen-
dency length tends to correlate well with the Yngve
score. Cross entropy correlates with Yngve and de-
pendency length for the SWBD and Wechsler tree-
banks, but not for the WSJ treebank.
5.2 Manually derived measures
Table 4 presents means and standard deviations
for measures derived from the LM I and LM II
retellings, along with the t-value and level of sig-
nificance. The first three measures presented in the
table are available without syntactic annotation: to-
tal number of words, total number of utterances, and
words per utterance in the retelling. None of these
three measures on either retelling show statistically
significant differences between the groups.
The first measure to rely upon syntactic annota-
tions is words per clause. The number of clauses are
automatically extracted from the parses by counting
the number of S nodes in the tree.5 Normalizing the
number of words by the number of clauses rather
than the number of utterances (as in words per ut-
terance) results in statistically significant differences
between the groups for LM I though not for LM II.
The other measures are as described in Section
3. Interestingly, Frazier score per word, the number
of tree nodes per word, and POS-tag cross entropy
all show a significant negative t-value on the LM I
retellings, meaning the CDR 0.5 subjects had sig-
nificantly higher scores than the CDR 0 subjects for
5For coordinated S nodes, the root of the coordination,
which in Penn Treebank style annotation also has an S label,
does not count as an additional clause.
these measures on this task. These measures showed
no significant difference on the LM II retellings.
The Yngve score per word and the dependency
length per word showed no significant difference on
LM I retellings but a statistically significant differ-
ence on LM II, with the expected outcome of higher
scores for the CDR 0 subjects. The D-Level measure
showed no significant differences.
5.3 Automatically derived measures
In addition to manual-parse derived measures, Table
4 also presents the same measures when automatic,
rather than manual, parses are used. Given the rela-
tively high quality of the automatic parses, most of
the means and standard deviations are quite close,
and all of the patterns observed in the upper half of
Table 4 are preserved, except that the Yngve score
per word no longer shows a statistically significant
difference for the LM II retelling.
5.4 Left-corner trees
For the tree-based complexity metrics (Frazier and
Yngve), we also investigated alternative imple-
mentations that make use of the left-corner trans-
formation (Rosenkrantz and Lewis II, 1970) of
the tree from which the measures were extracted.
This transformation is widely known for remov-
ing left-recursion from a context-free grammar, and
it changes the tree shape by transforming left-
branching structures into right-branching structures,
while leaving center-embedded structures center-
embedded. This property led Resnik (1992) to pro-
pose left-corner processing as a plausible mecha-
nism for human sentence processing, since it is pre-
cisely these center-embedded structures, and not the
left- or right-branching structures, that are problem-
atic for humans to process.
Table 5 presents results using either manually an-
notated trees or automatic parses to extract the Yn-
gve and Frazier measures after a left-corner trans-
form has been applied to the tree. The Frazier
scores are very similar to those without the left-
6
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Total words in retelling 71.0 26.0 58.1 31.9 1.49 70.6 21.5 58.5 36.7 1.43
Total utterances in retelling 8.86 4.16 7.72 3.28 0.99 8.17 2.77 7.06 4.86 1.01
Words per utterance in retelling 8.57 2.44 7.78 3.67 0.89 9.16 3.06 7.82 4.76 1.18
Manually extracted: Words per clause 6.33 1.39 5.25 1.25 2.68* 6.12 1.20 5.48 3.37 0.93
Frazier score per word 1.19 0.09 1.26 0.11 ?2.68* 1.19 0.09 1.13 0.43 0.67
Tree nodes per word 1.96 0.07 2.01 0.10 ?2.08* 1.96 0.07 1.80 0.66 1.36
Yngve score per word 1.44 0.23 1.39 0.30 0.61 1.53 0.27 1.26 0.62 2.01*
Dependency length per word 1.54 0.25 1.47 0.27 0.90 1.63 0.30 1.34 0.60 2.19*
POS-tag Cross Entropy 1.83 0.16 1.96 0.26 ?2.18* 1.93 0.14 1.86 0.59 0.54
D-Level 1.07 0.75 1.03 1.23 0.14 1.23 0.81 1.68 1.41 ?1.42
Auto extracted: Words per clause 6.42 1.53 5.10 1.16 3.13** 6.04 1.25 5.61 3.67 0.59
Frazier score per word 1.16 0.10 1.24 0.10 ?2.92** 1.15 0.10 1.09 0.41 0.69
Tree nodes per word 1.96 0.07 2.03 0.10 ?2.55* 1.96 0.08 1.79 0.66 1.38
Yngve score per word 1.41 0.23 1.37 0.29 0.54 1.50 0.27 1.28 0.64 1.70
Dependency length per word 1.51 0.25 1.47 0.28 0.54 1.61 0.28 1.35 0.61 2.04*
POS-tag Cross Entropy 1.83 0.17 1.96 0.26 ?2.12* 1.92 0.14 1.86 0.58 0.53
D-Level 1.09 0.73 1.11 1.20 ?0.08 1.28 0.77 1.61 1.22 ?1.15
Table 4: Syntactic complexity measure group differences when measures are derived from either manual or automatic parse trees.
**p < 0.01 ; *p < 0.05
corner transform, while the Yngve scores are re-
duced across the board. With the left-corner trans-
formed tree, the automatically derived Yngve mea-
sure retains the statistically significant difference
shown by the manually derived measure.
6 Discussion and future directions
The results presented in the last section demonstrate
that NLP techniques applied to clinically elicited
spoken language samples can be used to automat-
ically derive measures that may be useful for dis-
criminating between healthy and MCI subjects. In
addition, we see that different measures show differ-
ent patterns when applied to these language samples,
with Frazier scores and tree nodes per word giving
quite different results than Yngve scores and depen-
dency length. It would thus appear that, for Penn
Treebank style annotations at least, these measures
are quite complementary.
There are two surprising aspects of these results:
the significantly higher means of three measures on
LM I samples for MCI subjects, and the fact that one
set of measures show significant differences on LM
I while another shows differences on LM II. We do
not have definitive explanations for these phenom-
ena, but we can speculate about why such results
were obtained.
First, there is an important difference between the
manner of elicitation for LM I versus LM II. LM I
is an immediate recall, so there will likely be, for
unimpaired subjects, much higher verbatim recall of
the story than in the delayed recall of LM II. For
the MCI group, which exhibits memory impairment,
there will be little in the way of verbatim recall, and
potentially much more in the way of spoken lan-
guage phenomena such as filled pauses, parenthet-
icals and off-topic utterances. This may account for
the higher Frazier score per word for the MCI group
on LM I. Such differences will likely be lessened in
the delayed recall.
Second, the Frazier and Yngve metrics differ in
how they score long, flat phrases, such as typical
base NPs. Consider the ternary NP in Figure 1. The
first word in that NP (?a?) receives an Yngve score
of 2, but a Frazier score of only 1 (Figure 2), while
the second word in the NP receives an Yngve score
of 1 and a Frazier score of 0. For a flat NP with
5 children, that difference would be 4 to 1 for the
first child, 3 to 0 for the second child, and so forth.
This difference in scoring relatively common syn-
tactic constructions, even those which may not affect
human memory load, may account for such different
scores achieved with these different measures.
In summary, we have demonstrated an important
clinical use for NLP techniques, where automatic
syntactic annotation provides sufficiently accurate
parse trees for use in automatic extraction of syntac-
tic complexity measures. Different syntactic com-
plexity measures appear to be measuring quite com-
plementary characteristics of the retellings, yielding
statistically significant differences from both imme-
diate and delayed retellings.
There are quite a number of questions that we will
7
Logical Memory I Logical Memory II
CDR = 0 CDR = 0.5 CDR = 0 CDR = 0.5
Measure M SD M SD t(45) M SD M SD t(45)
Manually extracted: Left-corner Frazier 1.20 0.10 1.28 0.12 ?2.60* 1.20 0.11 1.18 0.45 0.29
Left-corner Yngve 1.33 0.20 1.25 0.23 1.20 1.37 0.21 1.14 0.52 2.14*
Auto extracted: Left-corner Frazier 1.16 0.10 1.27 0.13 ?3.02** 1.15 0.11 1.10 0.42 0.64
Left-corner Yngve 1.31 0.19 1.23 0.21 1.33 1.36 0.21 1.13 0.51 2.11*
Table 5: Syntactic complexity measure group differences when measures are derived from left-corner parse trees.
**p < 0.01 ; *p < 0.05
continue to pursue. Most importantly, we will con-
tinue to examine this data, to try to determine what
characteristics of the spoken language are leading to
the unexpected patterns in the results. In addition,
we will begin to explore composite measures, such
as differences in measures between LM I and LM II,
which promise to better capture some of the patterns
we have observed. Ultimately, we would like to
build classifiers making use of a range of measures
as features, although in order to demonstrate statisti-
cally significant differences between classifiers, we
will need much more data than we currently have.
Eventually, longitudinal tracking of subjects may be
the best application of such measures on clinically
elicited spoken language samples.
Acknowledgments
This research was supported in part by NSF Grant #IIS-
0447214 and pilot grants from the Oregon Center for Aging
& Technology (ORCATECH, NIH #1P30AG024978-01) and
the Oregon Partnership for Alzheimer?s Research. Also, the
third author of this paper was supported under an NSF Grad-
uate Research Fellowship. Any opinions, findings, conclusions
or recommendations expressed in this publication are those of
the authors and do not necessarily reflect the views of the NSF.
Thanks to Jeff Kaye, John-Paul Hosom, Jan van Santen, Tracy
Zitzelberger, Jessica Payne-Murphy and Robin Guariglia for
help with the project.
References
M. Bacchiani, M. Riley, B. Roark, and R. Sproat. 2006. MAP
adaptation of stochastic grammars. Computer Speech and
Language, 20(1):41?68.
E. Charniak and M. Johnson. 2001. Edit detection and parsing
for transcribed speech. In Proceedings of the 2nd Confer-
ence of the North American Chapter of the ACL.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of the 1st Conference of the North American
Chapter of the ACL, pages 132?139.
H. Cheung and S. Kemper. 1992. Competing complexity met-
rics and adults? production of complex sentences. Applied
Psycholinguistics, 13:53?76.
L. Frazier. 1985. Syntactic complexity. In D.R. Dowty,
L. Karttunen, and A.M. Zwicky, editors, Natural Language
Parsing. Cambridge University Press, Cambridge, UK.
E. Gibson. 1998. Linguistic complexity: locality of syntactic
dependencies. Cognition, 68(1):1?76.
S. Kemper, E. LaBarge, F.R. Ferraro, H. Cheung, H. Cheung,
and M. Storandt. 1993. On the preservation of syntax in
Alzheimer?s disease. Archives of Neurology, 50:81?86.
D. Lin. 1996. On structural complexity. In Proceedings of
COLING-96.
K. Lyons, S. Kemper, E. LaBarge, F.R. Ferraro, D. Balota, and
M. Storandt. 1994. Oral language and Alzheimer?s disease:
A reduction in syntactic complexity. Aging and Cognition,
1(4):271?281.
D.M. Magerman. 1995. Statistical decision-tree models for
parsing. In Proceedings of the 33rd Annual Meeting of the
ACL, pages 276?283.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
J.F. Miller and R.S. Chapman. 1981. The relation between
age and mean length of utterance in morphemes. Journal of
Speech and Hearing Research, 24:154?161.
J. Morris, C. Ernesto, K. Schafer, M. Coats, S. Leon, M. Sano,
L. Thal, and P. Woodbury. 1997. Clinical dementia
rating training and reliability in multicenter studies: The
Alzheimer?s disease cooperative study experience. Neurol-
ogy, 48(6):1508?1510.
J. Morris. 1993. The clinical dementia rating (CDR): Current
version and scoring rules. Neurology, 43:2412?2414.
P. Resnik. 1992. Left-corner parsing and psychological plausi-
bility. In Proceedings of COLING-92, pages 191?197.
B. Roark, J.P. Hosom, M. Mitchell, and J.A. Kaye. 2007. Au-
tomatically derived spoken language markers for detecting
mild cognitive impairment. In Proceedings of the 2nd Inter-
national Conference on Technology and Aging (ICTA).
S. Rosenberg and L. Abbeduto. 1987. Indicators of linguis-
tic competence in the peer group conversational behavior of
mildly retarded adults. Applied Psycholinguistics, 8:19?32.
S.J. Rosenkrantz and P.M. Lewis II. 1970. Deterministic left
corner parsing. In IEEE Conference Record of the 11th An-
nual Symposium on Switching and Automata, pages 139?
152.
K. Sagae, A. Lavie, and B. MacWhinney. 2005. Automatic
measurement of syntactic development in child langugage.
In Proceedings of the 43rd Annual Meeting of the ACL.
H.S. Scarborough. 1990. Index of productive syntax. Applied
Psycholinguistics, 11:1?22.
S. Singh, R.S. Bucks, and J.M. Cuerden. 2001. Evaluation of
an objective technique for analysing temporal variables in
DAT spontaneous speech. Aphasiology, 15(6):571?584.
V.H. Yngve. 1960. A model and an hypothesis for language
structure. Proceedings of the American Philosophical Soci-
ety, 104:444?466.
8
Finite-State Chart Constraints for Reduced
Complexity Context-Free Parsing Pipelines
Brian Roark?
Oregon Health & Science University
Kristy Hollingshead??
University of Maryland
Nathan Bodenstab?
Oregon Health & Science University
We present methods for reducing the worst-case and typical-case complexity of a context-free
parsing pipeline via hard constraints derived from finite-state pre-processing. We perform O(n)
predictions to determine if each word in the input sentence may begin or end a multi-word
constituent in chart cells spanning two or more words, or allow single-word constituents in
chart cells spanning the word itself. These pre-processing constraints prune the search space
for any chart-based parsing algorithm and significantly decrease decoding time. In many cases
cell population is reduced to zero, which we term chart cell ?closing.? We present methods for
closing a sufficient number of chart cells to ensure provably quadratic or even linear worst-case
complexity of context-free inference. In addition, we apply high precision constraints to achieve
large typical-case speedups and combine both high precision and worst-case bound constraints
to achieve superior performance on both short and long strings. These bounds on processing
are achieved without reducing the parsing accuracy, and in some cases accuracy improves.
We demonstrate that our method generalizes across multiple grammars and is complementary
to other pruning techniques by presenting empirical results for both exact and approximate
inference using the exhaustive CKY algorithm, the Charniak parser, and the Berkeley parser. We
also report results parsing Chinese, where we achieve the best reported results for an individual
model on the commonly reported data set.
1. Introduction
Although there have been great advances in the statistical modeling of hierarchical
syntactic structure over the past 15 years, exact inference with suchmodels remains very
costly andmost rich syntactic modeling approaches resort to heavy pruning, pipelining,
? Center for Spoken Language Understanding, Oregon Health & Science University, Beaverton, Oregon,
97006 USA. E-mails: roarkbr@gmail.com, bodenstab@gmail.com.
?? Some of the work in this paper was done while Kristy Hollingshead was at OHSU. She is currently at the
University of Maryland Institute for Advanced Computer Studies, College Park, Maryland, 20740 USA.
E-mail: hollingk@gmail.com.
Submission received: 9 August 2011; revised submission received: 30 November 2011; accepted for
publication: 4 January 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
or both. Pipeline systems make use of simpler models with more efficient inference to
reduce the search space of the full model. For example, the well-known Ratnaparkhi
(1999) parser used a part-of-speech (POS)-tagger and a finite-state noun phrase (NP)
chunker as initial stages of a multi-stageMaximum Entropy parser. The Charniak (2000)
parser uses a simple probalistic context-free grammar (PCFG) to sparsely populate a
chart for a richer model, and Charniak and Johnson (2005) added a discriminatively
trained reranker to the end of that pipeline.
Finite-state pre-processing for context-free parsing is very common as a means
of reducing the amount of search required in the later stage. As mentioned earlier,
the Ratnaparkhi pipeline used a finite-state POS-tagger and a finite-state NP-chunker
to reduce the search space at the parsing stage, and achieved linear observed-time
performance. Other recent examples of the utility of finite-state constraints for parsing
pipelines include Glaysher and Moldovan (2006), Djordjevic, Curran, and Clark (2007),
and Hollingshead and Roark (2007). Similar hard constraints have been applied for
dependency parsing, as will be outlined in Section 2. Note that by making use of pre-
processing constraints, such approaches are no longer performing full exact inference?
these are approximate inference methods, as are the methods presented in this article.
Using finite-state chunkers early in a syntactic parsing pipeline has shown both an
efficiency (Glaysher and Moldovan 2006) and an accuracy (Hollingshead and Roark
2007) benefit for parsing systems. Glaysher and Moldovan (2006) demonstrated an
efficiency gain by explicitly disallowing constituents that cross chunk boundaries.
Hollingshead and Roark (2007) demonstrated that high-precision constraints on
early stages of the Charniak and Johnson (2005) pipeline (in the form of base phrase
constraints derived either from a chunker or from later stages of an earlier iteration
of the same pipeline) achieved significant accuracy improvements, by moving the
pipeline search away from unlikely areas of the search space. All of these approaches
(as with Ratnaparkhi earlier) achieve improvements by ruling out parts of the search
space, and the gain can either be realized in efficiency (same accuracy, less time) and/or
accuracy (same time, greater accuracy).
Rather than extracting constraints from taggers or chunkers built for different
purposes, in this study we have trained prediction models to more directly reduce the
number of entries stored in cells of a dynamic programming chart during parsing?even
to the point of ?closing? chart cells to all entries. We demonstrate results using three
finite-state taggers that assign each word position in the sequence with a binary class
label. The first tagger decides if the word can begin a constituent of span greater than
one word; the second tagger decides if the word can end a constituent of span greater
than oneword; and the third tagger decides if a chart cell spanning a single word should
contain phrase-level non-terminals, or only POS tags. Following the prediction of each
word, chart cells spanning multiple words can be completely closed as follows: Given a
chart cell (b, e) spanning words wb . . .we where b < e, we can ?close? cell (b, e) if the first
tagger decides that wb cannot be the first word of a multi-word constituent (MWC) or if
the second tagger decides thatwe cannot be the last word in aMWC. Completely closing
sufficient chart cells allows us to impose worst-case complexity bounds on the overall
pipeline, a bound that none of the other previously mentioned methods for finite-state
preprocessing can guarantee.
To complement closing multi-word constituent chart cells, our third tagger restricts
the population of span-1 chart cells. We note that all span-1 chart cells must contain
at least one POS tag and can therefore never be closed completely. Instead, our tagger
restricts unary productions with POS tags on their right-hand side that span a single
word. We term these single word constituents (SWCs). Disallowing SWCs alters span-1
720
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
cell population from potentially containing all non-terminals to just POS non-terminals.
In practice, this decreases the number of entries in span-1 chart cells by 70% during ex-
haustive parsing, significantly reducing the number of allowable constituents in larger
spans (Bodenstab, Hollingshead, and Roark 2011). Span-1 chart cells are also the most
frequently queried cells in the Cocke-Younger-Kasami (CKY) algorithm. The search over
possible midpoints will always include two cells spanning a single word?one as the
first left child and one as the last right child. It is therefore beneficial to minimize the
number of entries in these span-1 cells.
The pre-processing framework we have outlined is straightforward to incorporate
into most existing context-free constituent parsers, a task we have already done for sev-
eral state-of-the art parsers. In the following sections we formally define our approach
to finite-state chart constraints and analyze the accuracy of each of the three taggers
and their impact on parsing efficiency and accuracy when used to prune the search
space of a constituent parser. We apply our methods to exhaustive CYK parsing with
simple grammars, as well as to high-accuracy parsing approaches such as the Charniak
and Johnson (2005) parsing pipeline and the Berkeley parser (Petrov and Klein 2007a,
2007b). Various methods for applying finite-state chart constraints are investigated,
including methods that guarantee quadratic or linear complexity of the context-free
parser.
2. Related Work
Hard constraints are ubiquitous within parsing pipelines. One of the most basic and
standard techniques is the use of a POS-tag dictionary, whereby words are only allowed
to be assigned one of a subset of the POS-tag vocabulary, often based on what has been
observed in the training data. This will overly constrain polysemous word types that
happen not to have been observedwith one of their possible tags; yet the large efficiency
gain of so restricting the tags is typically seen as outweighing the loss in coverage. POS-
tag preprocessing has been used for both context-free constituent parsing (Ratnaparkhi
1999) and dependency parsing (McDonald et al 2005). Richer tag sets can also be used
to further constrain the parser, such as supertags (Bangalore and Joshi 1999), which
contain information about how the word will syntactically integrate with other words
in the sequence. Supertagging has been widely used to make parsing algorithms effi-
cient, particularly those making use of context-sensitive grammars (Clark and Curran
2004).
By applying finite-state chart constraints to constituent parsing, the approaches
pursued in this article constrain the possible shapes of unlabeled trees, eliminating from
consideration trees with constituents over specific spans. There is thus some similarity
with other tagging approaches (e.g., supertagging) that dictate how words combine
with the rest of the sentence via specific syntactic structures. Supertagging is generally
used to enumerate which sorts of structures are licensed, whereas the constraints in
this article indicate unlabeled tree structures that are proscribed. Along the same lines,
there is a very general similarity with coarse-to-fine search methods, such as those
used in the Berkeley (Petrov and Klein 2007a) and Charniak (2000) parsers, and more
general structured prediction cascades (Weiss, Sapp, and Taskar 2010; Weiss and Taskar
2010). Our approach also uses simpler models that reduce the search space for larger
downstream models.
Dependency parsing involves constructing a graph of head/dependent relations,
andmanymethods for constraining the space of possible dependency graphs have been
721
Computational Linguistics Volume 38, Number 4
investigated, such as requiring that each word have a single head or that the graph be
acyclic. Nivre (2006) investigated the impact of such constraints on coverage and the
number of candidate edges in the search space. Most interestingly, that paper found
that constraining the degree of non-projectivity that is allowed can greatly reduce the
number of arcs that must be considered during search, and, as long as some degree
of non-projectivity is allowed, coverage is minimally impacted. Of course, the total
absence of projectivity constraints allows for the use of spanning tree algorithms that
can be quadratic complexity for certain classes of statistical models (McDonald et al
2005), so the ultimate utility of such constraints varies depending on the model being
used.
Other hard constraints have been applied to dependency parsing, including con-
straints on the maximum length of dependencies (Eisner and Smith 2005; Dreyer, Smith,
and Smith 2006), which is known as vine parsing. Such vine parsers can be further
constrained using taggers to determine the directionality and distance of each word?s
head in a way similar to our use of taggers (Sogaard and Kuhn 2009). More general arc
filtering approaches, using a variety of features (including some inspired by previously
published results of methods presented in this article) have been proposed to reduce
the number of arcs considered for the dependency graph (Bergsma and Cherry 2010;
Cherry and Bergsma 2011), resulting in large parsing speedups.
In a context-free constituent parsing pipeline, constraints on the final parse struc-
ture can be made in stages preceding the CYK algorithm. For example, base phrase
chunking (Hollingshead and Roark 2007) involves identifying a span as a base phrase
of some category, often NP. A base phrase constituent has no children other than
pre-terminal POS-tags, which all have a single terminal child (i.e., there is no in-
ternal structure in the base phrase involving non-POS non-terminals). This has a
number of implications for the context-free parser. First, there is no need to build
internal structure within the identified base phrase constituent. Second, constituents
that cross brackets with the base phrase cannot be part of the final tree structure.
This second constraint on possible trees can be thought of as a constraint on chart
cells, as pointed out in Glaysher and Moldovan (2006): No multi-word constituent
can begin at a word falling within a base-phrase chunk, other than the first word
of that chunk. Similarly, no multi-word constituent can end at a word falling within
a base-phrase chunk, other than the last word of that chunk. These constraints rule
out many possible structures that the full context-free parser would have otherwise
considered.
These begin and end constraints can be extracted from the output of the chunker,
but the chunker is most often trained to optimize chunking accuracy, not parsing
accuracy (or parsing precision). Further, these constraints can apply even for words
that fall outside of typical chunks. For example, in English, verbs and prepositions tend
to occur before their arguments, hence are often unlikely to end constituents, yet verbs
and prepositions are rarely inside a typically defined base phrase. Instead of imposing
parsing constraints from NLP pre-processing steps such as chunking, we propose that
building specific prediction models to constrain the search space within the CYK chart
will more directly optimize efficiency within a parsing pipeline.
In this article, we focus on linear complexity finite-state methods for deriving
constraints on the chart. Recent work has also examined methods for constraining each
of theO(N2) chart cell independently (Bodenstab et al 2011), permitting a finer-grained
pruning (e.g., not just ?open? or ?closed? but an actual beam width prediction) and the
use of features beyond the scope of our tagger. We discuss this and other extensions of
the current methods in our concluding remarks.
722
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
3. Preliminaries
3.1 Dynamic Programming Chart
Dynamic programming for context-free inference generally makes use of a chart struc-
ture, as shown in Figure 1c for the left-binarized gold parse tree in Figure 1b. Each cell
in the chart represents a collection of possible constituents covering a substring, which
is identified by the indices of the first and last words of the substring. Let w1 . . .wn be
a string of n words to be parsed. The cell identified with (b, e) will contain possible
constituents spanning the substring wb . . .we, where the span of a constituent or a chart
cell is defined as the number of words it covers, or e? b+ 1. As an example, in this
Figure 1
Gold parse tree (a), left-binarized representation of the same tree (b), and corresponding
dynamic programming chart (c) for the sentence As usual, the real-estate market had overreacted.
(sentence 1094 in WSJ Section 24). Each cell in the chart spans a unique substring of the input
sentence. Non-terminals preceded with the symbol ?@? are created through binarization
(see Section 3.3).
723
Computational Linguistics Volume 38, Number 4
article we will occasionally refer to span-1 chart cells, meaning all chart cells covering a
single word.
Context-free inference using dynamic programming over a chart structure builds
longer-span constituents by combining smaller span constituents, guided by rules in a
context-free grammar. A context-free grammar G = (V,T,S?,P) consists of: a set of non-
terminal symbols V, including a special start symbol S?; a set of terminal symbols T;
and a set of rule productions P of the form A ? ? for A ? V and ? ? (V ? T)?, i.e., a
single non-terminal on the left-hand side of the rule production, and a sequence of 0 or
more terminals or non-terminals on the right-hand side of the rule. If we have a rule
production A ? B C ? P, a completed B entry in chart cell (b,m) and a completed C
entry in chart cell (m+1, e), we can place a completed A entry in chart cell (b, e). Such
a chart cell entry is sometimes called an ?edge? and can be represented by the tuple
(A ? B C, b,m, e).
Context-free inference has cubic complexity in the length of the string N, due to the
O(N2) number of chart cells and O(N) possible child configurations at each cell. As an
example, a cell spanning (b, e) must consider all possible configurations of two child
constituents (child cells) that span a proper prefix (b, m) and a proper suffix (m+1, e)
where b ? m < e, leading to O(N) possible midpoints.
3.2 The CYK Algorithm
Algorithm 1 contains pseudocode for the CYK algorithm (Kasami 1965; Younger 1967;
Cocke and Schwartz 1970), where the context-free grammar G is assumed to be bina-
rized. The function ? maps each grammar production in P to a probability. Lines 1?3
Algorithm 1 CYK
Pseudocode of the CYK algorithm using a binarized PCFG. Unary processing is sim-
plified to allow only chains of length one (excluding lexical unary productions). Back-
pointer storage is omitted.
Input:
w1 . . .wn: Input sentence
G: Left-binarized PCFG
Output:
?: Viterbi-max score for all non-terminals over every span
CYK(w1 . . .wn, G = (V,T,S
?,P,?))
1: for s = 1 to n do  Span width: bottom-up traversal
2: for b = 1 to n? s+ 1 do  Begin word position
3: e ? b+s?1
4: for Ai ? V do
5: if s = 1 then  Add lexical productions
6: ?i(b, e)? ?(Ai ? wb)
7: else
8: ?i(b, e)? max
b?m<e
(
max
j,k
?(Ai ? Aj Ak) ?j(b,m) ?k(m+ 1, e)
)
9: for Ai ? V do  Add unary productions
10: ?i(b, e)? max
(
?i(b, e) , max
j
?(Ai ? Aj) ?j(b, e)
)
11: ?(b, e)? ?(b, e)
12: return ?
724
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
iterate over all O(N2) chart cells in a bottom?up traversal. Line 6 initializes span-1 cells
with all possible part-of-speech tags, and line 8 introduces the cubic complexity of the
algorithm: maximization over all midpoints m, which is O(N). The variable ? stores the
Viterbi-max score for each non-terminalAi ? V at each cell (b, e), representing the span?s
most probable derivation rooted in Ai. Backpointers indicating which argument(s)
maximize each ?i(b, e) can be optionally recorded to efficiently extract the maximum
likelihood solution at the end of inference, but we omit these for clarity; they are easily
recoverable by storing the argmax for each max in lines 8 and 10.
We have also included pseudocode in Algorithm 1 to process unary productions in
lines 9 and 10. Unary processing is a necessary step to recover the gold-standard trees of
the Penn treebanks (Marcus, Marcinkiewicz, and Santorini 1993; Xue et al 2005), but is
often ignored in the presentation of the CYK algorithm. Because there is little discussion
about unary processing in the literature, implementation details often differ from parser
to parser. In Algorithm 1 we present a simplified version of unary processing that only
allows unary chains of length 1 per span (excluding lexical productions). This approach
is efficient and can be iterated as needed to represent the length of observed unary
chain in the treebank. Note that line 10 uses the temporary variable ? to store the
accumulated Viterbi-max scores ?. This temporary variable is necessary due to the
iterative nature in which we update ?. If we were to write the result of line 10 directly to
?i(b, e), then the subsequent maximization of ?i+1(b, e) would use an unstable version of
?, some of which would already be updated with unary productions, and some which
would not.
3.3 Incomplete Edges: Chomsky Normal Form and Dotted-Rules
A key feature to the efficiency of the CYK algorithm is that all productions in the
grammar G are assumed to have no more than two right-hand-side children. Rather
than trying to combine an arbitrary number of smaller substrings (child cells), the CYK
algorithm exploits shared structure between rules and only needs to consider pairwise
combination. To conform to this requirement, incomplete edges are needed to represent
that further combination is required to achieve a complete edge. This can either be
performed in advance, for example, by transforming a grammar into Chomsky Normal
Form resulting in ?incomplete? non-terminals created by the transform, or incomplete
edges can be represented through so-called dotted rules, as with the Earley (1970)
algorithm, in which transformation is essentially performed on the fly. For example,
if we have a rule production A ? B C D ? P, a completed B entry in chart cell (b,m1)
and a completed C entry in chart cell (m1+1,m2), then we can place an incomplete edge
A ? B C ?D in chart cell (b,m2). The dot signifies the division between what has already
been combined (left of the dot), and what remains to be combined. Then, if we have an
incomplete edge A ? B C ?D in chart cell (b,m2) and a complete D in cell (m2+1, e), we
can place a completed A entry in chart cell (b, e).
Transforming a grammar into ChomskyNormal Form (CNF) is an off-line operation
that converts rules with more than two children on the right-hand side into multiple
binary rules. To do this, composite non-terminals are created during the transformation,
which represent incomplete constituents (i.e., those edges that require further combina-
tion to be made complete).1 For example, if we have a rule production A ? B C D in
1 In this section we assume that edges are extended from left-to-right, which requires a left-binarization of
the grammar, but everything carries over straightforwardly to the right-binarized case.
725
Computational Linguistics Volume 38, Number 4
the context-free grammar G, then a new composite non-terminal would be created (e.g.,
@A:BC) and two binary rules would replace the previous ternary rule: A ? @A:BC D
and @A:BC ? B C. The @A:BC non-terminal represents part of a rule expansion that
needs to be combined with something else to produce a complete non-terminal from
the original set of non-terminals.2
In addition to binarization, one frequent modification to the grammar is to create
a Markov model of the dependencies on the right-hand side of the rule. One way to
do this is to reduce the number of children categories annotated on our new composite
non-terminals introduced by binarization. For example, if instead of @A:BC we assign
the label @A:C to our new non-terminal?with the semantics ?an incomplete A con-
stituent with rightmost child C??then the two rules that result from binarization are:
A ? @A:C D and @A:C ? B C. Probabilistically, the result of this is that the children
non-terminals B and D are conditionally independent of each other given A and C.
This approach will provide probability mass to combinations of children of the original
category A that may not have been observed together, hence it should be seen as a form
of smoothing. One can go further and remove all children categories from the new non-
terminals (i.e., replacing @A:BC with just @A). This is the binarization pursued in the
Berkeley parser, and is shown in Figure 1b.
In this article, we explicitly discuss unary productions of type A ? B where B is a
non-terminal, and include these productions in our grammar. These productions violate
the definition of a CNF grammar, and therefore we will use the term ?binarized gram-
mar? for the remainder of the article to indicate a grammar in CNF with the addition
of unary productions. We will assume that all of our grammars have been previously
binarized and we define V? to be the set of non-terminals that are created through
binarization, and denote edges where A ? V? as incomplete edges. Note that categories
A ? V? are only used in binary productions, not unary productions, a consideration that
will be used in our constrained parsing algorithm.
4. Finite-State Chart Constraints
In this section, we will explicitly define our chart constraints, and present methods for
using the constraints to constrain parsing. We begin with constraints on beginning or
ending multi-word constituents, then move to constraining span-1 chart cells.
4.1 Constituent Begin and End Constraints
Our task is to learn which words (in the appropriate context) can begin (B) or end (E)
multi-word constituents. We will treat this as a pre-processing step to parsing and use
these constraints to either completely or partially close chart cells during execution of
the CYK algorithm.
First, let us introduce notation. Given a set of labeled pairs (S,T) where S is a string
of n words w1 . . .wn and T is the target constituent parse tree for S, we say that word
wb ? B if there is a constituent spanning wb . . .we for some e > b and wb ? B otherwise.
Similarly, we say that word we ? E if there is a constituent spanning wb . . .we for some
b < e and we ? E otherwise. Recovery of these labels will be treated as two separate
binary tagging tasks (B/B and E/E).
2 In this example, the symbol ?@? indicates that the new non-terminal is incomplete, and the symbol ?:?
separates the original parent non-terminal from the children.
726
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Figure 2
Begin (a), End (b), Unary (c), and the combination of the three (d) type of constraints for the
dynamic programming chart used to parse As usual, the real-estate market had overreactedwith
a left-binarized grammar (see Figure 1). Black denotes ?closed? cells, white cells are ?open,?
and gray cells are only open to a restricted population (gray cells closed by E constraints only
allow incomplete edges; gray cells closed by U constraints only allow POS tags).
Although it may be obvious that we can rule out multi-word constituents with
particular begin and end positions, there may be incomplete structures within the parser
that should not be ruled out by these same constraints. Hence the notion of ?closing?
a chart cell is slightly more complicated than it may initially seem (which accounts
for our use of quotes around the term). Consider the chart representation in Figure 2
with the following constraints, where B is the set of words disallowed from beginning a
multi-word constituent and E is the set of words disallowed from ending a multi-word
constituent3
B : {?usual?, ?, ?, ?real-estate?, ?market?, ?overreacted?}
E : {?, ?, ?the?, ?real-estate?, ?had?}
Given our constraints, suppose that wb is in class B and we is in class E, for b < e.
We can ?close? all cells (b,m1) such that m1 > b and all cells (m2, e) such that m2 < e,
based on the fact that multi-word constituents cannot begin with word wb and cannot
end with we. In Figure 2 this is depicted by the black and gray diagonals through the
chart, ?closing? those chart cells.
3 In this example we list the actual words from the sentence for clarity with Figure 2, but in practice we
classify word positions as a specific word may occur multiple times in the same sentence with potentially
different B or E labels.
727
Computational Linguistics Volume 38, Number 4
If a chart cell (b, e) has been ?closed? due to begin or end constraints then it is clear
that complete edges should not be permitted in the cell since these represent precisely
the multi-word constituents that are being ruled out. But what about incomplete edges
that are introduced through grammar binarization or dotted rule parsing? To the extent
that an incomplete edge can be extended to a valid complete edge, it must be allowed.
There are two cases where this is possible. If wb ? B, then under the assumption that
incomplete edges are extended from left-to-right (see footnote 1), the incomplete edge
should be discarded, because any completed edges that could result from extending that
incomplete edge would have the same begin position. Stated another way, if wb ? B
for chart cell (b, e) then all chart cells (b, i) for i > b must also be closed. For example,
in Figure 2, the cell associated with the two-word substring real-estate market can be
closed to both complete and incomplete edges, since real-estate ? B, and any complete
edge built from entries in that cell would also have to start with the same word and
hence would be discarded. Thus, the whole diagonal is closed. If, however, wb ? B and
we ? E, such as the cell associated with the two-word substring the real-estate in Figure 2,
a complete edge?achieved by extending the incomplete edge?may end at wi for i > e,
and cell (b, i) may be open (the real-estate market), hence the incomplete edge should be
allowed in cell (b, e).
In Section 4.4 we discuss limitations on how such incomplete edges arise in closed
cells, which has consequences for the worst-case complexity under certain conditions.
4.2 Unary Constraints
In addition to begin and end constraints, we also introduce unary constraints in span-1
cells. Although we cannot close span-1 cells entirely because each of these cells must
contain at least one POS tag, we can reduce the population of these cells by restricting
the type of constituents they contain.We define a single-word constituent (SWC) as any
unary production A ? B in the grammar such that B is a non-terminal (not a lexicon
entry) and the production spans a single word. The productions ADJP ? JJ and VP ?
VBN in Figure 1a are examples of SWCs. Note that TOP ? S and JJ ?usual in Figure 1a
are also unary productions, but by definition they are not SWC unary productions. We
train a distinct tagger, as is done for B and E constraints, to label each word position as
either in U or U, indicating that the word position may or may not be extended by a
SWC, respectively.
Because the search over possible grammar extension from two child cells in the
CYK algorithm is analogous to a database JOIN operation, the efficiency of this cross-
product hinges on the population of the two child cells that are intersected. We focus on
constraining the population of span-1 chart cells for three reasons. First, the begin/end
constituent constraints only affect chart cells spanning more than one word and leave
span-1 chart cells completely unpruned. By pruning entries in these span-1 cells, we
complement multi-word constituent pruning so that all chart cells are now candidates
for finite-state tagging constraints. Second, span-1 chart cells are the most frequently
queried cells in the CYK algorithm. The search over possible midpoints will always
include two cells spanning a single word?one as the first left child and one as the
last right child. It is therefore important that the number of entries in these cells be
minimized to make bottom?up CYK processing efficient. Finally, as we will show in
Section 5, only 11.2% of words in the WSJ treebank are labeled with SWC productions.
With oracle unary constraints, the possibility of constraining nearly 90% of span-1 chart
cells has promising efficiency benefits to downstream processing.
728
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
To reiterate, unary constraints in span-1 chart cells never close the cell completely
because each span-1 cell must contain at least one POS non-terminal. Instead, if wi ? U
then we simply do not add phrase-level non-terminals to chart cell (i, i). Similar to chart
cells spanning multiple words that cannot be closed completely, these span-1 chart cells
partially restrict the population of the cell, which we will empirically show to reduce
processing time over unconstrained CYK parsing. These partially closed chart cells are
represented as gray in Figure 2.
4.3 The Constrained CYK Algorithm
Our discussion of cell closing in Section 4.1 highlighted different conditions for closing
cells for complete and incomplete edges. We will refer to specific conditions in the pseu-
docode, which we enumerate here. The constraints determine three possible conditions
for cell (b, e) spanning multiple words:
1. wb ? B: cell is closed to all constituents, both complete and incomplete
2. wb ? B and we ? E: cell is closed to complete constituents
3. wb ? B and we ? E: cell is open to all constituents
and two possible conditions for cell (i, i) spanning a single word:
4. wi ? U: cell is closed to unary phrase-level constituents
5. wi ? U: cell is open to all constituents
We will refer to a chart cell (b, e) that matches the above condition as ?case 1 cells,?
?case 2 cells,? and so on, throughout the remainder of this article. Figure 2 represents
these five cases pictorially, where case 1 cells are black, case 2 and 4 cells are gray, and
case 3 and 5 cells are white.
Algorithm 2 contains pseudocode of our modified CYK algorithm that takes into
account B, E, and U constraints. Line 4 of Algorithm 2 is the first modification from
the standard CYK processing of Algorithm 1: We now consider only words in set B to
begin multi-word constituents. Chart cells excluded from this criteria fall into case 1
and require no work. Line 5 determines if chart cell (b, e) is of case 2 (partially open)
or case 3 (completely open). If we ? E, then we skip to lines 16?17 and only incomplete
edges are permitted. Note that there is only one possible midpoint for case 2 chart cells,
which results in constant-time work (see proof in Section 4.4) and unary productions
are not considered because all entries in the cell are incomplete constituents, which only
participate in binary productions. Otherwise, if we ? E on Line 5, then the cell is open
to all constituents and processing occurs as in the standard CYK algorithm (lines 6?10
of Algorithm 2 are identical to lines 4?8 of Algorithm 1). Finally, unary productions are
added in lines 12?14, but restricted to multi-word spanning chart cells, or span-1 cells
where wb ? U.
4.4 Proofs to Bound Worst-Case Complexity
All of the proofs in this section rely on constraints imposed by B and E. Pruning
provided by the U constraints reduces decoding time in practice, but does not provide
additional reductions in complexity. Our proofs of complexity bounds will rest on
the number of cells that fall in cases 1?3 outlined in the Section 4.3, and the amount
729
Computational Linguistics Volume 38, Number 4
Algorithm 2 CONSTRAINEDCYK
Pseudocode of a modified CYK algorithm, with constituent begin/end and unary con-
straints. Unary processing is simplified to allow only chains of length one (excluding
lexical unary productions). Backpointer storage is omitted.
Input:
w1 . . .wn: Input sentence
G: Left-binarized PCFG
V?: Set of binarized non-terminals from V
B,E,U: Begin, End, and Unary chart constraints
Output:
?: Viterbi-max scores for all non-terminals over every span
CONSTRAINEDCYK(w1 . . .wn, G = (V,T, S
?,P,?),V?,B,E,U)
1: for s = 1 to n do  Span width: bottom-up traversal
2: for b = 1 to n?s+1 do  Begin word position
3: e ? b+s?1
4: if wb ? B or s = 1 then  Case 1 cells excluded
5: if we ? E or s = 1 then
6: for Ai ? V do
7: if s = 1 then  Add lexical productions
8: ?i(b, b)? ?(Ai ? wb)
9: else  Case 3: cell open
10: ?i(b, e)? max
b?m<e
(
max
j,k
?(Ai ? Aj Ak) ?j(b,m) ?k(m+ 1, e)
)
11: if s > 1 or wb ? U then  Case 5 for span-1 cells
12: for Ai ? V do  Add unary productions
13: ?i(b, e)? max
(
?i(b, e) , max
j
?(Ai ? Aj) ?j(b, e)
)
14: ?(b, e)? ?(b, e)
15: else  Case 2: closed to complete constituents
16: for Ai ? V? do  Only consider binarized non-terminals
17: ?i(b, e)? max
j,k
?(Ai ? Aj Ak) ?j(b, e? 1) ?k(e, e)
18: return ?
of work in each case. The amount of work for each case is related to how the CYK
and CONSTRAINEDCYK algorithms performs their search. Each cell (b, e) in the chart
spans the substring wb . . .we, and building non-terminal categories in that cell involves
combining non-terminal categories (via rules in the context-free grammar) found in cells
spanning adjacent substrings wb . . .wm and wm+1 . . .we. The substring wb . . .we can be
as large as the entire sentence, requiring a search overO(N) possiblemidpointwordswm.
This accounts for the linear amount of work in these case 3, open, cells.
More formally, we can define an upper bound on the work required in a chart cell
as W = |P| ? |M| where |P| is the number grammar productions bounded above by the
constant size of the grammar, and |M| is the number ofmidpoints considered.We denote
the upper bound on the amount of work done in case 1 cells byW1 and the total number
of case 1 cells considered by C1 (similarly for case 2 and 3). With this decomposition, the
complexity of the CONSTRAINEDCYK algorithm can be written as O(|C1|W1 + |C2|W2 +
|C3|W3). Because there is no work for case 1 cells (W1 = 0), these can be ignored. In fact,
we can reduce the complexity even further.
730
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Theorem 1
W2 is constant and CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).
Proof
Without loss of generality, we assume a left-binarized grammar. The upper bound on re-
quiredwork for cell (b, e) is defined asW = |P| ? |M|where |P| is the number of grammar
productions bounded above by the constant size of the grammar, and |M| is the num-
ber of midpoints considered. For some (b, e) where 1 ? b < e ? n, assume wb ? B and
we ? E, that is, (b, e) ? C2. The possible child cells of (b, e) are (b,m) and (m+ 1, e) where
b ? m < e. Because we assume a left-binarized grammar, the production?s right child
must be a complete non-terminal and (m+ 1, e) ? C3 must hold. But we ? E so (m+ 1, e)
cannot be in C3 unless m = e? 1 (span 1 cell). Therefore, the only valid midpoint is
m = e? 1 andW2 = |P|?1 is constant, hence O(|C2|W2 + |C3|W3) = O(|C2|+ |C3|W3). 
In summary, we find that case 2 chart cells (gray in Figure 2) are surprisingly cheap
to process because the right child cell must also be in case 2, resulting in only one
possible midpoint to consider. Next, we show that bounding the number of open cells
and applying Theorem 1 leads to quadratically bounded parsing.
Theorem 2
If |C3| < ?N for some constant ? then CONSTRAINEDCYK has complexity O(N2).
Proof
The total number of cells in the chart for a string of length N is N(N + 1)/2, therefore
|C2| < N2. Further, the number of midpoints for any cell is less than N, hence W3 < N.
If we bound |C3| < ?N, then it follows directly from Theorem 1 that CONSTRAINEDCYK
has complexity O(N2 + ?N ?N) = O(N2). 
We have just proved in Theorem 2 that we can reduce the complexity of constituent
parsing from O(N3) to O(N2) by restricting the number of open cells in the chart via
constraints B and E. Next, we will show that this bound can be reduced even further
to O(N) when the number of words in B is held constant. We call this approach ?linear
constraints.? There are two things to note when considering these constraints. First,
the proof relies only on the size of set B, leaving E potentially empty and limiting
the efficiency gains on short sentence compared to high-precision and quadratically
bounded constraints. Second, limiting the size of B to a constant value is a restrictive
criterion. When applied to a long sentence, this will over-constrain the search space and
impose a branching bias on possible trees. In Section 7.2 we show that if the branching
bias from linear constraints matches the branching bias of the treebank, then accuracy
is not severely affected and linear constraints can provide a useful bound on long
sentences that often dominate computational resources.
We first prove limits on the total number of open chart cells (case 3) and work
required in these cells, which directly leads to the proof bounding the complexity of
the algorithm.
Lemma 1
If |B| ? ? for some ?, thenW3 ? ?|P|.
731
Computational Linguistics Volume 38, Number 4
Proof
It has been defined thatW1 is zero and shown in Theorem 1 thatW2 is constant. For chart
cell (b, e) ? C3 there are e? b possiblemidpoints to consider. For somemidpointmwhere
b ? m < e, ifwm ? B then (m, e) ? C1 and contains no complete or incomplete constituent
entries. Therefore midpoint m is not a valid midpoint for cell (b, e). Because |B| ? ?,
there are at most ? midpoints to consider and the work required in (b, e) is bounded
above by the constant number of productions in the grammar (|P|) and ? midpoints,
thusW3 ? ?|P|. 
Lemma 2
If |B| ? ? for some ?, then |C2|+ |C3| ? ?N.
Proof
For a string of length N and some wb ? B, there are at most N substrings wb . . .we to
consider, hence O(N) chart cells for each word wb ? B. Because |B| ? ?, then there are
at most ?N cells (b, e) ? C1 and |C2|+ |C3| ? ?N. 
Theorem 3
If |B| ? ? for some ? then CONSTRAINEDCYK has complexity O(N).
Proof
From Theorem 1 we know that CONSTRAINEDCYK has complexity O(|C2|+ |C3|W3).
Given that |B| ? ?, it follows from Lemmas 1 and 2 that CONSTRAINEDCYK has com-
plexity O(?2N|P|) = O(N). 
In this section, we have been discussing worst-case complexity bounds, but there is
a strong expectation for large typical-case complexity savings that aren?t explicitly
accounted for here. To provide some intuition as to why this might be, consider again
the chart structure in Figure 2d. The black cells in the chart represent the cells that have
been closed when wj ? B (case 1 cells). Because there is no work required for these cells,
the total amount of work required to parse the sentence is reduced. The quadratic bound
does not include any potential reduction of work for the remaining open cells, however.
Thus the amount of work to parse the sentence will be less than the worst-case quadratic
bound because of this reduction in processing. In Section 7.2 we compute the empirical
complexity of each constraint method and compare that with its theoretical bound.
5. Tagging Chart Constraints
To better understand the proposed tagging tasks and their likely utility, we will first
look at the distribution of classes and our ability to automatically assign them correctly.
Note that we do not consider the first word w1 and the last word wN during the
begin-constituent and end-constituent prediction tasks because they are unambiguous
in terms of whether they begin or end constituents of span greater than one. The first
wordw1 must begin a constituent spanning the whole string, and the last wordwN must
end that same constituent. The first word w1 cannot end a constituent of length greater
than 1; similarly, the last word wN cannot begin a constituent of length greater than 1.
We therefore omit B and E at these two word positions from prediction, leading to
N ? 2 begin-constituent and N ? 2 end-constituent ambiguous predictions for a string
of length N.
732
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Table 1 displays word statistics from the training sections of the Penn English
treebank (Marcus, Marcinkiewicz, and Santorini 1993) and the Penn Chinese treebank
(Xue et al 2005), and the positive and negative class label frequency of all words in
the data.
From the nearly 1 million words in the English corpus, just over 870,000 are neither
the first nor the last word in the string, therefore possible members of the sets B or E (i.e.,
neither beginning a multi-word constituent (B) nor ending a multi-word constituent
(E)). Of these 870,000 words, over half (50.5%) do not begin multi-word constituents,
and nearly three quarters (74.3%) do not end multi-word constituents. The skewed dis-
tribution of E to E reflects the right-branching structure of English. Finally, almost 90%
of words are not labeled with a single-word constituent, demonstrating the infrequency
at which these productions occur in the English treebank.
The Chinese treebank is approximately half of the size of the English treebank
in terms of the number of sentences and word count. Again, we see a bias towards
right-branching trees (|B| > |E|), but the skew of the distributions is much smaller
than it is for English. The most significant difference between the two corpora is the
percentage of single-word constituents in Chinese compared with English. Due to the
annotation guidelines of the Chinese treebank, nearly 40% of words contain single-word
constituent unary productions. Because these occur with such high frequency, we can
assume that unary constraints may have a smaller impact on efficiency for Chinese than
for English, because an accurate tagger will constrain fewer cells. We still have the
expectation, though, that accurate tagging of SWC productions will increase parsing
accuracy for both English and Chinese.
To automatically predict the class of each word position, we train a binary predictor
from supervised data for each language/word-class pair, tuning performance on the
respective development sets (WSJ Section 24 for English and Penn Chinese Treebank
articles 301-325 for Chinese). Word classes are extracted from the treebank trees by
observing constituents beginning or ending at each word position, or by observing
single word constituents. We use the tagger from Hollingshead, Fisher, and Roark
(2005) to train six linear models (three for English, three for Chinese) with the averaged
perceptron algorithm (Collins 2002).
Table 2 summarizes the features implemented in our tagger for B, E, and U identifi-
cation. In the table, the? features are instantiated as POS-tags (provided by a separately
trained log-linear POS-tagger) and ? features are instantiated as constituent tags (B,
E, and U class labels). The feature set used in the tagger includes the n-grams of
surrounding words, the n-grams of surrounding POS-tags, and the constituent tags of
Table 1
Statistics on extracted word classes for English (Sections 2?21 of the Penn WSJ treebank) and
Chinese (articles 1?270 and 400?1151 of the Penn Chinese treebank).
Corpus totals Begin class End class Unary class
Strings Words B B E E U U
English
Count 39,832 950,028 430,841 439,558 223,544 646,855 105,973 844,055
Percent 49.5 50.5 25.7 74.3 11.2 88.8
Chinese
Count 18,086 493,708 188,612 269,000 165,591 292,021 196,732 296,976
Percent 41.2 58.8 36.2 63.8 39.9 60.1
733
Computational Linguistics Volume 38, Number 4
Table 2
Tagger features for B, E, and U.
LEX ORTHO POS
?i ?i,wi ?i,wi[0] ?i,?i
?i?1, ?i ?i,wi?1 ?i,wi[0..1] ?i,?i?1
?i?2, ?i ?i,wi+1 ?i,wi[0..2] ?i,?i?1,?i
?i?2, ?i?1, ?i ?i,wi?2 ?i,wi[0..3] ?i,?i+1
?i,wi+2 ?i,wi[n] ?i,?i,?i+1
?i,wi?1,wi ?i,wi[n-1..n] ?i,?i?1,?i,?i+1
?i,wi,wi+1 ?i,wi[n-2..n] ?i,?i?2
?i,wi[n-3..n] ?i,?i?2,?i?1
?i,wi ? Digit ?i,?i?2,?i?1,?i
?i,wi ? UpperCase ?i,?i+2
?i,wi ? Hyphen ?i,?i+1,?i+2
?i,?i,?i+1,?i+2
All lexical (LEX), orthographic (ORTHO), and part-of-speech (POS) features are duplicated to also occur
with ?i?1; e.g., {?i?1, ?i,wi} as a LEX feature.
the preceding words. The n-gram features are represented by the words within a three-
wordwindow of the current word. The tag features are represented as unigram, bigram,
and trigram tags (i.e., constituent tags from the current and two previous words). These
features are based on the feature set implemented by Sha and Pereira (2003) for NP
chunking. Additional orthographical features are used for unknown and rare words
(words that occur fewer than five times in the training data), such as the prefixes and
suffixes of the word (up to the first and last four characters of the word), and the pres-
ence of a hyphen, a digit, or a capitalized letter, following the features implemented by
Ratnaparkhi (1999). Note that the orthographic feature templates, including the prefix
(e.g., wi[0..1]) and suffix (e.g., wi[n-2..n]) templates, are only activated for unknown and
rare words. When applying our tagging model to Chinese data, all feature functions
were left in the model as-is, and not tailored to the specifics of the language.
We ran various tagging experiments on the development set and report accuracy
results in Table 3 for all three predictions tasks, using Viterbi decoding. We trained
Table 3
Tagging accuracy on the respective development sets (WSJ Section 24 for English and Penn
Chinese Treebank articles 301?325 for Chinese) for binary classes B, E, and U, for various
Markov orders.
Tagging Task Markov order
0 1 2
English
B (no multi-word constituent begin) 96.7 96.9 96.9
E (no multi-word constituent end) 97.3 97.3 97.3
U (no span-1 unary constituent) 98.3 98.3 98.3
Chinese
B (no multi-word constituent begin) 94.8 95.4 95.2
E (no multi-word constituent end) 96.2 96.4 96.6
U (no span-1 unary constituent) 95.9 96.2 96.3
734
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
models with Markov order-0 (constituent tags predicted for each word independently),
order-1 (features with constituent tag pairs), and order-2 (features with constituent tag
triples). In general, tagging accuracy for English is higher than for Chinese, especially
for the U and B tasks. Given the consistent improvement from Markov order-0 to
Markov order-1 (particularly on the Chinese data), and for the sake of consistency, we
have chosen to perform Markov order-1 prediction for all results in the remainder of
this article.
6. Experimental Set-up
In the sections that follow, we present empirical trials to examine the behavior of chart
constraints under a variety of conditions. First, we detail the data, evaluation, and
parsers used in these experiments.
6.1 Data Sets and Evaluation
For English, all stochastic grammars are induced from the Penn WSJ Treebank (Marcus,
Marcinkiewicz, and Santorini 1993). Sections 2?21 of the treebank are used as training,
Section 00 as held-out (for determining stopping criteria during training and some
parameter tuning), Section 24 as development, and Section 23 as test set. For Chinese,
we use the Penn Chinese Treebank (Xue et al 2005). Articles 1?270 and 400?1151 are
used for training, articles 301?325 for both held-out and development, and articles 271?
300 for testing. Supervised class labels are extracted from the non-binarized treebank
trees for B, E, and U (as well as their complements).
All results report F-measure labeled bracketing accuracy (harmonic mean of labeled
precision and labeled recall) for all sentences in the data set (Black et al 1991), and
timing is reported using an Intel 3.00GHz processor with 6MB of cache and 16GB of
memory. Timing results include both the pre-processing time to tag the chart constraints
as well as the subsequent context-free inference, but tagging time is relatively negligible
as it takes less than three seconds to tag the entire development corpus.
6.2 Tagging Methods and Closing Chart Cells
We have three separate tagging tasks, each with two possible tags for every word wi in
the input string: (1) B or B; (2) E or E; and (3) U or U. Our taggers are as described in
Section 5.
Within a pipeline system that leverages hard constraints, one may want to choose
a tagger operating point that favors precision of constraints over recall to avoid over-
constraining the downstream parser. We have two methods for trading recall for preci-
sion that will be detailed later in this section, both relying on calculating the cumulative
score Si for each of the binary tags at each word position wi. That is, (using B as the
example tag):
Si(B | w1 . . .wn) = log
?
?1...?n
?(?i,B) e
?(w1...wn,?1...?n )?w (1)
where
?
sums over all possible tag sequences for sentence w1 . . .wn; ?(?i,B) = 1 if ?i =
B and 0 otherwise; ?(w1 . . .wn, ?1 . . . ?n) maps the word string and particular tag string
to a d-dimensional (global) feature vector; andw is the d-dimensional parameter vector
735
Computational Linguistics Volume 38, Number 4
estimated by the averaged perceptron algorithm. Note that this cumulative score over
all tag sequences that have B in position i can be calculated efficiently using the forward?
backward algorithm. We can compare the cumulative scores Si(B) and Si(B) to decide
how to tag word wi, and define the cumulative score ratio (CSR) as follows:
CSR(wi) = Si(B)? Si(B) (2)
If we want to tag Bwith high precision, and thus avoid over-constraining the parser, we
can change our decision criterion to produce fewer such tags. We present two different
selection criteria in the next two subsections.
To show the effect of precision-oriented decision criteria, Figure 3 shows the preci-
sion/recall tradeoff at various operating points, using the global high-precision method
detailed herein, for all three tags on both the English and the Chinese development
sets. As expected, we see that the English B curve is significantly lower than the
English E and U curves. This is due to the near-uniform prior on B in the data (E and
U are much higher-frequency classes). Still, we can achieve 99% precision for B with
recall above 70%. We do much better with E and U and see that when precision is 99%
for these two tags, recall does not drop below 90%. For the Chinese tagging task, B, E,
and U all have similar performance as we trade precision for recall. Here, as with the
English B tag, we achieve 99% precision with recall still above 70%.
We can see from these results that our finite-state tagging approach yields very
high accuracy on these tasks, as well as the ability to provide high precision (above
99%) operating points with a tolerable loss in recall. In what follows, we present two
approaches to adjusting precision: first by adjusting the overall precision and recall of
the tagger directly, as shown here; and second, by adjusting the precision and recall
of tagging results on a per-sentence basis. We then discuss how complexity-bounded
constraints are implemented in our experiments. In Section 7.1 we discuss empirical
results showing how adjusting the tagger in these ways affects parsing performance.
Figure 3
Tagger precision/recall tradeoff of B, E, and U on the development set for English (a) and
Chinese (b).
736
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
6.2.1 Global High Precision. Global high precision constraints (GHP) are implemented
using the cumulative score ratios directly to determine if wb ? B or B.4 Given the
cumulative score ratio as defined herein, we define the set B under global high precision
constraints as:
GHP?(wi . . .wN ) = {wi ? B : CSR(wi) > ?} (3)
The default decision boundary ? = 0 provides high tagging accuracy, but as men-
tioned in Section 6.2 we may want to increase the precision of the tagger so that
the subsequent parser is not over constrained. To do this, we increase the threshold
parameter ? towards positive infinity, potentially moving some words in the corpus
from B to B. The same procedure is applied to E and U tags.
6.2.2 Sentence-Level High Precision. Global high precision constraints, as defined here,
affect the precision of the tagger over the entire corpus, but may or may not affect
individual sentences. Because parsing operates on a sentence-by-sentence basis, it may
be beneficial to modify the precision of the tagger based on the current sentence being
processed. We call this approach sentence-level high precision (HP).5
To increase tagging precision at the sentence level, we compute the cumulative
scores Si and the cumulative scores ratio (CSR) at all word positions as described in
this article. We next tag all word positions with a CSR(wi) score less than zero with
B, and rank the remaining word positions according to their CSR score. The lowest-
ranking ? fraction of this sorted list are tagged as B, and the rest default to B. When
? = 0, zero percent of words are tagged with B (i.e., no constraints are applied); and
when ? = 1, 100 percent of the words with CSR(wi) > 0 are tagged with B. This ensures
that the high-precision threshold is adapted to each sentence, even if its absolute CSR
scores are not similar to others in the corpus.
6.2.3 Quadratic Bounds.We impose quadratic bounds on context-free parsing by restrict-
ing the number of open cells (case 3 in Section 4.3) to be less than or equal to ?N for some
tuning parameter ? and sentence lengthN. This only involves the sets B and E, as unary
constraints are restricted to span-1 cells. Given gold parse trees t? and a function T(B,E)
to generate the set of all valid parse trees satisfying constraints B and E, the optimal
quadratically bounded constraints would be:
Quad?(wi . . .wN ) = argmax
B,E
( max
t?T(B,E)
F1(t
?, t) s.t. |C3| ? ?N) (4)
that is, the set of constraints that provides the optimal F-score of the best remaining
candidate in the chart while still imposing the required bound on the number of C3
cells. This equation is an instance of combinatorial optimization and solving it exactly
is NP-complete. Furthermore, we do not have access to gold parse trees t? during
parsing and must rely on the posterior scores of the tagger.
We instead use a greedy approach to approximate Equation (4). To accomplish this,
at every sentence we first sort both the B and E CSR scores for each word position
4 In our experiments these scores are in the range ?103.
5 As a historical note, we referred to this approach as simply ?high precision constraints? in Roark and
Hollingshead (2008) and Roark and Hollingshead (2009).
737
Computational Linguistics Volume 38, Number 4
into a single list. Next, we assume all word positions are in B and E, then starting
from the top of the sorted list (highest posterior probability for set inclusion), we
continue to add word positions to their respective open set and compute the number
of open cells with the given constraints while |C3| < ?N. By doing this, we guarantee
that only a linear number of case 3 cells are open in the chart, which leads to quadratic
worst-case parsing complexity.
6.2.4 Linear Bounds. Imposing O(N) complexity bounds requires constraining the
size of the set B such that |B| ? ? for some constant ?. As with quadratic complexity
bounds, we wish to find the optimal set B to fulfill these requirements:
Linear?(wi . . .wN ) = argmax
B
( max
t?T(B)
F1(t
?, t) such that |B| ? ?) (5)
We again resort to a greedy method to determine the set B, as this optimization
problem is still NP-complete and gold trees are not available. B is constructed by sorting
all word positions by their CSR scores for B, and then adding only the highest-scoring
? entries to the inclusion set. All other word positions are closed and in the set B.
Because this method does not consider the set E to impose limits on processing,
it will be shown in Section 7.2 that O(N) complexity bounding is not as effective as
a stand-alone method when compared to the quadratic complexity or high precision
constraints presented here.
6.3 Parsers
We will present results constraining several different parsers. We first present results
for exhaustive parsing using both the CYK and the CONSTRAINEDCYK algorithms. We
use a basic CYK exhaustive parser, termed the BUBS parser,6 to parse with a simple
PCFG model that uses non-terminal node-labels as provided by the Penn Treebank,
after removing empty nodes, node indices, and function tags. The results presented
here replicate and extend the results presented in Roark and Hollingshead (2009), using
a different CYK parser.7 The BUBS parser is an open-source high-efficiency parser
that is grammar agnostic and can be run in either exhaustive mode or with various
approximate inference options (detailed more fully in Section 8.1). It has been shown to
parse exhaustively with very competitive or superior efficiency compared with other
highly optimized CYK parsers (Dunlop, Bodenstab, and Roark 2011). In contrast to
the results in Roark and Hollingshead (2009), here we present results with both left-
and right-binarized PCFGs induced using a Markov order-2 transform, as detailed in
Section 3.3, and also present results for parsing Chinese.
We will then present results applying finite-state chart constraints to state-of-the-
art parsers, and evaluate the additional efficiency gains these constraints provide, even
when these parsers are already heavily pruned. To simplify the presentation of these
6 http://code.google.com/p/bubs-parser.
7 Note that there are several differences between the two parsers, including the way in which grammars
are induced, leading to different baseline accuracies and parsing times. Most notably, the parser from
Roark and Hollingshead (2009) relied upon POS-tagger output, and collapsed unary productions in a
way that effectively led to parent annotation in certain unary chain constructions. The current results
do not exploit those additional annotations, hence the baseline F-measure accuracy is a couple of
percentage points lower.
738
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
results, we concentrate in Section 7.1 on parsing with the simple Markov order-2
grammars, and then move to higher accuracy parsers in Section 8.
7. CYK Parsing with Finite-State Chart Constraints
7.1 High-Precision Constraints
As mentioned in Section 6.2, we can adjust the severity of pruning to favor preci-
sion over recall. Figure 4a shows parse time versus F-measure labeled parse accuracy
on the English development set for the baseline (unconstrained) exact-inference CYK
parser, and for various parameterizations of global high precision (GHP), sentence-level
high precision (HP), and unary constraints. Note that we see accuracy increasing over
the baseline in Figure 4a with the imposition of these constraints at some operating
points. This is not surprising, though, as the finite-state tagger makes use of lexical
Figure 4
English development set results (WSJ Section 24) applying global high precision (GHP),
sentence-level high precision (HP), and unary constraints with the CONSTRAINEDCYK
algorithm. We sweep over multiple values of ? in (a) and plot results in (b), (c), and (d) with the
optimal value for each constraint found in (a). F-measure accuracy in (b) is computed over
binned sentence lengths. Figure (d) plots the same data as (c), but zoomed in.
739
Computational Linguistics Volume 38, Number 4
information that the simple PCFG does not, hence there is complementary information
added that improves the model. The best operating points?fast parsing and relatively
high accuracy?are achieved for GHP constraints at ? = 40, for sentence-level HP at
? = 0.95, and for unary constraints at ? = 40. These high-precision parameterizations
achieve roughly an order of magnitude speed-up and between 0.2 absolute (for unary
constraints) and 3.7 absolute (for high-precision constraints) F-measure improvement
over the baseline unconstrained parser.
In order to analyze how these constraints affect accuracy with respect to sentence
length, we turn to Figure 4b. In this plot, F-measure accuracy is computed over binned
sentence lengths in increments of ten. All sentences of length greater than 60 are in-
cluded in the final bin. As one might expect, accuracy declines with sentence length
for all models because the number of possible trees is exponential in the sentence length
and errors in one portion of the tree can adversely affect prediction of other constituents
in nearby structures. We see that all constraints provide accuracy gains over the baseline
at all sentence lengths, but point out that the gains by GHP B and E constraints are larger
for longer sentences. As stated earlier, this is due to themodel-correcting behavior of cell
constraints: Lexical features are leveraged to prune trees that the PCFG may favor but
are not syntactically correct with respect to the entire sentence. Because longer sentences
are poorly modeled by the PCFG, cell constraints play a larger role in restricting the
space of possible trees considered and correcting modeling errors.
We can get a different perspective of how these constraints affect parsing time by
considering the scatter plots in Figures 4c and 4d, which plot each sentence according
to its length and parsing time at four operating points: baseline (unconstrained); global
high precision at ? = 40; sentence-level high precision at ? = 0.95; and unary at ? =
40. Figure 4c shows data points with up to 80 words and 400 msec of parsing time.
Figure 4d zooms in to under 200 msec and up to 40 words. It can be seen in each graph
that unconstrained CYK parsing quickly leaves the plotted area via a steep cubic curve
(least-squares fit is N2.9). Unary constraints operate at the same cubic complexity as
the baseline, but with a constant factor decrease in parsing time (least-squares fit is
also N2.9). The plots for GHP and HP show dramatic decreases in typical-case runtime
compared with the baseline. We again run a least-squares fit to the data and find that
both GHP and HP constraints follow a N2.5 trajectory.
The empirical complexity and run-time performance of GHP and HP are nearly
identical relative to all other chart constraints. But looking a little closer, we see in
Figures 4c and 4d that that GHP constraints are slightly faster than HP for some
sentences, and accumulated over the entire development set, this leads to both higher
accuracy (75.1 vs. 74.6 F-measure) and faster parsing time (46 vs. 50 seconds).8 This
leads to the conclusion that when optimizing parameters for high-precision constraints,
we can better tune the overall pipeline by choosing an operating point based on the
corpus-level tagger performance as opposed to tuning for sentence-specific precision
goals. Consequently, other than Table 4, we only apply GHP constraints on test set
results in the remainder of this section.
Although Figure 4a displays the constrained F-measure parse accuracy as a function
of parsing time, one may also be interested in how tagger precision directly affects parse
accuracy. To answer this question, we apply high precision constraints to sets B and E in
isolation and plot results in Figure 5. Note that when only constraints on E are applied,
no chart cells can be completely closed and parsing time does not significantly decrease.
8 See Table 4 for additional performance comparisons.
740
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Likewise, when we apply constraints on B, a chart cell is either open to all constituents
(case 3) or closed to all constituents (case 1). When constraining either B or E, we are
predicting the structure of the final parse tree, which (as can be seen in Figure 5) has
large effects on parse F-measure.
We point out in Figure 5 that to achieve optimal parsing F-measure, tagger precision
must be above 97% for both English and Chinese. For both languages, B constraints
are more forgiving and do not adversely affect parsing F-measure as quickly as E con-
straints. These results may lead one to tune two separate high-precision parameters?
one to constrain B and one to constrain E. We ran such experiments but found no
significant gains compared to tying these parameters together.
7.2 Complexity-Bounded Constraints
Figure 6a plots F-measure accuracy versus time to parse the entire development set for
two complexity bounded constraints: O(N2) and O(N). We also include the previously
plotted global high-precision constraints for comparison. The large asterisk in the plot
depicts the baseline accuracy and efficiency of standard CYK parsing without con-
straints. We sweep over various parameterizations for each method, from very lightly
constrained to very heavily constrained. The complexity-bounded constraints are not
combined with the high-precision constraints for this plot (but are later in the article).
As can be seen in Figure 6a, the linear-bounded method does not, as applied,
achieve a favorable accuracy/efficiency tradeoff curve compared with the quadratic
bound or high-precision constraints. This is not surprising, given that no words are
excluded from the set E for this method, hence far fewer constraints overall are applied
with the linear-bounded constraints.
In addition, we see in Figure 6b that unlike high precision and quadratic constraints,
the linear method hurts accuracy on longer sentences over the baseline unconstrained
algorithm, notably for sentences greater than 50 words. We attribute this to the fact
that for longer sentences, say length 65, only ? = 16 words are allowed in B for linear
Figure 5
The effects of tagger precision on parsing F-measure. B and E constraints are applied in isolation;
no other constraints are used during parsing. Results on the English development set in (a) and
Chinese in (b). Baseline unconstrained F-measure accuracy is indicated with the horizontal
black line.
741
Computational Linguistics Volume 38, Number 4
Figure 6
English development set results (WSJ Section 24), applying complexity-bounding constraints
with the CONSTRAINEDCYK algorithm. We sweep over multiple values of ? in (a) and plot
results in (b), (c), and (d) with the optimal value for each constraint found in (a). F-measure
accuracy in (b) is computed over binned sentence lengths. Figure (d) plots the same data as (c),
but zoomed in.
constraints, which severely limits the search space for these sentences to the point of
pruning gold constituents and decreasing overall accuracy.
Next we turn to the scatter plots of Figures 6c and 6d. Fitting an exponential curve
to the data for each constraint via least-squares, we find that global high-precision
constraints follow a N2.5 trajectory, quadratic at N1.6, and linear at N1.4. It is interesting
that quadratic constraints actually perform at sub-quadratic run-time complexity. This
is because the quadric complexity proof in Section 4.4 assumes that the linear number
of open cells each process O(N) midpoints. But in practice, many midpoints are not
considered due to the heavily constrained chart, decreasing the average-case runtime
of CONSTRAINEDCYK with quadratically bounded constraints.
Also interesting is that linear constraints perform worse than O(N) at N1.4. We
attribute this to the nature of the data set. When parsing with linear constraints, we
see that for short sentences parsing complexity is actually cubic. Because we allow
a constant number of word positions in the open set B, sentences with length less
742
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
than ? are completely unconstrained and all chart cells remain open. In Figure 6d it
looks as if parsing becomes linear after the sentence length notably exceeds ?, around
N = 20. Assuming our corpus contained a disproportionate number of long sentences,
empirical complexity of linear constraints should approach O(N), but due to the large
number of short sentences, we see an empirical complexity greater than O(N).
Three important points can be taken away from Figure 6. First, although we can
provably constrain parsing to linear speeds, in practice this method is inferior to
both quadratically bounded constraints and high-precision constraints. Second, high-
precision constraints are more accurate (see Figure 6b) and more efficient (see Fig-
ure 6d) for shorter strings than the quadratically bound constraints; yet with longer
strings the quadratic constraints better control parsing time than the high-precision
constraints (see Figure 6c). Finally, at the ?crossover? point, where quadratic constraints
start becoming more efficient than high-precision constraints (roughly 50?60 words,
see Figure 4c), there is a larger variance in the parsing time with high-precision con-
straints versus those with quadratic bounds. This illustrates the difference between
the two methods of selecting constraints: High-precision constraints can provide very
strong typical-case gains, but there is no guarantee of worst-case performance. In this
way, the high-precision constraints are similar to other tagging-derived constraints like
POS-tags or chunks.
7.3 Combining Constraints
Depending on the length of the string, the complexity-bound constraints may close
more or fewer chart cells than the high-precision constraints?more for long strings,
fewer for short strings. We can achieve worst-case bounds along with superior typical-
case speed-ups by combining both methods. This is accomplished by taking the union
of high-precision B, E, andU constraints with their respective complexity-bounded sets.
When combining complexity-bound constraints with high-precision constraints, we
first chose operating parameters for each complexity-bounded method at the point
where efficiency is greatest and accuracy has yet to decline. These operating points can
be seen as the ?knee? of the curves in Figure 6a. For the quadratic complexity method,
we set ? = 4, limiting the number of open cells to 4N. For the linear complexity method,
we set ? = 12, limiting the number of word positions in B to a maximum of 12 members.
Table 4 displays F-measure accuracy and parsing time (in seconds) for many indi-
vidual and combined constraints on the development set: unconstrained CYK parsing;
unary constraints; global high-precision (GHP) and sentence-level high-precision (HP)
constraints; and O(N2) and O(N) complexity-bounded constraints (Quad and Linear,
respectively). We present all parsing results in Table 4 using both a left- and right-
binarized Markov order-2 grammar so that the effects of grammar binarization on
finite-state constraints can be evaluated. Pre-processing the grammar with a right or
left binarization alters the nature and distribution of child non-terminals for grammar
productions. Because B and E constraints prune the chart differently depending on the
grammar binarization, we suspect that one method may outperform the other due to
the branching bias of the language being parsed.
We find three general trends in Table 4. First, the efficiency benefits of combin-
ing constraints are relatively small. We suspect this is because the data set contains
mostly shorter sentences. Global high-precision constraints outperform the complexity
bounded constraints on sentences of length 10 to 50, which makes up the majority of
the development set. It is not until we parse longer sentences that the trends start to
differ and exhibit characteristics of the complexity bounds. Thus by combining high-
743
Computational Linguistics Volume 38, Number 4
Table 4
English development set results (WSJ Section 24) for the CONSTRAINEDCYK algorithm with both
left- and right-binarized Markov order-2 grammars under various individual and combined
constraints.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t-
b
in
a
ri
z
e
d
g
ra
m
m
a
r
None (baseline CYK) 71.5 74.5 68.8 451
GHP(40) 75.3 78.6 72.3 46 9.8x
HP(0.95) 74.6 77.8 71.7 50 8.9x
Quad(4) 73.8 77.0 70.9 70 6.4x
Linear(12) 72.4 75.4 69.6 106 4.3x
Unary(40) 72.0 75.4 68.9 386 1.2x
HP(0.95) + Quad(4) 74.6 77.8 71.7 48 9.5x
HP(0.95) + Linear(12) 74.4 77.6 71.5 48 9.5x
GHP(40) + Quad(4) 75.3 78.5 72.3 45 10.0x
GHP(40) + Linear(12) 75.1 78.4 72.1 44 10.2x
GHP(40) + Unary(40) 75.7 79.4 72.4 34 13.4x
GHP(40) + Unary(40) + Quad(4) 75.8 79.5 72.4 33 13.9x
L
e
ft
-b
in
a
ri
z
e
d
g
ra
m
m
a
r
None (baseline CYK) 71.7 74.5 69.1 774
GHP(40) 75.4 78.5 72.5 66 11.2x
HP(0.95) 74.9 77.9 72.1 75 10.3x
Quad(4) 74.0 77.0 71.2 99 7.8x
Linear(24) 71.7 74.5 69.1 448 1.7x
Unary(40) 71.9 75.2 69.0 607 1.3x
HP(0.95) + Quad(4) 74.9 78.0 72.1 69 11.2x
HP(0.95) + Linear(24) 74.7 77.8 71.9 71 11.0x
GHP(40) + Quad(4) 75.4 78.5 72.5 62 12.6x
GHP(40) + Linear(24) 75.2 78.4 72.3 62 12.6x
GHP(40) + Unary(40) 75.7 79.3 72.5 37 20.9x
GHP(40) + Unary(40) + Quad(4) 75.7 79.3 72.5 37 20.9x
precision and complexity constraints, we attain the typical-case efficiency benefits of
high-precision constraints with worst-case complexity bounds.
Second, we see that the efficiency gain combining unary and high-precision con-
straints is more than additive. Unary constraints alone for the right-binarized grammar
decreased parsing time by 1.3x, but in conjunction with high-precision constraints,
parsing time is decreased from 66 seconds to 37 seconds, an additional 1.8x speed-up.
We suspect that this additional gain comes from cache efficiencies (due to the heavily
pruned nature of the chart, the population of commonly-queried span-1 chart cells have
a higher likelihood of remaining in high-speed cache, decreasing overall parsing time),
but we leave empirical verification of this theory to future work.
The third trend we see in Table 4 is that there are significant efficiency differences
when parsing with a right- or left-binarized grammar. The difference in baseline per-
formance has been previously studied (Song, Ding, and Lin 2008; Dunlop, Bodenstab,
and Roark 2010), and our results confirm that a right-binarized grammar is superior for
parsing the WSJ treebank due to the right-branching bias of parse trees in this corpus.
Furthermore, linear constraints were far less effective with a left-binarized grammar,
744
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Table 5
English test set results (WSJ Section 23) for the CONSTRAINEDCYK algorithm with both left- and
right-binarized Markov order-2 grammars.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t None (baseline CYK) 71.7 74.6 69.0 628
Unary(40) 72.1 75.5 69.0 525 1.2x
GHP(40) 75.8 79.0 72.8 75 8.4x
GHP(40) + Unary(40) 76.1 79.8 72.7 57 11.0x
L
e
ft
None (baseline CYK) 72.0 74.8 69.4 1,063
Unary(40) 72.4 75.8 69.4 910 1.2x
GHP(40) 76.1 79.3 73.2 106 10.1x
GHP(40) + Unary(40) 76.4 80.0 73.1 60 17.9x
Table 6
Chinese test set results (PCTB Sections 271?300) for the CONSTRAINEDCYK algorithm with both
left- and right-binarized Markov order-2 grammars.
Constraints F1 Precision Recall Seconds Speed-up
R
ig
h
t None (baseline CYK) 60.5 64.6 56.9 157
Unary(20) 62.3 67.6 57.8 141 1.1x
GHP(20) 66.9 71.4 63.0 17 9.1x
GHP(20) + Unary(20) 68.9 74.4 64.1 15 10.2x
L
e
ft
None (baseline CYK) 60.4 64.5 56.9 269
Unary(20) 62.1 67.2 57.8 234 1.2x
GHP(20) 66.0 70.5 62.1 33 8.2x
GHP(20) + Unary(20) 68.0 73.5 63.2 23 11.7x
requiring a tuning parameter of ? = 24 such that accuracy was not adversely affected
(compare with ? = 12 for right-binarized results). This is also caused by the branching
bias inherent in the treebank. For example, consider a left-binarized grammar and
linear constraints; in the extreme case where ? = 0, only w1 will be in the open set B,
forcing all constituents in the final tree to start at w1. This results in a completely left-
branching tree. With a right-binarized grammar, only the last word position will be in
E, resulting in a completely right-branching tree. Thus, an overly constrained linear-
bounded parse will favor one branching direction over the other. Because the treebank
is biased towards right-branching trees, a right-binarized grammar is more favorable
when linear constraints are applied.
Finally, we note that after all constraints have been applied, the accuracy and
efficiency differences between the two binarization strategies nearly disappear.
To validate the selected operating points on unseen data, we present results on the
test sets for English in Table 5 and Chinese in Table 6.9 We parse individually with global
high-precision and unary constraints, then present the combined result as this gave the
best performance on the development set. In all cases, we see efficiency improvements
greater than 10-fold and accuracy improvements of 4.4 absolute F-measure for English,
9 We used the development set (articles 301?325 of the Penn Chinese treebank) to tune chart constraint
parameters for Chinese, exactly as was done for English.
745
Computational Linguistics Volume 38, Number 4
and 8.4 absolute F-measure for Chinese. Note that these efficiency improvements are
achieved with no additional techniques for speeding up search; modulo the cell closing
mechanism, the CYK parsing is exhaustive?it explores all possible category combina-
tions from all open child cells. Techniques such as coarse-to-fine, A? parsing, or beam-
search are all orthogonal to the current approach, and could be applied in conjunction
to achieve additional speedups.
In the next section, we investigate the use of chart constraints with a number of
high-accuracy parsers, and empirically evaluate the combination of our chart constraint
methods with popular heuristic search methods for parsing. These parsers include the
Charniak parser, the Berkeley parser, and an in-house beam-search parser.
8. High-Accuracy Parsing with Finite-State Chart Constraints
In this section we evaluate the additive efficiency gains provided by finite-state con-
straints to state-of-the-art parsers. We apply B, E, and U constraints to three parsers, all
of which already prune the search space to make decoding time with large grammars
more practical. Each high-accuracy parser that we evaluate also prunes the search space
in a different way?agenda-based search, coarse-to-fine pruning, and beam-search.
Applying finite-state constraints to these three distinct parsers demonstrates how our
constraints interact with other well-known pruning algorithms. In what follows we
describe the pruning inherent in the three parsers and how we apply finite-state con-
straints within each framework.
8.1 Parsers
8.1.1 Charniak Parser. The Charniak (2000) parser is a multi-stage, agenda-driven parser
that can be constrained by pruning edges before they are placed on the agenda. The first
stage of the Charniak parser uses an agenda and a simple PCFG to build a sparse chart,
which is used to limit the search in later stages with the full model. We focus on this
first stage, because it is here that we will be constraining the parser. The edges on the
agenda and in the chart are dotted rules, as described in Section 3.3. When edges are
created, they are pushed onto the agenda. Edges that are popped from the agenda are
placed in the chart, and then combined with other chart entries to create new edges that
are pushed onto the agenda. Once a complete edge spanning the whole string is placed
in the chart, at least one full solution must exist. Instead of terminating the initial chart
population at this point, a technique called ?over-parsing? is used that continues adding
edges to the chart (and agenda) until a parameterized number of additional edges have
been added. A small over-parsing value will heavily constrain the search space of the
later stages within the pipeline, and a large value will often increase accuracy at the
expense of efficiency. Upon reaching the desired number of edges, the next stage of
the pipeline receives the chart as input and any edges remaining on the agenda are
discarded.
We constrain the first stage of the Charniak parser by restricting agenda edges.
When an edge is created for cell (b, e), where b < e, it is not placed on the agenda if
either of the following two conditions hold: 1) wb ? B; or 2) the edge is complete and
we ? E. With these constraints, a large number of edges that would have previously
been considered in the first stage of this pipeline will now be ignored. This allows us
to either reduce the amount of over-parsing, which will increase efficiency, or maintain
the over-parsing threshold and expand the search space in more promising directions
according to the chart constraints. In this article, we have chosen to do the latter. Note
that speed-ups are still observed, presumably due to the parser finding a complete edge
746
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
spanning the whole sentence more quickly, thus leading to slight reductions in total
edges added to the chart.
8.1.2 Berkeley Parser. The Berkeley parser (Petrov and Klein 2007a) is a multi-level coarse-
to-fine parser that operates over a set of coarse-to-fine grammars, G1 . . .Gt. At each
grammar level, the inside and outside constituent probabilities are computed with
coarse grammarGi and used to prune the subsequent search withinGi+1. This continues
until the sentence is parsed with the target grammar Gt. The initial grammar G1 is
a Markov order-0 grammar, and the target grammar Gt is a latent-variable grammar
induced through multiple iterations of splitting and merging non-terminals to maxi-
mize the likelihood of the training data (Petrov and Klein 2007b). This explicit target
grammar is very large, consisting of 4.3 million productions, 2.4 million of which are
lexical productions. We refer to Gt as the Berkeley grammar.
We apply chart constraints to the Berkeley parser during the initial inside pass
with grammar G1. Inside scores are computed via the CONSTRAINEDCYK algorithm
of Algorithm 2 modulo the fact that the standard inside/outside sum over scores is
used in lines 10, 13, and 17 instead of the Viterbi-max. It is unnecessary to constrain
either the subsequent outside pass or the search with the following grammars G2 . . .Gt,
as the coarse-to-fine algorithm only considers constituents remaining in the chart from
the previous coarse grammar. Reducing the population of chart cells during the initial
G1 inside pass consequently prunes the search space for all levels of the coarse-to-fine
search. We also note that even though there are t = 6 grammar levels in our experiments
with the Berkeley parser, exhaustive parsing with G1 consumes nearly 50% of the total
parse time (Petrov, personal communication). Applying chart constraints during this
initial pass is where we see the largest efficiency gain?much more than when chart
constraints supplement coarse-to-fine pruning in subsequent passes.
8.1.3 BUBS Parser. The bottom?up beam-search parser (BUBS) is a variation of the CYK
algorithm where, at each chart cell, all possible edges are sorted by a Figure-of-Merit
(FOM) and only the k-best edges are retained (Bodenstab et al 2011). We follow this set-
up and use the Boundary FOM (Caraballo and Charniak 1997), but do not apply beam-
width prediction in these experiments as chart constraints and beam-width prediction
prune the search space in similar ways (see Bodenstab et al [2011] for a comparison of
the two methods). The BUBS parser is grammar agnostic, so to achieve high accuracy
we parse with the Berkeley latent variable grammar (Gt described in the previous
subsection), yet only require a single pass over the chart. The BUBS parser performs
Viterbi decoding and does not marginalize over the latent variables or compute the
max-rule solution as is done in the Berkeley parser. This leads to a lower F-measure
score in the final results even though both parsers use the same grammar.
In this article, we apply finite-state constraints to the BUBS parser in a fashion
almost identical to the CONSTRAINEDCYK algorithm. Because the BUBS parser is a
beam-search parser, the difference is that instead of retaining the max score for all non-
terminals Ai at each chart cell, we only retain the max score for the k-best non-terminals.
Otherwise, B, E, and U constraints are used to prune the search space in the same way.
8.2 High-Accuracy Parsing Results
Figure 7 displays accuracy and efficiency results of applying three independent
constraints to the three parsers: high precision, quadratically bounded, and unary
constraints. We sweep over possible tuning parameters from unconstrained (baseline
747
Computational Linguistics Volume 38, Number 4
Figure 7
English accuracy and efficiency results of applying high precision, quadratic, and unary
constraints at multiple values of ? to the Charniak, Berkeley, and BUBS parsers, all of which
already heavily prune the search space.
asterisk) to overly constrained such that accuracy is adversely affected. We also plot the
optimal combination of high precision and quadratic constraints (diamond) and the
combination of all three constraints (circle) for each parser which was computed via a
grid search over the joint parameter space.
There are many interesting patterns in Figure 7. First, all three constraints inde-
pendently improve the accuracy and efficiency of all three parsers, with the exception
of accuracy in the Berkeley parser. This is a powerful result considering each of these
parsers is simultaneously performing various alternative forms of pruning, which were
(presumably) tuned for optimal accuracy and efficiency on this same data set. We also
note that the efficiency gains from all three constraints are not identical. In particular,
high precision and quadratic constraints outperforms unary constraints in isolation. But
this should be expected as unary constraints only partially closeO(n) chart cells whereas
both high precision and quadratic constraints affect O(n2) chart cells. Nevertheless,
looking at the optimal point combining all three constraints, we see that adding unary
constraints to begin/end constraints does provide additional gains (in both accuracy
and efficiency) for the BUBS and Charniak parsers.
The Berkeley parser appears to benefit from B and E constraints, but sees almost
no gain from unary constraints. The reason for this has to do with the implementation
details of combining (joining) two child cells within the inner loop of the CYK algorithm
(line 8 in Algorithm 1). In bottom?up CYK parsing, to extend derivations of adjacent
substrings into new constituents spanning the combined string, one can either iterate
over all binary productions in the grammar and test if the new derivation is valid (we
call this ?grammar loop?), or one can take the cross-product of active entries in the cells
spanning the substrings and poll the grammar for possible derivations (we call this
?cross-product?). With the cross-product approach, fewer active entries in either child
cell leads to fewer grammar access operations. Thus, pruning constituents in smaller-
span cells directly affects the overall efficiency of parsing. On the other hand, with the
748
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
grammar loop method there is a constant number of grammar access operations (i.e.,
the number of grammar productions) and the number of active entries in each child
cell has little impact on efficiency. Therefore, with the grammar loop implementation
of the CYK algorithm, pruning techniques such as unary constraints will have very
little impact on the final run-time efficiency of the parser unless the list of grammar
productions is modified given the constraints. For example, alternate iterators over
grammar productions could be created such that they only consider a subset of all
possible productions. If the left child is a span-1 chart cell in U, then only grammar
productions with a POS tag as the left child need to be considered. Looping over
this smaller list, opposed to all possible grammar productions, can reduce the overall
runtime. The Berkeley parser contains the grammar-loop implementation of the CYK
algorithm. Although all grammar productions are iterated over at each cell, the parser
maintains meta-information indicating where non-terminals have been placed in the
chart, allowing it to quickly skip over a subset of the productions that are incompatible
with state of the chart. This optimization improves efficiency, but does not take full
advantage of restricted cell population imposed by unary constraints, and thus does
not benefit as greatly when compared to the BUBS or Charniak parsers.
The second trend we see in Figure 7 is that accuracy actually improves with ad-
ditional constraints. This is expected with the Charniak parser as we keep the amount
of search space fixed in hopes of pursuing more accurate global paths, but both the
Berkeley and BUBS parsers are simply eliminating search paths they would have
otherwise considered. Although it is unusual that pruning leads to higher accuracy
during search, it is not wholly unexpected here as our finite-state tagger makes use of
lexical relationships that the PCFG does not (i.e., lexical relationships based on the linear
string rather than over the syntactic structure). By leveraging this new information to
constrain the search space, we are indirectly improving the quality of the model. We
also suspect that the Berkeley parser sees less of an accuracy improvement than the
BUBS parsers because the coarse-to-fine pruning within the Berkeley parser is more
?globally informed? than the beam-search within the BUBS parser. By leveraging the
coarse-grained inside/outside distribution of trees over the input sentence, the Berkeley
parser can more intelligently prune the search space with respect to the target grammar
and may not benefit from the additional information inherent in the finite-state tagging
model.
The third observation we point out in Figure 7 is that we see no additional gains
from combining high-precision constraints with quadratic complexity constraints. With
all three parsers, high-precision constraints are empirically superior to quadratic con-
straints, even though high-precision constraints come with no guarantee on worst-case
complexity reduction. It is our hypothesis that the additional pruning provided by
quadratic constraints for exhaustive CYK parsing is already removed by the internal
pruning of each of the three high-accuracy parsers. We therefore report testing results
using only high-precision and unary constraints for these high-accuracy parsers.
We apply models tuned on the development set to unseen English test data (WSJ
Section 23) in Table 7, and Chinese test data (PCTB articles 271?300) in Table 8. For
English, we see similar trends as we did on the development set results: Decoding time
is nearly halved when chart constraints are applied to these already heavily constrained
parsers, without any loss in accuracy. We also see independent gains from both unary
and high-precision constraints, and additive efficiency gains when combined.
Applying chart constraints to Chinese parsing in Table 8 gives substantially larger
accuracy and efficiency gains than English for both the BUBS and Berkeley parser. In
particular, the accuracy of the BUBS parser increases by 2.3 points absolute (p = 0.0002),
749
Computational Linguistics Volume 38, Number 4
Table 7
English test set results (WSJ Section 23) applying sentence-level high precision and unary
constraints to three parsers with parameter settings tuned on development data.
Parser F1 Precision Recall Seconds Speed-up
BUBS (2010) 88.4 88.5 88.3 586
+ Unary(100) 88.5 88.7 88.3 486 1.2x
+ HP(0.9) 88.7 88.9 88.6 349 1.7x
+ HP(0.9) + Unary(100) 88.7 89.0 88.4 283 2.1x
Charniak (2000) 89.7 89.7 89.6 1,116
+ Unary(100) 89.8 89.8 89.7 900 1.2x
+ HP(0.8) 89.8 90.0 89.6 716 1.6x
+ HP(0.8) + Unary(100) 89.7 90.0 89.5 679 1.6x
Berkeley (2007) 90.2 90.3 90.0 564
+ Unary(125) 90.1 90.3 89.9 495 1.1x
+ HP(0.7) 90.2 90.4 90.0 320 1.8x
+ HP(0.7) + Unary(125) 90.2 90.4 89.9 289 2.0x
Table 8
Chinese test set results (PCTB articles 271?300) applying sentence-level high-precision and unary
constraints to two parsers with parameter settings tuned on development data.
Parser F1 Precision Recall Seconds Speed-up
BUBS (2010) 79.5 79.5 79.1 169
+ Unary(50) 80.7 82.1 79.4 153 1.1x
+ HP(0.8) 81.1 81.5 80.7 75 2.3x
+ HP(0.8) + Unary(50) 81.8 83.1 80.5 44 3.8x
Berkeley (2007) 83.9 84.5 83.3 141
+ Unary(50) 84.5 85.9 83.0 125 1.1x
+ HP(0.7) 84.5 85.1 83.8 64 2.2x
+ HP(0.7) + Unary(50) 84.7 86.1 83.4 57 2.5x
and the Berkeley parser increases by 0.8 points absolute to 84.7 (p = 0.0119), the highest
accuracy we are aware of for an individual model on this data set.10,11 These increases
relative to English may be surprising as chart constraint tagging accuracy for Chinese
is worse than English (see Table 3). We attribute this large gain to the lower baseline
accuracy of parsing with the Chinese treebank, allowing our method to contribute
additional syntactic constraints that were otherwise unmodeled by the PCFG.
9. Conclusion and Future Work
We have presented finite-state pre-processing methods to constrain context-free
parsing that reduce both the worst-case complexity and overall run time. Four unique
10 Significance was tested using stratified shuffling.
11 Zhang et al (2009) report an F-measure of 85.5 with a k-best combination of parsers, and Burkett, Blitzer,
and Klein (2010) report an F-measure of 86.0 by leveraging parallel English data for training, but our
model is trained from the Chinese treebank alone and is integrated into the Berkeley parser, making it
very efficient.
750
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
bounding methods were presented, with high-precision constraints showing superior
performance in empirical trials. Applying these constraints to context-free parsing
increased efficiency by over 20 times for exhaustive CYK parsing, and nearly doubled
the speed of the Charniak and Berkeley parsers?both of which have been previously
tuned for optimal accuracy/efficiency performance. We have shown that our method
generalizes across multiple grammars and languages, and that constraining both
multi-word chart cells and single-word chart cells produces additive efficiency gains.
In future work we plan to investigate additional methods to make context-free
parsing more efficient. In particular, we believe the dynamic programming chart could
be constrained even further if we take into account the population and structure of
chart cells at a finer level. For example, the B and E constraints we have presented here
do not take into account the height of the constituent they are predicting. Instead, they
simply open or close the entire diagonal in the chart. Refining this structured prediction
to bins, as is done in Zhang et al (2010), or classifying chart cells directly, as is done
in Bodenstab et al (2011), has shown promise, but we believe that further research in
this area would yield addition efficiency gains. In particular, those two papers do not
differentiate between non-terminals when opening or closing cells, and we hypothesize
that learning separate finite-state classifiers for automatically derived clusters of non-
terminals may increase performance.
Acknowledgments
Portions of this paper have appeared in
conference papers: Roark and Hollingshead
(2008), Roark and Hollingshead (2009), and
Bodenstab, Hollingshead, and Roark (2011).
We thank three anonymous reviewers for
their insightful comments and suggestions.
Also thanks to Aaron Dunlop for being so
swell. This research was supported in part
by NSF grants IIS-0447214 and IIS-0811745,
and DARPA grant HR0011-09-1-0041.
Any opinions, findings, conclusions,
or recommendations expressed in this
publication are those of the authors and
do not necessarily reflect the views of the
NSF or DARPA.
References
Bangalore, Srinivas and Aravind K. Joshi.
1999. Supertagging: an approach to
almost parsing. Computational Linguistics,
25:237?265.
Bergsma, Shane and Colin Cherry. 2010.
Fast and accurate arc filtering for
dependency parsing. In Proceedings
of the 23rd International Conference on
Computational Linguistics (Coling 2010),
pages 53?61, Beijing.
Black, E., S. Abney, S. Flickenger,
C. Gdaniec, C. Grishman, P. Harrison,
D. Hindle, R. Ingria, F. Jelinek, J. Klavans,
M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991.
Procedure for quantitatively comparing
the syntactic coverage of English
grammars. In Proceedings of the Workshop
on Speech and Natural Language, HLT ?91,
pages 306?311, Pacific Grove, CA.
Bodenstab, Nathan, Aaron Dunlop, Keith
Hall, and Brian Roark. 2011. Beam-width
prediction for efficient context-free
parsing. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics, pages 440?449, Portland, OR.
Bodenstab, Nathan, Kristy Hollingshead,
and Brian Roark. 2011. Unary constraints
for efficient context-free parsing. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics,
pages 676?681, Portland, OR.
Burkett, David, John Blitzer, and Dan Klein.
2010. Joint parsing and alignment with
weakly synchronized grammars. In Human
Language Technologies: The 2010 Annual
Conference of the North American Chapter
of the Association for Computational
Linguistics, HLT ?10, pages 127?135,
Los Angeles, CA.
Caraballo, Sharon A. and Eugene Charniak.
1997. New figures of merit for best-first
probabilistic chart parsing. Computational
Linguistics, 24:275?298.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 132?139, Seattle, WA.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt
751
Computational Linguistics Volume 38, Number 4
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 173?180, Sydney.
Cherry, Colin and Shane Bergsma. 2011.
Joint training of dependency parsing
filters through latent support vector
machines. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 200?205, Toulouse.
Clark, Stephen and James R. Curran.
2004. The importance of supertagging
for wide-coverage CCG parsing. In
Proceedings of COLING, pages 282?288,
Geneva.
Cocke, John and Jacob T. Schwartz. 1970.
Programming languages and their
compilers: Preliminary notes. Courant
Institute of Mathematical Sciences,
New York University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 1?8, Philadelphia, PA.
Djordjevic, Bojan, James R. Curran, and
Stephen Clark. 2007. Improving the
efficiency of a wide-coverage CCG parser.
In Proceedings of the 10th International
Conference on Parsing Technologies,
IWPT ?07, pages 39?47, Prague.
Dreyer, Markus, David A. Smith, and
Noah A. Smith. 2006. Vine parsing and
minimum risk reranking for speed and
precision. In Proceedings of the Tenth
Conference on Computational Natural
Language Learning (CoNLL-X),
pages 201?205, New York, NY.
Dunlop, Aaron, Nathan Bodenstab, and
Brian Roark. 2010. Reducing the grammar
constant: an analysis of CYK parsing
efficiency. Technical report CSLU-2010-02,
Oregon Health & Science University,
Beaverton, OR.
Dunlop, Aaron, Nathan Bodenstab,
and Brian Roark. 2011. Efficient
matrix-encoded grammars and low
latency parallelization strategies for CYK.
In Proceedings of the 12th International
Conference on Parsing Technologies (IWPT),
pages 163?174, Dublin.
Earley, Jay. 1970. An efficient context-free
parsing algorithm. Communications of the
ACM, 6(8):451?455.
Eisner, Jason and Noah A. Smith. 2005.
Parsing with soft and hard constraints on
dependency length. In Proceedings of the
Ninth International Workshop on Parsing
Technology (IWPT), pages 30?41,
Vancouver.
Glaysher, Elliot and Dan Moldovan. 2006.
Speeding up full syntactic parsing by
leveraging partial parsing decisions. In
Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 295?300,
Sydney.
Hollingshead, Kristy, Seeger Fisher, and
Brian Roark. 2005. Comparing and
combining finite-state and context-free
parsers. In Proceedings of the Human
Language Technology Conference and the
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 787?794, Vancouver.
Hollingshead, Kristy and Brian Roark.
2007. Pipeline iteration. In Proceedings
of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 952?959, Prague.
Kasami, Tadao. 1965. An efficient
recognition and syntax analysis
algorithm for context-free languages.
Technical report AFCRL-65-758, Air Force
Cambridge Research Lab, Bedford, MA.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn treebank. Computational
Linguistics, 19:313?330.
McDonald, Ryan, Fernando Pereira,
Kiril Ribarov, and Jan Hajic. 2005.
Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of
Human Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 523?530, Vancouver.
Nivre, Joakim. 2006. Constraints on
non-projective dependency parsing.
In Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL),
pages 73?80, Trento.
Petrov, Slav and Dan Klein. 2007a. Improved
inference for unlexicalized parsing. In
Proceedings of Human Language Technologies
2007: The Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 404?411,
Rochester, NY.
Petrov, Slav and Dan Klein. 2007b. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd National
Conference on Artificial Intelligence -
Volume 2, pages 1663?1666, Vancouver.
752
Roark, Hollingshead, and Bodenstab Chart Constraints for Reduced Complexity Parsing
Ratnaparkhi, Adwait. 1999. Learning to
parse natural language with maximum
entropy models.Machine Learning,
34(1-3):151?175.
Roark, Brian and Kristy Hollingshead.
2008. Classifying chart cells for quadratic
complexity context-free inference. In
Proceedings of the 22nd International
Conference on Computational Linguistics
(COLING), pages 745?752, Manchester.
Roark, Brian and Kristy Hollingshead. 2009.
Linear complexity context-free parsing
pipelines via chart constraints. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL),
pages 647?655, Boulder, CO.
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of HLT-NAACL, pages 134?141,
Edmonton.
Sogaard, Anders and Jonas Kuhn. 2009.
Using a maximum entropy-based tagger
to improve a very fast vine parser.
In Proceedings of the 11th International
Conference on Parsing Technologies,
IWPT ?09, pages 206?209, Paris.
Song, Xinying, Shilin Ding, and Chin-Yew
Lin. 2008. Better binarization for the CKY
parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 167?176,
Honolulu, HI.
Weiss, David, Benjamin Sapp, and Ben
Taskar. 2010. Sidestepping intractable
inference with structured ensemble
cascades. In Proceedings of NIPS,
pages 2415?2423, Vancouver.
Weiss, David and Benjamin Taskar. 2010.
Structured prediction cascades. Journal of
Machine Learning Research - Proceedings
Track, 9:916?923.
Xue, Naiwen, Fei Xia, Fu-dong Chiou, and
Marta Palmer. 2005. The Penn Chinese
treebank: Phrase structure annotation
of a large corpus. Natural Language
Engingeering, 11:207?238.
Younger, Daniel H. 1967. Recognition and
parsing of context-free languages in time
n3. Information and Control, 10(2):189?208.
Zhang, Hui, Min Zhang, Chew Lim Tan, and
Haizhou Li. 2009. K-best combination of
syntactic parsers. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing: Volume 3, EMNLP ?09,
pages 1552?1560, Singapore.
Zhang, Yue, Byung Gyu Ahn, Stephen Clark,
Curt Van Wyk, James R. Curran, and
Laura Rimell. 2010. Chart pruning for
fast lexicalised-grammar parsing. In
Proceedings of the 23rd International
Conference on Computational Linguistics,
pages 1472?1479, Beijing.
753

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 676?681,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unary Constraints for Efficient Context-Free Parsing
Nathan Bodenstab? Kristy Hollingshead? and Brian Roark?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?University of Maryland Institute for Advanced Computer Studies, College Park, MD
{bodensta,roark}@cslu.ogi.edu hollingk@umiacs.umd.edu
Abstract
We present a novel pruning method for
context-free parsing that increases efficiency
by disallowing phrase-level unary productions
in CKY chart cells spanning a single word.
Our work is orthogonal to recent work on
?closing? chart cells, which has focused on
multi-word constituents, leaving span-1 chart
cells unpruned. We show that a simple dis-
criminative classifier can learn with high ac-
curacy which span-1 chart cells to close to
phrase-level unary productions. Eliminating
these unary productions from the search can
have a large impact on downstream process-
ing, depending on implementation details of
the search. We apply our method to four pars-
ing architectures and demonstrate how it is
complementary to the cell-closing paradigm,
as well as other pruning methods such as
coarse-to-fine, agenda, and beam-search prun-
ing.
1 Introduction
While there have been great advances in the statis-
tical modeling of hierarchical syntactic structure in
the past 15 years, exact inference with such models
remains very costly and most rich syntactic mod-
eling approaches resort to heavy pruning, pipelin-
ing, or both. Graph-based pruning methods such
as best-first and beam-search have both be used
within context-free parsers to increase their effi-
ciency. Pipeline systems make use of simpler mod-
els to reduce the search space of the full model. For
example, the well-known Charniak parser (Char-
niak, 2000) uses a simple grammar to prune the
search space for a richer model in a second pass.
Roark and Hollingshead (2008; 2009) have re-
cently shown that using a finite-state tagger to close
cells within the CKY chart can reduce the worst-case
and average-case complexity of context-free pars-
ing, without reducing accuracy. In their work, word
positions are classified as beginning and/or ending
multi-word constituents, and all chart cells not con-
forming to these constraints can be pruned. Zhang
et al (2010) and Bodenstab et al (2011) both ex-
tend this approach by classifying chart cells with a
finer granularity. Pruning based on constituent span
is straightforwardly applicable to all parsing archi-
tectures, yet the methods mentioned above only con-
sider spans of length two or greater. Lexical and
unary productions spanning a single word are never
pruned, and these can, in many cases, contribute sig-
nificantly to the parsing effort.
In this paper, we investigate complementary
methods to prune chart cells with finite-state pre-
processing. Informally, we use a tagger to re-
strict the number of unary productions with non-
terminals on the right-hand side that can be included
in cells spanning a single word. We term these sin-
gle word constituents (SWCs) (see Section 2 for a
formal definition). Disallowing SWCs alters span-1
cell population from potentially containing all non-
terminals to just pre-terminal part-of-speech (POS)
non-terminals. In practice, this decreases the num-
ber of active states in span-1 chart cells by 70%,
significantly reducing the number of allowable con-
stituents in larger spans. Span-1 chart cells are also
the most frequently queried cells in the CKY algo-
rithm. The search over possible midpoints will al-
ways include two cells spanning a single word ? one
as the first left child and one as the last right child. It
is therefore critical that the number of active states
676
(a) Original tree (b) Transformed tree (c) Dynamic programming chart
Figure 1: Example parse structure in (a) the original Penn treebank format and (b) after standard transformations have been
applied. The black cells in (c) indicate CKY chart cells containing a single-word constituent from the transformed tree.
in these cells be minimized so that the number of
grammar access requests is also minimized. Note,
however, that some methods of grammar access ?
such as scanning through the rules of a grammar and
looking for matches in the chart ? achieve less of a
speedup from diminished cell population than oth-
ers, something we investigate in this paper.
Importantly, our method is orthogonal to prior
work on tagging chart constraints and we expect ef-
ficiency gains to be additive. In what follows, we
will demonstrate that a finite-state tagger can learn,
with high accuracy, which span-1 chart cells can be
closed to SWCs, and how such pruning can increase
the efficiency of context-free parsing.
2 Grammar and Parsing Preliminaries
Given a probabilistic context-free grammar (PCFG)
defined as the tuple (V, T, S?, P, ?) where V is the
set of non-terminals, T is the set of terminals, S? is a
special start symbol, P is the set of grammar produc-
tions, and ? is a mapping of grammar productions to
probabilities, we divide the set of non-terminals V
into two disjoint subsets VPOS and VPHR such that
VPOS contains all pre-terminal part-of-speech tags
and VPHR contains all phrase-level non-terminals.
We define a single word constituent (SWC) unary
production as any production A?B ? P such that
A ? VPHR and A spans (derives) a single word. An
example SWC unary production, VP? VBP, can be
seen in Figure 1b. Note that ROOT ? SBAR and
RB ? ?quickly? in Figure 1b are also unary pro-
ductions, but by definition they are not SWC unary
productions.
One implementation detail necessary to leverage
the benefits of sparsely populated chart cells is the
grammar access method used by the inner loop of
the CKY algorithm.1 In bottom-up CKY parsing,
to extend derivations of adjacent substrings into new
constituents spanning the combined string, one can
either iterate over all binary productions in the gram-
mar and test if the new derivation is valid (gram-
mar loop), or one can take the cross-product of ac-
tive states in the cells spanning the substrings and
poll the grammar for possible derivations (cross-
product). With the cross-product approach, fewer
active states in either child cell leads to fewer gram-
mar access operations. Thus, pruning constituents
in lower cells directly affects the overall efficiency
of parsing. On the other hand, with the grammar
loop method there is a constant number of gram-
mar access operations (i.e., the number of grammar
rules) and the number of active states in each child
cell has no impact on efficiency. Therefore, with
the grammar loop implementation of the CYK algo-
rithm, pruning techniques such as unary constraints
will have very little impact on the final run-time effi-
ciency of the parser. We will report results in Section
5 with parsers using both approaches.
3 Treebank Unary Productions
In this section, we discuss the use of unary produc-
tions both in the Penn WSJ treebank (Marcus et al,
1999) and during parsing by analyzing their func-
tion and frequency. All statistics reported here are
computed from sections 2-21 of the treebank.
A common pre-processing step in treebank pars-
ing is to transform the original WSJ treebank be-
fore training and evaluation. There is some flex-
1Some familiarity with the CKY algorithm is assumed. For
details on the algorithm, see Roark and Sproat (2007).
677
Orig. Trans.
Empty nodes 48,895 0
Multi-Word Const. unaries 1,225 36,608
SWC unaries 98,467 105,973
Lexical unaries 950,028 950,028
Pct words with SWC unary 10.4% 11.2%
Table 1: Unary production counts from sections 2-21 of the
original and transformed WSJ treebank. All multisets are dis-
joint. Lexical unary count is identical to word count.
ibility in this process, but most pre-processing ef-
forts include (1) affixing a ROOT unary production
to the root symbol of the original tree, (2) removal
of empty nodes, and (3) striping functional tags and
cross-referencing annotations. See Figure 1 for an
example. Additional transforms include (4) remov-
ing X? X unary productions for all non-terminals
X, (5) collapsing unary chains to a single (possibly
composite) unary production (Klein and Manning,
2001), (6) introducing new categories such as AUX
(Charniak, 1997), and (7) collapsing of categories
such as PRT and ADVP (Collins, 1997). For this
paper we only apply transforms 1-3 and otherwise
leave the treebank in its original form. We also note
that ROOT unaries are a special case that do not af-
fect search, and we choose to ignore them for the
remainder of this paper.
These tree transformations have a large impact
on the number and type of unary productions in
the treebank. Table 1 displays the absolute counts
of unaries in the treebank before and after process-
ing. Multi-word constituent unary productions in the
original treebank are rare and used primarily to mark
quantifier phrases as noun phrases. But due to the
removal of empty nodes, the transformed treebank
contains many more unary productions that span
multiple words, such as S ? VP, where the noun
phrase was left unspecified in the original clause.
The number of SWC unaries is relatively un-
changed after processing the original treebank, but
note that only 11.2% of words in the transformed
treebank are covered by SWCs. This implies that
we are unnecessarily adding SWC productions to al-
most 90% of span-1 chart cells during search. One
may argue that an unsmoothed grammar will nat-
urally disallow most SWC productions since they
are never observed in the training data, for example
Mk2 Mk2+S Latent
|VPOS| 45 45 582
|VPHR| 26 26 275
SWC grammar rules 159 1,170 91,858
Active VPOS states 2.5 45 75
Active VPHR states 5.9 26 152
Table 2: Grammar statistics and averaged span-1 active state
counts for exhaustive parsing of section 24 using a Markov
order-2 (Mk2), a smoothed Markov order-2 (Mk2+S), and the
Berkeley latent variable (Latent) grammars.
VP ? DT. This is true to some extent, but gram-
mars induced from the WSJ treebank are notorious
for over-generation. In addition, state-of-the-art ac-
curacy in context-free parsing is often achieved by
smoothing the grammar, so that rewrites from any
one non-terminal to another are permissible, albeit
with low probability.
To empirically evaluate the impact of SWCs on
span-1 chart cells, we parse the development set
(section 24) with three different grammars induced
from sections 2-21. Table 2 lists averaged counts
of active Viterbi states (derivations with probabil-
ity greater than zero) from span-1 cells within the
dynamic programming chart, as well as relevant
grammar statistics. Note that these counts are ex-
tracted from exhaustive parsing ? no pruning has
been applied. We notice two points of interest.
First, although |VPOS| > |VPHR|, for the unsmoothed
grammars more phrase-level states are active within
the span-1 cells than states derived from POS tags.
When parsing with the Markov order-2 grammar,
70% of active states are non-terminals from VPHR,
and with the latent-variable grammar, 67% (152 of
227). This is due to the highly generative nature
of SWC productions. Second, although using a
smoothed grammar maximizes the number of active
states, the unsmoothed grammars still provide many
possible derivations per word.
Given the infrequent use of SWCs in the treebank,
and the search-space explosion incurred by includ-
ing them in exhaustive search, it is clear that restrict-
ing SWCs in contexts where they are unlikely to oc-
cur has the potential for large efficiency gains. In the
next section, we discuss how to learn such contexts
via a finite-state tagger.
678
4 Tagging Unary Constraints
To automatically predict if word wi from sentence
w can be spanned by an SWC production, we train a
binary classifier from supervised data using sections
2-21 of the Penn WSJ Treebank for training, section
00 as heldout, and section 24 as development. The
class labels of all words in the training data are ex-
tracted from the treebank, where wi ? U if wi is
observed with a SWC production and wi ? U other-
wise. We train a log linear model with the averaged
perceptron algorithm (Collins, 2002) using unigram
word and POS-tag2 features from a five word win-
dow. We also trained models with bi-gram and tri-
gram features, but tagging accuracy did not improve.
Because the classifier output is imposing hard
constraints on the search space of the parser, we
may want to choose a tagger operating point that fa-
vors precision over recall to avoid over-constraining
the downstream parser. To compare the tradeoff be-
tween possible precision/recall values, we apply the
softmax activation function to the perceptron output
to obtain the posterior probability of wi ? U :
P (U |wi, ?) = (1 + exp(?f(wi) ? ?))
?1 (1)
where ? is a vector of model parameters and f(?) is a
feature function. The threshold 0.5 simply chooses
the most likely class, but to increase precision we
can move this threshold to favor U over U . To tune
this value on a per-sentence basis, we follow meth-
ods similar to Roark & Hollingshead (2009) and
rank each word position with respect to its poste-
rior probability. If the total number of words wi
with P (U |wi, ?) < 0.5 is k, we decrease the thresh-
old value from 0.5 until ?k words have been moved
from class U to U , where ? is a tuning parameter be-
tween 0 and 1. Although the threshold 0.5 produces
tagging precision and recall of 98.7% and 99.4%
respectively, we can adjust ? to increase precision
as high as 99.7%, while recall drops to a tolerable
82.1%. Similar methods are used to replicate cell-
closing constraints, which are combined with unary
constraints in the next section.
2POS-tags were provided by a separately trained tagger.
5 Experiments and Results
To evaluate the effectiveness of unary constraints,
we apply our technique to four parsers: an exhaus-
tive CKY chart parser (Cocke and Schwartz, 1970);
the Charniak parser (Charniak, 2000), which uses
agenda-based two-level coarse-to-fine pruning; the
Berkeley parser (Petrov and Klein, 2007a), a multi-
level coarse-to-fine parser; and the BUBS parser
(Bodenstab et al, 2011), a single-pass beam-search
parser with a figure-of-merit constituent ranking
function. The Berkeley and BUBS parsers both
parse with the Berkeley latent-variable grammar
(Petrov and Klein, 2007b), while the Charniak
parser uses a lexicalized grammar, and the exhaus-
tive CKY algorithm is run with a simple Markov
order-2 grammar. All grammars are induced from
the same data: sections 2-21 of the WSJ treebank.
Figure 2 contrasts the merit of unary constraints
on the three high-accuracy parsers, and several inter-
esting comparisons emerge. First, as recall is traded
for precision within the tagger, each parser reacts
quite differently to the imposed constraints. We ap-
ply constraints to the Berkeley parser during the ini-
tial coarse-pass search, which is simply an exhaus-
tive CKY search with a coarse grammar. Applying
unary and cell-closing constraints at this point in the
coarse-to-fine pipeline speeds up the initial coarse-
pass significantly, which accounted for almost half
of the total parse time in the Berkeley parser. In ad-
dition, all subsequent fine-pass searches also bene-
fit from additional pruning as their search is guided
by the remaining constituents of the previous pass,
which is the intersection of standard coarse-to-fine
pruning and our imposed constraints.
We apply constraints to the Charniak parser dur-
ing the first-pass agenda-based search. Because an
agenda-based search operates at a constituent level
instead of a cell/span level, applying unary con-
straints alters the search frontier instead of reduc-
ing the absolute number of constituents placed in the
chart. We jointly tune lambda and the internal search
parameters of the Charniak parser until accuracy de-
grades.
Application of constraints to the CKY and BUBS
parsers is straightforward as they are both single
pass parsers ? any constituent violating the con-
straints is pruned. We also note that the CKY and
679
Figure 2: Development set results applying unary constraints
at multiple values of ? to three parsers.
BUBS parsers both employ the cross-product gram-
mar access method discussed in Section 2, while
the Berkeley parser uses the grammar loop method.
This grammar access difference dampens the benefit
of unary constraints for the Berkeley parser.3
Referring back to Figure 2, we see that both speed
and accuracy increase in all but the Berkeley parser.
Although it is unusual that pruning leads to higher
accuracy during search, it is not unexpected here as
our finite-state tagger makes use of lexical relation-
ships that the PCFG does not. By leveraging this
new information to constrain the search space, we
are indirectly improving the quality of the model.
Finally, there is an obvious operating point for
each parser at which the unary constraints are too
severe and accuracy deteriorates rapidly. For test
conditions, we set the tuning parameter ? based on
the development set results to prune as much of the
search space as possible before reaching this degra-
dation point.
Using lambda-values optimized for each parser,
we parse the unseen section 23 test data and present
results in Table 3. We see that in all cases, unary
constraints improve the efficiency of parsing without
significant accuracy loss. As one might expect, ex-
haustive CKY parsing benefits the most from unary
constraints since no other pruning is applied. But
even heavily pruned parsers using graph-based and
pipelining techniques still see substantial speedups
3The Berkeley parser does maintain meta-information about
where non-terminals have been placed in the chart, giving it
some of the advantages of cross-product grammar access.
Parser F-score Seconds Speedup
CKY 72.2 1,358
+ UC (?=0.2) 72.6 1,125 1.2x
+ CC 74.3 380 3.6x
+ CC + UC 74.6 249 5.5x
BUBS 88.4 586
+ UC (?=0.2) 88.5 486 1.2x
+ CC 88.7 349 1.7x
+ CC + UC 88.7 283 2.1x
Charniak 89.7 1,116
+ UC (?=0.2) 89.7 900 1.2x
+ CC 89.7 716 1.6x
+ CC + UC 89.6 679 1.6x
Berkeley 90.2 564
+ UC (?=0.4) 90.1 495 1.1x
+ CC 90.2 320 1.8x
+ CC + UC 90.2 289 2.0x
Table 3: Test set results applying unary constraints (UC) and
cell-closing (CC) constraints (Roark and Hollingshead, 2008)
to various parsers.
with the additional application of unary constraints.
Furthermore, unary constraints consistently provide
an additive efficiency gain when combined with cell-
closing constraints.
6 Conclusion
We have presented a new method to constrain
context-free chart parsing and have shown it to be or-
thogonal to many forms of graph-based and pipeline
pruning methods. In addition, our method parallels
the cell closing paradigm and is an elegant com-
plement to recent work, providing a finite-state tag-
ging framework to potentially constrain all areas of
the search space ? both multi-word and single-word
constituents.
Acknowledgments
We would like to thank Aaron Dunlop for his valu-
able discussions, as well as the anonymous review-
ers who gave very helpful feedback. This research
was supported in part by NSF Grants #IIS-0447214,
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA.
680
References
Nathan Bodenstab, Aaron Dunlop, Keith Hall, and Brian
Roark. 2011. Beam-width prediction for efficient
context-free parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, Portland, Oregon. Association for Com-
putational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Arti-
ficial Intelligence, pages 598?603, Menlo Park, CA.
AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 132?139, Seattle, Washington.
Morgan Kaufmann Publishers Inc.
John Cocke and Jacob T. Schwartz. 1970. Programming
languages and their compilers. Technical report Pre-
liminary notes, Courant Institute of Mathematical Sci-
ences, NYU.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, page 1623, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1?
8, Philadelphia, July. Association for Computational
Linguistics.
Dan Klein and Christopher D. Manning. 2001. Parsing
with treebank grammars: Empirical bounds, theoret-
ical models, and the structure of the Penn treebank.
In Proceedings of 39th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 338?345,
Toulouse, France, July. Association for Computational
Linguistics.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, ed-
itors, Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING 2008),
pages 745?752, Manchester, UK, August. Association
for Computational Linguistics.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647?655, Boulder, Colorado,
June. Association for Computational Linguistics.
Brian Roark and Richard W Sproat. 2007. Computa-
tional Approaches to Morphology and Syntax. Oxford
University Press, New York.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast lexicalised-grammar parsing. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1472?1479, Beijing,
China, June.
681
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 344?350,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Noisy SMS Machine Translation in Low-Density Languages
Vladimir Eidelman?, Kristy Hollingshead?, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,hollingk,resnik}@umiacs.umd.edu
Abstract
This paper presents the system we developed
for the 2011 WMT Haitian Creole?English
SMS featured translation task. Applying stan-
dard statistical machine translation methods to
noisy real-world SMS data in a low-density
language setting such as Haitian Creole poses
a unique set of challenges, which we attempt
to address in this work. Along with techniques
to better exploit the limited available train-
ing data, we explore the benefits of several
methods for alleviating the additional noise
inherent in the SMS and transforming it to
better suite the assumptions of our hierarchi-
cal phrase-based model system. We show
that these methods lead to significant improve-
ments in BLEU score over the baseline.
1 Introduction
For the featured translation task of the Sixth Work-
shop on Statistical Machine Translation, we devel-
oped a system for translating Haitian Creole Emer-
gency SMS messages. Given the nature of the task,
translating text messages that were sent during the
January 2010 earthquake in Haiti to an emergency
response service called Mission 4636, we were not
only faced with the problem of dealing with a low-
density language, but additionally, with noisy, real-
world data in a domain which has thus far received
relatively little attention in statistical machine trans-
lation. We were especially interested in this task be-
cause of the unique set of challenges that it poses
for existing translation systems. We focused our re-
search effort on techniques to better utilize the lim-
ited available training resources, as well as ways in
which we could automatically alleviate and trans-
form the noisy data to our advantage through the
use of automatic punctuation prediction, finite-state
raw-to-clean transduction, and grammar extraction.
All these techniques contributed to improving trans-
lation quality as measured by BLEU score over our
baseline system.
The rest of this paper is structured as follows.
First, we provide a brief overview of our baseline
system in Section 2, followed by an examination of
issues posed by this task and the steps we have taken
to address them in Section 3, and finally we con-
clude with experimental results and additional anal-
ysis.
2 System Overview
Our baseline system is based on a hierarchical
phrase-based translation model, which can formally
be described as a synchronous context-free gram-
mar (SCFG) (Chiang, 2007). Our system is imple-
mented in cdec, an open source framework for align-
ing, training, and decoding with a number of differ-
ent translation models, including SCFGs. (Dyer et
al., 2010). 1 SCFG grammars contain pairs of CFG
rules with aligned nonterminals, where by introduc-
ing these nonterminals into the grammar, such a sys-
tem is able to utilize both word and phrase level re-
ordering to capture the hierarchical structure of lan-
guage. SCFG translation models have been shown
to produce state-of-the-art translation for most lan-
guage pairs, as they are capable of both exploit-
ing lexical information for and efficiently comput-
ing all possible reorderings using a CKY-based de-
coder (Dyer et al, 2009).
1http://cdec-decoder.org
344
One benefit of cdec is the flexibility allowed with
regard to the input format, as it expects either a
string, lattice, or context-free forest, and subse-
quently generates a hypergraph representing the full
translation forest without any pruning. This forest
can now be rescored, by intersecting it with a lan-
guage model for instance, to obtain output transla-
tions. These capabilities of cdec allow us to perform
the experiments described below, which may have
otherwise proven to be quite impractical to carry out
in another system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), lexical translation
probabilities Plex(e|f) and Plex(f |e), a count of the
total number of rules used, a target word penalty,
and a count of the number of times the glue rule
is used. The number of non-terminals allowed in
a synchronous grammar rule was restricted to two,
and the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-based transla-
tion grammar was extracted using a suffix array rule
extractor (Lopez, 2007).
To optimize the feature weights for our model, we
used an implementation of the hypergraph minimum
error rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary loss
function. The error function we used was BLEU (Pa-
pineni et al, 2002), and the decoder was configured
to use cube pruning (Huang and Chiang, 2007) with
a limit of 100 candidates at each node.
2.1 Data Preparation
The SMS messages were originally translated by
English speaking volunteers for the purpose of pro-
viding first responders with information and loca-
tions requiring their assistance. As such, in order to
create a suitable parallel training corpus from which
to extract a translation grammar, a number of steps
had to be taken in addition to lowercasing and tok-
enizing both sides of training data. Many of the En-
glish translations had additional notes sections that
were added by the translator to the messages with
either personal notes or further informative remarks.
As these sections do not correspond to any text on
the source side, and would therefore degrade the
alignment process, these had to be identified and re-
moved. Furthermore, the anonymization of the data
resulted in tokens such as firstname and phonenum-
ber which were prevalent and had to be preserved
as they were. Since the total amount of Haitian-
English parallel data provided is quite limited, we
found additional data and augmented the available
set with data gathered by the CrisisCommons group
and made it available to other WMT participants.
The combined training corpus from which we ex-
tracted our grammar consisted of 123,609 sentence
pairs, which was then filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) to obtain one-to-many align-
ments in either direction and symmetrized using the
grow-diag-final-and method (Koehn et al, 2003).
We trained a 5-gram language model using the
SRI language modeling toolkit (Stolcke, 2002) from
the English monolingual News Commentary and
News Crawl language modeling training data pro-
vided for the shared task and the English portion of
the parallel data with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1996). We have previ-
ously found that since the beginnings and ends of
sentences often display unique characteristics that
are not easily captured within the context of the
model, explicitly annotating beginning and end of
sentence markers as part of our translation process
leads to significantly improved performance (Dyer
et al, 2009).
A further difficulty of the task stems from the fact
that there are two versions of the SMS test set, a raw
version, which contains the original messages, and a
clean version which was post-edited by humans. As
the evaluation of the task will consist of translating
these two versions of the test set, our baseline sys-
tem consisted of two systems, one built on the clean
data using the 900 sentences in SMS dev clean to
tune our feature weights, and evaluated using SMS
devtest clean, and one built analogously for the raw
data tuned on the 900 sentences in SMS dev raw and
evaluated on SMS devtest raw. We report results on
these sets as well as the 1274 sentences in the SMS
test set.
3 Experimental Variation
The results produced by the baseline systems are
presented in Table 1. As can be seen, the clean ver-
sion performs on par with the French-English trans-
345
BASELINE
Version Set BLEU TER
clean
dev 30.36 56.04
devtest 28.15 57.45
test 27.97 59.19
raw
dev 25.62 63.27
devtest 24.09 63.82
test 23.33 65.93
Table 1: Baseline system BLEU and TER scores
lation quality in the 2011 WMT shared translation
task,2 and significantly outperforms the raw version,
despite the content of the messages being identical.
This serves to underscore the importance of proper
post-processing of the raw data in order to attempt to
close the performance gap between the two versions.
Through analysis of the raw and clean data we iden-
tified several factors which we believe greatly con-
tribute to the difference in translation output. We
examine punctuation in Section 3.2, grammar post-
processing in Section 3.3, and morphological differ-
ences in Sections 3.4 and 3.5.
3.1 Automatic Resource Confidence Weighting
A practical technique when working with a low-
density language with limited resources is to du-
plicate the same trusted resource multiple times in
the parallel training corpus in order for the transla-
tion probabilities of the duplicated items to be aug-
mented. For instance, if we have confidence in the
entries of the glossary and dictionary, we can dupli-
cate them 10 times in our training data to increase
the associated probabilities. The aim of this strat-
egy is to take advantage of the limited resources and
exploit the reliable ones.
However, what happens if some resources are
more reliable than others? Looking at the provided
resources, we saw that in the Haitisurf dictionary,
the entry for paske is matched with for, while in
glossary-all-fix, paske is matched with because. If
we then consider the training data, we see that in
most cases, paske is in fact translated as because.
Motivated by this type of phenomenon, we em-
ployed an alternative strategy to simple duplication
which allows us to further exploit our prior knowl-
edge.
2http://matrix.statmt.org/matrix
First, we take the previously word-aligned base-
line training corpus and for each sentence pair and
word ei compute the alignment link count c(ei, fj)
over the positions j that ei is aligned with, repeating
for c(fi, ej) in the other direction. Then, we pro-
cess each resource we are considering duplicating,
and augment its score by c(ei, fj) for every pair of
words which was observed in the training data and
is present in the resource. This score is then normal-
ized by the size of the resource, and averaged over
both directions. The outcome of this process is a
score for each resource. Taking these scores on a
log scale and pinning the top score to associate with
20 duplications, the result is a decreasing number of
duplications for each subsequent resources, based on
our confidence in its entries. Thus, every entry in the
resource receives credit, as long as there is evidence
that the entries we have observed are reliable. On
our set of resources, the process produces a score of
17 for the Haitisurf dictionary and 183 for the glos-
sary, which is in line with what we would expect.
It may be that the resources may have entries which
occur in the test set but not in the training data, and
thus we may inadvertently skew our distribution in
a way which negatively impacts our performance,
however, overall we believe it is a sound assumption
that we should bias ourselves toward the more com-
mon occurrences based on the training data, as this
should provide us with a higher translation probabil-
ity from the good resources since the entries are re-
peated more often. Once we obtain a proper weight-
ing scheme for the resources, we construct a new
training corpus, and proceed forward from the align-
ment process.
Table 2 presents the BLEU and TER results of the
standard strategy of duplication against the confi-
dence weighting scheme outlined above. As can be
CONF. WT. X10
Version Set BLEU TER BLEU TER
clean
dev 30.79 55.71 30.61 55.31
devtest 27.92 57.66 28.22 57.06
test 27.97 59.65 27.74 59.34
raw
dev 26.11 62.64 25.72 62.99
devtest 24.16 63.71 24.18 63.71
test 23.66 65.69 23.06 66.78
Table 2: Confidence weighting versus x10 duplication
346
seen, the confidence weighting scheme substantially
outperforms the duplication for the dev set of both
versions, but these improvements do not carry over
to the clean devtest set. Therefore, for the rest of the
experiments presented in the paper, we will use the
confidence weighting scheme for the raw version,
and the standard duplication for the clean version.
3.2 Automatic Punctuation Prediction
Punctuation does not usually cause a problem in
text-based machine translation, but this changes
when venturing into the domain of SMS. Punctua-
tion is very informative to the translation process,
providing essential contextual information, much
as the aforementioned sentence boundary markers.
When this information is lacking, mistakes which
would have otherwise been avoided can be made.
Examining the data, we see there is substantially
more punctuation in the clean set than in the raw.
For example, there are 50% more comma?s in the
clean dev set than in the raw. A problem of lack of
punctuation has been studied in the context of spo-
ken language translation, where punctuation predic-
tion on the source language prior to translation has
been shown to improve performance (Dyer, 2007).
We take an analogous approach here, and train a hid-
den 5-gram model using SRILM on the punctuated
portion of the Haitian side of the parallel data. We
then applied the model to punctuate the raw dev set,
and tuned a system on this punctuated set. How-
ever, the translation performance did not improve.
This may have been do to several factors, including
the limited size of the training set, and the lack of
in-domain punctuated training data. Thus, we ap-
plied a self-training approach. We applied the punc-
tuation model to the SMS training data, which is
only available in the raw format. Once punctuated,
we re-trained our punctuation prediction model, now
including the automatically punctuated SMS data
AUTO-PUNC
Version Set BLEU TER
raw
dev 26.09 62.84
devtest 24.38 64.26
test 23.59 65.91
Table 3: Automatic punctuation prediction results
as part of the punctuation language model training
data. We use this second punctuation prediction
model to predict punctuation for the tuning and eval-
uation sets. We continue by creating a new parallel
training corpus which substitutes the original SMS
training data with the punctuated version, and build
a new translation system from it. The results from
using the self-trained punctuation method are pre-
sented in Table 3. Future experiments on the raw
version are performed using this punctuation.
3.3 Grammar Filtering
Although the grammars of a SCFG model per-
mit high-quality translation, the grammar extraction
procedure extracts many rules which are formally li-
censed by the model, but are otherwise incapable of
helping us produce a good translation. For example,
in this task we know that the token firstname must al-
ways translate as firstname, and never as phonenum-
ber. This refreshing lack of ambiguity allows us to
filter the grammar after extracting it from the train-
ing corpus, removing any grammar rule where these
conditions are not met, prior to decoding. Filtering
removed approximately 5% of the grammar rules.3
Table 4 shows the results of applying grammar fil-
tering to the raw and clean version.
GRAMMAR
Version Set BLEU TER
clean
dev 30.88 54.53
devtest 28.69 56.21
test 28.29 58.78
raw
dev 26.41 62.47
devtest 24.47 63.26
test 23.96 65.82
Table 4: Results of filtering the grammar in a post-
processing step before decoding
3.4 Raw-Clean Segmentation Lattice
As noted above, a major cause of the performance
degradation from the clean to the raw version is re-
lated to the morphological errors in the messages.
Figure 1 presents a segmentation lattice with two
versions of the same sentence; the first being from
3We experimented with more aggressive filtering based
on punctuation and numbers, but translation quality degraded
rapidly.
347
the raw version, and the second from the clean. We
can see that that Ilavach has been broken into two
segments, while ki sou has been combined into one.
Since we do not necessarily know in advance
which segmentation is the correct one for a better
quality translation, it may be of use to be able to
utilize both segmentations and allow the decoder to
learn the appropriate one. In previous work, word
segmentation lattices have been used to address the
problem of productive compounding in morphologi-
cally rich languages, such as German, where mor-
phemes are combined to make words but the or-
thography does not delineate the morpheme bound-
aries. These lattices encode alternative ways of seg-
menting compound words, and allow the decoder
to automatically choose which segmentation is best
for translation, leading to significantly improved re-
sults (Dyer, 2009). As opposed to building word
segmentation lattices from a linguistic morphologi-
cal analysis of a compound word, we propose to uti-
lize the lattice to encode all alternative ways of seg-
menting a word as presented to us in either the clean
or raw versions of a sentence. As the task requires
us to produce separate clean and raw output on the
test set, we tune one system on a lattice built from
the clean and raw dev set, and use the single system
to decode both the clean and raw test set separately.
Table 5 presents the results of using segmentation
lattices.
3.5 Raw-to-Clean Transformation Lattice
As can be seen in Tables 1, 2, and 3, system per-
formance on clean text greatly outperforms system
performance on raw text, with a difference of almost
5 BLEU points. Thus, we explored the possibility of
automatically transforming raw text into clean text,
based on the ?parallel? raw and clean texts that were
provided as part of the task.
One standard approach might have been to train
SEG-LATTICE
Version Set BLEU TER
raw
dev 26.17 61.88
devtest 24.64 62.53
test 23.89 65.27
Table 5: Raw-Clean segmentation lattice tuning results
FST-LATTICE
Version Set BLEU TER
raw
dev 26.20 62.15
devtest 24.21 63.45
test 22.56 67.79
Table 6: Raw-to-clean transformation lattice results
a Haitian-to-Haitian MT system to ?translate? from
raw text to clean text. However, since the training set
was only available as raw text, and only the dev and
devtest datasets had been cleaned, we clearly did not
have enough data to train a raw-to-clean translation
system. Thus, we created a finite-state transducer
(FST) by aligning the raw dev text to the clean dev
text, on a sentence-by-sentence basis. These raw-to-
clean alignments were created using a simple mini-
mum edit distance algorithm; substitution costs were
calculated according to orthographic match.
One option would be to use the resulting raw-to-
clean transducer to greedily replace each word (or
phrase) in the raw input with the predicted transfor-
mation into clean text. However, such a destructive
replacement method could easily introduce cascad-
ing errors by removing text that might have been
translated correctly. Fortunately, as mentioned in
Section 2, and utilized in the previous section, the
cdec decoder accepts lattices as input. Rather than
replacing raw text with the predicted transformation
into ?clean? text, we add a path to the input lat-
tice for each possible transform, for each word and
phrase in the input. We tune a system on a lattice
built from this approach on the dev set, and use the
FST developed from the dev set in order to create
lattices for decoding the devtest and test sets. An
example is shown in Figure 3.4. Note that in this
example, the transformation technique correctly in-
serted new paths for ilavach and ki sou, correctly
retained the single path for zile, but overgenerated
many (incorrect) options for nan. Note, though, that
the original path for nan remains in the lattice, de-
laying the ambiguity resolution until later in the de-
coding process. Results from creating raw-to-clean
transformation lattices are presented in Table 6.
By comparing the results in Table 6 to those in
Table 5, we can see that the noise introduced by the
finite-state transformation process outweighed the
348
1 2Eske?ske 3
nou ap kite nou mouri nan zile
4ila
5Ilavach
vach
6la 7
kisou
9
ki 8okay 10lasou
Figure 1: Partial segmentation lattice combining the raw and clean versions of the sentence:
Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes.
15
16
a
m
pa
te
l
17
an
nan
lan
nan
18
nanak
19
zile
20
tant
21
ila
22ilavach
nan vach
23
e
24
sa
la
lanan 25
ki
26kisou
sou
Figure 2: Partial input lattice for sentence in Figure 3.4, generated using the raw-to-clean transform technique
described in Section 3.5.
gains of adding new phrases for tuning.
4 System Comparison
Table 7 shows the performance on the devtest set
of each of the system variations that we have pre-
sented in this paper. From this table, we can see
that our best-performing system on clean data was
the GRAMMAR system, where the training data was
multiplied by ten as described in Section 3.1, then
the grammar was filtered as described in Section 3.3.
Our performance on clean test data, using this sys-
tem, was 28.29 BLEU and 58.78 TER. Table 7 also
demonstrates that our best-performing system on
raw data was the SEG-LATTICE system, where the
training data was confidence-weighted (Section 3.1),
the grammar was filtered (Section 3.3), punctuation
was automatically added to the raw data as described
in Section 3.2, and the system was tuned on a lattice
created from the raw and clean dev dataset. Our per-
formance on raw test data, using this system, was
23.89 BLEU and 65.27 TER.
5 Conclusion
In this paper we presented our system for the 2011
WMT featured Haitian Creole?English translation
task. In order to improve translation quality of low-
density noisy SMS data, we experimented with a
number of methods that improve performance on
both the clean and raw versions of the data, and help
clean raw
System BLEU TER BLEU TER
BASELINE 28.15 57.45 24.09 63.82
CONF. WT. 27.92 57.66 24.16 63.71
X10 28.22 57.06 24.18 63.71
GRAMMAR 28.69 56.21 24.47 63.26
AUTO-PUNC ? ? 24.38 64.26
SEG-LATTICE ? ? 24.64 62.53
FST-LATTICE ? ? 24.21 63.45
Table 7: Comparison of all systems? performance on
devtest set
close the gap between the post-edited and real-world
data according to BLEU and TER evaluation. The
methods employed were developed to specifically
address shortcomings we observed in the data, such
as segmentation lattices for morphological ambigu-
ity, confidence weighting for resource utilization,
and punctuation prediction for lack thereof. Overall,
this work emphasizes the feasibility of adapting ex-
isting translation technology to as-yet underexplored
domains, as well as the shortcomings that need to be
addressed in future work in real-world data.
6 Acknowledgments
The authors gratefully acknowledge partial support
from the DARPA GALE program, No. HR0011-06-
2-001. In addition, the first author was supported by
the NDSEG Fellowship. Any opinions or findings
do not necessarily reflect the view of the sponsors.
349
References
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics, pages
310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The University of Maryland statistical
machine translation system for the Fourth Workshop
on Machine Translation. In Proceedings of the EACL-
2009 Workshop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL System Demonstrations.
Chris Dyer. 2007. The University of Maryland Trans-
lation system for IWSLT 2007. In Proceedings of
IWSLT.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 48?54.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. In
Computational Linguistics, volume 29(21), pages 19?
51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
350
