Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 218?227, Prague, June 2007. c?2007 Association for Computational Linguistics
Unsupervised Part-of-Speech Acquisition for Resource-Scarce Languages 
Sajib Dasgupta and Vincent Ng 
Human Language Technology Research Institute 
University of Texas at Dallas 
Richardson, TX 75083-0688 
{sajib,vince}@hlt.utdallas.edu 
 
 
 
Abstract 
This paper proposes a new bootstrapping 
approach to unsupervised part-of-speech 
induction. In comparison to previous 
bootstrapping algorithms developed for this 
problem, our  approach aims to improve 
the quality of the seed clusters by 
employing seed words that are both 
distributionally and morphologically 
reliable. In particular, we present a novel 
method for combining morphological and 
distributional information for seed 
selection. Experimental results demonstrate 
that our approach works well for English 
and Bengali, thus providing suggestive 
evidence that it is applicable to both 
morphologically impoverished languages 
and highly inflectional languages. 
1 Introduction 
The availability of a high-quality lexicon is crucial 
to the development of fundamental text-processing 
components such as part-of-speech (POS) taggers 
and syntactic parsers. While hand-crafted lexicons 
are readily available for resource-rich languages 
such as English, the same is not true for resource-
scarce languages. Unfortunately, manually 
constructing a lexicon requires a lot of linguistic 
expertise, and is practically infeasible for highly 
inflectional and agglutinative languages, which 
contain a very large number of lexical items. Given 
the scarcity of annotated data for acquiring the 
lexicon in a supervised manner, researchers have 
instead investigated unsupervised POS induction 
techniques for automating the lexicon construction 
process. In essence, the goal of unsupervised POS 
induction is to learn the set of possible POS tags 
for each lexical item from an unannotated corpus. 
 The most common approach to unsupervised 
POS induction to date has been motivated by Har-
ris?s (1954) distributional hypothesis: words with 
similar co-occurrence patterns should have similar 
syntactic behavior. More specifically, unsupervised 
POS induction algorithms typically operate by (1) 
representing each target word (i.e., a word to be 
tagged with its POS) as a context vector that en-
codes its left and right context, (2) clustering dis-
tributionally similar words, and (3) manually label-
ing each cluster with a POS tag by inspecting the 
members of the cluster. 
This distributional approach works under the as-
sumption that the context vector of each word en-
codes sufficient information for enabling accurate 
word clustering. However, many words are dis-
tributionally unreliable: due to data sparseness, 
they occur infrequently and hence their context 
vectors do not capture reliable statistical informa-
tion. To overcome this problem, Clark (2000) pro-
poses a bootstrapping approach, in which he (1) 
clusters the most distributionally reliable words, 
and then (2) incrementally augments each cluster 
with words that are distributionally similar to those 
already in the cluster. 
The goal of this paper is to propose a new boot-
strapping approach to unsupervised POS induction 
that can operate in a resource-scarce setting. Most 
notably, our approach aims to improve the quality 
of the seed clusters by employing seed words that 
are both distributionally and morphologically reli-
able. In particular, we present a novel method for 
combining morphological and distributional infor-
mation for seed selection. Furthermore, given our 
218
emphasis on resource-scarce languages, our ap-
proach does not rely on any language resources. In 
particular, the morphological information that it 
exploits is provided by an unsupervised morpho-
logical analyzer.  
It is perhaps not immediately clear why morpho-
logical information would play a crucial role in the 
induction process, especially since the distribu-
tional approach has achieved considerable success 
for English POS induction (see Lamb (1961), 
Sch?tze (1995) and Clark (2000)). To understand 
the role and significance of morphology, it is im-
portant to first understand why the distributional 
approach works well for English. Recall from the 
above that the distributional approach assumes that 
the information encoded in the context vector of 
each word, which typically consists of the 250 
most frequent words of a given language, is suffi-
cient for accurately clustering the words. This ap-
proach works well for English because the most 
frequent English words are composed primarily of 
closed-class words such as ?to? and ?is?, which 
provide strong clues to the POS of the target word. 
However, this assumption is not necessarily valid 
for fairly free word order and highly inflectional 
languages such as Bengali. The reason is that (1) 
co-occurrence statistics collected from free word 
order languages are not as reliable as those from 
fixed word order languages; and (2) many of the 
closed-class words that appear in the context vec-
tor for English words are realized as inflections in 
Bengali. The absence of these highly informative 
words implies that the context vectors may no 
longer capture sufficient information for accurately 
clustering Bengali words, and hence the use of 
morphological information becomes particularly 
important for unsupervised POS induction for 
these inflectional languages.  
We will focus primarily on labeling open-class 
words with their POS tags. Our decision is moti-
vated by the fact that closed-class words generally 
comprise a small percentage of the lexical items of 
a language. In Bengali, the percentage of closed-
class words is even smaller than that in English: as 
mentioned before, many closed-class words in 
English are realized as suffixes in Bengali. 
Although our attempt to incorporate morpho-
logical information into the distributional POS in-
duction framework was originally motivated by 
inflectional languages, experimental results show 
that our approach works well for both English and 
Bengali, suggesting its applicability to both mor-
phologically impoverished languages and highly 
inflectional languages. Owing to the lack of pub-
licly available resources for Bengali, we manually 
created a 5000-word Bengali lexicon for evaluation 
purposes. Hence, one contribution of our work lies 
in the creation of an annotated dataset for Bengali. 
By making this dataset publicly available 1 , we 
hope to facilitate the comparison of different unsu-
pervised POS induction algorithms and to stimu-
late interest in Bengali language processing.  
The rest of the paper is organized as follows. 
Section 2 discusses related work on unsupervised 
POS induction. Section 3 describes our tagsets for 
English and Bengali. The next three sections de-
scribe the three steps of our bootstrapping ap-
proach: cluster the words using morphological in-
formation (Section 4), remove potentially misla-
beled words from each cluster (Section 5), and 
bootstrap each cluster using a weakly supervised 
learner (Section 6). Finally, we present evaluation 
results in Section 7 and conclusions in Section 8.  
2 Related Work 
Several unsupervised POS induction algorithms 
have also attempted to incorporate morphological 
information into the distributional framework, but 
our work differs from these in two respects.  
Computing morphological information. Previous 
POS induction algorithms have attempted to derive 
morphological information from dictionaries (Ha-
ji, 2000) and knowledge-based morphological 
analyzers (Duh and Kirchhoff, 2006). However, 
these resources are generally not available for re-
source-scarce languages. Consequently, research-
ers have attempted to derive morphological infor-
mation heuristically (e.g., Cucerzan and Yarowsky 
(2000), Clark (2003), Freitag (2004)). For instance, 
Cucerzan and Yarowsky (2000) posit a character 
sequence x as a suffix if there exists a sufficient 
number of distinct words w in the vocabulary such 
that the concatentations wx are also in the vocabu-
lary.  It is conceivable that such heuristically com-
puted morphological information can be inaccurate, 
thus rendering the usefulness of a more accurate 
morphological analyzer. To address this problem, 
we exploit morphological information provided by 
an unsupervised word segmentation algorithm.   
                                                 
1
 See http://www.utdallas.edu/~sajib/posDatasets.html. 
219
Tag Description Treebank tags 
JJ Adjective JJ 
JJR Adjective, comparative JJR 
JJS Adjective, superlative JJS 
NN Singular noun NN, NNP 
NNS Plural noun NNS, NNPS 
RB Adverb RB 
VB Verb, non-3rd ps. sing. present VB, VBP 
VBD Verb, past tense or past participle VBD, VBN 
VBG Verb, gerund/present participle VBG 
VBZ Verb, 3rd ps. sing. present VBZ 
Table 1: The English tagset 
 
Using morphological information. Perhaps due to 
the overly simplistic methods employed to com-
pute morphological information, morphology has 
only been used as what Biemann (2006) called 
add-on?s in existing POS induction algorithms, 
which remain primarily distributional in nature. In 
contrast, our approach more tightly integrates mor-
phology into the distributional framework. As we 
will see, we train SVM classifiers using both mor-
phological and distributional features to select seed 
words for our bootstrapping algorithm, effectively 
letting SVM combine these two sources of infor-
mation and perform automatic feature weighting. 
Another appealing feature of our approach is that 
when labeling each unlabeled word with its POS 
tag, an SVM classifier also returns a numeric value 
that indicates how confident the word is labeled. 
This opens up the possibility of having a human 
improve our automatically constructed lexicon by 
manually checking those entries that are tagged 
with low confidence by an SVM classifier. 
Recently, there have been attempts to perform 
(mostly) unsupervised POS tagging without rely-
ing on a POS lexicon. Haghighi and Klein?s (2006) 
prototype-driven approach requires just a few pro-
totype examples for each POS tag, exploiting these 
labeled words to constrain the labels of their dis-
tributionally similar words when training a genera-
tive log-linear model for POS tagging. Smith and 
Eisner (2005) train a log-linear model for POS tag-
ging in an unsupervised manner using contrastive 
estimation, which seeks to move probability mass 
to a positive example e from its neighbors (i.e., 
negative examples created by perturbing e). 
3 The English and Bengali Tagsets 
Given our focus on automatically labeling open 
class words, our English and Bengali tagsets are 
designed  to essentially  cover  all of the open-class 
Tag Description Examples 
JJ Adjective vhalo, garam, kharap 
NN Singular noun kanna, ridoy, shoshon 
NN2 2nd order inflectional noun dhopake, kalamtike 
NN6 6th order inflectional noun gharer, manusher 
NN7 7th order inflectional noun dhakai, barite, graame 
NNP Proper noun arjun, ahmmad 
NNS Plural noun manushgulo, pakhider 
NNSH Noun ending with ?sh? barish, jatrish 
VB Finite verb kheyechi, krlam, krI 
VBN Non-finite verb kre, giye, jete, kadte 
Table 2: The Bengali tagset 
 
words. Our English tagset, which is composed of 
ten tags, is shown in Table 1. As we can see, a tag 
in our tagset can be mapped to more than one Penn 
Treebank tags. For instance, we use the tag ?NN? 
for both singular and plural common nouns. Our 
decision of which Penn Treebank tags to group 
together is based on that of Sch?tze (1995).  
Our Bengali tagset, which also consists of ten 
tags, is adapted from the one proposed by Saha et 
al. (2004) (see Table 2). It is worth noting that 
unlike English, we assign different tags to Bengali 
proper nouns and common nouns. The reason is 
that for English, it is not particularly crucial to dis-
tinguish the two types of nouns during POS induc-
tion, since they can be distinguished fairly easily 
using heuristics such as initial capitalization. For 
Bengali, such simple heuristics do not exist, as the 
Bengali alphabet does not have any upper and 
lower case letters. Hence, it is important to distin-
guish Bengali proper nouns and common nouns 
during POS induction. 
4 Clustering the Morphologically Similar 
Words 
As mentioned before, our approach aims to more 
tightly integrate morphological information into 
the distributional POS induction framework. In 
fact, our POS induction algorithm begins by clus-
tering the morphologically similar words (i.e., 
words that combine with the same set of suffixes). 
The motivation for clustering morphologically 
similar words can be attributed to our hypothesis 
that words having similar POS should combine 
with a similar set of suffixes. For instance, verbs in 
English combine with suffixes like ?ing?, ?ed? and 
?s?, whereas adjectives combine with suffixes like 
?er? and ?est?. Note, however, that the suffix ?s? 
can attach to both verbs and nouns in English, and 
so it is not likely to be a useful feature for identify-
220
ing the POS of a word. The question, then, is how 
to determine which suffixes are useful for the POS 
identification task in an unsupervised setting where 
we do not have any prior knowledge of language-
specific grammatical constraints. This section pro-
poses a method for identifying the ?useful? suf-
fixes and employing them to cluster the morpho-
logically similar words. As we will see, our clus-
tering algorithm not only produces soft clusters, 
but it also automatically determines the number of 
clusters for a particular language.   
Before we describe how to identify the useful 
suffixes, we need to (1) induce all of the suffixes 
and (2) morphologically segment the words in our 
vocabulary. 2  However, neither of these tasks is 
simple for a truly resource-scarce language for 
which we do not have a dictionary or a knowledge-
based morphological analyzer. As mentioned in the 
introduction, our proposed solution to both tasks is 
to use an unsupervised morphological analyzer that 
can be built just from an unannotated corpus. In 
particular, we have implemented an unsupervised 
morphological analyzer that outperforms Gold-
smith?s (2001) Linguistica and Creutz and Lagus?s 
(2005) Morfessor for our English and Bengali 
datasets and compares favorably to the best-
performing morphological parsers in MorphoChal-
lenge 20053 (see Dasgupta and Ng (2007)).  
Given the segmentation of each word and the 
most frequent 30 suffixes4 provided by our mor-
phological analyzer, our clustering algorithm oper-
ates by (1) clustering the similar suffixes and then 
(2) assigning words to each cluster based on the 
suffixes a word combines with. To cluster similar 
suffixes, we need to define the similarity between 
two suffixes. Informally, we say that two suffixes x 
and y are similar if a word that combines with x 
also combines with y and vice versa. In practice, 
we will rarely posit two suffixes as similar under 
this definition unless we assume access to a com-
plete vocabulary ? an assumption that is especially 
unrealistic for resource-scarce languages. As a re-
sult, we relax this definition and consider two suf-
fixes x and y similar if P(x | y) > t and P(y | x) > t, 
where P(x | y) is the probability of a word combin-
ing with suffix x given that it combines with suffix 
                                                 
2
 A vocabulary is simply a set of (distinct) words extracted 
from an unannotated corpus. We extracted our English and 
Bengali vocabulary from WSJ and Prothom Alo, respectively.  
3
 http://www.cis.hut.fi/morphochallenge2005/ 
4
 We found that 30 suffixes are sufficient to cluster the words. 
y, and t is a threshold that we set to 0.4 in all of our 
experiments. Note that both probabilities can be 
estimated from an unannotated corpus.5 Given this 
definition of similarity, we can cluster the similar 
suffixes using the following steps: 
Creating the initial clusters.  First, we create a 
suffix graph, in which we have (1) one node for 
each of the 30 suffixes, and (2) a directed edge 
from suffix x to suffix y if P(y | x) > 0.4. We then 
identify the strongly connected components of this 
graph using depth-first search. These strongly con-
nected components define our initial partitioning of 
the 30 suffixes. We denote the suffixes assigned to 
a cluster the primary keys of the cluster.   
Improving the initial clusters. Recall that we 
ultimately want to cluster the words by assigning 
each word w to the cluster in which w combines 
with all of its primary keys. Given this goal, it is 
conceivable that singleton clusters are not 
desirable. For instance, a cluster that has ?s? as its 
only primary key is not useful, because although a 
lot of words combine with ?s?, they do not 
necessarily have the same POS. As a result, we 
improve each initial cluster by adding more 
suffixes to the cluster, in hopes of improving the 
resulting clustering of the words by placing 
additional constraints on each cluster. More 
specifically, we add a suffix y to a cluster c if, for 
each primary key x of c, P(y | x) > 0.4. If this 
condition is satisfied, then y becomes a secondary 
key of c. For each initial cluster c?, we perform this 
check using each of the suffixes x? not in c? to see 
if x? can be added to c?. If, after this expansion 
step, we still have a cluster c* defined by a single 
primary key x that also serves as a secondary key 
in other clusters, then x is probably ambiguous 
(i.e., x can probably attach to words belonging to 
different POSs); and consequently, we remove c*. 
We denote the resulting set of clusters by C. 
Populating the clusters with words. Next, for 
each word w in our vocabulary, we check whether 
w can be assigned to any of the clusters in C. Spe-
cifically, we assign w to a cluster c if w can com-
bine with each of its primary keys and at least half 
of its secondary keys.  
Labeling and merging the clusters. After popu-
lating each cluster with words, we manually label 
                                                 
5
 For instance, we compute P(x | y) as the ratio of the number 
of distinct words that combines with both x and y to the num-
ber of distinct words that combine with y only. 
221
each of them with a POS tag from the tagset. We 
found that all of the clusters are labeled as NN, 
VB, or JJ. The reason is that the clustered words 
are mostly root words. We then merge all the clus-
ters labeled with the same POS tag, yielding only 
three ?big? clusters. Note that these ?big? clusters 
are soft clusters, since a word can belong to more 
than one of them. For instance, ?cool? can combine 
with ?s? or ?ing? to form a VB, and it can also 
combine with ?er? or ?est? to form a JJ. 
Generating sub-clusters. Recall that each ?big? 
cluster contains a set of suffixes and also a set of 
words that combines with those suffixes. Now, for 
each ?big? cluster c, we create one sub-cluster cx 
for each suffix x that appears in c. Then, for each 
word w in c, we use our unsupervised morphologi-
cal analyzer to generate w+x and add the surface 
form to the corresponding sub-cluster. 
Labeling the sub-clusters. Finally, we manually 
label each sub-cluster with a POS tag from our 
tagset. For example, all the words ending in ?ing? 
will be labeled as VBG. As before, we merge two 
clusters if they are labeled with the same POS tag. 
The resulting clusters are our morphologically 
formed clusters. 
5 Purifying the Seed Set 
The clusters formed thus far cannot be expected to 
be perfectly accurate, since (1) our unsupervised 
morphological analyzer is not perfect, and (2) 
morphology alone is not always sufficient for de-
termining the POS of a word. In fact, we found that 
many adjectives are mislabeled as nouns for both 
languages. For instance, ?historic? is labeled as a 
noun, since it combines with suffixes like ?al? and 
?ally? that ?accident? combines with. In addition, 
many words are labeled with the POS that does not 
correspond to their most common word sense. For 
instance, while words like ?chair?, ?crowd? and 
?cycle? are more commonly used as nouns than 
verbs, they are labeled as verbs by our clustering 
algorithm. The reason is that suffixes that typically 
attach to verbs (e.g., ?s?, ?ed?, ?ing?) also attach to 
these words. Such labelings, though not incorrect, 
are undesirable, considering the fact that these 
words are to be used as seeds to bootstrap our mor-
phologically formed clusters in a distributional 
manner. For instance, since ?chair? and ?crowd? 
are distributionally similar to nouns, their presence 
in the verb clusters can potentially contaminate the 
clusters with nouns during the bootstrapping proc-
ess. Hence, for the purpose of effective bootstrap-
ping, we also consider these words ?mislabeled?.  
To identify the words that are potentially misla-
beled, we rely on the following assumption: words 
that are morphologically similar should also be 
distributionally similar and vice versa. Based on 
this assumption, we propose a purification method 
that posits a word w as potentially mislabeled (and 
therefore should be removed or relabeled) if the 
POS of w as predicted using distributional infor-
mation differs from that as determined by mor-
phology. 
The question, then, is how to predict the POS 
tag of a word using distributional information? Our 
idea is to use ?supervised? learning, where we train 
and test on the seed set. Conceptually, we (1) train 
a multi-class classifier on the morphologically la-
beled words, each of which is represented by its 
context vector, and (2) apply the classifier to rela-
bel the same set of words. If the new label of a 
word w differs from its original label, then mor-
phology and context disagree upon the POS of w; 
and as mentioned above, our method then deter-
mines that the word is potentially misclassified. 
Note, however, that (1) the training instances are 
not perfectly labeled and (2) it does not make sense 
to train a classifier on data that is seriously misla-
beled. Hence, we make the assumption that a large 
percentage (> 70%) of the training instances is cor-
rectly labeled6, and that our method would work 
with a training set labeled at this level of accuracy. 
In addition, since we are training a classifier based 
on distributional features, we train and test on only 
distributionally reliable words, which we define to 
be words that appear at least five times in our cor-
pus. Distributionally unreliable words will all be 
removed from the morphologically formed clus-
ters, since we cannot predict their POS using dis-
tributional information.  
In our implementation of this method, rather 
than train a multi-class classifier, we train a set of 
binary classifiers using SVMlight (Joachims, 1999) 
together with the distributional features for deter-
mining the POS tag of a given word.7 More spe-
cifically, we train one classifier for each pair of 
                                                 
6
 An inspection of the morphologically formed clusters reveals 
that this assumption is satisfied for both languages. 
7
 In this and all subsequent uses of SVMlight, we set al the 
training parameters to their default values. 
222
POS tags. For instance, since we have ten POS 
tags for English, we will train 45 binary classifi-
ers.8 To determine the POS tag of a given English 
word w, we will use these 45 pairwise classifiers to 
independently assign a label to w. For instance, the 
NN-JJ classifier will assign either NN or JJ to w. 
We then count how many times w is tagged with 
each of the ten POS tags. If there is a POS tag t 
whose count is nine, it means that all the nine clas-
sifiers associated with t have classified w as t, and 
so our method will label w as t. Otherwise, we re-
move w from our seed set, since we cannot confi-
dently label it using our classifier ensemble. 
To create the training set for the NN-JJ classi-
fier, for instance, we can possibly use all of the 
words labeled with NN and JJ as positive and 
negative instances, respectively. However, to en-
sure that we do not have a skewed class distribu-
tion, we use the same number of instances from 
each class to train the classifier. More formally, let 
INN be the set of instances labeled with NN, and IJJ 
be the set of instances labeled with JJ. Without loss 
of generality, assume that |INN| < |IJJ|, where |X| de-
notes the size of the set X. To avoid class skew-
ness, we have to sample from IJJ, since it is the lar-
ger set. Our sampling method is motivated by bag-
ging (Breiman, 1996). More specifically, we create 
10 training sets from IJJ, each of which has size |INN| and is formed by sampling with replacement 
from IJJ. We then combine each of these 10 train-
ing sets separately with INN, and train 10 SVM 
classifiers from the 10 resulting training sets. 
Given a test instance i, we first apply the 10 classi-
fiers independently to i and obtain the signed con-
fidence values9 of the predictions provided by the 
classifiers. We then take the average of the 10 con-
fidence values, assigning i the positive class if the 
average is at least 0, and negative otherwise.   
As mentioned above, we use distributional fea-
tures to represent an instance created from a word 
w. The distributional features are created based on 
Sch?tze?s (1995) method. Specifically, the left 
context and the right context of w are each encoded 
using the most frequent 500 words from the vo-
cabulary. A feature in the left (right) context has 
                                                 
8
 We could have trained just one 10-class classifier, but the 
fairly large number of classes leads us to speculate that this 
multi-class classifier will not achieve a high accuracy. 
9
 Here, a large positive number indicates that the classifier 
confidently labels the instance as NN, and a large negative 
number represents confident prediction for JJ. 
the value 1 if the corresponding word appears to 
the left (right) of w in our corpus, and 0 otherwise. 
However, we found that using distributional fea-
tures alone would erroneously classify words like 
?car? and ?cars? as having the same POS because 
the two words are distributionally similar. In gen-
eral, it is difficult to distinguish words in NN from 
those in NNS by distributional means. The same 
problem occurs for words in VB and VBD. To ad-
dress this problem, we augment the feature set with 
suffixal features. Specifically, we create one binary 
feature for each of the 30 most frequent suffixes 
that we employed in the previous section. The fea-
ture corresponding to suffix x has the value 1 if x is 
the suffix of w. Moreover, we create an additional 
suffixal feature whose value is 1 if none of the 30 
most frequent suffixes is the suffix of w.  
6 Augmenting the Seed Set 
After purification, we have a set of clusters filled 
with distributionally and morphologically reliable 
seed words that receive the same POS tag when 
predicted independently by morphological features 
and distributional features. Our goal in this section 
is to augment this seed set. Since we have a small 
seed set (5K words for English and 8K words for 
Bengali) and a large number of unlabeled words, 
we believe that it is most natural to apply a weakly 
supervised learning algorithm to bootstrap the clus-
ters. Specifically, we employ a version of self-
training together with SVM as the underlying 
learning algorithm. 10  Below we first present the 
high-level idea of our self-training algorithm and 
then discuss the implementation details. 
Conceptually, our self-training algorithm works 
as follows. We first train a multi-class SVM classi-
fier on the seed set for determining the POS tag of 
a word using the morphological and distributional 
features described in the previous section, and then 
apply it to label the unlabeled (i.e., unclustered) 
words. Words that are labeled with a confidence 
value that exceeds the current threshold (which is 
initially set to 1 and -1 for positively and nega-
tively labeled instances, respectively) will be 
                                                 
10
 As a related note, Clark?s (2001) bootstrapping algorithm 
uses KL-divergence to measure the distributional similarity 
between an unlabeled word and a labeled word, adding to a 
cluster the words that are most similar to its current member. 
For us, SVM is a more appealing option because it automati-
cally combines the morphological and distributional features. 
223
added to the seed set.  In the next iteration, we re-
train the classifier on the augmented labeled data, 
apply it to the unlabeled data, and add to the la-
beled data those instances whose predicted confi-
dence is above the current threshold. If none of the 
instances has a predicted confidence above the cur-
rent threshold, we reduce the threshold by 0.1. (For 
instance, if the original thresholds are 1 and -1, 
they will be changed to 0.9 and -0.9.) We then re-
peat the above procedure until the thresholds reach 
0.5 and -0.5. 11  Finally, we apply the resulting 
bootstrapped classifier to label all of the unlabeled 
words that have a corpus frequency of at least five, 
using a threshold of 0. 
In our implementation of the self-training algo-
rithm, rather than train a multi-class classifier in 
each bootstrapping iteration, we train pairwise 
classifiers (recall that for English, 45 classifiers are 
formed from 10 POS tags) using the morphological 
and distributional features described in the previ-
ous section. Again, since we employ distributional 
features, we apply the 45 pairwise classifiers only 
to the distributionally reliable words (i.e., words 
with corpus frequency at least 5). To classify an 
unlabeled word w, we apply the 45 pairwise classi-
fiers to independently assign a label to w.12  We 
then count how many times w is tagged with each 
of the ten POS tags. If there is a POS tag whose 
count is nine and all of these nine votes are associ-
ated with confidence that exceeds the current 
threshold, then we add w to the labeled data to-
gether with its assigned tag.  
7 Evaluation 
7.1 Experimental Setup 
Corpora. Recall that our bootstrapping algorithm 
assumes as input an unannotated corpus from 
which we (1) extract our vocabulary (i.e., the set of 
words to be labeled) and (2) collect the statistics 
needed in morphological and distributional cluster-
                                                 
11
 We decided to stop the bootstrapping procedure at thresh-
olds of 0.5 and -0.5, because the more bootstrapping iterations 
we use, the lower are the quality of the bootstrapped data as 
well as the accuracy of the bootstrapped classifier.  
12
 As in purification, each pairwise classifier is implemented 
as a set of 10 classifiers, each of which is trained on an equal 
number of instances from both classes. Testing also proceeds 
as before: the label of an instance is derived from the average 
of the confidence values returned by the 10 classifiers, and the 
confidence value associated with the label is just the average 
of the 10 confidence values. 
ing. We use as our English corpus the Wall Street 
Journal (WSJ) portion of the Penn Treebank (Mar-
cus et al, 1993). Our Bengali corpus is composed 
of five years of articles taken from the Bengali 
newspaper Prothom Alo.  
Vocabulary creation. To extract our English vo-
cabulary, we pre-processed each document in the 
WSJ corpus by first tokenizing them and then re-
moving the most frequent 500 words (as they are 
mostly closed class words), capitalized words, 
punctuations, numbers, and unwanted character 
sequences (e.g., ?***?). The resulting English vo-
cabulary consists of approximately 35K words. We 
applied similar pre-processing steps to the Prothom 
Alo articles to generate our Bengali vocabulary, 
which consists of 80K words. 
Test set preparation. Our English test set is com-
posed of the 25K words in the vocabulary that ap-
pear at least five times in the WSJ corpus.  The 
gold-standard POS tags for each word w are de-
rived automatically from the parse trees in which w 
appears. To create the Bengali test set, we ran-
domly chose 5K words from the vocabulary that 
appear at least five times in Prothom Alo. Each 
word in the test set was then labeled with its POS 
tags by two of our linguists. 
Evaluation metric. Following Sch?tze (1995), we 
report performance in terms of recall, precision, 
and F1. Recall is the percentage of POS tags cor-
rectly proposed, precision is the percentage of POS 
tags proposed that are correct, and F1 is simply the 
harmonic mean of recall and precision. To exem-
plify, suppose the correct tagset for ?crowd? is 
{NN, VB}; if our system outputs {VB, JJ, RB}, 
then recall is 50%, precision is 33%, and F1 is 
40%.  Importantly, all of our results will be re-
ported on word types. This prevents the frequently 
occurring words from having a higher influence on 
the results than their infrequent counterparts. 
7.2 Results and Discussion 
The baseline system. We use as our baseline sys-
tem one of the best existing unsupervised POS in-
duction algorithms (Clark, 2003). More specifi-
cally, we downloaded from Clark?s website13 the 
code that implements a set of POS induction algo-
rithms he proposed. Among these implementa-
tions, we chose cluster_neyessenmorph, which 
combines morphological and distributional infor-
                                                 
13
 http://www.cs.rhul.ac.uk/home/alexc/ 
224
mation and achieves the best performance in his 
paper. When running his program, we use WSJ and 
Prothom Alo as the input corpora. In addition, we 
set the number of clusters produced to be 128, 
since this setting yields the best result in his paper. 
Results of the baseline system for the English and 
Bengali test sets are shown under the ?After Boot-
strapping? column in row 1 of Tables 3 and 4. As 
we can see, the baseline achieves F1-scores of 59% 
and 45% for English and Bengali, respectively. 
The other results in row 1 will be discussed below. 
Our induction system. Recall that our unsuper-
vised POS induction algorithm operates in three 
steps. To better understand the performance con-
tribution of each of these steps, we show in row 2 
of Tables 3 and 4 the results of our system after we 
(1) morphologically cluster the words, (2) purify 
the seed set, and (3) augment the seed set. Impor-
tantly, the numbers shown for each step are com-
puted over the set of words in the test set that are 
labeled at the end of that step. For instance, the 
morphological clustering algorithm labeled 11K 
English words and 25K Bengali words, and so re-
call, precision and F1-score are computed over the 
subset of these labeled words that appear in the test 
set. Similarly, after bootstrapping, all the words 
that appear at least five times in our corpus are la-
beled; since our labeled data is now a superset of 
our test data, the numbers in the last column are 
the results of our algorithm for the entire test set.  
As we can see, after morphological clustering, 
our system achieves F1-scores of 79% and 78% for 
English and Bengali, respectively. When measured 
on exactly the same set of words, the baseline only 
achieves F-scores of 59% and 56%. In fact, com-
paring rows 1 and 2, we outperform the baseline in 
each of the three steps of our algorithm. In particu-
lar, our system yields F1-scores of 73% and 77% 
for the entire English and Bengali test sets, thus 
outperforming the baseline by 14% and 18% for 
English and Bengali, respectively.  
Two additional points deserve mentioning. First, 
for both languages, the highest F1-score is 
achieved after the purification step. A closer analy-
sis of the labeled words reveals the reason. For 
English, many of the nouns incorrectly labeled as 
verbs by the morphological clustering algorithm 
were subsequently removed during the purification 
step when distributional similarity was used on top 
of morphological similarity. For Bengali, many 
proper nouns were assigned by the morphological 
clustering algorithm to the clusters dominated by 
common nouns (because the two types of Bengali 
nouns are morphologically similar), and many of 
these mislabeled proper nouns were subsequently 
removed during purification. Second, as expected, 
precision drops after the seed augmentation step, 
since the quality of the labeled data deteriorates as 
bootstrapping progresses. Nevertheless, with a lot 
more words labeled in the bootstrapping step, we 
still achieve F1-scores of 73% for English and 76% 
for Bengali.  
The remaining rows of the Tables 3 and 4 show 
the performance of our algorithm for each tag in 
our two POS tagsets. Different observations can be 
made for the two languages. For English, the poor 
results for VBZ and NNS can be attributed to the 
fact that it is not easy to distinguish between these 
two tags: ?s? is a typical suffix for words that are 
NNS and words that are the third person singular 
of a verb. In addition, results for verbs are better 
than those for nouns, since verbs are easier to iden-
tify using only morphological knowledge. 
For Bengali, results for adjectives are not good, 
since (1) adjectives and nouns have very similar 
distributional property in Bengali and (2) there are 
not enough suffixes to induce the adjectives mor-
phologically. Moreover, we achieve high precision 
but low recall for proper nouns. This implies that 
most of the words that our algorithm labels as 
proper nouns are indeed correct, but there are also 
many proper nouns that are mislabeled. A closer 
examination of the clusters reveals that many of 
these proper nouns are mislabeled as common 
nouns, presumably because these two types of 
Bengali nouns are morphologically and distribu-
tionally similar and therefore it is difficult to sepa-
rate them. We will leave the identification of Ben-
gali proper nouns as a topic for future research.   
7.3 Additional Experiments 
Labeling rare words with morphological infor-
mation. Although our discussion thus far has fo-
cused on words whose corpus frequency is at least 
five, it would be informative to examine how well 
our algorithm performs on rare, distributionally 
unreliable words (i.e., words with corpus fre-
quency less than five). Recall that our morphologi-
cal clustering algorithm also clusters rare words. In 
fact, these rare words comprise 15% of the English 
words and 18% of the Bengali words in our mor-
phological formed clusters. Perhaps more impor-
225
After Morphological Clustering After Purification After Bootstrapping  
P R F1 P R F1 P R F1 
Baseline 84.1 45.3 58.9 84.9 51.4 64.1 75.6 48.0 59.0 
Ours 85.9 74.0 79.4 89.3 74.4 81.7 80.4 66.8 73.1 
JJ 88.7 49.1 63.2 91.4 51.9 66.1 57.7 62.9 60.2 
JJR 91.1 86.2 88.6 92.1 92.0 92.0 62.1 83.1 71.0 
JJS 100 98.3 99.1 100 100 100 81.3 86.9 83.9 
NN 91.6 43.7 59.2 94.8 42.8 58.8 95.2 47.1 62.8 
NNS 90.6 39.2 53.5 93.5 41.3 57.2 96.6 44.7 60.9 
RB 100 76.1 86.4 100 82.2 90.6 98.8 63.5 77.3 
VB 74.0 97.7 84.1 79.8 96.0 87.1 65.7 92.8 76.9 
VBD 96.6 98.9 97.7 97.6 100 98.8 96.7 91.9 93.3 
VBG 89.9 100 94.7 91.1 100 95.7 90.8 93.5 92.1 
VBZ 60.9 99.9 74.7 65.1 96.8 77.7 52.8 92.6 67.3 
Table 3: POS induction results for English based on word type 
 
After Morphological Clustering After Purification After Bootstrapping  
P R F1 P R F1 P R F1 
Baseline 82.1 42.3 55.5 83.1 45.3 58.3 78.1 43.3 49.3 
Ours 74.1 81.3 77.5 83.4 78.0 80.7 74.1 79.2 76.6 
JJ 50.0 51.8 50.9 56.1 55.0 55.5 57.5 51.4 54.3 
NN 63.0 96.8 76.4 67.0 96.0 78.9 62.2 92.2 74.3 
NN2 96.3 100 98.1 99.0 100 99.5 99.0 99.0 99.0 
NN6 95.5 89.2 92.2 97.2 90.0 93.9 97.1 91.0 93.9 
NN7 88.4 94.1 89.7 92.1 99.2 93.1 90.1 78.7 84.1 
NNP 87.2 37.3 52.3 92.8 43.8 59.4 92.7 51.5 66.1 
NNS 62.7 93.1 75.0 66.8 93.5 77.9 65.2 94.1 77.1 
NNSH 91.0 100 95.6 91.0 100 95.7 91.0 100 95.7 
VB 68.9 93.0 79.2 77.0 94.6 84.9 73.9 91.8 81.9 
VBN 84.3 49.1 62.1 82.4 50.1 62.9 56.1 46.7 50.1 
Table 4: POS induction results for Bengali based on word type
 
tantly, when measuring performance on just these 
morphologically clustered rare words, our algo-
rithm achieves F1-scores of 81% and 79% for Eng-
lish and Bengali, respectively. These results pro-
vide empirical support for the claim that morpho-
logical information can be usefully employed to 
label rare words (Clark, 2003). 
Soft clustering. Many words have more than one 
POS tag. For instance, ?received? can be labeled as 
VBD and JJ. Although our morphological cluster-
ing algorithm can predict some of these ambigui-
ties, those are at the ?big? cluster level. At the sub-
cluster level, the algorithm imposes a hard cluster-
ing on the words. In other words, no word appears 
in more than one sub-cluster. 
Ideally, a POS induction algorithm should pro-
duce soft clusters due to lexical ambiguity. In fact, 
Jardino and Adda (1994), Sch?tze (1997) and 
Clark (2000) have attempted to address the ambi-
guity problem to a certain extent. We have also 
experimented with a very simple method for han-
dling ambiguity in our bootstrapping algorithm: 
when augmenting the seed set, instead of labeling a  
 
word with a tag that receives 9 votes from the 45 
pairwise classifiers, we label a word with any tag 
that receives at least 8 votes, effectively allowing 
the assignment of more than one label to a word. 
However, our experimental results (not shown due 
to space limitations) indicate that the incorporation 
of this method does not yield better overall per-
formance, since many of the additional labels are 
erroneous and hence their presence deteriorates the 
quality of the bootstrapped data.  
8 Conclusions 
We have proposed a new bootstrapping algorithm 
for unsupervised POS induction. In contrast to ex-
isting algorithms developed for this problem, our 
algorithm is designed to (1) operate under a re-
source-scarce setting in which no language-
specific tools or resources are available and (2) 
more tightly integrate morphological information 
with the distributional POS induction framework. 
In particular, our algorithm (1) improves the qual-
ity of the seed clusters by employing seed words 
226
that are distributionally and morphologically reli-
able and (2) uses support vector learning to com-
bine morphological and distributional information. 
Our results show that it outperforms Clark?s algo-
rithm for English and Bengali, suggesting that it is 
applicable to both morphologically impoverished 
and highly inflectional languages.  
Acknowledgements 
We thank the five anonymous EMNLP-CoNLL 
referees for their valuable comments. We also 
thank Zeeshan Abedin and Mahbubur Rahman 
Haque for creating the Bengali lexicon. 
References 
Chris Biemann. 2006. Unsupervised part-of-speech tag-
ging employing efficient graph clustering. In Pro-
ceedings of the COLING/ACL 2006 Student Research 
Workshop.  
Leo Breiman. 1996. Bagging predictors. Machine 
Learning 24(2):123-140. 
Alexander Clark. 2000. Inducing syntactic categories by 
context distributional clustering. In Proceedings of 
CoNLL, pages 91-94. 
Alexander Clark. 2003. Combining distributional and 
morphological information for part of speech induc-
tion. In Proceedings of the EACL.  
Mathias Creutz and Krista Lagus. 2005. Unsupervised 
morpheme segmentation and morphology induction 
from text corpora using Morfessor 1.0. In Computer 
and Information Science, Report A81, Helsinki Uni-
versity of Technology. 
Silviu Cucerzan and David Yarowsky. 2000. Language 
independent, minimally supervised induction of lexi-
cal probabilities. In Proceedings of the ACL, pages 
270-277. 
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological 
segmentation. In Proceedings of NAACL-HLT, pages 
155-163. 
Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-
sition for dialectal Arabic using transductive learn-
ing. In Proceedings of EMNLP, pages 399-407. 
Dayne Freitag. 2004. Toward unsupervised whole-
corpus tagging. In Proceedings of COLING, pages 
357-363. 
John Goldsmith. 2001. Unsupervised learning of the 
morphology of a natural language. In Computational 
Linguistics 27(2):153-198. 
Aria Haghighi and Dan Klein. 2006. Prototype-driven 
learning for sequence models. In Proceedings of 
HLT-NAACL, pages 320-327. 
Jan Haji. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the NAACL, pages 94-
101. 
Zellig Harris. 1954. Distributional structure. In Word, 
10(2/3):146-162. 
Michele Jardino and Gilles Adda. 1994. Automatic de-
termination of a stochastic bi-gram class language 
model. In Proceedings of Grammatical Inference and 
Applications, Second International Colloquium, 
ICGI-94, pages 57-65. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical. In Advances in Kernel Methods ? 
Support Vector Learning, pages 44-56. MIT Press. 
Sydney Lamb. 1961. On the mechanization of syntactic 
analysis. In Proceedings of the 1961 Conference on 
Machine Translation of Languages and Applied Lan-
guage Analysis, Volume 2, pages 674-685. HMSO, 
London. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Bea-
trice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational 
Linguistics, 19(2):313-330. 
Andrei Mikheev. 1997. Automatic rule induction for 
unknown word-guessing. Computational Linguistics, 
23(3):405-423. 
Goutam Kumar Saha, Amiya Baran Saha, and Sudipto 
Debnath. 2004. Computer assisted Bangla words 
POS tagging. In Proceedings of the International 
Symposium on Machine Translation NLP and TSS 
(iTRANS, 2004). 
Hinrich Sch?tze. 1995. Distributional part-of-speech 
tagging. In Proceedings of the EACL, pages 141-148. 
Hinrich Sch?tze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI Publications.  
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data. In 
Proceedings of the ACL, pages 354-362. 
 
227
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 580?589,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Topic-wise, Sentiment-wise, or Otherwise?
Identifying the Hidden Dimension for Unsupervised Text Classification
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
Abstract
While traditional work on text clustering
has largely focused on grouping docu-
ments by topic, it is conceivable that a user
may want to cluster documents along other
dimensions, such as the author?s mood,
gender, age, or sentiment. Without know-
ing the user?s intention, a clustering al-
gorithm will only group documents along
the most prominent dimension, which may
not be the one the user desires. To ad-
dress this problem, we propose a novel
way of incorporating user feedback into
a clustering algorithm, which allows a
user to easily specify the dimension along
which she wants the data points to be clus-
tered via inspecting only a small number
of words. This distinguishes our method
from existing ones, which typically re-
quire a large amount of effort on the part
of humans in the form of document an-
notation or interactive construction of the
feature space. We demonstrate the viabil-
ity of our method on several challenging
sentiment datasets.
1 Introduction
Text clustering is one of the most important appli-
cations in Natural Language Processing (NLP). A
common approach to this problem consists of (1)
computing the similarity between each pair of doc-
uments, each of which is typically represented as a
bag of words; and (2) using an unsupervised clus-
tering algorithm to partition the documents. The
majority of existing work on text clustering has
focused on topic-based clustering, where high ac-
curacies can be achieved even for datasets with a
large number of classes (e.g., 20 Newsgroups).
On the other hand, there has been relatively lit-
tle work on sentiment-based clustering and the re-
lated task of unsupervised polarity classification,
where the goal is to cluster (or classify) a set of
documents (e.g., reviews) according to the po-
larity (e.g., ?thumbs up? or ?thumbs down?) ex-
pressed by the author in an unsupervised man-
ner. Despite the large amount of recent work on
sentiment analysis and opinion mining, much of
it has focused on supervised methods (e.g., Pang
et al (2002), Kim and Hovy (2004), Mullen and
Collier (2004)). One weakness of these existing
supervised polarity classification systems is that
they are typically domain- and language-specific.
Hence, when given a new domain or language,
one needs to go through the expensive process of
collecting a large amount of annotated data in or-
der to train a high-performance polarity classifier.
Some recent attempts have been made to leverage
existing sentiment corpora or lexica to automati-
cally create annotated resources for new domains
or languages. However, such methods require
the existence of either a parallel corpus/machine
translation engine for projecting/translating anno-
tations/lexica from a resource-rich language to the
target language (Banea et al, 2008; Wan, 2008),
or a domain that is ?similar? enough to the target
domain (Blitzer et al, 2007). When the target do-
main or language fails to meet this requirement,
sentiment-based clustering or unsupervised polar-
ity classification become appealing alternatives.
Unfortunately, to our knowledge, these tasks are
largely under-investigated in the NLP community.
Turney?s (2002) work is perhaps one of the most
notable examples of unsupervised polarity classi-
fication. However, while his system learns the se-
mantic orientation of the phrases in a review in an
unsupervised manner, this information is used to
predict the polarity of a review heuristically.
Despite its practical significance, sentiment-
based clustering is a challenging task. To illus-
trate its difficulty, consider the task of clustering
a set of movie reviews. Since each review may
contain a description of the plot and the author?s
580
sentiment, a clustering algorithm may cluster re-
views along either the plot dimension or the senti-
ment dimension; and without knowing the user?s
intention, they will be clustered along the most
prominent dimension. Assuming the usual bag-
of-words representation, the most prominent di-
mension will more likely be plot, as it is not un-
common for a review to be devoted almost exclu-
sively to the plot, with the author briefly express-
ing her sentiment only at the end of the review.
Even if the reviews contain mostly subjective ma-
terial, the most prominent dimension may still not
be sentiment, due to the fact that many reviews are
sentimentally ambiguous. Specifically, a reviewer
may have negative opinions on the actors but at the
same time talk enthusiastically about how much
she enjoyed the plot. The presence of both posi-
tive and negative sentiment-bearing words in these
reviews renders the sentiment dimension hidden
(i.e., less prominent) as far as clustering is con-
cerned. Therefore, there is no guarantee that the
clustering algorithm will automatically produce a
sentiment-based clustering of the reviews.
Hence, it is important for a user to provide feed-
back on the clustering process to ensure that the
reviews are clustered along the sentiment dimen-
sion, possibly in an interactive manner. One way
to do this would be to ask the user to annotate
a small number of reviews with polarity infor-
mation, possibly through an active learning pro-
cedure to minimize human intervention (Dredze
and Crammer, 2008). Another way would be to
have the user explicitly identify the relevant fea-
tures (in our case, the sentiment-bearing words) at
the beginning of the clustering process (Liu et al,
2004), or incrementally construct the set of rele-
vant features in an interactive fashion (Bekkerman
et al, 2007; Raghavan and Allan, 2007; Roth and
Small, 2009). In addition, the user may supply
constraints on which pairs of documents must or
must not appear in the same cluster (Wagstaff et
al., 2001), or simply tell the algorithm whether
two clusters should be merged or split during the
clustering process (Balcan and Blum, 2008). It is
worth noting that many of these feedback mech-
anisms were developed by machine learning re-
searchers for general clustering tasks and not for
sentiment-based clustering.
Our goal in this paper is to propose a novel
mechanism allowing a user to cluster a set of docu-
ments along the desired dimension, which may be
a hidden dimension, with very limited user feed-
back. In comparison to the aforementioned feed-
back mechanisms, ours is arguably much simpler:
we only require that the user select a dimension
by examining a small number of features for each
dimension, as opposed to having the user gener-
ate the feature space in an interactive manner or
identify clusters that need to be merged or split. In
particular, identifying clusters for merging or split-
ting in Balcan and Blum?s algorithm may not be as
easy as it appears: for each MERGE or SPLIT de-
cision the user makes, she has to sample a large
number of documents from the cluster(s), read
through the documents, and base her decision on
the extent to which the documents are (dis)similar
to each other. Perhaps more importantly, our hu-
man experiments involving five users indicate that
all of them can easily identify the sentiment di-
mension based on the features, thus providing sug-
gestive evidence that our method is viable.
In sum, our contributions in this paper are three-
fold. First, we propose a novel feedback mecha-
nism for clustering allowing a user to easily spec-
ify the dimension along which she wants data
points to be clustered and apply the mechanism
to the challenging, yet under-investigated problem
of sentiment-based clustering. Second, spectral
learning, which is the core of our method, has not
been applied extensively to NLP problems, and we
hope that our work can increase the awareness of
this powerful machine learning technique in the
NLP community. Finally, we demonstrate the via-
bility of our method not only by evaluating its per-
formance on sentiment datasets, but also via a set
of human experiments, which is typically absent
in papers that involve algorithms for incorporating
user feedback.
The rest of the paper is organized as follows.
Section 2 presents the basics of spectral clustering,
which will facilitate the discussion of our feedback
mechanism in Section 3. We describe our human
experiments and evaluation results on several sen-
timent datasets in Section 4, and present our con-
clusions in Section 5.
2 Spectral Clustering
When given a clustering task, an important ques-
tion to ask is: which clustering algorithm should
we use? A popular choice is k-means. Neverthe-
less, it is well-known that k-means has the major
drawback of not being able to separate data points
581
that are not linearly separable in the given feature
space (e.g., see Dhillon et al (2004) and Cai et al
(2005)). Spectral clustering algorithms were de-
veloped in response to this problem with k-means.
The central idea behind spectral clustering is to
(1) construct a low-dimensional space from the
original (typically high-dimensional) space while
retaining as much information about the original
space as possible, and (2) cluster the data points in
this low-dimensional space. The rest of this sec-
tion provides the details of spectral clustering.
2.1 Algorithm
Although there are several well-known spectral
clustering algorithms in the literature (e.g., Weiss
(1999), Shi and Malik (2000), Kannan et al
(2004)), we adopt the one proposed by Ng et al
(2002), as it is arguably the most widely-used. The
algorithm takes as input a similarity matrix S cre-
ated by applying a user-defined similarity function
to each pair of data points. Below are the main
steps of the algorithm:
1. Create the diagonal matrix D whose (i,i)-
th entry is the sum of the i-th row of S,
and then construct the Laplacian matrix L =
D
?1/2
SD
?1/2
.
2. Find the eigenvalues and eigenvectors of L.
3. Create a new matrix from the m eigenvectors
that correspond to the m largest eigenvalues.1
4. Each data point is now rank-reduced to a
point in the m-dimensional space. Normal-
ize each point to unit length (while retaining
the sign of each value).
5. Cluster the resulting data points using k-
means.
In essence, each dimension in the reduced space
is defined by exactly one eigenvector. The reason
why eigenvectors with large eigenvalues are used
is that they capture the largest variance in the data.
As a result, each of them can be thought of as re-
vealing an important dimension of the data.
2.2 Clustering with Eigenvectors
As Ng et al (2002) point out, ?different authors
still disagree on which eigenvectors to use, and
how to derive clusters from them?. There are two
common methods for deriving clusters using the
eigenvectors. These methods will serve as our
baselines in our evaluation.
1For brevity, we will refer to the eigenvector with the n-th
largest eigenvalue simply as the n-th eigenvector.
Method 1: Using the second eigenvector only
The first method is to use only the second eigen-
vector, e
2
, to partition the points. Besides reveal-
ing one of the most important dimensions of the
data, this eigenvector induces an intuitively ideal
partition of the data ? the partition induced by the
minimum normalized cut of the similarity graph2,
where the nodes are the data points and the edge
weights are the pairwise similarity values of the
points (Shi and Malik, 2000). Clustering in a one-
dimensional space is trivial: since we have a lin-
earization of the points, all we need to do is to
determine a threshold for partitioning the points.
However, we follow Ng et al (2002) and cluster
using 2-means in this one-dimensional space.
Method 2: Using m eigenvectors
Recall from Section 2.1 that after eigen-
decomposing the Laplacian matrix, each data
point is represented by m co-ordinates. In the
second method, we simply use 2-means to cluster
the data points in this m-dimensional space,
effectively exploiting all of the m eigenvectors.
3 Our Approach
As mentioned before, sentiment-based clustering
is challenging, in part due to the fact that the re-
views can be clustered along more than one di-
mension. In this section, we propose and incor-
porate a user feedback mechanism into a spec-
tral clustering algorithm, which makes it easy for
a user to specify the dimension along which she
wants to cluster the data points.
Recall that our method first applies spectral
clustering to reveal the most important dimensions
of the data, and then lets the user select the de-
sired dimension. To motivate the importance of
user feedback, it helps to understand why the two
baseline clustering algorithms described in Sec-
tion 2.2, which are also based on spectral meth-
ods but do not rely on user feedback, may not al-
ways yield a sentiment-based clustering. To be-
gin with, consider the first method, where only
the second eigenvector is used to induce the par-
tition. Recall that the second eigenvector reveals
the most prominent dimension of the data. Hence,
if sentiment is not the most prominent dimension
(which can happen if the non-sentiment-bearing
2Using the normalized cut (as opposed to the usual cut)
ensures that the size of the two clusters are relatively bal-
anced, avoiding trivial cuts where one cluster is empty and
the other is full. See Shi and Malik (2000) for details.
582
words outnumber the sentiment-bearing words in
the bag-of-words representation of a review), then
the resulting clustering of the reviews may not be
sentiment-oriented. A similar line of reasoning
can be used to explain why the second baseline
clustering algorithm, which clusters based on all
of the eigenvectors in the low-dimensional space,
may not always work well. Since each eigenvector
corresponds to a different dimension (and, in par-
ticular, some of them correspond to non-sentiment
dimensions), using all of them to represent a re-
view may hamper the accurate computation of the
similarity of two reviews as far as clustering along
the sentiment dimension is concerned. In the rest
of this section, we discuss the major steps of our
user-feedback mechanism in detail.
Step 1: Identify the important dimensions
To identify the important dimensions of the given
reviews, we take the top eigenvectors computed
from the eigen-decomposition of the Laplacian
matrix, which is in turn formed from the input sim-
ilarity matrix. We compute the similarity between
two reviews by taking the dot product of their fea-
ture vectors (see Section 4.1 for details on feature
vector generation). Following Ng et al, we set the
diagonal entries of the similarity matrix to 0.
Step 2: Identify the relevant features
Given the eigen-decomposition from Step 1, we
first obtain the second through the fifth eigenvec-
tors3, which as mentioned above, correspond to
the most important dimensions of the data. Then,
we ask the user to select one of the four dimen-
sions defined by these eigenvectors according to
their relevance to sentiment. One way to do this
is to (1) induce one partition of the reviews from
each of the four eigenvectors, using a procedure
identical to Method 1 in Section 2.2, and (2) have
the user inspect the four partitions and decide
which corresponds most closely to a sentiment-
based clustering. The main drawback associated
with this kind of user feedback is that the user may
have to read a large number of reviews in order to
make a decision. Hence, to reduce human effort,
we employ an alternative procedure: we (1) iden-
tify the most informative features for characteriz-
ing each partition, and (2) have the user inspect
just the features rather than the reviews.
While traditional feature selection techniques
such as log-likelihood ratio and information
3The first eigenvector is not used because it is a constant
vector, meaning that it cannot be used to partition the data.
gain can be applied to identify these informa-
tive features (see Yang and Pedersen (1997)
for an overview), we employ a more sophisti-
cated feature-ranking method that we call max-
imum margin feature ranking (MMFR). Recall
that a maximum margin classifier (e.g., a support
vector machine) separates data points from two
classes while maximizing the margin of separa-
tion. Specifically, a maximum margin hyperplane
is defined by w ? x ? b = 0, where x is a fea-
ture vector representing an arbitrary data point,
and w (a weight vector) and b (a scalar) are pa-
rameters that are learned by solving the following
constrained optimization problem:
argmin
1
2
?w?
2
+ C
?
i
?
i
subject to
c
i
(w ? x
i
? b) ? 1? ?
i
, 1 ? i ? n,
where c
i
? {+1,?1} is the class of the i-th train-
ing point x
i
, ?
i
is the degree of misclassification
of x
i
, and C is a regularization parameter that bal-
ances training error and model complexity.
We use w to identify the most informative fea-
tures for a partition. Note that a feature with a
large positive weight is strongly indicative of the
positive class, whereas a feature with a large neg-
ative weight is strongly indicative of the negative
class. In other words, the most informative fea-
tures are those with large absolute weight values.
We exploit this observation and identify the most
informative features for a partition by (1) training
an SVM classifier4 on the partition, where data
points in the same cluster belong to the same class;
(2) sorting the features according to the SVM-
learned feature weights; and (3) generating two
ranked lists of informative features using the top
and bottom 100 features, respectively.
Given the ranked lists generated for each of the
four partitions, the user will select one of the parti-
tions/dimensions as most relevant to sentiment by
inspecting as many features in the ranked lists as
needed. After picking the most relevant dimen-
sion, the user will label one of the two feature lists
associated with this dimension as POSITIVE and
the other as NEGATIVE. Since each feature list
represents one of the clusters, the cluster associ-
ated with the positive list is labeled POSITIVE and
4All the SVM classifiers in this paper are trained using
the SVMlight package (Joachims, 1999), with the learning
parameters set to their default values.
583
the cluster associated with the negative list is la-
beled NEGATIVE.
In comparison to existing user feedback mech-
anisms for assisting a clustering algorithm, ours
requires comparatively little human intervention:
we only require that the user select a dimension by
examining a small number of features, as opposed
to having the user construct the feature space or
identify clusters that need to be merged or split as
is required with other methods.
Step 3: Identify the unambiguous reviews
There is a caveat, however. As mentioned in the
introduction, many reviews contain both positive
and negative sentiment-bearing words. These am-
biguous reviews are more likely to be clustered
incorrectly than their unambiguous counterparts.
Now, since the ranked lists of features are derived
from the partition, the presence of these ambigu-
ous reviews can adversely affect the identification
of informative features using MMFR. As a result,
we remove the ambiguous reviews before deriving
informative features from a partition.
We employ a simple method for identifying un-
ambiguous reviews. In the computation of eigen-
values, each data point factors out the orthogo-
nal projections of each of the other data points
with which they have an affinity. Ambiguous data
points receive the orthogonal projections from
both the positive and negative data points, and
hence they have near zero values in the pivot
eigenvectors. We exploit this important informa-
tion. The basic idea is that the data points with
near zero values in the eigenvectors are more am-
biguous than those with large absolute values. As
a result, we posit 250 reviews from each cluster
whose corresponding values in the eigenvector are
farthest away from zero as unambiguous, and in-
duce the ranked list of features only from the re-
sulting 500 unambiguous reviews.5
Step 4: Cluster along the selected dimension
Finally, we employ the 2-means algorithm to clus-
ter all the reviews along the dimension (i.e., the
eigenvector) selected by the user, regardless of
whether a review is ambiguous or not.
5Note that 500 is a somewhat arbitrary choice. Under-
lying this choice is our assumption that a fraction of the re-
views is unambiguous. As we will see in the evaluation sec-
tion, these 500 reviews can be classified with a high accuracy;
consequently, the features induced from the resulting clus-
ters are also of high quality. Additional experiments reveal
that the list of top-ranking features does not change signifi-
cantly when induced from a smaller number of unambiguous
reviews.
4 Evaluation
4.1 Experimental Setup
Datasets. We use five sentiment classification
datasets, including the widely-used movie review
dataset [MOV] (Pang et al, 2002) as well as four
datasets containing reviews of four different types
of products from Amazon [books (BOO), DVDs
(DVD), electronics (ELE), and kitchen appliances
(KIT)] (Blitzer et al, 2007). Each dataset has
2000 labeled reviews (1000 positives and 1000
negatives). To illustrate the difference between
topic-based clustering and sentiment-based clus-
tering, we will also show topic-based clustering
results on POL, a dataset created by taking all the
documents from two sections of 20 Newsgroups,
namely, sci.crypt and talks.politics.
To preprocess a document, we first tokenize and
downcase it, and then represent it as a vector of
unigrams, using frequency as presence. In ad-
dition, we remove from the vector punctuation,
numbers, words of length one, and words that oc-
cur in only a single review. Following the common
practice in the information retrieval community,
we also exclude words with high document fre-
quency, many of which are stopwords or domain-
specific general-purpose words (e.g., ?movies? in
the movie domain). A preliminary examination
of our evaluation datasets reveals that these words
typically comprise 1?2% of a vocabulary. The de-
cision of exactly how many terms to remove from
each dataset is subjective: a large corpus typically
requires more removals than a small corpus. To be
consistent, we simply sort the vocabulary by doc-
ument frequency and remove the top 1.5%.
Evaluation metrics. We employ two evaluation
metrics. First, we report results in terms of the ac-
curacy achieved on the 2000 labeled reviews for
each dataset. Second, following Kamvar et al
(2003), we evaluate the clusters produced by our
approach against the gold-standard clusters using
the Adjusted Rand Index (ARI). ARI ranges from
?1 to 1; better clusterings have higher ARI values.
4.2 Baseline Systems
Clustering using the second eigenvector only.
As our first baseline, we adopt Shi and Malik?s ap-
proach and cluster the reviews using only the sec-
ond eigenvector, e
2
, as described in Section 2.2.
Results on POL and the five sentiment datasets are
584
Accuracy Adjusted Rand Index
System Variation POL MOV KIT BOO DVD ELE POL MOV KIT BOO DVD ELE
Baseline: 2nd eigenvector 93.7 70.9 69.7 58.9 55.3 50.8 0.76 0.17 0.15 0.03 0.01 0.01
Baseline: m eigenvectors 95.9 59.3 63.2 60.1 62.5 63.8 0.84 0.03 0.07 0.04 0.06 0.08
Our approach 93.7 70.9 69.7 69.5 70.8 65.8 0.76 0.17 0.15 0.15 0.17 0.10
Table 1: Results in terms of accuracy and Adjusted Rand Index for the six datasets.
shown in row 1 of Table 1.6 As we can see, this
baseline achieves an accuracy of 90% on POL, but
a much lower accuracy (of 50?70%) on the sen-
timent datasets. The same performance trend can
be observed with ARI. These results provide sup-
port for the claim that sentiment-based clustering
is more difficult than topic-based clustering.
In addition, it is worth noting that the base-
line achieves much lower accuracies and ARI val-
ues on BOO, DVD, and ELE than on the re-
maining two sentiment datasets. Since e
2
cap-
tures the most prominent dimension, these results
suggest that sentiment dimension is not the most
prominent dimension in these three datasets. In
fact, this is intuitively plausible. For instance,
in the book domain, positive book reviews typ-
ically contain a short description of the content,
with the reviewer only briefly expressing her sen-
timent somewhere in the review. Similarly for the
electronics domain: electronic product reviews are
typically aspect-oriented, with the reviewer talk-
ing about the pros and cons of each aspect of the
product (e.g., battery, durability). Since the re-
views are likely to contain both positive and nega-
tive sentiment-bearing words, the sentiment-based
clustering is unlikely to be captured by e
2
.
Clustering using top five eigenvectors. As our
second baseline, we represent each data point
using the top five eigenvectors (i.e., e
1
through
e
5
), and cluster them using 2-means in this 5-
dimensional space, as described in Section 2.2.
Hence, this can be thought of as an ?ensemble?
approach, where the clustering decision is collec-
tively made by the five eigenvectors.
Results are shown in row 2 of Table 1. In
comparison to the first baseline, we see improve-
ments in accuracy and ARI for the three datasets
on which the first baseline performs poorly (i.e.,
BOO, DVD, and ELE), with the most drastic
improvement observed on ELE. On the other
hand, performance on the remaining two senti-
6Owing to the randomness in the choice of seeds for 2-
means, these and all other experimental results involving 2-
means are averaged over ten independent runs.
ment datasets deteriorates. These results can be
attributed to the fact that for BOO, DVD, and
ELE, e
2
does not capture the sentiment dimension,
but since some other eigenvector in the ensemble
does, we see improvements. On the other hand, e
2
has already captured the sentiment dimension in
MOV and KIT; as a result, employing additional
dimensions, which may not be sentiment-related,
may only introduce noise into the computation of
the similarities between the reviews.
4.3 Our Approach
Human experiments. Unlike the two baselines,
our approach requires users to specify which of the
four dimensions (defined by the second through
fifth eigenvectors) are most closely related to sen-
timent by inspecting a set of features derived from
the unambiguous reviews for each dimension us-
ing MMFR. To better understand how easy it is
for a human to select the desired dimension given
the features, we performed the experiment inde-
pendently with five humans (all of whom are com-
puter science graduate students not affiliated with
this research) and computed the agreement rate.
More specifically, for each dataset, we showed
each human judge the top 100 features for each
cluster according to MMFR (see Tables 4?6 for
a snippet). In addition, we informed them of the
intended dimension: for example, for POL, the
judge was told that the intended clustering is Poli-
tics vs. Science. Also, if she determined that more
than one dimension was relevant to the intended
clustering, she was instructed to rank these dimen-
sions in terms of their degree of relevance, where
the most relevant one would appear first in the list.
The dimensions (expressed in terms of the IDs
of the eigenvectors) selected by each of the five
judges for each dataset are shown in Table 2. The
agreement rate (shown in the last row of the ta-
ble) was computed based on only the highest-
ranked dimension selected by each judge. As we
can see, perfect agreement is achieved for four of
the five sentiment datasets, and for the remaining
two datasets, near-perfect agreement is achieved.
585
Judge POL MOV KIT BOO DVD ELE
1 2,3,4 2 2 4 3 3
2 2,4 2 2 4 3 3
3 4 2,4 4 4 3 3
4 2,3 2 2 4 3 3,4
5 2 2 2 4 3 3
Agr 80% 100% 80% 100% 100% 100%
Table 2: Human agreement rate.
POL MOV KIT BOO DVD ELE
Acc 99.8 87.0 87.6 86.2 87.4 77.6
Table 3: Accuracies on unambiguous documents.
These results together with the fact that it took 5?
6 minutes to identify the relevant dimension, indi-
cate that asking a human to determine the intended
dimension based on solely the ?informative? fea-
tures is a viable task.
Clustering results. Next, we cluster all 2000
documents for each dataset using the dimension
selected by the majority of the human judges. The
clustering results are shown in row 3 of Table 1. In
comparison to the better baseline for each dataset,
we see that our approach performs substantially
better on BOO, DVD and ELE, at almost the same
level on MOV and KIT, but slightly worse on POL.
Note that the improvements observed for BOO,
DVD and ELE can be attributed to the failure of e
2
to capture the sentiment dimension. Perhaps most
importantly, by exploiting human feedback, our
approach has achieved more stable performance
across the datasets than the baselines, with accura-
cies ranging from 65.8% to 93.7% and ARI rang-
ing from 0.10 to 0.76.
Role of unambiguous documents. Recall that
the features with the largest MMFR were com-
puted from the unambiguous documents only. To
get an intuitive understanding of the role of unam-
biguous documents in our approach, we show in
Table 3 the accuracy when the unambiguous doc-
uments in each dataset were clustered using the
eigenvector selected by the majority of the judges.
As we can see, the accuracy of each dataset is
higher than the corresponding accuracy shown in
row 3 of Table 1. In fact, an accuracy of more than
85% was achieved on all but one dataset. This sug-
gests that our method of identifying unambiguous
documents is useful.
Note that it is crucial to be able to achieve a high
accuracy on the unambiguous documents: if clus-
tering accuracy is low, the features induced from
the clusters may not be an accurate representation
of the corresponding dimension, and the human
judge may have a difficult time identifying the in-
tended dimension. In fact, some human judges re-
ported difficulty in identifying the correct dimen-
sion for the ELE dataset, and this can be attributed
in part to the low accuracy achieved on the unam-
biguous documents.
Features as summary. Recall that the method
we proposed represents each dimension with a
small number of features and asks a user to se-
lect the desired dimension by inspecting the corre-
sponding feature lists. In other words, each feature
list serves as a ?summary? of its corresponding di-
mension, and inspecting the features induced for
each dimension can give us insights into the dif-
ferent dimensions of a dataset. Hence, if a user is
not sure how she wants the data points to be clus-
tered (due to lack of knowledge of the data, for
instance), our automatically induced features may
serve as an overview of the different dimensions
of the data. To better understand whether these
features can indeed provide a user with additional
useful information about a dataset, we show in Ta-
bles 4?6 the top ten features induced for each clus-
ter and each dimension for the six datasets. As an
example, consider the MOV dataset. Inspecting
the induced features, we can determine that it has
a sentiment dimension (e
2
), as well as a humor vs.
thriller dimension (e
4
). In other words, if we clus-
ter along e
2
, we get a sentiment-based clustering;
and if we cluster along e
4
, we obtain a genre-based
(humor vs. thriller) clustering.
User feedback vs. labeled data. Recall that our
two baselines are unsupervised, whereas our ap-
proach can be characterized as semi-supervised, as
it relies on user feedback to select the intended di-
mension. Hence, it should not be surprising to see
that the average clustering performance of our ap-
proach is better than that of the baselines.
To do a fairer comparison, we conduct another
experiment in which we compare our approach
against a semi-supervised sentiment classification
system, which uses transductive SVM as the un-
derlying semi-supervised learner. More specifi-
cally, the goal of this experiment is to determine
how many labeled documents are needed in or-
der for the transductive learner to achieve the same
level of performance as our approach. To answer
this question, we first give the transductive learner
access to the 2000 documents for each dataset as
586
POL MOV
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
serder beyer serbs escrow relationship production jokes starts
armenian arabs palestinians serial son earth kids person
turkey andi muslims algorithm tale sequences live saw
armenians research wrong chips husband aliens animation feeling
muslims israelis department ensure perfect war disney lives
sdpa tim bosnia care drama crew animated told
argic uci live strong focus alien laughs happen
davidian ab matter police strong planet production am
dbd@ura z@virginia freedom omissions beautiful horror voice felt
troops holocaust politics excepted nature evil hilarious happened
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
sternlight escrow standard internet worst sex thriller comic
wouldn sternlight sternlight uucp stupid romantic killer sequences
pgp algorithm des uk waste school murder michael
crypto access escrow net bunch relationship crime supporting
algorithm net employer quote wasn friends police career
isn des net ac video jokes car production
likely privacy york co worse laughs dead peter
access uk jake didn boring sexual killed style
idea systems code ai guess cute starts latest
cryptograph pgp algorithm mit anyway mother violence entertaining
Table 4: Top ten features induced for each dimension for the POL and MOV domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
BOO ELE
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
history series loved must mouse music easy amazon
must man highly wonderful cable really used cable
modern history easy old cables ipod card card
important character enjoyed feel case too fine recommend
text death children away red little using dvd
reference between again children monster headphones problems camera
excellent war although year picture hard fine fast
provides seems excellent someone kit excellent drive far
business political understand man overall need computer printer
both american three made paid fit install picture
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
plot buy money boring working worked money phone
didn bought bad series never problem worth off
thought information nothing history before never amazon worked
boring easy waste pages phone item over power
got money buy information days amazon return battery
character recipes anything between headset working years unit
couldn pictures doesn highly money support much set
ll look already page months months headphones phones
ending waste instead excellent return returned sony range
fan copy seems couldn second another received little
Table 5: Top ten features induced for each dimension for the BOO and ELE domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
unlabeled data. Next, we randomly sample 50 un-
labeled documents and assign them the true label.
We then re-train the classifier and compute its ac-
curacy on the 2000 documents. We keep adding
more labeled data (50 in each iteration) until it
reaches the accuracy achieved by our system. Re-
sults of this experiment are shown in Table 7. Ow-
ing in the randomness involved in the selection of
unlabeled documents, these results are averaged
over ten independent runs. As we can see, our
587
KIT DVD
e
2
e
3
e
4
e
5
e
2
e
3
e
4
e
5
C
1
C
1
C
1
C
1
C
1
C
1
C
1
C
1
love works really pan worth music video money
clean water nice oven bought collection music quality
nice clean works cooking series excellent found video
size work too made money wonderful feel worth
set ice quality pans season must bought found
kitchen makes small better fan loved workout version
easily thing sturdy heat collection perfect daughter picture
sturdy need little cook music highly recommend waste
recommend keep think using tv makes our special
price best item clean thought special disappointed sound
C
2
C
2
C
2
C
2
C
2
C
2
C
2
C
2
months price ve love young worst series saw
still item years coffee between money cast watched
back set love too actors thought fan loved
never ordered never recommend men boring stars enjoy
worked amazon clean makes cast nothing original whole
money gift months over seems minutes comedy got
did got over size job waste actors family
amazon quality pan little beautiful saw worth series
return received been maker around pretty classic season
machine knives pans cup director reviews action liked
Table 6: Top ten features induced for each dimension for the KIT and DVD domains. The shaded columns
correspond to the dimensions selected by the human judges. e
2
, . . ., e
5
are the top eigenvectors; C
1
and C
2
are the clusters.
POL MOV KIT BOO DVD ELE
# labels 400 150 200 350 350 200
Table 7: Transductive SVM results.
user feedback is equivalent to the effort of hand-
annotating 275 documents per dataset on average.
Multiple relevant dimensions. As seen from
Table 2, some human judges selected more than
one dimension for some datasets (e.g., 2,3,4 for
POL; 2,4 for MOV; and 3,4 for ELE). However,
we never took into account these ?extra? dimen-
sions in our previous experiments. To better un-
derstand whether these extra dimensions can help
improve accuracy and ARI, we conduct another
experiment in which we apply 2-means to clus-
ter the documents in a space that is defined by
all of the selected dimensions. The final accu-
racy turns out to be 95.9%, 70.9%, and 67.5% for
POL, MOV, and ELE respectively, which is con-
siderably better than using only the optimal di-
mension and suggests that the extra dimensions
contain useful information.
5 Conclusions
Unsupervised clustering algorithms typically
group objects along the most prominent di-
mension, in part owing to their objective of
simultaneously maximizing inter-cluster similar-
ity and intra-cluster dissimilarity. Hence, if the
user?s intended clustering dimension is not the
most prominent dimension, these unsupervised
clustering algorithms will fail miserably. To
address this problem, we proposed to integrate a
novel user feedback mechanism into a spectral
clustering algorithm, which allows us to mine
the intended, possibly hidden, dimension of the
data and produce the desired clustering. This
mechanism differs from competing methods in
that it requires very limited feedback: to select the
intended dimension, the user only needs to inspect
a small number of features. We demonstrated its
viability via a set of human and automatic experi-
ments with unsupervised sentiment classification,
obtaining promising results.
In future work, we plan to explore several ex-
tensions to our proposed method. First, we plan to
use our user-feedback method in combination with
existing methods (e.g., Bekkerman et al (2007))
for improving its performance. For instance, in-
stead of having the user construct a relevant fea-
ture space from scratch, she can simply extend
the set of informative features identified for the
user-selected dimension. Second, since none of
the steps in our method is specifically designed
for sentiment classification, we plan to apply it to
other non-topic-based text classification tasks.
588
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
Maria-Florina Balcan and Avrim Blum. 2008. Clus-
tering with interactive feedback. In Proceedings of
ALT, pages 316?328.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
of EMNLP, pages 127?135.
Ron Bekkerman, Hema Raghavan, James Allan, and
Koji Eguchi. 2007. Interactive clustering of text
collections according to a user-specified criterion.
In Proceedings of IJCAI, pages 684?689.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the ACL, pages 440?447.
Deng Cai, Xiaofei He, and Jiawei Han. 2005. Doc-
ument clustering using locality preserving indexing.
IEEE Transactions on Knowledge and Data Engi-
neering, 17(12):1624?1637.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. 2004.
Kernel k-means, spectral clustering and normalized
cuts. In Proceedings of KDD, pages 551?556.
Mark Dredze and Koby Crammer. 2008. Active learn-
ing with confidence. In Proceedings of ACL-08:HLT
Short Papers (Companion Volume), pages 233?236.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Sepandar Kamvar, Dan Klein, and Chris Manning.
2003. Spectral learning. In Proceedings of IJCAI,
pages 561?566.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING, pages 1367?1373.
Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu.
2004. Text classification by labeling words. In Pro-
ceedings of AAAI, pages 425?430.
Tony Mullen and Nigel Collier. 2004. Sentiment
analysis using support vector machines with diverse
information sources. In Proceedings of EMNLP,
pages 412?418.
Andrew Ng, Michael Jordan, and Yair Weiss. 2002.
On spectral clustering: Analysis and an algorithm.
In Advances in NIPS 14.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Hema Raghavan and James Allan. 2007. An interac-
tive algorithm for asking and incorporating feature
feedback into support vector machines. In Proceed-
ings of SIGIR, pages 79?86.
Dan Roth and Kevin Small. 2009. Interactive feature
space construction using semantic information. In
Proceedings of CoNLL, pages 66?74.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Ste-
fan Schro?dl. 2001. Constrained k-means cluster-
ing with background knowledge. In Proceedings of
ICML, pages 577?584.
Xiaojun Wan. 2008. Using bilingual knowledge and
ensemble techniques for unsupervised Chinese sen-
timent analysis. In Proceedings of EMNLP, pages
553?561.
Yair Weiss. 1999. Segmentation using eigenvectors: A
unifying view. In Proceedings of ICCV, pages 975?
982.
Yiming Yang and Jan Pedersen. 1997. A comparative
study on feature selection in text categorization. In
Proceedings of ICML, pages 412?420.
589
Proceedings of NAACL HLT 2007, pages 155?163,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
High-Performance, Language-Independent Morphological Segmentation 
 
 
Sajib Dasgupta and Vincent Ng 
Human Language Technology Research Institute 
University of Texas at Dallas 
Richardson, TX 75083-0688 
{sajib,vince}@hlt.utdallas.edu 
 
 
 
Abstract 
This paper introduces an unsupervised 
morphological segmentation algorithm 
that shows robust performance for four 
languages with different levels of mor-
phological complexity. In particular, our 
algorithm outperforms Goldsmith?s Lin-
guistica and Creutz and Lagus?s Mor-
phessor for English and Bengali, and 
achieves performance that is comparable 
to the best results for all three PASCAL 
evaluation datasets. Improvements arise 
from (1) the use of relative corpus fre-
quency and suffix level similarity for de-
tecting incorrect morpheme attachments 
and (2) the induction of orthographic rules 
and allomorphs for segmenting words 
where roots exhibit spelling changes dur-
ing morpheme attachments. 
1 Introduction 
Morphological analysis is the task of segmenting a 
word into morphemes, the smallest meaning-
bearing elements of natural languages. Though 
very successful, knowledge-based morphological 
analyzers operate by relying on manually designed 
segmentation heuristics (e.g. Koskenniemi (1983)), 
which require a lot of linguistic expertise and are 
time-consuming to construct. As a result, research 
in morphological analysis has exhibited a shift to 
unsupervised approaches, in which a word is typi-
cally segmented based on morphemes that are 
automatically induced from an unannotated corpus. 
Unsupervised approaches have achieved consider-
able success for English and many European lan-
guages (e.g. Goldsmith (2001), Schone and Juraf-
sky (2001), Freitag (2005)). The recent PASCAL 
Challenge on Unsupervised Segmentation of 
Words into Morphemes1  has further intensified 
interest in this problem, selecting as target lan-
guages English as well as two highly agglutinative 
languages, Turkish and Finnish. However, the 
evaluation of the Challenge reveals that (1) the 
success of existing unsupervised morphological 
parsers does not carry over to the two agglutinative 
languages, and (2) no segmentation algorithm 
achieves good performance for all three languages. 
Motivated by these state-of-the-art results, our 
goal in this paper is to develop an unsupervised  
morphological segmentation algorithm that can 
work well across different languages. With this 
goal in mind, we evaluate our algorithm on four 
languages with different levels of morphological 
complexity, namely English, Turkish, Finnish and 
Bengali. It is worth noting that Bengali is an under-
investigated Indo-Aryan language that is highly 
inflectional and lies between English and Turk-
ish/Finnish in terms of morphological complexity. 
Experimental results demonstrate the robustness of 
our algorithm across languages: it not only outper-
forms Goldsmith?s (2001) Linguistica and Creutz 
and Lagus?s (2005) Morphessor for English and 
Bengali, but also compares favorably to the best-
performing PASCAL morphological parsers when 
evaluated on all three datasets in the Challenge.  
The performance improvements of our segmen-
tation algorithm over existing morphological ana-
lyzers can be attributed to our extending Keshava 
and Pitler?s (2006) segmentation method, the best 
performer for English in the aforementioned 
                                                          
1
 http://www.cis.hut.fi/morphochallenge2005/ 
155
PASCAL Challenge, with the capability of han-
dling two under-investigated problems: 
Detecting incorrect attachments. Many existing 
morphological parsers incorrectly segment ?candi-
date? as ?candid?+?ate?, since they fail to identify 
that the morpheme ?ate? should not attach to the 
word ?candid?. Schone and Jurafsky?s (2001) work 
represents one of the few attempts to address this 
inappropriate morpheme attachment problem, in-
troducing a method that exploits the semantic re-
latedness between word pairs. In contrast, we 
propose two arguably simpler, yet effective tech-
niques that rely on relative corpus frequency and 
suffix level similarity to solve the problem. 
Inducing orthographic rules and allomorphs.  
One problem with Keshava and Pitler?s algorithm 
is that it fails to segment words where the roots 
exhibit spelling changes during attachment to mor-
phemes (e.g. ?denial? = ?deny?+?al?). To address 
this problem, we automatically acquire allomorphs 
and orthographic change rules from an unannotated 
corpus. These rules also allow us to output the ac-
tual segmentation of the words that exhibit spelling 
changes during morpheme attachment, thus avoid-
ing the segmentation of ?denial? as ?deni?+?al?, as 
is typically done in existing morphological parsers. 
    In addition to addressing the aforementioned 
problems, our segmentation algorithm has two ap-
pealing features. First, it can segment words with 
any number of morphemes, whereas many analyz-
ers can only be applied to words with one root and 
one suffix (e.g. D?Jean (1998), Snover and Brent 
(2001)). Second, it exhibits robust performance 
even when inducing morphemes from a very large 
vocabulary, whereas Goldsmith?s (2001) and 
Freitag?s (2005) morphological analyzers perform 
well only when a small vocabulary is employed, 
showing deteriorating performance as the vocabu-
lary size increases.  
The rest of this paper is organized as follows. 
Section 2 presents related work on unsupervised 
morphological analysis. In Section 3, we describe 
our basic morpheme induction algorithm. We then 
show how to exploit the induced morphemes to (1) 
detect incorrect attachments by using relative cor-
pus frequency (Section 4) and suffix level similar-
ity (Section 5) and (2) induce orthographic rules 
and allomorphs (Section 6). Section 7 describes 
our algorithm for segmenting a word using the in-
duced morphemes. We present evaluation results 
in Section 8 and conclude in Section 9. 
2 Related Work 
As mentioned in the introduction, the problem of 
unsupervised morphological learning has been ex-
tensively studied for English and many other 
European languages. In this section, we will give 
an overview of the related work on this problem. 
Harris (1955) develops a strategy for identifying 
morpheme boundaries that checks whether the 
number of different letters following a sequence of 
letters exceeds some given threshold. D?Jean 
(1998) improves Harris?s segmentation algorithm 
by first inducing a list of 100 most frequent mor-
phemes and then using those morphemes for word 
segmentation. The aforementioned PASCAL Chal-
lenge on Unsupervised Word Segmentation has 
undoubtedly intensified interest in this problem. 
Among the participating groups, Keshava and Pit-
ler?s (2006) segmentation algorithm combines the 
ideas of D?Jean and Harris and achieves the best 
result for the English dataset, but it only offers me-
diocre performance for Finnish and Turkish.   
There is another class of unsupervised morpho-
logical learning algorithms whose design is driven 
by the Minimum Description Length (MDL) prin-
ciple. Specifically, EM is used to iteratively seg-
ment a list of words using some predefined 
heuristics until the length of the morphological 
grammar converges to a minimum. Brent et al 
(1995) are the first to introduce an information-
theoretic notion of compression to represent the 
MDL framework. Goldsmith (2001) also adopts 
the MDL approach, providing a new compression 
system that incorporates signatures when measur-
ing the length of the morphological grammar. 
Creutz (2003) proposes a probabilistic maximum a 
posteriori formulation that uses prior distributions 
of morpheme length and frequency to measure the 
goodness of an induced morpheme, achieving bet-
ter results for Finnish but worse results for English 
in comparison to Goldsmith?s Linguistica. 
3 The Basic Morpheme Induction Algo-
rithm 
Our unsupervised segmentation algorithm is com-
posed of two steps: (1) inducing prefixes, suffixes 
and roots from a vocabulary that consists of words 
taken from a large corpus, and (2) segmenting a 
word using these induced morphemes. This section 
describes our basic morpheme induction method. 
156
3.1 Extracting a List of Candidate Affixes 
The first step of our morpheme induction method 
involves extracting a list of candidate prefixes and 
suffixes. We rely on a fairly simple idea originally 
proposed by Keshava and Pitler (2006) for extract-
ing candidate affixes. Assume that    and 

 are two 
character sequences and  

 is the concatenation of 
 
 and 

. If  

 and    are both found in the vocabu-
lary, then we extract 

 as a candidate suffix. Simi-
larly, if  

 and 

 are both found in the vocabulary, 
then we extract    as a candidate prefix.  
The above affix induction method is arguably 
overly simplistic and therefore can generate many 
spurious affixes. To filter spurious affixes, we (1) 
score each affix by multiplying its frequency (i.e. 
the number of distinct words to which each affix 
attaches) and its length2, and then (2) retain only 
the K top-scoring affixes, where K is set differently 
for prefixes and suffixes. The value of K is some-
what dependent on the vocabulary size, as the af-
fixes in a larger vocabulary system are generated 
from a larger number of words.  For example, we 
set the thresholds to 70 for prefixes and 50 for suf-
fixes for English; on the other hand, since the Fin-
nish vocabulary is almost six times larger than that 
of English, we set the corresponding thresholds to 
be approximately six times larger (400 and 300 for 
prefixes and suffixes respectively).3  
3.2 Detecting Composite Suffixes 
Next, we detect and remove composite suffixes (i.e. 
suffixes that are formed by combining multiple 
suffixes [e.g. ?ers? = ?er?+?s?]) from our induced 
suffix list, because their presence can lead to un-
der-segmentation of words (e.g. ?walkers?, whose 
correct segmentation is ?walk?+?er?+?s?, will be 
erroneously segmented as ?walk?+?ers?). Compos-
ite suffix detection is a particularly important prob-
lem for languages like Bengali in which composite 
suffixes are abundant (see Dasgupta and Ng 
(2007)). Note, however, that simple concatenation 
of multiple suffixes does not always produce a 
composite suffix. For example, ?ent?, ?en? and ?t? 
all are valid suffixes in English, but ?ent? is not a 
                                                          
2
 The dependence on frequency and length is motivated by the observation that 
less-frequent and shorter affixes (especially those of length 1) are more likely to 
be erroneous (see Goldsmith (2001)). 
3
 Since this method for setting our vocabulary-dependent thresholds is fairly 
simple, the use of these thresholds should not be viewed as rendering our seg-
mentation algorithm language-dependent.    
composite suffix. Hence, we need a more sophisti-
cated method for composite suffix detection.  
 Our detection method is motivated by the fol-
lowing observation: if xy is a composite suffix and 
a word w combines with xy, then it is highly likely 
that w will also combine with its first component 
suffix x. Note that this property does not hold for 
non-composite suffixes. For instance, words that 
combine with the non-composite suffix ?ent? (e.g. 
?absorb?) do not combine with its first component 
suffix ?en?. Consequently, given two suffixes x 
and y, our method posits xy as a composite suffix if 
xy and x are similar in terms of the words to which 
they attach. Specifically, we consider xy and x to 
be similar if their similarity value as computed by 
the formula below is greater than 0.6: 
||
|?|)|(),(
W
W
xyxPxxySimilarity == , 
where |W  | is the number of distinct words that 
combine with both xy and x, and |W| is the number 
of  distinct words that combine with xy. 
3.3 Extracting a List of Candidate Roots 
Finally, we extract a list of candidate roots using 
the induced list of affixes as follows. For each 
word, w, in the vocabulary, we check whether w is 
divisible, i.e. whether w can be segmented as r+x 
or p+r, where p is an induced prefix, x is an in-
duced suffix, and r is a word in the vocabulary. We 
then add w to the root list if it is not divisible. 
Note, however, that the resulting root list may con-
tain compound words (i.e. words with multiple 
roots). Hence, we make another pass over our root 
list to remove any word that is a concatenation of 
multiple words in the vocabulary. 
4 Detecting Incorrect Attachments Using 
Relative Frequency 
Our induced root list is not perfect: many correct 
roots are missing due to over-segmentation. For 
example, since ?candidate? and ?candid? are in the 
vocabulary and ?ate? is an induced suffix, our root 
induction method will incorrectly segment ?candi-
date? as ?candid?+?ate?; as a result, it does not 
consider ?candidate? as a root. So, to improve the 
root induction method, we need to determine that 
the attachment of the morpheme ?ate? to the root 
word ?candid? is incorrect. In this section, we pro-
pose a simple yet novel idea of using relative cor-
157
pus frequency to determine whether the attachment 
of a morpheme to a root word is plausible or not.  
Consider again the two words ?candidate? and 
?candid?. While ?candidate? occurs 6380 times in 
our corpus, ?candid? occurs only 119 times. This 
frequency disparity can be an important clue to 
determining that there is no morphological relation 
between ?candidate? and ?candid?. Similar obser-
vation is also made by Yarowsky and Wicentowski 
(2000), who successfully employ relative fre-
quency similarity or disparity to rank candidate 
VBD/VB pairs (e.g. ?sang?/?sing?) that are irregu-
lar in nature. Unlike Yarowsky and Wicentowski, 
however, our goal is to detect incorrect affix at-
tachments and improve morphological analysis.  
Our incorrect attachment detection algorithm, 
which exploits frequency disparity, is based on the 
following hypothesis: if a word w is formed by 
attaching an affix m to a root word r, then the cor-
pus frequency of w is likely to be less than that of r 
(i.e. the frequency ratio of w to r is less than one). 
In other words, we hypothesize that the inflectional 
or derivational forms of a root word occur less fre-
quently in a corpus than the root itself.  
To illustrate this hypothesis, Table 1 shows 
some randomly chosen English words together 
with their word-root frequency ratios (WRFRs).  
The <word, root> pairs in the left side of the table 
are examples of correct attachments, whereas those 
in the right side are not. Note that only those words 
that represent correct attachments have a WRFR 
less than 1. 
The question, then, is: to what extent does our 
hypothesis hold? To investigate this question, we 
generated examples of correct attachments by ran-
domly selecting 400 words from our English vo-
cabulary and then removing those that are root 
words, proper nouns, or compound words. We then 
manually segmented each of the remaining 378 
words as Prefix+Root or Root+Suffix with the aid 
of the CELEX lexical database (Baayean et al, 
1996). Somewhat surprisingly, we found that the 
WRFR is less than 1 in only 71.7% of these at-
tachments. When the same experiment was re-
peated on 287 hand-segmented Bengali words, the 
hypothesis achieves a higher accuracy of 83.6%.  
To better understand why our hypothesis does 
not work well for English, we measured its accu-
racy separately for the Root+Suffix words and the 
Prefix+Root words, and found that the hypothesis 
fails mostly on the suffixal attachments (see Table 
2).  Though surprising at first glance, the relatively 
poor accuracy on suffixal attachments can be at-
tributed to the fact that many words in English 
(e.g. ?amusement?, ?winner?) appear more fre-
quently in our corpus than their corresponding root 
forms (e.g. ?amuse?, ?win?). For Bengali, our hy-
pothesis fails mainly on verbs, whose inflected 
forms occur more often in our corpus than their 
roots. This violation of the hypothesis can be at-
tributed to the grammatical rule that the main verb 
of a Bengali sentence has to be inflected according 
to the subject in order to maintain sentence order. 
To improve the accuracy of our hypothesis on 
detecting correct attachments, we relax our initial 
hypothesis as follows: if an attachment is correct, 
then the corresponding WRFR is less than some 
predefined threshold t, where t > 1. However, we 
do not want t to be too large, since our algorithm 
may then determine many incorrect attachments as 
correct. In addition, since our hypothesis has a high 
accuracy for prefixal attachments than suffixal at-
tachments, the threshold we employ for prefixes 
can be smaller than that for suffixes. Taking into 
account these considerations, we use a threshold of 
10 for suffixes and 2 for prefixes for all the lan-
guages we consider in this paper. 
 
Correct Parses Incorrect Parses 
Word Root WRFR Word Root WRFR 
bear-able bear 0.01 candid-ate candid 53.6 
attend-ance attend 0.24 medic-al medic 483.9 
arrest-ing arrest 0.06 prim-ary prim 327.4 
sub-group group 0.0002 ac-cord cord 24.0 
re-cycle cycle 0.028 ad-diction diction 52.7 
un-settle settle 0.018 de-crease crease 20.7 
Table 1: Word-root frequency ratios 
 
 Root+Suffix Prefix+Root Overall 
 # of words 344 34 378 
WRFR < 1 70.1% 88.2% 71.7% 
Table 2: Hypothesis validation for English 
 
Now we can employ our hypothesis to detect in-
correct attachments and improve root induction as 
follows. For each word, w, in the vocabulary, we 
check whether (1) w can be segmented as r+x or 
p+r, where p and x are valid prefixes and suffixes 
respectively and r is another word in the vocabu-
lary, and (2) the WRFR for w and r is less than our 
predefined thresholds (10 for suffixes and 2 for 
prefixes). If both conditions are satisfied, it means 
that w is divisible. Hence, we add w into the list of 
roots if at least one of the conditions is violated. 
158
5 Suffix Level Similarity 
Many of the incorrect suffixal attachments have a 
WRFR between 1 and 10, but the detection algo-
rithm described in the previous section will deter-
mine all of them as correct attachments. Hence, in 
this section, we propose another technique, which 
we call suffix level similarity, to identify some of 
these incorrect attachments. 
Suffix level similarity is motivated by the fol-
lowing observation: if a word w combines with a 
suffix x, then w should also combine with the suf-
fixes that are ?morphologically similar? to x. To 
exemplify, consider the suffix ?ate? and the root 
word ?candid?. The words that combine with the 
suffix ?ate? (e.g. ?alien?, ?fabric?, ?origin?) also 
combine with suffixes like ?ated?, ?ation? and ?s?. 
Given this observation, the question of whether 
?candid? combines with the suffix ?ate? then lies 
in whether or not ?candid? combines with ?ated?, 
?s? and ?ation?. The fact that ?candid? does not 
combine with many of the above suffixes provides 
suggestive evidence that ?candidate? cannot be 
derived from ?candid?.  
More specifically, to check whether a word w 
combines with a suffix x using suffix level simial-
rity, we (1) find the set of words Wx that can com-
bine with x; (2) find the set of suffixes Sx that 
attach to all of the words in Wx under the constraint 
that Sx does not contain x; and (3) find the 10 suf-
fixes in Sx that are most ?similar? to x. The ques-
tion, then, is how to define similarity. Intuitively, a 
good similarity metric should reflect, for instance, 
the fact that ?ated? is a better suffix to consider in 
the attachment decision for ?ate? than ?s? (i.e. 
?ated? is more similar to ?ate? than ?s?), since ?s? 
attaches to most nouns and verbs in English and 
hence should not be a distinguishing feature for 
incorrect attachment detection.  
We employ a probabilistic measure (PM) that 
computes the similarity between suffixes x and y as 
the product of (1) the probability of a word com-
bining with y given that it combines with x and (2) 
the probability of a word combining with x given 
that it combines with y. More specifically, 
,*)|(*)|(),(
21 n
n
n
nyxPxyPyxPM ==  
where n1 is the number of distinct words that com-
bine with x, n2 is the number of distinct words that 
combine with y, and n is the number of distinct 
words that combine with both x and y.4  
After getting the 10 suffixes that are most simi-
lar to x, we employ them as features and use the 
associated similarity values (we scale them linearly 
between 1 and 10) as the weights of these 10 fea-
tures. The decision of whether a suffix x can attach 
to a word w depends on whether the following ine-
quality is satisfied: 
,
10
1
twf ii >  
where fi is a boolean variable that has the value 1 if 
w combines with xi, where xi is one of the 10 suf-
fixes that are most similar to x; wi is the scaled 
similarity between x and xi; and t is a predefined 
threshold that is greater than 0. 
One potential problem with suffix level similar-
ity is that it is an overly strict condition for those 
words that combine with only one or two suffixes 
in the vocabulary. For example, if the word ?char-
acter? has just one variant in the vocabulary (e.g. 
?characters?), suffix level similarity will determine 
the attachment of ?s? to ?character? as incorrect, 
since the weighted sum in the above inequality will 
be 0. To address this sparseness problem, we rely 
on both relative corpus frequency and suffix level 
similarity to identify incorrect attachments. Spe-
cifically, if the WRFR of a <word, root> pair is 
between 1 and 10, we determine that an attachment 
to the root is incorrect if 
 
-WRFR +    * (suffix level similarity) < 0, 
 
where    is set to 0.15. 
Finally, since long words have a higher chance 
of getting segmented, we do not apply suffix level 
similarity to words whose length is greater than 10. 
6 Inducing Orthographic Rules and Al-
lomorphs 
The biggest drawback of the system, described 
thus far, is its failure to segment words where the 
roots exhibit spelling changes during attachment to 
morphemes (e.g. ?denial? = ?deny?+?al?). The 
reasons are (1) the system does not have any 
knowledge of language-specific orthographic rules 
(e.g. in English, the character ?y? at the morpheme 
boundary is changed to ?i? when the root combines 
                                                          
4
 Note that this metric has the desirable property of returning a low similarity 
value for ?s?: while n is likely to be large, it will be offset by a large n2. 
159
with the suffix ?al?), and (2) the vocabulary we 
employ for morpheme induction does not normally 
contain the allomorphic variations of the roots 
(e.g. ?deni? is allomorphic variation of ?deny?). To 
segment these words correctly, we need to generate 
the allomorphs and orthographic rules automati-
cally given a set of induced roots and affixes.  
Before giving the details of the generation 
method, we note that the induction of orthographic 
rules is a challenging problem, since different lan-
guages exhibit orthographic changes in different 
ways. For some languages (e.g. English) rules are 
mostly predictable, whereas for others (e.g. Fin-
nish) rules are highly irregular. It is hard to obtain 
a generalized mapping function that aligns every 
<root, allomorph> pair, considering the fact that 
our system is unsupervised. An additional chal-
lenge is to ensure that the incorporation of these 
orthographic rules will not adversely affect system 
performance (i.e. they will not be applied to regu-
lar words and thus segment them incorrectly). 
Yarowsky and Wicentowski (2000) propose an 
interesting algorithm that employs four similarity 
measures to successfully identify the most prob-
able root of a highly irregular word. Unlike them, 
however, our goal is to (1) check whether the 
learned rules can actually improve an unsupervised 
morphological system, not just to align <root, al-
lomorph> pair, and (2) examine whether our sys-
tem is extendable to different languages.  
Taking into consideration the aforementioned 
challenges, our induction algorithm will (1) handle 
orthographic character changes that occur only at 
morpheme boundaries; (2) generate allomorphs for 
suffixal attachments only5, assuming that roots ex-
hibit the character changes during attachment, not 
suffixes; and (3) learn rules that aligns <root, allo-
morph> pairs of edit distance 1 (which may in-
volve 1-character replacement, deletion or 
insertion). Despite these limitations, we will see 
that the incorporation of the induced rules im-
proves segmentation accuracy significantly. 
 Let us first discuss how we learn a replacement 
rule, which identifies <allomorph, root> pairs 
where the last character of the root is replaced by 
another character. The steps are as follows: 
(1) Inducing candidate allomorphs 
If   A

 is a word in the vocabulary (e.g. ?denial?, 
where   =?den?, A=?i?, and 

=?al?),   is an in-
                                                          
5
 We only learn rules for suffixes of length greater than 1, since most suffixes of 
length 1 do not participate in orthographic changes.  
duced suffix,   B is an induced root (e.g. ?deny?, 
where B=?y?), and the attachment of   to   B is 
correct according to relative corpus frequency (see 
Section 4), then we hypothesize that   A is an allo-
morph of   B. For each induced suffix, we use this 
hypothesis to generate the allomorphs and identify 
those that are generated from at least two suffixes 
as candidate allomorphs. We denote the list of 
<candidate allomorph, root, suffix> tuples by L. 
(2) Learning orthographic rules 
Every <candidate allomorph, root, suffix> tuple as 
learned above is associated with an orthographic 
rule. For example, from the words ?denial?, ?deny? 
and suffix ?al?, we learn the rule ?y:i / _ + al?6; 
from ?social?, ?sock? and ?al?, we learn the rule 
?k:i / _ + al?, which, however, is erroneous. So, we 
check whether each of the learned rules occurs fre-
quently enough for all the <allomorph, root> pairs 
associated with a suffix, with the goal of filtering 
the low-frequency orthographic rules. Specifically, 
for each suffix 

, we repeat the following steps: 
(a) Counting the frequency of rules. Let L   be the 
list of <candidate allomorph, root> pairs in L that 
are associated with the suffix 

. For each pair p in 
L  , we first check whether its candidate allomorph 
appears in any other <candidate allomorph, root> 
pairs in L  . If not, we increment the frequency of 
the orthographic rule associated with p by 1. For 
example, the pair <?deni?, ?deny?> increases the 
frequency of the rule ?y:i? by 1 on condition that 
?deni? does not appear in any other pairs.  
(b) Filtering the rules. We first remove the infre-
quent rules, specifically those that are induced by 
less than 15% of the tuples in L  . Then we check 
whether there exists two rules of the form A:B and 
A:C in the induced rule list. If so, then we have a 
morphologically undesirable situation where the 
character A changes to B and C under the same 
environment (i.e.  ). To address this problem, we 
first calculate the strength of a rule as follows: 
=
@
@):(
):():(
Afrequency
BAfrequencyBAstrength  
We then retain only those rules whose fre-
quency*strength is greater than some predefined 
threshold. We denote the list of rules that satisfy 
the above constraints by R  . 
(c) Identifying valid allomorphs. For each rule in 
R  , we identify the associated <candidate allo-
                                                          
6
 This is the Chomsky and Halle notation for representing orthographic rules. a:b 
/ c _ d means a changes to b when the left context is c and the right context is d. 
160
morph, root> pairs in L  . We refer to the candidate 
allomorphs in each of those pairs as valid allo-
morphs and add them to the list of roots. We also 
remove from the original root list the words that 
can be segmented by the induced allomorphs and 
the associated rules (e.g. ?denial?). 
(d) Identifying composite suffixes. For each rule 
in R  , we also check whether it can identify com-
posite suffixes where the first component suffix?s 
last character is replaced during attachment to the 
second component suffix (e.g.  ?liness? = 
?ly?+?ness?). Specifically, if (1) A:B / _   is a rule 
in R  , (2)   A  (say ?liness?),   (say ?ness?) and   B 
(say ?ly?) are induced suffixes, and (3)   A  satis-
fies the requirements of a composite suffix (see 
Section 3.2), then we determine that   A  is a com-
posite suffix composed of   B and 

. 
We employ the same procedure for learning in-
sertion and deletion rules, except that strength is 
always set to 1 for these two types of rules. The 
threshold we set at step (b) is somewhat dependent 
on the vocabulary size, since the frequency count 
of each rule will naturally be larger when a larger 
vocabulary is used. Following our method for set-
ting vocabulary-dependent thresholds (see Section 
3.1), we set the threshold to 4 for English and 25 
for Finnish, for instance. 
Finally, we adapt our candidate allomorph de-
tection method described above to induce allo-
morphs that are generated through orthographic 
changes of edit distance greater than 1. Specifi-
cally, if  

 is a word in the induced root list (e.g. 
?stability?7, where   =?stabil? and 

=?ity?),   is an 
induced suffix, and the attachment of 

 to    is cor-
rect according to suffix level similarity, then we 
hypothesize that    (?stabil?) is a candidate allo-
morph. For each induced suffix, we use this hy-
pothesis to generate candidate allomorphs and 
consider as valid allomorphs only those that are 
generated from at least three different suffixes.8 
7 Word Segmentation 
After inducing the morphemes, we can use them to 
segment a word w in the test set. Specifically, we 
                                                          
7
 The correct segmentation of ?stability? is ?stable?+?ity?.  The ?stabil?-?stable? 
allomorph-root pair is an example of an orthographic change of edit distance 2. 
8
 This technique can also be used to induce out-of-vocabulary (OOV) roots. For 
example, the presence of ?perplexity?, ?perplexed? and ?perplexing? in a vo-
cabulary allows us to induce the root ?perplex?. OOV root induction is particu-
larly important for languages like Bengali, where verb roots mostly take the 
imperative form and hence are absent in a vocabulary created from a newspaper 
corpus, which normally comprises only the first and third person verb forms. 
(1) generate all possible segmentations of w using 
only the induced affixes and roots, and (2) apply a 
sequence of tests to remove candidate segmenta-
tions until we are left with only one candidate, 
which we take to be the final segmentation of w. 
Our first test involves removing any candidate 
segmentation m1m2 ? mn that violates any of the 
linguistic constraints below: 
? At least one of m1, m2, ?, mn is a root. 
? For 1 ? i < n, if mi is a prefix, then mi+1 must 
be a root or a prefix. 
? For 1 < i ? n, if mi is a suffix, then mi-1 must 
be a root or a suffix. 
? m1 can?t be a suffix and mn can?t be a prefix.  
Next, we apply our second test, in which we re-
tain only those candidate segmentations that have 
the smallest number of morphemes. For example, 
if ?friendly? has two candidate segmentations 
?friend?+?ly? and ?fri?+?end?+?ly?, we will select 
the first one to be the segmentation of w. 
If more than one candidate segmentation still ex-
ists, we score each remaining candidate using the 
heuristic below, selecting the highest-scoring can-
didate to be the final segmentation of w.  Basically, 
we score each candidate segmentation by adding 
the strength of each morpheme in the segmenta-
tion, where (1) the strength of an affix is the num-
ber of distinct words in the vocabulary to which 
the affix attaches, multiplied by the length of the 
affix, and (2) the strength of a root is the number of 
distinct morphemes with which the root combines, 
again multiplied by the length of the root. 
8 Evaluation 
In this section, we will first evaluate our segmenta-
tion algorithm for English and Bengali, and then 
examine its performance on the PASCAL datasets. 
8.1 Experimental Setup 
Vocabulary creation. We extracted our English 
vocabulary from the Wall Street Journal corpus of 
the Penn Treebank and the BLLIP corpus, preproc-
essing the documents by first tokenizing them and 
then removing capitalized words, punctuations and 
numbers. In addition, we removed words of fre-
quency 1 from BLLIP, because many of them are 
proper nouns and misspelled words. The final Eng-
lish vocabulary consists of approximately 60K dis-
tinct words. We applied the same pre-processing 
161
steps to five years of articles taken from the Ben-
gali newspaper Prothom Alo to generate our Ben-
gali vocabulary, which consists of 140K words. 
Test set preparation. To create our English test 
set, we randomly chose 000 words from our vo-
cabulary that are at least 4-character long9 and also 
appear in CELEX. Although 95% of the time we 
adopted the segmentation proposed by CELEX, in 
some cases the CELEX segmentations are errone-
ous (e.g. ?rolling? and ?lodging? remain unseg-
mented in CELEX). As a result, we cross-check 
with the online version of Merriam-Webster to 
make the necessary changes. To create the Bengali 
test set, we randomly chose 5000 words from our 
vocabulary and manually removed proper nouns 
and misspelled words from the set before giving it 
to two of our linguists for hand-segmentation. The 
final test set contains 4191 words. 
Evaluation metrics.  We use two standard metrics 
-- exact accuracy and F-score -- to evaluate the 
performance of our segmentation algorithm on the 
test sets. Exact accuracy is the percentage of the 
words whose proposed segmentation is identical to 
the correct segmentation. F-score is the harmonic 
mean of recall and precision, which are computed 
based on the placement of morpheme boundaries.10   
8.2 Results for English and Bengali 
The baseline systems. We use two publicly avail-
able and widely used unsupervised morphological 
learning systems -- Goldsmith?s (2001) Linguis-
tica11 and Creutz and Lagus?s (2005) Morphessor 
1.012 -- as our baseline systems. The first two rows 
of Table 3 show the results of these systems for our 
test sets (with all the training parameters set to 
their default values). As we can see, Linguistica 
performs substantially better for English in terms 
of both exact accuracy and F-score, whereas Mor-
phessor outperforms Linguistica for Bengali.   
Our segmentation algorithm. Results of our 
segmentation algorithm are shown in rows 3-6 of 
Table 3. Specifically, row 3 shows the results of 
our basic segmentation system as described in Sec-
tion 3. Rows 4-6 show the results where our three 
techniques (i.e. relative frequency, suffix level 
                                                          
9
 Words of length less than 4 do not have any morphological segmentation in 
English. Hence, by imposing this length restriction on the words in our test set, 
we effectively make the evaluation more challenging. This is also the reason for 
our using words that are at least 3-character long in the Bengali test set.  
10
 See http://www.cis.hut.fi/morphochallenge2005/evaluation.shtml for details. 
11
 http://humanities.uchicago.edu/faculty/goldsmith/Linguistica2000/ 
12
 http://www.cis.hut.fi/projects/morpho/ 
similarity and allomorph detection) are incorpo-
rated into the basic system one after the other. It is 
worth mentioning that (1) our basic algorithm al-
ready outperforms the baseline systems in terms of 
both exact accuracy and F-score; and (2) while 
each of our additions to the basic algorithm boosts 
system performance, relative corpus frequency and 
allomorph detection contribute to performance im-
provements particularly significantly. As we can 
see, the best segmentation performance is achieved 
when all of our three additions are applied to the 
basic algorithm.  
 
 English 
 
Bengali 
 A P R F A P R F 
Linguistica 68.9 
 
84.8 75.7 80.0 36.3 58.2 63.3 60.6 
Morphessor 64.9 
 
69.6 85.3 76.6 56.5 89.7 67.4 76.9 
Basic in-
duction 
68.1 79.4 82.8 81.1 57.7 79.6 81.2 80.4 
Relative 
frequency 
74.0 86.4 82.5 84.4 63.2 85.6 79.9 82.7 
Suffix level 
similarity 
74.9 88.6 82.3 85.3 66.1 89.7 78.8 83.9 
Allomorph 
detection 
78.3 88.3 86.4 87.4 68.3 89.3 81.3 85.1 
Table 3: Results (reported in terms of exact accu-
racy (A), precision (P), recall (R) and F-score (F)) 
8.3 PASCAL Challenge Results 
To get an idea of how our algorithm performs in 
comparison to the PASCAL participants, we con-
ducted evaluations on the PASCAL datasets for 
English, Finnish and Turkish. Table 4 shows the F-
scores of four segmentation algorithms for these 
three datasets: the best-performing PASCAL sys-
tem (Winner), Morphessor, our system that uses 
the basic morpheme induction algorithm (Basic), 
and our system with all three extensions incorpo-
rated (Complete). Below we discuss these results. 
English. There are 533 test cases in this dataset. 
Using the vocabulary created as described in Sec-
tion 8.1, our Complete algorithm achieves an F-
score of 79.4%, which outperforms the winner 
(Keshava and Pitler, 2006) by 2.6%. Although our 
basic morpheme induction algorithm is similar to 
that of Keshava and Pitler, a closer examination of 
the results reveals that F-score increases signifi-
cantly with the incorporation of relative frequency 
and allomorph detection. 
Finnish and Turkish. The real challenge in the 
PASCAL Challenge is the evaluation on Finnish 
162
and Turkish due to their morphological richness. 
We use the 400K and 300K most frequent words 
from the Finnish and Turkish datasets provided by 
the organizers as our vocabulary. When tested on 
the gold standard of 661 Finnish and 775 Turkish 
words, our Complete system achieves F-scores of 
65.2% and 66.2%, which are better than the win-
ner?s scores (Bernhard (2006)). In addition, Com-
plete outperforms Basic by 3-6% in F-score; these 
results suggest that the new techniques proposed in 
this paper (especially allomorph detection) are also 
very effective for Finnish and Turkish. 
 
 English Finnish Turkish 
Winner 76.8 64.7 65.3 
Morphessor 66.2 66.4 70.1 
Basic 75.8 59.2 63.4 
Complete 79.4 65.2 66.2 
Table 4: F-scores for the PASCAL gold standards 
 
   As mentioned in the introduction, none of the 
participating PASCAL systems offers robust per-
formance across different languages. For instance, 
Keshava and Pitler?s algorithm, the winner for 
English, has F-scores of only 47% and 54% for 
Finnish and Turkish respectively, whereas Bern-
hard?s algorithm, the winner for Finnish and Turk-
ish, achieves an F-score of only 66% for English. 
On the other hand, our algorithm outperforms the 
winners for all the languages in the competition, 
demonstrating its robustness across languages.  
Finally, although Morphessor achieves better re-
sults for Turkish and Finnish than our Complete 
system, it performs poorly for English, having an 
F-score of only 66.2%. On the other hand, our re-
sults for Finnish and Turkish are not significantly 
poorer than those of Morphessor. 
9 Conclusions 
We have presented an unsupervised word segmen-
tation algorithm that offers robust performance 
across languages with different levels of morpho-
logical complexity. Our algorithm not only outper-
forms Linguistica and Morphessor for English and 
Bengali, but also compares favorably to the best-
performing PASCAL morphological parsers when 
evaluated against all three target languages --
English, Turkish, and Finnish -- in the Challenge. 
Experimental results indicate that the use of rela-
tive corpus frequency for incorrect attachment de-
tection and the induction of orthographic rules and 
allomorphs have contributed to the performance of 
our algorithm particularly significantly. 
References  
R. H. Baayen, R. Piepenbrock and L. Gulikers. 1996. The 
CELEX2 lexical database (CD-ROM), LDC, Univ of 
Pennsylvania, Philadephia, PA. 
D. Bernhard. 2006. Unsupervised morphological segmentation 
based on segment predictability and word segment align-
ment. In PASCAL Challenge Workshop on Unsupervised 
Segmentation of Words into Morphemes. 
M. R. Brent, S. K. Murthy and A. Lundberg. 1995. Discover-
ing morphemic suffixes: A case study in minimum descrip-
tion length induction. In Proceedings of the Fifth 
International Workshop on AI and Statistics. 
M. Creutz. 2003. Unsupervised segmentation of words using 
prior distributions of morph length and frequency. In Pro-
ceedings of the ACL, pages 280-287. 
M. Creutz and K. Lagus. 2005. Unsupervised morpheme seg-
mentation and morphology induction from text corpora us-
ing Morfessor 1.0. In Computer and Information Science, 
Report A81, Helsinki University of Technology. 
S. Dasgupta and V. Ng. 2007. Unsupervised word segmenta-
tion for Bangla. In Proceedings of ICON, pages 15-24. 
H. D?Jean. 1998. Morphemes as necessary concepts for struc-
tures discovery from untagged corpora. In Workshop on 
Paradigms and Grounding in Natural Language Learning, 
pages 295-299. 
D. Freitag. 2005. Morphology induction from term clusters. In 
Proceedings of CoNLL, pages 128-135. 
J. Goldsmith. 2001. Unsupervised learning of the morphology 
of a natural language. In Computational Linguistics 27(2), 
pages 153-198. 
Z. Harris. 1955. From phoneme to morpheme. In Language, 
31(2): 190-222.  
S. Keshava and E. Pitler. 2006. A simpler, intuitive approach 
to morpheme induction. In PASCAL Challenge Workshop 
on Unsupervised Segmentation of Words into Morphemes. 
K. Koskenniemi. 1983. Two-level morphology: a general 
computational model for word-form recognition and pro-
duction. Publication No. 11. Helsinki: University of Hel-
sinki Department of General Linguistics. 
P. Schone and D. Jurafsky. 2001. Knowledge-free induction of 
inflectional morphologies. In Proceedings of the NAACL, 
pages 183-191. 
M. G. Snover and M. R. Brent. 2001. A Bayesian model for 
morpheme and paradigm identification. In Proceedings of 
the ACL, pages 482-490. 
D. Yarowsky and R. Wicentowski. 2000. Minimally super-
vised morphological analysis by multimodal alignment. In 
Proceedings of the ACL, pages 207-216. 
163
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611?618,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Examining the Role of Linguistic Knowledge Sources in the Automatic
Identification and Classification of Reviews
Vincent Ng and Sajib Dasgupta and S. M. Niaz Arifin
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{vince,sajib,arif}@hlt.utdallas.edu
Abstract
This paper examines two problems in
document-level sentiment analysis: (1) de-
termining whether a given document is a
review or not, and (2) classifying the po-
larity of a review as positive or negative.
We first demonstrate that review identifi-
cation can be performed with high accu-
racy using only unigrams as features. We
then examine the role of four types of sim-
ple linguistic knowledge sources in a po-
larity classification system.
1 Introduction
Sentiment analysis involves the identification of
positive and negative opinions from a text seg-
ment. The task has recently received a lot of
attention, with applications ranging from multi-
perspective question-answering (e.g., Cardie et al
(2004)) to opinion-oriented information extraction
(e.g., Riloff et al (2005)) and summarization (e.g.,
Hu and Liu (2004)). Research in sentiment analy-
sis has generally proceeded at three levels, aim-
ing to identify and classify opinions from doc-
uments, sentences, and phrases. This paper ex-
amines two problems in document-level sentiment
analysis, focusing on analyzing a particular type
of opinionated documents: reviews.
The first problem, polarity classification, has
the goal of determining a review?s polarity ? pos-
itive (?thumbs up?) or negative (?thumbs down?).
Recent work has expanded the polarity classifi-
cation task to additionally handle documents ex-
pressing a neutral sentiment. Although studied
fairly extensively, polarity classification remains a
challenge to natural language processing systems.
We will focus on an important linguistic aspect
of polarity classification: examining the role of a
variety of simple, yet under-investigated, linguis-
tic knowledge sources in a learning-based polarity
classification system. Specifically, we will show
how to build a high-performing polarity classifier
by exploiting information provided by (1) high or-
der n-grams, (2) a lexicon composed of adjectives
manually annotated with their polarity information
(e.g., happy is annotated as positive and terrible as
negative), (3) dependency relations derived from
dependency parses, and (4) objective terms and
phrases extracted from neutral documents.
As mentioned above, the majority of work on
document-level sentiment analysis to date has fo-
cused on polarity classification, assuming as in-
put a set of reviews to be classified. A relevant
question is: what if we don?t know that an input
document is a review in the first place? The sec-
ond task we will examine in this paper ? review
identification ? attempts to address this question.
Specifically, review identification seeks to deter-
mine whether a given document is a review or not.
We view both review identification and polar-
ity classification as a classification task. For re-
view identification, we train a classifier to dis-
tinguish movie reviews and movie-related non-
reviews (e.g., movie ads, plot summaries) using
only unigrams as features, obtaining an accuracy
of over 99% via 10-fold cross-validation. Simi-
lar experiments using documents from the book
domain also yield an accuracy as high as 97%.
An analysis of the results reveals that the high ac-
curacy can be attributed to the difference in the
vocabulary employed in reviews and non-reviews:
while reviews can be composed of a mixture of
subjective and objective language, our non-review
documents rarely contain subjective expressions.
Next, we learn our polarity classifier using pos-
itive and negative reviews taken from two movie
611
review datasets, one assembled by Pang and Lee
(2004) and the other by ourselves. The result-
ing classifier, when trained on a feature set de-
rived from the four types of linguistic knowl-
edge sources mentioned above, achieves a 10-fold
cross-validation accuracy of 90.5% and 86.1% on
Pang et al?s dataset and ours, respectively. To our
knowledge, our result on Pang et al?s dataset is
one of the best reported to date. Perhaps more im-
portantly, an analysis of these results show that the
various types of features interact in an interesting
manner, allowing us to draw conclusions that pro-
vide new insights into polarity classification.
2 Related Work
2.1 Review Identification
As noted in the introduction, while a review can
contain both subjective and objective phrases, our
non-reviews are essentially factual documents in
which subjective expressions can rarely be found.
Hence, review identification can be viewed as an
instance of the broader task of classifying whether
a document is mostly factual/objective or mostly
opinionated/subjective. There have been attempts
on tackling this so-called document-level subjec-
tivity classification task, with very encouraging
results (see Yu and Hatzivassiloglou (2003) and
Wiebe et al (2004) for details).
2.2 Polarity Classification
There is a large body of work on classifying the
polarity of a document (e.g., Pang et al (2002),
Turney (2002)), a sentence (e.g., Liu et al (2003),
Yu and Hatzivassiloglou (2003), Kim and Hovy
(2004), Gamon et al (2005)), a phrase (e.g., Wil-
son et al (2005)), and a specific object (such as a
product) mentioned in a document (e.g., Morinaga
et al (2002), Yi et al (2003), Popescu and Etzioni
(2005)). Below we will center our discussion of
related work around the four types of features we
will explore for polarity classification.
Higher-order n-grams. While n-grams offer a
simple way of capturing context, previous work
has rarely explored the use of n-grams as fea-
tures in a polarity classification system beyond un-
igrams. Two notable exceptions are the work of
Dave et al (2003) and Pang et al (2002). Interest-
ingly, while Dave et al report good performance
on classifying reviews using bigrams or trigrams
alone, Pang et al show that bigrams are not use-
ful features for the task, whether they are used in
isolation or in conjunction with unigrams. This
motivates us to take a closer look at the utility of
higher-order n-grams in polarity classification.
Manually-tagged term polarity. Much work has
been performed on learning to identify and clas-
sify polarity terms (i.e., terms expressing a pos-
itive sentiment (e.g., happy) or a negative senti-
ment (e.g., terrible)) and exploiting them to do
polarity classification (e.g., Hatzivassiloglou and
McKeown (1997), Turney (2002), Kim and Hovy
(2004), Whitelaw et al (2005), Esuli and Se-
bastiani (2005)). Though reasonably successful,
these (semi-)automatic techniques often yield lex-
icons that have either high coverage/low precision
or low coverage/high precision. While manually
constructed positive and negative word lists exist
(e.g., General Inquirer1), they too suffer from the
problem of having low coverage. This prompts us
to manually construct our own polarity word lists2
and study their use in polarity classification.
Dependency relations. There have been several
attempts at extracting features for polarity classi-
fication from dependency parses, but most focus
on extracting specific types of information such as
adjective-noun relations (e.g., Dave et al (2003),
Yi et al (2003)) or nouns that enjoy a dependency
relation with a polarity term (e.g., Popescu and Et-
zioni (2005)). Wilson et al (2005) extract a larger
variety of features from dependency parses, but
unlike us, their goal is to determine the polarity of
a phrase, not a document. In comparison to previ-
ous work, we investigate the use of a larger set of
dependency relations for classifying reviews.
Objective information. The objective portions
of a review do not contain the author?s opinion;
hence features extracted from objective sentences
and phrases are irrelevant with respect to the po-
larity classification task and their presence may
complicate the learning task. Indeed, recent work
has shown that benefits can be made by first sepa-
rating facts from opinions in a document (e.g, Yu
and Hatzivassiloglou (2003)) and classifying the
polarity based solely on the subjective portions of
the document (e.g., Pang and Lee (2004)). Moti-
vated by the work of Koppel and Schler (2005), we
identify and extract objective material from non-
reviews and show how to exploit such information
in polarity classification.
1http://www.wjh.harvard.edu/?inquirer/
spreadsheet guid.htm
2Wilson et al (2005) have also manually tagged a list of
terms with their polarity, but this list is not publicly available.
612
Finally, previous work has also investigated fea-
tures that do not fall into any of the above cate-
gories. For instance, instead of representing the
polarity of a term using a binary value, Mullen
and Collier (2004) use Turney?s (2002) method to
assign a real value to represent term polarity and
introduce a variety of numerical features that are
aggregate measures of the polarity values of terms
selected from the document under consideration.
3 Review Identification
Recall that the goal of review identification is
to determine whether a given document is a re-
view or not. Given this definition, two immediate
questions come to mind. First, should this prob-
lem be addressed in a domain-specific or domain-
independent manner? In other words, should a re-
view identification system take as input documents
coming from the same domain or not?
Apparently this is a design question with no
definite answer, but our decision is to perform
domain-specific review identification. The reason
is that the primary motivation of review identifi-
cation is the need to identify reviews for further
analysis by a polarity classification system. Since
polarity classification has almost exclusively been
addressed in a domain-specific fashion, it seems
natural that its immediate upstream component ?
review identification ? should also assume do-
main specificity. Note, however, that assuming
domain specificity is not a self-imposed limita-
tion. In fact, we envision that the review identifica-
tion system will have as its upstream component a
text classification system, which will classify doc-
uments by topic and pass to the review identifier
only those documents that fall within its domain.
Given our choice of domain specificity, the next
question is: which documents are non-reviews?
Here, we adopt a simple and natural definition:
a non-review is any document that belongs to the
given domain but is not a review.
Dataset. Now, recall from the introduction that
we cast review identification as a classification
task. To train and test our review identifier, we
use 2000 reviews and 2000 non-reviews from the
movie domain. The 2000 reviews are taken from
Pang et al?s polarity dataset (version 2.0)3, which
consists of an equal number of positive and neg-
ative reviews. We collect the non-reviews for the
3Available from http://www.cs.cornell.edu/
people/pabo/movie-review-data.
movie domain from the Internet Movie Database
website4, randomly selecting any documents from
this site that are on the movie topic but are not re-
views themselves. With this criterion in mind, the
2000 non-review documents we end up with are
either movie ads or plot summaries.
Training and testing the review identifier. We
perform 10-fold cross-validation (CV) experi-
ments on the above dataset, using Joachims?
(1999) SVMlight package5 to train an SVM clas-
sifier for distinguishing reviews and non-reviews.
All learning parameters are set to their default
values.6 Each document is first tokenized and
downcased, and then represented as a vector of
unigrams with length normalization.7 Following
Pang et al (2002), we use frequency as presence.
In other words, the ith element of the document
vector is 1 if the corresponding unigram is present
in the document and 0 otherwise. The resulting
classifier achieves an accuracy of 99.8%.
Classifying neutral reviews and non-reviews.
Admittedly, the high accuracy achieved using such
a simple set of features is somewhat surpris-
ing, although it is consistent with previous re-
sults on document-level subjectivity classification
in which accuracies of 94-97% were obtained (Yu
and Hatzivassiloglou, 2003; Wiebe et al, 2004).
Before concluding that review classification is an
easy task, we conduct an additional experiment:
we train a review identifier on a new dataset where
we keep the same 2000 non-reviews but replace
the positive/negative reviews with 2000 neutral re-
views (i.e., reviews with a mediocre rating). In-
tuitively, a neutral review contains fewer terms
with strong polarity than a positive/negative re-
view. Hence, this additional experiment would al-
low us to investigate whether the lack of strong
polarized terms in neutral reviews would increase
the difficulty of the learning task.
Our neutral reviews are randomly chosen from
Pang et al?s pool of 27886 unprocessed movie re-
views8 that have either a rating of 2 (on a 4-point
scale) or 2.5 (on a 5-point scale). Each review then
undergoes a semi-automatic preprocessing stage
4See http://www.imdb.com.
5Available from svmlight.joachims.org.
6We tried polynomial and RBF kernels, but none yields
better performance than the default linear kernel.
7We observed that not performing length normalization
hurts performance slightly.
8Also available from Pang?s website. See Footnote 3.
613
where (1) HTML tags and any header and trailer
information (such as date and author identity) are
removed; (2) the document is tokenized and down-
cased; (3) the rating information extracted by reg-
ular expressions is removed; and (4) the document
is manually checked to ensure that the rating infor-
mation is successfully removed. When trained on
this new dataset, the review identifier also achieves
an accuracy of 99.8%, suggesting that this learning
task isn?t any harder in comparison to the previous
one.
Discussion. We hypothesized that the high accu-
racies are attributable to the different vocabulary
used in reviews and non-reviews. As part of our
verification of this hypothesis, we plot the learn-
ing curve for each of the above experiments.9 We
observe that a 99% accuracy was achieved in all
cases even when only 200 training instances are
used to acquire the review identifier. The abil-
ity to separate the two classes with such a small
amount of training data seems to imply that fea-
tures strongly indicative of one or both classes are
present. To test this hypothesis, we examine the
?informative? features for both classes. To get
these informative features, we rank the features by
their weighted log-likelihood ratio (WLLR)10:
P (wt|cj) log
P (wt|cj)
P (wt|?cj)
,
where wt and cj denote the tth word in the vocab-ulary and the jth class, respectively. Informally,
a feature (in our case a unigram) w will have a
high rank with respect to a class c if it appears fre-
quently in c and infrequently in other classes. This
correlates reasonably well with what we think an
informative feature should be. A closer examina-
tion of the feature lists sorted by WLLR confirms
our hypothesis that each of the two classes has its
own set of distinguishing features.
Experiments with the book domain. To under-
stand whether these good review identification re-
sults only hold true for the movie domain, we
conduct similar experiments with book reviews
and non-reviews. Specifically, we collect 1000
book reviews (consisting of a mixture of positive,
negative, and neutral reviews) from the Barnes
9The curves are not shown due to space limitations.
10Nigam et al (2000) show that this metric is effec-
tive at selecting good features for text classification. Other
commonly-used feature selection metrics are discussed in
Yang and Pedersen (1997).
and Noble website11, and 1000 non-reviews that
are on the book topic (mostly book summaries)
from Amazon.12 We then perform 10-fold CV ex-
periments using these 2000 documents as before,
achieving a high accuracy of 96.8%. These results
seem to suggest that automatic review identifica-
tion can be achieved with high accuracy.
4 Polarity Classification
Compared to review identification, polarity classi-
fication appears to be a much harder task. This
section examines the role of various linguistic
knowledge sources in our learning-based polarity
classification system.
4.1 Experimental Setup
Like several previous work (e.g., Mullen and Col-
lier (2004), Pang and Lee (2004), Whitelaw et al
(2005)), we view polarity classification as a super-
vised learning task. As in review identification,
we use SVMlight with default parameter settings
to train polarity classifiers13 , reporting all results
as 10-fold CV accuracy.
We evaluate our polarity classifiers on two
movie review datasets, each of which consists of
1000 positive reviews and 1000 negative reviews.
The first one, which we will refer to as Dataset A,
is the Pang et al polarity dataset (version 2.0). The
second one (Dataset B) was created by us, with the
sole purpose of providing additional experimental
results. Reviews in Dataset B were randomly cho-
sen from Pang et al?s pool of 27886 unprocessed
movie reviews (see Section 3) that have either a
positive or a negative rating. We followed exactly
Pang et al?s guideline when determining whether
a review is positive or negative.14 Also, we took
care to ensure that reviews included in Dataset B
do not appear in Dataset A. We applied to these re-
views the same four pre-processing steps that we
did to the neutral reviews in the previous section.
4.2 Results
The baseline classifier. We can now train our
baseline polarity classifier on each of the two
11www.barnesandnoble.com
12www.amazon.com
13We also experimented with polynomial and RBF kernels
when training polarity classifiers, but neither yields better re-
sults than linear kernels.
14The guidelines come with their polarity dataset. Briefly,
a positive review has a rating of ? 3.5 (out of 5) or ? 3 (out
of 4), whereas a negative review has a rating of ? 2 (out of 5)
or ? 1.5 (out of 4).
614
System Variation Dataset A Dataset B
Baseline 87.1 82.7
Adding bigrams 89.2 84.7
and trigrams
Adding dependency 89.0 84.5
relations
Adding polarity 90.4 86.2
info of adjectives
Discarding objective 90.5 86.1
materials
Table 1: Polarity classification accuracies.
datasets. Our baseline classifier employs as fea-
tures the k highest-ranking unigrams according to
WLLR, with k/2 features selected from each class.
Results with k = 10000 are shown in row 1 of Ta-
ble 1.15 As we can see, the baseline achieves an
accuracy of 87.1% and 82.7% on Datasets A and
B, respectively. Note that our result on Dataset
A is as strong as that obtained by Pang and Lee
(2004) via their subjectivity summarization algo-
rithm, which retains only the subjective portions
of a document.
As a sanity check, we duplicated Pang et al?s
(2002) baseline in which all unigrams that appear
four or more times in the training documents are
used as features. The resulting classifier achieves
an accuracy of 87.2% and 82.7% for Datasets A
and B, respectively. Neither of these results are
significantly different from our baseline results.16
Adding higher-order n-grams. The negative
results that Pang et al (2002) obtained when us-
ing bigrams as features for their polarity classi-
fier seem to suggest that high-order n-grams are
not useful for polarity classification. However, re-
cent research in the related (but arguably simpler)
task of text classification shows that a bigram-
based text classifier outperforms its unigram-
based counterpart (Peng et al, 2003). This
prompts us to re-examine the utility of high-order
n-grams in polarity classification.
In our experiments we consider adding bigrams
and trigrams to our baseline feature set. However,
since these higher-order n-grams significantly out-
number the unigrams, adding all of them to the
feature set will dramatically increase the dimen-
15We experimented with several values of k and obtained
the best result with k = 10000.
16We use two-tailed paired t-tests when performing signif-
icance testing, with p set to 0.05 unless otherwise stated.
sionality of the feature space and may undermine
the impact of the unigrams in the resulting clas-
sifier. To avoid this potential problem, we keep
the number of unigrams and higher-order n-grams
equal. Specifically, we augment the baseline fea-
ture set (consisting of 10000 unigrams) with 5000
bigrams and 5000 trigrams. The bigrams and tri-
grams are selected based on their WLLR com-
puted over the positive reviews and negative re-
views in the training set for each CV run.
Results using this augmented feature set are
shown in row 2 of Table 1. We see that accu-
racy rises significantly from 87.1% to 89.2% for
Dataset A and from 82.7% to 84.7% for Dataset B.
This provides evidence that polarity classification
can indeed benefit from higher-order n-grams.
Adding dependency relations. While bigrams
and trigrams are good at capturing local dependen-
cies, dependency relations can be used to capture
non-local dependencies among the constituents of
a sentence. Hence, we hypothesized that our n-
gram-based polarity classifier would benefit from
the addition of dependency-based features.
Unlike most previous work on polarity classi-
fication, which has largely focused on exploiting
adjective-noun (AN) relations (e.g., Dave et al
(2003), Popescu and Etzioni (2005)), we hypothe-
sized that subject-verb (SV) and verb-object (VO)
relations would also be useful for the task. The
following (one-sentence) review illustrates why.
While I really like the actors, the plot is
rather uninteresting.
A unigram-based polarity classifier could be con-
fused by the simultaneous presence of the posi-
tive term like and the negative term uninteresting
when classifying this review. However, incorpo-
rating the VO relation (like, actors) as a feature
may allow the learner to learn that the author likes
the actors and not necessarily the movie.
In our experiments, the SV, VO and AN re-
lations are extracted from each document by the
MINIPAR dependency parser (Lin, 1998). As
with n-grams, instead of using all the SV, VO and
AN relations as features, we select among them
the best 5000 according to their WLLR and re-
train the polarity classifier with our n-gram-based
feature set augmented by these 5000 dependency-
based features. Results in row 3 of Table 1 are
somewhat surprising: the addition of dependency-
based features does not offer any improvements
over the simple n-gram-based classifier.
615
Incorporating manually tagged term polarity.
Next, we consider incorporating a set of features
that are computed based on the polarity of adjec-
tives. As noted before, we desire a high-precision,
high-coverage lexicon. So, instead of exploiting a
learned lexicon, we manually develop one.
To construct the lexicon, we take Pang et al?s
pool of unprocessed documents (see Section 3),
remove those that appear in either Dataset A or
Dataset B17, and compile a list of adjectives from
the remaining documents. Then, based on heuris-
tics proposed in psycholinguistics18 , we hand-
annotate each adjective with its prior polarity (i.e.,
polarity in the absence of context). Out of the
45592 adjectives we collected, 3599 were labeled
as positive, 3204 as negative, and 38789 as neu-
tral. A closer look at these adjectives reveals that
they are by no means domain-dependent despite
the fact that they were taken from movie reviews.
Now let us consider a simple procedure P for
deriving a feature set that incorporates information
from our lexicon: (1) collect all the bigrams from
the training set; (2) for each bigram that contains at
least one adjective labeled as positive or negative
according to our lexicon, create a new feature that
is identical to the bigram except that each adjec-
tive is replaced with its polarity label19; (3) merge
the list of newly generated features with the list
of bigrams20 and select the top 5000 features from
the merged list according to their WLLR.
We then repeat procedure P for the trigrams
and also the dependency features, resulting in a
total of 15000 features. Our new feature set com-
prises these 15000 features as well as the 10000
unigrams we used in the previous experiments.
Results of the polarity classifier that incorpo-
rates term polarity information are encouraging
(see row 4 of Table 1). In comparison to the classi-
fier that uses only n-grams and dependency-based
features (row 3), accuracy increases significantly
(p = .1) from 89.2% to 90.4% for Dataset A, and
from 84.7% to 86.2% for Dataset B. These results
suggest that the classifier has benefited from the
17We treat the test documents as unseen data that should
not be accessed for any purpose during system development.
18http://www.sci.sdsu.edu/CAL/wordlist
19Neutral adjectives are not replaced.
20A newly generated feature could be misleading for the
learner if the contextual polarity (i.e., polarity in the presence
of context) of the adjective involved differs from its prior po-
larity (see Wilson et al (2005)). The motivation behind merg-
ing with the bigrams is to create a feature set that is more
robust in the face of potentially misleading generalizations.
use of features that are less sparse than n-grams.
Using objective information. Some of the
25000 features we generated above correspond to
n-grams or dependency relations that do not con-
tain subjective information. We hypothesized that
not employing these ?objective? features in the
feature set would improve system performance.
More specifically, our goal is to use procedure P
again to generate 25000 ?subjective? features by
ensuring that the objective ones are not selected
for incorporation into our feature set.
To achieve this goal, we first use the following
rote-learning procedure to identify objective ma-
terial: (1) extract all unigrams that appear in ob-
jective documents, which in our case are the 2000
non-reviews used in review identification [see Sec-
tion 3]; (2) from these ?objective? unigrams, we
take the best 20000 according to their WLLR com-
puted over the non-reviews and the reviews in the
training set for each CV run; (3) repeat steps 1 and
2 separately for bigrams, trigrams and dependency
relations; (4) merge these four lists to create our
80000-element list of objective material.
Now, we can employ procedure P to get a list of
25000 ?subjective? features by ensuring that those
that appear in our 80000-element list are not se-
lected for incorporation into our feature set.
Results of our classifier trained using these sub-
jective features are shown in row 5 of Table 1.
Somewhat surprisingly, in comparison to row 4,
we see that our method for filtering objective fea-
tures does not help improve performance on the
two datasets. We will examine the reasons in the
following subsection.
4.3 Discussion and Further Analysis
Using the four types of knowledge sources pre-
viously described, our polarity classifier signifi-
cantly outperforms a unigram-based baseline clas-
sifier. In this subsection, we analyze some of these
results and conduct additional experiments in an
attempt to gain further insight into the polarity
classification task. Due to space limitations, we
will simply present results on Dataset A below,
and show results on Dataset B only in cases where
a different trend is observed.
The role of feature selection. In all of our ex-
periments we used the best k features obtained via
WLLR. An interesting question is: how will these
results change if we do not perform feature selec-
tion? To investigate this question, we conduct two
616
experiments. First, we train a polarity classifier us-
ing all unigrams from the training set. Second, we
train another polarity classifier using all unigrams,
bigrams, and trigrams. We obtain an accuracy of
87.2% and 79.5% for the first and second experi-
ments, respectively.
In comparison to our baseline classifier, which
achieves an accuracy of 87.1%, we can see that
using all unigrams does not hurt performance, but
performance drops abruptly with the addition of
all bigrams and trigrams. These results suggest
that feature selection is critical when bigrams and
trigrams are used in conjunction with unigrams for
training a polarity classifier.
The role of bigrams and trigrams. So far we
have seen that training a polarity classifier using
only unigrams gives us reasonably good, though
not outstanding, results. Our question, then, is:
would bigrams alone do a better job at capturing
the sentiment of a document than unigrams? To
answer this question, we train a classifier using all
bigrams (without feature selection) and obtain an
accuracy of 83.6%, which is significantly worse
than that of a unigram-only classifier. Similar re-
sults were also obtained by Pang et al (2002).
It is possible that the worse result is due to the
presence of a large number of irrelevant bigrams.
To test this hypothesis, we repeat the above exper-
iment except that we only use the best 10000 bi-
grams selected according to WLLR. Interestingly,
the resulting classifier gives us a lower accuracy
of 82.3%, suggesting that the poor accuracy is not
due to the presence of irrelevant bigrams.
To understand why using bigrams alone does
not yield a good classification model, we examine
a number of test documents and find that the fea-
ture vectors corresponding to some of these docu-
ments (particularly the short ones) have all zeroes
in them. In other words, none of the bigrams from
the training set appears in these reviews. This sug-
gests that the main problem with the bigram model
is likely to be data sparseness. Additional experi-
ments show that the trigram-only classifier yields
even worse results than the bigram-only classifier,
probably because of the same reason.
Nevertheless, these higher-order n-grams play a
non-trivial role in polarity classification: we have
shown that the addition of bigrams and trigrams
selected via WLLR to a unigram-based classifier
significantly improves its performance.
The role of dependency relations. In the previ-
ous subsection we see that dependency relations
do not contribute to overall performance on top
of bigrams and trigrams. There are two plausi-
ble reasons. First, dependency relations are simply
not useful for polarity classification. Second, the
higher-order n-grams and the dependency-based
features capture essentially the same information
and so using either of them would be sufficient.
To test the first hypothesis, we train a clas-
sifier using only 10000 unigrams and 10000
dependency-based features (both selected accord-
ing to WLLR). For Dataset A, the classifier
achieves an accuracy of 87.1%, which is statis-
tically indistinguishable from our baseline result.
On the other hand, the accuracy for Dataset B is
83.5%, which is significantly better than the cor-
responding baseline (82.7%) at the p = .1 level.
These results indicate that dependency informa-
tion is somewhat useful for the task when bigrams
and trigrams are not used. So the first hypothesis
is not entirely true.
So, it seems to be the case that the dependency
relations do not provide useful knowledge for po-
larity classification only in the presence of bigrams
and trigrams. This is somewhat surprising, since
these n-grams do not capture the non-local depen-
dencies (such as those that may be present in cer-
tain SV or VO relations) that should intuitively be
useful for polarity classification.
To better understand this issue, we again exam-
ine a number of test documents. Our initial in-
vestigation suggests that the problem might have
stemmed from the fact that MINIPAR returns de-
pendency relations in which all the verb inflections
are removed. For instance, given the sentence My
cousin Paul really likes this long movie, MINIPAR
will return the VO relation (like, movie). To see
why this can be a problem, consider another sen-
tence I like this long movie. From this sentence,
MINIPAR will also extract the VO relation (like,
movie). Hence, this same VO relation is cap-
turing two different situations, one in which the
author himself likes the movie, and in the other,
the author?s cousin likes the movie. The over-
generalization resulting from these ?stemmed? re-
lations renders dependency information not useful
for polarity classification. Additional experiments
are needed to determine the role of dependency re-
lations when stemming in MINIPAR is disabled.
617
The role of objective information. Results
from the previous subsection suggest that our
method for extracting objective materials and re-
moving them from the reviews is not effective in
terms of improving performance. To determine the
reason, we examine the n-grams and the depen-
dency relations that are extracted from the non-
reviews. We find that only in a few cases do these
extracted objective materials appear in our set of
25000 features obtained in Section 4.2. This ex-
plains why our method is not as effective as we
originally thought. We conjecture that more so-
phisticated methods would be needed in order to
take advantage of objective information in polar-
ity classification (e.g., Koppel and Schler (2005)).
5 Conclusions
We have examined two problems in document-
level sentiment analysis, namely, review identifi-
cation and polarity classification. We first found
that review identification can be achieved with
very high accuracies (97-99%) simply by training
an SVM classifier using unigrams as features. We
then examined the role of several linguistic knowl-
edge sources in polarity classification. Our re-
sults suggested that bigrams and trigrams selected
according to the weighted log-likelihood ratio as
well as manually tagged term polarity informa-
tion are very useful features for the task. On the
other hand, no further performance gains are ob-
tained by incorporating dependency-based infor-
mation or filtering objective materials from the re-
views using our proposed method. Nevertheless,
the resulting polarity classifier compares favorably
to state-of-the-art sentiment classification systems.
References
C. Cardie, J. Wiebe, T. Wilson, and D. Litman. 2004. Low-
level annotations and summary representations of opin-
ions for multi-perspective question answering. In New Di-
rections in Question Answering. AAAI Press/MIT Press.
K. Dave, S. Lawrence, and D. M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. In Proc. of WWW, pages
519?528.
A. Esuli and F. Sebastiani. 2005. Determining the semantic
orientation of terms through gloss classification. In Proc.
of CIKM, pages 617?624.
M. Gamon, A. Aue, S. Corston-Oliver, and E. K. Ringger.
2005. Pulse: Mining customer opinions from free text.
In Proc. of the 6th International Symposium on Intelligent
Data Analysis, pages 121?132.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In Proc. of the
ACL/EACL, pages 174?181.
M. Hu and B. Liu. 2004. Mining and summarizing customer
reviews. In Proc. of KDD, pages 168?177.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In Advances in Kernel Methods - Support Vector
Learning, pages 44?56. MIT Press.
S.-M. Kim and E. Hovy. 2004. Determining the sentiment of
opinions. In Proc. of COLING, pages 1367?1373.
M. Koppel and J. Schler. 2005. Using neutral examples for
learning polarity. In Proc. of IJCAI (poster).
D. Lin. 1998. Dependency-based evaluation of MINIPAR.
In Proc. of the LREC Workshop on the Evaluation of Pars-
ing Systems, pages 48?56.
H. Liu, H. Lieberman, and T. Selker. 2003. A model of tex-
tual affect sensing using real-world knowledge. In Proc.
of Intelligent User Interfaces (IUI), pages 125?132.
S. Morinaga, K. Yamanishi, K. Tateishi, and T. Fukushima.
2002. Mining product reputations on the web. In Proc. of
KDD, pages 341?349.
T. Mullen and N. Collier. 2004. Sentiment analysis using
support vector machines with diverse information sources.
In Proc. of EMNLP, pages 412?418.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000.
Text classification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103?134.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proc. of the ACL, pages 271?278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proc. of EMNLP, pages 79?86.
F. Peng, D. Schuurmans, and S. Wang. 2003. Language and
task independent text categorization with simple language
models. In HLT/NAACL: Main Proc. , pages 189?196.
A.-M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proc. of HLT-
EMNLP, pages 339?346.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proc. of AAAI, pages 1106?1111.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proc. of the ACL, pages 417?424.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proc. of CIKM,
pages 625?631.
J. M. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational Lin-
guistics, 30(3):277?308.
T. Wilson, J. M. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment analysis.
In Proc. of EMNLP, pages 347?354.
Y. Yang and J. O. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Proc. of ICML,
pages 412?420.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a given
topic using natural language processing techniques. In
Proc. of the IEEE International Conference on Data Min-
ing (ICDM).
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proc. of
EMNLP, pages 129?136.
618
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701?709,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Mine the Easy, Classify the Hard:
A Semi-Supervised Approach to Automatic Sentiment Classification
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
Abstract
Supervised polarity classification systems
are typically domain-specific. Building
these systems involves the expensive pro-
cess of annotating a large amount of data
for each domain. A potential solution
to this corpus annotation bottleneck is to
build unsupervised polarity classification
systems. However, unsupervised learning
of polarity is difficult, owing in part to the
prevalence of sentimentally ambiguous re-
views, where reviewers discuss both the
positive and negative aspects of a prod-
uct. To address this problem, we pro-
pose a semi-supervised approach to senti-
ment classification where we first mine the
unambiguous reviews using spectral tech-
niques and then exploit them to classify
the ambiguous reviews via a novel com-
bination of active learning, transductive
learning, and ensemble learning.
1 Introduction
Sentiment analysis has recently received a lot
of attention in the Natural Language Processing
(NLP) community. Polarity classification, whose
goal is to determine whether the sentiment ex-
pressed in a document is ?thumbs up? or ?thumbs
down?, is arguably one of the most popular tasks
in document-level sentiment analysis. Unlike
topic-based text classification, where a high accu-
racy can be achieved even for datasets with a large
number of classes (e.g., 20 Newsgroups), polarity
classification appears to be a more difficult task.
One reason topic-based text classification is easier
than polarity classification is that topic clusters are
typically well-separated from each other, result-
ing from the fact that word usage differs consid-
erably between two topically-different documents.
On the other hand, many reviews are sentimentally
ambiguous for a variety of reasons. For instance,
an author of a movie review may have negative
opinions of the actors but at the same time talk
enthusiastically about how much she enjoyed the
plot. Here, the review is ambiguous because she
discussed both the positive and negative aspects of
the movie, which is not uncommon in reviews. As
another example, a large portion of a movie re-
view may be devoted exclusively to the plot, with
the author only briefly expressing her sentiment at
the end of the review. In this case, the review is
ambiguous because the objective material in the
review, which bears no sentiment orientation, sig-
nificantly outnumbers its subjective counterpart.
Realizing the challenges posed by ambiguous
reviews, researchers have explored a number of
techniques to improve supervised polarity classi-
fiers. For instance, Pang and Lee (2004) train an
independent subjectivity classifier to identify and
remove objective sentences from a review prior to
polarity classification. Koppel and Schler (2006)
use neutral reviews to help improve the classi-
fication of positive and negative reviews. More
recently, McDonald et al (2007) have investi-
gated a model for jointly performing sentence- and
document-level sentiment analysis, allowing the
relationship between the two tasks to be captured
and exploited. However, the increased sophistica-
tion of supervised polarity classifiers has also re-
sulted in their increased dependence on annotated
data. For instance, Koppel and Schler needed to
manually identify neutral reviews to train their po-
larity classifier, and McDonald et al?s joint model
requires that each sentence in a review be labeled
with polarity information.
Given the difficulties of supervised polarity
classification, it is conceivable that unsupervised
polarity classification is a very challenging task.
Nevertheless, a solution to unsupervised polarity
classification is of practical significance. One rea-
son is that the vast majority of supervised polarity
701
classification systems are domain-specific. Hence,
when given a new domain, a large amount of an-
notated data from the domain typically needs to be
collected in order to train a high-performance po-
larity classification system. As Blitzer et al (2007)
point out, this data collection process can be ?pro-
hibitively expensive, especially since product fea-
tures can change over time?. Unfortunately, to
our knowledge, unsupervised polarity classifica-
tion is largely an under-investigated task in NLP.
Turney?s (2002) work is perhaps one of the most
notable examples of unsupervised polarity clas-
sification. However, while his system learns the
semantic orientation of phrases in a review in an
unsupervised manner, such information is used to
heuristically predict the polarity of a review.
At first glance, it may seem plausible to apply
an unsupervised clustering algorithm such as k-
means to cluster the reviews according to their po-
larity. However, there is reason to believe that such
a clustering approach is doomed to fail: in the ab-
sence of annotated data, an unsupervised learner
is unable to identify which features are relevant
for polarity classification. The situation is further
complicated by the prevalence of ambiguous re-
views, which may contain a large amount of irrel-
evant and/or contradictory information.
In light of the difficulties posed by ambiguous
reviews, we differentiate between ambiguous and
unambiguous reviews in our classification process
by addressing the task of semi-supervised polar-
ity classification via a ?mine the easy, classify the
hard? approach. Specifically, we propose a novel
system architecture where we first automatically
identify and label the unambiguous (i.e., ?easy?)
reviews, then handle the ambiguous (i.e., ?hard?)
reviews using a discriminative learner to bootstrap
from the automatically labeled unambiguous re-
views and a small number of manually labeled re-
views that are identified by an active learner.
It is worth noting that our system differs from
existing work on unsupervised/active learning in
two aspects. First, while existing unsupervised
approaches typically rely on clustering or learn-
ing via a generative model, our approach distin-
guishes between easy and hard instances and ex-
ploits the strengths of discriminative models to
classify the hard instances. Second, while exist-
ing active learners typically start with manually la-
beled seeds, our active learner relies only on seeds
that are automatically extracted from the data. Ex-
perimental results on five sentiment classification
datasets demonstrate that our system can gener-
ate high-quality labeled data from unambiguous
reviews, which, together with a small number of
manually labeled reviews selected by the active
learner, can be used to effectively classify ambigu-
ous reviews in a discriminative fashion.
The rest of the paper is organized as follows.
Section 2 gives an overview of spectral cluster-
ing, which will facilitate the presentation of our
approach to unsupervised sentiment classification
in Section 3. We evaluate our approach in Section
4 and present our conclusions in Section 5.
2 Spectral Clustering
In this section, we give an overview of spectral
clustering, which is at the core of our algorithm
for identifying ambiguous reviews.
2.1 Motivation
When given a clustering task, an important ques-
tion to ask is: which clustering algorithm should
be used? A popular choice is k-means. Neverthe-
less, it is well-known that k-means has the major
drawback of not being able to separate data points
that are not linearly separable in the given feature
space (e.g, see Dhillon et al (2004)). Spectral
clustering algorithms were developed in response
to this problem with k-means clustering. The cen-
tral idea behind spectral clustering is to (1) con-
struct a low-dimensional space from the original
(typically high-dimensional) space while retaining
as much information about the original space as
possible, and (2) cluster the data points in this low-
dimensional space.
2.2 Algorithm
Although there are several well-known spectral
clustering algorithms in the literature (e.g., Weiss
(1999), Meila? and Shi (2001), Kannan et al
(2004)), we adopt the one proposed by Ng et al
(2002), as it is arguably the most widely used. The
algorithm takes as input a similarity matrix S cre-
ated by applying a user-defined similarity function
to each pair of data points. Below are the main
steps of the algorithm:
1. Create the diagonal matrix G whose (i,i)-
th entry is the sum of the i-th row of S,
and then construct the Laplacian matrix L =
G?1/2SG?1/2.
2. Find the eigenvalues and eigenvectors of L.
702
3. Create a new matrix from the m eigenvectors
that correspond to the m largest eigenvalues.1
4. Each data point is now rank-reduced to a
point in the m-dimensional space. Normal-
ize each point to unit length (while retaining
the sign of each value).
5. Cluster the resulting data points using k-
means.
In essence, each dimension in the reduced space
is defined by exactly one eigenvector. The rea-
son why eigenvectors with large eigenvalues are
retained is that they capture the largest variance in
the data. Therefore, each of them can be thought
of as revealing an important dimension of the data.
3 Our Approach
While spectral clustering addresses a major draw-
back of k-means clustering, it still cannot be ex-
pected to accurately partition the reviews due to
the presence of ambiguous reviews. Motivated by
this observation, rather than attempting to cluster
all the reviews at the same time, we handle them in
different stages. As mentioned in the introduction,
we employ a ?mine the easy, classify the hard?
approach to polarity classification, where we (1)
identify and classify the ?easy? (i.e., unambigu-
ous) reviews with the help of a spectral cluster-
ing algorithm; (2) manually label a small number
of ?hard? (i.e., ambiguous) reviews selected by an
active learner; and (3) using the reviews labeled
thus far, apply a transductive learner to label the
remaining (ambiguous) reviews. In this section,
we discuss each of these steps in detail.
3.1 Identifying Unambiguous Reviews
We begin by preprocessing the reviews to be clas-
sified. Specifically, we tokenize and downcase
each review and represent it as a vector of uni-
grams, using frequency as presence. In addition,
we remove from the vector punctuation, numbers,
words of length one, and words that occur in a
single review only. Finally, following the com-
mon practice in the information retrieval commu-
nity, we remove words with high document fre-
quency, many of which are stopwords or domain-
specific general-purpose words (e.g., ?movies? in
the movie domain). A preliminary examination
of our evaluation datasets reveals that these words
1For brevity, we will refer to the eigenvector with the n-th
largest eigenvalue simply as the n-th eigenvector.
typically comprise 1?2% of a vocabulary. The de-
cision of exactly how many terms to remove from
each dataset is subjective: a large corpus typically
requires more removals than a small corpus. To be
consistent, we simply sort the vocabulary by doc-
ument frequency and remove the top 1.5%.
Recall that in this step we use spectral clustering
to identify unambiguous reviews. To make use of
spectral clustering, we first create a similarity ma-
trix, defining the similarity between two reviews
as the dot product of their feature vectors, but fol-
lowing Ng et al (2002), we set its diagonal entries
to 0. We then perform an eigen-decomposition of
this matrix, as described in Section 2.2. Finally,
using the resulting eigenvectors, we partition the
length-normalized reviews into two sets.
As Ng et al point out, ?different authors still
disagree on which eigenvectors to use, and how to
derive clusters from them?. To create two clusters,
the most common way is to use only the second
eigenvector, as Shi and Malik (2000) proved that
this eigenvector induces an intuitively ideal par-
tition of the data ? the partition induced by the
minimum normalized cut of the similarity graph2,
where the nodes are the data points and the edge
weights are the pairwise similarity values of the
points. Clustering in a one-dimensional space is
trivial: since we have a linearization of the points,
all we need to do is to determine a threshold for
partitioning the points. A common approach is to
set the threshold to zero. In other words, all points
whose value in the second eigenvector is positive
are classified as positive, and the remaining points
are classified as negative. However, we found that
the second eigenvector does not always induce a
partition of the nodes that corresponds to the min-
imum normalized cut. One possible reason is that
Shi and Malik?s proof assumes the use of a Lapla-
cian matrix that is different from the one used by
Ng et al To address this problem, we use the first
five eigenvectors: for each eigenvector, we (1) use
each of its n elements as a threshold to indepen-
dently generate n partitions, (2) compute the nor-
malized cut value for each partition, and (3) find
the minimum of the n cut values. We then select
the eigenvector that corresponds to the smallest of
the five minimum cut values.
Next, we identify the ambiguous reviews from
2Using the normalized cut (as opposed to the usual cut)
ensures that the size of the two clusters are relatively bal-
anced, avoiding trivial cuts where one cluster is empty and
the other is full. See Shi and Malik (2000) for details.
703
the resulting partition. To see how this is done,
consider the example in Figure 1, where the goal
is to produce two clusters from five data points.
( 1 1 1 0 0
1 1 1 0 0
0 0 1 1 0
0 0 0 1 1
0 0 0 1 1
) (?0.6983 0.7158
?0.6983 0.7158
?0.9869 ?0.1616
?0.6224 ?0.7827
?0.6224 ?0.7827
)
Figure 1: Sample data and the top two eigenvec-
tors of its Laplacian
In the matrix on the left, each row is the feature
vector generated for Di, the i-th data point. By in-
spection, one can identify two clusters, {D1,D2}
and {D4,D5}. D3 is ambiguous, as it bears re-
semblance to the points in both clusters and there-
fore can be assigned to any of them. In the ma-
trix on the right, the two columns correspond to
the top two eigenvectors obtained via an eigen-
decomposition of the Laplacian matrix formed
from the five data points. As we can see, the sec-
ond eigenvector gives us a natural cluster assign-
ment: all the points whose corresponding values
in the second eigenvector are strongly positive will
be in one cluster, and the strongly negative points
will be in another cluster. Being ambiguous, D3 is
weakly negative and will be assigned to the ?neg-
ative? cluster. Before describing our algorithm for
identifying ambiguous data points, we make two
additional observations regarding D3.
First, if we removed D3, we could easily clus-
ter the remaining (unambiguous) points, since the
similarity graph becomes more disconnected as
we remove more ambiguous data points. The
question then is: why is it important to produce
a good clustering of the unambiguous points? Re-
call that the goal of this step is not only to iden-
tify the unambiguous reviews, but also to annotate
them as POSITIVE or NEGATIVE, so that they can
serve as seeds for semi-supervised learning in a
later step. If we have a good 2-way clustering of
the seeds, we can simply annotate each cluster (by
sampling a handful of its reviews) rather than each
seed. To reiterate, removing the ambiguous data
points can help produce a good clustering of their
unambiguous counterparts.
Second, as an ambiguous data point, D3 can in
principle be assigned to any of the two clusters.
According to the second eigenvector, it should be
assigned to the ?negative? cluster; but if feature
#4 were irrelevant, it should be assigned to the
?positive? cluster. In other words, the ability to
determine the relevance of each feature is crucial
to the accurate clustering of the ambiguous data
points. However, in the absence of labeled data,
it is not easy to assess feature relevance. Even if
labeled data were present, the ambiguous points
might be better handled by a discriminative learn-
ing system than a clustering algorithm, as discrim-
inative learners are more sophisticated, and can
handle ambiguous feature space more effectively.
Taking into account these two observations, we
aim to (1) remove the ambiguous data points while
clustering their unambiguous counterparts, and
then (2) employ a discriminative learner to label
the ambiguous points in a later step.
The question is: how can we identify the
ambiguous data points? To do this, we ex-
ploit an important observation regarding eigen-
decomposition. In the computation of eigenvalues,
each data point factors out the orthogonal projec-
tions of each of the other data points with which
they have an affinity. Ambiguous data points re-
ceive the orthogonal projections from both the
positive and negative data points, and hence they
have near-zero values in the pivot eigenvectors.
Given this observation, our algorithm uses the
eight steps below to remove the ambiguous points
in an iterative fashion and produce a clustering of
the unambiguous points.
1. Create a similarity matrix S from the data
points D.
2. Form the Laplacian matrix L from S.
3. Find the top five eigenvectors of L.
4. Row-normalize the five eigenvectors.
5. Pick the eigenvector e for which we get the
minimum normalized cut.
6. Sort D according to e and remove ? points in
the middle of D (i.e., the points indexed from
|D|
2 ? ?2 + 1 to
|D|
2 +
?
2 ).
7. If |D| = ?, goto Step 8; else goto Step 1.
8. Run 2-means on e to cluster the points in D.
This algorithm can be thought of as the oppo-
site of self-training. In self-training, we iteratively
train a classifier on the data labeled so far, use it
to classify the unlabeled instances, and augment
the labeled data with the most confidently labeled
instances. In our algorithm, we start with an ini-
tial clustering of all of the data points, and then
iteratively remove the ? most ambiguous points
from the dataset and cluster the remaining points.
Given this analogy, it should not be difficult to see
the advantage of removing the data points in an it-
erative fashion (as opposed to removing them in a
704
single iteration): the clusters produced in a given
iteration are supposed to be better than those in
the previous iterations, as subsequent clusterings
are generated from less ambiguous points. In our
experiments, we set ? to 50 and ? to 500.3
Finally, we label the two clusters. To do this,
we first randomly sample 10 reviews from each
cluster and manually label each of them as POS-
ITIVE or NEGATIVE. Then, we label a cluster as
POSITIVE if more than half of the 10 reviews from
the cluster are POSITIVE; otherwise, it is labeled
as NEGATIVE. For each of our evaluation datasets,
this labeling scheme always produces one POSI-
TIVE cluster and one NEGATIVE cluster. In the rest
of the paper, we will refer to these 500 automati-
cally labeled reviews as seeds.
A natural question is: can this algorithm pro-
duce high-quality seeds? To answer this question,
we show in the middle column of Table 1 the label-
ing accuracy of the 500 reviews produced by our
iterative algorithm for our five evaluation datasets
(see Section 4.1 for details on these datasets). To
better understand whether it is indeed beneficial
to remove the ambiguous points in an iterative
fashion, we also show the results of a version of
this algorithm in which we remove all but the 500
least ambiguous points in just one iteration (see
the rightmost column). As we can see, for three
datasets (Movie, Kitchen, and Electronics), the
accuracy is above 80%. For the remaining two
(Book and DVD), the accuracy is not particularly
good. One plausible reason is that the ambiguous
reviews in Book and DVD are relatively tougher
to identify. Another reason can be attributed to
the failure of the chosen eigenvector to capture the
sentiment dimension. Recall that each eigenvector
captures an important dimension of the data, and
if the eigenvector that corresponds to the minimum
normalized cut (i.e., the eigenvector that we chose)
does not reveal the sentiment dimension, the re-
sulting clustering (and hence the seed accuracy)
will be poor. However, even with imperfectly la-
beled seeds, we will show in the next section how
we exploit these seeds to learn a better classifier.
3.2 Incorporating Active Learning
Spectral clustering allows us to focus on a small
number of dimensions that are relevant as far as
creating well-separated clusters is concerned, but
3Additional experiments indicate that the accuracy of our
approach is not sensitive to small changes to these values.
Dataset Iterative Single Step
Movie 89.3 86.5
Kitchen 87.9 87.1
Electronics 80.4 77.6
Book 68.5 70.3
DVD 66.3 65.4
Table 1: Seed accuracies on five datasets.
they are not necessarily relevant for creating po-
larity clusters. In fact, owing to the absence of la-
beled data, unsupervised clustering algorithms are
unable to distinguish between useful and irrelevant
features for polarity classification. Nevertheless,
being able to distinguish between relevant and ir-
relevant information is important for polarity clas-
sification, as discussed before. Now that we have
a small, high-quality seed set, we can potentially
make better use of the available features by train-
ing a discriminative classifier on the seed set and
having it identify the relevant and irrelevant fea-
tures for polarity classification.
Despite the high quality of the seed set, the re-
sulting classifier may not perform well when ap-
plied to the remaining (unlabeled) points, as there
is no reason to believe that a classifier trained
solely on unambiguous reviews can achieve a
high accuracy when classifying ambiguous re-
views. We hypothesize that a high accuracy can
be achieved only if the classifier is trained on both
ambiguous and unambiguous reviews.
As a result, we apply active learning (Cohn
et al, 1994) to identify the ambiguous reviews.
Specifically, we train a discriminative classifier us-
ing the support vector machine (SVM) learning al-
gorithm (Joachims, 1999) on the set of unambigu-
ous reviews, and then apply the resulting classifier
to all the reviews in the training folds4 that are not
seeds. Since this classifier is trained solely on the
unambiguous reviews, it is reasonable to assume
that the reviews whose labels the classifier is most
uncertain about (and therefore are most informa-
tive to the classifier) are those that are ambigu-
ous. Following previous work on active learning
for SVMs (e.g., Campbell et al (2000), Schohn
and Cohn (2000), Tong and Koller (2002)), we de-
fine the uncertainty of a data point as its distance
from the separating hyperplane. In other words,
4Following Dredze and Crammer (2008), we perform
cross-validation experiments on the 2000 labeled reviews in
each evaluation dataset, choosing the active learning points
from the training folds. Note that the seeds obtained in the
previous step were also acquired using the training folds only.
705
points that are closer to the hyperplane are more
uncertain than those that are farther away.
We perform active learning for five iterations.
In each iteration, we select the 10 most uncertain
points from each side of the hyperplane for human
annotation, and then re-train a classifier on all of
the points annotated so far. This yields a total of
100 manually labeled reviews.
3.3 Applying Transductive Learning
Given that we now have a labeled set (composed
of 100 manually labeled points selected by active
learning and 500 unambiguous points) as well as
a larger set of points that are yet to be labeled
(i.e., the remaining unlabeled points in the train-
ing folds and those in the test fold), we aim to
train a better classifier by using a weakly super-
vised learner to learn from both the labeled and
unlabeled data. As our weakly supervised learner,
we employ a transductive SVM.
To begin with, note that the automatically ac-
quired 500 unambiguous data points are not per-
fectly labeled (see Section 3.1). Since these unam-
biguous points significantly outnumber the manu-
ally labeled points, they could undesirably domi-
nate the acquisition of the hyperplane and dimin-
ish the benefits that we could have obtained from
the more informative and perfectly labeled active
learning points otherwise. We desire a system that
can use the active learning points effectively and at
the same time is noise-tolerant to the imperfectly
labeled unambiguous data points. Hence, instead
of training just one SVM classifier, we aim to re-
duce classification errors by training an ensemble
of five classifiers, each of which uses all 100 man-
ually labeled reviews and a different subset of the
500 automatically labeled reviews.
Specifically, we partition the 500 automatically
labeled reviews into five equal-sized sets as fol-
lows. First, we sort the 500 reviews in ascending
order of their corresponding values in the eigen-
vector selected in the last iteration of our algorithm
for removing ambiguous points (see Section 3.1).
We then put point i into set Li mod 5. This ensures
that each set consists of not only an equal number
of positive and negative points, but also a mix of
very confidently labeled points and comparatively
less confidently labeled points. Each classifier Ci
will then be trained transductively, using the 100
manually labeled points and the points in Li as la-
beled data, and the remaining points (including all
points in Lj , where i 6= j) as unlabeled data.
After training the ensemble, we classify each
unlabeled point as follows: we sum the (signed)
confidence values assigned to it by the five ensem-
ble classifiers, labeling it as POSITIVE if the sum
is greater than zero (and NEGATIVE otherwise).
Since the points in the test fold are included in the
unlabeled data, they are all classified in this step.
4 Evaluation
4.1 Experimental Setup
For evaluation, we use five sentiment classifica-
tion datasets, including the widely-used movie re-
view dataset [MOV] (Pang et al, 2002) as well as
four datasets that contain reviews of four differ-
ent types of product from Amazon [books (BOO),
DVDs (DVD), electronics (ELE), and kitchen ap-
pliances (KIT)] (Blitzer et al, 2007). Each dataset
has 2000 labeled reviews (1000 positives and 1000
negatives). We divide the 2000 reviews into 10
equal-sized folds for cross-validation purposes,
maintaining balanced class distributions in each
fold. It is important to note that while the test fold
is accessible to the transductive learner (Step 3),
only the reviews in training folds (but not their la-
bels) are used for the acquisition of seeds (Step 1)
and the selection of active learning points (Step 2).
We report averaged 10-fold cross-validation re-
sults in terms of accuracy. Following Kamvar et al
(2003), we also evaluate the clusters produced by
our approach against the gold-standard clusters us-
ing Adjusted Rand Index (ARI). ARI ranges from
?1 to 1; better clusterings have higher ARI values.
4.2 Baseline Systems
Recall that our approach uses 100 hand-labeled re-
views chosen by active learning. To ensure a fair
comparison, each of our three baselines has ac-
cess to 100 labeled points chosen from the train-
ing folds. Owing to the randomness involved in
the choice of labeled data, all baseline results are
averaged over ten independent runs for each fold.
Semi-supervised spectral clustering. We im-
plemented Kamvar et al?s (2003) semi-supervised
spectral clustering algorithm, which incorporates
labeled data into the clustering framework in the
form of must-link and cannot-link constraints. In-
stead of computing the similarity between each
pair of points, the algorithm computes the similar-
ity between a point and its k most similar points
only. Since its performance is highly sensitive to
706
Accuracy Adjusted Rand Index
System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD
1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.02
2 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.02
3 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.03
4 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.01
5 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.04
6 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06
Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets.
k, we tested values of 10, 15, . . ., 50 for k and re-
ported in row 1 of Table 2 the best results. As we
can see, accuracy ranges from 56.2% to 67.3%,
whereas ARI ranges from 0.02 to 0.12.
Transductive SVM. We employ as our second
baseline a transductive SVM5 trained using 100
points randomly sampled from the training folds
as labeled data and the remaining 1900 points as
unlabeled data. Results of this baseline are shown
in row 2 of Table 3. As we can see, accuracy
ranges from 57.3% to 68.7% and ARI ranges from
0.02 to 0.14, which are significantly better than
those of semi-supervised spectral learning.
Active learning. Our last baseline implements
the active learning procedure as described in Tong
and Koller (2002). Specifically, we begin by train-
ing an inductive SVM on one labeled example
from each class, iteratively labeling the most un-
certain unlabeled point on each side of the hyper-
plane and re-training the SVM until 100 points are
labeled. Finally, we train a transductive SVM on
the 100 labeled points and the remaining 1900 un-
labeled points, obtaining the results in row 3 of Ta-
ble 1. As we can see, accuracy ranges from 58%
to 68.9%, whereas ARI ranges from 0.03 to 0.14.
Active learning is the best of the three baselines,
presumably because it has the ability to choose the
labeled data more intelligently than the other two.
4.3 Our Approach
Results of our approach are shown in rows 4?6 of
Table 2. Specifically, rows 4 and 5 show the re-
sults of the SVM classifier when it is trained on
the labeled data obtained after the first step (unsu-
pervised extraction of unambiguous reviews) and
the second step (active learning), respectively. Af-
ter the first step, our approach can already achieve
5All the SVM classifiers in this paper are trained using
the SVMlight package (Joachims, 1999). All SVM-related
learning parameters are set to their default values, except in
transductive learning, where we set p (the fraction of unla-
beled examples to be classified as positive) to 0.5 so that the
system does not have any bias towards any class.
comparable results to the best baseline. Per-
formance increases substantially after the second
step, indicating the benefits of active learning.
Row 6 shows the results of transductive learn-
ing with ensemble. Comparing rows 5 and 6,
we see that performance rises by 0.7%-2.9% for
all five datasets after ?ensembled? transduction.
This could be attributed to (1) the unlabeled data,
which may have provided the transductive learner
with useful information that are not accessible to
the other learners, and (2) the ensemble, which is
more noise-tolerant to the imperfect seeds.
4.4 Additional Experiments
To gain insight into how the design decisions we
made in our approach impact performance, we
conducted the following additional experiments.
Importance of seeds. Table 1 showed that for
all but one dataset, the seeds obtained through
multiple iterations are more accurate than those
obtained in a single iteration. To envisage the im-
portance of seeds, we conducted an experiment
where we repeated our approach using the seeds
learned in a single iteration. Results are shown in
the first row of Table 3. In comparison to row 6 of
Table 2, we can see that results are indeed better
when we bootstrap from higher-quality seeds.
To further understand the role of seeds, we ex-
perimented with a version of our approach that
bootstraps from no seeds. Specifically, we used
the 500 seeds to guide the selection of active learn-
ing points, but trained a transductive SVM using
only the active learning points as labeled data (and
the rest as unlabeled data). As can be seen in row
2 of Table 3, the results are poor, suggesting that
our approach yields better performance than the
baselines not only because of the way the active
learning points were chosen, but also because of
contributions from the imperfectly labeled seeds.
We also experimented with training a transduc-
tive SVM using only the 100 least ambiguous
seeds (i.e., the points with the largest unsigned
707
Accuracy Adjusted Rand Index
System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD
1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.05
2 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.01
3 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.07
4 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.04
5 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.03
6 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.22
7 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38
Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets.
second eigenvector values) in combination with
the active learning points as labeled data (and the
rest as unlabeled data). Note that the accuracy of
these 100 least ambiguous seeds is 4?5% higher
than that of the 500 least ambiguous seeds shown
in Table 1. Results are shown in row 3 of Table 3.
As we can see, using only 100 seeds turns out to be
less beneficial than using all of them via an ensem-
ble. One reason is that since these 100 seeds are
the most unambiguous, they may also be the least
informative as far as learning is concerned. Re-
member that SVM uses only the support vectors to
acquire the hyperplane, and since an unambiguous
seed is likely to be far away from the hyperplane,
it is less likely to be a support vector.
Role of ensemble learning To get a better idea
of the role of the ensemble in the transductive
learning step, we used all 500 seeds in combina-
tion with the 100 active learning points to train a
single transductive SVM. Results of this experi-
ment (shown in row 4 of Table 3) are worse than
those in row 6 of Table 2, meaning that the en-
semble has contributed positively to performance.
This should not be surprising: as noted before,
since the seeds are not perfectly labeled, using all
of them without an ensemble might overwhelm the
more informative active learning points.
Passive learning. To better understand the role
of active learning in our approach, we replaced it
with passive learning, where we randomly picked
100 data points from the training folds and used
them as labeled data. Results, shown in row 5 of
Table 3, are averaged over ten independent runs
for each fold. In comparison to row 6 of Table 2,
we see that employing points chosen by an active
learner yields significantly better results than em-
ploying randomly chosen points, which suggests
that the way the points are chosen is important.
Using more active learning points. An interest-
ing question is: how much improvement can we
obtain if we employ more active learning points?
In row 6 of Table 3, we show the results when the
experiment in row 6 of Table 2 was repeated using
500 active learning points. Perhaps not surpris-
ingly, the 400 additional labeled points yield a 4?
11% increase in accuracy. For further comparison,
we trained a fully supervised SVM classifier using
all of the training data. Results are shown in row
7 of Table 3. As we can see, employing only 500
active learning points enables us to almost reach
fully-supervised performance for three datasets.
5 Conclusions
We have proposed a novel semi-supervised ap-
proach to polarity classification. Our key idea
is to distinguish between unambiguous, easy-to-
mine reviews and ambiguous, hard-to-classify re-
views. Specifically, given a set of reviews, we
applied (1) an unsupervised algorithm to identify
and classify those that are unambiguous, (2) an
active learner that is trained solely on automati-
cally labeled unambiguous reviews to identify a
small number of prototypical ambiguous reviews
for manual labeling, and (3) an ensembled trans-
ductive learner to train a sophisticated classifier
on the reviews labeled so far to handle the am-
biguous reviews. Experimental results on five sen-
timent datasets demonstrate that our ?mine the
easy, classify the hard? approach, which only re-
quires manual labeling of a small number of am-
biguous reviews, can be employed to train a high-
performance polarity classification system.
We plan to extend our approach by exploring
two of its appealing features. First, none of the
steps in our approach is designed specifically for
sentiment classification. This makes it applica-
ble to other text classification tasks. Second, our
approach is easily extensible. Since the semi-
supervised learner is discriminative, our approach
can adopt a richer representation that makes use
of more sophisticated features such as bigrams or
manually labeled sentiment-oriented words.
708
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grant IIS-0812261.
References
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the ACL, pages 440?447.
Colin Campbell, Nello Cristianini, , and Alex J. Smola.
2000. Query learning with large margin classifiers.
In Proceedings of ICML, pages 111?118.
David Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15(2):201?221.
Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. 2004.
Kernel k-means, spectral clustering and normalized
cuts. In Proceedings of KDD, pages 551?556.
Mark Dredze and Koby Crammer. 2008. Active learn-
ing with confidence. In Proceedings of ACL-08:HLT
Short Papers (Companion Volume), pages 233?236.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Sepandar Kamvar, Dan Klein, and Chris Manning.
2003. Spectral learning. In Proceedings of IJCAI,
pages 561?566.
Ravi Kannan, Santosh Vempala, and Adrian Vetta.
2004. On clusterings: Good, bad and spectral. Jour-
nal of the ACM, 51(3):497?515.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2):100?109.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the ACL, pages 432?439.
Marina Meila? and Jianbo Shi. 2001. A random walks
view of spectral segmentation. In Proceedings of
AISTATS.
Andrew Ng, Michael Jordan, and Yair Weiss. 2002.
On spectral clustering: Analysis and an algorithm.
In Advances in NIPS 14.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proceedings of ICML, pages 839?846.
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts
and image segmentation. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 22(8):888?
905.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Yair Weiss. 1999. Segmentation using eigenvectors: A
unifying view. In Proceedings of ICCV, pages 975?
982.
709
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84?85,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discriminative Models for Semi-Supervised Natural Language Learning
Sajib Dasgupta and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{sajib,vince}@hlt.utdallas.edu
1 Discriminative vs. Generative Models
An interesting question surrounding semi-
supervised learning for NLP is: should we use
discriminative models or generative models? De-
spite the fact that generative models have been
frequently employed in a semi-supervised setting
since the early days of the statistical revolution
in NLP, we advocate the use of discriminative
models. The ability of discriminative models to
handle complex, high-dimensional feature spaces
and their strong theoretical guarantees have made
them a very appealing alternative to their gen-
erative counterparts. Perhaps more importantly,
discriminative models have been shown to offer
competitive performance on a variety of sequential
and structured learning tasks in NLP that are
traditionally tackled via generative models , such
as letter-to-phoneme conversion (Jiampojamarn
et al, 2008), semantic role labeling (Toutanova
et al, 2005), syntactic parsing (Taskar et al,
2004), language modeling (Roark et al, 2004), and
machine translation (Liang et al, 2006). While
generative models allow the seamless integration
of prior knowledge, discriminative models seem
to outperform generative models in a ?no prior?,
agnostic learning setting. See Ng and Jordan (2002)
and Toutanova (2006) for insightful comparisons of
generative and discriminative models.
2 Discriminative EM?
A number of semi-supervised learning systems can
bootstrap from small amounts of labeled data using
discriminative learners, including self-training, co-
training (Blum and Mitchell, 1998), and transduc-
tive SVM (Joachims, 1999). However, none of them
seems to outperform the others across different do-
mains, and each has its pros and cons. Self-training
can be used in combination with any discriminative
learning model, but it does not take into account the
confidence associated with the label of each data
point, for instance, by placing more weight on the
(perfectly labeled) seeds than on the (presumably
noisily labeled) bootstrapped data during the learn-
ing process. Co-training is a natural choice if the
data possesses two independent, redundant feature
splits. However, this conditional independence as-
sumption is a fairly strict assumption and can rarely
be satisfied in practice; worse still, it is typically not
easy to determine the extent to which a dataset sat-
isfies this assumption. Transductive SVM tends to
learn better max-margin hyperplanes with the use
of unlabeled data, but its optimization procedure is
non-trivial and its performance tends to deteriorate if
a sufficiently large amount of unlabeled data is used.
Recently, Brefeld and Scheffer (2004) have pro-
posed a new semi-supervised learning technique,
EM-SVM, which is interesting in that it incorpo-
rates a discriminative model in an EM setting. Un-
like self-training, EM-SVM takes into account the
confidence of the new labels, ensuring that the in-
stances that are labeled with less confidence by the
SVM have less impact on the training process than
the confidently-labeled instances. So far, EM-SVM
has been tested on text classification problems, out-
performing transductive SVM. It would be interest-
ing to see whether EM-SVM can beat existing semi-
supervised learners for other NLP tasks.
84
3 Effectiveness of Bootstrapping
How effective are the aforementioned semi-
supervised learning systems in bootstrapping from
small amounts of labeled data? While there are quite
a few success stories reporting considerable perfor-
mance gains over an inductive baseline (e.g., parsing
(McClosky et al, 2008), coreference resolution (Ng
and Cardie, 2003), and machine translation (Ueff-
ing et al, 2007)), there are negative results too (see
Pierce and Cardie (2001), He and Gildea (2006),
Duh and Kirchhoff (2006)). Bootstrapping perfor-
mance can be sensitive to the setting of the param-
eters of these semi-supervised learners (e.g., when
to stop, how many instances to be added to the la-
beled data in each iteration). To date, however, re-
searchers have relied on various heuristics for pa-
rameter selection, but what we need is a principled
method for addressing this problem. Recently, Mc-
Closky et al (2008) have characterized the condi-
tions under which self-training would be effective
for semi-supervised syntactic parsing. We believe
that the NLP community needs to perform more re-
search of this kind, which focuses on identifying the
algorithm(s) that achieve good performance under a
given setting (e.g., few initial seeds, large amounts
of unlabeled data, complex feature space, skewed
class distributions).
4 Domain Adaptation
Domain adaptation has recently become a popular
research topic in the NLP community. Labeled data
for one domain might be used to train a initial classi-
fier for another (possibly related) domain, and then
bootstrapping can be employed to learn new knowl-
edge from the new domain (Blitzer et al, 2007). It
would be interesting to see if we can come up with
a similar semi-supervised learning model for pro-
jecting resources from a resource-rich language to
a resource-scarce language.
References
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT.
Ulf Brefeld and Tobias Scheffer. 2004. Co-EM support
vector learning. In Proceedings of ICML.
Kevin Duh and Katrin Kirchhoff. 2006. Lexicon acqui-
sition for dialectal Arabic using transductive learning.
In Proceedings of EMNLP.
Shan He and Daniel Gildea. 2006. Self-training and co-
training for semantic role labeling.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proceed-
ings of ACL-08:HLT.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of ICML.
Percy Liang, Alexandre Bouchard, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In Proceedings of the ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of HLT-NAACL.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs.generative classifiers: A comparison of logistic
regression and Naive Bayes. In Advances in NIPS.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of EMNLP.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proceedings of the ACL.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher Manning. 2004. Max-margin pars-
ing. In Proceedings of EMNLP.
Kristina Toutanova, Aria Haghighi, , and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the ACL.
Kristina Toutanova. 2006. Competitive generative mod-
els with structure learning for NLP classification tasks.
In Proceedings of EMNLP.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the ACL.
85
