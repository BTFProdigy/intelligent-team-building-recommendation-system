Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 61?64,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Combined One Sense Disambiguation of Abbreviations 
 
 
Yaakov HaCohen-Kerner 
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
21 Havaad Haleumi St., P.O.B. 
16031, 91160 Jerusalem, Israel 
kerner@jct.ac.il 
Ariel Kass 
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
21 Havaad Haleumi St., P.O.B. 
16031, 91160 Jerusalem, Israel 
ariel.kass@gmail.com 
Ariel Peretz 
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
21 Havaad Haleumi St., P.O.B. 
16031, 91160 Jerusalem, Israel 
relperetz@gmail.com 
 
 
Abstract 
A process that attempts to solve abbreviation 
ambiguity is presented. Various context-
related features and statistical features have 
been explored. Almost all features are domain 
independent and language independent. The 
application domain is Jewish Law documents 
written in Hebrew. Such documents are 
known to be rich in ambiguous abbreviations. 
Various implementations of the one sense per 
discourse hypothesis are used, improving the 
features with new variants. An accuracy of 
96.09% has been achieved by SVM. 
1 Introduction 
An abbreviation is a letter or sequence of letters, 
which is a shortened form of a word or a sequence 
of words, which is called the sense of the 
abbreviation. Abbreviation disambiguation means 
to choose the correct sense for a specific context. 
Jewish Law documents written in Hebrew are 
known to be rich in ambiguous abbreviations 
(HaCohen-Kerner et al, 2004). They can, 
therefore, serve as an excellent test-bed for the 
development of models for abbreviation 
disambiguation. 
As opposed to the documents investigated in 
previous systems, Jewish Law documents usually 
do not contain the sense of the abbreviations in the 
same discourse. Therefore, the abbreviations are 
regarded as more difficult to disambiguate. 
This research defines features, as well as 
experiments with various variants of the one sense 
per discourse hypothesis. The developed process 
considers other languages and does not define pre-
execution assumptions. The only limitation to this 
process is the input itself: the languages of the 
different text documents and the man-made 
solution database inputted during the learning 
process limit the datasets of documents that may be 
solved by the resulting disambiguation system.  
The proposed system, preserves its portability 
between languages and domains because it does 
not use any natural language processing (NLP) 
sub-system (e.g.: tokenizer and tagger). In this 
matter, the system is not limited to any specific 
language or dataset. The system is only limited by 
the different inputs used during the system?s 
learning stage and the set of abbreviations defined. 
This paper is organized as follows: Section 2 
presents previous systems dealing with 
disambiguation of abbreviations. Section 3 
describes the features for disambiguation of 
Hebrew abbreviations. Section 4 presents the 
implementation of the one sense per discourse 
hypothesis. Section 5 describes the experiments 
that have been carried out. Section 6 concludes and 
proposes future directions for research. 
2 Abbreviation Disambiguation 
The one sense per collocation hypothesis was 
introduced by Yarowsky (1993). This hypothesis 
states that natural languages tend to use consistent 
spoken and written styles. Based on this 
hypothesis, many terms repeat themselves with the 
same meaning in all their occurrences. Within the 
context of determining the sense of an 
abbreviation, it may be assumed that authors tend 
to use the same words in the vicinity of a specific 
long form of an abbreviation. The words may be 
reused as indicators of the proper solution of an 
additional unknown abbreviation with the same 
words in its vicinity. This is the basis for all 
contextual features defined in this research. 
The one sense per discourse hypothesis (OS) 
was introduced by Gale et al (1992). This 
61
hypothesis assumes that in natural languages, there 
is a tendency for an author to be consistent in the 
same discourse or article. That is, if in a specific 
discourse, an ambiguous phrase or term has a 
specific meaning, any other subsequent instance of 
this phrase or term will have the same specific 
meaning. Within the context of determining the 
sense of an abbreviation, it may be assumed that 
authors tend to use a specific abbreviation in a 
specific sense throughout the discourse or article. 
Research has been done within this domain, 
mainly for English medical documents. Systems 
developed by Pakhomov (2002; 2005), Yu et al 
(2003) and Gaudan et al (2005) achieved 84% to 
98% accuracy. These systems used various 
machine learning (ML) methods, e.g.: Maximum 
Entropy, SVM and C5.0. 
In our previous research (HaCohen-Kerner et 
al., 2004), we developed a prototype abbreviation 
disambiguation system for Jewish Law documents 
written in Hebrew, without using any ML method. 
The system integrated six basic features: common 
words, prefixes, suffixes, two statistical features 
and a Hebrew specific feature. It achieved about 
60% accuracy while solving 50 abbreviations with 
an average of 2.3 different senses in the dataset. 
3 Abbreviation Disambiguation Features 
Eighteen different features of any abbreviation 
instance were defined. They are divided into three 
distinct groups, as follows: 
Statistical attributes: Writer/Dataset 
Common Rule (WC/DS). The most common 
solution used for the specific abbreviation by the 
discussed writer/ in the entire dataset. 
Hebrew specific attribute: Gimatria Rule 
(GM). The numerical sum of the numerical values 
attributed to the Hebrew letters forming the 
abbreviation (HaCohen-Kerner et al, 2004). 
Contextual relationship attributes: 
1. Prefix Counted Rule (PRC): The selected 
sense is the most commonly appended sense by the 
specific prefix. 
2. Before/After K (1,2,3,4) Words Counted 
Rule (BKWC/AKWC): The selected sense is the 
most commonly preceded/succeeded sense by the 
K specific words in the sentence of the specific 
abbreviation instance. 
3. Before/After Sentence Counted Rule 
(BSC/ASC): The selected sense is the most 
commonly preceded/succeeded sense by all the 
specific words in the sentence of the specific 
abbreviation instance. 
4. All Sentence/Article Counted Rule 
(AllSC/AllAC): The selected sense is the most 
commonly surrounded sense by all the specific 
words in the sentence/article of the specific 
abbreviation instance. 
5. Before/After Article Counted Rule 
(BAC/AAC): The selected sense is the most 
commonly preceded/succeeded sense by all the 
specific words in the article of the specific 
abbreviation instance. 
4 Implementing the OS Hypothesis 
As mentioned above, the basic assumption of the 
OS hypothesis is that there exists at least one 
solvable abbreviation in the discourse and that the 
sense of that abbreviation is the same for all the 
instances of this abbreviation in the discourse. The 
correctness of all the features was investigated 
based on this hypothesis for several variants of 
"one sense" based on the discussed discourse: none 
(No OS), a sentence (osS), an article (osA) or all 
the articles of the writer (osW). 
The OS hypothesis was implemented in two 
forms. The ?pure? form (with the suffix S/A/W 
without C) uses the sense found by the majority 
voting method for an abbreviation in the discourse 
and applies it ?blindly? to all other instances. 
The ?combined? form (with the suffix C) tries to 
find the sense of the abbreviation using the 
discussed feature only. If the feature is 
unsuccessful, then we use the relevant one sense 
variant using the majority voting method. This 
form is derived from the possibility that more than 
one sense may be used within a single discourse 
and only instances with an unknown sense 
conform to the hypothesis. 
The use of the OS hypothesis, in both forms, is 
only relevant for context based features, since the 
solutions by other features are static and identical 
from one instance to another. 
Therefore, for each of the 15 context based 
features, 6 variants of the hypothesis were 
implemented. This produces 90 variants, which 
together with the 18 features in their normal form, 
results in a total of 108 variants. In addition, the 
ML methods were experimented together with the 
OS hypothesis. Of the 108 possible variants, for 
the 18 features, the best variant for each feature 
62
was chosen. In each step, the next best variant is 
added, starting from the 2 best variants. 
5 Experiments 
The examined dataset includes Jewish Law 
Documents written by two Jewish scholars: Rabbi 
Y. M. HaCohen (1995) and Rabbi O. Yosef (1977; 
1986). This dataset includes 564,554 words where 
114,814 of them are abbreviations instances, and 
42,687 of them are ambiguous. That is, about 7.5% 
of the words are ambiguous abbreviations. These 
ambiguous abbreviations are instances of a set of 
135 different abbreviations. Each one of the 
abbreviations has between 2 to 8 relevant possible 
senses. The average number of senses for an 
abbreviation in the dataset is 3.27. 
To determine the accuracy of the system, all the 
instances of the ambiguous abbreviations were 
solved beforehand. Some of them were based on 
published solutions (HaCohen, 1995) and some of 
them were solved by experienced readers. 
5.1 Results of the variants of OS Hypothesis 
The results of the OS hypothesis variants, for all 
the features, are presented in Table 1. These results 
are obtained without using any ML methods. 
 
Accuracy Percentage % Use of OS / 
Feature  No OS osS osSC osA osAC osW osWC 
PRC 33.67 34.41 34.52 52.77 54.54 66.66 71.04 
B1WC 56.05 56.41 56.61 67.74 71.84 72.93 82.51 
B2WC 55.72 56.23 56.35 69 72.34 74.85 82.84 
B3WC 60.54 60.89 61.01 72.67 75.48 75.44 82.86 
B4WC 64.49 64.72 64.85 74.29 76.5 75.52 82.2 
BSC 75.21 75.18 75.24 76.85 78.15 74.92 78.52 
BAC 76 76 76 76.01 76 75.39 76 
A1WC 78.79 79.01 79.21 78.72 83.81 76.32 87.75 
A2WC 77.57 78.07 78.26 79.15 83.43 78.54 87.62 
A3WC 78.64 79.11 79.28 79.61 83 78.19 85.8 
A4WC 75.44 79.28 79.5 79.41 82.42 78.01 84.99 
ASC 78.59 78.61 78.62 78.25 78.94 77.37 79.04 
AAC 75.44 75.44 75.44 75.34 75.44 77.28 75.44 
AllSC 77.97 77.97 77.97 77.9 78.02 77.22 78.04 
AllAC 74.12 74.12 74.12 74.12 74.12 76.93 74.12 
GM 46.82 46.82 46.82 46.82 46.82 46.82 46.82 
WC 82.84 82.84 82.84 82.84 82.84 82.84 82.84 
DC 78.34 78.34 78.34 78.34 78.34 78.34 78.34 
Table 1. Results of the OS Variants for all the Features. 
 
The two best pure features were WC and A1WC 
with 82.84% and 78.79% of accuracy, respectively. 
The first finding shows that about 83% of the 
abbreviations have the same sense in the whole 
dataset. The second finding shows that about 79% 
of the abbreviations can be solved by the first word 
that comes after the abbreviation.  
Generally, contextual features based on the 
context that comes after the abbreviation, achieve 
considerably better results than all other contextual 
features. Specifically, the A1WC_osWC feature 
variant achieves the best result with 87.75% 
accuracy. These results suggest that each 
individual abbreviation has stronger relationship to 
the words after a specific instance, especially to the 
first word. 
Almost every feature has at least one variant that 
achieves a substantial improvement in results 
compared the results achieved by the feature in its 
normal form. The average relative improvement is 
about 18%. 
For all features, except BAC, the best variant 
uses the OS implementation with the discourse 
defined as the entire dataset. This may be 
attributed to the similarity of the different articles 
in the dataset. This is supported by the fact that the 
best feature, in its normal form, is the WC feature. 
In addition, for all but three features (BAC, 
AAC, AllAC), the best variant used the combined 
form of the OS implementation. This is intuitively 
understandable, since ?blindly? overwriting 
probably erases many successes of the feature in its 
normal form. 
5.2   The Results of the Supervised ML Methods 
Several well-known supervised ML methods have 
been selected: artificial neural networks (ANN), 
Na?ve Bayes (NB), Support Vector Machines 
(SVM) and J48 (Witten and Frank, 1999) an 
improved variant of the C4.5 decision tree 
induction. These methods have been applied with 
default values and no feature normalization using 
Weka (Witten and Frank, 1999). Tuning is left for 
future research. To test the accuracy of the models, 
10-fold cross-validation was used.  
Table 2 presents the results of these supervised 
ML methods, by incrementally combining the best 
variant for each feature (according to Table 1). 
Table 2 shows that SVM achieved the best result 
with 96.09% accuracy. The best improvement is 
63
about 13%, from 82.84% accuracy for the best 
variant of any feature to 96.02% accuracy. This 
table also reveals that incremental combining of 
most of the variants leads to better results for most 
of the ML methods. 
 
# of 
Vari-
ants 
 
Variants /  
ML Method ANN NB SVM J48 
2 A1WC_osWC 
+A2WC_osWC 91.56 91.40 94.29 91.94 
3 + A3WC_osWC 91.72 91.42 94.43 92.20 
4 + A4WC_osWC 91.75 91.51 94.43 92.34 
5 + B3WC_osWC 92.68 92.11 95.33 93.33 
6 + WC 92.95 92.16 95.71 93.54 
7 + B2WC_osWC 92.81 91.79 95.67 93.59 
8 + B1WC_osWC 92.91 91.06 95.68 93.56 
9 + B4WC_osWC 92.83 91.15 95.62 93.55 
10 + ASC_osWC 92.83 91.10 95.60 93.52 
11 + BSC_osWC 92.95 91.17 95.65 93.58 
12 + DC 92.98 91.17 95.63 93.58 
13 + AllSC_osWC 92.82 91.50 95.63 93.58 
14 + AAC_osW 92.84 91.42 95.59 93.58 
15 + AllAC_osW 93.10 91.43 95.77 93.58 
16 + BAC_osA 93.09 91.28 95.79 93.70 
17 + PRC_osWC 93.25 91.50 96.09 93.71 
18 + GM 93.28 91.52 96.02 93.93 
Table 2. The Results of the ML Methods. 
 
The comparison of the SVM results to the 
results of previous (Section 2) shows that our 
system achieves relatively high accuracy. 
However, most previous systems researched 
ambiguous abbreviations in the English language, 
as well as different abbreviations and texts. 
6   Conclusions, Summary and Future Work 
This is the first ML system for disambiguation of 
abbreviations in Hebrew. High accuracy 
percentages were achieved, with improvement 
ascribed to the use of OS hypothesis combined 
with ML methods. These results were achieved 
without the use of any NLP features. Therefore, the 
developed system is adjustable to any specific type 
of texts, simply by changing the database of texts 
and abbreviations. 
This system is the first that applies many 
versions of the one sense per discourse hypothesis. 
In addition, we performed a comparison between 
the achievements of four different standard ML 
methods, to the goal of achieving the best results, 
as opposed to the other systems that mainly 
focused on one ML method, each.  
Future research directions are: comparison to 
abbreviation disambiguation using the standard 
bag-of-words or collocation feature 
representations, definition and implementation of 
other NLP-based features and use of these features 
interlaced with the already defined features, 
applying additional ML methods, and augmenting 
the databases with articles from additional datasets 
in the Hebrew language and other languages. 
References 
Y. M. Hacohen (Kagan). 1995. Mishnah Berurah (in 
Hebrew), Hotzaat Leshem, Jerusalem. 
Y. HaCohen-Kerner, A. Kass and A. Peretz. 2004. 
Baseline Methods for Automatic Disambiguation of 
Abbreviations in Jewish Law Documents. 
Proceedings of the 4th International Conference on 
Advances in Natural Language LNAI, Springer 
Berlin/Heidelberg, 3230: 58-69. 
W. Gale, K. Church and D. Yarowsky. 1992. One Sense 
Per Discourse. Proceedings of the 4th DARPA 
speech in Natural Language Workshop, 233-237. 
S. Gaudan, H. Kirsch and D. Rebholz-Schuhmann. 
2005. Resolving abbreviations to their senses in 
Medline. Bioinformatics, 21 (18): 3658-3664. 
S. Pakhomov. 2002. Semi-Supervised Maximum 
Entropy Based Approach to Acronym and 
Abbreviation Normalization in Medical Texts. 
Association for Computational Linguistics (ACL), 
160-167. 
S. Pakhomov, T. Pedersen and C. G. Chute. 2005. 
Abbreviation and Acronym Disambiguation in 
Clinical Discourse. American Medical Informatics 
Association Annual Symposium, 589-593. 
D. Yarowsky. 1993. One sense per collocation. 
Proceedings of the workshop on Human Language 
Technology, 266-271. 
O. Yosef. 1977. Yechave Daat (in Hebrew), Publisher: 
Chazon Ovadia, Jerusalem. 
O. Yosef. 1986. Yabia Omer (in Hebrew), Publisher: 
Chazon Ovadia, Jerusalem.  
Z. Yu, Y. Tsuruoka and J. Tsujii. 2003. Automatic 
Resolution of Ambiguous Abbreviations in 
Biomedical Texts using SVM and One Sense Per 
Discourse Hypothesis. SIGIR?03 Workshop on Text 
Analysis and Search for Bioinformatics. 
H. Witten and E. Frank. 2007. Weka 3.4.12: Machine 
Learning Software in Java: 
http://www.cs.waikato.ac.nz/~ml/weka. 
64
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 421?429,
Beijing, August 2010
Detection of Simple Plagiarism in Computer Science Papers 
 
Yaakov HaCohen-Kerner 
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
kerner@jct.ac.il 
Aharon Tayeb  
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
aharontayeb@gmail.com
Natan Ben-Dror  
Department of Computer 
Science, Jerusalem College of 
Technology (Machon Lev) 
bd.natan@gmail.com
 
Abstract 
Plagiarism is the use of the language and 
thoughts of another work and the repre-
sentation of them as one's own original 
work. Various levels of plagiarism exist 
in many domains in general and in aca-
demic papers in particular. Therefore, di-
verse efforts are taken to automatically 
identify plagiarism. In this research, we 
developed software capable of simple 
plagiarism detection. We have built a 
corpus (C) containing 10,100 academic 
papers in computer science written in 
English and two test sets including pa-
pers that were randomly chosen from C. 
A widespread variety of baseline me-
thods has been developed to identify 
identical or similar papers. Several me-
thods are novel. The experimental results 
and their analysis show interesting find-
ings. Some of the novel methods are 
among the best predictive methods. 
1 Introduction 
In light of the explosion in the number of availa-
ble documents, fast and accurate searching for 
plagiarism is becoming more needed. Identifica-
tion of identical and similar documents is becom-
ing very important. 
Plagiarism is the use of the language and 
thoughts of another work and the representation 
of them as one's own original work (Wikipedia, 
2010; Library and Information Services, 2010). 
Plagiarism can be committed by "recycling" oth-
er's work as well as by one?s own work (self- 
plagiarism). 
Various levels of plagiarism exist in many 
domains in general and in academic papers in 
particular. In addition to the ethical problem, 
plagiarism in Academics can be illegal if copy-
right of the previous publication has been trans-
ferred to another entity. 
It is important to mention, that in many cases 
similar papers are different versions of the same 
work, e.g., a technical report, a poster paper, a 
conference paper, a journal paper and a Ph. D. 
dissertation. 
To avoid any kind of plagiarism, all sources 
which were used in the completion of a 
work/research must be mentioned (Library and 
Information Services, 2010). 
Over the last decade, various softwares have 
been built to automatically identify plagiarism 
(e.g., Collberg et al (2005), Sorokina et al 
(2006), and Keuskamp and Sliuzas (2007)).  
In this research, we developed such a system. 
This system is planned to deal with simple kinds 
of plagiarism, e.g., copying of sentences or part 
of sentences. We have built a corpus that con-
tains academic papers in computer science writ-
ten in English. Most of the papers are related to 
the domain research of Natural Language 
Processing (NLP) and are from the last ten years. 
The remainder of this paper is organized as 
follows: Section 2 gives a background regarding 
plagiarism. Section 3 overviews researches and 
systems dealing with detection of plagiarism. 
Section 4 describes five groups of baseline me-
thods, which have been implemented by us to 
detect plagiarism. Section 5 presents the experi-
ments that have been performed and their analy-
sis. Section 6 gives an illustrative example. Sec-
tion 7 concludes and proposes future directions 
for research.  
2 Plagiarism 
Plagiarism is defined in the 1995 Random House 
Compact Unabridged Dictionary as the "use or 
close imitation of the language and thoughts of 
another author and the representation of them as 
one's own original work."  
421
Self-plagiarism is the reuse of significant, 
identical, or nearly identical parts of one?s own 
work without citing the original work. In addi-
tion to the ethical issue, this phenomenon can be 
illegal if copyright of the previous work has been 
transferred to another entity. Usually, self-
plagiarism is considered to be a serious ethical 
problem in cases where a publication needs to 
contain an important portion of a new material, 
such as in academic papers (Wikipedia, 2010). 
On the other hand, it is common for research-
ers to rephrase and republish their research, tai-
loring it for different academic journals and con-
ference articles, to disseminate their research to 
the widest possible interested public. However, 
these researchers must include in each publica-
tion a meaningful or an important portion of a 
new material (Wikipedia, 2010). 
There are various classifications for levels of 
plagiarism. For instance, IEEE (2010) catego-
rized plagiarism into five levels, or degrees, of 
misconduct, ranging from the most serious (Lev-
el One) to the least serious (Level Five): 
Level One: The uncredited verbatim copying 
of a full paper, or the verbatim copying of a ma-
jor portion (greater than half of the original pa-
per)  
Level Two: The uncredited verbatim copying 
of a large portion (less than half of the original 
paper). 
Level Three: The uncredited verbatim copy-
ing of individual elements (e.g., paragraphs, sen-
tences, figures). 
Level Four: The uncredited improper paraph-
rasing of pages or paragraphs.  
Level Five: The credited verbatim copying of 
a major portion of a paper without clear delinea-
tion (e.g., quotes or indents). 
Loui (2002) handled eight allegations of pla-
giarism related to students' works. Collberg et al 
(2005) proposes eight ranks of plagiarism. 
3 Related Research 
There are two main attitudes concerning discov-
ery of similar documents: ranking and finger-
printing. Ranking methods are derived from in-
formation retrieval (IR) and are widely used in 
IR systems and Internet search engines. Known 
ranking methods are the cosine measure, the in-
ner product, and the normalized inner product. 
Hoad and Zobel (2003) extended the ranking 
family by defining identity measures, designed 
for identification of co-derivative documents. 
Fingerprinting aims to compare between two 
documents based on their fingerprints. Finger-
print methods have been used by many previous 
researches, e.g., Manber (1994). Heintze (1996), 
Lyo et al (2001), Hoad and Zobel (2003), and 
Shivakumar and Garcia-Molina (1996). 
3.1 Full Fingerprinting 
Given a document, a full fingerprint of the 
document consists of the set of all the possible 
sequential substrings of length ? in words (a 
definition that is based on characters is also pos-
sible). There are N??+1 such substrings, where 
N is the length of the document in words. This 
fingerprinting selects overlapping sub-strings. 
For instance, if ? is 3, this method selects the 3-
word phrases that begin at position 0; 1; 2; etc. 
The size of ? is known as the fingerprint granu-
larity. This variable can have a significant impact 
of the accuracy of fingerprinting (Shivakumar 
and Garcia-Molina, 1996). 
Comparing a document X to a document Y 
where X's size is |X| and if n is the number of 
substrings common to both documents then n/|X| 
is the measure of how much of X is contained in 
Y. 
3.2 Selective Fingerprinting 
To decrease the size of a full fingerprint, there 
are various versions of selective fingerprints. 
The simplest kind of selective fingerprinting 
is the "All substrings selection" described in 
Hoad and Zobel (2003). This fingerprinting is 
similar to the full fingerprinting, but it does not 
select overlapping sub-strings. Rather, it selects 
all non-overlapping substrings of size ? (in 
words) from the document. For example, if ? is 
3, this strategy selects the 3-word phrases that 
begin at position 0; 3; 6; 9; etc. 
Heintze (1996) performed various experi-
ments using a fixed number of fingerprints inde-
pendent of the size of the document and a fixed 
number of substrings of size ? (in characters). 
The best results were achieved by 1,000 finger-
prints with ?=50. Another possibility is to work 
with a fixed proportion of the substrings, so that 
the size of the selective fingerprint is propor-
tional to the size of the document. The main dis-
422
advantage of this possibility is space consump-
tion. 
Hoad and Zobel (2003) suggested many addi-
tional general types of selective fingerprinting, 
e.g., positional, frequency-based, and structure-
based. 
3.3 Additional Similarity Measures 
SymmetricSimilarity 
Monostori1 et al (2002) defined a measure 
called SymmetricSimilarity as follows: 
SS(X, Y) = ?d(X) ? d(Y)?/?d(X) + d(Y)? 
where X and Y are the two compared docu-
ments, d(X) and d(Y) are the number of the 
fingerprints of X and Y, respectively, and 
?d(X)?d(Y)? is the number of the common 
fingerprints. 
 
S2 and S3 
Bernstein and Zobel (2004) defined several 
additional similarity measures, such as S2 
and S3: 
S2(X, Y) = ?d(X) ? d(Y)?/min(?d(X)?, 
?d(Y)?) 
 S3(X, Y)= ?d(X) ? d(Y)?/ (d(X) + d(Y))/2) 
where min(?d(X)?, ?d(Y)?) is the minimal 
number of the fingerprints of X and Y, re-
spectively, and d(X) + d(Y) is the average 
number of the fingerprints of X and Y. 
 
Rarest-in-document 
The Rarest-in-Document method is one of 
the frequency-based methods defined by 
Hoad and Zobel (2003). This method choos-
es the substrings that produce the rarest sub-
strings with length of k words in the docu-
ment. This means that all of the substrings 
must be calculated and sorted according to 
their frequency in the document, and then the 
rarest of them are selected. The intuition is 
that sub-strings, which are less common, are 
more effective discriminators when compar-
ing documents for similarity. 
Anchor methods 
Hoad and Zobel (2003) defined anchor me-
thods. These methods are based on specific, 
predefined strings (called anchors), in the 
text of the document. The anchors are chosen 
to be common enough that there is at least 
one in almost every document, but not so 
common that the fingerprint becomes very 
large (Manber, 1994).  
Various anchors were used by Hoad and Zo-
bel. The anchors were randomly selected, but 
extremely common strings such as "th" and "it" 
were rejected. The 35 2-character anchor method 
detects all of the documents that were consi-
dered as similar by a human expert. 
Additional experiments have been applied to 
identify the optimal size of an anchor. Manber 
(1994) used 50-character anchors in a collection 
of over 20,000 "readme" documents, identifying 
3,620 sets of identical files and 2,810 sets of sim-
ilar files. Shivakumar and Garcia-Molina (1996) 
achieved the best results with one-sentence anc-
hors and Heintze (1996) achieved the best results 
with 1000-character anchors. 
4 Baseline Detection Methods 
To find whether there is a plagiarism, novel 
and old baseline methods have been imple-
mented. These methods can be divided into 
five groups: full fingerprint methods, selec-
tive fingerprint methods, anchor methods, 
word comparison methods, and combinations 
of methods. 
Full fingerprint methods  
All the full fingerprint methods are defined for 
overlapping substrings of length k in words from 
the beginning of the document. 
1. FF(k) - Full Fingerprints of length k  
2. SSF(k) - SymmetricSimilarity for  
     Full fingerprints of length k 
3. S2F(k) - S2 for Full fingerprints of length k 
4. S3F(k) - S3 for Full fingerprints of length k  
5. RDF(k) - Rarest-in-Document for Full  
     fingerprints of length k 
6. CA -  Compare between the abstracts of the   
     two documents using FF(3) 
 
Selective Fingerprint methods 
In this research, all the selective fingerprint 
methods are selective by the sense of non-
overlapping substrings of length k in words 
from the beginning of the document. 
7. SF(k) -  Selective Fingerprints of length k  
423
8. SSS(k) - SymmetricSimilarity for Selective 
fingerprints of length k 
9. S2S(k) - S2 for Selective fingerprints of 
length k 
10. S3S(k) - S3 for Selective fingerprints of 
length k 
11. RDS(k) - Rarest-in-Document for Selective 
fingerprints of length k 
 
Anchor methods 
We decided to work with seventy (N=70) 3-
character anchors. Based on these anchors we 
have defined the following methods: 
12. AFW -  Anchor First Words -  First 3-
charcters from each one of the first N words 
in the tested document  
13. AFS -  Anchor First Sentences -  First 3-
charcters from each one of the first N sen-
tences in the tested document 
14. AF -  most Frequent Anchors -  N most 
frequent 3-charcter prefixes in the tested 
document 
15. AR -  Rarest Anchors - N rarest frequent 3-
charcter prefixes in the tested document 
16. ALW -  Anchor Last Words -  First 3-
charcters from each one of the last N words 
in the tested document  
17. ALS -  Anchor Last Sentences -  First 3-
charcters from each one of the last N sen-
tences in the tested document Word compari-
sons 
18. CR - CompareReferences. This method 
compares between the titles of the papers in-
cluded in the references section of the two 
examined papers. 
 
Combinations of methods  
19. CARA-   CompareAbstractReferencesAve-
rage. This method returns the average value 
of CA and CR. 
20. CARM -  CompareAbstractReferencesMin. 
This method returns the minimal value be-
tween CA and CR. 
 
As mentioned above, Hoad and Zobel (2003) 
defined anchor methods based on the first/last N 
sentences/words/3-charcter prefixes in the tested 
document. As shown in Table 1 and in its analy-
sis, the anchor methods are not successful, prob-
ably because they use a small portion of data. 
Therefore, we decided to implement methods 
defined for the following portions of the paper: 
the first third (first), the middle third (middle), 
and the last third (end) of the paper according to 
the number of the words in the discussed paper. 
All the first, middle and end methods use FF(3). 
These methods were combined with CA or CR. 
CA was not combined with the first methods be-
cause the abstract is included in the first part of 
the paper. CR was not combined with the last 
methods because the references are included in 
the end part of the paper. 
21. CAMA- CompareAbstractMiddleAve. This 
method returns the average value of CA and 
FF(3) computed for the middle parts of the 
two examined papers. 
22. CAMM - CompareAbstractMiddleMin. 
This method returns the minimal value be-
tween CA and FF(3) computed for the mid-
dle parts of the two examined papers. 
23. CAEA - CompareAbstractEndAverage. 
This method returns the average value of CA 
and FF(3) computed for the end parts of the 
two examined papers. 
24. CAEM - CompareAbstractEndMin. This 
method returns the minimal value between 
CA and FF(3) computed for the end parts of 
the two examined papers. 
25. CRFA -  CompareReferencesFirstAverage. 
This method returns the average value of CR 
and FF(3) computed for the first parts of the 
two examined papers. 
26. CRFM - CompareReferencesFirstMin. This 
method returns the minimal value between 
between CR and FF(3) computed for the first 
parts of the two examined papers. 
27. CRMA - CompareReferencesMiddleAve-
rage. This method returns the average value 
of CR and FF(3) computed for the middle 
parts of the two examined papers. 
28. CRMM - CompareReferencesMiddleMin. 
This method returns the minimal value be-
tween CR and FF(3) computed for the mid-
dle parts of the two examined papers. 
 
To the best of our knowledge, we are the first 
to implement methods that compare special and 
important sections in academic papers: abstract 
and references: CA and CR, and combinations of 
them. In addition, we implemented new methods 
defined for the three thirds: the first (F) third, the 
middle (M) third, and the last (E) third of the 
paper. These methods were combined with CA 
and CR in various variants. All in total, we have 
defined 12 new baseline methods. 
424
5     Experimental Results 
5.1 Dataset 
As mentioned above, the examined dataset 
includes 10,100 academic papers in computer 
science. Most of the papers are related to NLP 
and are from the last ten years. Most of the 
papers were downloaded from 
http://www.aclweb.org/anthology/. 
 These documents include 52,909,234 words 
that are contained in 3,722,766 sentences. Each 
document includes in average 5,262 words. The 
minimum and maximum number of words in a 
document are 28,758 and 305, respectively. 
 The original PDF files were downloaded 
using IDM - Internet Download Manager 
(http://www.internetdownloadmanager.com/). 
Then we convert them to TXT files using 
ghostscript (http://pages.cs.wisc.edu/~ghost/). 
Many PDF files were not papers and many others 
were converted to gibberish files. Therefore, the 
examined corpus contains only 10,100 papers. 
5.2 Experiment I 
Table 1 presents the results of the 38 imple-
mented methods regarding the corpus of 10,100 
documents. The test set includes 100 papers that 
were randomly chosen from the examined 
dataset. For each tested document, all the other 
10,099 documents were compared using the var-
ious baseline methods. 
 The IDN, VHS, HS, MS columns present the 
number of the document pairs that found as iden-
tical, very high similar, high similar, and medium 
similar to the 100 tested documents, respectively. 
The IDN, VHS, HS, MS levels were granted to 
document pairs that got the following similarity 
values: 100%, [80%, 100%), [60%, 80%), and 
[40%, 60%), respectively. However, similar pair 
of papers is not always a case of plagiarism, e.g., 
in case where one paper cites the second one. 
The first left column indicates a simple ordin-
al number. The second left column indicates the 
serial number of the baseline method (Section 4) 
and the number in parentheses indicates the 
number of the chosen words (3 or 4) to be in-
cluded in each substring. 
On the one hand, the anchor methods (# 12-
17) tried on the interval of 70-500 anchors report 
on relatively high numbers of suspicious docu-
ment pairs, especially at the MS level. According 
to our expert, these high numbers are rather ex-
aggerated. The reason might be that such fix 
numbers of anchors are not suitable for detection 
of similar papers in various degrees of similarity. 
 
Table 1. Results of the 38 implemented me-
thods for 100 tested papers. 
# #(k) Method IDN VHS HS MS 
1 1(3) FF(3) 9 0 2 1 
2 1(4) FF(4) 9 0 1 1 
3 2(3) SSF(3) 0 0 0 9 
4 2(4) SSF(4) 0 0 0 9 
5 3(3) S2F(3) 9 0 2 2 
6 3(4) S2F(4) 9 0 1 1 
7 4(3) S3F(3) 0 0 9 0 
8 4(4) S3F(4) 0 0 9 0 
9 5(3) RDF(3) 1 5 1 3 
10 5(4) RDF(4) 1 6 0 3 
11 6 CA 9 0 1 0 
12 7(3) SF(3) 9 0 0 1 
13 7(4) SF(4) 9 0 0 1 
14 8(3) SSS(3) 0 0 0 9 
15 8(4) SSS(4) 0 0 0 9 
16 9(3) S2S(3) 9 0 0 1 
17 9(4) S2S(4) 9 0 0 1 
18 10(3) S3S(3) 0 0 9 0 
19 10(4) S3S(4) 0 0 9 0 
20 11(3) RDS(3) 0 0 0 1 
21 11(4) RDS(4) 0 0 0 0 
22 12 AFW 4 6 18 2772 
23 13 AFS 6 3 10 708 
24 14 AF 6 4 4 313 
25 15 AR 4 6 19 2789 
26 16 ALW 4 6 9 500 
27 17 ALS 4 5 12 704 
28 18 CR 9 0 1 3 
29 19 CARA 8 2 1 0 
30 20 CARM 8 0 2 0 
31 21 CAMA 9 0 1 0 
32 22 CAMM 9 0 0 1 
33 23 CAEA 9 0 1 0 
34 24 CAEM 9 0 0 1 
35 25 CRFA 8 0 3 0 
36 26 CRFM 8 0 2 0 
37 27 CRMA 8 0 3 0 
38 28 CRMM 8 0 1 1 
425
 On the other hand, the SSF(k), S3F(k), 
S3S(k), RDF(k), and RDS(k) methods report on 
relatively very low numbers of suspicious docu-
ment pairs. According to our expert, these num-
bers are too low. The reason for this finding 
might be that these methods are quite stringent 
for detection of similar document pairs.  
The full fingerprint methods: FF(k), S2F(k) 
and the selective fingerprint methods SF(k), and 
S2S(k) present very similar results, which are 
reasonable according to our expert. Most of these 
methods report on 9 IDN, 0 VHS, 0-2 HS, and 1-
2 MS document pairs. The full fingerprint me-
thods report on slightly more HS and MS docu-
ment pairs. According to our expert, these me-
thods are regarded as the best. 
Our novel methods: CA and CR also report 
on 9 IDN, 0 VHS, one HS, and 0 or 3 MS docu-
ment pairs, respectively. The sum (10-13) of the 
IDN, VHS, HS and MS document pairs found by 
the best full and selective fingerprint methods 
mentioned in the last paragraph is the same sum 
of the IDN, VHS, HS and MS document pairs 
found by the CA and CR methods. That is, the 
CA and CR are very close in their quailty to the 
best methods. However, the CA and the CR have 
a clear advantage on the other methods. They 
check a rather small portion of the papers, and 
therfore their run time is much more smaller.  
On the one hand, CR seems to be better than 
CA (and even the best selective fingerprint me-
thods SF(k), and S2S(k)) because it reports on 
more MS document pairs, which means that CR 
is closer in its quality to the best full fingerprint 
methods. On the other hand, according to our 
expert CA is better than CR, since CR has more 
detection failures. 
The combinations of CA and/or CR and/or 
the methods defined for the three thirds of the 
papers report on results that are less or equal 
from the viewpoint of their quality to CA or CR. 
Several general conclusions can be drawn 
from the experimental results as follows: 
(1) There are 9 documents (in the examined 
corpus) that are identical to one of the 100 tested 
papers. According to our expert, each one of 
these documents is IDN to a different paper from 
the 100 tested papers. This means that at least 
9% of our random tested papers have IDN files 
in a corpus that contains 10, 099 files (for each 
test file). 
(2) Several papers that have been found as 
IDN might be legal copies. For example: (a) by 
mistake, the same paper might be stored twice at 
the same conference website or (b) the paper, 
which is stored in its conference website might 
also be stored at its author's website. 
(3) All the methods that run with two possible 
values of k (3 or 4 words) present similar results 
for the two values of k.  
(4) FF(3) found as better than FF(4). FF(3) 
discovers 9 IDN papers, 2 HS papers, and 1 MS 
paper. These results were approved by a human 
expert.  FF(4) missed one paper. One HS paper 
identified by FF(3) was identified as MS by 
FF(4) and one MS paper identified by FF(3) was 
identified as less than MS by FF(4). Moreover, 
also for other methods, variants with K=3 were 
better or equal to those with K=4. The main rea-
son for these findings might be that the variants 
with K=4 check less substrings because the 
checks are done for each sentence. Substrings 
that end at the sequential sentence are not 
checked. Therefore, it is likely that additional 
equal substrings from the checked papers are not 
identified.  
 (5) S2F(3) discovers one more MS paper 
compared to FF(3). According to the human ex-
pert, the similarity measure of this paper should 
be less than MS. Therefore, we decided to select 
FF(3) as the best method.  
(6) FF(3)'s run time is very high since it 
works on overlapping substrings for the whole 
papers. 
(7) Our two novel methods: CA and CR are 
among the best methods for identification of var-
ious levels of plagiarism. As mentioned before, 
CA was found as a better predictor. 
5.3 Selection of Methods and Experiment II 
Sixteen methods out of the thirty-eight methods 
presented in Table 1, were selected for additional 
experiments. All the methods with k=4, the anc-
hor methods, SSF, S3F, S3S, RDF, and RDS me-
thods were omitted, due to their faulty results (as 
explained above). The remaining 16 methods 
(with k=3) are: FF, S2F, S2F, SF, S2S and all our 
12 baseline methods: CA, and CR- CRMM. 
Table 2 presents the results of these methods 
regarding the corpus of 10,100 documents. Since 
we selected less than half of the original methods 
426
we allow ourselves to test 1,000 documents in-
stead of 100. 
 
 
Table 2. Results of the 16 selected methods for 
1,000 tested papers. 
  
 Again, according to our expert, FF has been 
found as the best predictive method. Surprising-
ly, CA achieved the second best results with one 
additional VHS paper. 11 HS documents and 5 
MS documents have been identified by CA as by 
FF. The meaning of this finding is that the ab-
stracts in almost all the simple similar documents 
were not significantly changed. That is, the au-
thors of the non-IDN documents did not invest 
enough to change their abstracts.  
 CR indentified 41 documents as identical. The 
reason for this is probably because 3 additional 
papers have the same reference section as in 3 
other tested papers, although these 3 document 
pairs are different in other sections. Furthermore, 
CR reports on relatively high number of suspi-
cious document pairs, especially at the MS level. 
The meaning of this finding is that the references 
in many document pairs are not significantly dif-
ferent although these documents have larger dif-
ferences in other sections. Consequently, combi-
nations with CA achieved better results than 
combinations with CR. 
 A very important finding is that the run time 
of FF was very expensive (one day, 3 hours and 
57.3 minutes) compared to the run time of CA (9 
hours and 16.7 minutes). In other words, CA 
achieved almost the same results as FF but more 
efficiently. 
5.4 An Error Analysis 
The selected methods presented in Table 2 were 
analyzed according to the results of FF. Table 3 
shows the distributions of false true positives 
(TP), false positives (FP), true negatives (TN), 
and false negatives (FN), regarding the 10,099 
retrieved documents for the 1,000 tested docu-
ment.  
The false positive rate is the proportion in 
percents of positive test results (i.e., a plagiarism 
was identified by a baseline function) that are 
really negative values (i.e., the truth is that there 
is no plagiarism). The false negative rate is the 
proportion of negative test results that are really 
positive values. 
 
 
Table 3. Distributions of the various possible 
statistical results. 
 
 
FF is the only method that detects all cases of 
simple plagiarism. According to FF, there are 
0.534% true positives. That is, 54 papers out of 
10,099 are suspected as plagiarized versions of 
# Method IDN VHS HS MS Time 
d:h:m 
1 FF 38 0 11 5 1:3:57.3 
2 S2F 41 1 10 18 32:00.0 
3 SF 37 1 1 6 31:12.2 
4 S2 38 1 1 14 20:10.8 
5 CA 38 1 11 5 09:16.7 
6 CR 41 2 11 67 05:57.7 
7 CARA 33 2 1 21 31:53.4 
8 CARM 30 4 1 5 33:40.1 
9 CAMA 38 0 5 6 11:26.5 
10 CAMM 38 0 3 4 10:09.8 
11 CAEA 38 0 6 7 10:42.1 
12 CAEM 38 0 3 4 12:35.3 
13 CRFA 32 1 3 25 54:20.7 
14 CRFM 30 3 3 6 54:10.0 
15 CRMA 33 2 3 25 58:52.2 
16 CRMM 30 2 2 5 54:17.7 
# Method TP FP TN FN 
1 FF 0.534 0 99.465 0
2 S2F 0.524 0.168 99.296 0.010
3 SF 0.425 0.019 99.445 0.108
4 S2 0.435 0.099 99.366 0.099
5 CA 0.534 0.010 99.455 0
6 CR 0.534 0.663 98.801 0
7 CARA 0.386 0.178 99.287 0.148
8 CARM 0.356 0.039 99.425 0.178
9 CAMA 0.475 0 99.465 0.059
10 CAMM 0.445 0 99.465 0.089
11 CAEA 0.485 0.020 99.445 0.049
12 CAEM 0.445 0 99.465 0.089
13 CRFA 0.396 0.207 99.257 0.138
14 CRFM 0.376 0.039 99.425 0.158
15 CRMA 0.405 0.217 99.247 0.128
16 CRMM 0.366 0.020 99.445 0.168
427
54 papers of the 1,000 tested papers. This finding 
fits the results of FF(3) in Table 2, where there 
are 38 IDN, 11 HS, and 5 MS. 
 CA, the second best method has 0% false po-
sitives, and 0.01% false negatives, which means 
that CA identified one suspected plagiarized ver-
sion that is really a non-plagiarized document. 
This finding is presented in Table 2, where CA 
identified 55 suspected plagiarized documents, 
one more than FF.  
 CR has 0% false positives, and 0.663% false 
negatives, which means that CR identified 67 
suspected plagiarized versions that are really 
non-plagiarized documents. This finding is pre-
sented in Table 2, where CR identified 121 sus-
pected plagiarized documents, 67 more than FF.  
6 Illustrative Example 
Due to space limitations, we briefly present an 
illustrative example of comparison between a 
couple of papers found as HS (High Similar) 
according to FF(3), the best detection method. 
However, this is not a case of plagiarism, since 
the longer paper cited the shorter one as needed 
and there are differences in the submission length 
and quality. 
The tested paper (Snider and Diab, 2006A) 
contains 4 pages and it was published on June 
06. The retrieved paper (Snider and Diab, 
2006B) contains 8 pages and it was published a 
month later. The title of the tested paper is 
identical to the first eight words of the title of the 
retrieved paper. The authors of both papers are 
the same and their names appear in the same 
order. Most of the abstracts are the same. One of 
the main differences is the report of other results 
(probably updated results).  
A relatively big portion of the beginning of 
the Introduction section in both papers is 
identical. Very similar sentences are found at the 
beginning of different sections (Section 2 in the 
4-page paper and Section 3 in the the 8-page 
paper).  
Many sentences or phrases from the rest of 
the papers are identical and some are very similar 
(e.g., addition of 'The' before "verbs are 
classified" in the abstract of the retrieved paper. 
It is important to point that the authors in their 
8-page paper wrote "This paper is an extension 
of our previous work in Snider and Diab (2006)". 
This sentence together with the detailed 
reference prove that the authors cite their 
previous work as required. 
 
Concerning the references in both papers, at 
the first glance we found many differences be-
tween the two papers. The short paper contains 
only 7 references while the larger paper contains 
14 references. However, a second closer look 
identifies that 5 out of the 7 references in the 
shorter paper are found in the reference section 
of the larger paper. Indeed, regarding the refer-
ence sections we did not find HS; but we have to 
remember that the larger paper include 8 pages 
twice than the shorter paper and therfore, more 
references could be included. 
7 Conclusions and Future Work 
To the best of our knowledge, we are the first to 
implement the CA and CR methods that compare 
two basic and important sections in academic 
papers: the abstract and references, respectively. 
In addition, we defined combinations of them. 
Furthermore, we implemented methods defined 
for the three thirds of the paper. These methods 
were combined with CA or CR in various va-
riants. All in total, we have defined 12 new base-
line methods.  
Especially CA and also CR are among the 
best methods for identification of various levels 
of plagiarism. In contrast to the best full and 
selective fingerprint methods, CA and CR check 
a rather small portion of the papers, and 
therefore, their run time is much more smaller. 
The success of CA and CR teaches us that 
most documents that are suspected as simple 
plagiarized papers include abstracts and 
references, which have not been significantly 
changed compared to other documents or vice 
versa. 
There is a continuous need for automatic 
detection of plagiarism due to web influences, 
and advanced and more complex levels of 
plagiarism. Therefore, some possible future 
directions for research are: (1) Developing new 
kinds of selective fingerprint methods and new 
combinations of methods to improve detection, 
(2) Applying this research to larger and/or other 
corpora, and (3) Dealing with complex kinds of 
plagiarism, e.g., the use of synonyms, 
paraphrases, and transpositions of active 
sentences to passive sentences and vice versa. 
428
References 
Bernstein, Y., and Zobel, J., 2004. A Scalable System 
for Identifying Co-Derivative Documents. In Pro-
ceedings of 11th International Conference on 
String Processing and Information Retrieval 
(SPIRE), vol. 3246, pp. 55-67. 
Bretag, T., and Carapiet, S., 2007. A Preliminary 
Study to Identify the Extent of Self Plagiarism in 
Australian Academic Research. Plagiary, 2(5), pp. 
1-12. 
Collberg, C., Kobourov, S., Louie, J., and Slattery, T., 
2005. Self-Plagiarism in Computer Science. Com-
munications of the ACM, 48(4), pp. 88-94. 
Heintze, N., 1996. Scalable Document Fingerprinting. 
In Proceedings of the USENIX Workshop on Elec-
tronic Commerce, Oakland California. 
Hoad, T. C., and Zobel, J., 2003. Methods for Identi-
fying Versioned and Plagiarised Documents. Jour-
nal of the American Society for Information 
Science and Technology, Vol 54(3), pp. 203-215. 
IEEE, 2010. Introduction to the Guidelines for Han-
dling Plagiarism Complaints. 
http://www.ieee.org/publications_standards/publica
tions/rights/plagiarism.html. 
Keuskamp, D., and Sliuzas, R., 2007. Plagiarism Pre-
vention or Detection? The Contribution of Text-
Matching Software to Education about Academic 
Integrity. Journal of Academic Language and 
Learning, Vol 1(1), pp. 91-99. 
Library and Information Services, 2010. Cyprus Uni-
versity of Technology in Scopus, 
http://www.cut.ac.cy/library/english/services/refere
nces_en.html#plagiarism.  
Loui, M. C., 2002. Seven Ways to Plagiarize: Han-
dling Real Allegations of Research Misconduct. 
Science and Engineering Ethics, 8, pp. 529-539. 
 
 
 
 
 
 
 
 
 
 
Lyon, C., Malcolm, J., and Dickerson, B., 2001. De-
tecting Short Passages of Similar Text in Large 
Document Collections. In Proceedings of Confe-
rence on Empirical Methods in Natural Language 
Processing, pp. 118-125. 
Manber, U., 1994. Finding Similar Files in a Large 
File System, In Proceedings of the USENIX Tech-
nical Conference, pp. 1-10. 
Monostori1, K., Finkel, R., Zaslavsky, A., Hodasz, 
G., and Patke, M., 2002. Comparison of Overlap 
Detection Techniques. In Proceedings of the 2002 
International Conference on Computational 
Science, Lecture Notes in Computer Science, vol 
2329, pp. 51-60. 
Shivakumar, N., and Garcia-Molina, H., 1996. Build-
ing a Scalable and Accurate Copy Detection Me-
chanism. In Proceedings of the International Con-
ference on Digital Libraries, pp. 160-168. 
Snider, N., and Diab, M., JUNE 2006A. Unsupervised 
Induction of Modern Standard Arabic Verb 
Classes. In Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the ACL, pp. 153- 156, June 2006. 
Snider, N., and Diab, M., JULY 2006B. Unsupervised 
Induction of Modern Standard Arabic Verb Classes 
Using Syntactic Frames and LSA. In Proceedings 
of the COLING/ACL 2006 Main Conference Poster 
Sessions, pp. 795- 802.  
Sorokina, D., Gehrke, J., Warner, S., Ginsparg, P., 
2006. Plagiarism Detection in arXiv. In Proceed-
ings of Sixth International Conference on Data 
Mining (ICDM), pp. 1070-1075. 
Wikipedia, 2010. Plagiarism. 
http://en.wikipedia.org/wiki/Plagiarism. 
Witten, I. H., Moffat, A., and Bell, T. C., 1999. Man-
aging Gigabytes: Compressing and Indexing Doc-
uments and Images. Morgan Kaufmann, second 
edition. 
429
Proceedings of the 25th International Conference on Computational Linguistics, pages 121?123,
Dublin, Ireland, August 23-29 2014.
 
Keyphrase Extraction using Textual and Visual Features
*
                                                          
* This work is licensed under a Creative Commons Attribution 4.0 International Licence. License de-
tails: http://creativecommons.org/licenses/by/4.0/ 
Yaakov HaCohen-Kerner1, Stefanos Vrochidis2, Dimitris Liparas2, Anastasia Moumtzidou2, 
Ioannis Kompatsiaris2 
1 Dept. of Computer Science, Jerusalem College of Technology ? Lev Academic Center,  
21 Havaad Haleumi St., P.O.B. 16031, 9116001 Jerusalem, Israel, kerner@jct.ac.il 
2 Information Techologies Institute, Centre for Research and Technology Hellas, Thermi-
Thessaloniki, Greece, {stefanos, dliparas, moumtzid, ikom}@iti.gr 
 
Abstract 
Many current documents include multimedia consisting of text, images and embedded videos. This pa-
per presents a general method that uses Random Forests to automatically extract keyphrases that can be 
used as very short summaries and to help in retrieval, classification and clustering processes.  
1   Introduction 
A keyphrase is an important concept, presented either as a single word (unigram), e.g.: 'extraction', 
'keyphrase' or as a collocation, i.e., a meaningful group of two or more words, e.g.: 'keyphrase extrac-
tion'. Keyphrases can be regarded as very short summaries and can be used for representing documents 
in retrieval, classification and clustering problems.  
Nowadays, many documents (e.g. web pages, articles) include multimedia consisting of text, imag-
es and embedded videos. In this case, the keyphrase extraction process should not be limited to the 
textual data but also consider the audiovisual data.  
In this context, this paper proposes a novel framework for automatic keyphrase extraction from 
documents containing text and images based on supervised learning and textual and visual features.  
2  Baseline Methods for Keyphrase Extraction 
In this section, we introduce the baseline methods we use for keyphrase extraction using textual and 
visual information.  
2.1 Textual Keyprhase Extraction 
In all methods, words and terms that have a grammatical role for the language are excluded from the 
key words list according to Fox's stop list. This stop list contains 421 high frequency stop list words 
(e.g.: we, this, and, when, in, usually, also, near). 
(1) Term Frequency (TF): This method rates a term according to the number of its occurrences in 
the text. Only the N terms with the highest TF in the document are selected. 
(2) Term length (TL): TL rates a term according to the number of the words included in the term. 
(3) First N Terms (FN): Only the first N terms in the document are selected. The assumption is that 
the most important keyphrases are found at the beginning of the document because people tend to 
place important information at the beginning. This method is based on the baseline summariza-
tion method which chooses the first N sentences. This simple method provides a relatively strong 
baseline for the performance of any text-summarization method. 
(4) Last N Terms (LN): Only the last N terms in the document are selected. The assumption is that 
the most important keyphrases are found at the end of the document because people tend to place 
their important keyphrases in their conclusions which are usually placed near to the end. 
121
 (5) At the Beginning of its Paragraph (PB): This method rates a term according to its relative posi-
tion in its paragraph. The assumption is that the most important keyphrases are likely to be found 
close to the beginning of their paragraphs. 
(6) At the End of its Paragraph (PE): This method rates a term according to its relative position in 
its paragraph. The assumption is that the most important keyphrases are likely to be found close 
to end of their paragraphs. 
(7) Resemblance to Title (RT): This method rates a term according to the resemblance of its sen-
tence to the title of the article. Sentences that resemble the title will be granted a higher score. 
(8) Maximal Section Headline Importance (MSHI): This method rates a term according to its most 
important presence in a section or headline of the article. It is a known that some parts of papers 
are more important from the viewpoint of presence of keyphrases. Such parts can be headlines 
and sections as: abstract, introduction and conclusions. 
(9) Accumulative Section Headline Importance (ASHI): This method is very similar to the previ-
ous one. However, it rates a term according to all its presences in important sections or headlines 
of the article. 
(10) Negative Brackets (NBR): Phrases found in brackets are not likely to be keyphrases. Therefore, 
they are defined as negative phrases, and will grant negative scores. 
These methods were applied to extract and learn keyphrases from scientific articles (HaCohen-
Kerner et al., 2005). 
2.2 Visual Keyprhase Extraction 
On the other hand, visual keyphrase extraction is performed for a pre-defined set of keyphrases (e.g. 
demonstration, moving car, etc.). The predefined keyphrases are selected in order to be relevant to the 
domain of interest. In the following, low level visual features (SIFT, SURF) are extracted (Markato-
poulou, et al., 2013). We apply supervised machine learning using Random Forests (RF) (Breiman, 
2001) to detect the presence of each concept in an image. RF have been successfully applied to several 
image classification problems (e.g. (Bosch et al., 2007; Xu et al., 2012)). Moreover, an important mo-
tivation for using RF was the application of late fusion based on the RF operational capabilities, which 
is discussed below. 
In the training phase, the feature vectors from each low level feature vector are used as input for the 
construction of a single RF. The training set can be constructed either manually or automatically. In 
the automatic case, we submit a text query to a general purpose web search engine (e.g. Google, Bing) 
to retrieve relevant images, while irrelevant images can be selected randomly from the web. From the 
RFs that are constructed (one for each descriptor), we compute the weights for each modality in the 
following way. From the out-of-bag (OOB) error estimate of each modality?s RF, the corresponding 
OOB accuracy values are computed. These values are computed for each concept separately. Then the 
values are normalized and serve as weights for the different modalities. Finally, each image is repre-
sented with a vector that includes the scores for each predefined visual keyphrase.  
It should be noted that the visual concept/keyphrase detectors perform decently for specific visual 
concepts (e.g. news studio: 0,5 MEIAP (Mean Extended Inferred Average Precision)), while for some 
others (e.g. bridge: 0,02MEIAP) the performance is very low (Markatopoulou, et al., 2013). Therefore, 
the representation is based on visual concepts for which the trained models can perform decently.  
3   The Proposed Supervised Extraction Model 
Our model, in general, is composed of the following steps:  
    For each document: 
(1) Extract all possible n-grams (n=1, 2, 3) that do not contain stop-list words.  
(2) Transform these n-grams into lower case. 
(3) Apply all baseline textual extraction methods on these n-grams.  
(4) Apply variable selection using Random Forests on all textual features (the results of the textual 
baseline methods) in order to find the best combination of the textual features (Genuer, et al. 
2010).  
122
 (5) Extract visual keyphrases for each image and calculate the average score for each visual 
keyphrase to represent the document. 
(6) Apply variable selection using Random Forests on all visual features in order to find the best per-
forming visual features (Genuer, et al. 2010). 
(7) After the feature selection two fusion techniques are investigated: 
a. Early fusion: Concatenation of the textual and visual vectors in a single vector. In the case 
of unsupervised tasks (e.g. retrieval, clustering) the L1 distances between these vectors are 
considered to compute similarity measures. In supervised tasks (e.g. classification) we train 
a RF with the concatenated vector using as training set manually annotated documents. 
b. Weighted late fusion: In the case of unsupervised tasks similarity scores are computed in-
dependently for each modality and the results are fused. In order to calculate the weights a 
regression model based on Support Vector Machines is applied. In the case of supervised 
tasks we train two RF (i.e. one for each modality) using a manually constructed training set 
and finally we apply weighted late fusion based on the OOB error estimate using the ap-
proach mentioned in chapter 2. 
4   Conclusions and Future Work 
The proposed approach is work in progress so specific results are not yet available. However, initial 
results using weighted late fusion (based on OOB estimate) of textual features and visual low level 
features for a representative (i.e. histograms and not concepts) have shown that the results are im-
proved when compared to the ones generated with using only textual features. The next steps of this 
work include application of the proposed method to retrieval, clustering and classification problems of 
web pages and news articles, which include multimodal information such as text and images. 
Future directions for research are: (1) Developing additional baseline methods for keyphrase ex-
traction, (2) Applying other ML methods in order to find the most effective combination between 
these baseline methods, (3) Conducting more experiments using additional documents from additional 
domains (5) Development of Methodology for predefined visual concept selection, and (6) Applying 
ML to extract keyphrases using both textual and low level visual features. 
Concerning research on additional domains, there are many potential research directions. For in-
stance the following research questions can be addressed: (1) Which baseline extraction methods are 
good for which domains? (2) Which are the specific reasons for methods to perform better or worse on 
different domains? (3) Which are the guidelines to choose the correct methods for a certain domain? 
(4) Can the appropriateness of a method for a domain be estimated automatically?  
Acknowledgment: This work is supported by MULTISENSOR project (FP7-610411) partially funded 
by the European Commission. The authors would like to acknowledge networking support by the 
COST Action IC1307: The European Network on Integrating Vision and Language (iV&L Net) and 
the COST Action IC1002: MUMIA. 
References 
1. Anna Bosch., Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random forests and 
ferns. In ICCV, pp. 1-8. 
2. Leo Breiman. 2001. Random Forests.  In Machine Learning, 45(1): 5-32. 
3. Christopher Fox. 1990. A Stop List for General Text. ACM-SIGIR Forum, 24, pp. 19?35. 
4. Yaakov HaCohen-Kerner, Zuriel Gross, and Asaf Masa. 2005. Automatic extraction and learning of 
keyphrases from scientific articles. In Computational Linguistics and Intelligent Text Processing, pp. 657-
669, Springer Berlin Heidelberg. 
5. Fotini Markatopoulou, Anastasia Moumtzidou, Christos Tzelepis, Kostas Avgerinakis, Nikolaos Gkalelis, 
Stefanos Vrochidis, Vasileios Mezaris, and Ioannis Kompatsiaris. 2013. ?ITI-CERTH participation to 
TRECVID 2013,? in TRECVID 2013 Workshop, Gaithersburg, MD, USA. 
6. Baoxun Xu, Yunming Ye, and Lei Nie. 2012. An improved random forest classifier for image classification. 
In, International Conference on Information and Automation (ICIA), pp. 795-800, IEEE. 
7. Robin Genuera, Jean-Michel Poggi, and Christine Tuleau-Malot. 2010. Variable Selection using Random 
Forests, In Pattern Recognition Letters 31(14):2225-2236. 
123
