Proceedings of ACL-08: HLT, pages 923?931,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Credibility Improves Topical Blog Post Retrieval
Wouter Weerkamp
ISLA, University of Amsterdam
weerkamp@science.uva.nl
Maarten de Rijke
ISLA, University of Amsterdam
mdr@science.uva.nl
Abstract
Topical blog post retrieval is the task of rank-
ing blog posts with respect to their relevance
for a given topic. To improve topical blog post
retrieval we incorporate textual credibility in-
dicators in the retrieval process. We consider
two groups of indicators: post level (deter-
mined using information about individual blog
posts only) and blog level (determined using
information from the underlying blogs). We
describe how to estimate these indicators and
how to integrate them into a retrieval approach
based on language models. Experiments on
the TREC Blog track test set show that both
groups of credibility indicators significantly
improve retrieval effectiveness; the best per-
formance is achieved when combining them.
1 Introduction
The growing amount of user generated content avail-
able online creates new challenges for the informa-
tion retrieval (IR) community, in terms of search and
analysis tasks for this type of content. The introduc-
tion of a blog retrieval track at TREC (Ounis et al,
2007) has created a platform where we can begin to
address these challenges. During the 2006 edition
of the track, two types of blog post retrieval were
considered: topical (retrieve posts about a topic)
and opinionated (retrieve opinionated posts about a
topic). Here, we consider the former task.
Blogs and blog posts offer unique features that
may be exploited for retrieval purposes. E.g.,
Mishne (2007b) incorporates time in a blog post
retrieval model to account for the fact that many
blog queries and posts are a response to a news
event (Mishne and de Rijke, 2006). Data quality
is an issue with blogs?the quality of posts ranges
from low to edited news article-like. Some ap-
proaches to post retrieval use indirect quality mea-
sures (e.g., elaborate spam filtering (Java et al,
2007) or counting inlinks (Mishne, 2007a)).
Few systems turn the credibility (Metzger, 2007)
of blog posts into an aspect that can benefit the re-
trieval process. Our hypothesis is that more credible
blog posts are preferred by searchers. The idea of us-
ing credibility in the blogosphere is not new: Rubin
and Liddy (2006) define a framework for assessing
blog credibility, consisting of four main categories:
blogger?s expertise and offline identity disclosure;
blogger?s trustworthiness and value system; infor-
mation quality; and appeals and triggers of a per-
sonal nature. Under these four categories the authors
list a large number of indicators, some of which can
be determined from textual sources (e.g., literary ap-
peal), and some of which typically need non-textual
evidence (e.g., curiosity trigger); see Section 2.
We give concrete form to Rubin and Liddy
(2006)?s indicators and test their impact on blog post
retrieval effectiveness. We do not consider all indi-
cators: we only consider indicators that are textual in
nature, and to ensure reproducibility of our results,
we only consider indicators that can be derived from
the TRECBlog06 corpus (and that do not need addi-
tional resources such as bloggers? profiles that may
be hard to obtain for technical or legal reasons).
We detail and implement two groups of credibility
indicators: post level (these use information about
individual posts) and blog level (these use informa-
tion from the underlying blogs). Within the post
level group, we distinguish between topic depen-
dent and independent indicators. To make matters
concrete, consider Figure 1: both posts are relevant
to the query ?tennis,? but based on obvious surface
level features of the posts we quickly determine Post
2 to be more credible than Post 1. The most obvious
features are spelling errors, the lack of leading capi-
tals, and the large number of exclamation marks and
923
Post 1
as for today (monday) we had no school! yaay
labor day. but we had tennis from 9-11 at the
highschool. after that me suzi melis & ashley
had a picnic at cecil park and then played ten-
nis. i just got home right now. it was a very
very very fun afternoon. (...) we will have a
short week. mine will be even shorter b/c i
wont be there all day on friday cuz we have
the Big 7 Tournament at like keystone oaks or
sumthin. so i will miss school the whole day.
Post 2
Wimbledon champion Venus Williams has
pulled out of next week?s Kremlin Cup with
a knee injury, tournament organisers said on
Friday. The American has not played since
pulling out injured of last month?s China
Open. The former world number one has been
troubled by various injuries (...) Williams?s
withdrawal is the latest blow for organisers af-
ter Australian Open champion and home fa-
vorite Marat Safin withdrew (...).
Figure 1: Two blog posts relevant to the query ?tennis.?
personal pronouns?i.e., topic independent ones?
and the fact that the language usage in the second
post is more easily associated with credible infor-
mation about tennis than the language usage in the
first post?i.e., a topic dependent feature.
Our main finding is that topical blog post retrieval
can benefit from using credibility indicators in the
retrieval process. Both post and blog level indi-
cator groups each show a significant improvement
over the baseline. When we combine all features
we obtain the best retrieval performance, and this
performance is comparable to the best performing
TREC 2006 and 2007 Blog track participants. The
improvement over the baseline is stable across most
topics, although topic shift occurs in a few cases.
The rest of the paper is organized as follows. In
Section 2 we provide information on determining
credibility; we also relate previous work to the cred-
ibility indicators that we consider. Section 3 speci-
fies our retrieval model, a method for incorporating
credibility indicators in our retrieval model, and es-
timations of credibility indicators. Section 4 gives
the results of our experiments aimed at assessing
the contribution of credibility towards blog post re-
trieval effectiveness. We conclude in Section 5.
2 Credibility Indicators
In our choice of credibility indicators we use (Ru-
bin and Liddy, 2006)?s work as a reference point.
We recall the main points of their framework and
relate our indicators to it. We briefly discuss other
credibility-related indicators found in the literature.
2.1 Rubin and Liddy (2006)?s work
Rubin and Liddy (2006) proposed a four factor an-
alytical framework for blog-readers? credibility as-
sessment of blog sites, based in part on evidential-
ity theory (Chafe, 1986), website credibility assess-
ment surveys (Stanford et al, 2002), and Van House
(2004)?s observations on blog credibility. The four
factors?plus indicators for each of them?are:
1. blogger?s expertise and offline identity disclo-
sure (a: name and geographic location; b: cre-
dentials; c: affiliations; d: hyperlinks to others;
e: stated competencies; f : mode of knowing);
2. blogger?s trustworthiness and value system (a:
biases; b: beliefs; c: opinions; d: honesty; e:
preferences; f : habits; g: slogans)
3. information quality (a: completeness; b: ac-
curacy; c: appropriateness; d: timeliness; e:
organization (by categories or chronology); f :
match to prior expectations; g: match to infor-
mation need); and
4. appeals and triggers of a personal nature (a:
aesthetic appeal; b: literary appeal (i.e., writing
style); c: curiosity trigger; d: memory trigger;
e: personal connection).
2.2 Our credibility indicators
We only consider credibility indicators that avoid
making use of the searcher?s or blogger?s identity
(i.e., excluding 1a, 1c, 1e, 1f, 2e from Rubin and
Liddy?s list), that can be estimated automatically
from available test collections only so as to facilitate
repeatability of our experiments (ruling out 3e, 4a,
4c, 4d, 4e), that are textual in nature (ruling out 2d),
and that can be reliably estimated with state-of-the-
art language technology (ruling out 2a, 2b, 2c, 2g).
For reasons that we explain below, we also ignore
the ?hyperlinks to others? indicator (1d).
The indicators that we do consider?1b, 2f, 3a,
3b, 3c, 3d, 3f, 3g, 4b?are organized in two groups,
924
depending on the information source that we use
to estimate them, post level and blog level, and the
former is further subdivided into topic independent
and topic dependent. Table 1 lists the indicators we
consider, together with the corresponding Rubin and
Liddy indicator(s).
Let us quickly explain our indicators. First, we
consider the use of capitalization to be an indicator
of good writing style, which in turn contributes to
a sense of credibility. Second, we identify West-
ern style emoticons (e.g., :-) and :-D) in blog
posts, and assume that excessive use indicates a less
credible blog post. Third, words written in all caps
are considered shouting in a web environment; we
consider shouting to be indicative for non-credible
posts. Fourth, a credible author should be able to
write without (a lot of) spelling errors; the more
spelling errors occur in a blog post, the less credi-
ble we consider it to be. Fifth, we assume that cred-
ible texts have a reasonable length; the text should
supply enough information to convince the reader of
the author?s credibility. Sixth, assuming that much
of what goes on in the blogosphere is inspired by
events in the news (Mishne and de Rijke, 2006), we
believe that, for news related topics, a blog post is
more credible if it is published around the time of
the triggering news event (timeliness). Seventh, our
semantic indicator also exploits the news-related na-
ture of many blog posts, and ?prefers? posts whose
language usage is similar to news stories on the
topic. Eighth, blogs are a popular place for spam-
mers; spam blogs are not considered credible and we
want to demote them in the search results. Ninth,
comments are a notable blog feature: readers of a
blog post often have the possibility of leaving a com-
ment for other readers or the author. When peo-
ple comment on a blog post they apparently find the
post worth putting effort in, which can be seen as an
indicator of credibility (Mishne and Glance, 2006).
Tenth, blogs consist of multiple posts in (reverse)
chronological order. The temporal aspect of blogs
may indicate credibility: we assume that bloggers
with an irregular posting behavior are less credible
than bloggers who post regularly. And, finally, we
consider the topical fluctuation of a blogger?s posts.
When looking for credible information we would
like to retrieve posts from bloggers that have a cer-
tain level of (topical) consistency: not the fluctuating
indicator topic de- post level/ related Rubin &
pendent? blog level Liddy indicator
capitalization no post 4b
emoticons no post 4b
shouting no post 4b
spelling no post 4b
post length no post 3a
timeliness yes post 3d
semantic yes post 3b, 3c
spam no blog 3b, 3c, 3f, 3g
comments no blog 1b
regularity no blog 2f
consistency no blog 2f
Table 1: Credibility indicators
behavior of a (personal) blogger, but a solid interest.
2.3 Other work
In a web setting, credibility is often couched in
terms of authoritativeness and estimated by exploit-
ing the hyperlink structure. Two well-known exam-
ples are the PageRank and HITS algorithms (Liu,
2007), that use the link structure in a topic indepen-
dent or topic dependent way, respectively. Zhou and
Croft (2005) propose collection-document distance
and signal-to-noise ratio as priors for the indication
of quality in web ad hoc retrieval. The idea of using
link structure for improving blog post retrieval has
been researched, but results do not show improve-
ments. E.g., Mishne (2007a) finds that retrieval per-
formance decreased. This confirms lessons from
the TREC web tracks, where participants found no
conclusive benefit from the use of link information
for ad hoc retrieval tasks (Hawking and Craswell,
2002). Hence, we restrict ourselves to the use of
content-based features for blog post retrieval, thus
ignoring indicator 1d (hyperlinks to others).
Related to credibility in blogs is the automatic as-
sessment of forum post quality discussed by Weimer
et al (2007). The authors use surface, lexical, syn-
tactic and forum-specific features to classify forum
posts as bad posts or good posts. The use of forum-
specific features (such as whether or not the post
contains HTML, and the fraction of characters that
are inside quotes of other posts), gives the highest
benefits to the classification. Working in the com-
munity question/answering domain, Agichtein et al
(2008) use a content features, as well non-content in-
formation available, such as links between items and
925
explicit quality ratings from members of the com-
munity to identify high-quality content.
As we argued above, spam identification may be
part of estimating a blog (or blog post?s) credibility.
Spam identification has been successfully applied in
the blogosphere to improve retrieval effectiveness;
see, e.g., (Mishne, 2007b; Java et al, 2007).
3 Modeling
In this section we detail the retrieval model that we
use, incorporating ranking by relevance and by cred-
ibility. We also describe how we estimate the credi-
bility indicators listed in Section 2.
3.1 Baseline retrieval model
We address the baseline retrieval task using a
language modeling approach (Croft and Lafferty,
2003), where we rank documents given a query:
p(d|q) = p(d)p(q|d)p(q)?1. Using Bayes? Theo-
rem we rewrite this, ignoring expressions that do not
influence the ranking, obtaining
p(d|q) ? p(d)p(q|d), (1)
and, assuming that query terms are independent,
p(d|q) ? p(d)
?
t?q p(t|?d)
n(t,q), (2)
where ?d is the blog post model, and n(t, q) denotes
the number of times term t occurs in query q. To
prevent numerical underflows, we perform this com-
putation in the log domain:
log p(d|q) ? log p(d) +
?
t?q
n(t, q) log p(t|?d) (3)
In our final formula for ranking posts based on rel-
evance only we substitute n(t, q) by the probability
of the term given the query. This allows us to assign
different weights to query terms and yields:
log p(d|q) ? log p(d) +
?
t?q
p(t|q) log p(t|?d). (4)
For our baseline experiments we assume that all
query terms are equally important and set p(t|q) set
to be n(t, q) ? |q|?1. The component p(d) is the topic
independent (?prior?) probability that the document
is relevant; in the baseline model, priors are ignored.
3.2 Incorporating credibility
Next, we extend Eq. 4 by incorporating estimations
of the credibility indicators listed in Table 1. Recall
that our credibility indicators come in two kinds?
post level and blog level?and that the post level
indicators can be topic indepedent or topic depen-
dent, while all blog level indicators are topic inde-
pendent. Now, modeling topic independent indi-
cators is easy?they can simply be incorporated in
Eq. 4 as a weighted sum of two priors:
p(d) = ? ? ppl(d) + (1? ?) ? pbl(d), (5)
where ppl(d) and pbl(d) are the post level and blog
level prior probability of d, respectively. The priors
ppl and pbl are defined as equally weighted sums:
ppl(d) =
?
i
1
5 ? pi(d)
pbl(d) =
?
j
1
4 ? pj(d),
where i ? {capitalization, emoticons, shouting,
spelling, post length} and j ? {spam, comments,
regularity, consistency}. Estimations of the priors
pi and pj are given below; the weighting parameter
? is determined experimentally.
Modeling topic dependent indicators is slighty
more involved. Given a query q, we create a query
model ?q that is a mixture of a temporal query model
?temporal and a semantic query model ?semantic:
p(t|?q) = (6)
? ? p(t|?temporal) + (1? ?) ? p(t|?semantic).
The component models ?temporal and ?semantic will
be estimated below; the parameter ? will be esti-
mated experimentally.
Our final ranking formula, then, is obtained by
plugging in Eq. 5 and 6 in Eq. 4:
log p(d|q) ? log p(d)
+ ? (
?
t p(t|q) ? log p(t|?d)) (7)
+ (1? ?) (
?
t p(t|?q) ? log p(t|?d)) .
3.3 Estimating credibility indicators
Next, we specify how each of the credibility indica-
tors is estimated; we do so in two groups: post level
and blog level.
926
3.3.1 Post level credibility indicators
Capitalization We estimate the capitalization
prior as follows:
pcapitalization(d) = n(c, s) ? |s|
?1, (8)
where n(c, s) is the number of sentences starting
with a capital and |s| is the number of sentences;
we only consider sentences with five or more words.
Emoticons The emoticons prior is estimated as
pemoticons(d) = 1? n(e, d) ? |d|
?1, (9)
where n(e, d) is the number of emoticons in the post
and |d| is the length of the post in words.
Shouting We use the following equation to esti-
mate the shouting prior:
pshouting(d) = 1? n(a, d) ? |d|
?1, (10)
where n(a, d) is the number of all caps words in blog
post d and |d| is the post length in words.
Spelling The spelling prior is estimated as
pspelling(d) = 1? n(m, d) ? |d|
?1, (11)
where n(m, d) is the number of misspelled (or un-
known) words and |d| is the post length in words.
Post length The post length prior is estimated us-
ing |d|, the post length in words:
plength(d) = log(|d|). (12)
Timeliness We estimate timeliness using the time-
based language models ?temporal proposed in (Li
and Croft, 2003; Mishne, 2007b). I.e., we use a news
corpus from the same period as the blog corpus that
we use for evaluation purposes (see Section 4.2). We
assign a timeliness score per post based on:
p(d|?temporal) = k
?1 ? (n(date(d), k) + 1) , (13)
where k is the number of top results from the initial
result list, date(d) is the date associated with doc-
ument d, and n(date(d), k) is the number of docu-
ments in k with the same date as d. For our initial
result list we perform retrieval on both the blog and
the news corpus and take k = 50 for both corpora.
Semantic A semantic query model ?semantic is
obtained using ideas due to Diaz and Metzler (2006).
Again, we use a news corpus from the same period
as the evaluation blog corpus and estimate ?semantic.
We issue the query to the external news corpus, re-
trieve the top 10 documents and extract the top 10
distinctive terms from these documents. These terms
are added to the original query terms to capture the
language usage around the topic.
3.3.2 Blog level credibility indicators
Spam filtering To estimate the spaminess of a
blog, we take a simple approach. We train an SVM
classifier on a labeled splog blog dataset (Kolari
et al, 2006) using the top 1500 words for both spam
and non-spam blogs as features. For each classified
blog d we have a confidence value s(d). If the clas-
sifier cannot make a decision (s(d) = 0) we set
pspam(d) to 0, otherwise we use the following to
transform s(d) into a spam prior pspam(d):
pspam(d) =
s(d)
2|s(d)|
+
?1 ? s(d)
2s(d)2 + 2|s(d)|
+
1
2
. (14)
Comments We estimate the comment prior as
pcomment(d) = log(n(r, d)), (15)
where n(r, d) is the number of comments on post d.
Regularity To estimate the regularity prior we use
pregularity(d) = log(?interval), (16)
where ?interval expresses the standard deviation of
the temporal intervals between two successive posts.
Topical consistency Here we use an approach
similar to query clarity (Cronen-Townsend and
Croft, 2002): based on the list of posts from the
same blog we compare the topic distribution of blog
B to the topic distribution in the collection C and
assign a ?clarity? value to B; a score further away
from zero indicates a higher topical consistency. We
estimate the topical consistency prior as
ptopic(d) = log(clarity(d)), (17)
where clarity(d) is estimated by
clarity(d) =
?
w p(w|B) ? log
(
p(w|B)
p(w)
)
?
w p(w|B)
(18)
with p(w) = count(w,C)|C| and p(w|B) =
count(w,B)
|B| .
927
3.3.3 Efficiency
All estimators discussed above can be imple-
mented efficiently: most are document priors and
can therefore be calculated offline. The only topic
dependent estimators are timeliness and language
usage; both can be implemented efficiently as spe-
cific forms of query expansion.
4 Evaluation
In this section we describe the experiments we con-
ducted to answer our research questions about the
impact of credibility on blog post retrieval.
4.1 Research questions
Our research revolves around the contribution of
credibility to the effectiveness of topical blog post
retrieval: what is the contribution of individual indi-
cators, of the post level indicators (topic dependent
or independent), of the blog level indicators, and of
all indicators combined? And do different topics
benefit from different indicators? To answer our re-
search question we compared the performance of the
baseline retrieval system (as detailed in Section 3.1)
with extensions of the baseline system with a single
indicator, a set of indicators, or all indicators.
4.2 Setup
We apply our models to the TREC Blog06 cor-
pus (Macdonald and Ounis, 2006). This corpus
has been constructed by monitoring around 100,000
blog feeds for a period of 11 weeks in early 2006,
downloading all posts created in this period. For
each permalink (HTML page containing one blog
post) the feed id is registered. We can use this id
to aggregate post level features to the blog level. In
our experiments we use only the HTML documents,
3.2M permalinks, which add up to around 88 GB.
The TREC 2006 and 2007 Blog tracks each offer
50 topics and assessments (Ounis et al, 2007; Mac-
donald et al, 2007). For topical relevancy, assess-
ment was done using a standard two-level scale: the
content of the post was judged to be topically rele-
vant or not. The evaluation metrics that we use are
standard ones: mean average precision (MAP) and
precision@10 (p@10) (Baeza-Yates and Ribeiro-
Neto, 1999). For all our retrieval tasks we use the
title field (T) of the topic statement as query.
To estimate the timeliness and semantic cred-
ibility indicators, we use AQUAINT-2, a set of
newswire articles (2.5 GB, about 907K documents)
that are roughly contemporaneous with the TREC
Blog06 collection (AQUAINT-2, 2007). Articles are
in English and come from a variety of sources.
Statistical significance is tested using a two-tailed
paired t-test. Significant improvements over the
baseline are marked with M (? = 0.05) or N (? =
0.01). We use O and H for a drop in performance
(for ? = 0.05 and ? = 0.01, respectively).
4.3 Parameter estimation
The models proposed in Section 3.2 contain param-
eters ?, ? and ?. These parameters need to be esti-
mated and, hence, require a training and test set. We
use a two-fold parameter estimation process: in the
first cycle we estimate the parameters on the TREC
2006 Blog topic set and test these settings on the top-
ics of the TREC 2007 Blog track. The second cycle
goes the other way around and trains on the 2007
set, while testing on the 2006 set.
Figure 2 shows the optimum values for ?, ?, and
? on the 2006 and the 2007 topic sets for both MAP
(bottom lines) and p@10 (top lines). When look-
ing at the MAP scores, the optimal setting for ? is
almost identical for the two topic sets: 0.4 for the
2006 set and 0.3 for the 2007 set, and also the op-
timal setting for ? is very similar for both sets: 0.4
for the 2006 set and 0.5 for the 2007 set. As to ?,
it is clear that timeliness does not improve the per-
formance over using the semantic feature alone and
the optimal setting for ? is therefore 0.0. Both ?
and ? show similar behavior on p@10 as on MAP,
but for ? we see a different trend. If early precision
is required, the value of ? should be increased, giv-
ing more weight to the topic-independent post level
features compared to the blog level features.
4.4 Retrieval performance
Table 2 lists the retrieval results for the baseline, for
each of the credibility indicators (on top of the base-
line), for four subsets of indicators, and for all in-
dicators combined. The baseline performs similar
to the median scores at the TREC 2006 Blog track
(MAP: 0.2203; p@10: 0.564) and somewhat below
the median MAP score at 2007 Blog track (MAP:
0.3340) but above the median p@10 score: 0.3805.
928
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
lambda
20062007
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
beta
20062007
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
MAP
 / P1
0
mu
20062007
Figure 2: Parameter estimation on the TREC 2006 and 2007 Blog topics. (Left): ?. (Center): ?. (Right): ?.
2006 2007
map p@10 map p@10
baseline 0.2156 0.4360 0.2820 0.5160
capitalization 0.2155 0.4500 0.2824 0.5160
emoticons 0.2156 0.4360 0.2820 0.5200
shouting 0.2159 0.4320 0.2833 0.5100
spelling 0.2179M 0.4480M 0.2839N 0.5220
post length 0.2502N 0.4960N 0.3112N 0.5700N
timeliness 0.1865H 0.4520 0.2660 0.4860
semantic 0.2840N 0.6240N 0.3379N 0.6640N
spam filtering 0.2093 0.4700 0.2814 0.5760N
comments 0.2497N 0.5000N 0.3099N 0.5600N
regularity 0.1658H 0.4940M 0.2353H 0.5640M
consistency 0.2141H 0.4220 0.2785O 0.5040
post level 0.2374N 0.4920N 0.2990N 0.5660N
(topic indep.)
post level 0.2840N 0.6240N 0.3379N 0.6640N
(topic dep.)
post level 0.2911N 0.6380N 0.3369N 0.6620N
(all)
blog level 0.2391N 0.4500 0.3023N 0.5580N
all 0.3051N 0.6880N 0.3530N 0.6900N
Table 2: Retrieval performance on 2006 and 2007 topics,
using ? = 0.3, ? = 0.4, and ? = 0.0.
Some (topic independent) post level indicators
hurt the MAP score, while others help (for both
years, and both measures). Combined, the topic
independent post level indicators perform less well
than the use of one of them (post length). As to
the topic dependent post level indicators, timeliness
hurts performance on MAP for both years, while
the semantic indicator provides significant improve-
ments across the board (resulting in a top 2 score in
terms of MAP and a top 5 score in terms of p@10,
when compared to the TREC 2006 Blog track par-
ticipants that only used the T field).
Some of the blog level features hurt more than
they help (regularity, consistency), while the com-
ments feature helps, on all measures, and for both
years. Combined, the blog level features help less
than the use of one of them (comments).
As a group, the combined post level features help
more than either of the two post level sub groups
alone. The blog level features show similar results to
the topic-independent post level features, obtaining
a significant increase on both MAP and p@10, but
lower than the topic-dependent post level features.
The grand combination of all credibility indica-
tors leads to a significant improvement over any of
the single indicators and over any of the four subsets
considered in Table 2. The MAP score of this run
is higher than the best performing run in the TREC
2006 Blog track and has a top 3 performance on
p@10; its 2007 performance is just within the top
half on both MAP and p@10.
4.5 Analysis
Next we examine the differences in average preci-
sion (per topic) between the baseline and subsets of
indicators (post and blog level) and the grand com-
bination. We limit ourselves to an analysis of the
MAP scores. Figure 3 displays the per topic average
precision scores, where topics are sorted by absolute
gain of the grand combination over the baseline.
In 2006, 7 (out of 50) topics were negatively af-
fected by the use of credibility indicators; in 2007,
15 (out of 50) were negatively affected. Table 3 lists
the topics that displayed extreme behavior (in terms
of relative performance gain or drop in AP score).
While the extreme drops for both years are in the
same range, the gains for 2006 are more extreme
than for 2007.
The topic that is hurt most (in absolute terms)
by the credibility indicators is the 2007 topic 910:
aperto network (AP -0.2781). The semantic indi-
cator is to blame for this decrease is: the terms in-
cluded in the expanded query shift the topic from a
wireless broadband provider to television networks.
929
-0.4
-0.3
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
AP
 di
ffe
ren
ce
topics
all
post
blog
-0.4
-0.3
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
AP
 di
ffe
ren
ce
topics
all
post
blog
Figure 3: Per-topic AP differences between baseline run and runs with blog level features (triangles), post level features
(circles) and all feature (squares) on the 2006 (left) en 2007 (right) topics.
Table 3: Extreme performance gains/drops of the grand
combination over the baseline (MAP).
2006
id topic % gain/loss
900 mcdonalds +525.9%
866 foods +446.2%
865 basque +308.6%
862 blackberry -21.5%
870 barry bonds -35.2%
898 business intelligence resources -78.8%
2007
id topic % gain/loss
923 challenger +162.1%
926 hawthorne heights +160.7%
945 bolivia +125.5%
943 censure -49.4%
928 big love -80.0%
904 alterman -84.2%
Topics that gain most (in absolute terms) are 947
(sasha cohen; AP +0.3809) and 923 (challenger; AP
+0.3622) from the 2007 topic set.
Finally, the combination of all credibility indica-
tors hurts 7 (2006) plus 15 (2007) equals 22 topics;
for the post level indicators get a performance drop
in AP for 28 topics (10 plus 18, respectively) and for
the blog level indicators we get a drop for 15 topics
(4 plus 11, respectively). Hence, the combination of
all indicators strikes a good balance between overall
performance gain and per topic risk.
5 Conclusions
We provided efficient estimations for 11 credibility
indicators and assessed their impact on topical blog
post retrieval, on top of a content-based retrieval
baseline. We compared the contribution of these in-
dicators, both individually and in groups, and found
that (combined) they have a significant positive im-
pact on topical blog post retrieval effectiveness. Cer-
tain single indicators, like post length and comments,
make good credibility indicators on their own; the
best performing credibility indicator group consists
of topic dependent post level ones. Other future
work concerns indicator selection: instead of taking
all indicators on board, consider selected indicators
only, in a topic dependent fashion.
Our choice of credibility indicators was based on
a framework proposed by Rubin and Liddy (2006):
the estimators we used are natural implementations
of the selected indicators, but by no means the only
possible ones. In future work we intend to extend
the set of indicators considered so as to include, e.g.,
stated competencies (1e), by harvesting and analyz-
ing bloggers? profiles, and to extend the set of esti-
mators for indicators that we already consider such
as reading level measures (e.g., Flesch-Kincaid) for
the literary appeal indicator (4b).
Acknowledgments
We would like to thank our reviewers for their feed-
back. Both authors were supported by the E.U. IST
programme of the 6th FP for RTD under project
MultiMATCH contract IST-033104. De Rijke was
also supported by NWO under project numbers
017.001.190, 220-80-001, 264-70-050, 354-20-005,
600.065.120, 612-13-001, 612.000.106, 612.066.-
302, 612.069.006, 640.001.501, and 640.002.501.
930
References
Agichtein, E., Castillo, C., Donato, D., Gionis, A., and
Mishne, G. (2008). Finding high-quality content in
social media. In WSDM ?08.
AQUAINT-2 (2007). URL: http://trec.
nist.gov/data/qa/2007_qadata/qa.
07.guidelines.html#documents.
Baeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern
Information Retrieval. Addison Wesley.
Chafe, W. (1986). Evidentiality in English conversion
and academic writing. In Chaf, W. and Nichols, J., ed-
itors, Evidentiality: The Linguistic Coding of Episte-
mology, volume 20, pages 261?273. Ablex Publishing
Corporation.
Croft, W. B. and Lafferty, J., editors (2003). Language
Modeling for Information Retrieval. Kluwer.
Cronen-Townsend, S. and Croft, W. (2002). Quantifying
query ambiguity. In Proceedings of Human Language
Technology 2002, pages 94?98.
Diaz, F. and Metzler, D. (2006). Improving the estima-
tion of relevance models using large external corpora.
In SIGIR ?06: Proceedings of the 29th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 154?161, New
York. ACM Press.
Hawking, D. and Craswell, N. (2002). Overview of the
TREC-2001 web track. In The Tenth Text Retrieval
Conferences (TREC-2001), pages 25?31.
Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.
(2007). The blogvox opinion retrieval system. In The
Fifteenth Text REtrieval Conference (TREC 2006).
Kolari, P., Finin, T., Java, A., and Joshi, A.
(2006). Splog blog dataset. URL: http:
//ebiquity.umbc.edu/resource/html/
id/212/Splog-Blog-Dataset.
Li, X. and Croft, W. (2003). Time-based language mod-
els. In Proceedings of the 12th International Con-
ference on Information and Knowledge Managment
(CIKM), pages 469?475.
Liu, B. (2007). Web Data Mining. Springer-Verlag, Hei-
delberg.
Macdonald, C. and Ounis, I. (2006). The trec blogs06
collection: Creating and analyzing a blog test collec-
tion. Technical Report TR-2006-224, Department of
Computer Science, University of Glasgow.
Macdonald, C., Ounis, I., and Soboroff, I. (2007).
Overview of the trec 2007 blog track. In TREC 2007
Working Notes, pages 31?43.
Metzger, M. (2007). Making sense of credibility on the
web: Models for evaluating online information and
recommendations for future research. Journl of the
American Society for Information Science and Tech-
nology, 58(13):2078?2091.
Mishne, G. (2007a). Applied Text Analytics for Blogs.
PhD thesis, University of Amsterdam, Amsterdam.
Mishne, G. (2007b). Using blog properties to improve
retrieval. In Proceedings of ICWSM 2007.
Mishne, G. and de Rijke, M. (2006). A study of blog
search. In Lalmas, M., MacFarlane, A., Ru?ger, S.,
Tombros, A., Tsikrika, T., and Yavlinsky, A., editors,
Advances in Information Retrieval: Proceedings 28th
European Conference on IR Research (ECIR 2006),
volume 3936 of LNCS, pages 289?301. Springer.
Mishne, G. and Glance, N. (2006). Leave a reply: An
analysis of weblog comments. In Proceedings of
WWW 2006.
Ounis, I., de Rijke, M., Macdonald, C., Mishne, G.,
and Soboroff, I. (2007). Overview of the trec-2006
blog track. In The Fifteenth Text REtrieval Conference
(TREC 2006) Proceedings.
Rubin, V. and Liddy, E. (2006). Assessing credibility
of weblogs. In Proceedings of the AAAI Spring Sym-
posium: Computational Approaches to Analyzing We-
blogs (CAAW).
Stanford, J., Tauber, E., Fogg, B., and Marable, L. (2002).
Experts vs online consumers: A comparative cred-
ibility study of health and finance web sites. URL:
http://www.consumerwebwatch.org/
news/report3_credibilityresearch/
slicedbread.pdf.
Van House, N. (2004). Weblogs: Credibility and
collaboration in an online world. URL: people.
ischool.berkeley.edu/?vanhouse/Van\
%20House\%20trust\%20workshop.pdf.
Weimer, M., Gurevych, I., and Mehlhauser, M. (2007).
Automatically assessing the post quality in online dis-
cussions on software. In Proceedings of the ACL 2007
Demo and Poster Sessions, pages 125?128.
Zhou, Y. and Croft, W. B. (2005). Document quality
models for web ad hoc retrieval. In CIKM ?05: Pro-
ceedings of the 14th ACM international conference on
Information and knowledge management, pages 331?
332.
931
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1057?1065,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Generative Blog Post Retrieval Model that Uses
Query Expansion based on External Collections
Wouter Weerkamp
w.weerkamp@uva.nl
Krisztian Balog
k.balog@uva.nl
ISLA, University of Amsterdam
Maarten de Rijke
mdr@science.uva.nl
Abstract
User generated content is characterized
by short, noisy documents, with many
spelling errors and unexpected language
usage. To bridge the vocabulary gap be-
tween the user?s information need and
documents in a specific user generated
content environment, the blogosphere, we
apply a form of query expansion, i.e.,
adding and reweighing query terms. Since
the blogosphere is noisy, query expansion
on the collection itself is rarely effective
but external, edited collections are more
suitable. We propose a generative model
for expanding queries using external col-
lections in which dependencies between
queries, documents, and expansion doc-
uments are explicitly modeled. Differ-
ent instantiations of our model are dis-
cussed and make different (in)dependence
assumptions. Results using two exter-
nal collections (news andWikipedia) show
that external expansion for retrieval of user
generated content is effective; besides,
conditioning the external collection on the
query is very beneficial, and making can-
didate expansion terms dependent on just
the document seems sufficient.
1 Introduction
One of the grand challenges in information re-
trieval is to bridge the vocabulary gap between a
user and her information need on the one hand and
the relevant documents on the other (Baeza-Yates
and Ribeiro-Neto, 1999). In the setting of blogs
or other types of user generated content, bridging
this gap becomes even more challenging. This has
several causes: (i) the spelling errors, unusual, cre-
ative or unfocused language usage resulting from
the lack of top-down rules and editors in the con-
tent creation process, and (ii) the (often) limited
length of user generated documents.
Query expansion, i.e., modifying the query by
adding and reweighing terms, is an often used
technique to bridge the vocabulary gap. In gen-
eral, query expansion helps more queries than
it hurts (Balog et al, 2008b; Manning et al,
2008). However, when working with user gener-
ated content, expanding a query with terms taken
from the very corpus in which one is searching
tends to be less effective (Arguello et al, 2008a;
Weerkamp and de Rijke, 2008b)?topic drift is
a frequent phenomenon here. To be able to ar-
rive at a richer representation of the user?s infor-
mation need, while avoiding topic drift resulting
from query expansion against user generated con-
tent, various authors have proposed to expand the
query against an external corpus, i.e., a corpus dif-
ferent from the target (user generated) corpus from
which documents need to be retrieved.
Our aim in this paper is to define and evaluate
generative models for expanding queries using ex-
ternal collections. We propose a retrieval frame-
work in which dependencies between queries,
documents, and expansion documents are explic-
itly modeled. We instantiate the framework in
multiple ways by making different (in)dependence
assumptions. As one of the instantiations we ob-
tain the mixture of relevance models originally
proposed by Diaz and Metzler (2006).
We address the following research questions:
(i) Can we effectively apply external expansion in
the retrieval of user generated content? (ii) Does
conditioning the external collection on the query
help improve retrieval performance? (iii) Can we
obtain a good estimate of this query-dependent
collection probability? (iv) Which of the collec-
tion, the query, or the document should the selec-
tion of an expansion term be dependent on? In
other words, what are the strongest simplifications
in terms of conditional independencies between
variables that can be assumed, without hurting per-
formance? (v) Do our models show similar behav-
ior across topics or do we observe strong per-topic
1057
differences between models?
The remainder of this paper is organized as fol-
lows. We discuss previous work related to query
expansion and external sources in ?2. Next, we
introduce our retrieval framework (?3) and con-
tinue with our main contribution, external expan-
sion models, in ?4. ?5 details how the components
of the model can be estimated. We put our models
to the test, using the experimental setup discussed
in ?6, and report on results in ?7. We discuss our
results (?8) and conclude in ?9.
2 Related Work
Related work comes in two main flavors: (i) query
modeling in general, and (ii) query expansion us-
ing external sources (external expansion). We
start by shortly introducing the general ideas be-
hind query modeling, and continue with a quick
overview of work related to external expansion.
2.1 Query Modeling
Query modeling, i.e., transformations of simple
keyword queries into more detailed representa-
tions of the user?s information need (e.g., by as-
signing (different) weights to terms, expanding the
query, or using phrases), is often used to bridge the
vocabulary gap between the query and the doc-
ument collection. Many query expansion tech-
niques have been proposed, and they mostly fall
into two categories, i.e., global analysis and local
analysis. The idea of global analysis is to expand
the query using global collection statistics based,
for instance, on a co-occurrence analysis of the en-
tire collection. Thesaurus- and dictionary-based
expansion as, e.g., in Qiu and Frei (1993), also
provide examples of the global approach.
Our focus in this paper is on local approaches
to query expansion, that use the top retrieved doc-
uments as examples from which to select terms
to improve the retrieval performance (Rocchio,
1971). In the setting of language modeling ap-
proaches to query expansion, the local analysis
idea has been instantiated by estimating addi-
tional query language models (Lafferty and Zhai,
2003; Tao and Zhai, 2006) or relevance mod-
els (Lavrenko and Croft, 2001) from a set of feed-
back documents. Yan and Hauptmann (2007) ex-
plore query expansion in a multimedia setting.
Balog et al (2008b) compare methods for sam-
pling expansion terms to support query-dependent
and query-independent query expansion; the lat-
ter is motivated by the wish to increase ?aspect
recall? and attempts to uncover aspects of the in-
formation need not captured by the query. Kur-
land et al (2005) also try to uncover multiple as-
pects of a query, and to that they provide an iter-
ative ?pseudo-query? generation technique, using
cluster-based language models. The notion of ?as-
pect recall? is mentioned in (Buckley, 2004; Har-
man and Buckley, 2004) and identified as one of
the main reasons of failure of the current informa-
tion retrieval systems. Even though we acknowl-
edge the possibilities of our approach in improving
aspect recall, by introducing aspects mainly cov-
ered by the external collection being used, we are
currently unable to test this assumption.
2.2 External Expansion
The use of external collections for query expan-
sion has a long history, see, e.g., (Kwok et al,
2001; Sakai, 2002). Diaz and Metzler (2006) were
the first to give a systematic account of query ex-
pansion using an external corpus in a language
modeling setting, to improve the estimation of rel-
evance models. As will become clear in ?4, Diaz
and Metzler?s approach is an instantiation of our
general model for external expansion.
Typical query expansion techniques, such as
pseudo-relevance feedback, using a blog or blog
post corpus do not provide significant perfor-
mance improvements and often dramatically hurt
performance. For this reason, query expansion
using external corpora has been a popular tech-
nique at the TREC Blog track (Ounis et al, 2007).
For blog post retrieval, several TREC participants
have experimented with expansion against exter-
nal corpora, usually a news corpus, Wikipedia, the
web, or a mixture of these (Zhang and Yu, 2007;
Java et al, 2007; Ernsting et al, 2008). For the
blog finding task introduced in 2007, TREC par-
ticipants again used expansion against an exter-
nal corpus, usually Wikipedia (Elsas et al, 2008a;
Ernsting et al, 2008; Balog et al, 2008a; Fautsch
and Savoy, 2008; Arguello et al, 2008b). The mo-
tivation underlying most of these approaches is to
improve the estimation of the query representa-
tion, often trying to make up for the unedited na-
ture of the corpus from which posts or blogs need
to be retrieved. Elsas et al (2008b) go a step fur-
ther and develop a query expansion technique us-
ing the links in Wikipedia.
Finally, Weerkamp and de Rijke (2008b) study
1058
external expansion in the setting of blog retrieval
to uncover additional perspectives of a given topic.
We are driven by the same motivation, but where
they considered rank-based result combinations
and simple mixtures of query models, we take
a more principled and structured approach, and
develop four versions of a generative model for
query expansion using external collections.
3 Retrieval Framework
We work in the setting of generative language
models. Here, one usually assumes that a doc-
ument?s relevance is correlated with query likeli-
hood (Ponte and Croft, 1998; Miller et al, 1999;
Hiemstra, 2001). Within the language model-
ing approach, one builds a language model from
each document, and ranks documents based on the
probability of the document model generating the
query. The particulars of the language modeling
approach have been discussed extensively in the
literature (see, e.g., Balog et al (2008b)) and will
not be repeated here. Our final formula for ranking
documents given a query is based on Eq. 1:
logP (D|Q) ?
logP (D) +
?
t?Q
P (t|?Q) logP (t|?D) (1)
Here, we see the prior probability of a document
being relevant, P (D) (which is independent of the
query Q), the probability of a term t for a given
query model, ?Q, and the probability of observ-
ing the term t given the document model, ?D.
Our main interest lies in in obtaining a better es-
timate of P (t|?Q). To this end, we take the query
model to be a linear combination of the maximum-
likelihood query estimate P (t|Q) and an expanded
query model P (t|Q?):
P (t|?Q) = ?Q ?P (t|Q)+ (1??Q) ?P (t|Q?) (2)
In the next section we introduce our models for es-
timating p(t|Q?), i.e., query expansion using (mul-
tiple) external collections.
4 Query Modeling Approach
Our goal is to build an expanded query model that
combines evidence from multiple external collec-
tions. We estimate the probability of a term t in the
expanded query Q? using a mixture of collection-
specific query expansion models.
P (t|Q?) =
?
c?C P (t|Q, c) ? P (c|Q), (3)
where C is the set of document collections.
To estimate the probability of a term given the
query and the collection, P (t|Q, c), we compute
the expectation over the documents in the collec-
tion c:
P (t|Q, c) =
?
D?c
P (t|Q, c,D) ? P (D|Q, c). (4)
Substituting Eq. 4 back into Eq. 3 we get
P (t|Q?) = (5)
?
c?C
P (c|Q) ?
?
D?c
P (t|Q, c,D) ? P (D|Q, c).
This, then, is our query model for combining evi-
dence from multiple sources.
The following subsections introduce four in-
stances of the general external expansion model
(EEM) we proposed in this section; each of the in-
stances differ in independence assumptions:
? EEM1 (?4.1) assumes collection c to be inde-
pendent of query Q and document D jointly,
and document D individually, but keeps the
dependence on Q and of t and Q on D.
? EEM2 (?4.2) assumes that term t and collec-
tion c are conditionally independent, given
document D and query Q; moreover, D and
Q are independent given c but the depen-
dence of t and Q on D is kept.
? EEM3 (?4.3) assumes that expansion term t
and original query Q are independent given
document D.
? On top of EEM3, EEM4 (?4.4) makes one
more assumption, viz. the dependence of col-
lection c on query Q.
4.1 External Expansion Model 1 (EEM1)
Under this model we assume collection c to be
independent of query Q and document D jointly,
and document D individually, but keep the depen-
dence on Q. We rewrite P (t|Q, c) as follows:
P (t|Q, c)
=
?
D?c
P (t|Q,D) ? P (t|c) ? P (D|Q)
=
?
D?c
P (t, Q|D)
P (Q|D)
? P (t|c) ?
P (Q|D)P (D)
P (Q)
?
?
D?c
P (t, Q|D) ? P (t|c) ? P (D) (6)
Note that we drop P (Q) from the equation as it
does not influence the ranking of terms for a given
1059
query Q. Further, P (D) is the prior probability
of a document, regardless of the collection it ap-
pears in (as we assumed D to be independent of
c). We assume P (D) to be uniform, leading to the
following equation for ranking expansion terms:
P (t|Q?) ?
?
c?C
P (t|c) ? P (c|Q) ?
?
D?c
P (t, Q|D). (7)
In this model we capture the probability of the ex-
pansion term given the collection (P (t|c)). This
allows us to assign less weight to terms that are
less meaningful in the external collection.
4.2 External Expansion Model 2 (EEM2)
Here, we assume that term t and collection c are
conditionally independent, given document D and
query Q: P (t|Q, c,D) = P (t|Q,D). This leaves
us with the following:
P (t|Q,D) =
P (t, Q,D)
P (Q,D)
=
P (t, Q|D) ? P (D)
P (Q|D) ? P (D)
=
P (t, Q|D)
P (Q|D)
(8)
Next, we assume document D and query Q to
be independent given collection c: P (D|Q, c) =
P (D|c). Substituting our choices into Eq. 4 gives
us our second way of estimating P (t|Q, c):
P (t|Q, c) =
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c) (9)
Finally, we put our choices so far together, and
implement Eq. 9 in Eq. 3, yielding our final term
ranking equation:
P (t|Q?) ? (10)
?
c?C
P (c|Q) ?
?
D?c
P (t, Q|D)
P (Q|D)
? P (D|c).
4.3 External Expansion Model 3 (EEM3)
Here we assume that expansion term t and both
collection c and original query Q are independent
given document D. Hence, we set P (t|Q, c,D) =
P (t|D). Then
P (t|Q, c)
=
?
D?c
P (t|D) ? P (D|Q, c)
=
?
D?c
P (t|D) ?
P (Q|D, c) ? P (D|c)
P (Q|c)
?
?
D?c
P (t|D) ? P (Q|D, c) ? P (D|c)
We dropped P (Q|c) as it does not influence the
ranking of terms for a given query Q. Assuming
independence of Q and c given D, we obtain
P (t|Q, c) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D)
so
P (t|Q?) ?
?
c?C
P (c|Q) ?
?
D?c
P (D|c) ? P (t|D) ? P (Q|D).
We follow Lavrenko and Croft (2001) and assume
that P (D|c) = 1|Rc| , the size of the set of top
ranked documents in c (denoted by Rc), finally ar-
riving at
P (t|Q?) ?
?
c?C
P (c|Q)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (11)
4.4 External Expansion Model 4 (EEM4)
In this fourth model we start from EEM3 and drop
the assumption that c depends on the query Q, i.e.,
P (c|Q) = P (c), obtaining
P (t|Q?) ?
?
c?C
P (c)
|Rc|
?
?
D?Rc
P (t|D) ? P (Q|D). (12)
Eq. 12 is in fact the ?mixture of relevance models?
external expansion model proposed by Diaz and
Metzler (2006). The fundamental difference be-
tween EEM1, EEM2, EEM3 on the one hand and
EEM4 on the other is that EEM4 assumes inde-
pendence between c and Q (thus P (c|Q) is set to
P (c)). That is, the importance of the external col-
lection is independent of the query. How reason-
able is this choice? Mishne and de Rijke (2006)
examined queries submitted to a blog search en-
gine and found many to be either news-related
context queries (that aim to track mentions of a
named entity) or concept queries (that seek posts
about a general topic). For context queries such as
cheney hunting (TREC topic 867) a news collec-
tion is likely to offer different (relevant) aspects
of the topic, whereas for a concept query such as
jihad (TREC topic 878) a knowledge source such
as Wikipedia seems an appropriate source of terms
that capture aspects of the topic. These observa-
tions suggest the collection should depend on the
query.
1060
EEM3 and EEM4 assume that expansion term t
and original query Q are independent given doc-
ument D. This may or may not be too strong an
assumption. Models EEM1 and EEM2 also make
independence assumptions, but weaker ones.
5 Estimating Components
The models introduced above offer us several
choices in estimating the main components. Be-
low we detail how we estimate (i) P (c|Q), the
importance of a collection for a given query,
(ii) P (t|c), the unimportance of a term for an ex-
ternal collection, (iii) P (Q|D), the relevance of
a document in the external collection for a given
query, and (iv) P (t, Q|D), the likelihood of a term
co-occurring with the query, given a document.
5.1 Importance of a Collection
Represented as P (c|Q) in our models, the im-
portance of an external collection depends on the
query; how we can estimate this term? We con-
sider three alternatives, in terms of (i) query clar-
ity, (ii) coherence and (iii) query-likelihood, using
documents in that collection.
First, query clarity measures the structure of a
set of documents based on the assumption that a
small number of topical terms will have unusu-
ally large probabilities (Cronen-Townsend et al,
2002). We compute the query clarity of the top
ranked documents in a given collection c:
clarity(Q, c) =
?
t
P (t|Q) ? log
P (t|Q)
P (t|Rc)
Finally, we normalize clarity(Q, c) over all col-
lections, and set P (c|Q) ? clarity(Q,c)P
c??C clarity(Q,c
?) .
Second, a measure called ?coherence score? is
defined by He et al (2008). It is the fraction of
?coherent? pairs of documents in a given set of
documents, where a coherent document pair is one
whose similarity exceeds a threshold. The coher-
ence of the top ranked documents Rc is:
Co(Rc) =
?
i6=j?{1,...,|Rc|} ?(di, dj)
|Rc|(|Rc| ? 1)
,
where ?(di, dj) is 1 in case of a similar pair (com-
puted using cosine similarity), and 0 otherwise.
Finally, we set P (c|Q) ? Co(Rc)P
c??C Co(Rc? )
.
Third, we compute the conditional probability
of the collection using Bayes? theorem. We ob-
serve that P (c|Q) ? P (Q|c) (omitting P (Q) as it
will not influence the ranking and P (c) which we
take to be uniform). Further, for the sake of sim-
plicity, we assume that all documents within c are
equally important. Then, P (Q|c) is estimated as
P (Q|c) =
1
|c|
?
?
D?c
P (Q|D) (13)
where P (Q|D) is estimated as described in ?5.3,
and |c| is the number of documents in c.
5.2 Unimportance of a Term
Rather than simply estimating the importance of
a term for a given query, we also estimate the
unimportance of a term for a collection; i.e., we
assign lower probability to terms that are com-
mon in that collection. Here, we take a straight-
forward approach in estimating this, and define
P (t|c) = 1 ? n(t,c)P
t? n(t
?,c) .
5.3 Likelihood of a Query
We need an estimate of the probability of a query
given a document, P (Q|D). We do so by using
Hauff et al (2008)?s refinement of term dependen-
cies in the query as proposed by Metzler and Croft
(2005).
5.4 Likelihood of a Term
Estimating the likelihood of observing both the
query and a term for a given document P (t, Q|D)
is done in a similar way to estimating P (Q|D), but
now for t, Q in stead of Q.
6 Experimental Setup
In his section we detail our experimental setup:
the (external) collections we use, the topic sets
and relevance judgements available, and the sig-
nificance testing we perform.
6.1 Collections and Topics
We make use of three collections: (i) a collec-
tion of user generated documents (blog posts),
(ii) a news collection, and (iii) an online knowl-
edge source. The blog post collection is the TREC
Blog06 collection (Ounis et al, 2007), which con-
tains 3.2 million blog posts from 100,000 blogs
monitored for a period of 11 weeks, from Decem-
ber 2005 to March 2006; all posts from this period
have been stored as HTML files. Our news col-
lection is the AQUAINT-2 collection (AQUAINT-
2, 2007), from which we selected news articles
that appeared in the period covered by the blog
1061
collection, leaving us with about 150,000 news
articles. Finally, we use a dump of the English
Wikipedia from August 2007 as our online knowl-
edge source; this dump contains just over 3.8 mil-
lion encyclopedia articles.
During 2006?2008, the TRECBlog06 collec-
tion has been used for the topical blog post re-
trieval task (Weerkamp and de Rijke, 2008a) at the
TREC Blog track (Ounis et al, 2007): to retrieve
posts about a given topic. For every year, 50 topics
were developed, consisting of a title field, descrip-
tion, and narrative; we use only the title field, and
ignore the other available information. For all 150
topics relevance judgements are available.
6.2 Metrics and Significance
We report on the standard IR metrics Mean Aver-
age Precision (MAP), precision at 5 and 10 doc-
uments (P5, P10), and the Mean Reciprocal Rank
(MRR). To determine whether or not differences
between runs are significant, we use a two-tailed
paired t-test, and report on significant differences
for ? = .05 (M and O) and ? = .01 (N and H).
7 Results
We first discuss the parameter tuning for our four
EEM models in Section 7.1. We then report on the
results of applying these settings to obtain our re-
trieval results on the blog post retrieval task. Sec-
tion 7.2 reports on these results. We follow with a
closer look in Section 8.
7.1 Parameters
Our model has one explicit parameter, and one
more or less implicit parameter. The obvious pa-
rameter is ?Q, used in Eq. 2, but also the num-
ber of terms to include in the final query model
makes a difference. For training of the param-
eters we use two TREC topic sets to train and
test on the held-out topic set. From the training
we conclude that the following parameter settings
work best across all topics: (EEM1) ?Q = 0.6,
30 terms; (EEM2) ?Q = 0.6, 40 terms; (EEM3
and EEM4) ?Q = 0.5, 30 terms. In the remainder
of this section, results for our models are reported
using these parameter settings.
7.2 Retrieval Results
As a baseline we use an approach without exter-
nal query expansion, viz. Eq. 1. In Table 1 we
list the results on the topical blog post finding task
model P (c|Q) MAP P5 P10 MRR
Baseline 0.3815 0.6813 0.6760 0.7643
EEM1
uniform 0.3976N 0.7213N 0.7080N 0.7998
0.8N/0.2W 0.3992 0.7227 0.7107 0.7988
coherence 0.3976 0.7187 0.7060 0.7976
query clarity 0.3970 0.7187 0.7093 0.7929
P (Q|c) 0.3983 0.7267 0.7093 0.7951
oracle 0.4126N 0.7387M 0.7320N 0.8252M
EEM2
uniform 0.3885N 0.7053M 0.6967M 0.7706
0.9N/0.1W 0.3895 0.7133 0.6953 0.7736
coherence 0.3890 0.7093 0.7020 0.7740
query clarity 0.3872 0.7067 0.6953 0.7745
P (Q|c) 0.3883 0.7107 0.6967 0.7717
oracle 0.3995N 0.7253N 0.7167N 0.7856
EEM3
uniform 0.4048N 0.7187M 0.7207N 0.8261N
coherence 0.4058 0.7253 0.7187 0.8306
query clarity 0.4033 0.7253 0.7173 0.8228
P (Q|c) 0.3998 0.7253 0.7100 0.8133
oracle 0.4194N 0.7493N 0.7353N 0.8413
EEM4 0.5N/0.5W 0.4048N 0.7187M 0.7207N 0.8261N
Table 1: Results for all model instances on all top-
ics (i.e., 2006, 2007, and 2008); aN/bW stands
for the weights assigned to the news (a) and
Wikipedia corpora (b). Significance is tested be-
tween (i) each uniform run and the baseline, and
(ii) each other setting and its uniform counterpart.
of (i) our baseline, and (ii) our model (instanti-
ated by EEM1, EEM2, EEM3, and EEM4). For
all models that contain the query-dependent col-
lection probability (P (c|Q)) we report on multi-
ple ways of estimating this: (i) uniform, (ii) best
global mixture (independent of the query, obtained
by a sweep over collection probabilities), (iii) co-
herence, (iv) query clarity, (v) P (Q|c), and (vi) us-
ing an oracle for which optimal settings were ob-
tained by the same sweep as (ii). Note that meth-
ods (i) and (ii) are not query dependent; for EEM3
we do not mention (ii) since it equals (i). Finally,
for EEM4 we only have a query-independent com-
ponent, P (c): the best performance here is ob-
tained using equal weights for both collections.
A few observations. First, our baseline per-
forms well above the median for all three years
(2006?2008). Second, in each of its four instances
our model for query expansion against external
corpora improves over the baseline. Third, we
see that it is safe to assume that a term is depen-
dent only on the document from which it is sam-
pled (EEM1 vs. EEM2 vs. EEM3). EEM3 makes
the strongest assumptions about terms in this re-
spect, yet it performs best. Fourth, capturing the
dependence of the collection on the query helps,
as we can see from the significant improvements
of the ?oracle? runs over their ?uniform? counter-
parts. However, we do not have a good method
yet for automatically estimating this dependence,
1062
as is clear from the insignificant differences be-
tween the runs labeled ?coherence,? ?query clar-
ity,? ?P (Q|c)? and the run labeled ?uniform.?
8 Discussion
Rather than providing a pairwise comparison of all
runs listed in the previous section, we consider two
pairwise comparisons?between (an instantion of)
our model and the baseline, and between two in-
stantiations of our model?and highlight phenom-
ena that we also observed in other pairwise com-
parisons. Based on this discussion, we also con-
sider a combination of approaches.
8.1 EEM1 vs. the Baseline
We zoom in on EEM1 and make a per-topic com-
parison against the baseline. First of all, we
observe behavior typical for all query expansion
methods: some topics are helped, some are not af-
fected, and some are hurt by the use of EEM1; see
Figure 1, top row. Specifically, 27 topics show a
slight drop in AP (maximum drop is 0.043 AP), 3
topics do not change (as no expansion terms are
identified) and the remainder of the topics (120)
improve in AP. The maximum increase in AP is
0.5231 (+304%) for topic 949 (ford bell); Top-
ics 887 (world trade organization, +87%), 1032
(I walk the line, +63%), 865 (basque, +53%), and
1014 (tax break for hybrid automobiles, +50%)
also show large improvements. The largest drop (-
20% AP) is for topic 1043 (a million little pieces,
a controversial memoir that was in the news dur-
ing the time coverd by the blog crawl); because we
do not do phrase or entity recognition in the query,
but apply stopword removal, it is reduced to mil-
lion pieces which introduced a lot of topic drift.
Let us examine the ?collection preference? of
topics: 35 had a clear preference for Wikipedia, 32
topics for news, and the remainder (83 topics) re-
quired a mixture of both collections. First, we look
at topics that require equal weights for both collec-
tions; topic 880 (natalie portman, +21% AP) con-
cerns a celebrity with a largeWikipedia biography,
as well as news coverage due to new movie re-
leases during the period covered by the blog crawl.
Topic 923 (challenger, +7% AP) asks for infor-
mation on the space shuttle that exploded dur-
ing its launch; the 20th anniversary of this event
was commemorated during the period covered by
the crawl and therefore it is newsworthy as well
as present in Wikipedia (due to its historic im-
pact). Finally, topic 869 (muhammad cartoon,
+20% AP) deals with the controversy surrounding
the publication of cartoons featuring Muhammad:
besides its obvious news impact, this event is ex-
tensively discussed in multiple Wikipedia articles.
As to topics that have a preference for
Wikipedia, we see some very general ones (as is to
be expected): Topic 942 (lawful access, +30%AP)
on the government accessing personal files; Topic
1011 (chipotle restaurant, +13% AP) on infor-
mation concerning the Chipotle restaurants; Topic
938 (plug awards, +21% AP) talks about an award
show. Although this last topic could be expected to
have a clear preference for expansion terms from
the news corpus, the awards were not handed out
during the period covered by the news collection
and, hence, full weight is given to Wikipedia.
At the other end of the scale, topics that show a
preference for the news collection are topic 1042
(david irving, +28% AP), who was on trial dur-
ing the period of the crawl for denying the Holo-
caust and received a lot of media attention. Further
examples include Topic 906 (davos, +20% AP),
which asks for information on the annual world
economic forum meeting in Davos in January,
something typically related to news, and topic 949
(ford bell, +304% AP), which seeks information
on Ford Bell, Senate candidate at the start of 2006.
8.2 EEM1 vs. EEM3
Next we turn to a comparison between EEM1
and EEM3. Theoretically, the main difference
between these two instantiations of our general
model is that EEM3 makes much stronger sim-
plifying indepence assumptions than EEM1. In
Figure 1 we compare the two, not only against
the baseline, but, more interestingly, also in terms
of the difference in performance brought about by
switching from uniform estimation of P (c|Q) to
oracle estimation. Most topics gain in AP when
going from the uniform distribution to the oracle
setting. This happens for both models, EEM1 and
EEM3, leading to less topics decreasing in AP
over the baseline (the right part of the plots) and
more topics increasing (the left part). A second
observation is that both gains and losses are higher
for EEM3 than for EEM1.
Zooming in on the differences between EEM1
and EEM3, we compare the two in the same way,
now using EEM3 as ?baseline? (Figure 2). We ob-
serve that EEM3 performs better than EEM1 in 87
1063
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
-0.4
-0.2
 0
 0.2
 0.4
AP dif
ferenc
e
topics
Figure 1: Per-topic AP differences between the
baseline and (Top): EEM1 and (Bottom): EEM3,
for (Left): uniform P (c|Q) and (Right): oracle.
-0.4
-0.2
 0
 0.2
 0.4
AP
 dif
fere
nce
topics
Figure 2: Per-topic AP differences between EEM3
and EEM1 in the oracle setting.
cases, while EEM1 performs better for 60 topics.
Topics 1041 (federal shield law, 47% AP), 1028
(oregon death with dignity act, 32%AP), and 1032
(I walk the line, 32% AP) have the highest differ-
ence in favor of EEM3; Topics 877 (sonic food in-
dustry, 139% AP), 1013 (iceland european union,
25% AP), and 1002 (wikipedia primary source,
23% AP) are helped most by EEM1. Overall,
EEM3 performs significantly better than EEM1 in
terms of MAP (for ? = .05), but not in terms of
the early precision metrics (P5, P10, and MRR).
8.3 Combining Our Approaches
One observation to come out of ?8.1 and 8.2 is that
different topics prefer not only different external
expansion corpora but also different external ex-
pansion methods. To examine this phenomemon,
we created an articificial run by taking, for ev-
ery topic, the best performing model (with settings
optimized for the topic). Twelve topics preferred
the baseline, 37 EEM1, 20 EEM2, and 81 EEM3.
The articifical run produced the following results:
MAP 0.4280, P5 0.7600, P10 0.7480, and MRR
0.8452; the differences in MAP and P10 between
this run and EEM3 are significant for ? = .01.
We leave it as future work to (learn to) predict for
a given topic, which approach to use, thus refining
ongoing work on query difficulty prediction.
9 Conclusions
We explored the use of external corpora for query
expansion in a user generated content setting. We
introduced a general external expansion model,
which offers various modeling choices, and in-
stantiated it based on different (in)dependence as-
sumptions, leaving us with four instances.
Query expansion using external collection is
effective for retrieval in a user generated con-
tent setting. Furthermore, conditioning the collec-
tion on the query is beneficial for retrieval perfor-
mance, but estimating this component remains dif-
ficult. Dropping the dependencies between terms
and collection and terms and query leads to bet-
ter performance. Finally, the best model is topic-
dependent: constructing an artificial run based on
the best model per topic achieves significant better
results than any of the individual models.
Future work focuses on two themes: (i) topic-
dependent model selection and (ii) improved es-
timates of components. As to (i), we first want
to determine whether a query should be expanded,
and next select the appropriate expansion model.
For (ii), we need better estimates of P (Q|c);
one aspect that could be included is taking P (c)
into account in the query-likelihood estimate of
P (Q|c). One can make this dependent on the task
at hand (blog post retrieval vs. blog feed search).
Another possibility is to look at solutions used in
distributed IR. Finally, we can also include the es-
timation of P (D|c), the importance of a document
in the collection.
Acknowledgements
We thank our reviewers for their valuable feed-
back. This research is supported by the DuOMAn
project carried out within the STEVIN programme
which is funded by the Dutch and Flemish Gov-
ernments (http://www.stevin-tst.org) under project
number STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project numbers 017.001.190, 640.001.501, 640.-
002.501, 612.066.512, 612.061.814, 612.061.815,
640.004.802.
1064
References
AQUAINT-2 (2007). URL: http://trec.nist.gov/
data/qa/2007 qadata/qa.07.guidelines.
html#documents.
Arguello, J., Elsas, J., Callan, J., and Carbonell, J. (2008a).
Document representation and query expansion models for
blog recommendation. In Proceedings of ICWSM 2008.
Arguello, J., Elsas, J. L., Callan, J., and Carbonell, J. G.
(2008b). Document representation and query expansion
models for blog recommendation. In Proc. of the 2nd Intl.
Conf. on Weblogs and Social Media (ICWSM).
Baeza-Yates, R. and Ribeiro-Neto, B. (1999). Modern Infor-
mation Retrieval. ACM.
Balog, K., Meij, E., Weerkamp, W., He, J., and de Rijke, M.
(2008a). The University of Amsterdam at TREC 2008:
Blog, Enterprise, and Relevance Feedback. In TREC 2008
Working Notes.
Balog, K., Weerkamp, W., and de Rijke, M. (2008b). A few
examples go a long way: constructing query models from
elaborate query formulations. In SIGIR ?08: Proceedings
of the 31st annual international ACM SIGIR conference on
Research and development in information retrieval, pages
371?378, New York, NY, USA. ACM.
Buckley, C. (2004). Why current IR engines fail. In SIGIR
?04, pages 584?585.
Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002). Pre-
dicting query performance. In SIGIR02, pages 299?306.
Diaz, F. and Metzler, D. (2006). Improving the estimation of
relevance models using large external corpora. In SIGIR
?06: Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in infor-
mation retrieval, pages 154?161, New York, NY, USA.
ACM.
Elsas, J., Arguello, J., Callan, J., and Carbonell, J. (2008a).
Retrieval and feedback models for blog distillation. In The
Sixteenth Text REtrieval Conference (TREC 2007) Pro-
ceedings.
Elsas, J. L., Arguello, J., Callan, J., and Carbonell, J. G.
(2008b). Retrieval and feedback models for blog feed
search. In SIGIR ?08: Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 347?354, New
York, NY, USA. ACM.
Ernsting, B., Weerkamp, W., and de Rijke, M. (2008). Lan-
guage modeling approaches to blog post and feed finding.
In The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
Fautsch, C. and Savoy, J. (2008). UniNE at TREC 2008: Fact
and Opinion Retrieval in the Blogsphere. In TREC 2008
Working Notes.
Harman, D. and Buckley, C. (2004). The NRRC reliable in-
formation access (RIA) workshop. In SIGIR ?04, pages
528?529.
Hauff, C., Murdock, V., and Baeza-Yates, R. (2008). Im-
proved query difficulty prediction for the web. In CIKM
?08: Proceedings of the seventeenth ACM conference on
Conference on information and knowledge management,
pages 439?448.
He, J., Larson, M., and de Rijke, M. (2008). Using
coherence-based measures to predict query difficulty.
In 30th European Conference on Information Retrieval
(ECIR 2008), page 689694. Springer, Springer.
Hiemstra, D. (2001). Using Language Models for Informa-
tion Retrieval. PhD thesis, University of Twente.
Java, A., Kolari, P., Finin, T., Joshi, A., and Martineau, J.
(2007). The blogvox opinion retrieval system. In The Fif-
teenth Text REtrieval Conference (TREC 2006) Proceed-
ings.
Kurland, O., Lee, L., and Domshlak, C. (2005). Better than
the real thing?: Iterative pseudo-query processing using
cluster-based language models. In SIGIR ?05, pages 19?
26.
Kwok, K. L., Grunfeld, L., Dinstl, N., and Chan, M. (2001).
TREC-9 cross language, web and question-answering
track experiments using PIRCS. In TREC-9 Proceedings.
Lafferty, J. and Zhai, C. (2003). Probabilistic relevance mod-
els based on document and query generation. In Language
Modeling for Information Retrieval, Kluwer International
Series on Information Retrieval. Springer.
Lavrenko, V. and Croft, W. B. (2001). Relevance based lan-
guage models. In SIGIR ?01, pages 120?127.
Manning, C. D., Raghavan, P., and Schu?tze, H. (2008). Intro-
duction to Information Retrieval. Cambridge University
Press.
Metzler, D. and Croft, W. B. (2005). A markov random field
model for term dependencies. In SIGIR ?05, pages 472?
479, New York, NY, USA. ACM.
Miller, D., Leek, T., and Schwartz, R. (1999). A hidden
Markov model information retrieval system. In SIGIR ?99,
pages 214?221.
Mishne, G. and de Rijke, M. (2006). A study of blog search.
In Lalmas, M., MacFarlane, A., Ru?ger, S., Tombros, A.,
Tsikrika, T., and Yavlinsky, A., editors, Advances in In-
formation Retrieval: Proceedings 28th European Confer-
ence on IR Research (ECIR 2006), volume 3936 of LNCS,
pages 289?301. Springer.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 Blog
Track. In The Fifteenth Text Retrieval Conference (TREC
2006). NIST.
Ponte, J. M. and Croft, W. B. (1998). A language modeling
approach to information retrieval. In SIGIR ?98, pages
275?281.
Qiu, Y. and Frei, H.-P. (1993). Concept based query expan-
sion. In SIGIR ?93, pages 160?169.
Rocchio, J. (1971). Relevance feedback in information re-
trieval. In The SMART Retrieval System: Experiments in
Automatic Document Processing. Prentice Hall.
Sakai, T. (2002). The use of external text data in cross-
language information retrieval based on machine transla-
tion. In Proceedings IEEE SMC 2002.
Tao, T. and Zhai, C. (2006). Regularized estimation of mix-
ture models for robust pseudo-relevance feedback. In SI-
GIR ?06: Proceedings of the 29th annual international
ACM SIGIR conference on Research and development in
information retrieval, pages 162?169, New York, NY,
USA. ACM.
Weerkamp, W. and de Rijke, M. (2008a). Credibility im-
proves topical blog post retrieval. In ACL-08: HLT, pages
923?931.
Weerkamp, W. and de Rijke, M. (2008b). Looking at things
differently: Exploring perspective recall for informal text
retrieval. In 8th Dutch-Belgian Information Retrieval
Workshop (DIR 2008), pages 93?100.
Yan, R. and Hauptmann, A. (2007). Query expansion us-
ing probabilistic local feedback with application to mul-
timedia retrieval. In CIKM ?07: Proceedings of the six-
teenth ACM conference on Conference on information and
knowledge management, pages 361?370, New York, NY,
USA. ACM.
Zhang, W. and Yu, C. (2007). UIC at TREC 2006 Blog Track.
In The Fifteenth Text REtrieval Conference (TREC 2006)
Proceedings.
1065
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585?594,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Focused Topic-specific Sentiment Lexicons
Valentin Jijkoun Maarten de Rijke Wouter Weerkamp
ISLA, University of Amsterdam, The Netherlands
jijkoun,derijke,w.weerkamp@uva.nl
Abstract
We present a method for automatically
generating focused and accurate topic-
specific subjectivity lexicons from a gen-
eral purpose polarity lexicon that allow
users to pin-point subjective on-topic in-
formation in a set of relevant documents.
We motivate the need for such lexicons
in the field of media analysis, describe
a bootstrapping method for generating a
topic-specific lexicon from a general pur-
pose polarity lexicon, and evaluate the
quality of the generated lexicons both
manually and using a TREC Blog track
test set for opinionated blog post retrieval.
Although the generated lexicons can be an
order of magnitude more selective than the
general purpose lexicon, they maintain, or
even improve, the performance of an opin-
ion retrieval system.
1 Introduction
In the area of media analysis, one of the key
tasks is collecting detailed information about opin-
ions and attitudes toward specific topics from var-
ious sources, both offline (traditional newspapers,
archives) and online (news sites, blogs, forums).
Specifically, media analysis concerns the follow-
ing system task: given a topic and list of docu-
ments (discussing the topic), find all instances of
attitudes toward the topic (e.g., positive/negative
sentiments, or, if the topic is an organization or
person, support/criticism of this entity). For every
such instance, one should identify the source of
the sentiment, the polarity and, possibly, subtopics
that this attitude relates to (e.g., specific targets
of criticism or support). Subsequently, a (hu-
man) media analyst must be able to aggregate
the extracted information by source, polarity or
subtopics, allowing him to build support/criticism
networks etc. (Altheide, 1996). Recent advances
in language technology, especially in sentiment
analysis, promise to (partially) automate this task.
Sentiment analysis is often considered in the
context of the following two tasks:
? sentiment extraction: given a set of textual
documents, identify phrases, clauses, sen-
tences or entire documents that express atti-
tudes, and determine the polarity of these at-
titudes (Kim and Hovy, 2004); and
? sentiment retrieval: given a topic (and possi-
bly, a list of documents relevant to the topic),
identify documents that express attitudes to-
ward this topic (Ounis et al, 2007).
How can technology developed for sentiment
analysis be applied to media analysis? In order
to use a sentiment extraction system for a media
analysis problem, a system would have to be able
to determine which of the extracted sentiments are
actually relevant, i.e., it would not only have to
identify specific targets of all extracted sentiments,
but also decide which of the targets are relevant
for the topic at hand. This is a difficult task, as
the relation between a topic (e.g., a movie) and
specific targets of sentiments (e.g., acting or spe-
cial effects in the movie) is not always straight-
forward, in the face of ubiquitous complex lin-
guistic phenomena such as referential expressions
(?. . . this beautifully shot documentary?) or bridg-
ing anaphora (?the director did an excellent jobs?).
In sentiment retrieval, on the other hand, the
topic is initially present in the task definition, but
it is left to the user to identify sources and targets
of sentiments, as systems typically return a list
of documents ranked by relevance and opinion-
atedness. To use a traditional sentiment retrieval
system in media analysis, one would still have to
manually go through ranked lists of documents re-
turned by the system.
585
To be able to support media analysis, we need to
combine the specificity of (phrase- or word-level)
sentiment analysis with the topicality provided by
sentiment retrieval. Moreover, we should be able
to identify sources and specific targets of opinions.
Another important issue in the media analysis
context is evidence for a system?s decision. If the
output of a system is to be used to inform actions,
the system should present evidence, e.g., high-
lighting words or phrases that indicate a specific
attitude. Most modern approaches to sentiment
analysis, however, use various flavors of classifi-
cation, where decisions (typically) come with con-
fidence scores, but without explicit support.
In order to move towards the requirements of
media analysis, in this paper we focus on two of
the problems identified above: (1) pinpointing ev-
idence for a system?s decisions about the presence
of sentiment in text, and (2) identifying specific
targets of sentiment.
We address these problems by introducing a
special type of lexical resource: a topic-specific
subjectivity lexicon that indicates specific relevant
targets for which sentiments may be expressed; for
a given topic, such a lexicon consists of pairs (syn-
tactic clue, target). We present a method for au-
tomatically generating a topic-specific lexicon for
a given topic and query-biased set of documents.
We evaluate the quality of the lexicon both manu-
ally and in the setting of an opinionated blog post
retrieval task. We demonstrate that such a lexi-
con is highly focused, allowing one to effectively
pinpoint evidence for sentiment, while being com-
petetive with traditional subjectivity lexicons con-
sisting of (a large number of) clue words.
Unlike other methods for topic-specific senti-
ment analysis, we do not expand a seed lexicon.
Instead, we make an existing lexicon more fo-
cused, so that it can be used to actually pin-point
subjectivity in documents relevant to a given topic.
2 Related Work
Much work has been done in sentiment analy-
sis. We discuss related work in four parts: sen-
timent analysis in general, domain- and target-
specific sentiment analysis, product review mining
and sentiment retrieval.
2.1 Sentiment analysis
Sentiment analysis is often seen as two separate
steps for determining subjectivity and polarity.
Most approaches first try to identify subjective
units (documents, sentences), and for each of these
determine whether it is positive or negative. Kim
and Hovy (2004) select candidate sentiment sen-
tences and use word-based sentiment classifiers
to classify unseen words into a negative or posi-
tive class. First, the lexicon is constructed from
WordNet: from several seed words, the structure
of WordNet is used to expand this seed to a full
lexicon. Next, this lexicon is used to measure the
distance between unseen words and words in the
positive and negative classes. Based on word sen-
timents, a decision is made at the sentence level.
A similar approach is taken by Wilson et al
(2005): a classifier is learnt that distinguishes be-
tween polar and neutral sentences, based on a prior
polarity lexicon and an annotated corpus. Among
the features used are syntactic features. After this
initial step, the sentiment sentences are classified
as negative or positive; again, a prior polarity lexi-
con and syntactic features are used. The authors
later explored the difference between prior and
contextual polarity (Wilson et al, 2009): words
that lose polarity in context, or whose polarity is
reversed because of context.
Riloff and Wiebe (2003) describe a bootstrap-
ping method to learn subjective extraction pat-
terns that match specific syntactic templates, using
a high-precision sentence-level subjectivity clas-
sifier and a large unannotated corpus. In our
method, we bootstrap from a subjectivity lexi-
cion rather than a classifier, and perform a topic-
specific analysis, learning indicators of subjectiv-
ity toward a specific topic.
2.2 Domain- and target-specific sentiment
The way authors express their attitudes varies
with the domain: An unpredictable movie can be
positive, but unpredictable politicians are usually
something negative. Since it is unrealistic to con-
struct sentiment lexicons, or manually annotate
text for learning, for every imaginable domain or
topic, automatic methods have been developed.
Godbole et al (2007) aim at measuring over-
all subjectivity or polarity towards a certain entity;
they identify sentiments using domain-specific
lexicons. The lexicons are generated from man-
ually selected seeds for a broad domain such as
Health or Business, following an approach simi-
lar to (Kim and Hovy, 2004). All named entites
in a sentence containing a clue from a lexicon are
586
considered targets of sentiment for counting. Be-
cause of the data volume, no expensive linguistic
processing is performed.
Choi et al (2009) advocate a joint topic-
sentiment analysis. They identify ?sentiment top-
ics,? noun phrases assumed to be linked to a sen-
timent clue in the same expression. They address
two tasks: identifying sentiment clues, and clas-
sifying sentences into positive, negative, or neu-
tral. They start by selecting initial clues from Sen-
tiWordNet, based on sentences with known polar-
ity. Next, the sentiment topics are identified, and
based on these sentiment topics and the current list
of clues, new potential clues are extracted. The
clues can be used to classifiy sentences.
Fahrni and Klenner (2008) identify potential
targets in a given domain, and create a target-
specific polarity adjective lexicon. To this end,
they find targets using Wikipedia, and associated
adjectives. Next, the target-specific polarity of ad-
jectives is detemined using Hearst-like patterns.
Kanayama and Nasukawa (2006) introduce po-
lar atoms: minimal human-understandable syn-
tactic structures that specify polarity of clauses.
The goal is to learn new domain-specific polar
atoms, but these are not target-specific. They
use manually-created syntactic patterns to identify
atoms and coherency to determine polarity.
In contrast to much of the work in the literature,
we need to specialize subjectivity lexicons not for
a domain and target, but for ?topics.?
2.3 Product features and opinions
Much work has been carried out for the task of
mining product reviews, where the goal is to iden-
tify features of specific products (such as picture,
zoom, size, weight for digital cameras) and opin-
ions about these specific features in user reviews.
Liu et al (2005) describe a system that identifies
such features via rules learned from a manually
annotated corpus of reviews; opinions on features
are extracted from the structure of reviews (which
explicitly separate positive and negative opinions).
Popescu and Etzioni (2005) present a method
that identifies product features for using corpus
statistics, WordNet relations and morphological
cues. Opinions about the features are extracted us-
ing a hand-crafted set of syntactic rules.
Targets extracted in our method for a topic are
similar to features extracted in review mining for
products. However, topics in our setting go be-
yond concrete products, and the diversity and gen-
erality of possible topics makes it difficult to ap-
ply such supervised or thesaurus-based methods to
identify opinion targets. Moreover, in our method
we directly use associations between targets and
opinions to extract both.
2.4 Sentiment retrieval
At TREC, the Text REtrieval Conference, there
has been interest in a specific type of sentiment
analysis: opinion retrieval. This interest materi-
alized in 2006 (Ounis et al, 2007), with the opin-
ionated blog post retrieval task. Finding blog posts
that are not just about a topic, but also contain an
opinion on the topic, proves to be a difficult task.
Performance on the opinion-finding task is domi-
nated by performance on the underlying document
retrieval task (the topical baseline).
Opinion finding is often approached as a two-
stage problem: (1) identify documents relevant to
the query, (2) identify opinions. In stage (2) one
commonly uses either a binary classifier to distin-
guish between opinionated and non-opinionated
documents or applies reranking of the initial result
list using some opinion score. Opinion add-ons
show only slight improvements over relevance-
only baselines.
The best performing opinion finding system at
TREC 2008 is a two-stage approach using rerank-
ing in stage (2) (Lee et al, 2008). The authors
use SentiWordNet and a corpus-derived lexicon
to construct an opinion score for each post in an
initial ranking of blog posts. This opinion score
is combined with the relevance score, and posts
are reranked according to this new score. We de-
tail this approach in Section 6. Later, the authors
use domain-specific opinion indicators (Na et al,
2009), like ?interesting story? (movie review), and
?light? (notebook review). This domain-specific
lexicon is constructed using feedback-style learn-
ing: retrieve an initial list of documents and use
the top documents as training data to learn an opin-
ion lexicon. Opinion scores per document are then
computed as an average of opinion scores over
all its words. Results show slight improvements
(+3%) on mean average precision.
3 Generating Topic-Specific Lexicons
In this section we describe how we generate a lex-
icon of subjectivity clues and targets for a given
topic and a list of relevant documents (e.g., re-
587
Extract all 
syntactic contexts 
of clue words 
Background 
corpus
Topic-independent 
subjectivity lexicon
Relevant docs
Topic
For each clue 
word, select D 
contexts with 
highest entropy
List of syntactic clues:
(clue word, syn. context)
Extract all 
occurrences 
endpoints of 
syntactic clues 
Extract all 
occurrences 
endpoints of 
syntactic clues 
Potential targets in 
background corpus
Potential targets in 
relevant doc. list
Compare frequencies 
using chi-square; 
select top T targets
List of T targets
For each target, 
find syn. clues it 
co-occurs with
Topic-specific lexicon of tuples:
(syntactic clue, target)
Step 1
Step 2
Step 3
Figure 1: Our method for learning a topic-
dependent subjectivity lexicon.
trieved by a search engine for the topic). As an ad-
ditional resource, we use a large background cor-
pus of text documents of a similar style but with
diverse subjects; we assume that the relevant doc-
uments are part of this corpus as well. As the back-
ground corpus, we used the set of documents from
the assessment pools of TREC 2006?2008 opin-
ion retrieval tasks (described in detail in section 4).
We use the Stanford lexicalized parser1 to extract
labeled dependency triples (head, label, modifier).
In the extracted triples, all words indicate their cat-
egory (noun, adjective, verb, adverb, etc.) and are
normalized to lemmas.
Figure 1 provides an overview of our method;
below we describe it in more detail.
3.1 Step 1: Extracting syntactic contexts
We start with a general domain-independent prior
polarity lexicon of 8,821 clue words (Wilson et al,
2005). First, we identify syntactic contexts in
which specific clue words can be used to express
1http://nlp.stanford.edu/software/
lex-parser.shtml
attitude: we try to find how a clue word can be syn-
tactically linked to targets of sentiments. We take a
simple definition of the syntactic context: a single
labeled directed dependency relation. For every
clue word, we extract all syntactic contexts, i.e.,
all dependencies, in which the word is involved
(as head or as modifier) in the background corpus,
along with their endpoints. Table 1 shows exam-
ples of clue words and contexts that indicate sen-
timents. For every clue, we only select those con-
texts that exhibit a high entropy among the lemmas
at the other endpoint of the dependencies. E.g.,
in our background corpus, the verb to like occurs
97,179 times with a nominal subject and 52,904
times with a direct object; however, the entropy of
lemmas of the subjects is 4.33, compared to 9.56
for the direct objects. In other words, subjects of
like are more ?predictable.? Indeed, the pronoun
I accounts for 50% of subjects, followed by you
(14%), they (4%), we (4%) and people (2%). The
most frequent objects of like are it (12%), what
(4%), idea (2%), they (2%). Thus, objects of to
like will be preferred by the method.
Our entropy-driven selection of syntactic con-
texts of a clue word is based on the following as-
sumption:
Assumption 1: In text, targets of sentiments
are more diverse than sources of sentiments
or other accompanying attributes such as lo-
cation, time, manner, etc. Therefore targets
exhibit higher entropy than other attributes.
For every clue word, we select the top D syntac-
tic contexts whose entropy is at least half of the
maximum entropy for this clue.
To summarize, at the end of Step 1 of our
method, we have extracted a list of pairs (clue
word, syntactic context) such that for occurrences
of the clue word, the words at the endpoint of the
syntactic dependency are likely to be targets of
sentiments. We call such a pair a syntactic clue.
3.2 Step 2: Selecting potential targets
Here, we use the extracted syntantic clues to iden-
tify words that are likely to serve as specific tar-
gets for opinions about the topic in the relevant
documents. In this work we only consider individ-
ual words as potential targets and leave exploring
other options (e.g., NPs and VPs as targets) for fu-
ture work. In extracting targets, we rely on the
following assumption:
588
Clue word Syntactic context Target Example
to like has direct object u2 I do still like U2 very much
to like has clausal complement criticize I don?t like to criticize our intelligence services
to like has about-modifier olympics That?s what I like about Winter Olympics
terrible is adjectival modifier of idea it?s a terrible idea to recall judges for...
terrible has nominal subject shirt And Neil, that shirt is terrible!
terrible has clausal complement can It is terrible that a small group of extremists can . . .
Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies).
Assumption 2: The list of relevant documents
contains a substantial number of documents
on the topic which, moreover, contain senti-
ments about the topic.
We extract all endpoints of all occurrences of the
syntactic clues in the relevant documents, as well
as in the background corpus. To identify potential
attitude targets in the relevant documents, we com-
pare their frequency in the relevant documents to
the frequency in the background corpus using the
standard ?2 statistics. This technique is based on
the following assumption:
Assumption 3: Sentiment targets related to
the topic occur more often in subjective con-
text in the set of relevant documents, than
in the background corpus. In other words,
while the background corpus contains senti-
ments towards very diverse subjects, the rel-
evant documents tend to express attitudes re-
lated to the topic.
For every potential target, we compute the ?2-
score and select the top T highest scoring targets.
As the result of Steps 1 and 2, as candidate tar-
gets for a given topic, we only select words that oc-
cur in subjective contexts, and that do so more of-
ten than we would normally expect. Table 2 shows
examples of extracted targets for three TREC top-
ics (see below for a description of our experimen-
tal data).
3.3 Step 3: Generating topic-specific lexicons
In the last step of the method, we combine clues
and targets. For each target identified in Step 2,
we take all syntactic clues extracted in Step 1 that
co-occur with the target in the relevant documents.
The resulting list of triples (clue word, syntactic
context, target) constitute the lexicon. We conjec-
ture that an occurrence of a lexicon entry in a text
indicates, with reasonable confidence, a subjective
attitude towards the target.
Topic ?Relationship between Abramoff and Bush?
abramoff lobbyist scandal fundraiser bush fund-raiser re-
publican prosecutor tribe swirl corrupt corruption norquist
democrat lobbying investigation scanlon reid lawmaker
dealings president
Topic ?MacBook Pro?
macbook laptop powerbook connector mac processor note-
book fw800 spec firewire imac pro machine apple power-
books ibook ghz g4 ata binary keynote drive modem
Topic: ?Super Bowl ads?
ad bowl commercial fridge caveman xl endorsement adver-
tising spot advertiser game super essential celebrity payoff
marketing publicity brand advertise watch viewer tv football
venue
Table 2: Examples of targets extracted at Step 2.
4 Data and Experimental Setup
We consider two types of evaluation. In the next
section, we examine the quality of the lexicons
we generate. In the section after that we evaluate
lexicons quantitatively using the TREC Blog track
benchmark.
For extrinsic evaluation we apply our lexi-
con generation method to a collection of doc-
uments containing opinionated utterances: blog
posts. The Blogs06 collection (Macdonald and
Ounis, 2006) is a crawl of blog posts from 100,649
blogs over a period of 11 weeks (06/12/2005?
21/02/2006), with 3,215,171 posts in total. Be-
fore indexing the collection, we perform two pre-
processing steps: (i) when extracting plain text
from HTML, we only keep block-level elements
longer than 15 words (to remove boilerplate mate-
rial), and (ii) we remove non-English posts using
TextCat2 for language detection. This leaves us
with 2,574,356 posts with 506 words per post on
average. We index the collection using Indri,3 ver-
sion 2.10.
TREC 2006?2008 came with the task of opin-
ionated blog post retrieval (Ounis et al, 2007).
For each year a set of 50 topics was created, giv-
2http://odur.let.rug.nl/?vannoord/
TextCat/
3http://www.lemurproject.org/indri/
589
ing us 150 topics in total. Every topic comes with
a set of relevance judgments: Given a topic, a blog
post can be either (i) nonrelevant, (ii) relevant, but
not opinionated, or (iii) relevant and opinionated.
TREC topics consist of three fields (title, descrip-
tion, and narrative), of which we only use the title
field: a query of 1?3 keywords.
We use standard TREC evaluation measures for
opinion retrieval: MAP (mean average precision),
R-precision (precision within the top R retrieved
documents, where R is the number of known rel-
evant documents in the collection), MRR (mean
reciprocal rank), P@10 and P@100 (precision
within the top 10 and 100 retrieved documents).
In the context of media analysis, recall-oriented
measures such as MAP and R-precision are more
meaningful than the other, early precision-oriented
measures. Note that for the opinion retrieval task
a document is considered relevant if it is on topic
and contains opinions or sentiments towards the
topic.
Throughout Section 6 below, we test for signif-
icant differences using a two-tailed paired t-test,
and report on significant differences for ? = 0.01
(N and H), and ? = 0.05 (M and O).
For the quantative experiments in Section 6 we
need a topical baseline: a set of blog posts po-
tentially relevant to each topic. For this, we use
the Indri retrieval engine, and apply the Markov
Random Fields to model term dependencies in the
query (Metzler and Croft, 2005) to improve topi-
cal retrieval. We retrieve the top 1,000 posts for
each query.
5 Qualitative Analysis of Lexicons
Lexicon size (the number of entries) and selectiv-
ity (how often entries match in text) of the gen-
erated lexicons vary depending on the parame-
ters D and T introduced above. The two right-
most columns of Table 4 show the lexicon size
and the average number of matches per topic. Be-
cause our topic-specific lexicons consist of triples
(clue word, syntactic context, target), they actu-
ally contain more words than topic-independent
lexicons of the same size, but topic-specific en-
tries are more selective, which makes the lexicon
more focused. Table 3 compares the application
of topic-independent and topic-specific lexicons to
on-topic blog text.
We manually performed an explorative error
analysis on a small number of documents, anno-
There are some tragic mo-
ments like eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
There are some tragic mo-
ments l ike eggs freezing ,
and predators snatching the
females and little ones-you
know the whole NATURE
thing ... but this movie is
awesome
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nallywas able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Saturday was more errands,
then spent the evening with
Dad and Stepmum, and fi-
nally was able to see March
of the Penguins, which
was wonderful. Christmas
Day was lovely, surrounded
by family, good food and
drink, and little L to play
with.
Table 3: Posts with highlighted targets (bold) and
subjectivity clues (blue) using topic-independent
(left) and topic-specific (right) lexicons.
tated using the smallest lexicon in Table 4 for the
topic ?March of the Pinguins.? We assigned 186
matches of lexicon entries in 30 documents into
four classes:
? REL: sentiment towards a relevant target;
? CONTEXT: sentiment towards a target that
is irrelevant to the topic due to context (e.g.,
opinion about a target ?film?, but refering to
a film different from the topic);
? IRREL: sentiment towards irrelevant target
(e.g., ?game? for a topic about a movie);
? NOSENT: no sentiment at all
In total only 8% of matches were manually clas-
sified as REL, with 62% classified as NOSENT,
23% as CONTEXT, and 6% as IRREL. On the
other hand, among documents assessed as opio-
nionated by TREC assessors, only 13% did not
contain matches of the lexicon entries, compared
to 27% of non-opinionated documents, which
does indicate that our lexicon does attempt to sep-
arate non-opinionated documents from opinion-
ated.
6 Quantitative Evaluation of Lexicons
In this section we assess the quality of the gen-
erated topic-specific lexicons numerically and ex-
trinsically. To this end we deploy our lexicons to
the task of opinionated blog post retrieval (Ounis
et al, 2007). A commonly used approach to this
task works in two stages: (1) identify topically rel-
evant blog posts, and (2) classify these posts as
being opinionated or not. In stage 2 the standard
590
approach is to rerank the results from stage 1, in-
stead of doing actual binary classification. We take
this approach, as it has shown good performance
in the past TREC editions (Ounis et al, 2007) and
is fairly straightforward to implement. We also ex-
plore another way of using the lexicon: as a source
for query expansion (i.e., adding new terms to the
original query) in Section 6.2. For all experiments
we use the collection described in Section 4.
Our experiments have two goals: to compare
the use of topic-independent and topic-specific
lexicons for the opinionated post retrieval task,
and to examine how different settings for the pa-
rameters of the lexicon generation affect the em-
pirical quality.
6.1 Reranking using a lexicon
To rerank a list of posts retrieved for a given topic,
we opt to use the method that showed best per-
formance at TREC 2008. The approach taken
by Lee et al (2008) linearly combines a (top-
ical) relevance score with an opinion score for
each post. For the opinion score, terms from a
(topic-independent) lexicon are matched against
the post content, and weighted with the probability
of term?s subjectivity. Finally, the sum is normal-
ized using the Okapi BM25 framework. The final
opinion score Sop is computed as in Eq. 1:
Sop(D) =
Opinion(D) ? (k1 + 1)
Opinion(D) + k1 ? (1 ? b +
b?|D|
avgdl )
, (1)
where k1, and b are Okapi parameters (set to their
default values k1 = 2.0, and b = 0.75), |D| is the
length of document D, and avgdl is the average
document length in the collection. The opinion
score Opinion(D) is calculated using Eq. 2:
Opinion(D) =
?
w?O
P (sub|w) ? n(w,D), (2)
where O is the set of terms in the sentiment lex-
icon, P (sub|w) indicates the probability of term
w being subjective, and n(w,D) is the number of
times term w occurs in document D. The opinion
scoring can weigh lexicon terms differently, using
P (sub|w); it normalizes scores to cancel out the
effect of varying document sizes.
In our experiments we use the method de-
scribed above, and plug in the MPQA polarity
lexicon.4 We compare the results of using this
4http://www.cs.pitt.edu/mpqa/
topic-independent lexicon to the topic-dependent
lexicons our method generates, which are also
plugged into the reranking of Lee et al (2008).
In addition to using Okapi BM25 for opinion
scoring, we also consider a simpler method. As
we observed in Section 5, our topic-specific lexi-
cons are more selective than the topic-independent
lexicon, and a simple number of lexicon matches
can give a good indication of opinionatedness of a
document:
Sop(D) = min(n(O,D), 10)/10, (3)
where n(O,D) is the number of matches of the
term of sentiment lexicon O in document D.
6.1.1 Results and observations
There are several parameters that we can vary
when generating a topic-specific lexicon and when
using it for reranking:
D: the number of syntactic contexts per clue
T : the number of extracted targets
Sop(D): the opinion scoring function.
?: the weight of the opinion score in the linear
combination with the relevance score.
Note that ? does not affect the lexicon creation,
but only how the lexicon is used in reranking.
Since we want to assess the quality of lexicons,
not in the opinionated retrieval performance as
such, we factor out ? by selecting the best setting
for each lexicon (including the topic-independent)
and each evaluation measure.
In Table 4 we present the results of evaluation
of several lexicons in the context of opinionated
blog post retrieval.
First, we note that reranking using all lexi-
cons in Table 4 significantly improves over the
relevance-only baseline for all evaluation mea-
sures. When comparing topic-specific lexicons to
the topic-independent one, most of the differences
are not statistically significant, which is surpris-
ing given the fact that most topic-specific lexicons
we evaluated are substantially smaller (see the two
rightmost columns in the table). The smallest lex-
icon in Table 4 is seven times more selective than
the general one, in terms of the number of lexicon
matches per document.
The only evaluation measure where the topic-
independent lexicon consistently outperforms
topic-specific ones, is Mean Reciprocal Rank that
depends on a single relevant opinionated docu-
ment high in a ranking. A possible explanation
591
Lexicon MAP R-prec MRR P@10 P@100 |lexicon| hits per doc
no reranking 0.2966 0.3556 0.6750 0.4820 0.3666 ? ?
topic-independent 0.3182 0.3776 0.7714 0.5607 0.3980 8,221 36.17
D T Sop
3 50 count 0.3191 0.3769 0.7276O 0.5547 0.3963 2,327 5.02
3 100 count 0.3191 0.3777 0.7416 0.5573 0.3971 3,977 8.58
5 50 count 0.3178 0.3775 0.7246O 0.5560 0.3931 2,784 5.73
5 100 count 0.3178 0.3784 0.7316O 0.5513 0.3961 4,910 10.06
all 50 count 0.3167 0.3753 0.7264O 0.5520 0.3957 4,505 9.34
all 100 count 0.3146 0.3761 0.7283O 0.5347O 0.3955 8,217 16.72
all 50 okapi 0.3129 0.3713 0.7247H 0.5333O 0.3833O 4,505 9.34
all 100 okapi 0.3189 0.3755 0.7162H 0.5473 0.3921 8,217 16.72
all 200 okapi 0.3229N 0.3803 0.7389 0.5547 0.3987 14,581 29.14
Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topic-
independent lexicon. The two rightmost columns show the number of lexicon entries (average per topic)
and the number of matches of lexicon entries in blog posts (average for top 1,000 posts).
is that the large general lexicon easily finds a few
?obviously subjective? posts (those with heavily
used subjective words), but is not better at detect-
ing less obvious ones, as indicated by the recall-
oriented MAP and R-precision.
Interestingly, increasing the number of syntac-
tic contexts considered for a clue word (parame-
ter D) and the number of selected targets (param-
eter T ) leads to substantially larger lexicons, but
only gives marginal improvements when lexicons
are used for opinion retrieval. This shows that our
bootstrapping method is effective at filtering out
non-relevant sentiment targets and syntactic clues.
The evaluation results also show that the choice
of opinion scoring function (Okapi or raw counts)
depends on the lexicon size: for smaller, more fo-
cused lexicons unnormalized counts are more ef-
fective. This also confirms our intuition that for
small, focused lexicons simple presence of a sen-
timent clue in text is a good indication of subjec-
tivity, while for larger lexicons an overall subjec-
tivity scoring of texts has to be used, which can be
hard to interpret for (media analysis) users.
6.2 Query expansion with lexicons
In this section we evaluate the quality of targets
extracted as part of the lexicons by using them for
query expansion. Query expansion is a commonly
used technique in information retrieval, aimed at
getting a better representation of the user?s in-
formation need by adding terms to the original
retrieval query; for user-generated content, se-
lective query expansion has proved very benefi-
cial (Weerkamp et al, 2009). We hypothesize that
if our method manages to identify targets that cor-
respond to issues, subtopics or features associated
Run MAP P@10 MRR
Topical blog post retrieval
Baseline 0.4086 0.7053 0.7984
Rel. models 0.4017O 0.6867 0.7383H
Subj. targets 0.4190M 0.7373M 0.8470M
Opinion retrieval
Baseline 0.2966 0.4820 0.6750
Rel. models 0.2841H 0.4467H 0.5479H
Subj. targets 0.3075 0.5227N 0.7196
Table 5: Query expansion using relevance mod-
els and topic-specific subjectivity targets. Signifi-
cance tested against the baseline.
with the topic, the extracted targets should be good
candidates for query expansion. The experiments
described below test this hypothesis.
For every test topic, we select the 20 top-scoring
targets as expansion terms, and use Indri to re-
turn 1,000 most relevant documents for the ex-
panded query. We evaluate the resulting ranking
using both topical retrieval and opinionated re-
trieval measures. For the sake of comparison, we
also implemented a well-known query expansion
method based on Relevance Models (Lavrenko
and Croft, 2001): this method has been shown to
work well in many settings. Table 5 shows evalu-
ation results for these two query expansion meth-
ods, compared to the baseline retrieval run.
The results show that on topical retrieval query
expansion using targets significantly improves re-
trieval performance, while using relevance mod-
els actually hurts all evaluation measures. The
failure of the latter expansion method can be at-
tributed to the relatively large amount of noise
in user-generated content, such as boilerplate
592
material, timestamps of blog posts, comments
etc. (Weerkamp and de Rijke, 2008). Our method
uses full syntactic parsing of the retrieved doc-
uments, which might substantially reduce the
amount of noise since only (relatively) well-
formed English sentences are used in lexicon gen-
eration.
For opinionated retrieval, target-based expan-
sion also improves over the baseline, although the
differences are only significant for P@10. The
consistent improvement for topical retrieval sug-
gests that a topic-specific lexicon can be used both
for query expansion (as described in this section)
and for opinion reranking (as described in Sec-
tion 6.1). We leave this combination for future
work.
7 Conclusions and Future Work
We have described a bootstrapping method for de-
riving a topic-specific lexicon from a general pur-
pose polarity lexicon. We have evaluated the qual-
ity of generated lexicons both manually and using
a TREC Blog track test set for opinionated blog
post retrieval. Although the generated lexicons
can be an order of magnitude more selective, they
maintain, or even improve, the performance of an
opinion retrieval system.
As to future work, we intend to combine our
method with known methods for topic-specific
lexicon expansion (our method is rather concerned
with lexicon ?restriction?). Existing sentence-
or phrase-level (trained) sentiment classifiers can
also be used easily: when collecting/counting tar-
gets we can weigh them by ?prior? score provided
by such classifiers. We also want to look at more
complex syntactic patterns: Choi et al (2009) re-
port that many errors are due to exclusive use of
unigrams. We would also like to extend poten-
tial opinion targets to include multi-word phrases
(NPs and VPs), in addition to individual words.
Finally, we do not identify polarity yet: this can
be partially inherited from the initial lexicon and
refined automatically via bootstrapping.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430, by the DuOMAn project carried out
within the STEVIN programme which is funded
by the Dutch and Flemish Governments under
project nr STE-09-12, and by the Netherlands Or-
ganisation for Scientific Research (NWO) under
project nrs 612.066.512, 612.061.814, 612.061.-
815, 640.004.802.
References
Altheide, D. (1996). Qualitative Media Analysis. Sage.
Choi, Y., Kim, Y., and Myaeng, S.-H. (2009). Domain-
specific sentiment analysis using contextual feature gen-
eration. In TSA ?09: Proceeding of the 1st international
CIKM workshop on Topic-sentiment analysis for mass
opinion, pages 37?44, New York, NY, USA. ACM.
Fahrni, A. and Klenner, M. (2008). Old Wine or Warm
Beer: Target-Specific Sentiment Analysis of Adjectives.
In Proc.of the Symposium on Affective Language in Hu-
man and Machine, AISB 2008 Convention, 1st-2nd April
2008. University of Aberdeen, Aberdeen, Scotland, pages
60 ? 63.
Godbole, N., Srinivasaiah, M., and Skiena, S. (2007). Large-
scale sentiment analysis for news and blogs. In Proceed-
ings of the International Conference on Weblogs and So-
cial Media (ICWSM).
Kanayama, H. and Nasukawa, T. (2006). Fully automatic lex-
icon expansion for domain-oriented sentiment analysis. In
EMNLP ?06: Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing, pages
355?363, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kim, S. and Hovy, E. (2004). Determining the sentiment of
opinions. In Proceedings of COLING 2004.
Lavrenko, V. and Croft, B. (2001). Relevance-based language
models. In SIGIR ?01: Proceedings of the 24th annual
international ACM SIGIR conference on research and de-
velopment in information retrieval.
Lee, Y., Na, S.-H., Kim, J., Nam, S.-H., Jung, H.-Y., and Lee,
J.-H. (2008). KLE at TREC 2008 Blog Track: Blog Post
and Feed Retrieval. In Proceedings of TREC 2008.
Liu, B., Hu, M., and Cheng, J. (2005). Opinion observer: an-
alyzing and comparing opinions on the web. In Proceed-
ings of the 14th international conference on World Wide
Web.
Macdonald, C. and Ounis, I. (2006). The TREC Blogs06
collection: Creating and analysing a blog test collection.
Technical Report TR-2006-224, Department of Computer
Science, University of Glasgow.
Metzler, D. and Croft, W. B. (2005). A markov random feld
model for term dependencies. In SIGIR ?05: Proceed-
ings of the 28th annual international ACM SIGIR con-
ference on research and development in information re-
trieval, pages 472?479, New York, NY, USA. ACM Press.
Na, S.-H., Lee, Y., Nam, S.-H., and Lee, J.-H. (2009). Im-
proving opinion retrieval based on query-specific senti-
ment lexicon. In ECIR ?09: Proceedings of the 31th Eu-
ropean Conference on IR Research on Advances in In-
formation Retrieval, pages 734?738, Berlin, Heidelberg.
Springer-Verlag.
Ounis, I., Macdonald, C., de Rijke, M., Mishne, G., and
Soboroff, I. (2007). Overview of the TREC 2006 blog
track. In The Fifteenth Text REtrieval Conference (TREC
2006). NIST.
Popescu, A.-M. and Etzioni, O. (2005). Extracting prod-
uct features and opinions from reviews. In Proceedings
of Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP).
Riloff, E. and Wiebe, J. (2003). Learning extraction patterns
593
for subjective expressions. In Proceedings of the 2003
Conference on Empirical methods in Natural Language
Processing (EMNLP).
Weerkamp, W., Balog, K., and de Rijke, M. (2009). A gener-
ative blog post retrieval model that uses query expansion
based on external collections. In Joint conference of the
47th Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation of
Natural Language Processing (ACL-ICNLP 2009), Singa-
pore.
Weerkamp, W. and de Rijke, M. (2008). Credibility im-
proves topical blog post retrieval. In Proceedings of ACL-
08: HLT, page 923931, Columbus, Ohio. Association
for Computational Linguistics, Association for Computa-
tional Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Recognizing
contextual polarity in phrase-level sentiment analysis. In
HLT ?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Morristown, NJ, USA.
Association for Computational Linguistics.
Wilson, T., Wiebe, J., and Hoffmann, P. (2009). Recog-
nizing contextual polarity: an exploration of features for
phrase-level sentiment analysis. Computational Linguis-
tics, 35(3):399?433.
594
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 17?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Mining User Experiences from Online Forums: An Exploration?
Valentin Jijkoun Maarten de Rijke
Wouter Weerkamp
ISLA, University of Amsterdam
Science Park 107
1098 XG Amsterdam, The Netherlands
jijkoun,m.derijke,w.weerkamp@uva.nl
Paul Ackermans Gijs Geleijnse
Philips Research Europe
High Tech Campus 34
5656 AE Eindhoven, The Netherlands
paul.ackermans@philips.com
gijs.geleijnse@philips.com
1 Introduction
Recent years have shown a large increase in the
usage of content creation platforms?blogs, com-
munity QA sites, forums, etc.?aimed at the gen-
eral public.User generated data contains emotional,
opinionated, sentimental, and personal posts. This
characteristic makes it an interesting data source
for exploring new types of linguistic analysis, as is
demonstrated by research on, e.g., sentiment analy-
sis [4], opinion retrieval [3], and mood detection [1].
We introduce the task of experience mining. Here,
the goal is to gain insights into criteria that people
formulate to judge or rate a product or its usage.
These criteria can be formulated as the expectations
that people have of the product in advance (i.e., the
reasons to buy), but can also be expressed as reports
of experiences while using the product and compar-
isons with other products. We focus on the latter:
reports of experiences with products. In this paper,
we define the task, describe guidelines for manual
annotation and analyze linguistic features that can
be used in an automatic experience mining system.
2 Motivation
Our main use-case is user-centered design for prod-
uct development. User-centered design [2] is an in-
novation paradigm where users of a product are in-
volved in each step of the research and development
process. The first stage of the product design process
is to identify unmet needs and demands of users for
a specific product or a class of products. Forums,
?This research was supported by project STE-09-12 within
the STEVIN programme funded by the Dutch and Flemish gov-
ernments, and by the Netherlands Organisation for Scientific
Research (NWO) under projects 640.001.501, 640.002.501,
612.066.512, 612.061.814, 612.061.815, 640.004.802.
review sites, and mailing lists are platforms where
people share experiences about a subject they care
about. Although statements found in such platforms
may not always be representative for the general user
group, they can accelerate user-centered design.
Another use-case comes from online communi-
ties themselves. Users of online forums are often in-
terested in other people?s experiences with concrete
products and/or solutions for specific problems. To
quote one such user: [t]he polls are the only in-
formation we have, though, except for individual
[users] giving their own evaluations. With the vol-
ume of online data increasing rapidly, users need im-
proved access to previously reported experiences.
3 Experience mining
Experiences are particular instances of personally
encountering or undergoing something. We want
to identify experiences about a specific target prod-
uct, that are personal, involve an activity related to
the target and, moreover, are accompanied by judge-
ments or evaluative statements. Experience mining
is related to sentiment analysis and opinion retrieval,
in that it involves identifying attitudes; the key dif-
ference is, however, that we are looking for attitudes
towards specific experiences with products, not atti-
tudes towards the products themselves.
4 An explorative study
To assess the feasibility of automatic experience
mining, we carried out an explorative study: we
asked human assessors to find experiences in ac-
tual forum data and then examined linguistic fea-
tures likely to be useful for identifying experiences
automatically.
17
Mean and deviation in posts
Feature with exper. without exper.
subjectivity score2 0.07 ?0.23 0.17 ?0.35
polarity score2 0.87 ?0.30 0.77 ?0.38
#words per post 102.57 ?80.09 52.46 ?53.24
#sentences per post 6.00 ?4.16 3.34 ?2.33
# words per sentence 17.07 ?4.69 15.71 ?7.61
#questions per post 0.32 ?0.63 0.54 ?0.89
p(post contains question) 0.25 ?0.43 0.33 ?0.47
#I?s per post 5.76 ?4.75 2.09 ?2.88
#I?s per sentence 1.01 ?0.48 0.54 ?0.60
p(sentence in post contains I) 0.67 ?0.23 0.40 ?0.35
#non-modal verbs per post 19.62 ?15.08 9.82 ?9.57
#non-modal verbs per sent. 3.30 ?1.18 2.82 ?1.37
#modal verbs per sent. 0.22 ?0.22 0.26 ?0.36
fraction of past-tense verbs 0.26 ?0.17 0.17 ?0.19
fraction of present tense verbs 0.42 ?0.18 0.41 ?0.23
Table 1: Comparison of surface text features for posts
with and without experience; p(?) denotes probability.
We acquired data by crawling two forums on
shaving,1 with 111,268 posts written by 2,880 users.
Manual assessments Two assessors (both authors
of this paper) were asked to search for posts on five
specific target products using a standard keyword
search, and label each result post as:
? reporting no experience, or
? reporting an off-target experience, or
? reporting an on-target experience.
Moreover, posts should be marked as reporting an
experience only if (i) the author explicitly reports
his or someone else?s (a concrete person?s) use of
a product; and (ii) the author makes some conclu-
sions/judgements about the experience.
In total, 203 posts were labeled by the two asses-
sors, with 101 posts marked as reporting an experi-
ence by at least one assessor (71% of those an on-
target experience). The inter-annotator agreement
was 0.84, with Cohen?s ? = 0.71. If we merge
on- and off-target experience labels, the agreement
is 0.88, with ? = 0.76. The high level of agreement
demonstrates the validity of the task definition.
Features for experience mining We considered a
number of linguistic features and compared posts re-
porting experience (on- or off-target) to the posts
1www.shavemyface.com, www.menessentials.com/community
2Computed using LingPipe: http://alias-i.com/lingpipe
With experience Without experience
used 0.15, found 0.09,
bought 0.07, tried 0.07,
got 0.07, went 0.07, started
0.05, switched 0.04, liked
0.03, decided 0.03
got 0.09, thought 0.09,
switched 0.06, meant 0.06,
used 0.06, went 0.06, ig-
nored 0.03, quoted 0.03,
discovered 0.03, heard 0.03
Table 2: Most frequent past tense verbs following I in
posts with and without experience, with rel. frequencies.
with no experience. Table 1 lists the features and
the comparison results. Remarkably, the subjectiv-
ity score is lower for experience posts: this indicates
that our task is indeed different from sentiment re-
trieval. Experience posts are on average twice as
long as non-experience posts and contain more sen-
tences with pronoun I. They also contain more con-
tent (non-modal) verbs, especially past tense verbs.
Table 2 presents a more detailed analysis of the verb
use. Experience posts appear to contain more verbs
referring to concrete actions rather than to attitude
and perception. It is still to be seen, though, whether
this informal observation can be quantified using re-
sources such as standard semantic verb classification
(state, process, action), WordNet verb hierarchy or
FrameNet semantic frames.
5 Conclusions
We introduced the novel task of experience min-
ing. Users of products share their experiences, and
mining these could help define requirements for
next-generation products. We developed annotation
guidelines for labeling experiences, and used them
to annotate data from online forums. An initial ex-
ploration revealed multiple features that might prove
useful for automatic labeling via classification.
References
[1] K. Balog, G. Mishne, and M. de Rijke. Why are they
excited?: identifying and explaining spikes in blog
mood levels. In EACL ?06, pages 207?210, 2006.
[2] B. Buxton. Sketching User Experiences: Getting the
Design Right and the Right Design. Morgan Kauf-
mann Publishers Inc., 2007.
[3] I. Ounis, C. Macdonald, M. de Rijke, G. Mishne, and
I. Soboroff. Overview of the TREC 2006 Blog Track.
In TREC 2006, 2007.
[4] B. Pang and L. Lee. Opinion mining and senti-
ment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
2008.
18
