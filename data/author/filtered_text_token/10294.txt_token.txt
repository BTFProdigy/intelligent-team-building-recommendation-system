demonstration
,
alignment
,
monolingual
string
differencesadrien
lardilleux
,
lepagegreyc
,
caen
basse
normandie
,
caen
cedex
,
francefirstname
,
lastname
info
,
method
,
subsenten
tial
alignment
,
several
language
,
method
,
several
language
,
complexity
explosion
,
usual
pair
by
pair
processing
,
different
unit
,
character
,
morpheme
,
evaluation
,
word
alignments
,
trilingual
machine
translationcorpus
,
comparisonof
,
result
,
alignment
software
,
available
nowadays
,
alignmentof
pair
,
language
,
bilingualword
aligner
,
canperform
high
quality
alignment
,
wordsstatistics
,
efficient
tool
,
main
criticism
,
needsto
tune
numerous
parameter
,
result
,
particular
alignment
task
,
multilingual
alignment
,
language
pair
,
parameter
value
,
method
,
language
pair
,
result
,
complexity
explosion
,
multilingual
alignments
,
simard
,
bilingual
method
,
creative
commonsattribution
noncommercial
share
alike
,
creativecommons
,
version
,
languages
,
toalign
specific
unit
,
method
,
indifferentlyon
different
unit
,
language
,
character
based
approach
,
alternative
,
manylanguages
,
sentence
alignedparallel
corpus
,
differencesin
order
,
operation
,
well
known
similar
technique
,
hirschberg
,
string
,
subsequence
,
subsequence
,
sequence
,
contiguous
character
,
instance
,
space
character
,
underscore
,
statusas
,
character
,
please
,
character
,
string
difference
,
purpose
,
simplicity
,
thelcs
operation
,
difference
operation
,
plurality
,
several
isolated
character
,
result
,
malformed
string
,
interest
,
misinformed
string
,
resort
,
previous
,
please
,
character
,
difference
,
alignment
,
parallel
,
source
text
,
target
text
,
previous
englishsentences
,
translation
,
difference
be
tweena
andb
,
alignedtranslations
,
method
,
string
,
source
language
,
translation
,
string
,
target
,
numerousas
,
simplicity
,
thatlcsubstr
,
translation
equivalent
,
chicago
,
followingpairs
,
chicago
,
correct
,
boston
start
,
bosuton
,
ressya
ha
ban
syutu
masu
,
direct
application
,
method
describedabove
,
chicago
,
string
difference
,
theproblem
,
method
,
stringdifferences
,
chicago
,
chicago
,
thesame
process
,
parallel
,
string
,
constraint
thatthe
string
,
lcs
ubstr
,
a0containing
,
chicago
,
snwhere
snis
,
listof
,
substring
,
corresponding
differences
,
target
,
detail
,
execution
,
theiterative
process
,
large
amount
,
themethod
,
plurality
result
,
themmay
,
certain
number
,
weshall
,
quality
,
alignment
,
onthese
frequency
,
alignment
selectionin
practice
,
differences
,
particular
string
,
complexityexplosion
,
lcs
ubstr
,
non
reliable
alignment
,
chicago
,
achicago
,
detail
,
alignment
,
chicago
,
iterative
process
,
lcs
ubstr
,
complexity
,
onlythe
,
lcs
ubstr
,
predefined
threshold
,
threshold
,
thelongest
lcs
ubstr
,
different
value
,
cwere
testedfor
,
thisparameter
,
source
language
,
well
formedness
,
string
,
presence
,
n
sequences
ofcharacters
,
initial
data
,
inthe
target
language
,
alignment
,
frequency
,
reason
,
doingso
,
practice
,
lcs
ubstr
,
training
part
,
iws
lt
,
machine
translation
campaign
corpus
,
fordyce
,
bte
corpus
,
takezawa
,
result
,
advantage
,
methodis
,
string
,
thedata
,
sample
,
alignments
,
source
language
,
string
,
character
,
first
line
,
last
line
,
ostalignments
,
differ
,
expectedmeaning
,
slight
difference
,
particular
,
bilingual
word
alignment
,
language
,
japanese
,
oursystem
,
language
,
word
,
target
language
,
target
unit
,
different
value
,
parameter
value
,
good
result
,
eachsource
word
,
probable
pair
,
source
,
target
,
output
ofi
bm
model
,
produce
word
,
word
alignment
,
guaranty
,
single
target
word
,
processing
,
character
,
objective
assessment
,
totwo
bilingual
dictionary
,
bestresults
,
goodas
,
alignment
,
result
,
result
,
alignment
,
alignment
output
,
systemsthat
,
reference
,
method
,
extracharacter
,
alignmentto
,
english
arabic
dictionary
,
theedict
english
japanese
dictionary
,
monash
,
jwb
edict
,
part
,
thearabic
part
,
corpus
,
debiliand
achour
,
nglish
japanese
,
arigatou
,
nyuusyu
,
sample
,
alignment
,
source
language
,
japanesestrings
,
language
,
parameter
,
inthis
experiment
,
comparison
,
alignment
,
inthe
reference
dictionary
,
alignment
,
simple
method
,
subsen
tential
alignment
,
several
language
,
context
,
onunits
,
complexity
explosion
,
usual
pair
by
pair
processingby
,
simultaneous
application
,
amonolingual
operation
,
method
,
result
,
word
alignment
task
,
much
simpler
,
referencesdebili
,
hadhemi
achour
,
voyellationautomatique
,
proceeding
,
workshop
,
computational
approach
,
montreal
,
fordyce
,
overview
,
theiwslt
,
evaluation
campaign
,
proceedings
,
international
workshop
,
hirschberg
,
linear
space
algorithm
,
maximal
common
subsequence
,
communications
,
discriminative
framework
forbilingual
word
alignment
,
proceeding
,
humanlanguage
technology
conference
,
conferenceon
empirical
method
,
natural
language
processing
,
vancouver
,
columbia
,
oc
tober
,
ney
,
systematic
comparison
,
various
statistical
alignment
models
,
computational
linguistics
,
simard
,
text
translation
alignment
,
language
,
proceedings
,
joint
dat
nference
,
empiricalmethods
,
natural
language
processing
,
toshiyuki
,
eiichiro
sumita
,
fumiaki
sug
aya
,
hirofumi
yamamoto
,
seiichi
yamamoto
,
broad
coverage
bilingual
corpusfor
speech
translation
,
travel
conversation
,
thereal
world
,
proceeding
,
third
internationalconference
,
language
resource
,
palmas
,
grancanaria
,
workshop
,
statistical
machine
translation
,
edinburgh
,
association
,
bonneau
maynardhai
son
ure
,
lien
maxguillaume
wisniewskifranc
,
ois
yvonuniv
,
paris
sud
,
lim
si
cnrsb
,
orsay
cedex
,
francegilles
addajosep
,
cregoadrien
lardilleuxthomas
lavergneartem
sokolovlimsi
cnrsb
,
orsay
cedex
,
franceabstractthis
paper
,
lim
,
submission
,
sixth
workshop
,
statistical
machinetranslation
,
result
,
translation
task
,
direction
,
systemsuse
n
code
,
open
source
statistical
machine
translation
system
,
bilingualn
grams
,
french
english
task
,
efficient
,
advantage
,
heterogeneous
training
parallel
data
,
simple
filtering
strategy
,
bothprocessing
time
,
translation
quality
,
totranslate
,
ger
man
,
thesoul
language
model
,
machine
translation
,
significant
improvementswith
,10
gram
sou
model
,
several
alternative
tothe
standard
n
best
mer
procedure
,
lim
,
submission
,
workshop
,
statistical
machine
translation
,
lim
,
french
english
andgerman
english
task
,
direction
,
thisevaluation
,
open
source
,
provides
,
overview
,
data
preprocessing
,
filtering
step
,
large
amount
,
parallel
data
,
method
,
french
english
gigaword
corpus
,
ourprevious
participation
,
data
cleaning
,
non
negligible
part
,
thisincludes
detecting
,
otherlanguages
,
provided
development
set
,
monolingual
newsdata
,
amount
,
language
,
character
,
non
utf8
characterswhich
,
context
,
thegigaword
corpus
,
invisible
control
character
,
target
language
modeling
,
standard
back
off
n
gram
model
,
basedon
neural
network
,
sou
l
lm
,
anarbitrary
large
vocabulary
,
high
order
marko
vian
assumption
,
experimental
result
,
ble
score
,
translation
editrates
,
overviewour
in
house
n
code
smt
system
implement
thebilingual
n
gram
approach
,
statistical
machinetranslation
,
casacuberta
,
character
,
teletype
,
seventies
,
early
eighty
,
translation
hypothesis
,
linear
combination
,
feature
function
,
argmaxti1
,
weight
,
feature
function
,
translation
feature
,
log
score
,
translation
model
basedon
bilingual
unit
,
tuples
,
probability
,
translation
model
,
n
gram
assumption
,
refers
,
source
symbol
,
target
,
kth
tuple
,
bilingual
sentencepair
,
languagesare
,
tuples
,
context
information
,
translation
model
,
addition
,
translation
model
,
eleven
feature
functions
,
target
language
model
,
seesection
,
detail
,
lexicon
model
,
tillmann
,
aiming
,
orientation
,
next
translation
unit
,
distance
based
distortion
model
,
word
bonus
model
,
tuple
bonusmodel
,
compensate
,
system
preferencefor
short
translation
,
lexicon
model
,
standard
phrase
basedsystem
,
relative
frequencies
,
tuples
,
lexical
weight
,
generated
wordalignments
,
weight
,
functions
,
discriminativetraining
framework
,
detail
,
newstest2009
data
,
development
set
,
trainingcorpus
,
tuple
sequence
,
classical
smoothing
technique
,
tuples
,
froma
word
aligned
corpus
,
mgi
za
,
default
setting
,
unique
segmentation
,
bilingual
corpus
,
n
gram
model
,
figure
,
simple
,
unique
tuple
segmentation
,
word
aligned
pair
,
tuple
extraction
,
sequence
,
nul
word
,
source
side
,
whole
bilingual
training
data
,
tuples
,
n
gram
language
model
probabilities
,
thatthe
source
word
,
final
tuple
segmentation
,
target
word
,
original
order
,
decoding
,
word
lattice
,
reordering
hypothesis
,
word
order
modification
,
thetuple
extraction
process
,
reordering
hypothesis
,
hypothesis
,
usinga
set
,
fromthe
word
alignment
,
previous
,
perfect
translations
,
translation
,
swap
ofthe
word
,
frenchand
pair
,
information
,
generalizationpower
,
surface
word
form
,
refer2http
,
net
software310to
,
detail
,
tuple
extraction
,
reordering
rule
,
ata
preprocessing
,
selectionwe
,
available
parallel
data
,
word
alignments
,
french
english
task
,
united
nation
corpus
,
ourtranslation
model
,
target
languagemodels
,
monolingual
corpus
,
frenchand
,
parallel
corpus
,
treetagger
,
schmid
,
fine
grained
pos
information
usedfor
preprocessing
,
rft
ag
ger
,
schmid
,
tokenizationwe
,
advantage
,
in
house
text
processing
tool
,
tokenization
,
detokenizationsteps
,
normalizationtools
,
ble
score
,
papineni
,
true
case
,
complexthan
,
default
policy
,
word
form
,
plaguedwith
data
sparsity
,
difficulties
,
training
,
decoding
time
,
germanside
,
specific
pre
processingscheme
,
allauzen
,
lexical
redundancy
,
splittingcomplex
,
preprocessing
scheme
,
translate
,
german
,
output
,
preprocessing
,
inour
last
year
,
allauzen
,
preprocessing
step
,
atwo
step
decoding
,
decoding
step
,
forthis
direction
,
german
tokenizer
,
organizer
,
gigaword
corpusthe
available
parallel
data
,
english
french
includes
,
large
corpus
,
giga
word
parallel
corpus
,
corpus
,
large
portion
,
translating
news
text
,
first
filter
,
detectingforeign
language
,
perplexity
,
lexicalcoverage
,
subset
,
parallel
sentences
,
trigram
lm
,
frenchand
,
subset
,
availablenews
data
,
usedto
rank
,
corpus
,
perplexity
abovea
,
selected
set
,
following
experiments
,
threshold
,
median
,
upperquartile
value
,
perplexity
,
corpus
,
conventionaln
gram
model
,
schwenk
,
potential
meansto
,
conventional
n
gram
language
model
,
bottleneckwith
standard
nnl
m
,
computation
,
posterior
probability
,
output
layer
,
vocabulary
word
,
handling
,
large
vocabulary
,
consisting
,
hundred
,
infeasible
dueto
,
prohibitive
growth
,
computation
time
,
n
gram
distributions
,
frequent
word
,
thesoul
,
structured
output
layer
neural
network
,
language
model
,
vocabularies
,
arbitrary
size
,
setting
,
n
gram
lm
,
significantimprovement
,
data
sparsity
issue
,
drastic
increase
,
number
ofparameters
,
nnl
mhowever
,
increase
,
context
length
,
inputlayer
result
,
linear
growth
,
complexityin
,
schwenk
,
traininglonger
context
neural
network
model
,
language
model
,
selection
,
news
text
,
beginning
,
assumption
,
evaluation
,
language
,
development
corpus
,
target
language
model
,
development
set
,
vocabulary
,
order
tocover
different
period
,
development
set
,
first
one
,
corpus
istwo
year
,
targeted
time
period
,
therefore
,
second
development
corpus
,
dev2010
2011
,
bunchesof
,
newsdata
,
large
lm
,
vocabularywas
,
language
,
includingall
token
,
news
commentary
corpus
,
french
english
gi
gaword
corpus
,
frequent
propernames
,
monolingual
news
data
of2010
,
german
,
amountof
training
data
,
vocabulary
,
frequent
word
,
themonolingual
news
data
,
procedure
,
vocabulary
containing
around500k
word
,
language
,
training
data
,
constrained
task
,
several
set
,
standard
4
gram
lm
,
absolute
discounting
interpolatedwith
,
order
model
,
kneser
,
goodman
,
news
corpora
,
associated
coefficient
,
perplexity
,
dev2010
2011
,
lm
,
newstest2008
,
developmentdata
,
procedure
,
overestimatingthe
weight
,
l
mo
delwe
give
,
brief
overview
,
sou
l
lm
,
complete
trainingprocedure
,
classical
,
distributed
word
representation
,
output
vocabulary
,
clustering
tree
,
associated
subclasses
,
i
th
word
,
sequencec1
,
wordwi
,
clustering
tree
,
subclass
,
history
,
chain
rule
,
architecture
,
nnl
mto
estimate
,
distribution
,
depthd
,
sou
architecture
,
forthe
standard
model
,
output
layer
,
themain
difference
,
output
structure
,
several
layer
,
softmax
activation
function
,
first
softmax
layer
,
class
layer
,
output
subclass
layer
,
shortlist
,
special
,
subclasses
,
xperimental
resultsthe
experimental
result
,
thenewstest2010
corpus
,
evaluation
set
,
automatic
metric
,
script
,
detokenization
step
,
last
year
evaluation
,
amount
ofavailable
parallel
data
,
withabout
33m
,
context
spaceinput
layerhidden
layer
,
layerssub
class
layer
short
listfigure
,
architecture
,
structured
output
layerneural
network
language
model
,
corpus
,
nevertheless
,
salient
difference
,
gigaword
,
united
nation
corpora
,
corpus
drawn
,
different
website
,
someparts
,
news
text
,
whole
gigaword
corpus
,
united
nation
,
domain
,
illustration
,
united
nation
,
improves
performance
,
attempt
,
large
amount
,
parallel
data
,
translation
model
,
gigaword
corpora
,
last
data
set
,
result
,
reward
ofsentence
pair
selection
,
corpus
yield
,
significant
ble
improvement
,
translatingfrom
,
otherdirection
,
quartile
,
selection
,
median
line
,
overall
performance
,
theroom
,
improvement
,
accurate
dataselection
process
,
threshold
,
sophisticated
filteringstrategy
,
large
amountsystem
en2fr
fr2enbleu
ter
ble
u
,
english
french
translation
result
,
leu
score
,
newstest2010
withthe
nis
script
,
translation
model
,
wholegigaword
,
quartile
,
median
correspond
,
filtered
version
,
gigaword
,
mismatch
,
target
,
translation
model
,
translation
model
,
wordswhich
,
probabilities
,
probability
,
unknown
word
,
target
lmis
,
decoding
step
,
translation
task
,
impact
oftwo
different
pos
tagger
,
germanpart
,
parallel
data
,
result
,
slight
improvement
,
source
,
theother
direction
,
rft
agger
,
data
preprocessing
step
,
different
pos
tagger
,
model
training
,
system
en2de
de2enbleu
ter
ble
u
terrft
agger
,
translation
result
,
script
,
l
mo
delas
,
continuous
n
gram
model
,
sou
l
lm
canbe
,
prohibitive
increase
,
complexity
,
sou
l
lm
,
smt
pipeline
,
then
best
list
,
decoder
,
associated
weight
,
forthe
english
french
task
,
a
b
leu
improvement
of0
,
similar
trend
,4
gram
sou
l
lm
,
additional
ble
uimprovement
,
order
from4
,
important
gain
,10
gram
sou
llm
,
a
t
erdecrease
,
result
,
english
germantask
,
en2debleu
ter
ble
u
terw
ithout
,
translation
result
,
andenglish
,
a100
best
rescoring
,
sou
l
lm
,
different
order
,
margin
,
algorithm
,
system
optimization
,
standard
mer
procedure
,
suffer
,
instability
,
result
,
slowtraining
cycle
,
approximate
estimate
,
decoding
cycle
,
training
parameter
,
evaluation
,
several
alternatives
,
standard
n
best
mer
procedure
,
word
lattice
,
macherey
,
differentiable
variant
,
ble
uobjective
function
,
mer
cycle
,
specific
semiring
,
general
purpose
finite
state
automaton
framework
,
sokolovand
yvon
,
ble
uobjective
function
,
usual
ble
,
n
gram
count
,
ble
score
,
normal
n
gramcounts
,
expectations
,
n
gram
count
,
first
,
thebleu
score
,
hypotheses
,
n
best
list
,
alternative
optimizationmethods
,
performance
interms
,
ble
score
,
neitherapproach
,
significant
improvement
,
majority
,
exception
,
bbn
approach
,
hadalmost
,
n
best
mer
,
translation
direction
,
d
ditional
experiment
,
complementary
translation
model
,
additional
feature
,
substantial
improvement
,
inconclusive
experiment
,
classical
mer
,
result
,
submission
,
translation
task
,
direction
,
participation
,
n
code
,
open
source
statistical
machine
translation
system
,
bilingual
n
grams
,
contributionsare
threefold
,
n
grambased
system
,
state
of
the
art
performance
,
large
scale
task
,
automaticmetrics
,
byseveral
site
,
past
evaluation
,
significant
reward
,
data
selection
,
large
heterogeneous
data
source
,
gigaword
,
large
vocabulary
continuous
space
language
model
,
thesoul
model
,
significant
andconsistent
improvement
,
importantwork
,
preprocessing
,
participant
,
beingdone
,
several
time
,
difference
,
reliability
,
smt
system
evaluation
,
acknowledgmentthis
,
quaero
pro
gramme
,
state
agency
forinnovation
,
allauzen
,
durgar
el
kahlout
,
yvon
,
statistical
translation
system
,
thejoint
workshop
,
statistical
machine
translation
andmetricsmatr
,
uppsala
,
yoshua
bengio
,
andchristian
janvin
,
class
based
n
gram
model
,
natural
language
,
computational
linguistics
,
casacuberta
,
vidal
,
machine
translation
,
inferred
stochastic
finite
statetransducers
,
computational
linguistics
,
goodman
,
anempirical
study
,
technique
,
technical
report
tr
10
98
,
computer
science
group
,
josep
crego
,
improving
statistical
mt
,
machine
translation
,
adda
,
alexandre
allauzen
,
galibert
,
jean
luc
gauvain
,
ne
mey
nard
,
ois
yvon
,
statistical
translation
system
,
thenaacl
htl
statistical
machine
translation
workshop
,
columbus
,
goutte
,
kuhn
,
discriminative
instance
,
domain
adaptation
,
statistical
machine
translation
,
proceedingsof
,
conference
,
empirical
method
,
natural
language
processing
,
kneser
,
ney
,
improvedbacking
off
,
m
gram
language
modeling
,
proceedings
,
international
conference
,
acoustic
,
speech
,
alexandre
allauzen
,
jean
lucgauvain
,
ois
yvon
,
neural
network
language
model
,
iee
e
in
ter
national
conference
,
acoustic
,
speech
,
republic
,
macherey
,
och
,
thayer
,
uszkoreit
,
lattice
based
minimumerror
rate
training
,
statistical
machine
translation
,
emn
lp
,
och
,
minimum
error
rate
training
instatistical
machine
translation
,
annual
meeting
,
association
,
computational
linguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
annual
meeting
,
association
,
computational
linguistics
,
association
forcomputational
linguistics
,
antti
veikko
,
zhang
,
spyros
matsoukas
,
schwartz
,
wmt10
system
combination
task
,
proceeding
,
joint
fifth
workshop
,
statistical
machine
translation
,
association
,
computationallinguistics
,
helmut
schmid
,
law
,
estimationof
conditional
probability
,
decision
tree
,
anapplication
,
fine
grained
pos
,
proceedings
,
international
conference
,
computational
linguistics
,
coling
,
helmut
schmid
,
probabilistic
part
of
speech
tagging
,
decision
tree
,
internationalconference
,
new
method
,
language
processing
,
holger
schwenk
,
continuous
space
languagemodels
,
computer
,
artem
sokolov
,
ois
yvon
,
minimum
error
rate
training
,
proceeding
,15
thannual
conference
,
association
,
tillmann
,
unigram
orientation
modelfor
statistical
machine
translation
,
proceeding
ofh
lt
naacl
,
association
forcomputational
linguistics
,
taro
watanabe
,
suzuki
,
hajime
tsukada
,
hidekiisozaki
,
large
margin
training
,
statistical
machine
translation
,
proceeding
,
the2007
joint
conference
,
empirical
method
,
natural
language
processing
,
prague
,
republic
,
zen
,
sasa
hasan
,
ney
,
systematic
comparison
,
training
criterion
,
statistical
machine
translation
,
proceeding
,
the2007
joint
conference
,
empirical
method
,
natural
language
processing
