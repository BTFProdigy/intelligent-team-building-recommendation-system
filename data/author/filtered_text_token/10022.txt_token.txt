demonstration
,
sense
disambiguation
,
tree
structuredconditional
random
fieldsjun
hatori
,
yusuke
miyao
,
interdisciplinary
information
study
,
graduate
school
,
information
science
,
technology
,
text
mining
,
princess
street
,
manchester
,
school
,
computer
science
,
manchester
hatori
,
yusuke
,
usingtree
structured
conditional
random
field
,
tcr
f
,
dependency
treestructure
,
labelingproblem
,
tree
structure
,
incorporate
dependency
,
word
sens
,
treeedges
,
combination
,
coarse
grainedtagsets
,
improvement
,
wsd
accuracy
,
tree
structuredmodel
,
linear
chain
model
,
dataset
show
,
tcr
model
,
state
of
the
wsd
system
,
problem
,
computational
linguistics
,
appropriate
sense
,
task
setting
forwsd
,
lexical
sample
task
,
context
,
all
words
task
,
content
,
whilst
,
ofthe
wsd
research
,
lexical
sample
task
,
all
words
task
,
hatori
,
yusuke
miyao
,
ichi
tsu
jii
,
creative
commonsattribution
noncommercial
share
alike
,
creativecommons
,
le
attention
,
seriousknowledge
bottleneck
problem
,
necessary
step
,
practical
applications
,
urgent
need
,
theperformance
,
wsd
system
,
theall
words
task
,
novel
approach
,
all
words
task
,
tree
structured
conditional
random
field
,
inter
word
sense
dependency
,
incombination
,
wor
dnet
hierarchical
information
,
coarse
grained
tagset
,
super
senses
,
all
words
task
,
content
word
,
reasonable
toassume
,
assumptionthat
,
strong
sense
dependency
,
dependent
,
dependency
tree
,
dependency
tree
structures
,
wsd
system
,
inter
word
sense
dependency
,
mi
halcea
,
faruque
,
extent
,
knowledge
,
effectiveness
,
supervised
wsd
,
thelexicographers
,
file
id
,
wor
dnet
,
whicheach
noun
,
verb
synset
,
since43they
,
lexicographers
,
classification
,
expectedto
act
,
good
coarse
grained
semantic
category
,
supersenses
,
effectiveness
,
theuse
,
supersenses
,
coarse
grained
tagsetsfor
wsd
,
kohomban
,
mihalcea
,
number
ofa
sense
,
wor
dnet
,
frequency
,
sensenumber
,
powerful
feature
,
preference
,
frequent
sens
,
back
off
feature
,
ourmodel
,
output
,
first
sense
,
graph
based
probabilistic
discriminative
model
,
lafferty
,
linear
chain
crf
,
theprobabilistic
variable
,
tree
structure
,
linear
sequence
,
modelingthe
semantics
,
linear
structure
,
tcr
f
,
tow
sd
,
nlp
task
,
semantic
annotation
,
semantic
structure
,
formulation
,
conditional
probability
,
observation
sequence
xi
,
vertex
,
fjand
gkare
,
feature
vector
,
avertex
,
weight
vector
,
normalization
function
,
detailed
description
,
tcr
f
,
confidence
,
beginning
,
dependency
parser
,
left
hand
side
off
igure
,
dependency
tree
,
ll
x
dependency
format
,
outputted
tree
,
tree
ofcontent
word
,
right
hand
sideof
figure
,
onthe
disambiguation
,
function
word
,
labeling
task
ontree
structure
,
probability
,
word
sens
,
vertex
andedge
feature
,
information
,
wor
dnet
,
sense
label
,
synsets2
,
supersenses4
,
superordinate
synset
,
topmost
level
,
wor
dnet
hierarchy
,
allthese
label
,
vertex
,
features
,
following
,
vertex
,
usedby
,
speech
ofthe
head
,
dependent
,
dependency
tree
,
statistic
,
vertex
feature
thesense
number
,
possible
sense
bigram
,
combination
,
sense
bigram
,
dependency
relation
label
,
function
word
,
edge
feature
,
correspond
,
main
evaluation
data
,
brown
,
brown
,
last
file
,
categories
,
brown
,
development
,
andthe
rest
,
brown
,
brown
,
training
,
seval
,
all
words
data
,
snyderand
,
statistic
,
data
set
,
dependency
parser
,
tsujii
,
thetcrf
model
,
development
phase
,
parameter
,
l2regularization
,
contentwords
,
wor
dnet
synset
,
recall
,
precision
,
overall
performanceof
,
model
,
vertex
feature
,
sn
edge
modelsmakes
use
,
edge
feature
,
withsystem
recallpnnl
,
simil
prime
,
kohomban
,
decadt
,
mihalcea
,
comparison
,
performance
ofw
sd
system
,
en
glish
all
words
test
,
possible
combinations
,
sense
label
,
all
edg
,
features
,
dependency
relation
label
,
linear
,
thelinear
chain
model
,
moreedge
feature
,
no
edgeand
baseline
model
,
all
edg
,
recall
,
twodata
set
,
no
edge
model
,
stratified
shuffling
test
,
difference
,
theexception
,
s3
edge
model
,
tree
structured
model
all
edg
,
linear
chain
model
all
edg
,
linear
,
data
set
,
brown
1and
brown
,
thesenseval
all
words
task
data
,
comparison
,
state
of
the
wsd
system
,
difference
,
amount
,
training
data
,
tcr
modelis
,
state
of
the
wsd
system
,
simil
prime
,
kohomban
,
sense
annotated
data
,
wor
dnet
,
improvement
,
theyare
,
becausesense
bigram
feature
,
corpus
,
system
output
,
firstsenses
,
output
,
all
edg
,
first
sens
,
training
data
,
instance
weightingtechnique
,
combination
,
several
classifier
,
whichour
system
,
brown
brown
1model
recall
offset
,
recall
offset
correctall
edge
,
statistical
significanceof
,
improvement
,
no
edge
model
,
offset
,
novel
approach
,
theall
words
wsd
,
tcr
f
,
proposalsare
twofold
,
tree
structured
crf
sto
dependency
tree
,
bigramsof
fineand
coarse
grained
sens
,
edge
feature
,
sense
dependency
features
,
wsd
accuracy
,
combination
,
coarse
grained
tagsetsare
,
data
sparseness
problem
,
tree
structured
modeloutperforms
,
linear
chain
model
,
thatdependency
tree
,
representing
semantic
dependency
,
simple
framework
,
state
of
the
wsd
system
,
sense
annotated
resource
,
sophisticated
machine
,
technique
,
great
potential
,
improvement
,
broad
coveragesense
disambiguation
,
information
,
supersense
sequence
tagger
,
theconf
,
empirical
method
,
empirical
method
,
optimization
,
memory
based
wsd
,
evaluation
,
thesemantic
analysis
,
semantic
class
,
word
sense
disambiguation
,
inp
roc
,
annual
meeting
,
pereira
,
conditional
random
field
,
probabilistic
model
,
labeling
sequence
data
,
empirical
evaluation
,
knowledge
source
,
algorithmsfor
word
sense
disambiguation
,
empirical
method
,
faruque
,
senselearner
,
word
sense
,
open
text
,
barcelona
,
ciaramita
,
supersenselearner
,
combiningsenselearner
,
supersense
,
coarse
semantic
feature
,
semantic
evaluation
,
semeval
,
tsujii
,
maximum
entropy
estimation
,
tsujii
,
dependency
,
anddomain
adaptation
,
lr
model
,
ensembles
,
ll
shared
task
sessionof
emn
,
all
wordstask
,
theevaluation
,
semantic
analysis
oft
ext
,
tree
structured
conditional
random
field
,
semantic
annotation
,
supervised
maximum
entropy
approach
,
word
sense
disambiguation
,
semeval
,
roceedings
,50
th
annual
meeting
,
association
,
computational
linguistics
,
republic
,
association
,
computational
linguisticsincremental
joint
approach
,
anddependency
parsing
,
chinesejun
hatori1
takuya
matsuzaki2
yusuke
miyao2
,
ichi
tsujii31university
,
tokyo
hongo
,
bunkyo
,
informatics
hitotsubashi
,
chiyoda
,
japan3microsoft
research
d
,
street
,
haidian
district
,
comabstractwe
,
first
joint
model
,
dependency
,
extension
,
incrementaljoint
model
,
pos
tagging
,
dependency
parsing
,
hatori
,
efficientcharacter
based
decoding
method
,
dependency
parsing
model
,
method
,
comparable
state
,
different
characteristic
,
incremental
framework
,
baseline
model
,
pos
tagging
,
fordependency
,
comparisonexperiments
,
joint
model
,
natural
language
,
word
segmentation
,
crucial
first
step
,
necessaryto
perform
,
nlp
task
,
theword
level
information
,
thepos
tag
,
segmentation
,
form
thebasic
foundation
,
statistical
nlp
,
word
segmentation
,
pos
tagging
,
strong
interaction
,
many
study
,
joint
word
segmentation
,
pos
tagging
,
language
,
kruengkrai
,
becausesome
,
segmentation
,
grammatical
construction
,
sequence
,
pos
tag
,
joint
approach
,
word
segmentation
andpos
tagging
,
word
segmentation
,
researcher
,
joint
approachto
pos
tagging
,
dependency
parsing
,
hatori
,
ha
,
incremental
approachto
,
joint
task
,
context
,
furthera
question
,
joint
framework
,
word
segmentation
,
dependency
parsing
interact
,
following
chinesesentences
