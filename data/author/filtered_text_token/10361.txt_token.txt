proceeding
,
international
conference
,
computational
linguistics
,
coling
,
evaluation
,
machine
translation
system
using
,
constructed
linguistic
checkpoints
ming
zhou1
,
wang2
,
shujie
liu2
,
mu
li1
,
dongdong
zhang1
,
tiejun
zhao2
,
research
beijing
,
mingzhou
,
dozhang
microsoft
,
com
2h
,
technology
harbin
,
bowang
,
shujieliu
,
tjzhao
mtlab
,
diagnostic
evaluation
platform
,
multi
factored
evaluation
,
constructed
checkpoints
,
checkpoint
,
ambiguous
word
,
noun
phrase
,
verb
obj
collocation
,
prepositional
phrase
etc
,
linguistic
taxonomy
,
method
,
automatically
extract
checkpoints
,
checkpoints
,
method
,
a
m
system
,
important
linguistic
phenomenon
,
diagnostic
evaluation
,
effectiveness
,
diagnostic
evaluation
,
mt
system
,
automatic
mt
evaluation
,
crucial
issue
,
mt
system
developer
,
state
of
the
methods
,
automatic
mt
evaluation
,
pa
pineni
,
variant
,
invention
,
ble
score
,
benchmark
,
mt
system
evaluation
,
nevertheless
,
research
community
,
deficiency
,
callison
burch
,
instance
,
eu
fails
,
vitality
,
natural
language
,
creative
common
attribution
noncommercial
share
alike
,
license
,
creativecommons
,
linguistic
significance
,
consecutive
gram
,
skipped
gram
,
certain
linguistic
relation
,
translation
,
lexicon
,
reference
,
variation
,
lexical
choice
,
optimizing
,
statistical
mt
system
,
different
architecture
,
common
deficiency
,
state
of
the
evaluation
approach
,
inform
mt
developer
,
detailed
strength
,
mt
system
,
therefore
,
capability
,
certain
module
,
mt
system
,
capability
,
certain
kind
,
language
phenomenon
,
purpose
,
diagnostic
evaluation
approach
,
feedback
,
translation
ability
,
mt
system
,
various
important
linguistic
phenomenon
,
novel
diagnostic
evaluation
approach
,
general
score
,
mt
system
,
capability
,
various
important
linguistic
test
,
checkpoints
,
checkpoint
,
motivated
unit
,
ambiguous
word
,
noun
phrase
,
verb
obj
collocation
,
prepositional
phrase
etc
,
linguistic
taxonomy
,
diagnostic
evaluation
,
reference
,
checkpoint
,
corresponding
part
,
evaluation
,
candidate
translation
,
reference
,
checkpoints
,
extraction
,
checkpoints
,
automatic
process
,
word
aligner
,
parser
,
word
aligner
,
parser
,
tolerable
scope
,1121
reliable
subset
,
checkpoints
,
reference
,
confidence
,
checkpoints
,
various
kind
,
performing
diagnostic
evaluation
,
mt
system
,
addition
,
checkpoints
,
ranking
,
mt
systems
,
additional
feature
,
document
level
,
overview
,
process
,
diagnostic
evaluation
,
design
,
checkpoint
taxonomy
,
detail
,
construction
,
checkpoint
database
,
method
,
aligner
,
parser
,
matching
approach
,
different
mt
systems
,
capability
,
diagnostic
evaluation
,
checkpoints
,
current
ranking
method
,
mt
system
,
compares
,
related
evaluation
approaches
,
overview
,
diagnostic
evaluation
,
implementation
,
checkpoint
database
,
qualified
checkpoints
,
process
,
large
amount
,
parallel
sentences
,
book
collection
,
source
language
,
target
language
,
word
alignment
,
category
,
checkpoints
,
checkpoints
,
parsed
sentence
pair
,
reference
,
checkpoint
,
source
language
,
word
alignment
,
extracted
checkpoint
database
,
diagnostic
evaluation
,
mt
system
,
following
step
,
database
,
category
,
checkpoints
,
checkpoint
,
matched
n
grams
,
credit
,
mt
system
,
checkpoint
,
necessary
normalization
,
credit
,
category
,
credit
,
checkpoints
,
category
,
credit
,
mt
system
,
credit
,
category
,
category
group
,
detail
information
,
n
gram
matching
,
checkpoint
,
developer
,
mt
system
,
linguistic
checkpoint
taxonomy
,
taxonomy
,
automatic
diagnostic
evaluation
,
diagnostic
result
,
sophisticated
category
,
capability
,
current
nlp
tool
,
consideration
,
machine
translation
,
manual
taxonomy
,
capability
,
parser
,
taxonomy
,
typical
check
pints
,
phrase
,
representative
checkpoints
,
different
level
,
checkpoints
,
ew
word
,
name
entity
,
popular
word
,
object
,
passive
voice
,
our
implementation
,
checkpoint
taxonomy
,
category
,
checkpoint
taxonomy
,
category
,
taxonomy
,
practice
,
parser
,
subject
predicate
phrase
predicate
object
phrase
preposition
object
phrase
measure
phrase
location
phrase
sentence
level
ba
sentence
bei
sentence
shi
sentence
you
sentence
sentence
table
,
checkpoint
taxonomy
construction
,
checkpoint
database
given
,
bilingual
corpus
,
word
alignment
,
construction
,
checkpoint
database
,
information
,
pos
tag
,
dependency
structure
,
constituent
structure
,
parser
,
checkpoints
,
different
category
,
checkpoints
,
word
level
category
,
idiom
,
ambiguous
word
,
human
made
dictionary
,
checkpoints
,
new
word
,
new
word
list
,
human
made
rule
,
certain
category
,
checkpoint
,
word
alignment
information
,
corresponding
target
language
portion
,
reference
,
checkpoint
,
following
,
process
,
checkpoints
,
t
hey
,
building
,
reserve
fund
,
pos
tagging
,
dependency
result
,
word
alignment
,
checkpoint
extraction
t
,
category
,
checkpoints
,
different
schema
,
syntactic
analysis
,
constitute
structure
,
dependency
structure
,
parser
,
a
c
hinese
skeleton
parser
,
statistical
parser
,
next
,
multiple
parser
,
high
confident
checkpoints
,
word
alignment
,
parser
,
reliability
,
checkpoints
,
parser
,
high
quality
word
level
checkpoints
,
state
of
the
pos
tagger
,
precision
,
dictionary
,
various
purpose
,
parser
tag
,
category
,
phrase
level
,
parser
,
high
accuracy
,
checkpoints
,
multiple
parser
,
intersection
,
parser
result
,
improvement
,
column
,
precision
,
phrase
,
parser
,
column
,
precision
,
intersection
,
column
,
reduction
,
checkpoints
,
intersection
result
,
test
corpus
,
predicate
,
building
,
treebank
,
training
corpus
,
statistical
parser
,
s
tf
brk
inter
tpts
,
parser
,
intersection
,
berkelry
,
impact
,
alignment
noise
except
,
reference
,
new
word
,
idiom
checkpoints
,
reference
,
quality
,
alignment
accuracy
,
aligner
,
lexical
dictionary
,
reliability
,
suppose
,
checkpoint
,
reference
,
source
side
,
rcw
ordcntscdicrccocntmaxrcdm
,
word
bag
,
dictionary
translation
,
source
word
,
coc
nt
,
common
word
,
wordcnt
,
limitation
,
reference
,
dm
score
,
evaluation
,
reliability
,
checkpoints
,
impact
,
checkpoint
database
,
human
aligned
corpus
,
corpus
,
different
smt
systems4
,
spearman
correlation
,
ranked
list
,
evaluation
result
,
hese
system
,
in
house
phrase
,
smt
engine
,
different
parameter
set
,
correlation
score
,
impact
,
mistake
,
word
alignment
,
subset
,
database
,
impact
,
corpus
size
,
system
level
,
high
correlation
,
different
corpus
size
,
category
level
,
correlations
,
category
,
small
size
,
corpus
size
,
result
,
impact
,
alignment
quality
,
corpus
size
,
large
scale
,
checkpoints
,
checkpoint
database
,
extra
cost
,
effort
,
proper
scale
,
impact
,
word
alignment
,
different
size
,
test
corpus
,
checkpoints
,
evaluation
evaluation
,
multiple
option
,
certain
linguistic
category
,
categories
,
entire
taxonomy
,
instance
,
translation
task
,
a
m
developer
,
ability
,
idiom
checkpoints
,
database
,
reference
,
checkpoints
,16
k
ambiguous
word
,
pronoun
,
adverb
,
preposition
,
quantifier
,
repetitive
word
,
collocation
,
subject
predicate
phrase
,
predicate
object
phrase
,
preposition
object
phrase
,
measure
phrase
,
location
phrase
,
calculate
,
credit
,
different
occasion
,
matching
,
reference
,
checkpoint
,
credit
,
checkpoint
,
checkpoint
,
component
,
word
sequence
,
following
,
splitting
,
matching
,
reference
,
atched
n
grams
,
checkpoint
,
reference
,
candidate
translation
,
atched
n
grams
,
word
inflection
,
different
option
,
granularity
,
normal
,
matching
,
exact
form
,
matching
,
lowercase
,
matching
,
checkpoint
,
reference
,
translation
,
w
hen
,
recall
,
single
checkpoint
,
category
,
category
group
,
checkpoint
,
reference
,
recall
,
redundancy
,
length
,
average
length
,
length
,
average
length
,
t
hen
,
final
score
,
penaltyccscore
,
experiments
,
mt
system
diagnoses
,
ability
,
diagnosis
,
mt
system
,
diagnostic
evaluation
,
smt
system
,
strength
,
shortcoming
,
a
s
mt
system
,
rmt
system
,
test
corpus
,
nis
t05
test
data
,
checkpoints
,
irst
smt
system
,
implementation
,
classical
phrase
,
second
smt
system
,
decoder
,
preprocess
,
long
phrase
,
syntax
structure
,
chiho
li
,
smt
system
,
popular
internet
service
,
rmt
system
,
popular
commercial
system
,
first
experiment
,
result
,
significant
difference
,
diagnostic
result
,
information
,
result
,
systems
perform
,
word
level
category
,
phrase
level
category
,
result
,
benefit
,
reordering
,
complex
phrase
,
t
statistic
score
,
category
score
,
evaluation
,
random
copy
,
replacement
,
absolute
score
,
difference
,
sample
,
instance
,
checkpoint
,
evaluation
,
gramncountgramnmatchrdmr
,
pronoun
,
adverb
,
preposition
,
repetitive
word
,
phr
as
subject
predicate
,
measure
phrase
,
location
phrase
,
groups
word
,
phr
as
,
diagnose
,
country
,
reference
,
country
,
system
a
translation
,
prime
,
dex
in
,
search
,
system
b
translation
,
prime
,
dex
in
,
search
,
country
,
instance
,
checkpoint
,
system
a
system
b
r
,
match
total
,
match
total
,
n
gram
matching
rate
,
second
experiment
,
result
,
ble
score
,
diagnostic
result
,
category
,
significant
difference
,
option
,
lowercase
,
diagnostic
result
,
categories
,
category
,
pronoun
,
preposition
,
result
,
key
difference
,
mt
system
,
open
category
,
context
,
closed
category
,
linguistic
rule
,
result
,
experiment
demonstrate
,
diagnostic
evaluation
,
information
,
capability
,
various
important
linguistic
category
,
single
system
score
,
specific
difference
,
mt
system
,
system
level
performance
,
mt
system
,
different
architectures
,
diagnostic
evaluation
,
developer
,
direction
,
category
,
diagnostic
evaluation
,
system
translation
,
reference
,
checkpoint
,
developers
,
mt
system
,
single
instance
,
experiments
,
mt
system
,
general
evaluation
,
system
level
,
goal
,
state
of
the
evaluation
methods
,
n
gram
metric
,
absence
,
linguistic
knowledge
,
many
,
linguistic
features
,
evaluation
,
evaluation
,
smt
system
,
ranking
problem
,
different
linguistic
feature
,
dependency
relation
,
translation
candidate
,
dependency
matching
rate
feature
,
ranking
accuracy
,
dependency
structure
,
linguistic
category
,
approach
showcase
,
extensive
feature
,
linguistic
category
,
ranking
,
smt
system
,
i
experiment
,
linguistic
category
,
dependency
,
popular
metric
,
features
,
mt
system
,
svm
light
,
ranking
experiment
,
workshop
data
,
m
translation
,
threefold
cross
validation
,
document
level
,
spearman
score
,
type
normal
lower
stem
ambiguous
word
,
new
word
,
pronoun
,
preposition
,
collocation
,
subject
predicate
phrase
,
predicate
object
phrase
,
calculate
,
correlation
,
human
assessments
,
result
,
different
feature
set
,
document
level
,
human
assessment
,
baseline
metric
,
correlation
score
,
exact
form
,
meteor
,
document
level
,
meteor
,
exact
syn
,
wn
_
synonymy
module
,
document
level
,
result
,
ability
,
linguistic
feature
,
mean
correlation
ble
,
table11
,
dependency
,
linguistic
category
,
ean
correlation
ble
,
comparison
,
related
,
many
extension
,
mte
evaluation
system
,
checkpoints
,
machine
translation
system
,
human
craft
linguistic
taxonomy
,
checkpoints
,
checkpoints
,
human
expert
,
new
test
corpus
,
checkpoints
,
limitation
,
binary
score
,
credit
,
n
gram
matching
rate
,
coverage
,
different
level
,
many
,
n
gram
,
burch
et
,
inadequate
accuracy
,
evaluation
,
common
subsequence
,
bigram
statistic
,
anerjee
,
unigrams
,
surface
form
,
syntactic
feature
,
unlabeled
head
modifier
dependency
,
mt
quality
,
marquez
,
linguistic
feature
,
level
,
dependency
relation
,
mt
evaluation
,
ranking
problem
,
correlation
,
human
assessment
,
many
difference
,
n
gram
,
method
,
n
gram
approach
,
collection
,
different
length
,
specific
linguistic
phenomenon
,
collection
,
checkpoints
,
clear
linguistic
taxonomy
,
furthermore
,
n
gram
approach
,
general
score
,
system
level
,
system
diagnosis
,
linguistic
category
,
information
,
developer
,
concrete
strength
,
addition
,
general
score
,
different
architecture
,
similar
general
score
,
multiple
linguistic
level
,
latent
differences
,
future
,
diagnostic
evaluation
method
,
linguistic
checkpoints
,
contrast
,
metric
,
general
score
,
evaluation
system
,
developer
,
strength
,
mt
system
,
specific
linguistic
category
,
category
group
,
different
,
checkpoints
,
checkpoint
database
,
word
alignment
,
parsing
,
problem
,
parser
result
,
reference
,
confidence
score
,
large
quantity
,
checkpoints
,
method
,
specific
difference
,
mt
system
,
similar
architecture
,
different
architecture
,
linguistic
checkpoints
,
new
feature
,
ranking
task
,
mt
system
,
diagnostic
evaluation
method
,
language
pair
,
language
pair
,
parser
,
word
aligner
,
current
proposal
,
human
made
linguistic
system
,
interesting
problem
,
future
,
taxonomy
,
alon
lavie
,
automatic
metric
,
mt
evaluation
,
improved
correlation
,
human
judgement
,
proceeding
,
acl
workshop
,
intrinsic
,
extrinsic
evaluation
measure
,
machine
translation
,
summarization
,
callison
burch
,
mile
,
philipp
koehn
,
machine
translation
research
,
proceeding
,
chapter
,
chodorow
,
leacock
,
unsupervised
method
,
grammatical
error
,
meeting
,
north
chapter
,
large
scale
support
vector
machine
learning
practical
,
advance
,
kernel
method
,
gimenez
,
llis
marquez
,
linguistic
feature
,
automatic
evaluation
,
heterogeneous
mt
system
,
workshop
,
statistical
machine
translation
,
conjunction
,
klein
,
manning
,
accurate
unlexicalized
parsing
,
proceeding
,
meeting
,
philipp
koehn
,
statistical
significance
test
,
machine
translation
evaluation
,
emn
lp
,
barcelona
,
yi
guan
,
robabilistic
approach
,
syntax
based
reordering
,
proceeding
,45
th
ac
,
chin
yew
,
och
,
automatic
evaluation
,
machine
translation
quality
,
common
subsequence
,
bigram
statistics
,
proceeding
,
gildea
,
syntactic
feature
,
evaluation
,
intrinsic
,
extrinsic
evaluation
measures
,
machine
translation
,
summarization
,
shuxin
liu
,
linguistics
,
contemporary
chi
nese
language
,
education
publisher
,
foundation
,
grammar
,
och
,
ney
,
ystematic
comparison
,
various
statistical
alignment
models
,
computational
linguistics
,
kishore
papieni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
proceeding
,
shiwen
,
automatic
evaluation
,
output
quality
,
machine
translation
system
,
proceedings
,
evaluator
,
le
ras
,
chinyew
,
ranking
problem
,
workshop
,
statistical
machine
translation
,
conjunction
,
lock
based
robust
dependency
parser
,
unrestricted
text
,
proceeding
,
second
language
processing
workshop
,
conjunction
,
poster
volume
,
beijing
,
discriminative
itg
alignment
,
hierarchical
phrase
pair
,
semi
supervised
training
,
computer
science
,
technology
,
microsoft
research
chl
,
com
,
many
desirable
property
,
word
alignment
,
limitation
,
one
to
one
matching
,
limitation
,
phrase
pair
,
a
i
tg
formalism
,
noncontiguous
word
,
hierarchical
phrase
pair
,
parameter
estimation
method
,
unsupervised
learning
,
itg
formalism
,
itg
alignment
system
,
word
alignment
quality
,
translation
performance
,
adaptation
,
bilingual
parsing
,
language
,
phrasal
,
word
level
alignment
,
byproduct
,
biased
towards
short
distance
reordering
,
word
alignment
model
,
reason
itg
,
attention
,
word
alignment
community
,
haghighi
,
basic
itg
formalism
suffers
,
drawback
,
one
to
one
matching
,
limitation
render
,
certain
alignment
pattern
,
first
author
,
visiting
microsoft
research
,
alignment
,
reason
,
notion
,
phrase
,
sequence
,
contiguous
word
,
itg
formalism
,
haghighi
,
alignment
pattern
,
phrase
,
simple
,
clause
,
connective
,
connective
,
variable
,
connective
,
noncontiguous
connective
,
alignment
pattern
,
phrase
level
itg
,
hierarchical
phrase
based
smt
,
chiang
,
phrase
based
smt
,
incorporate
hierarchical
phrase
pair
,
henceforth
h
phrase
pair
,
a
i
tg
formalism
,
h
phrase
pair
,
itg
model
,
parameters
,
phrase
h
phrase
pair
,
probability
,
parameter
,
learning
,
repertoire
,
phrase
pair
,
human
annotated
alignment
,
useful
feature
,
semi
supervised
learning
algorithm
,
discrimina
730tive
training
,
error
minimization
,
approximate
,
estimation
,
numerous
parameter
,
learning
algorithm
,
experiment
result
,
alignment
quality
,
step
by
step
,
hierarchical
phrase
pair
,
itg
formalism
,
itg
parsing
,
semi
supervised
training
method
,
complete
system
,
experiments
,
tg
formalism
,
formulation
,
itg
contains
,
terminal
unary
rule
,
represent
word
,
foreign
language
,
binary
rule
,
component
,
foreign
phrase
,
inverted
order
,
viewpoint
,
word
alignment
,
terminal
unary
rule
,
word
pair
,
binary
rule
,
reordering
factor
,
alignment
,
phrase
pair
,
alignment
,
word
pair
,
figure
,
language
share
,
figure
,
figure
,
f2
f1
f2e2e1e2e1f1
f2e2e1f1
f2
f3e3
figure
,
multi
word
span
pair
,
formulation
,
drawback
,
simple
itg
,
word
alignment
,
sole
purpose
,
instance
,
consecutive
word
pair
,
problem
,
redundancy
,
itg
normal
form
,
itg
normal
form
grammar
,
appendix
,
second
drawback
,
itg
fails
,
certain
alignment
pattern
,
constraint
,
strong
limitation
,
multi
word
expression
,
single
word
,
constraint
,
inside
out
,
inside
out
alignment
,
phrase
pair
single
word
,
language
,
single
word
,
language
,
white
house
,
problem
,
single
word
,
one
to
one
constraint
,
w
i
tg
,
serious
limitation
,
reality
,
always
segmentation
,
tokenization
error
,
idiomatic
expression
,
research
,
addition
,
word
pair
,
phrase
,
sequence
,
source
language
word
,
sequence
,
target
language
word
,
method
,
w
i
tg
,
definition
,
terminal
production
,
phrase
,
figure
,
phrase
,
foreign
phrase
,
simple
phrase
pair
white
house
,
itg
rule
,
joint
probability
model
,
phrase
alignment
space
,
translation
lexicon
,
phrase
,
consideration
,
suboptimal
word
alignment
,
phrase
extraction
,
method
suffers
,
computational
complexity
,
possible
phrase
,
possible
alignment
,
efficient
method
,
search
space
,
high
confidence
word
alignment
,
pi
tg
,
phrase
pair
,
word
alignment
matrix
,
simpler
word
alignment
model
,
hp
hrase
,
pi
tg
,
first
enhancement
,
linguistic
phenomenon
,
language
,
single
unit
,
single
unit
,
language
,
pi
tg
,
contiguous
word
,
single
unit
,
single
unit
,
noncontiguous
word
,
single
word
,
corresponds
,
following
sentence
pair
,
last
weekend
,
matter
,
pi
tg
,
phrase
based
smt
,
notion
,
phrase
pair
,
possible
contiguous
sequence
,
serious
data
sparseness
,
lesson
,
hierarchical
phrase
based
smt
,
modeling
,
noncontiguous
word
sequence
,
rule
involving
h
phrase
pair
,
placeholder
,
phrase
pair
,
last
weekend
,
phrase
pair
,
reordering
,
well
known
,
chiang
,
following
bilingual
sentence
fragment
,
diplomatic
relation
,
north
,
potential
,
intra
phrase
reordering
,
alignment
pattern
,
inside
out
,
pattern
,
h
phrase
pair
,
a
i
tg
formalism
,
simple
phrase
pair
,
hierarchical
one
,
itg
grammar
,
phrase
,
figure
,
foreign
language
,
format
,
hp
itg
,
pi
tg
,
h
phrase
pair
,
itg
parsing
,
next
,
important
question
,
h
phrase
pair
,
pi
tg
,
h
phrase
pair
,
h
phrase
pair
,
word
alignment
matrix
,
simpler
word
alignment
model
,
i
tg
parsing
,
w
i
tg
word
alignment
,
similar
,
step
applies
,
relevant
terminal
unary
rule
,
word
pair
,
word
pair
,
span
pair
,
possible
,
larger
,
span
pair
,
figure
,
possible
derivation
,
toy
sentence
pair
,
rectangle
,
certain
phrase
category
,
h
phrase
pair
,
h
phrase
pair
,
constraint
,
f
span
,
span
,
e
span
,
upper
half
,
rectangle
,
associated
alignment
hypothesis
,
figure
,
derivation
,
alignment
hypothesis
,
various
derivation
,
itg
parsing
,
hypergraph
,
figure
,
hypernode
,
rectangle
,
span
pair
,
upper
half
,
possible
alignment
hypothesis
,
span
pair
,
hyperedges
,
span
pair
,
span
pair
,
hypernode
,
alignment
hypothesis
,
hypernode
,
topmost
hypernode
,
figure
,
normal
form
,
hypothesis
,
span
pair
,
pi
tg
parsing
,
span
pair
,
possible
combination
,
sub
span
pair
,
binary
rule
,
span
pair
,
phrase
pair
,
span
pair
,
valid
leaf
node
,
parse
tree
,
parse
tree
produce
,
complete
word
,
byproduct
,
alignment
link
,
phrase
pair
,
phrase
pair
,
matrix
,
simpler
model
,
alternative
alignment
hypothesis
,
span
pair
,
hp
itg
parsing
,
itg
rule
,
hierarchical
rule
,
span
pair
check
,
lexical
anchor
,
span
pair
check
,
remaining
word
,
sub
span
pair
,
reordering
constraint
,
category
,
itg
normal
form
grammar
,
condition
,
span
pair
,
alignment
hypothesis
,
alignment
link
,
lexical
anchor
,
sub
span
pair
,
e2
e3f1
f2
,
figure
,
h
phrase
,
phrase
pair
,
h
phrase
pair
,
matrix
,
w
i
tg
,
phrase
pair
,
word
pair
,
other
,
h
phrase
pair
,
word
pair
,
h
phrase
pair
,
h
phrase
pair
,
variable
,
similar
,
complexity
,
attempt
,
method
,
gildea
,
probability
,
span
pair
,
gildea
,
dynamic
programming
,
outside
score
,
span
pair
,
tic
tac
toe
,
method
,
original
formulation
,
generative
model
,
itg
tree
,
parameters
,
nonindependent
feature
,
generative
model
,
therefore
,
haghighi
,
discriminative
model
,
incorporate
feature
,
state
of
alignment
performance
,
discriminative
model
,
alignment
candidate
,
probability
,
log
linear
model
,
alignment
matrix
,
weight
,
corresponding
feature
,
discriminative
version
,
w
i
tg
,
pi
tg
,
hp
itg
,
hp
ditg
,
parameter
,
ost
feature
,
probability
,
phrase
h
phrase
pair
,
data
set
,
limited
size
,
feature
value
,
parameter
,
feature
weight
,
error
minimization
method
,
discriminative
training
,
approximate
training
,
semi
supervised
training
framework
,
discriminative
training
,
feature
weight
,
ert
estimate
,
parameter
,
objective
,
certain
measure
,
translation
error
,
certain
performance
measure
,
translation
quality
,
development
corpus
,
smt
system
,
model
parameter
,
k
best
candidate
translation
,
error
measure
,
particular
candidate
,
respect
,
reference
translation
,
optimal
parameter
value
,
m
ert
,
equation
,
parameter
tuning
,
different
interpretation
,
component
,
equation
,
development
corpus
,
reference
translations
,
collection
,
training
sample
,
annotated
alignment
result
,
itg
parser
output
,
alignment
result
,
current
parameter
value
,
mer
module
,
alignment
f
score
,
performance
measure
,
input
sentence
pair
,
reference
,
f
score
,
dit
g
produced
alignment
,
approximate
training
,
corpus
,
initial
alignment
result
,
conditional
probability
,
word
pair
,
conditional
probability
,
phrase
h
phrase
,
simplicity
,
semi
supervised
framework
,
initial
alignment
result
,
many
feature
,
supervised
,
unsupervised
training
,
solution
,
e
step
corresponds
,
probability
,
itg
tree
,
m
step
corresponds
,
feature
value
,
possible
itg
tree
,
viterbi
parse
,
feature
value
,
training
,
approximate
,
word
pair
,
viterbi
,
conditional
probability
,
h
phrase
,
similar
,
phrase
h
phrase
pair
,
viterbi
parse
tree
,
phrase
h
phrase
pair
,
alignment
link
,
conditional
probability
,
semi
supervised
training
algorithm
emd
,
semi
supervised
training
,
input
development
data
dev
,
test
data
test
,
initial
alignment
,
align_train
,
output
feature
weight
,
estimate
initial
feature
,
initial
weight
,
initial
feature
,
f
measure
,
viterbi
alignment
align_train
,
new
feature
weight
,
f
measure
,
figure
,
semi
supervised
training
,
hp
ditg
,
discriminative
training
,
error
minimization
,
feature
weight
,
approximate
learning
,
feature
value
,
single
semi
supervised
framework
,
initial
estimation
,
initial
alignment
matrix
,
simpler
word
alignment
model
,
initial
estimation
,
discriminative
training
process
,
approximate
learning
process
,
improvement
,
sketch
,
semi
supervised
training
,
figure
,
following
feature
,
alignment
link
,
word
pair
translation
probability
,
hmm
model
,
conditional
link
probability
,
association
,
rank
feature
,
distortion
feature
,
inversion
,
concatenation
,
phrase
h
phrase
pair
,
hp
ditg
model
,
rule
probability
,
to
foreign
,
foreign
to
direction
,
addition
,
discriminative
model
,
alignment
hypothesis
selection
,
conditional
probability
,
phrase
,
conditional
probability
,
foreign
phrase
h
phrase
,
phrase
h
phrase
,
e
valuation
,
hp
ditg
,
word
alignment
,
translation
,
a
c
hinese
setting
,
baseline
,
word
alignment
quality
,
recall
,
precision
,
f
measure
,
translation
performance
,
insensitive
ble
u4
,
experiment
data
,
small
human
,
alignment
set
,
discriminative
training
,
feature
weight
,
haghighi
,
dataset
,
word
segmentation
standard
,
training
data
,
unannotated
bilingual
corpus
,
approximate
learning
,
feature
value
,
training
set
,
smt
system
,
smt
experiment
,
gram
language
model
,
xinhua
,
gigaword
corpus
,
test
set
,
development
corpus
,
test
set
,
state
of
the
smt
system
,
a
m
axent
based
distortion
model
,
implementation
,
rule
table
,
terminal
node
,
hp
ditg
tree
,
criterion
,
phrase
based
system
,
chiang
,
hp
ditg
,
first
experiment
,
contribution
,
various
dit
alignment
model
,
semi
supervised
training
,
feature
value
,
dit
model
,
ibm
model
,
pi
tg
,
hp
itg
,
variation
,
h
phrase
pair
,
simple
phrase
pair
,
experiment
result
,
w
d
itg
,
pd
itg
,
h
d
itg
,
performance
gain
,
hp
ditg
,
itg
achieves
,
f
measure
,
semi
supervised
training
,
various
dit
model
,
dit
model
,
precision
,
w
i
tg
,
recall
,
limitation
,
one
to
one
matching
,
improvement
,
phrase
pair
,
h
phrase
pair
,
combination
,
phrase
,
result
,
hp
ditg
,
high
recall
,
promising
alignment
pattern
,
instance
,
following
sentence
pair
,
last
weekend
,
oth
giz
,
pattern
,
figure
,
hp
ditg
,
better
pattern
,
figure
,
h
phrase
pair
,
p
ditg
,
last
weekend
,
last
weekend
,
partial
alignment
result
,
alignment
quality
,
hp
ditg
,
emd
precision
recall
fmeasure
,
semi
supervised
training
task
,
f
measure
,
second
experiment
,
semi
supervised
method
,
emd
improves
hp
ditg
,
respect
,
word
alignment
quality
,
result
,
hp
ditg
model
,
first
iteration
,
training
,
f
measure
,
third
emd
iteration
,
hp
ditg
improves
,
weight
estimation
,
converges
,
new
hp
ditg
model
,
training
,
hp
ditg
,
third
experiment
,
alignment
model
,
last
experiment
,
respect
,
translation
quality
,
insensitive
ble
u4
,
result
,
difference
,
baseline
,
semi
supervised
training
task
,
emd
improves
smt
performance
,
iteration
,
objective
function
,
discriminative
training
,
alignment
f
measure
,
correlation
,
f
measure
,
intriguing
problem
,
hp
ditg
lead
,
b
leu
point
gain
,
datasets
mt
model
,
smt
performance
,
nis
t05
,
nis
t08
,
onclusion
,
future
,
itg
formalism
,
notion
,
phrase
h
phrase
pair
,
limitation
,
one
to
one
matching
,
formalism
,
alignment
model
,
linguistic
fact
,
single
concept
,
several
noncontiguous
word
,
formalism
,
semi
supervised
training
method
,
feature
value
,
feature
weight
,
alignment
quality
,
machine
translation
performance
,
formalism
,
semi
supervised
training
,
alignment
,
translation
,
baseline
,
fundamental
problem
,
current
framework
,
monotonic
increment
,
ble
score
,
course
,
semi
supervised
training
,
future
,
ble
score
,
objective
function
,
discriminative
training
,
certain
extent
,
normal
form
grammar
table
,
itg
rule
,
normal
form
,
normal
form
,
alignment
,
normal
form
,
start
symbol
,
category
,
combination
whereas
,
inverted
combination
,
terminal
category
,
subcategories
,
terminal
unary
rule
,
word
,
foreign
word
,
unary
rule
,
alignment
,
left
branching
manner
,
combine
,
last
word
,
foreign
word
,
word
,
null
precede
,
foreign
word
,
callison
burch
,
mile
os
borne
,
koehn
,
joint
probability
statistical
translation
model
,
proceeding
,
workshop
,
statistical
machine
translation
,
peitra
,
mercer
,
mathematics
,
statistical
machine
translation
,
parameter
estimation
,
computational
linguistics
,
dekang
,
soft
syntactic
constraint
,
word
alignment
,
discriminative
training
,
proceeding
,
international
conference
,
computational
linguistics
,
annual
meeting
,
association
,
dekang
,
inversion
transduction
grammar
,
joint
phrasal
translation
modeling
,
proceeding
,
second
workshop
,
syntax
,
structure
,
statistical
translation
,
hierarchical
phrase
based
translation
,
computational
linguistics
,
yonggang
,
jia
xu
,
yuqing
gao
,
phrase
table
training
,
precision
,
recall
,
ood
phrase
,7
th
international
conference
,
human
language
technology
research
,
annual
meeting
,
association
,
computational
linguistics
,
marcu
,
semi
supervised
training
,
statisticalword
alignment
,
proceeding
,
international
conference
,
computational
linguistics
,
annual
meeting
,
association
,
computational
linguistics
,
marcu
,
word
alignment
quality
,
statistical
machine
translation
,
computational
linguistics
,
blitzer
,
den
ero
,
klein
,
better
word
alignment
,
supervised
itg
model
,
proceeding
,
joint
conference
,47
th
annual
meeting
,
international
joint
conference
,
natural
language
,
manning
,
parsing
,
hypergraphs
,
proceeding
,
international
workshop
,
parsing
technology
,
liu
,
qun
liu
,
shouxun
,
log
linear
model
,
word
alignment
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
wong
,
joint
probability
model
,
statistical
machine
translation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
wen
tau
yih
,
bode
,
discriminative
bilingual
word
alignment
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
ney
,
alignment
template
approach
,
statistical
machine
translation
,
computational
linguistics
,
ney
,
till
mann
,
statistical
translation
,
proceeding
,
international
conference
,
computational
linguistics
,
stochastic
inversion
transduction
grammar
,
bilingual
parsing
,
parallel
corpora
,
computational
linguistics
,
qun
liu
,
shouxun
,
maximum
entropy
,
phrase
,
statistical
machine
translation
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
gildea
,
stochastic
icalized
inversion
transduction
grammar
,
alignment
,
proceeding
,
annual
meeting
,
association
,
quirk
,
gildea
,
bayesian
learning
,
non
compositional
phrase
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
joint
conference
,
empirical
method
,
natural
language
processing
,
computational
naturallanguage
learning
,
jeju
island
,
association
,
computational
linguisticsre
training
monolingual
parser
bilingually
,
school
,
computer
science
,
technology
,
harbin
,
microsoft
research
,
beijing
,
chl
,
training
,
syntactic
smt
approach
,
essential
component
,
word
alignment
,
monolingual
parser
,
current
state
,
component
,
problem
,
rule
generalization
,
violation
,
syntactic
correspondence
,
translation
rule
,
retraining
monolingual
parser
,
target
,
consistency
,
parse
tree
,
alignment
matrix
,
simple
evaluation
function
,
data
selection
,
alignment
,
bilingual
data
,
auxiliary
method
,
alignment
quality
,
alignment
matrix
,
respect
,
combination
,
novel
method
,
b
leu
,
iws
lt
task
,
b
leu
point
gain
,
nis
task
,
many
variety
,
attempt
,
tree
structure
,
ssm
approach
,
monolingual
parser
,
parse
tree
,
information
,
language
,
word
alignment
,
current
state
,
word
aligner
,
monolingual
parser
,
average
word
aligner
,
syntax
information
,
language
,
output
link
,
syntactic
correspondence
,
sl
word
,
a
s
parse
tree
node
,
alignment
link
,
tl
word
,
legitimate
syntactic
structure
,
parser
design
,
monolingual
activity
,
impact
,
ambati
,
good
translation
rule
,
good
monolingual
parser
,
translation
task
,
string
to
tree
ssm
model
,
galley
,
translation
rule
,
minimal
rule
,
composition
,
minimal
rule
,
minimal
rule
,
special
kind
,
frontier
node
,
tl
parse
tree
,
concept
,
figure
,
corresponding
tl
subtrees
,
word
alignment
link
,
tl
word
,
a
t
parse
node
,
corresponding
sl
word
,
alignment
link
,
diagram
,
parse
node
,
rectangle
,
phrase
label
,
complement
span
,
a
t
node
,
minimal
contiguous
sl
string
,
sl
word
,
descendant
,
ancestor
,
galley
,
al2006
,
complement
span
,
diagram
,
frontier
node
,
ssm
model
,
bilingual
information
,
parse
tree
,
alignment
matrix
,
problem
,
ssm
model
,
first
one
,
violation
,
incorrect
alignment
link
,
figure
,
extraction
,
herdsman
,
incorrect
alignment
link
,
translation
rule
,
generalization
ability
,
fossum
,
second
problem
,
figure
,
incorrect
pos
tagging
,
series
,
absence
,
extraction
,
good
rule
,
figure
,
parse
tree
,
incorrect
alignment
link
,
aligner
,
parse
tree
,
consideration
,
figure
,
parsing
error
,
parser
,
consideration
,
correct
alignment
link
,
propaganda
,
lecture
,
word
aligner
,
parser
,
correction
,
alignment
information
,
general
approach
,
parser
,
parse
tree
,
alignment
matrix
,
first
strategy
,
katz
brown
et
al2011
,
simple
evaluation
function
,
frontier
,
parser
,
parse
tree
,
second
strategy
,
forced
alignment
,
wuebker
,
bilingual
data
,
parse
tree
,
ssm
system
,
parser
,
besides
,
new
word
aligner
,
syntactic
information
,
new
method
,
alignment
matrix
,
direction
,
parse
tree
,
consideration
,6
lived1
in2
the3
herdsmen4
,
yurts6
,1
vd
b21
,
of4
people5
coming6
to7
listen8
to9
their10
propaganda11
lectures12
,
word
alignment
,
syntactic
tree
,
error
link
,
dash
line
,
syntactic
tree
,
parser
retraining
strategy
monolingual
parser
,
certain
tree
bank
,
parser
,
target
,
agreement
,
decision
,
syntactic
structure
,
decision
,
human
annotated
parse
tree
,
monolingual
syntactic
structure
,
translation
,
bilingual
information
,
word
alignment
,
parser
,
correct
structure
,
parser
,
bilingual
information
,
framework
,
parser
retraining
,
data
selection
strategy
,
consistency
,
parse
tree
,
alignment
matrix
,
solution
,
subsection
,
self
training
,
frontier
set
,
first
solution
,
self
training
,
parse
tree
,
current
parser
,
training
data
,
next
round
,
training
objective
,
correctness
,
monolingual
syntactic
structure
,
targeted
self
training
,
training
objective
shift
,
certain
external
evaluation
function
,
n
best
parse
tree
,
current
parser
,
accordance
,
external
evaluation
function
,
top
one
,
re
ranked
candidate
,
training
data
,
next
round
,
targeted
self
training
,
definition
,
external
evaluation
function
,
figure
,
incorrect
parse
tree
,
extraction
,
good
translation
rule
,
incorrect
tree
,
correct
tree
,
figure
,
figure
,
parse
tree
,
correct
syntactic
structure
,
figure
,
frontier
node
,
valid
translation
rule
,
simple
external
evaluation
function
,
frontier
,
alignment
matrix
,
n
best
parse
tree
,
parse
tree
,
parse
tree
,
descending
order
,
parse
tree
,
training
data
,
next
round
,
targeted
self
training
,
tl
parser
,
following
,
targeted
self
training
,
n
best
list
,
parse
tree
,
mistaken
structure
,
perfect
match
,
alignment
matrix
,
frontier
,
best
list
,
parse
tree
,
translation
performance
,
parse
tree
,
monolingual
parser
,
translation
purpose
,
parse
tree
,
ssm
system
,
parser
,
ssm
system
,
byproduct
tl
parse
tree
,
monolingual
parser
,
problem
,
translation
,
mt
system
,
associated
parse
tree
,
little
use
,
parser
,
alignment
,
wuebker
,
bilingual
data
,
promising
approach
,7
vp1
4npa1large2
number3
of4
people5
coming6
to7
listen8
to9
their10
propaganda11
lectures12
,7
prp41
7nn41
7vp
figure
,
parse
tree
,
tst
f
,
figure
,
phrase
segmentation
,
sl
side
,
parsing
,
tl
side
,
word
alignment
,
full
translation
system
,
decoding
path
,
tl
side
,
parse
tree
,
byproduct
,
parse
tree
,
alignment
,
simple
iterative
retraining
algorithm
,
baseline
monolingual
parser
,
ssm
system
,
alignment
,
ssm
system
,
bilingual
data
,
parse
tree
,
new
training
data
,
parser
,
new
parser
,
second
round
,
forced
alignment
,
iteration
,
alignment
,
retraining
,
criterion
,
following
,
alignment
,
algorithm
,
forced
alignment
,
use
parser
,
training
data
,
perform
,
alignment
,
parse
tree
,
training
data
,
new
parser
,
development
data
drop
,
preset
limit
,
important
implementation
detail
,
forced
alignment
,
parse
tree
,
translation
rule
,
pruning
,
reality
,
average
mt
system
,
translation
model
training
,
decoding
,
translation
rule
,
parse
tree
,
forced
alignment
,
translation
rule
,
constraint
,
decoder
,
stack
size
,
measure
,
existence
,
forced
alignment
,
tl
sentence
translate
,
figure
,
null
alignment
,
source
,
null
translation
scenario
,
null
translation
candidate
,
decoding
,
target
span
,
translation
candidate
,
source
span
,
large
number
,
people
,
translation
candidate
,
propaganda
,
combination
,
candidate
,
n
best
translation
list
,
sequence
,
target
part
,
translation
candidate
,
syntactic
label
,
context
,
weight
,
added
null
alignment
,
competition
,
normal
candidate
,
normal
tree
,
many
null
alignment
subtrees
,
retraining
,
target
span
,
null
aligned
subtree
,
mentioned
modification
,
forced
alignment
,
partial
target
tree
,
forced
alignment
,
figure
,
figure
,
number3
of4
people5
coming6
to7
listen8
to9
their10
propaganda11
,7
np1
7np
figure
,
parse
tree
,
figure
,
useful
rule
,
baseline
,
word
alignment
symmetrization
,
word
aligners
,
ibm
model
,
directional
aligners
,
aligner
,
alignment
matrix
,
sl
to
tl
direction
,
tl
to
sl
direction
,
symmetrization
refers
,
combination
,
alignment
matrix
,
popular
method
,
symmetrization
,
alignment
matrix
,
i
dg
,
considers
,
addition
,
phrase
pair
,
phrase
based
smt
,
new
symmetrization
method
,
adaptation
,
syntactic
information
,
consideration
,
algorithm
,
generate
,
candidate
link
,
frontier
,
repeat
step
,
new
link
,
main
task
,
constraint
,
new
criterion
,
link
selection
,
parse
tree
,
tl
side
,
iteration
ids
,
change
,
frontier
,
addition
,
maximum
number
,
process
,
syntactic
structure
,
consideration
,
violation
,
syntactic
structure
,
figure
,
incorrect
link
,
final
alignment
,
frontier
,
figure
,
correct
link
,
combining
tst
f
fa
pr
,
ids
g
pa
,
retraining
aim
,
parser
,
alignment
matrix
,
ids
aim
,
alignment
matrix
,
parse
tree
,
alternative
,
combination
,
application
,
alignment
matrix
,
parser
,
alignment
,
parser
,
alignment
matrix
,
syntactic
information
,
iterative
training
routine
,
parser
retraining
,6
lived1
in2
the3
herdsmen4
,
yurts6
,1
vd
b21
,
alignment
,
figure
,
translation
task
,
method
,
parser
retraining
,
word
alignment
symmetrization
,
evaluation
method
,
insensitive
ibm
ble
u
,
papineni
,
bootstrap
re
sampling
method
,
confidence
level
,
parser
,
smt
decoder
,
syntactic
parser
,
parser
,
grammar
,
wsj
corpus
,
training
method
,
petrov
,
in
house
implementation
,
string
to
tree
decoder
,
standard
used
feature
,
translation
probability
,
lexical
weight
,
language
model
probability
,
distortion
probability
,
feature
weight
,
experiment
data
setting
,
baseline
,
method
,
data
setting
,
iws
lt
data
set
,
nis
data
set
,
baseline
,
iws
lt
data
set
ni
st
,
baseline
,
baseline
,
nis
data
,
iws
lt
data
,
iws
lt
,
dialog
task
data
set
,
training
data
,
sld
training
data
,
training
data
,
word
,
word
,
language
model
,
gram
language
model
,
training
data
,
combination
,
dialog
,
development
set
,
test
set
,
training
data
,
generated
tree
,
training
data
,
parser
,
baseline
,
setting
,
bi
direction
alignment
,
parser
,
petrov
,
baseline
alignment
,
syntactic
tree
,
calculate
feature
,
baseline
result
,
nis
data
set
,
bilingual
training
data
,
training
,
hong
kong
law
,
hong
kong
hansard
,
training
data
,
word
,10
m
word
,
training
data
,
parser
retraining
,
language
model
,
gram
language
model
,
giga
word
corpus
,
training
data
,
development
data
,
feature
weight
,
decoder
,
evaluation
,
evaluation
set
,
baseline
,
nis
data
,
similar
,
iws
lt
,
parser
retraining
strategy
,
baseline
,
default
parser
,
retraining
,
retraining
approach
,
bilingual
datasets
,
translation
model
training
,
mt
performance
,
iws
lt
,
standard
self
training
,
translation
performance
,
retraining
,
tl
side
,
bilingual
data
,
domain
adaptation
,
self
training
achieves
,
noticeable
improvement
,
standard
self
training
,
word
alignment
information
,
parser
retraining
,
improvement
,
tst
f
,
poor
quality
,
forced
alignment
,
improvement
,
iws
lt
,
first
reason
,
news
domain
,
formal
writing
style
,
iws
lt
,
tourist
domain
,
colloquial
style
,
improvement
,
default
parser
,
nis
,
reason
,
dataset
,
impact
,
figure
,
retraining
,
iteration
,
iteration
,
significant
improvement
,
forced
alignment
,
bilingual
training
data
,
full
decoding
path
,
translation
rule
,
iws
lt
dataset
,
nis
dataset
,
likely
forced
alignment
,
proportion
,
symmetrization
,
new
symmetrization
method
ids
,
baseline
method
idg
,
symmetrization
method
,
iws
lt
data
set
,
result
,
symmetrization
method
,
nis
data
,
result
,
result
,
translation
rule
,
translation
performance
,
parser
retraining
,
improvement
,
iws
lt
task
,
nis
task
,
iws
lt
dataset
,
effect
,
rule
table
size
,
method
,
retraining
,
new
symmetrization
method
,
different
,
application
,
experiment
result
,
combination
,
significant
difference
,
combination
,
result
,
leu
point
,
iws
lt
,
b
leu
point
,
attempt
,
word
alignment
,
syntactic
information
,
hermjackob
,
parser
,
alignment
information
,
burkett
,
attempt
,
parser
retraining
strategy
,
iws
lt
data
set
,
result
,
rule
baseline
,
parser
retraining
strategy
,
nis
data
set
,
result
,
baseline
,
new
method
,
iws
lt
data
set
,
result
,
rule
baseline
,
new
method
,
nis
data
set
,
result
,
baseline
,
parser
,
syntactic
machine
translation
,
knight
,
method
,
handful
,
strategy
,
syntactic
tree
structure
,
ambati
,
target
parse
tree
,
isomorphic
target
tree
,
syntactic
boundary
,
constituent
,
original
parse
tree
,
parser
tree
,
restructuring
method
,
binarization
method
,
subconstituent
structure
,
statistical
relabeling
method
,
coarse
nonterminal
problem
,
generalization
ability
,
different
,
previous
,
tree
structure
,
post
processing
method
,
method
,
suitable
grammar
,
string
to
tree
smt
model
,
word
alignment
matrix
,
nstead
,
parse
tree
,
machine
translation
performance
,
many
method
,
word
alignment
,
syntactic
tree
,
consideration
,
incorrect
word
alignment
link
,
discriminative
model
,
fossum
,
realigning
sentence
pair
,
method
,
initial
alignment
,
ambiguous
alignment
,
functional
word
,
constraint
,
chunk
level
information
,
rule
extraction
,
pursuit
,
consistent
word
alignment
,
method
,
idg
symmetrization
method
,
string
to
tree
rule
extraction
,
method
,
much
simpler
,
previous
,
mutual
independence
,
monolingual
parser
,
word
aligner
,
contribution
,
strategy
,
retraining
parser
,
bilingual
information
,
alignment
matrix
,
proposal
,
frontier
,
evaluation
function
,
alignment
,
retraining
,
parser
,
standard
self
training
,
parser
,
auxiliary
method
,
alignment
matrix
,
alternative
,
information
,
alignment
information
,
discriminative
word
alignment
,
parser
,
reference
vamshi
ambati
,
alon
lavie
,
translation
model
,
restructuring
divergent
,
non
isomorphic
parse
tree
structure
,
student
research
workshop
,
eighth
conference
,
association
,
machine
translation
,
burkett
,
klein
,
language
,
conference
,
empirical
method
,
natural
language
processing
,
dekang
,
soft
syntactic
constraint
,
word
alignment
,
discriminative
training
,
proceeding
,
international
conference
,
computational
linguistics
,
annual
meeting
,
association
,
klein
,
word
alignment
,
syntactic
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
fossum
,
knight
,
abney
,
syntax
,
word
alignment
precision
,
syntax
based
machine
translation
,
proceeding
,
third
workshop
,
statistical
machine
translation
,
galley
,
graehl
,
knight
,
marcu
,
deneefe
,
wei
,
thayer
,
scalable
inference
,
training
,
context
syntactic
translation
model
,
proceeding
,
international
conference
,
computational
linguistics
,
annual
meeting
,
association
,
computational
linguistics
,
ulf
hermjackob
,
improved
word
alignment
,
statistic
,
linguistic
heuristic
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
huang
,
knight
,
syntax
tree
,
syntax
based
machine
translation
quality
,
proceeding
,
human
technology
conference
,
north
american
chapter
,
katz
brown
,
slav
petrov
,
mcd
onald
,
och
,
hiroshi
ichikawa
,
masakazu
seno
,
hideto
kazawa
,
parser
,
machine
translation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
philipp
koehn
,
statistical
significance
test
,
machine
translation
evaluation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
wei
,
knight
,
marcu
,
realignment
,
syntax
based
machine
translation
,
computational
linguistics
,
ichi
tsujii
,
effective
use
,
function
word
,
rule
generalization
,
based
translation
,
proceeding
,
association
,
computational
linguistics
,
och
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
och
,
ney
,
systematic
comparison
,
various
statistical
alignment
model
,
computational
linguistics
,
mauser
,
ney
,
phrase
translation
model
,
leaving
one
out
,
proceeding
,
association
,
computational
linguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
slav
petrov
,
klein
,
inference
,
unlexicalized
parsing
,
proceeding
,
human
language
technology
,
annual
conference
,
north
american
chapter
,
association
,
computational
linguistics
,
conference
,
empirical
method
,
natural
language
processing
,
seattle
,
association
,
computational
linguisticsefficient
collective
entity
,
yang
song
,
houfeng
,
laboratory
,
computational
linguistics
,
peking
,
ministry
,
education
,
microsoft
research
asiahezhengyan
,
com
shujliu
,
comsongyangmagic
gmail
,
com
wanghf
pku
,
ambiguous
mention
,
corresponding
real
world
entity
,
knowledge
base
,
collective
disambiguation
method
enforce
coherence
,
nontrivial
inference
process
,
fast
collective
disambiguationapproach
,
alocal
predictor
g0
,
baselearner
,
initial
ranking
list
,
candidates
,
second
,
top
candidate
,
relatedinstances
,
expressive
global
coherence
feature
,
global
predictor
g1
,
augmented
featurespace
,
stacking
,
thetrain
test
mismatch
problem
,
proposedmethod
,
experiments
,
effectiveness
,
various
algorithms
,
several
public
datasets
,
semantic
relatedness
measure
between
entity
category
,
knowledge
,
natural
languagetext
,
machine
readable
format
,
ambiguousnames
,
real
world
entity
,
name
refers
,
linkingnames
,
entity
linkingor
disambiguation
,
semantic
link
,
wikipedia
,
various
downstream
nlp
application
,
authorprevious
research
,
several
kindsof
effective
approach
,
problem
,
use
handcrafted
featuresf
,
similarity
,
entity
definitione
,
l2r
approach
,
name
matching
,
context
similarity
,
lehmann
etal
,
category
context
correlation
,
bunescuand
pasca
,
nevertheless
,
decision
,
andinconsistent
result
,
collective
approach
utilize
dependency
between
different
decision
,
ambiguousmentions
,
context
,
hoffart
,
kulkarniet
,
ratinov
,
local
evidence
,
utilizesemantic
relation
,
different
mention
,
global
approach
,
l2r
method
,
local
approach
,
ratinov
,
collective
inference
process
,
exponential
searchspace
,
collective
entity
,
generalization
,
wolpert
,
powerful
,
algorithm
thatuses
,
learner
,
prediction
,
thefirst
learner
,
augmented
feature
,
thesecond
learner
,
nice
property
,
stacking
,
base
learner
,
inthis
paper
,
base
learner
,
l2r
ranker
,
ranking
list
,
candidate
,
next
level
,
semantic
coherententities
,
top
candidate
,
neighboringmentions
,
second
learner
,
augmented
feature
space
,
semantic
coherence
,
stacking
,
train
test
mismatchproblem
,
collective
methods
,
inference
process
,
method
,
simple
form
,
base
learner
,
wikipedians
,
entity
,
source
,
propose
,
context
entity
correlation
,
word
category
correlation
,
method
,
word
level
,
scale
well
,
large
number
,
category
,
representation
,
technique
,
category
context
association
,
latent
semanticspace
,
large
knowledgebase
,
contribution
,
accurate
stacking
based
collectiveentity
,
method
,
benefitsof
,
coherence
modeling
,
collective
approachesand
expressivity
,
l2r
method
,
aneffective
usage
,
global
feature
,
key
improvement
,
problem
,
scalability
,
shallow
word
level
comparison
,
category
context
correlation
,
advance
,
representation
learning
,
extra
semantic
information
,
workmost
popular
entity
,
l2r
framework
,
bunescu
,
lehmann
,
discriminative
nature
,
model
enoughflexibility
,
expressivity
,
features
,
similarity
,
dissimilarity
ofcontext
,
candidate
entity
,
small
training
,
carefully
designed
feature
,
category
,
localapproach
,
decision
,
mentionare
,
ratinov
,
objective
function
,
collective
approach
,
author
,
approximation
method
,
large
search
space
,
mention
,
candidate
,
arious
methodslike
integer
linear
programming
,
kulkarni
,
pagerank
,
andgreedy
graph
cutting
,
hoffart
,
literature
,
method
,
method
,
ratinov
,
prediction
,
local
ranker
,
generate
feature
,
global
ranker
,
difference
,
local
ranker
,
train
test
mismatch
problem
,
top
candidates
,
global
ranker
,
generalization
,
wolpert
,
algorithm
,
multiple
learner
outputs
,
feature
space
,
subsequentlearners
,
cross
validation
strategy
,
train
set
testset
label
mismatch
problem
,
various
application
,
collective
document
classification
,
dependency
parsing
,
joint
word
segmentation
,
part
of
speech
tagging
,
propose
,
dependency
,
datawith
relational
template
,
method
,
bytheir
approach
,
difference
,
base
learner
isan
l2r
model
,
related
entity
,
large
semantic
relatedness
graph
,
theassumption
,
true
candidate
,
false
one
,
wikipedians
annotate
entry
,
wikipedia
withcategory
network
,
valuable
information
generalizes
entity
context
correlation
,
utilizecategory
word
,
ranking
model
,
kataria
,
hierarchical
topicmodel
,
inner
node
,
hierarchy
,
pruned
categories
,
large
number
,
noisy
category
,
problem
,
advancesof
representation
learning
,
relatedness
,
category
,
context
,
latent
continuous
space
,
method
,
large
knowledge
base
,
methodin
,
base
learnerand
local
feature
,
training
strategy
,
explanation
,
global
coherence
model
,
augmented
feature
space
,
tolearn
category
context
correlation
,
representation
learning
technique
,
local
predictor
g0e
ntity
,
givenan
ambiguous
name
mention
,
candidate
entitiese1
,
ourpredictor
,
ranking
score
,
usedto
construct
,
next
levellearner
,
end
system
,
answer
,
arg
maxe
,
l2r
framework
,
linear
combination
,
candidate
,
problem
,
parameterw
,
correct
entity
,
score
overnegative
one
,
preference
learningtechnique
svm
rank
,
margin
,
minimized
,
trade
off
,
error
andmargin
size
,
negative
candidate
,
anyform
,
similarity
,
dissimilarity
,
typicalfeatures
,
literature
,
inclusion
,
thesefeatures
,
purposeis
,
moderate
model
,
thesurface
matching
,
mention
,
substring
,
substring
,
substring
,
substring
,
redirect
pointing
,
inw
ikipedia6
,
mcontext
matching
,
cosine
similarity
,
tf
idf
score
betweencontext
,
entire
wikipedia
,
candidate2
,
cosine
similarity
,
tf
idf
score
betweencontext
,
wikipedia
page3
,
jaccard
distance
,
context
,
entirewikipedia
,
candidate4
,
jaccard
distance
,
context
,
wikipedia
pagepopularity
,
prominence
feature
,
wikipedia
hyperlink
,
detail
,
matching
,
entity
popularity
,
collective
approaches
,
hoffart
,
predictor
g0
,
reader
,
lehmann
,
furtherreference
,
training
,
global
predictor
g1s
,
generalization
,
wolpert
,
algorithm
,
predictors
,
predictorsh
,
onthe
original
d
dimensional
feature
space
,
level1
predictor
,
dimensional
feature
space
,
whichpredictions
,
extra
feature
,
graphi
428cal
learning
,
inference
,
relationaldata
,
stacked
graphical
learning
,
dependenciesamong
data
,
relational
template
,
search
,
related
instance
,
current
instance
,
augmented
feature
space
doesnot
,
declarative
feature
,
original
dataand
prediction
,
related
instance
,
instance
,
collective
document
classification
,
hen
,
relational
template
,
extractdocuments
,
aco
unt
aggregator
,
category
,
entity
,
single
predictor
g0
,
local
feature
,
g1are
l2r
model
,
document
candidate
entity
pair
,
related
entity
,
entity
,
construct
global
feature
,
function
,
detail
,
global
predictor
g1
,
original
feature
,
entire
training
,
related
instance
,
training
set
,
future
unseen
data
,
prediction
,
input
doesn
,
generalize
,
train
test
mismatch
problem
,
tomimic
test
time
behavior
,
training
,
across
validation
like
,
entire
training
set
,
partition
,
instance
,
related
instance
,
gapplied
,
prediction
,
predictor
g0
,
entire
,
expanding
feature
space
,
test
data4
,
train
level
,
predictor
g1
,
entire
,
theaugmented
feature
space
,
next
subsection
,
toconstruct
global
feature
,
prediction
,
g0on
neighbor
,
coherence
,
global
feature
gif
,
correct
entity
,
ambiguous
name
,
relatedentities
,
context
,
surrounding
entity
,
degree
ofambiguity
,
ideal
,
true
candidate
,
semantic
link
,
negativecandidates
,
following
question
,
relevant
entity
,
candidate
,
context
,
thereany
mention
,
relevant
entity
,
candidate
,
mention
,
andhow
,
mention
,
reason
,
top
candidate
,
recall
,
related
entity
,
mitt
romney
,
ambiguous
mention
,
entity
,
instance
,
search
,
entity
,
semantic
relatedness
,
entity
,
map
entity
,
unique
set
,
process
,
asparse
matrix
,
operation
,
large
precomputed
semantic
relatedness
matrix
,
logical
operation
,
threshold
,
process
,
global
feature
,
obama
democratic
party
,
united
state
,
mitt
romneyrepublican
party
,
united
state
,
fukuiobama
,
nagasakidemocratic
party
,
democratic
party
,
republican
party
,
minnesotarepublicanismromney
,
virginiahms
romney
,
national
attention
,
vectory
,
united
state
,
reelected
,
november2012
,
united
state
,
semantic
link
,
collective
entity
,
annotation
mention
entity
,
wikipedia
convention
,
semantic
relatedness
measure
,
common
in
links
,
eiand
ej
,
wikipedia
,
witten
,
in
links
,
entity
,
wikipediapages
,
method
,
trade
off
,
exact
collective
inference
,
instancewith
top
ranked
entity
,
ambiguous
mentionsinto
consideration
,
difficulty
,
inference
inlarge
search
space
,
kulkarni
,
hoffartet
,
construct
,
asthe
average
,
candidate
,
mention
,
level
architecture
,
prediction
,
resemblesour
approach
,
thetrain
test
mismatch
problem
,
construct
differentset
,
top
candidate
,
category
context
coherencemodel
catentities
,
wikipedia
,
semantic
structure
,
category
network
,
valuable
information
,
entity
,
mention
,
romney
,
candidate
,
mitt
romney
,
category
,
republican
party
presidential
nominee
,
context
,
election
,
campaign
,
candidate
,
romney
,
category
,
football
player
,
context
,
quarterback
,
network
form
,
category
,
democratic
party
presidential
nominees
,
category
,
united
state
presidential
candidate
,
mitt
romney
,
network
,
bunescu
,
propose
,
thecategory
context
correlation
,
word
level
throughcategory
word
pair
feature
,
method
createssparsity
problem
,
becausethe
number
,
feature
grows
,
thenumber
,
category
,
vocabulary
size
,
moreover
,
category
network
,
hierarchy
,
ten
thousand
category
,
many
irrelevantones
,
correlation
,
word
level
,
representation
,
method
thatlearns
category
context
correlation
,
latent
semantic
space
,
query
document
pairsto
,
degree
,
matching
,
comparison
,
latent
semantic
space
,
synonymy
,
polysemy
,
inner
mechanism
,
score
function
betweenquery
,
supervision
,
click
through
data
,
training
,
negative
target
,
modeloptimizes
,
training
objective
,
contrastive
estimation
,
eisner
,
big
vocabulary
size
,
replacing
,
identity
term
,
trade
off
,
thelatent
space
model
,
vector
space
model
,
thegradient
step
,
necessary
realquery
,
context
,
entity
,
category
,
entity
,
categories
,
candidate
entity
,
definition
,
entity
,
context
,
definition
,
becausedefinition
,
training
process
,
toreduce
noise
,
category
,
entity
,
word
vector
,
a
t
f
idf
vector
,
binary
vector
,
normalized
tf
idf
,
binary
input
,
diverse
datasets
forevaluation
,
comparisonwith
others
,
c
kbp
,
severalyears
,
entity
,
collective
approaches
,
dataset
,
forcollective
approach
evaluation
,
ll2003
ner
,
refined
previous
,
publiclyavailable
datasets
,
thanks
,
great
,
enough
data
,
setting
,
hoffart
,
aid
dataset
,
train
development
,
separatemodel
,
wikipedia
training
set
,
ratinov
,
brief
overview
,
datasets
,
knowledge
base
,
wikipedia
xml
,
entity
,
wikipedia
,
mention
,
entity
,
candidate
generation
,
redirects
,
disambiguation
,
hyperlink
,
candidate
generation
,
candidate
,
popularity
,
ratinov
,
recall
,
ratinov
,
mention
,
mentions
,
global
feature
,
thefinal
score
,
comparisonwe
,
algorithm
,
several
state
of
the
collective
entity
disambiguation
system
,
theaida
system
,
hoffart
,
usea
greedy
graph
,
algorithm
,
remove
entity
,
low
confidence
score
,
pagerank
,
evidence
,
different
decision
,
bothalgorithms
use
simple
local
feature
,
propose
,
forinference
,
reimplementation
,
mpi
inf
,
edu
data
,
themsnbc
dataset
,
zip
file
,
wikimedia
,
org
enwiki
,
ndocs
non
nilidentified
solvableaida
dev
,
wikipedia
,
mention
,
dataset
,
mention
,
name
dictionaryand
,
true
entity
,
top
30candidates
,
popularity
,
parenthesis
showsthe
result
,
ratinov
,
method
,
ilp
solution
,
online
demo
,
moderate
size
,
ilp
solution
,
doc
,
contrast
,
method
,
ratinov
,
layerlearner
architecture
,
difference
,
theirmethod
use
,
candidate
,
locallearner
,
global
feature
generation
,
top
candidate
,
train
test
mismatch
problem
inour
model
,
global
feature
,
ratinov
,
local
predictorg0
,
predictor
,
global
feature
,
common
in
links
,
interconnection
,
faircomparison
,
global
coherence
,
implement
svm
rank
,
adaptation
,
linear
svm
,
scikit
learn
,
wrapper
,
category
context
coherence
model
,
open
blas
library
,
entirewikipedia
hyperlink
annotation
,
entire
dataset
,
learningrate
,
many
parametersto
tune
,
context
documentwindow
size
,
compatibility
,
ratinov
,
hoffart
,
candidate
,
entity
,
spopularity
,
precision
,
default
parameter
forglobal
feature
,
stacking
,
default
,
trainingdata
,
entity
,
relational
template
,
default
,
category
context
modeling
,
vocabularysizes
,
context
,
category
,
and6k
unigrams
,
frequency
,
latent
dimension
,
rank
approximation
,
performance
measure
,
non
nilqueries
,
micro
precision
,
information
retrieval
measure
,
rankof
correct
answer
,
response
,
mention
,
ratinov
,
recallrate
,
recall
,
top
recall
,
local
predictor
,
evaluation
result
,
aid
adataset
,
result
,
first
group
,
showssome
baseline
feature
,
comparison
,
category
,
incomplete
andnoisy
information
,
entity
,
much432methods
devset
testsetmicrop
,
ratinov
,
context
,
hoffart
,
dataset
,
maximal
value
,
bold
font
,
upto
candidate
,
related
instance
,
relational
template
,
word
level
feature
,
table4
,
information
,
predictor
g0
,
show
theresults
,
different
fold
,
training
,
foldmeans
,
training
data
,
directlyaugment
training
data
,
substantial
difference
,
various
fold
size
,
wefire
global
feature
,
top
candidate
,
problem
,
train
test
mismatch
,
extending
feature
space
,
ranking
oftrue
entity
,
testset
,
trainingset
,
semantic
coherence
information
,
top
candidate
,
effect
,
top
global
feature
,
table4
,
effect
,
g1
performance
,
clearly
,
precision
,
onepossible
reason
,
improvement
,
recall
whensearching
,
related
instance
,
recall
,
improvement
,
method
benefit
,
strategy
,
global
features
,
ratinov
,
methodis
,
trade
off
,
expensive
exact
search
overall
mention
,
greedy
,
mentionswith
local
predictor
,
beamsearch
size
,
additional
computational
overhead
,
exception
,
dataset
,
sincethis
dataset
,
difference
,
mention
,
little
inconsistent
,
datasets
,
result
,
mention
,
microp
,
testg0
,
evaluation
,
wikipedia
,
annotation
style
,
development
,
location
entity
,
distant
information
,
context
,
aswe
increase
,
context
,
extra
performance
boost
,
development
set
,
general
problem
,
location
name
,
location
name
,
main
topicof
,
context
withits
definition
,
related
location
name
,
context
,
instance
,
national
football
team
,
butour
system
,
country
,
because
,
finer
context
,
local
syntactic
pattern
,
hoffart
,
system
misclassifies
,
new
york
city
,
newyork
,
holland
,
latter
one
,
wikipedia
,
difference
,
context
,
globalcoherence
,
onclusionswe
,
stacking
,
collective
entity
linking
method
,
global
predictor
,
local
predictor
,
coherence
,
decision
,
method
,
inefficientexact
search
,
mention
,
local
predictor
,
relatedentities
,
relational
template
,
stacked
graphical
learning
,
beam
size
,
furthermore
,
weadopt
progress
,
representation
,
tolearn
category
context
coherence
model
,
large
knowledgebase
,
performs
comparison
,
latent
semanticspace
,
technique
,
sophisticated
collective
approach
,
acknowledgmentsthis
research
,
national
social
science
fund
,
national
high
technology
researchand
development
program
,
program
,
national
natural
sciencefoundation
,
referencesb
,
weinberger
,
semantic
indexing
,
acm
conference
,
encyclopedicknowledge
,
entity
disambiguation
,
proceedings
,
volume
,
cucerzan
,
large
scale
,
entity
disambiguation
,
wikipedia
data
,
proceeding
ofe
mnlp
ll
,
volume
,
collective
entitylinking
,
text
,
graph
based
method
,
international
acm
ir
conference
,
research
,
development
,
informationretrieval
,
weikum
,
robust
disambiguation
,
namedentities
,
proceeding
,
conference
one
mpirical
method
,
natural
language
processing
,
association
,
computational
linguistics
,
hoa
trang
dang
,
grif
fitt
,
overview
,
base
population
track
,
proceeding
,
fourth
text
analysis
conference
,
search
,
clickthrough
data
,
proceeding
,
eighthacm
kdd
international
conference
,
knowledge
discovery
,
data
mining
,
sengamedu
,
entity
disambiguation
,
hierarchical
topic
model
,
proceeding
,
zhenzhen
kou
,
w
c
ohen
,
stackedgraphical
model
,
efficient
inference
,
markov
random
field
,
chakrabarti
,
collective
annotation
ofwikipedia
entity
,
text
,
proceeding
,
acm
kdd
international
conferenceon
knowledge
discovery
,
data
mining
,
pages457
,
base
populationat
tac
,
workshop
,
quantum
,
rte
track
,
inp
roceedings
,
test
analysis
conference
,
dipanjan
da
,
a
,
anderic
p
x
ing
,
dependency
parser
,
inp
roceedings
,
conference
,
empirical
methodsin
natural
language
processing
,
association
,
computational
linguistics
,
witten
,
withwikipedia
,
proceeding
,17
th
acm
conference
,
information
,
knowledge
management
,
anderson
,
global
algorithm
,
disambiguationto
wikipedia
,
proceeding
,
annual
meeting
ofthe
association
,
collective
context
aware
topic
modelsfor
entity
disambiguation
,
proceeding
,21
stinternational
conference
,
world
wide
,
pages729
,
nishio
,
entity
disambiguation
,
probabilistictaxonomy
,
technical
report
,
technical
reportmsr
tr
,
eisner
,
contrastive
estimation
,
training
log
linear
model
,
unlabeled
data
,
proceedings
,
annual
meeting
,
associationfor
computational
linguistics
,
association
,
computational
linguistics
,
weiwei
sun
,
stacked
sub
word
model
forjoint
word
segmentation
,
part
of
speechtagging
,
h
w
olpert
,
generalization
,
zhicheng
zheng
,
xiaoyanzhu
,
entity
,
knowledgebase
,
human
language
technology
,2010
annual
conference
,
north
american
chapter
,
association
,
computational
linguistics
,
pages483
,
los
angeles
,
associationfor
computational
linguistics
,
conference
,
empirical
method
,
natural
language
processing
,
seattle
,
association
,
computational
linguisticsmulti
domain
adaptation
,
smt
using
multi
task
learning
,
lei
cui1
,
xilun
chen2
,
dongdong
zhang3
,
shujie
liu3
,
mu
li3
,
technology
,
chinaleicui
,
cn2cornell
,
edu3microsoft
research
,
dozhang
,
shujliu
,
comabstractdomain
adaptation
,
individual
specific
domain
,
correlationamong
different
domain
,
commonknowledge
,
theoverall
translation
quality
,
novel
multi
domain
adaptation
approach
,
in
domain
model
,
general
domainmodel
,
different
domain
,
parameters
,
general
knowledgemore
,
exploit
domain
knowledge
,
large
scale
to
translation
task
validate
,
adaptation
,
thetranslation
quality
,
theindividual
adaptation
,
statistical
machine
learning
,
domain
mismatch
,
training
,
many
machine
,
data
distributions
,
training
,
testing
domain
,
assumption
,
forreal
world
smt
system
,
training
data
forsmt
model
,
variety
,
domain
,
translation
quality
,
second
author
,
microsoft
research
,
general
model
,
hotchpotch
,
bilingual
corpus
,
domain
adaptation
,
smt
system
,
previous
research
,
domain
adaptation
,
data
selection
,
weighting
,
andlewis
,
axelrod
,
mixture
models
,
sennrich
,
razmara
,
andsemi
supervised
transductive
learning
,
ueffing
etal
,
method
,
smt
model
,
dataand
,
good
performance
,
real
world
smt
system
,
models
,
domain
,
overall
translation
quality
,
method
,
domain
,
common
knowledge
acrossdifferent
domain
,
common
knowledge
,
multi
domain
adaptationapproach
,
smt
model
,
multi
domain
adaptation
,
sentiment
analysis
,
dredze
,
crammer
,
ranking
,
chapelle
,
commonality
,
difference
acrossmultiple
domain
,
target
problem
,
related
problems
,
shared
feature
representation
,
key
advantage
,
regularization
,
overall
translation
quality
,
mtl
based1055tm1s1
s2
s3m
ultiple
in
domain
translation
model
language
modelsone
general
domain
translation
model
language
modelt1lm1tm2lm2tm3lm3domain
specific
smt
systemsin
domaintraining
data
t2
t3t
mtl
,
predefined
domain
,
entire
training
corpus
,
in
domain
trainingdata
,
i
th
domain
,
bilingual
cross
entropy
,
method
,
axelrod
,
in
domain
training
data
,
general
domain
model
,
domain
specific
smt
system
,
i
th
domain
,
leveragingthe
in
domain
model
,
general
domain
model
,
multi
domain
adaptation
approach
,
smt
model
,
domain
,
multiple
smt
system
,
mixture
models
,
specificdomain
,
system
share
,
general
domaintm
,
smt
system
,
asseveral
related
task
,
shared
feature
representation
,
unified
mtl
framework
,
joint
tuning
,
generalknowledge
,
general
domain
model
,
domain
knowledge
,
in
domain
model
,
stochastic
learning
approach
,
simianer
,
feature
weight
,
multiple
smt
system
,
sametime
,
algorithm
,
treatin
domain
,
general
domain
feature
,
regularization
,
smt
systems
,
efficient
,
experimental
result
,
method
,
thetranslation
quality
,
multiple
domain
,
non
adapted
baseline
,
adaptation
,
conventional
individualadaptation
approach
,
domain
,
theproposed
approach
,
experimental
result
,
related
,
concludesthe
paper
,
future
research
direction
,
approachfigure
,
predefined
domains
,
main
idea
,
threesteps
,
training
phase
,
in
domain
training
data
,
predefined
domains
,
in
domain
model
andgeneral
domain
model
,
thedomain
specific
smt
system
,
multiple
domain
specific
smt
system
,
section2
,
first
step
,
in
domain
bilingual
data
,
bilingual
data
,
in
domain
tm
,
bilingual
cross
entropy
,
axelrod
,
bilingual
sentence
pair
,
represent
,
cross
entropy
,
string
,
in
domain
lm
,
source
language
,
language
,
thecross
entropy
difference
,
general
domain
source
side
lm
,
cross
entropy
difference
,
general
domain
target
side
lm
,
criterion
,
towards
sentence
pair
,
in
domain
corpus
,
general
domain
corpus
,
therefore
,
differences
,
question
,
sufficient
monolingual
data
,
in
domain
lm
,
straightforward
solution
,
in
ternet
,
large
number
,
monolingualwebpages
,
domain
information
,
portal
sites1
,
large
scale
real
world
smt
system
,
practical
domain
adaptation
technique
,
moredomains
,
crawler
,
collectmonolingual
webpage
,
domain
,
portal
site
,
source
language
,
target
language
,
statistic
,
crawled
data
,
in
domain
source
sidelms
,
in
domain
target
side
lm
,
target
sidegeneral
domain
lm
,
crawled
documents
,
different
domain
,
general
domain
lm
,
selectin
domain
bilingual
data
,
different
domain
,
mixture
modelsin
,
second
step
,
in
domain
training
data
,
smt
system
,
mixture
model
,
mixture
,
schroeder1many
,
portal
site
,
domain
informationfor
webpage
,
webpage
,
human
editor
,
different
domain
,
suchas
politics
,
business
,
n
s
mtsystems
,
eachsystem
,
typical
log
linear
model
,
translation
candidate
,
general
domain
,
in
domain
feature
function
andwj
,
corresponding
feature
weight
,
thefeature
weight
,
detailed
feature
description
isas
,
in
domain
feature
,
phrase
translationprobabilities
,
lexical
weight
,
directions
,
word
count
,
phrase
count
,
hierarchical
rule
,
general
domain
feature
,
phrase
translation
probability
,
lexical
weight
,
bothdirections
,
feature
description
,
smt
system
,
in
domain
tm
,
bilingual
training
data
,
thegeneral
domain
tm
,
entire
bilingual
training
data
,
target
side
in
domain
lm
,
general
domain
lm
trained1057for
data
selection
,
anormal
single
model
system
,
mixture
model
,
contribution
,
in
domain
knowledge
,
benefit
,
third
step
,
feature
weight
,
multipledomain
specific
smt
system
,
instead
,
domain
specific
system
,
different
system
,
related
task
,
mtl
framework
,
main
reason
,
tuning
,
domain
specific
translation
task
share
thesame
general
domain
lm
,
commonalities
,
different
task
,
general
domain
lm
andtm
perform
,
different
domain
,
regularization
,
objective
function
,
proposedmtl
based
approach
,
predefined
domain
,
in
domain
development
dataset
,
thei
th
domain
,
reference
translation
,
a
d
lengthfeature
,
column
vector
,
i
th
domain
,
dimension
,
feature
space
,
by
d
matrix
,
translation
,
fiwith
parameter
,
loss
between
,
output
,
reference
translations
,
basic
idea
,
objective
function
isto
,
loss
function
,
domains
,
domain
,
general
domain
feature
weight
,
translation
quality
,
different
domain
,
smt
system
,
algorithm
,
simianer
,
pairwise
ranking
approach
,
perceptron
algorithm
,
shenand
joshi
,
feature
weight
,
atranslation
candidate
,
pairwise
preference
,
training
,
candidate
,
hinge
loss
,
inner
product
,
vector
,
perceptron
algorithm
,
gradient
,
instance
,
discriminativelearning
,
pairwise
ranking
,
comparingthe
n
best
list
,
translation
candidate
,
smoothed
sentence
level
ble
,
simianer
,
n
bestlist
,
pairwise
ranking
,
thetranslation
preference
pair
,
thecandidates
,
high
middle
,
middle
low
,
high
low
,
candidate
,
figure
,
guarantee
,
ranker
,
good
translation
,
bad
one
,
middle
,
n
best
listfigure
,
training
instance
,
pairwise
ranking
,
asynchronous
,
distribute
domain
specific
decoder
,
ma
chines2
,
domain
,
n
best
list10
,
end
for15
,
end
for16
,
domain
,
domain
,
end
for24
,
end
for27
,
algorithm
,
algorithm1
,
column
vector
wi
,
twoparts
wii
,
in
domain
andgeneral
domain
feature
weight
,
algorithm
,
domain
specificsmt
decoder
,
different
machine
,
initializethe
feature
weight
,
sgd
algorithm
run
,
several
iteration
,
multiple
smt
decoder
,
parallel
,
feature
weight
,
in
domain
development
data
,
domain
,
domain
specific
decoder
,
then
best
translation
,
preference
pairsare
,
parameter
,
gradient
,
in
domain
development
data
multiple
time
,
iteration
,
feature
weight
,
decoder
,
contrast
,
original
algorithm
,
simianer
,
general
domain
feature
weight
,
in
domain
feature
weight
,
reason
,
general
knowledge
,
different
domain
,
algorithm
,
joint
mtl
,
tuning
,
featureweights
,
domain
specific
smt
systemsare
,
testing
data
,
in
domain
testing
data
,
domain
,
thedomain
specific
system
,
real
application
,
testingdomain
,
theeffectiveness
,
large
scale
to
machine
translation
task
,
training
data
consistedof
,
monolingual
data
,
bilingual
data
,
monolingual
data
,
target
side
lm
,
usedfor
data
selection
,
addition
,
thetarget
side
lm
,
smt
systemsas
feature
,
aweb
crawler
,
large
number
,
webpagesfrom
portal
site
,
sixpopular
domain
,
business
,
entertainment
,
health
,
politics
,
webpage
,
htm
ltags
,
main
content
,
data
statistic
,
bilingual
data
,
method
,
jianget
,
post
processing
step
,
ourbilingual
data
,
method
,
data
quality
,
addition
,
parallel
corpus
,
chinesedocs
word
doc
wordsbusiness
21m
,
statistic
,
crawled
monolingual
data
,
innumbers
,
refers
,
with404m
word
,
domain
,
cross
entropybased
method
,
entire
bilingual
data
,
bilingual
data
,
in
domaindata
,
developmentand
,
eachdomain
,
detail
,
dev
testen
ch
en
ch
en
chb
usiness
30m
28m
36k
35k
19k
19k
ent
,22
m
21k
18k
13k
12k
health
23m
20m
33k
33k
21k
22k
sci
tech
28m
26m
46k
45k
27k
27k
sport
19m
16m
18k
14k
10k
9kp
olitics
,24
m
19k
17k
13k
12k
table
,
statistic
,
in
domain
training
,
thecky
,
algorithm
,
cube
pruning
,
default
parametersettings
,
chiang
,
decoder
,
pairwise
,
translation
model
,
thebilingual
data
,
directions
,
diag
grow
final
heuristic
,
symmetric
word
alignment
,
phrasetables
,
top
translation
candidates
,
source
phrase
,
efficiency
,
anin
house
language
,
toolkit
,
gram
language
model
,
modifiedkneser
ney
smoothing
,
kneser
,
crawled
data
,
evaluation
metric
forthe
overall
translation
quality
,
papineni
,
bootstrap
re
sampling
method
,
baselinewe
,
baseline
,
first
baseline
,
non
adapted
hiero
,
implementation
,
general
domain
tm
,
standard
feature
,
addition
,
fix
discountmethod
,
phrase
table
smoothing
,
combination
,
in
domain
development
datasets
,
second
baseline
,
google
onlinetranslation
service3
,
to
translation
,
googletranslation
,
solid
comparison
,
method
,
theadapted
system
,
domain
,
superiority
,
different
domain
,
baseline
,
similarperformance
,
google
translation
,
certain
domains
,
business
,
sci
tech
,
politics
,
demonstrates
,
translation
quality
,
baseline
,
moreover
,
question
,
theexperimental
result
,
follow
,
domain
mismatch
,
significant
problem
,
real
world
smt
system
,
thesame
system
,
general
domain
tm
,
towards
,
domain
,
usingin
domain
dev
data
,
than3http
,
translate
,
google
,
com1060business
ent
,
translation
,
end
to
end
experimental
result
,
large
scale
training
data
,
general
domain
dev
data
,
towardseach
domain
,
usingour
mtl
,
general
domain
language
model
,
obtainedtranslations
,
google
translation
,
comparison
,
non
adapted
baseline
,
domain
,
addition
,
google
translation
,
domain
,
previous
research
,
domain
mismatch
,
exists
,
parameter
estimation
,
in
domaindev
data
,
mixture
model
,
adaptation
,
variety
,
domain
,
different
setting
,
multiple
tm
orl
m
,
large
scale
smt
system
,
in
domain
model
,
general
model
,
different
domain
,
reason
,
general
model
,
in
domain
data
,
data
coverage
,
probability
estimation
,
translation
quality
,
in
domain
lm
performs
,
general
domain
lm
,
monolingual
data
,
domain
,
in
domain
lm
,
goodperformance
,
in
domain
tm
,
result
,
main
reason
isthe
data
coverage
,
in
domain
tm
,
general
model
,
result
,
betterperformance
,
mixture
model
,
domain
adaptation
,
translation
quality
,
tune
multiple
domain
specific
system
,
commonality
,
relatedtasks
,
translation
quality
,
thenon
adapted
baseline
,
conventional
mixture
model
,
method
,
domain
benefit
,
in
domain
knowledge
,
domain
discrepancy
,
largeimprovements
,
certain
domain
,
overthe
out
of
domain
feature
weight
,
result
,
guarantee
,
ble
score
,
method
,
dev
datawas
,
baseline
system
,
out
of
domain
feature
,
domain
specific
dev
data
,
player
,
player
hitsthe
ball
,
player
,
thebaseline
,
attempt
,
different
translation
,
phrase
,
englishphrases
,
symbol
,
detail
,
different
domain
,
out
of
domain
feature
weight
,
kindof
regularization
,
sport
domainwith
translation
,
different
method
,
baseline
,
player
,
game
player
,
baseline
translates
,
service
box
,
thentranslates
,
service
,
contrast
,
adapted
model
,
conventional
adaptation
method
,
mixture
models
,
natural
question
,
approach
performs
,
individual
adaptation
,
question
,
thedetails
,
tuning
,
decoding
procedure
,
themtl
based
approach
,
ble
uscore
,
development
data
,
individualadaptation
,
algorithm
enforcing
,
general
feature
,
role
acrossdifferent
domain
,
approach
introduces
,
regularization
,
eachdomain
specific
system
,
regularization
,
general
feature
,
towards
certain
domains
,
extreme
,
property
,
real
world
smt
system
,
domain
specific
wordsand
,
general
word
,
improper
totranslate
,
in
domain
knowledge
,
translates
,
thesports
text
,
general
domain
text
,
individual
adaptation
method
,
in
domain
development
data
,
contrast
,
adaptationone
direction
,
domain
adaptation
,
thedata
selection
,
eck1062et
,
translation
resultsto
,
decode
,
testing
data
,
triedto
,
training
data
,
similarity
,
test
data
,
information
retrieval
models
,
discriminative
model
,
weight
,
sentencein
,
training
corpus
,
method
conducteddata
selection
,
cross
entropy
,
axelrod
,
cross
entropy
,
method
,
selection
,
bilingual
corpus
,
relevant
corpus
,
target
domain
,
smallermodels
,
semi
supervised
transductive
learning
technique
,
monolingual
in
domaindata
,
adaptation
method
,
utilization
,
mixture
model
,
variant
,
multiple
tm
,
interpolation
,
koehn
andschroeder
,
sennrich
,
tm
perplexity
minimization
,
methodto
,
model
weight
,
mixture
modeling
,
addition
,
system
combination
approach
,
razmara
,
ensemble
decodingmethod
,
multiple
translation
model
,
variety
,
strong
baseline
,
previous
method
,
conducted
domain
adaption
,
single
domain
,
ratherthan
multiple
domain
,
couldalso
,
build
multiple
smt
system
,
domain
,
hasbeen
little
research
,
multi
domain
adaptationproblem
,
mixture
model
,
smt
system
,
target
problem
,
related
problem
,
model
forthe
main
task
,
learner
,
thecommonality
,
parallel
,
sharedrepresentation
,
eachtask
,
blitzer
,
mtl
approach
,
adaptation
task
,
part
of
speech
tagging
,
collobert
andweston
,
deep
neural
networksto
,
part
of
speech
tagging
,
chunking
,
entity
recognition
,
semantic
role
,
sentiment
analysis
,
dredzeand
crammer
,
ranking
,
multi
domain
learningand
adaptation
,
n
best
re
ranking
,
sparsefeature
set
,
n
best
list
,
toa
distinct
task
,
simianer
,
distributed
stochastic
learning
,
feature
selection
,
distributed
learning
,
method
,
method
,
smt
system
,
eachsystem
,
in
domain
,
general
domain
model
,
shared
feature
representation
,
commonality
,
general
model
,
addition
,
domain
specific
translation
knowledge
wasalso
,
in
domain
model
,
onclusion
,
future
workin
,
approach
toaddress
multi
domain
adaptation
,
cross
entropy
,
data
selection
methodto
,
in
domain
bilingual
data
,
in
domain
tm
,
domain
specific
smt
system
,
addition
,
general
domain
tm
,
sharedacross
different
system
,
multiple
system
,
experimental
result
,
multi
domain
adaptation
problem
,
significant
improvement
,
thenon
adapted
baseline
,
conventional
domainadaptation
method
,
mixture
model
,
domain
information
,
testing1063data
,
real
world
smt
systems
,
real
applications
,
domain
information
,
future
,
popular
domain
,
automatic
domain
classifier
,
domain
,
high
confidence
,
domain
specific
system
,
general
system
,
furthermore
,
general
trainingmethod
,
approach
withother
domain
adaptation
method
,
performance
improvement
,
acknowledgmentswe
,
yang
,
yajuanduan
,
hong
sun
,
danran
,
helpfuldiscussions
,
anonymous
reviewers
,
insightful
comment
,
referencesrie
kubota
ando
,
tong
zhang
,
frameworkfor
learning
predictive
structure
,
multiple
tasksand
,
journal
,
amittai
axelrod
,
jianfeng
gao
,
adaptation
,
pseudo
in
domain
data
selection
,
proceeding
,
conference
one
mpirical
method
,
natural
language
processing
,
edinburgh
,
association
,
computational
linguistics
,
blitzer
,
mcd
onald
,
pereira
,
adaptation
,
structural
correspondence
learning
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
association
,
computational
linguistics
,
chapelle
,
pannagadatta
shivaswamy
,
srinivasvadrevu
,
kilian
weinberger
,
ya
zhang
,
belletseng
,
multi
task
learning
,
machinelearning
,
chiang
,
hierarchical
phrase
based
translation
,
computational
linguistics
,
ronan
collobert
,
unified
architecture
,
natural
language
processing
,
deep
neural
network
,
multitask
learning
,
proceedingsof
,25
th
international
conference
onm
achine
learning
,
dongdong
zhang
,
shujie
liu
,
mingzhou
,
bilingual
data
cleaning
,
smt
usinggraph
based
random
walk
,
proceeding
,51
stannual
meeting
,
association
,
computationallinguistics
,
volume
,
short
paper
,
association
,
computational
linguistics
,
dredze
,
koby
crammer
,
methodsfor
multi
domain
learning
,
adaptation
,
proceedings
,
conference
,
empirical
method
inn
atural
language
processing
,
olulu
,
association
,
computationallinguistics
,
duh
,
katsuhito
sudoh
,
hajime
tsukada
,
hidekiisozaki
,
masaaki
nagata
,
n
best
rerankingby
multitask
,
proceeding
,
joint
fifthworkshop
,
statistical
machine
translation
,
met
ricsmatr
,
uppsala
,
association
,
computational
linguistics
,
eck
,
vogel
,
waibel
,
model
adaptation
,
statistical
machinetranslation
,
information
retrieval
,
kuhn
,
mixture
modeladaptation
,
proceeding
,
secondworkshop
,
statistical
machine
translation
,
pages128
,
prague
,
republic
,
associationfor
computational
linguistics
,
kuhn
,
johnson
,
statistical
machinetranslation
,
proceeding
,
conference
one
mpirical
method
,
natural
language
processing
,
association
forcomputational
linguistics
,
goutte
,
kuhn
,
discriminative
instance
,
domain
adaptation
,
statistical
machine
translation
,
proceedingsof
,
conference
,
empirical
method
,
natural
language
processing
,
association
,
computational
linguistics
,
long
jiang
,
shiquan
yang
,
ming
zhou
,
xiaohua
liu
,
andqingsheng
zhu
,
bilingual
data
,
theweb
,
learnt
pattern
,
proceedingsof
,
joint
conference
,47
th
annual
meetingof
,
international
joint
conferenceon
natural
language
processing
,
afn
lp
,
pages870
,
suntec
,
association
forcomputational
linguistics
,
kneser
,
ney
,
improvedbacking
off
,
m
gram
language
modeling
,
acoustics
,
speech
,
signal
processing
,
experimentsin
domain
adaptation
,
statistical
machine
translation
,
proceeding
,
second
workshop
,
statistical
machine
translation
,
prague
,
republic
,
association
,
computationallinguistics
,
philipp
koehn
,
statistical
significance
test
,
translation
evaluation
,
dekang
,
proceeding
,
emn
lp
,
barcelona
,
associationfor
computational
linguistics
,
liang
,
klein
,
taskar
,
end
to
end
discriminative
approach
,
machine
translation
,
proceeding
,
the21st
international
conference
,
computational
linguistics
,
annual
meeting
,
associationfor
computational
linguistics
,
ney
,
association
,
computationallinguistics
,
jin
huang
,
qun
liu
,
improvingstatistical
machine
translation
performance
,
training
data
selection
,
optimization
,
proceedingsof
,
joint
conference
,
empirical
methodsin
natural
language
processing
,
pages343
,
prague
,
republic
,
associationfor
computational
linguistics
,
intelligentselection
,
language
model
training
data
,
proceedings
,
conference
short
paper
,
uppsala
,
associationfor
computational
linguistics
,
och
,
ney
,
systematic
comparison
,
various
statistical
alignment
models
,
computational
linguistics
,
och
,
minimum
error
rate
training
instatistical
machine
translation
,
proceeding
,
the41st
annual
meeting
,
association
,
computational
linguistics
,
sapporo
,
association
,
computational
linguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
proceeding
,
meeting
,
association
,
computationallinguistics
,
philadelphia
,
association
,
computational
linguistics
,
majid
razmara
,
baskaran
sankaran
,
andanoop
sarkar
,
multiple
translationmodels
,
statistical
machine
translation
,
proceedings
,50
th
annual
meeting
,
association
forcomputational
linguistics
,
volume
,
long
paper
,
jeju
island
,
associationfor
computational
linguistics
,
rico
sennrich
,
perplexity
minimization
,
translation
model
domain
adaptation
,
statistical
machinetranslation
,
proceeding
,
conference
ofthe
chapter
,
association
,
computational
linguistics
,
avignon
,
association
,
computational
linguistics
,
libin
shen
,
aravind
k
j
oshi
,
perceptron
,
machine
learning
,
simianer
,
riezler
,
dyer
,
joint
feature
selection
,
distributed
stochastic
learning
,
large
scale
discriminative
training
,
inp
roceedings
,50
th
annual
meeting
,
association
,
computational
linguistics
,
volume
,
longpapers
,
jeju
island
,
association
,
computational
linguistics
,
ueffing
,
gholamreza
haffari
,
anoop
sarkar
,
statistical
machinetranslation
,
proceeding
,45
th
annual
meeting
,
association
,
computational
linguistics
,
prague
,
republic
,
association
,
computational
linguistics
,
annual
meeting
,
association
,
computational
linguistics
,
uppsala
,
association
,
computational
linguisticsdiscriminative
pruning
,
computer
science
,
technology
,
harbin
,
microsoft
research
,
beijing
,
chl
,
com
,
attention
,
year
,
obstacle
,
discriminative
itg
pruning
framework
,
minimum
error
rate
training
,
various
feature
,
previous
,
itg
alignment
,
experiment
result
,
heuristic
,
itg
pruning
,
framework
,
discriminative
itg
alignment
model
,
hierarchical
phrase
pair
,
f
score
,
bleu
score
,
baseline
alignment
system
,
adaptation
,
parsing
,
language
,
phrasal
,
word
level
alignment
,
byproduct
,
reason
itg
,
attention
,
word
alignment
community
,
gildea
,
haghighi
,
itg
alignment
,
complexity
,
attempt
,
method
,
haghighi
,
probability
,
simpler
alignment
model
,
gildea
,
tic
tac
toe
pruning
,
probability
,
word
pair
,
principle
,
technique
,
certain
contribution
,
good
pruning
decision
,
itg
pruning
,
novel
discriminative
pruning
framework
,
discriminative
itg
,
training
data
,
discriminative
itg
parser
,
log
linear
model
,
correct
span
pair
,
probability
,
discriminative
pruning
method
,
discriminative
itg
alignment
system
,
hierarchical
phrase
pair
,
following
,
basic
detail
,
itg
formalism
,
itg
parsing
,
definition
,
discriminative
,
discriminative
itg
,
parser
,
basics
,
formulation
,
itg
contains
,
terminal
unary
rule
,
represent
word
,
foreign
language
,
binary
rule
,
component
,
foreign
phrase
,
inverted
order
,
viewpoint
,
word
alignment
,
terminal
unary
rule
,
word
pair
,
binary
rule
,
reordering
factor
,
biased
towards
short
distance
,
formulation
,
drawback
,
to
constraint
,
word
alignment
,
strong
limitation
,
multi
word
expression
,
single
word
,
various
attempt
,
to
constraint
,
itg
alignment
316approaches
,
constraint
,
simple
itg
,
word
alignment
,
sole
purpose
,
instance
,
consecutive
word
pair
,
problem
,
redundancy
,
itg
normal
form
,
normal
form
,
first
key
,
itg
normal
form
grammar
,
appendix
,
basics
,
itg
parsing
,
similar
,
step
applies
,
relevant
terminal
unary
rule
,
word
pair
,
word
pair
,
span
pair
,
possible
,
larger
,
span
pair
,
possible
derivation
,
toy
sentence
pair
,
rectangle
,
certain
phrase
category
,
foreign
span
,
f
span
,
span
,
e
span
,
upper
half
,
rectangle
,
associated
alignment
hypothesis
,
figure
,
derivation
,
various
derivation
,
itg
parsing
,
hypergraph
,
manning
,
figure
,
hypernode
,
rectangle
,
span
pair
,
upper
half
,
possible
alignment
hypothesis
,
span
pair
,
hyperedges
,
span
pair
,
span
pair
,
hypernode
,
alignment
hypothesis
,
hypernode
,
topmost
hypernode
,
figure
,
normal
form
,
hypotheses
,
span
pair
,
framework
,
unpromising
span
pair
,
unpromising
f
spans
,
unpromising
alignment
hypothesis
,
particular
span
pair
,
pruning
,
many
span
pair
,
alignment
performance
,
pruning
,
beam
size
,
alignment
hypothesis
,
hypernode
,
k
b
est
,
method
,
chiang
,
bottom
up
construction
,
span
pair
repertoire
,
span
pair
,
alignment
hypothesis
,
complete
parse
tree
,
k
best
list
,
topmost
span
,
alignment
hypothesis
,
minimal
number
,
span
pair
,
pruning
,
hypernodes
,
hyper
graph
,
itg
pruning
,
search
,
f
span
,
minimal
number
,
e
spans
,
likely
counterpart
,
method
,
balance
,
efficiency
,
many
correct
span
pair
,
search
,
minimal
number
,
f
span
,
arbitrary
decision
,
required
pruning
method
,
span
pair
,
e
spans
,
particular
f
span
,
correct
e
span
,
incorrect
one
,
i
framework
dpd
,
discriminative
pruning
model
,
probability
,
log
linear
model
,
span
pair
,
weight
,
corresponding
feature
,
question
,
training
sample
,
ection
,
ection
,
training
sample
discriminative
approach
,
word
alignment
use
,
alignment
,
discriminative
pruning
,
possible
span
pair
,
required
training
sample
,
various
f
spans
,
corresponding
e
spans
,
annotator
,
span
pair
,
parsing
algorithm
,
span
pair
annotation
,
base
step
,
word
pair
,
hypergraph
,
recursive
step
,
alignment
constraint
,
f
span
,
e
span
,
parse
tree
,
however
,
reality
,
foreign
word
,
word
,
f
span
covering
,
foreign
word
,
e
spans
,
figure
,
golden
link
,
alignment
annotation
,
foreign
word
,
aligns
,
word
,
aligns
,
hypernode
,
hypernode
,
situation
happens
,
product
,
outside
probability
,
alignment
hypothesis
,
span
pair
,
probabilities
,
simpler
alignment
model2
,
probable
hypothesis
,
alignment
,
figure
,
training
sample
collection
,
hypergraph
,
candidate
e
spans
,
f
span
,
automatic
span
pair
annotation
,
original
sentence
level
alignment
annotation
,
to
constraint
,
violation
,
situation
,
inside
out
,
figure
,
constraint
,
pattern
,
f1
f2
f3
f4e1
e2
e3
e4
figure
,
inside
out
alignment
,
training
sample
,
positive
training
sample
,
classifier
,
parameter
training
,
negative
sample
,
parameter
training
,
negative
sample
,
pruning
parameter
training
,
method
,
smt
estimate
,
parameter
,
objective
,
certain
measure
,
translation
error
,
certain
performance
measure
,
translation
quality
,
development
corpus
,
smt
system
,
inside
,
outside
probability
,
span
pair
,
simpler
alignment
model
,
parameter
,
respect
,
reference
translation
,
optimal
parameter
value
,
d
pdi
,
equation
,
parameter
tuning
,
different
interpretation
,
components
,
equation
,
development
corpus
,
reference
translation
,
collection
,
training
sample
,
method
,
itg
parser
output
,
f
a
k
,
current
parameter
value
,
function
,
presence
,
correct
e
span
,
correct
e
span
,
k
best
list
,
rationale
underlying
,
error
function
,
e
spans
,
k
best
list
,
e
spans
,
correct
e
spans
upward
,
k
best
list
,
new
error
measure
,
change
,
detail
,
training
algorithm
,
interval
boundary
,
error
measure
change
,
upper
envelope
,
dash
line
,
figure
,
performance
error
measure
,
candidate
translation
,
error
measure
,
correct
e
span
,
e
span
leading
,
system
score
,
interval
boundary
,
intersections
,
correct
e
span
,
candidate
e
spans
,
figure
,
correct
e
span
,
interval
,
figure
,
error
measure
,
interval
,
figure
,
k
,
interval
,
correct
e
spans
,
interval
,
error
measure
,
beam
size
,
e
spans
,
f
span
,
category
,
probability
,
probability
,
word
pair
,
span
pair
,
probability
,
length
,
category
,
figure
,
span
pair
,
interest
,
simpler
alignment
model
,
word
pair
,
span
pair
,
span
pair
,
f1
f2
f3
f4e1
e2
e3
e4
figure
,
heuristic
feature
,
span
pair
,
length
,
span
,
feature
value
,
span
pair
,
phrase
pair
,
average
ratio
,
foreign
sentence
length
,
eng
lish
sentence
length
,
training
dataset
,
rationale
underlying
,
span
length
,
average
ratio
,
feature
value
,
refers
,
position
,
f
span
,
position
,
first
last
word
,
f
span
,
rationale
,
phrase
,
position
,
equivalent
phrase
,
feature
value
,
a
inconsistent
link
,
phrase
pair
,
haghighi
,
posterior
probability
,
hmm
alignment
model
,
new
features
,
link
count
,
link
ratio
,
inconsistent
link
ratio
,
posterior
probability
,
discriminative
itg
alignment
,
two
staged
process
,
first
stage
dpd
,
good
span
pair
,
second
stage
good
alignment
hypothesis
,
span
pair
,
discriminative
itg
,
word
to
word
dit
,
to
constraint
,
alignment
,
hierarchical
phrase
pair
,
henceforth
hp
ditg
,
to
constraint
,
hierarchical
phrase
pair
,
chiang
,
alignment
hypotheses
,
span
pair
,
contribution
,
log
linear
model
,
discriminative
training
,
feature
weight
,
module
,
alignment
f
score
,
performance
measure
,
input
sentence
pair
,
reference
,
f
score
,
dit
g
produced
alignment
,
unlike
dpd
,
upper
envelope
,
interval
,
performance
measure
change
,
word
to
word
dit
g
th
,
alignment
link
,
word
pair
translation
probability
,
hmm
model
,
conditional
link
probability
,
association
,
rank
feature
,
distortion
feature
,
inversion
,
concatenation
,
difference
,
relative
position
,
relative
position
,
posi
320tion
,
word
pair
,
stop
word
,
hierarchical
phrase
pair
,
to
assumption
,
serious
limitation
,
reality
,
segmentation
,
tokenization
error
,
idiomatic
expressions
,
bilingual
segmentation
grammar
,
terminal
rule
,
phrase
pair
,
incorporate
phrase
pair
,
phrase
based
smt
,
haghighi
,
many
to
terminal
unary
rule
,
parsing
,
employing
simple
phrase
pair
,
hierarchical
phrase
pair
,
chiang
,
foreign
side
,
simple
hierarchical
,
phrase
pair
,
simple
phrase
pair
,
parsing
,
span
pair
,
possible
combination
,
sub
span
pair
,
binary
rule
,
span
pair
,
phrase
pair
,
alignment
link
,
phrase
pair
,
standard
phrase
pair
extraction
procedure
,
alternative
alignment
hypothesis
,
hierarchical
phrase
pair
,
parsing
,
span
pair
check
,
lexical
anchor
,
remaining
word
,
sub
span
pair
,
reordering
constraint
,
category
,
itg
normal
form
grammar
,
condition
,
span
pair
,
alignment
hypothesis
,
alignment
link
,
lexical
anchor
,
sub
span
pair
,
word
aligned
corpus
,
standard
phrase
pair
extraction
,
chiang
,
probability
,
lexical
weight
,
to
foreign
,
foreign
to
direction
,
addition
,
discriminative
model
,
alignment
hypothesis
selection
,
e
valuation
dpd
,
baseline
,
pruning
,
gildea
,
pruning
,
haghighi
,
respect
,
to
alignment
,
translation
,
alignment
system
,
evaluation
criterion
four
evaluation
criterion
,
addition
,
itg
parsing
,
evaluate
pruning
,
pruning
decision
,
first
evaluation
,
error
rate
,
henceforth
per
,
many
correct
e
spans
,
drawback
,
decision
,
alignment
quality
,
certain
f
spans
,
little
use
,
alternative
criterion
,
upper
bound
,
alignment
f
score
,
many
link
,
annotated
alignment
,
itg
parse
,
calculation
,
f
score
upper
bound
,
bottom
up
,
leaf
hypernodes
,
correct
link
,
non
leaf
hypernode
,
daughter
hypernodes
,
maximal
sum
,
hyperedges
,
hypernode
,
hypernode
,
variable
,
category
,
itg
grammar
,
golden
link
,
appendix
,
figure
,
calculation
,
hit
score
,
upper
bound
,
recall
,
hit
score
,
total
number
,
golden
link
,
beam
size
,
total
time
cost
per
f
u
b
fscore
d
pdi
,
t
tt
,
t
tt
,
evaluation
,
tic
tac
toe
,
dynamic
program
,
w
d
itg
id
,
beam
size
,
total
time
cost
per
f
u
b
fscore
d
pdi
,
t
tt
,
t
tt
,
evaluation
,
tic
tac
toe
,
dynamic
program
,
hp
ditg
,
precision
,
hit
score
,
practice
,
upper
bound
,
alignment
f
score
,
recall
upper
bound
calculation
finally
,
end
to
end
evaluation
,
f
score
,
alignment
,
bleu
score
,
translation
,
implementation
,
hierarchical
phrase
based
smt
,
chiang
,
standard
feature
,
smt
experiment
,
experiment
data
discriminative
pruning
,
alignment
need
training
data
,
test
data
,
dataset
,
hag
highi
,
dataset
,
word
segmentation
standard
,
training
data
,
corresponding
number
,
f
spans
,
training
,
test
data
,
bilingual
training
dataset
,
nis
training
set
,
hong
kong
law
,
hong
kong
hansard
,
gram
language
model
,
xinhua
,
gigaword
corpus
,
test
set
,
development
corpus
,
test
set
,
test
set
,
small
scale
evaluation
,
first
set
,
method
,
small
sentence
set
,
method
,
w
d
itg
,
hmm
alignment
model
,
method
,
result
,
w
d
itg
,
f
span
,
incorrect
pruning
decision
,
improvement
,
actual
f
score
,
upper
bound
,
beam
size
,
time
cost
,
performs
,
method
,
f
score
upper
bound
,
methods
,
intolerable
memory
consumption
,
different
hmm
model
implementation
,
result
,
hp
ditg
,
roughly
,
observation
,
w
d
itg
,
addition
,
superiority
,
hp
ditg
achieves
,
f
score
,
f
score
upper
bound
,
phrase
,
powerful
tool
,
to
constraint
,
f
score
upper
bound
,
corresponding
f
score
,
possible
explanation
,
pruning
,
parsing
alignment
process
,
search
process
,
promising
region
,
search
space
,
large
scale
end
to
end
experiment
id
pruning
beam
size
time
,
bleu
bleu
d
pdi
,
t
tt
,
t
tt
,
hp
ditg
id
wa
model
f
s
core
bleu
bleu
h
mm
,
g
iza
,
b
itg
,
bit
g
ta
,
word
alignment
time
cost
,
smt
performance
,
bleu
score
,
acceptable
time
cost
,
compares
hp
ditg
,
hag
highi
,
hp
ditg
,
baseline
,
alignment
f
score
,
bleu
score
,
bleu
score
difference
,
hp
ditg
,
baseline
,
hp
ditg
,
phrase
pair
extraction
,
good
phrase
pair
,
link
inconsistent
,
phrase
pair
selection
,
good
itg
,
subsequent
itg
alignment
process
,
link
inconsistent
,
good
phrase
pair
,
beam
size
,
methods
lead
,
alignment
f
score
,
future
,
word
alignment
,
itg
parsing
,
problem
,
itg
pruning
,
discriminative
pruning
model
,
discriminative
itg
alignment
system
,
method
,
hp
ditg
alignment
system
,
state
of
the
alignment
,
translation
quality
,
current
dpd
,
limited
set
,
many
feature
,
probability
,
word
pair
,
success
,
hp
ditg
,
hierarchical
phrase
pair
,
relationship
,
span
pair
,
hierarchical
phrase
pair
,
normal
form
grammar
table
,
itg
rule
,
normal
form
,
normal
form
,
alignment
,
normal
form
,
start
symbol
,
category
,
combination
whereas
,
inverted
combination
,
terminal
category
,
subcategories
,
terminal
unary
rule
,
word
,
unary
rule
,
alignment
,
left
branching
manner
,
combine
,
last
word
,
foreign
word
,323
words
,
null
precede
,
foreign
word
,
reference
,
peitra
,
mercer
,
mathematics
,
statistical
machine
translation
,
parameter
estimation
,
computational
linguistics
,
dekang
,
soft
syntactic
constraint
,
word
alignment
,
discriminative
training
,
proceeding
,
dekang
,
inversion
transduction
grammar
,
joint
phrasal
translation
modeling
,
proceeding
,
hierarchical
phrase
based
translation
,
computational
linguistics
,
mohit
bansal
,
klein
,
transducer
grammar
,
proceeding
,
naa
cl
,
pages
,
marcu
,
semi
supervised
training
,
statisticalword
alignment
,
proceeding
,
aria
haghighi
,
blitzer
,
den
ero
,
klein
,
better
word
alignment
,
supervised
itg
model
,
proceeding
,
liang
huang
,
chiang
,
k
best
parsing
,
proceeding
,
och
,
ney
,
statistical
alignment
model
,
proceeding
,
och
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceedings
,
klein
,
manning
,
parsing
,
hypergraphs
,
proceeding
,
philipp
koehn
,
statistical
significance
test
,
machine
translation
evaluation
,
proceedings
,
yang
liu
,
qun
liu
,
shouxun
,
log
linear
model
,
word
alignment
,
proceedings
,
iscriminative
framework
,
bilingual
word
alignment
,
proceeding
,
emn
lp
,
wen
tau
yih
,
bode
,
discriminative
bilingual
word
alignment
,
proceeding
,
vogel
,
ney
,
till
mann
,
statistical
translation
,
proceeding
,
col
ing
,
vogel
,
phrase
pair
extraction
,
mt
summit
,
stochastic
inversion
transduction
grammar
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
gildea
,
stochastic
icalized
inversion
transduction
grammar
,
alignment
,
proceeding
,
quirk
,
gildea
,
bayesian
learning
,
non
compositional
phrase
,
synchronous
parsing
,
proceeding
,50
th
annual
meeting
,
association
,
computational
linguistics
,
republic
,
association
,
computational
linguisticslearning
translation
consensus
,
technology
,
microsoft
research
harbin
,
beijing
,
cn
chl
,
com
,
translation
consensus
,
research
,
search
,
translation
consensus
,
previous
,
problem
,
structured
labeling
,
label
propagation
,
graph
based
translation
consensus
,
similar
source
string
,
useful
feature
,
n
best
output
re
ranking
,
algorithm
,
experimental
result
,
method
,
machine
translation
performance
,
iws
lt
,
nis
data
,
state
of
the
baseline
,
consensus
,
translation
,
attention
,
year
,
principle
,
consensus
,
translation
candidate
,
translation
candidate
,
actual
formulation
,
principle
,
translation
candidate
,
candidate
,
candidate
,
candidate
,
different
mt
system
,
first
author
,
microsoft
research
,
translation
consensus
,
loss
function
,
translation
,
respect
,
translation
candidate
,
translation
,
minimal
bayes
risk
,
extent
,
candidate
,
n
best
output
,
a
m
decoder
,
tromble
et
,
lattice
,
consensus
,
translation
,
mt
system
,
different
mt
system
,
collaborative
decoding
,
translation
,
source
span
,
n
gram
similarity
,
translation
,
hypothesis
mixture
decoding
,
second
decoding
process
,
search
space
,
new
hypothesis
,
hypothesis
,
multiple
system
,
utilizing
consensus
,
translation
,
consensus
,
translation
,
similar
source
sentence
span
,
good
candidate
selection
,
figure
,
source
,
mt
system
,
correct
translation
,
first
one
,
translation
,
consideration
,
translation
,
final
translation
output
,
reasoning
,
discriminative
learning
method
,
translation
,
similar
translation
,
n
best
output
,
decoder
,
information
,
agreement
,
similar
translation
,
alexandrescu
,
kirchhoff
,
graph
based
semi
supervised
model
,
re
rank
n
best
translation
output
,
attempt
,
translation
consensus
,
re
ranking
,
n
best
output
,
open
question
,
translation
consensus
,
similar
sentence
span
,
decoding
process
,
method
,
alexandrescu
,
kirchhoff
,
simple
label
propagation
,
large
graph
,
search
inefficient
,
translation
consensus
,
bilingual
training
data
,
novel
graph
based
model
,
translation
consensus
,
alexandrescu
,
kirchhoff
,
translation
consensus
,
structured
labeling
,
novel
label
propagation
algorithm
,
structured
labeling
,
simple
label
propagation
,
derive
useful
mt
decoder
,
iws
lt
,
nis
data
,
experimental
result
,
method
,
translation
performance
,
data
set
,
state
of
the
baseline
,
graph
based
translation
consensus
,
mt
system
,
graph
based
translation
consensus
,
conventional
log
linear
model
,
source
string
,
conditional
probability
,
translation
candidate
,
feature
vector
,
feature
weight
,
translation
hypothesis
,
commonly
,
graph
based
consensus
feature
,
consensus
,
translation
,
similar
sentence
span
,
local
consensus
feature
,
consensus
,
translation
,
structured
label
propagation
method
,
consensus
statistic
,
translation
candidate
,
similar
source
sentence
span
,
standard
,
simple
label
propagation
,
translation
consensus
,
problem
,
instance
,
structured
labeling
,
structured
label
propagation
algorithm
,
graph
model
,
consensus
,
decoder
,
n
best
output
re
ranking
,
consensus
feature
,
feature
weight
,
semi
supervised
,
raph
based
structured
learning
,
graph
based
model
assigns
label
,
instance
,
similar
instance
,
instance
,
weight
,
similarity
,
graph
based
model
,
instance
,
strong
edge
,
wslt
,
hundred
,
iws
lt
,
final
output
,
decoder
,
instance
,
possible
label
,
translation
candidate
,
scenario
,
general
,
graph
based
model
,
aspect
,
string
,
simple
category
,
following
,
structured
label
,
berlett
,
difference
,
correct
label
,
translation
,
principle
,
graph
based
translation
consensus
,
instance
,
source
span
,
translation
,
alexandrescu
,
kirchhoff
,
translation
,
structured
labeling
,
candidate
translation
,
possible
label
,
good
translation
pair
,
graph
based
model
,
normal
,
general
graph
based
model
,
problem
,
perspective
,
inefficiency
,
average
mt
decoder
,
vast
amount
,
translation
candidate
,
corresponding
graph
,
vast
amount
,
large
dataset
,
label
propagation
,
general
graph
based
model
general
graph
based
model
,
label
propagation
,
probability
,
respect
,
corresponding
probability
,
updating
rule
,
matrix
calculation
,
convenience
,
updating
rule
,
propagating
probability
,
weight
,
similarity
measure
,
training
instance
,
correct
label
,
probability
,
correct
label
,
training
instance
,
iteration
,
suitable
measure
,
instance
node
similarity
,
unlabeled
instance
node
,
suitable
label
,
structured
label
propagation
,
graph
based
learning
,
structured
learning
,
correct
label
,
new
updating
rule
,
respect
,
alexandrescu
,
kirchhoff
,
source
sentence
span
,
translation
candidate
,
updating
rule
,
neighbor
,
problem
,
structured
labeling
,
source
sentence
span
,
translation
candidate
,
natural
,
component
,
component
,
component
,
probability
,
source
sentence
span
,
neighbor
,
candidate
,
new
updating
rule
,
new
rule
,
probability
,
translation
,
source
sentence
span
,
probability
,
similar
translation
,
special
,
eatures
,
last
,
structured
label
propagation
algorithm
,
detail
,
actual
graph
,
graph
based
translation
consensus
,
mt
system
,
graph
based
consensus
feature
,
probability
,
new
feature
,
decoder
,
n
best
output
re
ranker
,
graph
based
consensus
,
recall
,
refers
,
source
sentence
span
,
refers
,
candidate
,
translation
posterior
,
translation
posterior
,
n
best
list
,
training
sentence
pair
,
posterior
,
probability
,
dice
coefficient
,
n
grams
,
n
grams
,
string
,
dice
coefficient
,
similarity
,
translation
candidate
,
symmetrical
sentence
level
ble
,
ibm
ble
,
hypothesis
,
reference
,
theory
,
similarity
measure
,
edit
distance
,
kernel
,
simple
n
gram
similarity
,
addition
,
graph
based
consensus
feature
,
local
consensus
feature
,
n
best
translation
candidate
,
b
leu
,
different
score
,
reference
,
hypothesis
,
respect
,
n
gram
similarity
measure
,
fundamental
feature
,
translation
probability
,
lexical
weight
,
distortion
probability
,
word
penalty
,
language
model
probability
,
training
method
,
graph
based
consensus
,
mt
system
,
development
,
test
data
,
detail
,
label
translation
,
training
data
node
,
dev
test
data
node
,
possible
label
,
n
best
translation
candidate
,
decoder
,
mutual
dependence
,
consensus
graph
,
decoder
,
mt
decoder
,
gc
feature
,
decoder
,
translation
candidate
,
possible
label
,
posterior
probability
,
initial
value
,
graph
based
consensus
feature
,
feature
weight
,
log
linear
model
,
algorithm
,
outline
,
semi
supervised
method
,
alternative
training
,
entire
process
,
decoder
,
consensus
feature
,
training
,
test
data
,
subsequent
structured
label
propagation
,
mt
decoder
,
decoder
,
new
feature
,
retrains
,
feature
weight
,
new
feature
weight
,
new
n
best
candidate
,
posterior
,
consensus
graph
,
next
round
,
alternation
,
structured
label
propagation
,
ble
score
,
dev
data
converges
,
preset
limit
,
graph
construction
technical
detail
,
description
,
graph
based
consensus
,
actual
consensus
graph
,
discussion
,
graph
construction
,
re
ranking
,
graph
based
consensus
,
n
best
output
,
decoder
,
separate
node
,
training
data
,
dev
data
,
test
data
,
henceforth
training
node
,
correct
translation
,
different
translation
,
translation
,
corresponding
probability
,
training
data
,
confidence
,
dev
test
data
,
henceforth
test
node
,
n
best
list
,
translation
candidate
,
possible
label
,
a
m
decoder
,
decoder
,
translation
posterior
,
initial
confidence
,
toy
graph
,
re
ranking
,
test
node
,
test
node
,
test
node
,
similarity
edge
,
similarity
,
symmetrical
sentence
level
ble
,
threshold
,
toy
graph
,
rectangle
,
upper
half
,
correct
,
possible
label
,
training
node
,
test
node
,
similarity
,
graph
construction
,
graph
based
consensus
,
decoding
algorithm
,
translation
candidate
,
source
span
,
source
span
,
candidate
label
,
source
span
,
test
node
,
purpose
,
mt
decoder
,
possible
segmentation
,
dev
test
data
,
search
,
translation
candidate
,
source
span
,
probability
,
candidate
,
search
space
,
decoder
,
test
node
,
alignment
,
forced
alignment
performs
,
segmentation
,
alignment
,
training
data
,
full
translation
system
,
decoding
,
wuebker
,
simpler
term
,
training
data
,
decoder
,
source
side
,
translation
candidate
,
substring
,
target
side
,
reduced
search
space
,
decoder
,
training
node
,
test
node
,
decoding
,
translation
model
training
,
alignment
,
decoder
,
target
side
,
search
space
,
training
node
,
source
span
,
translation
candidate
,
source
span
,
edge
creation
,
graph
construction
,
n
best
re
ranking
,
sub
span
,
exception
,
propagation
,
toy
,
node
represent
span
,
alignment
result
,
translation
candidate
,
substrings
,
dash
line
,
solid
line
,
sufficient
source
side
n
gram
similarity
,
figure
,
toy
graph
,
dash
line
,
relation
,
sub
span
,
whereas
edge
,
solid
line
,
source
side
similarity
,
result
,
graph
based
translation
consensus
,
translation
task
,
evaluation
method
,
insensitive
ibm
ble
u
,
papineni
,
bootstrap
re
sampling
method
,
confidence
level
,
experimental
data
setting
,
baseline
,
method
,
data
setting
,
iws
lt
data
set
,
nis
data
set
,
baseline
decoder
,
in
house
implementation
,
bracketing
transduction
grammar
,
cky
style
,
lexical
reordering
model
,
maximum
entropy
,
used
feature
,
standard
btg
decoder
,
translation
probability
,
lexical
weight
,
language
model
,
word
penalty
,
distortion
probability
,
ur
iws
lt
data
,
iws
lt
,
dialog
task
data
set
,
training
data
,
sld
training
data
,
training
data
,
word
,
word
,
language
model
,
gram
language
model
,
training
data
,
test
set
,
development
set
,
devset8
,
dialog
set
,
baseline
result
,
iws
lt
data
,
baseline
,
iws
lt
data
,
nis
data
set
,
bilingual
training
data
,
training
,
hong
kong
law
,
hong
kong
hansard
,
training
data
,
word
,10
m
word
,
language
model
,
gram
language
model
,
giga
word
corpus
,
training
data
,
development
data
,
feature
weight
,
decoder
,
evaluation
,
evaluation
set
,
baseline
result
,
nis
data
,
baseline
,
baseline
,
nis
data
,
consensus
based
re
ranking
,
iws
lt
data
set
,
consensus
based
re
ranking
,
baseline
decoder
,
n
best
list
,
development
,
test
data
,
n
best
list
,
semi
supervised
training
,
baseline
,
development
data
,
graph
based
consensus
confidence
,
log
linear
model
,
label
propagation
,
struct
lp
,
n
best
list
,
similarity
,
translation
candidate
,
symmetrical
sentence
level
ble
,
struct
lp
,
baseline
,
gr
e
rank
,
dialog
devset9baseline
,
g
decode
,
consensus
based
re
ranking
,
decoding
,
iws
lt
data
set
,
result
,
baseline
,
baseline
system
,
forced
alignment
procedure
,
training
data
,
span
node
,
derivation
tree
,
forced
alignment
,
development
,
test
data
,
consensus
based
decoding
,
decoding
,
semi
308supervised
training
,
graph
based
consensus
feature
,
weight
,
baseline
,
consensus
based
re
ranking
method
,
neighbor
local
similarity
,
final
n
best
output
,
contribution
,
local
consensus
feature
,
consensus
based
re
ranking
,
g
decode
gc
,
graph
based
consensus
feature
,
contribution
,
local
consensus
,
graph
based
consensus
feature
,
consensus
based
re
ranking
,
decoding
,
consensus
re
ranking
,
consensus
decoding
system
,
candidate
,
consensus
information
,
candidate
,
development
data
,
test
data
,
g
decode
,
machine
translation
,
baseline
,
gr
e
rank
gc
,
gr
e
rank
lc
,
g
decode
lc
,
g
decode
gc
,
aseline
,
g
decode
,
consensus
based
re
ranking
,
decoding
,
nis
data
set
,
result
,
baseline
,
nis
data
,
result
,
consensus
based
re
ranking
method
,
iws
lt
data
,
consensus
based
decoding
,
data
set
contains
,
many
sentence
pair
,
machine
,
method
,
alexandrescu
,
kirchhoff
,
separate
graph
,
development
,
global
connectivity
information
,
label
propagation
,
separate
graph
,
graph
based
consensus
,
n
best
list
,
graph
based
consensus
,
weight
,
struct
lp
,
local
consensus
,
g
decode
lc
,
combination
,
local
consensus
feature
,
translation
performance
,
smt
re
ranking
,
graph
based
consensus
feature
,
g
decode
gc
achieves
significant
performance
gain
,
local
consensus
feature
,
g
decode
performance
,
future
,
consensus
method
,
consensus
statistic
,
translation
candidate
,
source
sentence
span
,
similar
one
,
consensus
statistic
,
label
propagation
method
,
structured
learning
problem
,
machine
translation
,
structured
label
propagation
,
structured
learning
task
,
pos
tagging
,
syntactic
parsing
,
consensus
statistic
,
conventional
log
linear
model
,
weight
,
iterative
semi
supervised
method
,
iws
lt
,
nis
data
,
method
,
dice
coefficient
,
symmetrical
sentence
level
ble
,
similarity
measure
,
future
,
consensus
feature
,
similarity
measure
,
document
level
information
,
semantic
information
,
consideration
,
similarity
,
source
,
parameter
,
crf
model
,
reference
alexandrescu
,
katrin
kirchhoff
,
graph
based
learning
,
statistical
machine
translation
,
proceeding
,
human
language
technology
,
annual
conference
,
north
american
chapter
,
bertlett
,
taskar
,
mca
llester
,
gradient
algorithm
,
large
margin
structured
classification
,
proceeding
,
advance
,
neural
information
processing
system
,
den
ero
,
chiang
,
knight
,
fast
consensus
,
translation
,
proceeding
,
association
,
computational
linguistics
,
den
ero
,
shankar
kumar
,
ciprian
chelba
,
och
,
model
combination
,
machine
translation
,
proceeding
,
north
american
association
,
computational
linguistics
,
duan
,
ming
zhou
,
model
based
minimum
bayes
risk
,
multiple
machine
translation
system
,
proceeding
,
international
conference
,
computational
linguistics
,
philipp
koehn
,
statistical
significance
test
,
machine
translation
evaluation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
shankar
kumar
,
byrne
,
minimum
bayes
risk
decoding
,
statistical
machine
translation
,
proceeding
,
north
american
association
,
computational
linguistics
,
shankar
kumar
,
macherey
,
dyer
,
och
,
efficient
minimum
error
rate
training
,
minimum
bayes
risk
decoding
,
translation
hypergraphs
,
lattice
,
proceeding
,
association
,
computational
linguistics
,
dongdong
zhang
,
ming
zhou
,
collaborative
decoding
,
partial
hypothesis
re
ranking
,
translation
consensus
,
decoder
,
proceeding
,
association
,
computational
linguistics
,
liang
,
alexandre
bouchard
cote
,
klein
,
taskar
,
end
to
end
discriminative
approach
,
machine
translation
,
proceeding
,
international
conference
,
computational
linguistics
,
genabith
,
consistent
translation
,
discriminative
learning
,
translation
memory
inspired
approach
,
proceeding
,
association
,
computational
linguistics
,
och
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
tromble
,
shankar
kumar
,
och
,
macherey
,
lattice
minimum
bayes
risk
decoding
,
statistical
machine
translation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
stochastic
inversion
transduction
grammar
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
mauser
,
ney
,
phrase
translation
model
,
leaving
one
out
,
proceeding
,
association
,
computational
linguistics
,
deyi
xiong
,
qun
liu
,
shouxun
,
maximum
entropy
,
phrase
,
statistical
machine
translation
,
proceeding
,
association
,
computational
linguistics
,
xiaojin
zhu
,
thesis
,
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguisticsword
alignment
,
context
dependent
deep
neural
networknan
yang1
,
shujie
liu2
,
mu
li2
,
ming
zhou2
,
nenghai
yu11university
,
science
,
technology
,
china2microsoft
research
,
beijing
,
v
nayang
,
shujliu
,
comynh
ustc
,
novel
bilingual
word
alignment
approach
,
ond
nn
,
deep
neural
network
,
various
machine
learning
task
,
detailhow
,
cd
dnn
hmm
,
speech
recognition
,
word
alignment
model
,
whichbilingual
word
embedding
,
lexical
translationinformation
,
bilingual
correspondence
,
method
,
verycompact
model
,
parameters
,
large
scale
word
alignment
task
show
,
method
,
ibm
model
,
baseline
,
astrong
resurgent
interest
,
neural
network
,
name
deep
learning
,
groundbreaking
paper
,
hin
ton
,
procedures
,
deep
structure
,
method
,
support
vectormachine
,
conditional
random
field
,
maximum
entropy
,
handcraft
feature
,
suitable
feature
,
representations
,
raw
input
data
,
success
until2006
,
researcher
,
proper
wayto
intialize
,
deep
architecture
,
layer
wise
unsupervised
pre
training
,
supervised
fine
tuning
,
pre
training
,
hinton
,
bengio
,
sparse
coding
,
unsupervised
pre
training
,
network
,
parameter
,
layer
towardsbetter
region
,
parameter
space
,
bengio
,
fine
tuning
,
region
,
isshown
,
state
of
the
performance
,
various
area
,
dahl
etal
,
breakthrough
result
,
imagenetdataset
,
objective
recognition
,
krizhevsky
etal
,
context
dependent
neural
networkwith
large
vocabulary
,
convert
atomic
lexical
entry
,
real
valued
representation
,
wordembedding
,
dimension
,
latent
aspect
,
syntactic
property
,
bengio
,
embedding
,
huge
amount
ofmonolingual
text
,
task
specific
objective
,
ollobert
et
,
socher
,
recursive
neural
network
,
structural
predictiontasks
,
tagging
,
parsing
,
compositional
aspect
ofword
representation
,
successful
previous
,
new
dnn
,
word
alignment
method
,
semantic
similarities
,
word
pair
,
word
,
common
word
,
farmer
yibula
,
word
alignmentthe
word
,
similar
meaning
,
corresponding
word
,
last
paragraph
,
word
embedding
,
huge
monolingual
text
,
ability
,
vector
space
,
similar
word
,
figure
,
word
pair
,
yibula
,
word
,
yibula
,
word
,
yibula
,
rare
nameentities
,
chineseside
,
side
,
pattern
,
context
,
wordpair
,
variablex
,
pattern
,
gongcheng
,
word
pair
,
analysis
,
boththe
word
,
source
,
target
side
,
vector
,
trainedword
embeddings
,
word
pair
,
amulti
layer
neural
network
,
contexts
,
source
,
targetsides
,
consideration
,
mm
like
distortion
model
,
neural
networkto
,
structural
aspect
,
bilingual
sentences
,
related
aboutdnn
,
word
alignment
,
ins
ection
,
brief
ofd
nn
,
detailsof
,
word
alignment
,
includingthe
detail
,
network
structure
,
training
method
,
merits
,
experiments
,
workdnn
,
unsupervised
pre
training
,
hinton
,
mni
stdigit
image
classification
problem
,
layer
wise
pre
trainer
,
thelayer
wise
pre
training
phase
,
localmaximum
,
multilayer
network
,
recognitiontask
,
imagenet
dataset
,
thestate
of
the
error
rate
,
applycontext
dependent
deep
neural
network
,
recognitiontask
,
traditionalmodels
,
method
,
nlp
start
,
word
intoa
fixed
length
,
vector
,
engio
etal
,
multilayer
neural
network
,
language
,
several
nlp
task
,
suchas
part
of
speech
tagging
,
name
entityrecognition
,
semantic
labeling
,
syntactic
parsing
,
state
of
the
,
iehuesand
waibel
,
machine
translation
result
,
neurallanguage
model
,
n
gram
traditional
language
,
translation
quality
ofn
gram
translation
model
,
acontext
free
cross
lingual
word
embeddings
,
facilitate
cross
lingual
information
retrieval
,
related
,
word
alignment
,
popular
method
,
generativemodels
,
ibm
model
,
features
,
word
alignment
,
phrase
,
rulepairs
,
context
information
,
log
linear
framework
,
previous
discriminativemethods
,
handcrafted
feature
,
raw
word
,
dnn
structure
,
prevalent
feature
,
apply
dnn
,
first
step
,
discrete
word
,
word
embedding
,
alow
dimensional
,
real
valued
vector
,
gio
,
semantic
knowledge
ofthe
word
,
dimension
embedding
matrix
wv
,
pre
determinedembedding
length
,
embeddings
,
respective
column
,
lookup
layer
lt
,
first
layer
,
input
layer
,
neural
network
,
embeddings
,
subsequent
classical
network
layer
,
nonlinear
relations
,
output
,
lth
layer
,
matrix
,
zl
length
vector
,
activation
function
,
lastlayer
,
common
choice
,
sigmoid
function
,
hyperbolic
function
,
hyperbolic
function
,
col
lobert
et
,
hyperbolicfunction
,
activation
function
,
softmaxlayer
,
bridle
,
normalization
,
zli
ezl
,
sized
input
,
output
,
variable
length
,
convolution
layer
,
layer
,
variable
lengthinput
,
length
vector
,
processing
,
multilayer
neural
network
,
withthe
standard
,
network
,
thetask
specific
objective
,
many
local
maximum
,
special
care
,
theoptimization
process
,
good
parameter
,
technique
,
layerwise
pre
training
,
many
trick
,
neural
networks
,
neural
network
training
,
hyperparameters
,
learningrate
,
hidden
layer
,
word
alignmentour
dnn
word
alignment
model
,
classichmm
word
alignment
model
,
following
form
,
lexical
translation
probabilityand
pd
,
jump
distance
distortion
probability
,
straightforward
,
neural
network
,
emission
,
lexical
translation
,
probability
plex
,
softmax
layer
,
neural
network
,
vocabulary
,
natural
languagesis
,
normalization
,
upthe
probabilistic
interpretation
,
resort
,
lexical
translation
score
computedby
neural
network
,
distortion
score
,
classic
hmm
word
alignment
model
,
context
,
lexical
translationprobability
,
ei
fai
,
ei
context
,
context
,
many
additional
parameter
,
serious
over
fitting
problem
,
sparseness
,
matter
,
contexts
,
lexical
translation
table
,
hmm
already
contains
,
parameter
,
source
,
target
vocabularysizes
,
contrast
,
separate
translation
score
parameter
,
everysource
target
word
pair
,
multilayer
network
,
explosive
growthof
number
,
parameter
,
window
e
target
window
,
farmer
yibula
,
fefet
jilexfigure
,
network
structure
,
contextdependent
lexical
translation
score
,
examplecomputes
translation
score
,
word
pair
,
context
,
figure
,
neural
network
,
compute
context
dependent
lexical
translation
score
tlex
,
length
window
,
fjas
input
,
ei
sw2
,
source
andtarget
side
,
toembeddings
,
lookup
table
lt
,
thecatenation
,
embeddings
,
classic
neural
network
,
hidden
layers
,
outputof
,
network
,
lexical
translation
score
,
f2
layer
,
activation
function
,
linear
transformation
,
noactivation
function
,
distortion
,
window
,
neural
network
,
lexical
translation
score
,
jump
distance
,
bucket
,
length
,
theoutput
layer
,
dimension
,
theoutput
stand
,
different
bucket
,
jump
distances
,
initial
experimentson
small
scale
data
,
distortion
,
alignment
,
simple
jump
distance
,
lexicalizeddistortion
,
reverse
,
alignment
model
,
source
vocabulary
ve
,
collobert
,
additionto
real
word
,
vocabulary
,
specialunknown
word
symbol
,
unseenwords
,
special
null
symbol
,
null
word
,
window
,
identical
null
symbols
,
lexical
translationscores
,
source
target
wordpair
,
neural
network
,
after
,
forward
backward
algorithm
,
viterbi
path
,
classic
hmm
model
,
majority
,
tunable
parameter
,
ourmodel
resides
,
lookup
table
lt
,
large
vocabulary
,
parameter
,
classichmm
model
,
ability
,
context
,
unique
toour
model
,
discriminative
word
alignmentcan
model
context
,
arbitrary
feature
,
previous
discriminative
word
alignment
,
raw
word
,
maximum
entropy
model
,
bag
of
words
context
forword
alignment
,
model
treat
,
distinct
feature
,
thesimilarity
,
eisner
,
practice
,
nonzero
parameter
,
classic
hmm
model
,
many
word
,
bilingual
sentence
pair
,
nonzero
parameter
,
classic
hmm
modelis
,
million
,
nn
model
,4
millions
,
raw
sentence
pair
,
toocomputational
demanding
,
lexical
translation
probability
,
neural
network
,
simpler
supervised
approach
,
model
fromsentence
pair
,
word
alignment
,
corpus
,
traditional
word
alignment
model
mm
,
ibm
model
,
word
alignment
,
large
parallel
corpus
,
bidirectional
alignment
,
usual
grow
diag
final
heuristic
,
unidirectional
result
,
direction
,
usethe
result
,
training
data
,
similar
,
speech
recognition
task
,
dahl
etal
,
neural
networkmodel
,
traditional
gaussian
mixture
model
,
tunable
parameter
,
neural
network
alignment
model
include
,
word
embeddings
,
lookuptable
lt
,
parametersw
,
linear
transformations
,
hidden
layer
,
neural
network
,
distortion
parameter
,
jump
distance
,
following
ranking
loss
,
margin
,
criterion
,
tunable
parameter
,
scoring
incorrect
alignment
path
,
ismodel
score
,
alignment
path
,
nuance
,
gold
alignment
after
grow
diag
final
contains
many
to
many
link
,
solution
,
source
word
alignment
multiple
target
,
allcandidates
,
golden
link
,
multilayer
neural
network
,
criterion
,
unlikely
toyield
good
result
,
followingsteps
,
withmonolingual
datamost
parameter
,
good
initial
value
,
usual
approach
isto
,
embeddings
,
large
monolingual
corpus
,
train
word
embeddings
,
sourceand
target
language
,
monolingual
corpus
,
vocabulary
v
,
eachside
,
parallel
corpus
,
unknown
word
,
word
embedding
length
,
window
size
,
lengthof
,
hidden
layer
,
follow
,
turian
etal
,
stochastic
gradient
,
ranking
loss
,
fixed
learning
rate
,
null
wordin
,
monolingual
corpus
,
theinitial
value
,
monolingual
corpuslearn
strong
syntactic
knowledge
,
word
alignment
,
language
pair
,
englishand
,
many
wordscan
act
,
adjective
,
anychange
,
counter
part
,
distinct
word
,
different
word
embeddingsdue
,
different
syntactic
role
,
subsequent
step
,
bilingual
data
,
neural
network
,
network
,
distortion
parameter
,
neural
network
,
lexical
translation
,
local
pairwise
loss
,
correct
word
pair
,
awrong
word
pair
,
training
criterion
,
model
suffers
loss
,
correct
word
,
random
,
margin
,
lookup
table
,
embeddings
,
monolingual
training
,
stochastic
gradient
,
cycle
,
foreach
,
word
pair
,
null
alignment
,
positive
,
twonegative
,
either170side
,
sentencepair
,
criterion
,
stochastic
optimizer
,
parallel
corpus
,
iterations
,
model
concrete
,
stillhyper
parameters
,
windowsize
sw
,
length
,
hidden
layerll
,
l1
to120
,
minimal
losson
,
small
held
out
data
,
several
setting
,
distortion
parameterswe
,
neural
network
parameter
,
fromthe
last
step
,
distortion
parameterssd
,
respect
,
aseparate
parameter
,
jump
distance
,
parameter
,
forward
backward
jump
,
parameters
,
learning
rate
,
stochastic
optimizer
,
parameters
,
optimizer
,
small
portion
,
parallel
corpus
,
neural
network
,
parameter
,
lexical
translationneural
network
,
thesentence
level
criterion
,
online
training
,
distortion
parameter
,
distortion
parameter
,
neural
network
,
context
inmodeling
word
translation
,
parameter
,
tuning
,
initial
small
scale
experiment
,
resultswe
,
to
englishword
alignment
task
,
alignedchinese
alignment
corpus
,
haghighi
etal
,
segmentation
,
chineseside
,
word
segmentation
standard
,
millionunique
sentence
pair
,
minedfrom
,
monolingual
corpus
,
pre
train
word
embeddings
,
whichamounts
,
unique
sentencesfor
,
allenglish
word
,
special
token
,
addressesand
url
,
special
token
,
ibm
model
,
ourbaseline
,
results
,
classic
hmm
,
ibm
model
,
modelare
,
bidirectional
result
,
allmodels
,
precision
,
alignment
,
classichmm
,
word
alignment
result
,
first
rowand
third
show
baseline
result
,
byclassic
hmm
,
second
rowand
fourth
show
result
,
result
,
qualityof
baseline
result
,
training
,
method
,
word
alignment
model
,
effecton
end
to
end
smt
performance
,
hier
171archical
phrase
model
,
chiang
,
different
word
alignment
,
different
alignment
score
,
significant
difference
,
translation
performance
,
nis
t
,
nis
t
,
insensitive
ble
u
score
,
nis
t
08test
,
nn
alignment
result
,
result
,
parallel
corpus
,
similar
observation
,
inprevious
,
macherey
,
alignment
quality
,
end
to
end
result
,
f
score
,
ourmodel
,
precision
,
result
,
baseline
,
large
part
,
alignment
link
,
word
,
learnstheir
alignment
pattern
,
baseline
model
,
modelperforms
,
low
frequency
word
,
proper
noun
,
person
name
,
low
frequency
word
,
onwhich
baseline
hmm
,
garbage
collector
,
phenomenon
,
different
person
name
,
similar
word
embeddings
,
side
,
side
,
monolingual
pre
training
,
different
person
name
,
similar
contexts
,
model
considers
,
word
embeddings
,
context
,
personnames
,
person
name
,
baseline
model
,
precision
,
contextto
,
context
contribute
,
alignmentquality
,
different
window
size
,
result
,
ibm
model
,
fromfigure
,
increasethe
quality
,
learned
alignment
,
benefit
,
window
size
,
theother
hand
,
result
,
noticeable
over
,13
f
igure
,
effect
,
different
window
size
,
wordalignment
f
score
,
fitting
problem
,
surprising
considering
,
window
size
,
parameter
,
linear
layer
,
itis
,
context
,
window
size
,
performs
,
settings
,
window
size
,
baseline
,
explanation
,
model
usesthe
simple
jump
distance
,
distortion
,
whichis
weaker
,
sophisticated
distortionsin
ibm
model
,
context
,
window
size
,
accurate
translation
score
,
morecontexts
,
despitethe
simpler
distortion
,
nn
f
hidden
layer
,
effect
,
different
number
,
hidden
layers
,
hidden
layer
,
hiddenlayer
,
hidden
layer
,
further
improvement
,
hidden
layersour
neural
network
contains
,
hidden
layer
,
lookup
layer
,
question
,
and3
layer
,
result
,
ibm
model4
,
hidden
layer
setting
,
hiddenlayer
length
,
hidden
layer
length
,
hidden
layer
setting
,
hidden
layer
,
good
history
,
lab
zetian
laggardslmbad
tradition
,
network
,
underperformersgreat
culture
,
technology
,
transfereesstrong
practice
dutch
,
innovation
,
megabankstrue
style
,
system
xingzhi
mutualseasy
literature
canadian
,
industry
,
non
starterswanice
historical
offering
lab
hongzhang
underperformersgreat
historic
britain
,
laboratory
,
developed
serve
laboratory
xueqin
transfereespretty
record
,
exam
fuhuan
matriculantsexcellent
,
neighbor
,
several
word
,
neighborsof
word
embeddings
,
neighbor
,
wordembeddings
,
word
alignment
model
,
improvement
,
time
constraint
,
hyper
parameters
,
length
ofhidden
layer
,
hidden
layer
setting
,
setting
,
hidden
layers
,
setting
,
collobert
,
somewords
,
neighbor
,
theeuclidean
distance
,
embeddings
,
bilingual
training
,
opposite
semantic
meaning
,
neighbor
,
history
,
person
name
,
neighbor
,
zetian
,
name
,
setting
,
language
,
morphology
,
singleform
,
plural
form
,
oftencorrespond
,
word
,
word
,
word
embeddings
,
frequent
noun
,
rarer
noun
,
monolingualembeddings
,
bilingual
training
,
lastcolumn
,
neighborhood
,
laggard
,
plural
form
,
deep
neural
network
,
word
alignment
task
,
modelintegrates
,
multilayer
neural
network
,
anh
mm
like
framework
,
context
dependentlexical
translation
score
,
neuralnetwork
,
distortion
,
simple
jump
distance
scheme
,
bilingual
corpus
,
monolingual
data
,
pre
train
word
embeddings
,
large
scale
to
task
show
,
methodproduces
,
word
alignment
result
,
classic
hmm
model
,
future
,
settings
,
different
hyper
parameters
,
possibility
,
training
,
neural
word
alignment
model
,
reliance
,
alignment
resultof
model
,
current
modeluse
,
simple
distortion
,
sophisticated
model
,
recursive
neuralnetworks
,
socher
,
acknowledgmentswe
,
anonymous
reviewer
,
insightfulcomments
,
dongdong
zhang
,
leicui
,
chunyang
wu
,
zhenyan
,
fruitfuldiscussions
,
referencesyoshua
bengio
,
holger
schwenk
,
se
,
bastiensene
,
deric
morin
,
luc
gauvain
,
neural
probabilistic
language
model
,
innovations
,
machine
learning
,
yoshua
bengio
,
lamblin
,
popovici
,
and173hugo
larochelle
,
greedy
layer
wise
trainingof
deep
network
,
advance
,
yoshua
bengio
,
deep
architecture
,
foundation
,
machine
learning
,
berger
,
stephena
,
maximum
entropy
approach
,
natural
language
processing
,
comput
,
linguist
,
j
bridle
,
algorithm
,
architectures
,
application
,
chapter
probabilistic
interpretation
,
feedforward
classification
network
outputs
,
relationship
,
statistical
pattern
recognition
,
f
brown
,
j
d
,
a
d
ellapietra
,
l
mercer
,
mathematics
,
statistical
machine
translation
,
parameter
estimation
,
computational
linguistics
,
chiang
,
hierarchical
phrase
based
translation
,
computational
linguistics
,
ronan
collobert
,
bottou
,
michaelkarlen
,
koray
kavukcuoglu
,
kuksa
,
natural
language
processing
,
fromscratch
,
journal
,
machine
learning
research
,
e
d
ahl
,
acero
,
context
dependent
pre
trained
deep
neuralnetworks
,
large
vocabulary
speech
recognition
,
speech
,
den
ero
,
macherey
,
model
based
aligner
combination
,
dual
decomposition
,
alon
lavie
,
asmith
,
word
alignment
,
arbitrary
feature
,
proceeding
,
annualmeeting
,
association
,
computational
linguistics
,
human
language
technologies
volume
,
association
,
computational
linguistics
,
aria
haghighi
,
blitzer
,
den
ero
,
danklein
,
better
word
alignment
,
supervised
itg
model
,
proceeding
,
jointconference
,47
th
annual
meeting
,
international
joint
conference
,
natural
language
processing
,
afn
lp
,
volume
volume
,
association
,
computational
linguistics
,
e
h
inton
,
osindero
,
yee
whyeteh
,
fast
learning
algorithm
,
deep
belief
net
,
neural
computation
,
koray
kavukcuoglu
,
sermanet
,
boureau
,
yann
lec
un
,
convolutional
feature
hierarchy
forvisual
recognition
,
advance
,
neural
informationprocessing
system
,
philipp
koehn
,
och
,
marcu
,
statistical
phrase
based
translation
,
inp
roceedings
,
conference
,
northamerican
chapter
,
association
,
computational
linguistics
,
human
language
technology
volume
,
association
,
computational
linguistics
,
krizhevsky
,
ilya
sutskever
,
hinton
,
imagenet
classification
,
deep
convolu
tional
neural
network
,
advance
,
lec
un
,
bottou
,
yoshua
bengio
,
patrickhaffner
,
gradient
based
learning
,
todocument
recognition
,
proceeding
,
lec
un
,
learning
scheme
,
asymmetric
threshold
network
,
proceeding
,
honglak
,
battle
,
rajat
,
an
y
n
,
advance
,
neural
information
processingsystems
,
shujie
liu
,
ming
zhou
,
discriminative
pruning
,
discriminative
itg
alignment
,
proceeding
,
annual
meeting
,
association
,
lan
boureau
,
yann
le
cun
,
sparse
feature
,
deep
beliefnetworks
,
advance
,
neural
information
processing
system
,
c
m
,
discriminative
frameworkfor
bilingual
word
alignment
,
proceeding
,
conference
,
human
language
technology
andempirical
method
,
natural
language
processing
,
association
,
computationallinguistics
,
niehues
,
waibel
,
continuousspace
language
model
,
restricted
boltzmannmachines
,
proceeding
,
nineth
international
workshop
,
och
,
ney
,
statistical
translation
model
,
seide
,
conversational
speech
transcription
,
context
dependentdeep
neural
network
,
interspeech
,
pages437
,
a
,
eisner
,
contrastiveestimation
,
training
log
linear
model
,
unlabeleddata
,
proceeding
,
annual
meetingon
association
,
computational
linguistics
,
pages354
,
association
,
computational
linguistics
,
socher
,
c
l
,
y
n
,
naturalscenes
,
natural
language
,
recursive
neural
network
,
proceeding
,
international
conference
,
volume
,
huval
,
d
manning
,
y
n
,
semantic
compositional
ity
,
recursive
matrix
vector
space
,
proceedings
,
joint
conference
,
empirical
method
,
natural
language
processing
andcomputational
natural
language
learning
,
pages1201
,
association
,
computational
linguistics
,
le
hai
,
alexandre
allauzen
,
ois
yvon
,
continuous
space
translation
model
withneural
network
,
proceeding
,
conference
,
north
american
chapter
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
titov
,
alexandre
klementiev
,
binod
bhat
tarai
,
representations
,
turian
,
ratinov
,
yoshua
bengio
,
word
representation
,
simple
,
semi
supervised
learning
,
urbana
,
vogel
,
ney
,
tillmann
,
hmm
based
word
alignment
,
statisticaltranslation
,
proceeding
,
conferenceon
computational
linguistics
volume
,
association
,
computational
linguistics
,
stochastic
inversion
transductiongrammars
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguisticslearning
entity
representation
,
longkai
zhang
,
houfeng
,
laboratory
,
computational
linguistics
,
peking
,
ministry
,
education
,
microsoft
research
asiahezhengyan
,
com
shujliu
,
novel
entity
disambiguation
model
,
utilizing
simplesimilarity
measure
,
disjoint
combinations
,
method
,
entity
representation
,
agiven
similarity
measure
,
denois
ing
auto
encoders
,
initial
document
representation
inan
,
pre
training
stage
,
supervised
fine
tuning
stage
,
representation
,
similarity
measure
,
experiment
result
,
method
,
state
of
the
artperformance
,
public
datasets
,
complex
collective
approach
,
ntroductionentity
linking
,
disambiguation
,
much
attention
,
natural
language
processing
community
,
bunescu
,
kataria
,
subtasks
inknowledge
base
construction
,
grishman
,
attribute
,
entity
,
mention
,
creature
,
creature
,
earth
dragon
,
mythologyand
,
popular
programming
language
,
phi
,
auto
part
supplier
,
mount
par
nassus
,
straightforward
method
,
context
,
mention
,
definition
ofcandidate
entity
,
previous
,
relatedness
,
context
,
corresponding
authord
,
entity
,
product
,
cosine
similarity
,
kullback
leibler
divergence
,
jaccard
distance
,
complicated
one
,
kulkarni
,
hoffart
,
bunescuand
pasca
,
cucerzan
,
measure
,
atomic
nature
determines
,
internal
structure
,
focus
,
collective
disambiguation
,
kulkarni
,
ratinov
,
hoffart
,
ambiguous
mention
,
context
,
coherenceamong
decision
,
collective
approach
,
nontrivial
decision
process
,
ati
et
,
global
approaches
,
local
method
,
similarity
,
context
,
entity
,
somehow
,
importance
,
good
modeling
,
context
entity
association
,
word
level
,
topic
model
,
kataria
,
inthe
semantic
space
,
one
topic
per
entity
assumption
,
tolarge
knowledge
base
,
entity
,
training
objective
,
correspond
withdisambiguation
performance
,
disadvantage
,
previous
approaches
,
novel
method
,
context
entity
association
,
deep
architecture
,
deep
neural
network
,
hinton
,
bengio
,
hierarchical
manner
,
context
,
level
abstraction
,
lowerlevels
,
general
concept
,
entity
,
compact
model
,
disambiguationperformance
,
method
,
doc
30ument
,
entity
representation
,
fixed
similarity
measure
,
underlying
representations
,
similarity
measure
,
internal
structure
,
similarity
measure
,
features
,
large
scale
annotationof
wikipedia
,
manual
design
effort
,
learned
model
,
topic
model
,
expensive
sampling
strategy
,
simplicity
,
complex
collective
approach
,
ourexperiments
,
learned
similarity
measure
,
collectiveapproaches
,
boost
performance
,
representation
forcontextual
documentgiven
,
mention
,
candidate
entity
,
generated
form
,
candidate
entity
,
likely
refers
,
algorithm
consists
,
pre
training
stage
,
auto
encodersare
built
,
unsupervised
layer
wise
fashion
,
general
concept
,
thesupervised
fine
tuning
stage
,
entire
networkweights
,
similarityscore
,
bengio
,
building
block
,
deep
learning
,
vector
,
auto
encoder
consists
,
process
,
decoding
process
,
thereconstruction
error
,
retainingmaximum
information
,
stackingnew
auto
encoder
,
auto
encoders
,
thisway
,
multiple
level
,
representation
,
auto
encoder
,
allwords
,
matter
,
function
word
os
content
word
,
givena
random
corruption
,
globalstructure
,
author
showsin
image
processing
,
binary
bag
of
words
vector
,
general
concept
,
ignorenoise
,
function
word
,
maskingnoise
,
randomly
mask
,
fill
in
the
blank
property
,
etal
,
component
,
partial
input
,
python
,
hidden
unit
,
thehidden
unit
,
concept
ofg
mythology
,
mask
outactivefigure
,
reconstruction
sampling
,
large
number
,
entity
,
vocabulary
size
,
considerable
computationaloverhead
,
reconstruction
process
involves
expensive
dense
matrix
multiplication
,
reconstruction
,
sparse
propertyof
matrix
multiplication
,
smallsubset
,
original
input
,
quality
ofthe
,
representation
,
dauphin
,
fine
tuningthis
stage
,
learned
representation
,
rankingscore
,
large
scale
wikipedia
annotation
,
supervision
,
hyperlink
,
training
,
mention
string
,
candidate
generation
,
network
weight
,
hidden
layer
,
pre
training
stage
,
thelearned
representation
,
whole
network
,
final
supervised
objective
,
reasonto
stack
,
learned
representation
,
problem
specific
structures
,
encoding
,
problem
specificlayer
,
representation
,
weightand
bias
term
,
same31encoding
process
,
similarity
score
,
asthe
product
,
network
structure
,
fine
tuning
stage
,
correct
entity
,
rest
candidate
,
context
ofthe
mention
,
negative
candidate
pair
,
pairwise
,
criterion
,
candidate
pair
,
similarityscore
,
true
pair
,
loss
function
,
asnegative
log
,
softmax
function
,
exp
,
training
instance
,
loss
function
,
contrastive
estimation
,
eisner
,
positive
takesprobability
mass
,
penalizing
,
negative
,
convergence
,
thesoftmax
loss
function
,
ranking
loss
function
,
asour
default
setting
,
softmax
training
criterion
addsadditional
computational
overhead
,
mini
batch
size
,
toconverge
,
mini
batchsize
ism
,
candidate
,
network
,
similarity
matrix
,
pairwise
,
criterion
onlyneeds
,
problem
,
mentionm
,
candidate
entity
,
forward
backward
path
,
forward
backwardpasses
,
mini
batch
,
python
,
language
,
pythonidaepython
,
e0
e1
e2
enf
igure
,
mini
batch
,
reorganization
,
mini
batch
,
similarin
spirit
,
kuchler
,
general
backpropagation
algorithm
,
neural
network
,
parent
,
child
node
,
forwardpass
stage
,
child
node
,
gradient
,
sumof
derivative
,
parent
,
parent
node
,
score
node
,
childnodes
,
figure
,
rowshares
,
columnshares
,
backpropagationstage
,
gradient
,
scorenodes
,
column
,
bag
of
words
binary
vector
,
anyhandcrafted
feature
,
future
,
analysistraining
setting
,
pre
training
stage
,
inputlayer
,
hidden
layer
,
functionmax
,
following
,
glorot
,
first
reconstruction
layer
,
sigmoid
activation
function
,
cross
entropy
error
function
,
higherreconstruction
layer
,
activation
function
,
lossas
error
function
,
corruption
process
,
noise
probability
,
thefirst
layer
,
a
g
aussian
noise
,
standard
deviation
,
reconstruction
rate
,
final
layer
,
unit
withsigmoid
activation
function
,
mini
batch
size
,
a
l
inux
machine
,72
g
memory
,
core
xeon
cpu
,
themodel
,
python
,
extensions
,
openblas
library
,
thanks
,
refinedmini
batch
arrangement
,
day
toconverge
,
pre
training
,
fine
tuning
,
training
,
datasets
,
wikipedia
,
plain
text
,
article
,
pre
training
,40
m
hyperlink
,
byname
,
fine
tuning
stage
,
hyperlink
,
model
selection
,
layer
network
,
tac
kbp
,
grishman
,
dataset
,
non
collective
approach
,
dataset
,
collective
approach
,
bothdatasets
,
non
query
,
thetac
kbp
,
testb
dataset
,
non
query
,
candidate
generation
,
wikipedia
structure
,
cucerzan
,
candidates
,
speed
consideration
,
candidate
generation
recall
,
analysis
,
show
evaluation
,
collective
approach
,
personalized
pagerank
,
evidence
between1available
,
wikimedia
,
org
enwiki
,
mpi
inf
,
yago
naga
different
decision
,
surprise
,
methodwith
,
local
evidence
,
several
complex
collective
method
,
simple
word
similarity
,
importance
,
context
modeling
,
semantic
space
,
collective
approach
,
local
evidence
isnot
,
similarity
measure
,
result
,
typical
errors
,
prominence
feature
andname
matching
feature
,
candidate
,
entities
,
different
name
,
detection
module
,
realistic
application
scenario
,
first
thought
,
pseudo
withwikipedia
annotation
,
thethreshold
,
feature
weight
,
bunescu
,
kulkarni
,
method
,
evallcc
,
ai
da
dataset
,
collective
approach
,
cosine
,
evaluation
,
deep
learning
approach
,
context
entity
similarity
,
entity
disambiguation
,
intermediate
representations
,
large
scale
annotations
,
wikipedia
,
manual
effortof
,
learned
representationof
entity
,
largeknowledge
base
,
furthermore
,
experiment
revealsthe
importance
,
context
modeling
,
learned
measure
,
collective
approach
,
jie
liu
,
fei
,
helpful
discussions
,
research
,
national
hightechnology
research
,
development
program
,
national
social
science
fund
,
referencesy
,
larochelle
,
greedy
layer
wise
training
,
deep
network
,
advance
,
neural
information
,
bunescu
,
encyclopedic
knowledge
,
entity
disambiguation
,
inp
roceedings
,
volume
,
cucerzan
,
large
scale
,
entity
disambiguation
,
wikipedia
data
,
proceedingsof
emn
lp
conll
,
volume
,
bengio
,
large
scale
learning
,
embeddings
,
reconstruction
sampling
,
proceeding
,
twenty
eighth
international
conference
,
bengio
,
domainadaptation
,
large
scale
sentiment
classification
,
proceeding
,28
thinternational
conference
,
machine
learning
,
goller
,
kuchler
,
representation
,
structure
,
neural
networks
,
collective
entity
,
text
,
graph
based
method
,
inp
roceedings
,
international
acm
irconference
,
research
,
development
,
information
retrieval
,
fastlearning
algorithm
,
deep
belief
net
,
neural
computation
,
hoffart
,
weikum
,
robust
disambiguation
,
entity
,
proceeding
,
conference
,
empirical
method
,
natural
languageprocessing
,
association
,
computational
linguistics
,
heng
ji
,
grishman
,
knowledgebase
population
,
successful
approach
,
challenges
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
human
language
technology
,
portland
,
sengamedu
,
entity
disambiguation
,
hierarchical
topic
model
,
proceeding
,
chakrabarti
,
collective
annotation
ofwikipedia
entity
,
text
,
proceeding
,
acm
kdd
international
conference
onk
nowledge
discovery
,
data
mining
,
basepopulation
,
workshop
,
anderson
,
global
algorithm
,
disambiguation
,
proceeding
,
annualmeeting
,
association
,
collective
context
aware
topic
models
,
entity
disambiguation
,
proceeding
,
the21st
international
conference
,
world
wide
,
nishio
,
entitydisambiguation
,
probabilistic
taxonomy
,
technical
report
,
technical
report
msr
tr
,
eisner
,
contrastive
estimation
,
training
log
linear
model
,
unlabeled
data
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
man
zagol
,
robustfeatures
,
autoencoders
,
proceedings
,25
th
international
conference
,
machine
learning
,
larochelle
,
lajoie
,
yoshua
bengio
,
manzagol
,
autoencoders
,
learninguseful
representation
,
deep
network
,
localdenoising
criterion
,
journal
,
machine
learning
research
,
effective
acronym
expansion
,
instanceselection
,
topic
modeling
,
proceeding
,
twenty
second
international
joint
conference
ona
rtificial
intelligence
volume
volume
three
,
pages1909
,
zhicheng
zheng
,
xi
aoyan
zhu
,
entity
,
human
language
technologies
,
annual
conference
,
northamerican
chapter
,
association
,
computational
linguistics
,
los
,
association
,
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguisticsbilingual
data
cleaning
,
graph
based
random
walk
,
school
,
computer
science
,
technology
,
harbin
,
chinaleicui
,
beijing
,
dozhang
,
shujliu
,
comabstractthe
quality
,
bilingual
data
,
low
quality
bilingual
data
tends
,
incorrect
translation
knowledge
andalso
,
translation
,
previous
,
method
,
low
quality
data
,
fair
amount
,
humanlabeled
,
reliance
,
labeled
,
unsupervised
method
,
bilingual
data
,
method
,
mutual
reinforcement
,
sentencepairs
,
extracted
phrase
pair
,
observation
,
sentencepairs
,
phrase
extractionand
vice
versa
,
end
to
end
experimentsshow
,
method
,
onthe
amount
,
bilingual
data
,
quality
,
inreal
world
smt
system
,
bilingual
data
,
low
quality
data
,
low
quality
bilingual
data
degradesthe
quality
,
word
alignment
,
incorrect
phrase
pair
,
translation
performance
,
phrase
based
smt
system
,
here
fore
,
data
quality
information
,
translation
modeling
,
previous
,
bilingual
data
,
learning
method
,
several
bilingual
data
mining
system
,
resnik
,
first
author
,
visiting
microsoft
research
,
munteanu
andmarcu
,
post
processing
step
,
classifier
,
nonparallel
data
,
partial
parallel
data
,
method
,
low
qualitybilingual
data
,
instance
,
unsupervised
approach
,
bilingual
data
,
intuitivethat
high
quality
parallel
data
tends
,
phrase
pair
,
low
quality
data
,
meanwhile
,
phrase
,
bilingual
corpus
,
frequent
one
,
good
sentence
pair
areprone
,
frequent
phrase
pair
,
wuebker
,
kind
ofmutual
reinforcement
fit
,
frameworkof
graph
based
random
walk
,
phrase
pairp
,
phrase
pair
,
thephrase
pair
,
quality
,
sentencepair
,
extracted
phrase
pair
,
a
p
agerank
style
random
walk
algorithm
,
mihalcea
,
ta
rau
,
importance
score
,
quality
,
thebetter
,
method
,
method
,
importance
scoresof
sentence
pair
,
fractional
count
,
phrase
translation
probability
,
bilingual
data
,
experimental
result
,
large
scalechinese
to
translation
task
,
general
algorithmto
,
importance
,
vertex
withinthe
graph
,
global
view
,
method
,
ver
tices
,
phrase
pair
,
importance
,
vertex
,
vertex
,
different
scenario
,
unweighted
form
,
starting
,
initial
score
,
algorithm
,
compute
theimportance
score
,
vertex
,
orthe
difference
,
consecutive
iterationsfalls
,
predefined
threshold
,
weighted
bipartitegraph
,
sentencepairs
,
extracted
phrase
pair
,
ver
tices
,
phrase
pair
canbe
,
mutual
reinforcement
score
,
importance
score
,
between
vertex
,
figure
,
graph
structure
,
bipartite
graph
,
vertex
set
,
phrase
pairswhich
,
wordalignment
,
edgesare
,
standard
tf
idf
formula
,
otherwisesentence
pair
verticesphrase
pair
verticess1s2s3p1p3p4p5p6p2figure
,
circular
node
,
andsquare
node
,
line
capture
thesentence
phrase
mutual
reinforcement
,
phrase
pair
frequency
sentence
pair
,
inverse
phrasepair
frequency
,
whole
bilingual
corpus
,
mihalceaand
tarau
,
importance
score
,
a
p
agerank
style
algorithm
,
weight
rij
,
relationships
,
vertex
,
phrase
pair
vertex
,
default
value
,
detailed
process
,
phrase
pair
,
difference
,
twoconsecutive
iteration
,
parallelizationwhen
,
random
walk
,
large
bilingual
corpus
,
phrase
pair
,
several
day
,
pu
time
,
iteration
,
problem
,
distributed
algorithm341algorithm
m
,
random
walk1
,
end
for25
,
end
while28
,
iterative
computation
,
iterative
computation
start
,
outlink
weight
,
vertexis
,
equal
size
,
key
value
pairsin
,
different
machine
,
ineach
iteration
,
vertex
,
normalizedinlink
weight
,
key
value
pair
,
hese
key
value
pairsare
,
acrossdifferent
machine
,
long
sentence
pair
,
phrase
pair
,
importance
score
,
sentencelength
,
algorithm
,
mapre
duce
programming
model
,
ghemawat
,
implementation
,
translation
modelingafter
sufficient
number
,
iteration
,
importance
score
,
simple
filtering
,
thescores
,
fractional
count
,
translation
probability
,
phrasepairs
,
translation
probability
,
phrase
orphrase
pair
,
translation
probability
,
log
linear
model
,
addition
,
conventional
phrasetranslation
probability
,
bilingual
data
,
large
scale
to
machinetranslation
task
,
bilingual
data
,
united
nation
,
corpus
,
parallel
corpus
,
workshop
,
millionsentence
pair
,
duplicatedones
,
development
data
,
data
isshown
,
open
testnist
,
open
testnist
,
open
testnist
,
open
testcwmt
,
development
,
theexperiments
,
phrase
based
decoder
,
decoder
,
state
of
the
phrase
based
decoder
inm
os
,
implementation
,
following
feature
,
log
linear
model
,
data
cleaning
,
thepost
processing
,
corpus
,
fair
amount
ofnoisy
data
,
random
sampling
,
ih
i
i
,
wuebker
,
to
translation
task
,
datasets
,
bilingual
dataand
use
others
,
corpus
weighting
feature
,
incorporatessentence
score
,
random
walk
,
fractional
count
,
phrase
translation
probability
,
phrase
translation
probability
,
lexicalweights
,
direction
,
gram
language
model
,
kneser
neysmoothing
,
phrase
count
,
word
count
,
translation
model
,
theword
aligned
bilingual
corpus
,
direction
,
diag
grow
final
heuristic
,
symmetric
word
alignment
,
languagemodel
,
ldc
gigawordversion
,
part
,
bilingualcorpus
,
lexicalized
reordering
model
,
parallel
data
,
insensitive
ble
u4
,
papineni
,
evaluation
,
parameters
,
log
linear
model
,
optimizing
ble
,
development
data
,
bootstrap
re
sampling
,
baselinethe
experimental
result
,
inthe
baseline
system
,
phrase
,
bilingual
data
,
addition
,
fix
discount
method
,
phrase
table
smoothing
,
implementation
,
baseline
systemperform
,
model
size
,
basic
idea
,
cutoff
,
leaving
one
out
,
wuebker
,
result
,
uncharted
water
,
tansuo
,
lingyufigure
,
non
literal
translationin
,
bilingual
corpus
,
right
one
,
literaltranslation
,
comparison
,
leaving
one
out
,
method
,
baseline
,
benefit
,
bilingual
data
cleaning
method
,
addition
,
several
setting
,
low
qualitysentence
pair
,
bilingual
data
,
onthe
importance
score
,
process
,
simple
bilingual
data
filtering
,
performance
onsome
datasets
,
border
line
,
translation
performance
,
main
reason
,
random
walkapproach
,
bilingual
sentence
pair
,
non
literal
translation
,
thoseliteral
translation
,
datamay
,
translation
performance
,
bilingualcorpus
,
left
part
,
figure
,
althoughthe
translation
,
situation
,
word
,
lingyu
,
common
translation
,
outthis
kind
,
lossof
native
expression
,
trans
343lation
performance
,
nonparallel
sentence
pair
,
parallelsentence
pair
,
theimportance
score
,
phrase
translation
probability
,
substantial
improvement
,
graph
based
randomwalk
,
translation
modelingperformance
,
smt
system
,
goutte
,
phrase
based
smt
system
,
parallel
data
withdifferent
proportion
,
synthetic
noisy
data
,
noisy
parallel
data
,
phrase
based
smt
,
incorrect
alignment
,
experimental
result
,
findingson
,
datasets
,
method
,
sometimes
filtering
noisy
data
,
unexpected
results
,
reason
,
non
literal
parallel
data
,
false
positive
innoisy
data
detection
,
large
scalesmt
system
,
tolerant
tonoisy
data
,
frequency
phrase
pair
,
importance
score
,
phrasepair
probability
,
importancescores
,
contributionconstraint
,
high
quality
parallel
data
contributes
,
future
workin
,
effective
approachto
,
bilingual
data
,
graph
based
random
walk
,
significant
improvement
,
severaldatasets
,
forfuture
,
method
,
relationship
,
existingsentence
to
phrase
mutual
reinforcement
,
acknowledgmentswe
,
hongsun
,
yang
,
xilun
,
helpful
discussions
,
insightful
comment
,
referencessergey
brin
,
anatomyof
,
large
scale
hypertextual
search
engine
,
computer
network
,
isd
system
,
sanjay
ghemawat
,
mapre
duce
,
large
cluster
,
communication
,
kuhn
,
johnson
,
statistical
machinetranslation
,
proceeding
,
conferenceon
empirical
method
,
natural
language
processing
,
association
,
computational
linguistics
,
goutte
,
marine
carpuat
,
impact
,
inp
roceedings
,
san
,
association
,
machine
translation
,
theamericas
,
long
jiang
,
shiquan
yang
,
ming
zhou
,
xiaohua
liu
,
qingsheng
zhu
,
learnt
pattern
,
proceedings
,
joint
conference
,
annualmeeting
,
international
jointconference
,
natural
language
processing
,
theafnlp
,
suntec
,
association
,
computational
linguistics
,
philipp
koehn
,
och
,
marcu
,
statistical
phrase
based
translation
,
proceedings
,
hlt
naa
cl
,
main
paper
,
association
,
computational
linguistics
,
philipp
koehn
,
statistical
significance
test
,
translation
evaluation
,
dekang
,
proceeding
,
emn
lp
,
barcelona
,
associationfor
computational
linguistics
,
rada
mihalcea
,
tarau
,
textrank
,
bringing
order
,
dekang
,
dekaiwu
,
editor
,
proceeding
,
emn
lp
,
pages404
,
barcelona
,
association
forcomputational
linguistics
,
dragos
munteanu
,
marcu
,
improving
machine
translation
performance
,
exploiting
nonparallel
corpus
,
computational
linguistics
,
och
,
ney
,
systematic
comparison
,
various
statistical
alignmentmodels
,
computational
linguistics
,
och
,
ney
,
alignment
template
approach
,
statistical
machine
translation
,
computational
linguistics
,
och
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceedings
,
annual
meeting
,
associationfor
computational
linguistics
,
sap
poro
,
association
,
computationallinguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automaticevaluation
,
machine
translation
,
proceedingsof
,
annual
meeting
,
association
,
computational
linguistics
,
philadelphia
,
association
,
computational
linguistics
,
resnik
,
a
,
parallel
corpus
,
computational
linguistics
,
lei
shi
,
cheng
niu
,
ming
zhou
,
jianfeng
gao
,
tree
alignment
model
,
parallel
data
,
proceeding
,
international
conference
,
computational
linguistics
,
annual
meeting
,
association
forcomputational
linguistics
,
association
,
computational
linguistics
,
xiaojun
wan
,
jianwu
yang
,
jianguo
xiao
,
iterative
reinforcement
approach
,
simultaneous
document
summarization
,
keywordextraction
,
proceeding
,45
th
annual
meeting
,
association
,
computational
linguistics
,
prague
,
republic
,
association
,
computational
linguistics
,
stochastic
inversion
transductiongrammars
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
joern
wuebker
,
mauser
,
ney
,
phrase
translation
model
,
proceeding
,
annualmeeting
,
association
,
computational
linguistics
,
uppsala
,
association
,
computational
linguistics
,
deyi
xiong
,
qun
liu
,
shouxun
,
maximum
entropy
,
phrase
,
model
forstatistical
machine
translation
,
proceeding
,
international
conference
,
computationallinguistics
,
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
annual
meeting
,
association
,
computational
linguistics
,
baltimore
,
association
,
computational
linguisticsbilingually
constrained
phrase
embeddings
,
machine
translationjiajun
zhang1
,
shujie
liu2
,
mu
li2
,
ming
zhou2and
chengqing
zong11national
laboratory
,
jjzhang
,
cn2microsoft
research
,
shujliu
,
learnsemantic
phrase
embeddings
,
compactvector
representation
,
phrase
,
phrase
,
different
semantic
meaning
,
semantic
distance
,
translation
equivalent
,
semantic
distance
,
non
translation
pair
,
aftertraining
,
phrase
,
languagesand
,
language
,
methodon
,
end
to
end
smt
task
,
table
pruning
,
phrasal
semantic
similarity
,
semantic
similarity
,
sourcephrase
,
translation
candidate
,
extensive
experiment
,
bra
eis
,
powerful
capacity
,
feature
learning
,
representation
,
multilayer
,
great
success
inspeech
,
image
processing
,
kavukcuoglu
,
krizhevsky
,
community
,
strong
interest
,
adaptingand
,
many
task
,
wordalignment
,
translation
confidence
estimation
,
mikolov
,
prediction
,
translation
modelling
,
auli
etal
,
kalchbrenner
,
blunsom
,
andlanguage
modelling
,
vaswani
etal
,
component
,
wordembedding
,
low
dimensional
,
real
valued
vector
representation
,
bengio
,
bengio
,
collobertand
,
mikolov
,
phrase
,
basic
translation
unit
,
themodels
,
word
embeddings
,
direct
inputs
,
full
use
,
semantic
information
,
phrasaltranslation
rule
,
whole
translationprocess
,
decoding
process
,
compact
vector
representation
,
basic
phrasal
translation
unit
,
essential
andfundamental
,
phrase
embedding
,
phrase
,
sequence
,
real
valued
vector
,
previous
,
phrase
embedding
,
different
view
,
socher
,
phraseembeddings
,
sentiment
information
,
socher
,
phrase
embeddings
,
syntactic
knowledge
,
attempt
,
reorderingpattern
,
phrase
embeddings
,
kalchbrennerand
blunsom
,
simple
convolutionmodel
,
phrase
embeddings
,
wordembeddings
,
mikolov
,
aphrase
,
thesemethods
,
phrase
embeddings
,
focus
,
aspect
,
reorderingpattern
,
strong
assumption
,
bag
of
words
,
thesephrase
embeddings
,
phrasal
translation
unit
,
semantic
meaning
,
phrase
,
phrase
embeddings
,
semantic
meaning
,
phrase
embedding
,
thephrase
,
phrase
based
smt
,
phrase
,
meaningful
composition111of
,
internal
word
,
semantic
phrase
embeddings
,
core
ideabehind
,
phrase
,
correct
translationshould
share
,
semantic
meaning
,
semantic
phrase
embeddings
,
non
translationpairs
,
different
semantic
meaning
,
information
,
semantic
phrase
embeddings
,
method
,
phrase
,
unsupervised
algorithm
,
thereconstruction
error
,
socher
,
whilethe
bilingually
constrained
model
,
phrase
embedding
,
semantic
distance
,
translation
equivalentsand
,
semantic
distance
betweennon
translation
pair
,
phrase
,
theleft
,
phrase
,
translations
,
embeddingof
,
phrase
,
gold
representation
,
phraseand
use
,
process
,
englishphrase
,
direction
,
chi
nese
phrase
embedding
,
sameway
,
procedure
,
anco
training
style
algorithm
,
thesemantic
distance
,
translation
equiva
lents1
,
result
,
englishphrase
embeddings
,
semantics
,
transformationfunction
,
semantic
space
,
learned
model
,
semantic
similarity
,
sourcephrase
,
translation
candidate
,
bra
model
,
end
to
end
smt
task
,
phrase
table
pruning
,
decoding
,
phrasal
semantic
similarity
,
translation
candidate
,
thesource
phrase
,
meaning
,
phrasetable
pruning
,
phrasal
,
low
semantic
similarity
,
decodingwith
phrasal
semantic
similarity
,
thesemantic
similarity
,
phrase
,
newfeatures
,
translation
can
1for
simplicity
,
non
translation
,
source
phrase
,
target
phrase
,
pt
figure
,
motivation
,
bra
emodel
,
didate
selection
,
phrase
table
,
withoutsignificant
decrease
,
translation
quality
,
phrasal
semantic
similarity
,
state
of
the
baseline
,
addition
,
semantic
phrase
,
many
potential
application
,
instance
,
semantic
phrase
embeddings
,
process
,
semantic
phrase
embeddings
,
cross
lingual
question
,
monolingual
application
,
textual
entailment
,
question
answering
,
phrase
embedding
,
andmore
attention
,
monolingual
language
,
method
,
phrase
,
bag
of
words
,
convolution
model
,
embeddings
,
collobert
,
kalchbrenner
,
blun
som
,
bag
of
words
,
ble
sensitive
phrase
embeddings
,
theword
order
,
account
,
information
,
bilingually
constrained
recursive
auto
encoders
,
compositionmechanism
,
phrase
,
themodel
training
stage
,
fullinformation
,
phrase
,
internal
word
,
method
,
mikolov
,
phrase
,
meaning
,
asimple
composition
,
meaning
,
individual
word
,
new
york
time
,
phrase
,
phrase
,
indivisible
unit
,
learn
theirembeddings
,
context
information
,
how
112ever
,
phrase
embedding
,
capture
full
semantics
,
context
,
phraseis
,
method
,
small
part
,
phrase
,
phrase
,
contrast
,
semantic
vector
representation
,
phrase
,
third
method
view
,
phrase
,
meaningful
composition
,
internal
word
,
recursive
auto
encoder
,
composition
,
socher
,
socher
,
socher
,
socheret
,
pre
train
therae
,
unsupervised
algorithm
,
label
ofthe
phrase
,
syntactic
category
,
parsing
,
socher
,
polarity
,
sentimentanalysis
,
socher
,
socher
,
reordering
pattern
,
semi
supervised
phrase
,
isin
fact
,
phrase
,
phrase
label
,
phrase
,
li
etal
,
phrase
,
monotone
,
theprepositional
phrase
,
kind
methods
,
semi
supervised
phrase
,
semantic
meaning
,
phrase
,
composition
basedphrase
embedding
,
onthe
semantic
meaning
,
phrase
,
proposea
bilingually
constrained
model
,
semantic
information
,
learn
transformation
,
thesemantic
space
,
language
,
observation
,
therecursive
auto
encoder
,
reasonablecomposition
mechanism
,
phrase
,
semi
supervised
phrase
embedding
,
socher
,
socher
,
li
etal
,
phrase
,
respect
,
second
,
correct
semantic
phraserepresentation
,
gold
label
,
meaning
,
indirect
butfeasible
,
figure
,
recursive
auto
encoder
,
four
word
phrase
,
empty
node
,
reconstructions
,
unsupervisedphrase
embedding
,
semi
supervised
framework
,
network
structure
,
objectivefunction
,
parameter
inference
,
composition
,
wordvector
representation
,
theinput
,
neural
network
,
wordembeddings
,
bengio
,
col
lobert
,
mikolov
,
vocabulary
,
vector
,
phrase
,
ordered
list
,
mwords
,
columnsof
,
matrix
,
vector
representation
,
asimple
multiplication
,
binary
vector
whichis
zero
,
position
,
illustration
,
phrase
w1w2
,
vector
,
learnsthe
vector
representation
,
phrase
,
child
vector
,
bottom
up
manner
,
socher
,
illustratesan
instance
,
a
r
ae
,
binary
tree
,
standard
auto
encoder
,
standard
auto
encoder
aim
,
representation
,
auto
encoder
,
parent
vector
y1as
,
concatenation
,
bias
term
,
auto
encoder
,
representation
,
parent
,
dimensionality
,
parent
,
vector
represents
,
standard
auto
encoder
reconstructs
,
reconstruction
layer
,
parameter
matrix
,
bias
term
,
reconstruction
,
optimal
representation
ofthe
input
,
standard
auto
encoder
try
,
reconstruction
error
,
inputsand
,
reconstructed
one
,
training
,
compute
y2by
,
auto
encoder
,
reused
untilthe
vector
,
whole
phrase
,
unsupervised
phrase
embedding
,
onlyobjective
,
reconstructionerrors
,
vector
,
phrase
,
possible
binary
tree
,
greedy
algorithm
,
optimal
binarytree
,
parameter
,
phrase
,
training
data
,
general
representation
,
thereco
nstr
uctio
n
e
rro
pred
ictio
n
e
rro
,
illustration
,
semi
supervisedrae
unit
,
label
distribution
,
multi
word
phrase
,
several
researcher
,
original
,
semi
supervised
setting
,
induced
phrase
,
polarity
,
sentiment
analysis
,
socher
,
syntactic
category
,
socher
,
semi
supervised
,
phrase
embedding
,
objective
function
,
reconstruction
error
,
theprediction
error
,
hyper
parameter
,
reconstruction
,
prediction
error
,
labelprediction
,
cross
entropy
error
,
usedto
calculate
epred
,
phrase
,
vector
,
spacewill
,
e
mo
delwe
,
semi
supervised
phrase
embedding
,
learned
vector
representation
,
semantic
phrase
,
gold
vector
representations
,
phrase
,
gold
semantic
phrase
embeddingexists
,
thetwo
phrase
,
semantic
representation
,
meaning
,
inference
,
modelcan
learn
,
embedding
,
phrase
,
meaning
,
learned
embeddingmust
,
semantics
,
phrase
,
desire
,
translation
equivalent
share
,
semantic
meaning
,
high
quality
phrasetranslation
pair
,
training
corpus
,
thiswork
,
ce
recons
truction
err
,
sour
ce
prediction
err
,
rg
et
recons
truction
err
,
rg
et
prediction
err
,
source
language
phrase
target
language
phrase
figure
,
illustration
,
bilingual
constrained
recursive
auto
encoders
,
twophrases
,
translation
,
basic
goal
,
semantic
distance
,
phrase
,
translation
,
objective
functionunlike
previous
method
,
bra
model
jointlylearns
,
network
structure
,
source
language
,
fortarget
language
,
kindsof
error
,
reconstruction
errorerec
,
wellthe
,
vector
representation
,
phrase
,
semantic
error
esem
,
thesemantic
distance
,
learned
vector
representations
,
word
embeddings
,
language
,
different
vectorspace
,
phrase
embeddingsin
,
language
,
semantic
vectorspace
,
transformation
,
semantic
distance
,
distancebetween
ptand
,
transformation
,
thatbetween
psand
,
transformation
,
result
,
overall
semantic
error
,
transformation
,
asfollows
,
euclidean
distance
,
sameway
,
hyper
parameter
,
weight
,
reconstructionand
semantic
error
,
final
bra
,
overthe
phrase
pair
,
learned
bra
,
canmake
sure
,
semantic
error
,
positiveexample
,
source
phrase
,
correct
translation
,
negativeexample
,
source
phrase
,
bad
translationt
,
current
model
guaranteethis
,
semantic
error
esem
,
positive
one
,
semantic
error
,
negative
,
corresponding
semantic
margin
error
,
semantic
distance
betweentranslation
equivalent
,
semanticdistance
,
non
translation
pair
,
error
function
,
negative
,
positiveexample
,
suppose
,
correct
translation
,
bad
translation
,
wordsin
,
randomly
chosen
target
language
word
,
theparameters
,
bra
model
,
matrix
,
languages
,
bias
term
,
languages
,
transformation
matrix
wland
bias
,
direction
,
semantic
distance
computation
,
deep
understanding
,
parameter
,
parameter
,
source
language
,
target
language
,
equation
,
source
side
parameter
,
semanticrepresentation
,
target
phrase
,
tocompute
esem
,
similar
forthe
target
side
parameter
,
target
phrase
representation
ptis
,
optimization
,
source
sideparameters
,
algorithm
,
sgd
algorithm
,
problem
,
parameter
initialization
,
partial
gradient
calculation
,
parameter
initialization
,
recand
,
semfor
thesource
language
,
anormal
distribution
,
choice
,
parameter
,
second
,
matrix
lsis
,
bengio
,
collobert
,
mikolov
,
large
scale
unlabeledmonolingual
data
,
second
onesince
,
word
embedding
,
semantics
,
thiswork
,
toolkit
word2vec
,
forthe
source
,
target
language
,
word
embeddings
,
bra
model
tocapture
,
semantics
,
partial
gradient
,
instance
,
source
side
error
,
target
phraserepresentation
,
reconstruction
error
,
semantic
error
,
binarytree
,
greedy
algorithm
,
socher
,
derivative
,
parameter
,
binary
tree
,
back
propagation
,
structure
,
goller
,
kuch
ler
,
parameter
,
target
side
parameter
,
source
side
phraserepresentation
psi
,
paradox
,
sneeds
,
tneeds
,
problem
,
anco
training
style
algorithm
,
threesteps
,
pre
training
,
applying
unsupervised
phraseembedding
,
standard
,
pre
train
thesourceand
target
side
phrase
representation
,
fine
tuning
,
bra
model
,
using
target
side
phrase
representation
,
updatethe
source
side
parameter
,
fine
tuned
source
side
phrase
representation
,
psto
update
,
fine
tuned
,
joint
error
,
corpus
,
termination
check
,
joint
errorreaches
,
local
minimum
,
iteration
,
predefined
number
,
experiments
,
training
procedure
,
otherwise
,
semantic
phrase
embeddings
,
vector
space
transformation
function
,
semantic
similarity
betweena
source
phrase
,
translation
candidate
,
phrase
based
smt
,
inthe
experiment
,
phrase
table
,
discards
entry
,
semantic
similarity
,
phrasal
semantic
similarities
,
additional
new
feature
,
bra
model
includethe
dimensionality
,
balance
weight
,
learning
rate
,
dimensionality
,
learning
rate
,
overall
error
,
bra
model
,
search
procedure
,
phrase
based
translationsystem
,
maximum
entropy
,
reorderingmodel
,
smt
evaluation
,
to
translation
,
bra
emodel
,
thebilingual
training
data
,
entity
pair
,
word
,
gram
language
model
,
xinhua
portion
,
gigaword
corpus
,
en
glish
part
,
bilingual
training
data
,
nis
tmt0
,
development
data
,
news
data
,
thetest
data
,
insensitive
ble
,
evaluation
,
statistical
significance
test
,
re
sampling
approach
,
addition
,
toolkit
word2vec
,
large
scale
monolingualdata
,
aforementioned
data
,
monolingual
data
contains
,
obtain
high
quality
bilingual
phrase
pair
,
bra
model
,
collect
thephrase
pair
,
duplicate
,
phrase
table
,
muchimpact
,
translation
quality
,
importantfor
translation
,
environment
,
time
constraint
,
manyalgorithms
,
thisproblem
,
significance
pruning
,
johnson
etal
,
relevance
pruning
,
entropy
based
pruning2ldc
category
number
,
hese
algorithms
,
corpus
statistic
,
occurrence
statistic
,
phrase
pair
usage
,
composition
information
,
significance
pruning
,
effective
algorithm
,
probability
,
source
phrase
,
phrase
occur
,
bilingual
corpus
,
phrasepair
,
objective
,
corpus
statistic
,
thequality
,
phrase
pair
,
semantic
meaning
,
bra
emodel
,
transforms
,
phrase
,
lowsimilarity
,
phrase
pair
,
similarity
,
directionsare
,
threshold3
,
comparison
result
,
bra
e
based
pruning
method
,
significance
pruning
algorithm
,
commonphenomenon
,
algorithm
,
firstfew
threshold
,
phrase
table
becomes
,
translation
quality
,
certain
threshold
,
significance
pruning
,
bra
e
based
one
,
significance
algorithm
,
phrase
table
,
contrast
,
bra
e
based
algorithm
,
phrase
table
,
threshold0
,
ble
loss
,
overall
evaluation
,
algorithm
,
similarportion
,
phrase
table4
,
bra
and36
,
significance
,
bra
e
based
algorithmoutperforms
,
significance
algorithm
,
ourbrae
model
,
good
alternative
,
phrase
tablepruning
,
situation
,
translation
,
source
phrase
,
semantic
similarity
,
future
,
algorithm
,
portion
,
phrasetable117method
threshold
phrasetable
mt03
mt04
mt05
mt06
mt08
all
baseline
,
comparison
,
bra
e
based
pruning
,
significance
pruning
,
phrase
table
,
thresholdmeans
similarity
,
developmentand
test
set
,
bold
number
,
result
,
baseline
,
dimensionality
,
semanticsimilarity
,
phrasal
semanticsimilaritiesbesides
,
semantic
similarity
,
phrase
table
,
informative
feature
,
phrase
translation
probability
,
translation
hypothesis
,
decoding
,
translation
probabilities
,
phrase
based
smt
,
including
phrase
translation
probability
,
lexicalweights
,
direction
,
phrase
translation
probability
,
occurrence
statistics
,
lexical
weight
,
phrase
asbag
of
words
,
contrast
,
bra
model
focuses
,
compositional
semantics
,
word
tophrases
,
semantic
similarity
,
bra
model
,
translation
probability
,
semantic
similarity
,
intoour
baseline
phrase
based
model
,
influence
,
dimensionality
,
different
settings
,
matter
,
translation
quality
,
overall
test
data
,
largestimprovement
,
dimensionality
growing
,
translation
performance
isnot
,
translation
candidate
,
bad
one
,
semantic
phrase
embeddingto
,
intuition
,
thebrae
model
,
semantic
phrase
embeddings
,
giventhe
bra
model
,
phrase
training
,
wesearch
,
similarenglish
phrase
,
new
input
phrase
,
different
number
ofwords
,
unsupervisedrae
,
syntactic
propertywhen
,
phrase
,
find
,
input
phrase
,
phrase
,
unsupervised
,
syntactic
property
,
contrast
,
ourbrae
model
,
semantic
meaning
foreach
phrase
,
matter
,
proposedbrae
model
,
semanticphrase
embeddings
,
semantic
phrase
,
phrase
,
thephrase
based
smt
,
semantic
phraseembeddings
,
direction
,
future
,
semantic
phrase
embeddings
,
cross
lingual
task
,
cross
lingual
question
answering
,
sincethe
semantic
similarity
,
phrase
,
different
language
,
addition
,
cross
lingual
application
,
bra
model
,
experimental
result
,
phrasal
semantic
similarity
,
dimensionality
,
baseline
,
bra
emilitary
,
force
military
powermain
force
military
strengthlabor
force
,
meetingto
,
meeting
,
meetingat
,
meetinga
meeting
,
conferencedo
,
people
,
nationeach
country
regard
,
citizen
,
countryeach
country
,
people
,
countryeach
,
people
,
similar
phrase
,
training
set
,
new
phrase
,
monolingual
nlp
task
,
goodphrase
representation
,
semantic
similarity
between
phrase
,
entity
recognition
,
parsing
,
textual
entailment
,
question
answeringand
paraphrase
detection
,
phrase
,
meaning
aretranslation
equivalent
,
different
language
,
butare
paraphrase
,
language
,
ourmodel
,
semanticphrase
embeddings
,
paraphrase
,
bra
model
,
limitation
,
recursive
auto
encoder
share
,
weight
matrix
,
bra
emodel
,
semantic
representation
,
ten
ofwords
,
future
workthis
paper
,
bilingually
constrained
recursive
auto
encoders
,
learningphrase
embeddings
,
phraseswith
different
semantic
meaning
,
semantic
distance
betweentranslation
equivalent
,
semanticdistance
,
non
translation
pair
,
learned
model
,
phrase
,
language
,
semantic
space
,
language
,
end
to
end
smt
task
,
thesemantic
phrase
embeddings
,
experimentalresults
,
bra
model
,
phrase
table
pruning
,
decodingwith
phrasal
semantic
similarity
,
many
potential
applications
,
extension
,
bra
model
,
inthe
future
,
direction
,
decoding
process
withdnn
,
semantic
embeddings
,
thebasic
translation
unit
,
learnsemantic
phrase
embeddings
,
bra
model
,
cross
lingual
task
,
tolearn
semantic
sentence
embeddings
,
different
weight
matrix
,
different
node
,
bra
model
,
acknowledgmentswe
,
yang
,
baselinecode
,
anonymous
reviewer
,
valuable
comment
,
research
,
natural
science
foundation
,
and61303181
,
hi
tech
research
,
development
program
,
program
,
galley
,
quirk
,
frey
zweig
,
joint
language
,
recurrent
neural
network
,
proceedings
,
conference
,
empirical
methods
,
natural
language
processing
,
andchristian
jauvin
,
neural
probabilistic
language
model
,
journal
,
yoshua
bengio
,
holger
schwenk
,
jeans
,
ebastiensen
,
luc
gauvain
,
neural
probabilistic
language
model
,
innovations
,
machine
learning
,
ronan
collobert
,
unifiedarchitecture
,
natural
language
processing
,
deepneural
network
,
multitask
learning
,
proceedings
,25
th
international
conference
onm
achine
learning
,
ronan
collobert
,
michaelkarlen
,
koray
kavukcuoglu
,
kuksa
,
natural
language
processing
,
fromscratch
,
journal
,
machine
learning
research
,
e
d
ahl
,
acero
,
context
dependent
pre
trained
deep
neuralnetworks
,
speech
,
languageprocessing
,
duh
,
neubig
,
katsuhito
sudoh
,
ha
jime
tsukada
,
adaptation
data
selection
using
neural
language
model
,
machine
translation
,
annual
meeting
,
association
,
computational
linguistics
,
eck
,
vogal
,
waibel
,
phrase
pair
relevance
,
translationmodel
pruning
,
andli
deng
,
semantic
representationsfor
,
phrase
translation
model
,
arx
iv
preprintarxiv
,
goller
,
kuchler
,
representation
,
structure
,
iee
e
inter
national
conference
,
neural
network
,
volume
,
johnson
,
androland
kuhn
,
translation
,
proceedings
,
emn
lp
,
nal
kalchbrenner
,
blunsom
,
recurrentcontinuous
translation
model
,
proceeding
ofthe
,
conference
,
empirical
method
,
natural
language
processing
,
koray
kavukcuoglu
,
sermanet
,
boureau
,
el
mathieu
,
yann
l
c
un
,
convolutional
feature
hierarchy
forvisual
recognition
,
advance
,
neural
information
processing
system
,
philipp
koehn
,
statistical
significance
test
,
translation
evaluation
,
proceeding
ofe
mnlp
,
krizhevsky
,
ilya
sutskever
,
hinton
,
imagenet
classification
,
deep
convolu
tional
neural
network
,
advance
,
maosong
sun
,
recursive
autoencoders
,
itg
based
translation
,
proceedings
,
conference
,
empirical
method
inn
atural
language
processing
,
ling
,
grac
,
trancoso
,
alanblack
,
entropy
based
pruning
,
phrase
based
machine
translation
,
proceeding
,
the2012
joint
conference
,
empirical
method
inn
atural
language
processing
,
computationalnatural
language
learning
,
lemao
liu
,
taro
watanabe
,
eiichiro
sumita
,
andtiejun
zhao
,
additive
neural
network
forstatistical
machine
translation
,
annual
meeting
,
association
,
computational
linguistics
,
mikolov
,
jancernock
,
sanjeev
khudanpur
,
recurrent
neural
network
,
language
model
,
in
terspeech
,
mikolov
,
ilya
sutskever
,
cor
rado
,
representations
,
phrase
,
composition
ality
,
proceeding
,
d
manning
,
an
y
n
,
continuous
phraserepresentations
,
syntactic
parsing
,
recursiveneural
network
,
proceeding
,
nip
s
2010deep
learning
,
feature
learningworkshop
,
socher
,
pennington
,
h
h
uang
,
y
n
,
d
m
,
semi
supervised
recursive
autoencoders
,
predicting
sentiment
distribution
,
proceeding
,
theconference
,
empirical
method
,
natural
language
processing
,
socher
,
bauer
,
d
manning
,
y
n
,
compositional
vector
grammar
,
proceeding
,
socher
,
perelygin
,
y
w
,
jasonchuang
,
d
manning
,
y
n
,
potts
,
recursive
deep
models
,
semantic
compositionality
,
sentimenttreebank
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
nadi
tomeh
,
cancedda
,
dymetman
,
complexity
based
phrase
table
filtering
forstatistical
machine
translation
,
proceeding
ofs
ummit
xii
,
vaswani
,
yinggong
zhao
,
fossum
,
chiang
,
large
scale
neural
language
model
,
translation
,
proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
pages1387
,
stochastic
inversion
transductiongrammars
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
deyi
xiong
,
qun
liu
,
shouxun
,
maximum
entropy
,
phrase
,
statistical
machine
translation
,
proceeding
,
yang
,
shujie
liu
,
word
alignment
,
context
dependent
deep
neural
network
,
annualmeeting
,
association
,
computational
linguistics
,
zen
,
asystematic
comparison
,
phrase
,
techniques
,
proceeding
,
joint
conference
,
empirical
method
,
natural
languageprocessing
,
computational
natural
languagelearning
,
socher
,
cer
,
christo
pher
d
m
,
bilingual
word
,
phrase
based
machine
translation
,
proceedings
,
conference
,
empirical
methodsin
natural
language
processing
,
annual
meeting
,
association
,
computational
linguistics
,
baltimore
,
association
,
computational
linguisticslearning
topic
representation
,
neural
network
,
lei
cui1
,
dongdong
zhang2
,
shujie
liu2
,
qiming
chen3
,
mu
li2
,
ming
zhou2
,
muyun
yang11school
,
computer
science
,
technology
,
technology
,
chinaleicui
,
cn2microsoft
research
,
dozhang
,
shujliu
,
com3shanghai
jiao
tong
,
chinasimoncqm
gmail
,
utilizes
contextual
informationto
disambiguate
translation
candidate
,
broadertopical
information
,
novel
approachto
,
topic
representation
,
parallel
data
,
neural
network
architecture
,
abundant
topical
context
,
topic
relevant
monolingualdata
,
translation
,
topic
representation
,
topic
relevant
rule
,
distributional
similarity
,
source
,
smt
decoding
,
experimental
results
show
,
method
,
translation
accuracy
,
nis
tchinese
to
translation
task
,
translation
decision
,
systems
,
current
translation
,
context
dependent
information
,
disambiguate
translation
candidate
,
translation
sense
disambiguation
approach
,
carpuat
,
carpuat
,
phrase
based
smt
systems
,
hierarchical
phrase
basedor
syntax
based
smt
system
,
muchwork
,
context
,
rule
selection
,
martonand
resnik
,
lthoughthese
method
,
many
smt
system
,
leverage
,
microsoft
research
,
exploring
broader
information
,
worddriver
,
operator
,
motor
vehicle
,
common
text
,
user
response
,
driver
,
drivermeans
,
meaning
,
topicalcontext
,
relevant
knowledge
,
therefore
,
topic
informationto
learn
smarter
translation
model
,
achievebetter
translation
performance
,
topic
modeling
,
useful
mechanism
,
discovering
,
various
semantic
concepts
,
collection
,
attempts
,
topic
based
translation
,
include
topic
specific
lexicon
translation
model
,
topic
similarity
model
,
synchronous
rule
,
document
level
translationwith
topic
coherence
,
topic
based
approach
,
usedin
domain
adaptation
,
different
topicsas
different
domain
,
typical
property
,
theseapproaches
,
document
boundary
,
toolkits
suchas
latent
dirichlet
allocation
,
gruber
,
document
level
,
situation
,
isconsiderable
amount
,
parallel
data
,
document
boundary
,
addition
,
contemporary
smt
system
,
sentencelevel
,
efficiency
,
the133sentence
level
,
previous
approach
,
real
world
commercial
smt
systems
,
systematicalapproach
,
inferringits
,
novel
approach
,
topic
representation
,
sincethe
information
,
insufficientfor
topic
modeling
,
method
usingcontent
word
,
specific
topic
representation
,
eachsentence
,
neural
network
,
neural
network
,
effective
technique
,
learning
different
level
,
data
representation
,
thelevels
,
neural
network
correspond
todistinct
level
,
concept
,
high
level
representations
,
low
level
bag
of
words
input
,
correlation
amongany
subset
,
input
feature
,
non
lineartransformations
,
superiority
,
effect
,
noisy
word
whichare
irrelevant
,
problem
,
neural
network
framework
,
topic
representations
,
topic
representation
,
translation
knowledge
,
neural
networkbased
approach
,
similarity
,
source
language
,
target
language
,
acompact
topic
space
,
topic
spaceis
,
order
,
share
topic
information
,
sourceand
target
,
large
number
,
training
instance
,
method
,
learned
representation
toeach
bilingual
translation
rule
,
topic
related
rulesare
,
distributional
similaritywith
,
source
text
,
generation
,
smt
decoding
,
topic
similarity
feature
,
log
linear
model
,
evaluatethe
performance
,
nis
t
ch
inese
to
englishtranslation
task
,
experimental
result
,
translationaccuracy
,
deep
learningdeep
learning
,
yearswhich
,
many
machine
learningresearch
area
,
technique
,
public
awareness
,
mid
,
multilayer
feed
forward
neuralnetwork
,
training
procedure
,
layer
wise
unsupervised
pre
training
phase
,
supervised
fine
tuning
phase
,
bengio
,
sparse
coding
,
unsupervised
pre
training
train
,
parameters
,
layer
towards
,
region
,
parameter
space
,
bengio
,
fine
tuning
,
parameter
region
,
deep
learning
,
state
of
the
performance
,
various
research
area
,
resultson
,
imagenet
dataset
,
objective
recognition
,
krizhevsky
,
significant
error
reduction
,
speech
recognition
,
deep
learning
,
variety
,
nlp
task
,
part
of
speech
tagging
,
entity
recognition
,
semantic
role
labeling
,
collobert
,
socher
,
sentimentanalysis
,
socher
,
nlp
research
,
sparsebinary
representation
,
low
dimensional
andreal
valued
representation
,
low
dimensionalrepresentation
,
hugeamount
,
monolingual
text
,
pre
trainingphase
,
fine
tuned
towards
task
specificcriterion
,
previous
successful
research
,
pre
training
phase
,
bilingualsimilarity
,
neuralnetworkin
,
topic
similarity
model
,
detail
,
topic
similarity
,
smt
decoding
procedure
,
figure
,
sketchesthe
high
level
overview
,
overview
,
neural
network
,
topicsimilarity
model
,
topic
representation
,
first
step
,
ir
method
,
relevant
document
toenrich
contextual
information
,
ranking
model
,
tf
idf
weighted
vector
,
bag
of
words
input
,
representation
learning1
,
neural
networktraining
process
,
inthe
pre
training
phase
,
twoneural
network
,
structure
,
different
parameter
,
low
dimensional
representation
,
different
language
,
fine
tuning
phase
,
similarity
,
low
dimensional
representation
,
correlates
,
learned
representation
,
similarity
,
procedure
,
pre
training
phase
,
neuralnetwork
structure
,
high
dimensionalsparse
vector
,
low
dimensional
dense
vector
,
topic
similarity
,
thelearned
dense
vector
,
information
,
bag
of
1we
use
,
n
of
v
vector
,
word
input
,
alleviate
data
,
problem
,
mechanism
,
auto
encoder
,
problem
,
auto
encoder
,
bengio
,
thebasic
building
block
,
deep
learning
,
assuming
,
n
of
v
binary
vector
,
bag
of
words
,
vocabularysize
,
auto
encoder
consists
,
process
,
decoding
process
,
auto
encoder
,
thereconstruction
error
,
low
dimensional
vector
,
original
n
of
v
vector
,
problem
,
auto
encoder
,
distinguish
ment
,
function
word
,
content
word
,
representation
,
auto
encoders
,
function
word
,
problem
,
etal
,
destroyedvector
,
initial
input
,
version
,
global
structure
,
theinput
,
retrieved
relevantdocuments
,
convertit
,
bag
of
words
vector
,
figure
,
withdae
,
applying
masking
noise
,
training
,
et
,
component
,
fromthe
non
corrupted
component
,
word
driver
,
hidden
unit
,
neuralnetworks
,
active
signal
,
process
transformsthe
,
linear
layer
,
nonlinear
layer
,
dimension
,
linear
layer
form
,
n
of
v
vector
,
dimensional
hidden
layer
,
bag
of
words
input
,
subsequent
layerto
,
nonlinear
relation
amongwords
,
output
,
nonlinear
layer
,
figure
,
auto
encoder
,
bag
of
words
input
,
nonlinear
function
,
common
choice
,
sigmoid
function
,
hyperbolic
function
,
hyperbolic
function
,
function
,
function
,
nonlinear
function
due
toits
efficiency
,
glorot
,
process
,
linear
layerand
,
nonlinear
layer
,
similar
network
structures
,
different
parameter
,
loss
function
,
l2
,
difference
,
uncorrupted
input
,
thestandard
back
propagation
algorithm
,
gradient
,
loss
functionis
,
previouslayer
,
parameter
,
neural
networks
involves
many
factor
,
learningrate
,
length
,
hidden
layer
,
optimization
,
parameter
,
fine
tuning
phase
,
layer
ontop
,
low
dimensional
vector
,
similarity
,
source
,
languages
,
similarity
score
,
intothe
standard
log
linear
model
,
translation
decision
,
vector
,
information
,
monolingual
training
data
,
vector
,
bilingual
topic
similarity
,
different
topic
space
,
thisstage
,
parallel
sentence
pair
,
vector
,
different
language
,
fine
tuning
,
latent
topicspace
,
language
,
muchas
,
topic
space
,
smt
decoder
try
,
thesource
text
,
translation
candidate
,
target
language
,
representation
,
figure
,
vector
,
similarity
,
whole
neural
network
,
supervised
criterion
,
parallel
data
,
similarity
,
cosine
similarity
,
vector
,
parallel
sentence
pair
,
similarity
score
,
targetsentence
,
contrastive
estimationmethod
,
eisner
,
positive
instance
,
training
data
,
negative
instance
,
similarity
,
positive
instance
,
negative
instance
,
margin
,
following
pairwise
ranking
loss
,
behindthis
criterion
,
morewe
,
negative
instance
,
negative
instance
,
similar
topicdistributions
,
negative
instance
,
positive
instance
,
criterion
,
different
contentwords
,
fromthe
training
instance
,
network
learning
,
pairwise
,
training
instance
,
standard
back
propagation
algorithmto
,
neural
network
parametersw
,
learned
neural
networks
,
topicrepresentations
,
bilingual
translation
rule
,
smt
decodingwe
,
learned
topic
similarity
,
standard
log
linear
framework
,
thetopic
representation
,
countof
rule
occurrence
,
chiang
,
phrase
pair
,
fractional
count
,
hierarchical
phrasepair
,
topic
representation
,
instance
,
target
side
topicvectors
,
similarity
,
sourcetexts
,
bilingual
translation
rule
,
smt
decoder
,
topic
relevant
translation
candidate
,
topic
irrelevant
candidates
,
translation
model
,
embedded
topic
information
,
similarity
,
topic
representation
,
thesimilarity
,
thesource
to
source
,
source
to
target
similarity
,
topic
sensitivity
estimationsince
general
rule
,
flatter
distribution
whiletopic
specific
rule
,
distribution
,
astandard
entropy
,
sensitivity
,
source
side
,
component
,
vector
,
thetarget
side
sensitivity
,
similar
,
rule
manifest
,
addition
,
traditional
smt
feature
,
topic
related
feature
,
standard
log
linear
framework
,
smt
system
,
besttranslation
candidate
,
argmaxep
,
translation
probability
,
standard
,
standard
feature
function
,
corresponding
feature
weight
,
topic
related
feature
function
,
wkis
thefeature
weight
,
detailed
feature
description
isas
,
standard
feature
,
translation
model
,
including
translation
probability
,
lexical
weightsfor
,
direction
,
gram
languagemodel
,
word
count
,
phrasecount
,
hierarchical
rule
,
topic
related
feature
,
rule
similarity
score
,
rule
sensitivity
score
,
neural
network
,
topic
similarity
model
,
a
c
hinese
to
machine
translation
task
,
neural
network
training
,
large
number
,
monolingual
documents
,
source
,
languages
,
domains
,
weblog
,
and137english
gigaword
corpus
,
version
,
news
domain
,
addition
,
variety
,
topics
,
total
data
statistic
,
format
,
inverted
index
,
lucene2
,
parallel
sentence
pair
,
relevant
documents
,
domainchinese
englishdocs
word
doc
wordsnews
,
statistic
,
monolingual
data
,
numbers
,
refers
,
distributed
framework
,
training
process
,
neural
network
,
thenetwork
,
mini
batch
asynchronousgradient
,
adaptive
learning
rateprocedure
,
adagrad
,
model
replica
,
iteration
,
training
,
model
parameter
,
iteration
,
replica
,
thenext
iteration
,
vocabulary
size
,
inputlayer
,
hidden
layer
,
pre
training
phase
,
allparallel
data
,
neural
network
,
dae
training
,
network
parameters
,
inthe
fine
tuning
phase
,
parallel
sentencepair
,
select
ten
sentence
pairswhich
,
criterion
,
negative
instance
,
training
instance
,
similarity
,
vector
,
smt
training
,
in
house
hierarchicalphrase
based
smt
decoder
,
ourexperiments
,
cky
decoding
algorithm
,
cube
pruning
,
samedefault
parameter
setting
,
chiang
,
parallel
data
,
intotal
,
datasets
,
translation
model
,
parallel
data
,
lucene
,
apache
,
direction
,
diag
grow
final
heuristic
,
symmetricword
alignment
,
in
house
language
modelingtoolkit
,
gram
language
modelwith
,
kneser
ney
smoothing
,
kneser
,
monolingual
data
usedfor
language
modeling
,
table1
,
dataset
,
developmentdata
,
data
consists
,
datasets
,
evaluationmetric
,
overall
translation
quality
,
insensitive
ble
u4
,
papineni
,
ble
score
,
timesof
,
bootstrap
re
sampling
method
,
baselinethe
baseline
,
reimplementation
,
hierosystem
,
chiang
,
parallel
data
,
fix
discount
method
,
phrase
table
smoothing
,
thetranslation
model
size
,
method
,
lda
basedapproach
,
parallel
data
,
document
level
information
,
ir
method
,
simulatethis
approach
,
pld
toolkit
,
topic
distribution
,
lengthof
hidden
layerswe
,
relationship
,
length
,
hidden
layer
,
datasets
,
result
,
inf
igure
,
translation
accuracy
,
achievedwhen
,
setting
,
source
text
,
topic
related
documents
,
topic
representations
,
synchronousrule
selection
,
becomes
,
thetranslation
accuracy
,
moredocuments
,
relevant
information1380
,
bleunist
,
bleunist
,
bleunist
,
bleunist
,
topic
related
feature
,
withdifferent
setting
,
length
,
hidden
layer
,
neural
network
,
many
unrelated
topicwords
hence
,
neural
network
,
important
factor
,
length
,
hidden
layer
,
network
,
deep
learning
,
thisparameter
,
humanefforts
,
figure
,
translation
accuracy
,
obvious
distinction
,
performancewhen
,
equals1
,
translation
accuracy
,
othersettings
,
main
reason
,
parameter
,
neural
network
,
parameter
,
nonlinear
layer
,
network
,
limited
training
data
,
gettingclose
,
global
optimum
,
modelis
,
local
optimum
,
unacceptable
representation
,
new
topic
related
feature
,
log
linear
model
,
compare
,
translation
accuracy
,
dimension
,
topic
representation
,
setting
,
pre
training
phase
,
fine
tuning
phase
,
resultsconfirm
,
topic
information
,
indispensable
forsmt
,
neuralnetwork
,
outperformsthe
baseline
system
,
method
,
ble
point
onaverage
,
baseline
,
source
side
similarity
,
target
sidesimilarity
,
contribution
,
representation
,
neural
network
,
smt
system
disambiguate
translation
candidate
,
furthermore
,
rule
sensitivity
feature
,
smt
performance
,
similarity
features
,
topic
specific
rule
,
alarger
sensitivity
score
,
general
ruleswhen
,
similarity
,
new
features
,
xiao
et
,
ble
point
,
average
,
performanceof
,
settingswith
,
figure
,
notsimply
coincidence
,
theirapproach
,
special
,
neural
network
method
,
parallel
sentence
pair
has139settings
,
averagebaseline
,
src
trg
,
src
trg
,
src
trg
,
src
trg
,
src
trg
,
effectiveness
,
different
feature
,
rule
similarity
feature
,
source
side
target
side
rule
topic
vector
,
similarity
,
sensitivity
,
average
,
setting
,
averaged
result
,
datasets
,
document
level
information
,
willbe
,
training
,
monolingual
data
,
method
,
general
framework
,
previous
lda
,
study
,
method
,
translation
rule
disambiguation
,
thenist
,
dataset
,
figure
,
rescue
after
,
chi
nese
rule
,
deliver
,
baselinesystem
prefers
,
candidates
,
send
message
,
neural
network
,
learned
topic
distribution
,
thesentence
,
topic
related
featuresis
,
topic
consistency
,
worktopic
modeling
,
zhaoand
xing
,
bilingualtopical
admixture
approach
,
word
alignmentand
,
word
pair
,
topic
specific
model
,
extensive
empirical
analysis
,
word
alignment
accuracy
,
translation
quality
,
following
,
topic
specific
lexicon
translation
model
,
hierarchicalphrase
based
translation
model
,
topicinformation
,
synchronous
rule
,
document
level
information
,
translation
performance
butalso
,
previous
lexicon
based
lda
method
,
direction
,
technique
,
domain
adaptation
,
tamet
,
bilingual
lsa
,
latenttopic
distribution
,
different
language
,
one
to
one
topic
correspondence
duringmodel
training
,
bilingualtopic
information
,
language
model
,
lexicon
translation
model
adaptation
,
achieving
significant
improvement
,
relationship
,
out
of
domain
bilingual
data
,
monolingual
data
,
htm
method
,
phrase
topic
distribution
,
translation
model
adaptationand
,
translation
quality
,
vector
spacemodel
,
adaptation
,
genre
resemblance
,
translation
accuracy
,
wealso
,
multi
domain
adaptation
whereexplicit
topic
information
,
domainspecific
model
,
previous
research
,
technique
asl
da
,
novel
neural
network
,
topicrepresentations
,
parallel
data
,
theunitednationschildren
,
theunitednationschildren
,
theunitednationschildren
,
donotnumbertheacknowl
edgmentssection
,
referencesyoshuabengio
,
pascallamblin
,
danpopovici
,
hoffman
,
editor
,
pages153
,
learningdeeparchitecturesforai
,
trendsmach
,
uary
,
davidm
,
marinecarpuatanddekaiwu
,
proceedingsofmachinetrans
lationsummitxi
,
hierarchicalphrase
basedtrans
lation
,
computationallinguistics
,
ronancollobert
,
michaelkarlen
,
koraykavukcuoglu
,
andpavelkuksa
,
naturallanguageprocessing
,
audio
,
speechandlang
,
johnduchi
,
eladhazan
,
andyoramsinger
,
georgefoster
,
rolandkuhn
,
andhowardjohnson
,
as
sociationforcomputationallinguistics
,
amitgruber
,
michalrosen
zvi
,
andyairweiss
,
zhongjunhe
,
qunliu
,
andshouxunlin
,
coling2008
,
coling2008organizingcommittee
,
geoffreye
,
hinton
,
simonosindero
,
andyee
whyeteh
,
afastlearningalgorithmfordeepbeliefnets
,
neuralcomput
,
reinhardkneserandhermannney
,
speech
,
vol
ume1
,
pages181
,
editor
,
proceedingsofemnlp2004
,
pages388
,
barcelona
,
associationforcomputationallinguistics
,
alexkrizhevsky
,
ilyasutskever
,
andgeoffhinton
,
editors
,
pages1106
,
honglaklee
,
alexisbattle
,
rajatraina
,
hoffman
,
editor
,
pages801
,
zhongjunhe
,
yangliu
,
andshouxunlin
,
associationforcomputationallinguistics
,
zhiyuanliu
,
yuzhouzhang
,
edwardy
,
andmaosongsun
,
softwareavailableathttp
,
google
,
com
plda
,
src
trg
,
src
trg
,
src
trg
,
src
trg
,
effectivenessofdifferentfeaturesinbleu
,
denotestherulesimilarityfeatureand
,
denotesrulesensitivityfeature
,
meansutilizingsource
side
,
average
,
paredwithonlyusingsimilarityfeatures
,
preformingsubstantiallybetterthan
,
oneinterestingobservationis
,
theperformanceof
,
isquitesimilartotheset
tingswithn
,
inpreviouslda
basedmethod
,
ifadocumentdoccontainsmsentences
,
deliverx
,
zhaoandxing
,
zhaoandxing
,
follow
ingthiswork
,
tametal
,
suetal
,
exampl
,
dataset
,
normalized
topic
repres
ntations
,
ambiguous
synchronous
rule
,
detail
,
method
,
doc
ment
level
smt
,
restricti
n
,
addition
,
similarity
betweenparallel
sentence
pair
,
smt
decoding
,
document
level
topic
modeling
,
allsentences
,
contribution
,
general
approach
,
topic
information
,
using
ir
method
,
collection
,
relateddocuments
,
document
boundary
,
neural
network
,
topic
representations
,
scalable
modeling
technique
,
bilingual
topic
similarity
,
deep
learning
framework
withthe
help
,
learned
representation
,
future
workin
,
neural
network
basedapproach
,
bilingual
topic
representation
,
context
,
parallel
sentence
pair
,
monolingual
dataand
,
sentences
,
bag
of
words
input
,
neural
network
,
low
dimensional
vector
,
topic
representation
,
synchronous
rule
,
mt
decoding
,
appropriate
rule
,
match
source
text
,
similarity
,
topic
space
,
experimental
result
,
smt
system
,
translation
model
,
significantimprovement
,
state
of
the
hiero
system
,
conventional
lda
,
method
,
future
research
,
neuralnetwork
method
,
document
level
translation
,
topic
transition
,
crucial
problem
,
translation
,
toleverage
recurrent
neural
network
,
thisphenomenon
,
history
translation
information
,
acknowledgmentswe
,
anonymous
reviewer
,
insightful
comment
,
yajuan
duan
,
hong
sunand
duyu
tang
,
helpful
discussion
,
thiswork
,
national
natural
sciencefoundation
,
bengio
,
lamblin
,
popovici
,
andhugo
larochelle
,
layer
wise
training
,
deep
network
,
hoffman
,
editor
,
advance
,
yoshua
bengio
,
deep
architecture
,
uary
,
marine
carpuat
,
inp
roceedings
,
annual
meeting
,
association
,
arbor
,
association
,
computational
linguistics
,
marine
carpuat
,
context
dependent
phrasal
translation
lexicon
,
statisticalmachine
translation
,
proceeding
,
kuhn
,
vector
space
model
,
adaptation
,
statistical
machine
translation
,
proceeding
,
annual
meeting
,
association
,
computationallinguistics
,
volume
,
long
paper
,
association
,
computational
linguistics
,
chiang
,
hierarchical
phrase
based
translation
,
computational
linguistics
,
ronan
collobert
,
michaelkarlen
,
koray
kavukcuoglu
,
kuksa
,
natural
language
processing
,
lei
cui
,
xilun
,
dongdong
zhang
,
shujie
liu
,
ming
zhou
,
multi
domain
adaptation
,
multi
task
learning
,
proceedings
,
conference
,
empirical
methods
,
natural
language
processing
,
seattle
,
association
,
computational
linguistics
,
acero
,
context
dependent
pre
trained
deep
neuralnetworks
,
speech
,
languageprocessing
,
duchi
,
elad
hazan
,
yoram
singer
,
adaptive
subgradient
method
,
kuhn
,
johnson
,
statistical
machinetranslation
,
proceeding
,
conferenceon
empirical
method
,
natural
language
processing
,
association
,
computational
linguistics
,
glorot
,
bordes
,
yoshua
bengio
,
deep
sparse
rectifier
network
,
proceedings
,
international
conference
,
artificial
intelligence
,
amit
gruber
,
rosen
zvi
,
yair
wei
,
hidden
topic
markov
model
,
proceeding
ofa
rtificial
intelligence
,
statistic
,
shouxun
,
improving
statistical
machine
translation
,
lexical
ized
rule
selection
,
proceeding
,
international
conference
,
computational
linguistics
,
coling
,
organizing
,
hinton
,
osindero
,
yee
whyeteh
,
fast
learning
algorithm
,
deep
beliefnets
,
neural
comput
,
kneser
,
ney
,
improved
backing
off
,
m
gram
language
modeling
,
acoustic
,
speech
,
signal
processing
,
philipp
koehn
,
statistical
significance
test
,
translation
evaluation
,
dekang
,
proceeding
,
emn
lp
,
barcelona
,
associationfor
computational
linguistics
,
krizhevsky
,
ilya
sutskever
,
hinton
,
imagenet
classification
,
deep
convolu
tional
neural
network
,
weinberger
,
editors
,
advance
,
honglak
,
battle
,
rajat
,
an
,
hoffman
,
editor
,
advance
,
shouxun
,
maximum
entropy
,
rule
selection
,
syntax
based
statistical
machine
translation
,
inp
roceedings
,
conference
,
empiricalmethods
,
natural
language
processing
,
association
forcomputational
linguistics
,
zhiyuan
liu
,
yuzhou
zhang
,
data
placement
,
intelligent
system
and142technology
,
special
issue
,
large
scale
machinelearning
,
software
,
google
,
com
plda
,
yuval
marton
,
resnik
,
soft
syntacticconstraints
,
hierarchical
phrased
based
translation
,
proceeding
,
columbus
,
association
,
computational
linguistics
,
och
,
minimum
error
rate
training
,
statistical
machine
translation
,
proceedings
,
annual
meeting
,
associationfor
computational
linguistics
,
sap
poro
,
association
,
computationallinguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automaticevaluation
,
machine
translation
,
proceedingsof
,
annual
meeting
,
association
,
computational
linguistics
,
philadelphia
,
association
,
computational
linguistics
,
rumelhart
,
hinton
,
foundationsof
research
,
chapter
,
representationsby
back
propagating
error
,
eisner
,
contrastiveestimation
,
training
log
linear
model
,
unlabeleddata
,
proceeding
,
annual
meeting
,
association
,
arbor
,
michi
gan
,
association
,
computational
linguistics
,
socher
,
manning
,
naturalscenes
,
natural
language
,
recursive
neuralnetworks
,
proceeding
,
internationalconference
,
socher
,
pennington
,
manning
,
semi
supervised
recursive
autoencoders
,
predicting
sentiment
distribution
,
proceeding
,
the2011
conference
,
empirical
method
,
naturallanguage
processing
,
edinburgh
,
association
,
computationallinguistics
,
yidong
,
xiaodong
shi
,
huailin
dong
,
qun
liu
,
translation
model
adaptation
,
statistical
machinetranslation
,
monolingual
topic
information
,
inp
roceedings
,50
th
annual
meeting
,
association
,
computational
linguistics
,
jeju
island
,
association
,
computational
linguistics
,
yik
cheung
,
tanja
schultz
,
bilingual
lsa
based
adaptation
,
statistical
machine
translation
,
machine
translation
,
larochelle
,
yoshua
bengio
,
andpierre
manzagol
,
robust
feature
,
autoen
coders
,
proceeding
,25
th
internationalconference
,
larochelle
,
lajoie
,
yoshua
bengio
,
manzagol
,
autoencoders
,
learninguseful
representation
,
deep
network
,
xinyan
xiao
,
deyi
xiong
,
zhang
,
qun
liu
,
andshouxun
,
topic
similarity
model
forhierarchical
phrase
based
translation
,
proceedings
,50
th
annual
meeting
,
associationfor
computational
linguistics
,
volume
,
long
papers
,
jeju
island
,
association
,
computational
linguistics
,
deyi
xiong
,
zhang
,
topic
based
coherence
model
,
statistical
machine
translation
,
zhang
,
syntax
driven
bracketing
model
,
phrase
based
translation
,
proceeding
,
joint
conference
,47
th
annual
meeting
,
international
joint
conference
,
naturallanguage
processing
,
afn
lp
,
suntec
,
association
forcomputational
linguistics
,
zhao
,
bilingualtopic
admixture
model
,
word
alignment
,
proceedings
,
main
conference
poster
session
,
aus
tralia
,
association
,
computational
linguistics
,
zhao
,
bilingual
topic
exploration
,
word
alignment
,
translation
,
roweis
,
editor
,
advance
,
annual
meeting
,
association
,
computational
linguistics
,
baltimore
,
association
,
computational
linguisticsa
recursive
recurrent
neural
networkfor
statistical
machine
translationshujie
liu1
,
yang2
,
mu
li1and
ming
zhou11microsoft
research
,
beijing
,
china2university
,
science
,
technology
,
chinashujliu
,
v
nayang
,
comabstractin
,
end
to
end
decoding
process
,
statistical
machine
translation
,
acombination
,
recursive
neural
networkand
recurrent
neural
network
,
turnintegrates
,
respective
capability
,
new
information
,
next
hidden
state
,
recurrent
neural
network
,
language
model
andtranslation
model
,
tree
structure
,
asrecursive
neural
network
,
translation
candidate
,
bottomup
manner
,
semi
supervised
training
approach
,
parameters
,
phrase
pair
,
translation
confidence
,
en
glish
translation
task
show
,
proposed
r2n
,
state
of
the
baseline
,
multilayer
neural
network
,
re
gainedmore
,
attention
,
method
,
hinton
,
image
processing
,
results
,
kavukcuoglu
,
krizhevsky
,
representation
,
embedding
,
word
embedding
,
low
dimensional
,
real
valued
vector
,
dimension
,
vector
,
latent
aspect
ofthe
word
,
semanticproperties
,
bengio
,
large
amount
,
monolingual
corpus
,
special
distinct
task
,
collobert
,
proposea
multi
task
learning
framework
,
dnn
forvarious
nlp
task
,
part
of
speech
tagging
,
chunking
,
entity
recognition
,
semantic
role
labelling
,
recurrent
neural
networksare
,
language
model
,
history
information
,
thenetwork
,
long
time
,
mikolov
,
theability
,
output
,
natural
language
parsing
,
socher
,
neuraltensor
network
,
compositional
aspect
,
semantics
,
socher
,
several
componentsor
feature
,
conventional
framework
,
including
word
alignment
,
language
modelling
,
translation
modelling
,
distortion
modelling
,
cd
dnn
hmm
,
method
,
wordalignment
model
,
bilingual
wordembedding
,
lexical
translationinformation
,
utilized
tomodel
context
information
,
joint
language
,
translation
model
,
recurrent
neural
network
,
target
word
,
unbounded
history
,
source
,
target
word
,
additive
neural
network
,
smt
decoding
,
word
embedding
,
confidence
score
,
conventional
log
linear
model
,
distortion
modeling
,
liet
,
use
recursive
auto
encoders
,
entire
merging
phrase
pair
,
boundary
word
,
maximum
entropy
classifier
,
component
,
conventional
smt
framework
,
novel
r2n
,
end
to
end
decoding
process
,
combination
,
recursiveneural
network
,
recurrent
neural
network
,
new
information
,
next
hidden
state
,
recurrent
neural
networks
,
tree
structure
,
recursive
neural
network
,
translationcandidates
,
bottom
up
manner
,
recursive
neural
network
,
tree
structure
,
recursive
neuralnetworks
,
representation
,
child
node
,
difficultto
integrate
additional
global
information
,
aslanguage
model
,
distortion
model
,
order
tointegrate
,
crucial
information
,
translation
prediction
,
recurrent
neural
networks
,
recursive
neural
network
,
global
information
,
nexthidden
state
,
translation
candidate
,
three
step
semi
supervised
training
approach
,
parameter
,
recursive
auto
encodingfor
unsupervised
pre
training
,
derivation
tree
,
forced
decoding
,
global
training
,
early
update
strategy
,
translation
confidence
,
translation
phrase
pair
,
phrase
pair
,
leveragingthe
sparse
feature
,
recurrent
neural
network
,
sparse
feature
,
phrase
pair
,
recurrent
neural
network
,
smoothed
translation
score
,
sourceand
target
side
information
,
experiments
,
a
c
hinese
to
translation
taskto
,
method
,
astate
of
the
baseline
system
,
introduces
related
,
applyingdnn
,
r2n
framework
,
introducedin
detail
,
three
stepsemi
supervised
training
approach
,
method
,
translationconfidence
,
conducted
experiment
,
workyang
,
cd
dnn
hmm
,
word
alignment
,
initial
word
embedding
,
huge
monolingual
corpus
,
theword
embedding
,
context
depended
dnn
hmm
framework
,
lexical
translation
information
,
surrounding
word
modeling
context
information
,
word
alignment
performance
,
word
alignment
result
,
thismodel
,
bring
significant
performance
improvement
,
end
to
end
smt
evaluation
task
,
smt
performance
,
auliet
,
recurrent
neural
networklanguage
model
,
sourceand
target
side
information
,
translationcandidates
,
target
wordembedding
,
network
,
embedding
,
source
word
,
current
target
word
,
thelarge
search
space
,
weak
independenceassumption
,
lattice
algorithm
,
n
best
translation
candidate
,
smt
decoder
,
additive
neural
network
,
mikolov
,
source
,
target
word
embeddings
,
one
hidden
layer
neural
network
,
translationconfidence
score
,
commonly
,
translation
confidence
scoreis
,
conventional
log
linear
model
,
parameter
,
development
data
,
mini
batch
conjugate
sub
gradientmethod
,
distortion
modeling
,
previous
usingboundary
word
,
distortion
modeling
,
smt
decoder
,
apply
recursive
auto
encoder
,
full
use
,
theentire
,
recursive
auto
encoderis
,
representations
,
phrase
pair
,
recursiveauto
encoder
,
representation
,
parent
phrase
pair
,
reordering
confidence
score
,
combination
,
reconstructionerror
,
reordering
error
,
objective
function
,
model
training
,
modelin
,
theend
to
end
smt
decoding
process
,
applying
dnn
,
component
,
conventional
smt
framework
,
combination
,
recursive
neural
network
,
recurrent
neural
network
,
conventional
global
feature
,
input
information
,
combination
,
representation
,
theparent
node
,
future
candidate
generation
,
recurrent
neural
network
,
recursive
neural
network
ins
ection
,
ourr2nn
,
detail
,
forsequence
processing
,
language
model
,
mikolov
,
method
,
n
gram
language
model
,
limited
history
,
prediction
,
previous
state
,
history
,
n
gram
language
model
,
equal
to3
,
history
,
recurrent
neural
network
,
unboundedhistory
information
,
connections
,
hidden
state
,
history
,
network
,
figure
,
recurrent
neural
networkas
,
figure
,
network
containsthree
layer
,
input
layer
,
hidden
layer
,
input
layer
,
concatenation
ofht
,
real
valued
vector
,
history
information
,
embedding
,
withprevious
history
,
current
hidden
layer
,
probability
,
nextword
,
output
layer
,
newhistory
htis
,
future
prediction
,
new
information
,
word
embedding
,
sequential
structure
,
treestructure
,
variousnlp
task
,
parsing
,
smt
decoding
,
tree
structure
,
recursive
neural
networks
,
natural
language
parsing
,
socher
,
recurrent
neuralnetworks
,
recursive
neural
network
,
useunbounded
history
information
,
current
node
,
usedbinary
recursive
neural
network
,
representation
,
parent
node
,
representations
,
child
node
,
figure
,
recursive
neural
networkas
,
figure
,
arethe
representation
,
child
node
,
vector
,
generated
representation
,
confidence
score
ofhow
,
parent
node
,
string
,
nature
language
,
representation
,
parent
node
,
a
n
orv
node
,
representation
,
new
inputinformation
,
recurrent
neural
network
,
eachprediction
,
recursive
neural
network
,
additional
input
information
,
tworepresentation
vector
,
child
node
,
however
,
global
information
,
child
representation
,
crucial1493for
smt
performance
,
language
model
score
,
distortion
model
score
,
global
information
,
abilityto
generate
tree
structure
,
recurrent
neural
network
,
recursive
neural
network
,
recursive
recurrent
,
figure
,
recursivenetwork
,
input
vector
,
themrecurrent
input
vector
,
borrowedfrom
recurrent
neural
network
,
recurrentinput
vector
,
network
,
originalchild
node
representation
,
withparent
node
representation
,
theconfidence
score
,
hidden
,
output
layer
,
concatenation
operator
,
equation1
,
equation
,
nonlinear
function
,
htanh
function
,
definedas
,
r2n
architecture
forsmt
decoding
,
phrase
,
translation
candidate
,
thetranslation
table
,
thephrase
pair
,
recurrent
input
vector
,
global
feature
,
rule
matching
phase
,
translation
candidate
,
black
,
grey
,
matched
translation
candidate
,
conventional
cky
,
process
,
translation
pairsof
child
node
,
translation
candidates
,
parent
node
,
representationsand
plausible
score
,
n
best
translationcandidates
,
upper
combination
,
plausible
score
,
nul
lrule
match
rule
match
rule
matchcoming
,
russiar2nnfigure
,
smt
decodingwe
extract
phrase
,
conventionalmethod
,
translation
score
,
language
model
score
,
distortion
score
,
decoding
,
recurrentinput
vector
,
internal
node
,
difference
,
modeland
,
conventional
log
linear
model
,
conventionalmodel
,
phrase
pair
,
translation
performance
,
conventional
model
,
derivation
,
representation
,
internal
nodes
,
conventional
model
,
decoding
,
recursive
manner
,
one
hidden
layer
neural
network
,
theembedding
,
phrase
pair
,
input
vector
,
representation
,
translation
pair
,
childnodes
,
representation
,
phrase
pair
,
recursive
,
intheir
,
representation
,
distortion
model
,
recursive
neural
network
,
representation
,
child
nodes
,
end
to
endtranslation
process
,
recurrent
global
information
,
phrase
pair
embedding
method
,
translation
confidence
,
following
question
,
initial
representation
oftranslation
pair
,
trainingin
,
three
step
trainingmethod
,
parameter
,
proposedr2nn
,
unsupervised
pre
trainingusing
recursive
auto
encoding
,
derivation
tree
,
forced
decoding
,
global
training
,
early
updatetraining
strategy
,
socher
,
unsupervised
pre
training
,
main
idea
,
auto
encoding
,
parameter
,
neural
network
,
information
lost
,
much
information
,
thehidden
state
,
input
vector
,
figure
,
parts
,
encoder
,
representation
,
encoder
,
therepresentation
,
parentnode
representation
,
input
vector
,
decoder
,
representation
,
childnodes
,
loss
function
,
information
,
euclidean
,
recursive
auto
,
unsupervised
pre
trainingthe
training
sample
,
phrase
pair
,
translation
table
,
continuous
partial
sentence
pair
,
training
data
,
training
,
inthe
future
training
phase
,
tunethe
parameter
,
used
ranking
loss
,
margin
,
oracleisthe
plausible
score
,
oracle
translation
result
,
plausible
score
,
translation
candidate
,
loss
function
,
good
translation
candidate
,
oraclecandidate
,
bad
one
,
forced
decoding
,
wuebker
,
oracle
translation
,
positive
sample
,
forced
,
performs
sentence
pair
segmentation
,
translation
system
,
decoding
,
source
side
,
andany
candidate
,
partial
sub
stringof
,
decodingresult
,
ideal
derivation
tree
,
thedecoder
,
search
space
,
extract
positive
oracletranslation
candidate
,
local
training
,
nodes
sample
,
derivation
tree
,
decoding
,
trained
modeltends
,
local
decision
,
subsection
,
supervised
global
training
,
final
translationperformance
,
rootof
,
decoding
tree
,
propagation
,
tree
structure
,
inexact
search
nature
,
smt
decoding
,
search
errorsmay
,
theoretical
property
,
final
translation
result
,
suitablefor
model
training
,
problem
,
early
update
strategy
,
supervised
global
training
,
early
update
,
usefulfor
smt
training
,
large
scale
feature
,
yu
etal
,
model
usingthe
final
translation
result
,
early
update
,
oracle
translationcandidate
,
n
best
list
,
unrecoverable
mistake
,
back
propagation
,
tree
structure
,
phrase
pair
embeddings
,
leaf
node
,
loss
function
,
supervised
global
trainingis
,
model
score
,
oracle
translation
candidate
,
candidate
,
decoding
,
whole
sourcesentence
,
several
oracle
translationcandidates
,
much
fewer
training
sample
,
supervised
localtraining
,
lossfor
global
training
,
translationcandidates
,
oracle
one
,
translation
candidate
,
training
sample
,
hrase
pair
embeddingthe
next
question
,
phrasepair
,
translation
table
,
togenerate
,
leaf
node
,
derivation
tree
,
phrase
pair
,
mono
lingualwords
,
bilingual
corpus
,
monolingual
corpus
,
embedding
data
entry
parameterword
,
relationship
,
training
data
,
model
parameter
,
thenumbers
,
word
embedding
,
en
glish
giga
word
corpus
version
,
word
pairand
phrase
pair
,
iws
lt
,
dialog
training
,
theword
count
,
phrase
pair
,
relationship
,
sizeof
training
data
,
model
parameters
,
word
embedding
,
training
size
,
vector
,
length
,
parameter
,
parameter
,
butfor
source
target
word
pair
,
corpus
,
training
,
iws
lt
dataset
,
phrase
pair
,
situation
,
thelimitation
,
word
count
,
phrase
pair
,
phrase
,
word
embedding
,
mikolov
,
collobert
,
since
,
enough
training
data
,
simple
approach
,
phrase
pair
embedding
,
average
,
embeddingsof
,
phrase
pair
,
problem
isthat
,
word
embedding
,
translation
relationship
,
source
andtarget
phrase
,
phrase
level
,
phrasescannot
,
meaningof
,
hot
dog
,
composition
,
meanings
,
phrase
pair
,
partsto
model
,
translation
confidence
,
translation
confidence
,
sparse
feature
,
translation
confidence
,
recurrent
neural
network
,
translation
confidence
vector
,
sparse
feature
,
recurrent
neural
network
,
thephrase
pair
,
translation
confidence
,
phrase
pair
,
sparsefeatureslarge
scale
feature
training
,
attentions
,
sparse
,
log
linear
model
,
phrase
pair
,
forthe
,
frequent
translation
pair
,
ofthem
,
special
feature
,
infrequent
one
,
one
hot
representation
vector
,
theinput
,
one
hidden
layer
network
generatesa
confidence
score
,
neural
network
,
confidence
,
conventional
log
linear
model
,
forced
decodingis
,
positive
sample
,
contrastivedivergence
,
model
training
,
neural
network
,
space
dimensionof
sparse
feature
,
hidden
layer
,
network
,
phrase
pair
,
thelength
,
hidden
layer
,
recurrent
neural
network
,
translationconfidencewe
use
recurrent
neural
network
,
twosmoothed
translation
confidence
score
,
onsource
,
target
word
embeddings
,
sourceto
target
translation
confidence
score
,
otheris
target
,
source
,
confidence
scoresare
,
corresponding
target
,
recurrent
network
,
figure
,
recurrent
neuralnetwork
,
bilingual
corpus
,
resultsin
,
ourmethod
,
a
c
hinese
to
translation
task
,
evaluation
method
,
insensitive
ib
m
ble
u
,
papineni
,
bootstrap
re
samplingmethod
,
confidence
level
,
baselinethe
data
,
iws
lt
,
dialog
task
,
training
data
,
sld
btraining
data
,
training
data
,
word
,
en
glish
word
,
language
model
,
gram
language
model
,
target
,
inthe
training
data
,
test
set
,
development
set9
,
development
set
comprises
,
development
set
,
dialog
set
,
training
data
,
monolingual
word
embedding
,
giga
word
corpus
version
,
data
contains
,
embedding
,
top100
,
frequent
word
,
collobert
etal
,
trained
monolingual
word
embedding
,
thebilingual
word
,
iws
lt
bilingual
training
data
,
baseline
decoder
,
in
house
implementation
,
cky
style
,
lexical
reordering
model
,
thebaseline
,
used
feature
,
standardbtg
decoder
,
translation
probability
,
lexical
weight
,
language
model
,
word
penalty
anddistortion
probability
,
commonly
usedfeatures
,
recurrent
input
vector
inour
r2n
,
phrasepair
embeddings
,
word
embeddings
,
source
,
target
word
embeddings
,
large
monolingual
data
,
collobert
,
monolingual
word
embedding
,
initialization
,
bilingual
word
embedding
,
concatenation
operator
,
source
,
target
phrase
,
monolingual
word
embeddings
,
bilingualword
embeddings
,
length
,
wordembedding
,
therefore
,
lengthof
,
phrase
pair
,
phrase
pair
,
methods
,
baseline
system
,
r2n
modelswith
wep
pe
,
tcb
ppe
,
significant
improvement
,
tcb
ppe
,
betterthan
wep
pe
,
translation
result
,
r2n
nmodel
,
phrase
,
method
,
baseline
,
result
,
phrasepair
,
r2n
n
model
,
result
,
translation
confidence
,
phrase
pair
,
ourr2nn
model
,
result
,
baseline
,
translation
relationship
,
word
level
,
phrase
pair
respondent
,
phrasallevel
,
meaning
,
phrase
,
meaning
,
andalso
,
translation
task
,
difference
,
nlp
task
,
translation
confidence
,
confidence
,
onetarget
phrase
,
translation
,
source
phrase
,
tcb
ppe
,
purpose
,
global
recurrent
input
vectorin
order
,
recursive
networkfor
smt
decoding
,
recurrent
inputvector
,
effect
,
resultsare
,
recurrent
inputvectors
,
experimental
result
,
result
,
recurrent
input
vector
,
ble
point
,
development
data
,
leu
point
,
method
drop
,3
bl
eu
point
,
development
,
test
datasets
,
recurrent
input
vector
,
representation
,
recursive
network
,
child
node
,
integrateglobal
information
,
language
model
anddistortion
model
,
recurrent
networkfeaturesto
,
contribution
,
sparse
feature
,
recurrent
network
feature
,
therecurrent
network
feature
,
ourr2nn
model
,
sparse
features
,
contribution
,
recurrent
networkfeatures
,
experimental
result
,
effect
ofsparse
feature
,
recurrent
network
feature
,
result
,
theresults
,
sparse
feature
,
recurrent
network
,
little
bit
,
sparse
feature
,
thetranslation
correspondence
,
translation
candidate
,
whilerecurrent
neural
network
feature
,
future
workin
,
recurrent
neural
network
,
recursive
neural
network
,
integrate
global
input
information
,
combination
,
tree
structure
,
recursive
,
model
tos
mt
decoding
,
three
step
semi
supervised
training
method
,
addition
,
phrase
pair
,
method
,
models
translation
confidence
,
conduct
experiment
,
a
c
hinese
to
translationtask
,
method
,
state
of
the
baseline
,
pairembedding
,
future
,
method
,
translation
equivalent
,
source
,
target
phrase
,
wewill
,
tree
structure
learning
task
,
natural
language
parsing
,
referencesmichael
auli
,
galley
,
quirk
,
frey
zweig
,
joint
language
,
recurrent
neural
network
,
proceedings
,
conference
,
empirical
methods
,
natural
language
processing
,
seattle
,
association
,
computational
linguistics
,
yoshua
bengio
,
holger
schwenk
,
jeans
,
ebastiensen
,
luc
gauvain
,
neural
probabilistic
language
model
,
innovations
,
machine
learning
,
ronan
collobert
,
michaelkarlen
,
koray
kavukcuoglu
,
kuksa
,
natural
language
processing
,
fromscratch
,
journal
,
machine
learning
research
,
e
d
ahl
,
acero
,
context
dependent
pre
trained
deep
neuralnetworks
,
large
vocabulary
speech
recognition
,
speech
,
e
h
inton
,
osindero
,
yee
whyeteh
,
fast
learning
algorithm
,
deep
belief
net
,
neural
computation
,
koray
kavukcuoglu
,
sermanet
,
boureau
,
el
mathieu
,
yann
lec
un
,
convolutional
feature
hierarchy
forvisual
recognition
,
advance
,
neural
informationprocessing
system
,
philipp
koehn
,
statistical
significance
test
,
translation
evaluation
,
proceeding
,
conference
,
empirical
method
,
naturallanguage
processing
,
krizhevsky
,
ilya
sutskever
,
hinton
,
imagenet
classification
,
deep
convolu
tional
neural
network
,
advance
,
maosong
sun
,
recursive
autoencoders
,
translation
,
proceedings
,
conference
,
empirical
methods
,
natural
language
processing
,
seattle
,
association
,
computational
linguistics
,
liang
,
klein
,
taskar
,
end
to
end
discriminative
approach
,
machine
translation
,
proceedings
,
international
conference
,
computational
linguistics
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
lemao
liu
,
taro
watanabe
,
eiichiro
sumita
,
andtiejun
zhao
,
additive
neural
network
,
statistical
machine
translation
,
proceeding
,
the51st
annual
meeting
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
mikolov
,
jancernock
,
sanjeev
khudanpur
,
recurrent
neural
network
,
language
model
,
proceedings
,
annual
conference
,
international
speech
communication
association
,
och
,
ney
,
alignment
template
approach
,
statistical
machine
translation
,
computational
linguistics
,
kishore
papineni
,
roukos
,
wei
jing
zhu
,
method
,
automatic
evaluation
,
machine
translation
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
association
forcomputational
linguistics
,
socher
,
c
l
,
y
n
,
naturalscenes
,
natural
language
,
recursive
neuralnetworks
,
proceeding
,
international
conference
,
bauer
,
d
m
an
ning
,
compositional
vectorgrammars
,
proceeding
,
annual
meeting
,
association
,
computational
linguistics
,
volume
,
stochastic
inversion
transductiongrammars
,
bilingual
parsing
,
parallel
corpus
,
computational
linguistics
,
joern
wuebker
,
mauser
,
ney
,
phrase
translation
model
,
proceeding
,
annualmeeting
,
association
,
computational
linguistics
,
association
,
computational
linguistics
,
deyi
xiong
,
qun
liu
,
shouxun
,
maximum
entropy
,
phrase
,
statistical
machine
translation
,
proceeding
,
the43rd
annual
meeting
,
association
,
computational
linguistics
,
yang
,
shujie
liu
,
nenghaiyu
,
word
alignment
,
context
dependent
deep
neural
network
,
annualmeeting
,
association
,
computational
linguistics
,
zhao
,
violation
perceptron
,
decoding
,
scalable
mt
training
,
proceeding
ofthe
,
conference
,
empirical
method
,
natural
language
processing
,
seattle
,
association
,
computational
linguistics
