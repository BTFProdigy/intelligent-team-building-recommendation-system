new
word
,
differentsemantic
transparency
,
word
segmentationrichard
tzong
han
tsai
,
hsi
chuan
hung
,
computer
science
,
engineering
,
yuan
ze
,
chung
li
,
taoyuan
,
computer
science
,
information
engineering
,
national
,
taipei
,
taiwanthtsai
saturn
,
tw
yabthung
gmail
,
authorabstractthis
paper
exploit
,
text
datato
,
new
word
identification
andchinese
word
segmentation
performance
,
contribution
,
new
word
,
semantic
transparency
,
person
,
location
,
ortransliteration
name
,
association
metric
,
charactersegments
,
unlabeled
data
,
encodethis
information
,
second
,
internal
dictionary
,
usingan
initial
model
,
unlabeled
training
,
test
setto
maintain
,
coverage
,
thetraining
,
test
set
,
comparisonto
,
baseline
model
,
usesn
gram
feature
,
approach
increasesnew
word
,
segmentation
error
,
open
task
,
word
byspaces
,
word
segmentation
,
keystep
,
language
processing
task
,
languages
,
systems
,
supervised
learning
,
data
set
,
data
set
,
manual
annotation
effort
,
unlabeleddata
,
cws
performance
,
animportant
research
goal
,
addition
,
important
because
,
information
,
suchas
new
product
name
,
method
,
extractinginformation
,
newwords
,
categories
,
correlation
ofsemantic
meaning
,
morphemes
,
effective
strategy
,
identification
,
transductive
learning
,
theother
,
association
metric
,
character
sequence
,
character
,
undirected
graphical
model
,
conditional
probability
,
conditional
probability
,
normalization
,
theprobability
,
state
sequence
,
binary
valued
featurefunction
,
weight
,
feature
functions
,
aspect
,
state
transition
,
feature
function
,
statei
,
large
positive
values
,
preference
,
large
negative
value
,
crf
model
,
binary
feature
,
state
transition
,
simplicity
,
following
discussion
,
addition
,
c0
ratherthan
xt
,
current
character
,
baseline
n
gram
featurescharacter
n
gram
feature
,
ml
based
cws
,
unigram
feature
functions
,
bigram
feature
,
conjunction
,
unigram
feature
,
unlabeled
text
,
wordswith
,
low
st
,
disparity
,
morphological
characteristics
,
oursystem
,
strategy
,
twoclasses
,
high
st
word
,
meaning
,
morpheme
,
ssemantic
meaning
correlate
,
tendency
,
baseline
n
gram
model
,
baseline
model
,
segment
asentence
,
high
st
word
,
thistendency
,
inthe
baseline
model
,
zhi
nan
che
,
compass
,
training
set
,
baseline
n
gram
model
,
tendency
,
zhi
nan
,
prefix
,
highst
word
,
compass
,
n
gram
feature
,
someoccurrences
,
high
st
word
,
ambiguity
,
problem
,
sequence
,
character
,
asentence
match
,
difference
,
training
set
,
contains
word
,
unlabeled
testset
,
transductive
dictionary
,
named
because
,
general
conceptsof
transductive
learning
,
tdfeatures
,
recall
,
high
st
word
,
moredetails
,
high
st
word
,
new
word
lack
,
likely
,
baseline
n
gram
model
,
morpheme
,
morphological
tendencies
,
consistent
withthose
,
n
gram
feature
,
instance
,
suppose
,
asindividual
word
,
training
set
,
baseline
model
,
singer
,
low
st
word
,
morphological
tendency
,
recorded
one
,
multi
word
expression
,
connectedcollocations
,
sequence
,
exact
meaning
,
fromthe
meaning
,
connotation
,
component
,
pioneer
,
mwe
identificationmethods
,
association
metric
,
likelihood
ratio
,
dunning
,
method
,
low
st
word
canbe
,
filtering
,
merging
,
theformer
,
likelihood
,
acandidate
,
whole
word
,
cannotbe
,
candidate
,
thanthe
threshold
,
latter
strategy
merges
character
segment
,
nextcandidates
,
merging
,
method
,
main
drawback
,
segment
length
,
inability
,
relationalinformation
,
context
,
thefirst
,
charactersegment
pair
,
values
,
character
,
various
length
,
relationships
,
contextualinformation
,
abundant
inannotated
corpus
,
annotation
data
,
threshold
,
section6
,
encoding
am
,
weakness
,
alanced
transductive
dictionarythe
,
unlabeled
test
set
,
main
problem
,
disparity
,
training
,
coverage
,
training
,
coverage
is100
,
enabled
dictionary
feature
,
high
weight
,
n
gram
features932will
,
low
weight
,
coverage
,
mosttags
,
dictionary
feature
,
iv
word
,
n
gram
feature
,
little
influence
,
result
,
onlyiv
word
,
oov
word
,
coverage
,
training
set
,
degree
,
reliance
,
crf
model
,
corresponding
dictionary
feature
,
dictionary
,
potential
problemof
,
dictionary
,
coverage
,
training
,
coverage
,
test
set
,
latter
,
gold
training
setand
,
test
set
,
unbalanced
coverage
,
training
,
test
set
,
initial
feature
,
thewhole
training
,
test
set
,
next
step
,
training
,
test
set
,
coverageof
,
coverage
,
training
set
,
n
fold
cross
tagging
,
trainingset
data
,
training
set
,
initial
feature
,
thewords
,
cross
tagging
process
arethen
,
difference
betweenthe
ntd
,
ntd
extractswords
,
gold
training
,
td
extracts
word
,
cross
tagged
training
set
,
dictionaryfeatures
,
final
model
,
tdconstructed
,
cross
tagging
training
,
andtagged
test
set
,
balanced
coverageof
,
training
,
test
set
,
sequence
,
character
,
dictionary
,
sequence
,
character
,
traditional
wayis
,
information
,
binary
wordmatch
feature
,
position
,
length
,
newword
match
feature
,
frequency
information
,
original
binary
wordmatch
feature
,
character
,
length
,
considerwords
,
character
,
followingsections
,
dictionary
,
sequence
,
character
,
aword
ind
,
bytheir
position
,
length
,
word
match
feature
,
length
,
previous
character
,
different
word
,
sameposition
,
weight
,
problem
,
matchedwords
,
length
,
conflict
,
wordwith
,
frequency
,
ourwmwf
feature
,
log
feq
,
word
frequency
,
into10
bin
,
logarithmic
scale
,
log
feq
,
discretized
log
frequencyof
,
formulation
,
log
frequency
,
thecorrect
segmentation
,
frequency
,
discrete
feature
,
word
frequencymust
,
commonlyused
discretization
method
,
equal
frequency
interval
,
latter
,
skewed
distribution
,
word
frequency
distribution
,
word
frequency
,
adamic
,
huberman
,
frequency
,
thefrequency
table
,
distribution
,
farfrom
flat
uniform
,
choice
,
followingempirical
distribution
,
underlying
zip
fian
distribution
,
power
law
,
random
variable
denoting
theword
frequency
,
probability
density
function
,
integration
,
equal
number
,
constant
,
logscale
,
strategy
,
log
scaleis
,
hastheoretical
support
,
association
metric
,
theweakness
,
possible
character
segment
pairsbefore
,
segmentation
point
,
association
metric
,
feature
value
,
possible
pair
corresponds
,
individualfeature
,
computational
feasibility
,
pairswith
total
length
,
character
,
segment
pair
,
following
table
,
c0
am2
,
c0a
m1
,
c0c1
am2
,
c0c1c2
am3
,
c0w
use
dunning
,
method
,
dunning
,
assumption
ofnormality
,
comparison
,
significance
,
occurrence
,
common
phenomenon
,
likelihood
ratio
test
,
corpus
,
important
property
,
likelihood
ratio
,
wethen
,
several
bin
,
eachbin
,
significance
level
,
p
value
ofl
,
otherwisesince
,
last
interval
,
implies
,
tendency
,
current
character
,
addition
,
internal
dictionary
,
training
,
test
data
,
external
dictionaries
,
internaldictionaries
,
true
frequency
,
external
dictionary
,
external
dictionary
word
,
n
gramand
calculate
,
frequency
,
log
ngram
freq
,
frequency
,
previous
,
formulation
,
n
grams
withhigher
log
frequency
,
represent
correct
segmentation
,
xperiments
,
evaluation
metricswe
use
,
datasets
,
ba
keoff
,
simplified
,
traditional
,
city
univ
,
text
data
,
experiments
,
cit
yu
dataset
,
cir
b40
corpus1
,
upu
cdataset
,
contemporary
corpus
,
corpus
,
html2http
,
int
raw
,
int
raw
,
ext
raw
,
ext
raw
,
comparison
score
,
cit
yuso
urce
training
,
overview
,
segmentation
result
,
script
,
basic
metric
,
addition
,
italso
,
recall
rate
,
oov
word
,
recall
rate
,
ivwords
,
nchanges
,
emer
son
,
thecws
,
state
of
the
f
measure
,
n
gram
feature
,
baseline
feature
,
features
,
result
,
configuration
,
theresources
,
int
raw
,
internal
raw
data
,
unlabeled
trainingand
test
set
,
wmfeatures
,
config
,
sfeature
,
initial
feature
,
following
order
,
starting
,
baseline
model
,
config
,
tdfeatures
,
config
,
final
setting
,
config
,
closed
task
,
inthe
open
task
,
unlabeled
data
,
thelast
setting
,
external
dictionaries
,
association
metric
,
effect
,
internal
raw
data
,
datasets
,
oov
recall
,
onu
puc
,
cit
yu
,
transductive
dictionary
,
ofgenerality
,
dictionary
feature
,
config
,
int
ables
,
configurationwith
wm
,
outperforms
,
n
grams
,
n
grams
,
satisfactory
oov
recall
,
cit
yu
,
config
,
oov
recall
,
frequency
information
,
effectiveness
,
frequency
,
wmwith
wmw
feature
,
wmw
feature
,
config
,
outperformwm
feature
,
config
,
datasets
interms
,
f
m
easure
,
addition
,
config
,
f
score
,
a
b
td
,
segmentedwords
,
thenew
word
,
baseline
model
,
low
frequency
,
several
bin
,
share
lowweights
,
effect
,
config
,
n
gram
,
initialfeatures
,
wecan
,
increase
,
f
measure
,
state
of
the
artopen
task
performance
,
comparison
,
baseline
n
gram
model
,
significantnumber
,
upu
andcityu
datasets
,
theupuc
,
cit
yu
datasets
,
upu
dataset
,
limitedinformation
,
corpus
,
unlabeled
test
data
,
open
cws
system
,
unlimited
external
resource
,
external
unlabeled
datain
config
,
ngrams
,
asinitial
feature
,
external
unlabeled
data
,
internaldata
,
comparing
,
config
,
f
score
,
configuration
canreduce
nchanges
,
comparison
,
thebest
,
configuration
,
external
dictionariesto
demonstrate
,
external
dictionary
,
wmn
feature
,
externaldictionaries
,
config
,
thisto
,
grammatical
knowledge
base
,
electronic
dictionary
,
cit
yudataset
,
metric
,
config
,
config
,
thisis
,
new
word
,
inexternal
dictionary
,
externaldictionaries
,
result
,
onclusionthis
paper
,
unlabeleddata
,
cws
performance
,
new
high
st
word
,
relevantatomic
part
,
n
gram
model
,
property
,
internal
dictionary
,
model
toextract
word
,
unlabeled
trainingand
test
,
balanced
coverage
onthem
,
weight
,
internaldictionary
feature
,
frequency
,
dictionaryfeatures
,
effectiveness
,
low
st
word
,
mwe
,
baseline
n
gram
model
,
unknown
person
,
location
,
transliteration
name
,
baselinemodel
,
experiment
result
,
strategy
,
traditionaland
,
datasets
,
system
achieves
,
task
performance
,
bakeoff
,
datasets
,
under
,
stringent
constraint
,
task
performance
,
external
unlabeled
data
,
state
of
the
art
open
task
performance
,
bakeoff
,
datasets
,
referencesl
,
adamic
,
huberman
,
internet
,
glottometrics
,
unknown
worddetection
,
corpus
based
learningmethod
,
computational
linguistics
,
choueka
,
needle
,
haystackor
,
interesting
collocation
expression
,
textual
database
,
accurate
method
,
statistics
,
surprise
,
coincidence
,
computationallinguistics
,
ciesielski
,
empirical
investigation
,
impact
,
discretizationon
common
data
distribution
,
design
,
application
,
amsterdam
,
pereira
,
conditional
random
field
,
probabilistic
model
,
sequence
data
,
icm
l
01
,
sproat
,
first
international
word
segmentationbakeoff
,
huihsin
tseng
,
keh
jiann
,
design
ofchinese
morphological
analyzer
,
nianwen
xue
,
libin
shen
,
wordsegmentation
,
lmr
tagging
,
human
behavior
,
principleof
least
effort
,
addison
wesley
