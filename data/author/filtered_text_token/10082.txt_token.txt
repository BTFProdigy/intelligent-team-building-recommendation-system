proceeding
,
conference
,
empirical
method
,
natural
language
processing
,
honolulu
,
association
,
computational
linguisticsquestion
classification
,
head
word
,
hypernymszhiheng
huangeecs
departmentuniversity
,
californiaat
berkeleyca
,
thintintelligent
system
research
telecom
groupchief
technology
officemarcus
,
californiaat
berkeleyca
,
eduabstractquestion
classification
,
important
rolein
question
,
key
toobtain
,
accurate
question
classifier
,
contrast
,
approach
whichmakes
use
,
feature
space
,
effective
feature
,
head
word
feature
andpresent
,
augment
semanticfeatures
,
head
word
,
wordnet
,
addition
,
algorithm
,
depthof
hypernym
feature
,
further
augment
,
standard
feature
,
unigrams
,
linear
svm
,
accuracy
of89
,
standardbenchmark
dataset
,
question
,
dialog
system
,
answer
,
thequestion
,
information
,
search
space
,
correct
answer
string
,
addition
,
information
,
different
strategy
,
candidate
answer
,
instance
,
classification
,
question
,
autism
,
definition
typequestion
,
autism
,
autism
,
combination
,
entity
recognition
,
key
approach
,
modern
question
answering
system
,
voorhees
,
question
classification
,
simply
,
question
wh
words
,
achievesatisfactory
result
,
difficulty
,
question
,
consideringthe
,
capital
,
yugoslavia
,
location
,
ph
scaleis
,
considering
,
liand
,
tourist
attraction
,
inr
eims
,
tourist
attractionsin
reims
,
tourist
,
tourist
,
seeingin
reims
,
reformulations
,
location
,
different
,
syntactic
structure
,
classification
,
many
qa
system
,
setsof
rule
,
question
,
maintain
,
upgrading
,
increasingpopularity
,
statistical
approach
,
machine
learning
,
important
role
,
thistask
,
salient
advantage
,
machine
,
insightfulfeatures
,
process
,
addition
,
classifier
,
new
taxonomy
,
short
time
,
earlier
question
classification
,
languagemodel
,
rappier
rule
,
machine
,
snow
learning
architecture
,
khardon
,
uiu
question
classification
dataset
,
trainingand
,
test
question
,
question
,
datasetare
,
source
,
questions
,
constructed
question
,
rareclasses
,
question
,
question
,
thetest
dataset
,
question
,
dataset
,
coarseand
fine
,
category
,
withcoarse
class
,
fine
classrefinements
,
addition
,
distribution
,
test
question
,
categories
,
lexicalwords
,
speech
tag
,
non
overlappingphrases
,
head
chunk
,
first
noun
chunk
,
aquestion
,
entity
,
hand
built
dictionary
,
theirsystem
,
uiu
dataset
,
platform
,
thefollow
up
research
,
hacioglu
,
usedlinear
support
vector
machine
,
question
wordbigrams
,
error
correcting
output
,
linear
svm
,
possible
question
wordgrams
,
semantic
informationsources
,
entity
,
wordnet
,
class
specific
related
word
,
distributional
similarity
,
category
,
question
classificationtask
,
semantic
feature
,
syntactic
one
,
questions
,
accuracy
of89
,
test
set
,
question
,
fine
class
,
krishnan
,
subsequence
,
questiontokens
,
question
classification
,
theirmodel
,
uiu
cdataset
,
question
category
,
reported
accuracy
,
uiu
dataset
,
contrast
,
approachwhich
,
effective
feature
,
head
word
feature
,
cogcomp
data
qa
qcpresent
,
semantic
features
,
head
word
,
wordnet
,
addition
,
algorithm
,
hypernym
feature
,
augment
,
otherstandard
feature
,
unigrams
,
linear
svm
,
using
,
fine
class
,
lassifiersin
,
classifier
,
support
vector
machine
,
maximum
entropymodel
,
classifier
,
identical
inthe
question
classification
task
,
vapnik
,
data
classification
,
training
setof
instance
labeled
pair
,
solution
,
followingoptimization
problem
,
isubject
,
vector
,
dimensional
space
,
linear
separating
,
maximal
margin
,
dimensionalspace
,
penalty
parameter
,
error
term
,
kernel
function
,
basic
kernels
,
linear
,
radial
basis
function
,
andsigmoid
,
question
classification
context
,
binary
feature
,
instance
,
presence
,
absence
,
particular
word
,
question
,
large
number
offeatures
,
question
classification
,
map
data
,
dimensional
space
,
xit
xi
,
questionclassification
,
implementation
,
experiments
,
berger
,
manning
,
exponential
learning
model
,
providea
general
purpose
machine
,
technique
forclassification
,
prediction
,
natural
language
,
including
part
,
speech
tagging
,
entity
recognition
etc
,
maximum
entropy
model
,
many
heterogeneous
information
source
,
classification
,
constraint
,
context
ofquestion
classification
,
sample
feature
,
thepresence
,
particular
word
,
maximum
entropy
,
maximum
entropy
,
constraint
,
maximum
entropy
,
manning
,
implementation
,
eatureseach
question
,
featuresand
,
classifier
,
training
stage
,
wepresent
,
binary
feature
set
,
question
wh
word
,
head
word
,
wordnet
semantic
feature
,
word
gram
,
word
shape
feature
,
feature
set
,
theclassifiers
,
individual
contribution
,
addition
,
incrementalfashion
,
question
wh
word
ingiven
question
,
wh
word
ofquestion
,
population
,
question
wh
words
,
question
namea
food
,
rest
type
question
,
head
chunk
,
features
,
first
noun
chunk
,
first
verb
chunkafter
,
question
word
,
head
chunk
,
krishnan
,
contiguous
span
,
informer
span
,
bothapproaches
,
noisy
information
,
question
,
agroup
,
head
chunk
,
informer
span
,
question
,
theword
,
tothe
classification
,
type
ent
,
animal
,
hyper
nyms
,
wordnet
,
extra
word
group
,
ambiguity
,
question
intohuman
,
problem
,
head
word
,
singleword
,
object
,
question
seek
,
previous
,
head
word
,
doingso
,
misleading
word
group
,
another
,
bush
,
small
interest
,
baseball
team
,
head
chunk
,
informer
span
,
head
word
,
baseball
team
,
baseball
team
,
extra
wordbaseball
,
head
chunk
,
informer
span
,
question
,
head
chunkor
informer
span
,
head
word
,
headchunk
feature
,
informer
span
feature
,
useful
information
play
,
headword
,
head
word
feature
,
syntactic
parseris
,
syntactic
parser
,
grammatical
structure
,
accurate
parser
,
cha
niak
parser
,
charniak
,
johnson
,
parser
,
manning
,
berkeleyparser
,
petrov
,
parser
,
headword
,
figure
,
parse
tree
forquestions
,
titanic
sink
,
sale
tax
,
rule
,
toparse
tree
,
syntactic
head
word
,
forexample
,
whn
phrase
,
wh
noun
phrase
,
figure
,
wp
child
,
headword
,
bracket
,
wp
tag
,
syntactichead
word
,
whn
phrase
,
head
word
assignment
,
bottom
,
theword
,
head
word
,
wholequestion
,
edu
main
,
html
parsing929minnesotavpvbsinkrootsbarq
,
dt
nnp
,
npn
npsalesthe
tax
,
figure
,
parse
tree
,
head
wordsassignmentsyntactic
head
word
,
bottom
,
head
word
finder
rule
,
semantic
head
word
,
manning
,
semantic
head
finder
rulesto
,
thesemantic
head
word
,
phrase
sba
rq
,
clause
,
conjunction
,
subconstituent
,
sba
rq
,
wh
word
,
verb
phrase
,
declarativesentence
,
subject
aux
inversion
,
head
preference
,
noun
phraserather
,
verb
phrase
,
new
headword
assignment
,
head
word
,
kind
etc
,
post
fix
,
real
head
word
ifnecessary
,
tree
patternas
,
figure
,
pattern
,
question
parse
tree
,
head
word
,
head
word
,
npnode
,
tree
pattern
,
initial
headword
,
parse
tree
,
question
,
proper
name
,
female
walrus
,
assuch
parse
tree
,
right
off
igure
,
compiled
tree
pattern
,
postoperation
,
headword
,
tree
pattern
,
post
fix
,
question
,
animal
,
minnesotavpvbsinkrootsbarq
,
p
nnwh
,
dt
nnp
,
titanicrootsbarq
,
npn
npsalesthe
tax
,
figure
,
parse
tree
,
headword
,
jj
nn
npd
t
jj
nnthe
,
forum
femalefigure
,
post
fix
,
head
word
assignmentin
addition
,
question
head
word
,
describedabove
,
regular
expression
patterns
,
question
head
word
identification
,
pattern
,
question
type
taxonomy
,
question
,
whatare
invertebrate
,
head
word
,
invertebrates
,
question
,
binary
feature
,
string
regular
expression
,
question
,
regular
expression
,
binary
feature
,
placeholder
word
,
implementation
,
forinstance
des
,
feature
set
,
question
,
itis
beneficial
,
classifier
,
training
,
regular
expression
pattern
,
experimentsas
,
def
pattern
,
question
,
def
pattern
,
question
,
substance
,
question
,
andends
,
question
,
question
,
reason
pattern
,
question
,
reason
pattern
,
question
,
andends
,
question
,
question
,
capital
letter
,
pattern
,
asfeature
generator
,
question
,
featurebecomes
,
pattern
,
questions
,
algorithm
,
question
head
,
algorithm
,
type
question
,
asthese
hw
words
,
inclusion
,
noisy
information
,
question
,
head
word
,
pattern
,
question
,
typewhat
,
head
word
,
abovecondition
,
candidate
head
word
,
question
parse
tree
,
redefined
head
finder
rule
,
extracted
head
word
,
noun
phrase
tag
,
first
word
,
noun
phrasetag
,
last
step
,
previous
procedure
,
fellbaum
,
large
lexiconin
,
connectedvia
cognitive
synonym
,
synset
,
wordnet
,
useful
tool
,
word
semantics
analysis
,
hasbeen
,
question
classification
,
krish
,
schlaefer
,
wordnet
,
hypernym
,
pernym
,
question
,
dog
didalgorithm
question
head
word
extractionrequire
,
question
qensure
,
question
head
word1
,
return
null3
,
end
if4
,
end
if7
,
descpattern
,
placehold
word11
,
end
for12
,
end
if13
,
match
hum
,
desc
pattern
then14
,
return
,
string
candidate
head
word
,
question
parse
tree17
,
candidate
,
nn
then18
,
return
candidate19
,
end
if20
,
first
word
,
nnthe
hillbilly
,
knowledgeof
animal
,
hypernym
,
augment
wordnet
semantic
feature
,
hyper
nyms
,
head
word
,
second
making
use
,
a
w
ordnetsimilarity
package
,
structure
,
hypernym
,
hypernymsin
wordnet
,
hierarchieswith
hypernym
relationship
,
natural
,
hypernym
feature
,
theoriginal
head
word
,
hierarchy
,
noun
sense
,
domestic
dog
,
domestic
animal
,
nounsense
,
dull
unattractive
unpleasant
girl
,
unpleasant
woman
,
unpleasant
person
,
addition
,
verb
sense
,
pursue
,
travel
,
first
approach
,
extracted
head
word
,
augment
,
hyper
nyms
,
head
word
,
useful
information
,
head
word
,
question
,
speech
sens
,
many
depth931are
,
generality
,
moreinformative
,
specificity
,
first
question
,
mappingthe
treebank
part
,
speech
tag
,
givenhead
word
,
wordnet
part
,
speech
tag
,
second
question
,
problem
,
lesk
algorithm
,
classical
algorithm
,
assumption
,
context
,
basic
implementation
,
thelesk
algorithm
,
following
,
ambiguous
word
,
acontext2
,
definition
,
dictionary3
,
numberof
common
term
,
definition
,
chosen
wordsin
,
head
word
sense
disambiguation
,
contextwords
,
head
word
,
thequestion
,
dictionary
,
sensefor
,
adaptedlesk
algorithm
,
basically
,
head
word
,
thisalgorithm
head
word
sense
disambiguationrequire
,
question
,
head
word
hensure
,
context
word
,
int
submax
maximum
number
,
common
word
,
sdefinition
,
definition
,
end
for10
,
count
maxcount
then11
,
maxcount
count12
,
optimum
s13
,
end
if14
,
end
for15
,
return
optimumalgorithm
,
maximum
number
,
common
word
,
gloss
ofany
sense
,
context
word
,
headword
,
result
,
maximum
common
word
,
optimal
senseto
augment
hypernym
,
third
question
,
evaluating
randomly
,
trainingdataset
,
identification
,
significant
noisy
information
,
thatthe
use
,
result
overthe
validation
dataset
,
ourhead
word
feature
,
hypernymsintroduction
,
feature
space
,
hypernymsin
,
wordnetsimilarity
package
,
wordnet
hypernym
,
wordnet
similarity
package
model
,
length
,
fromone
word
,
wordnet
network
,
semantic
similarity
,
onthe
path
,
similarity
,
carand
automobile
,
similarity
betweenfilm
,
audience
,
question
,
wordnet
similarity
package
,
thesimilarity
,
head
word
,
questionand
,
description
word
,
question
categorization
,
description
word
,
question
categoryare
,
semantic
meaning
,
question
category3
,
description
word
,
categoryenty
,
disease
,
medicine
,
question
category
,
similarity
,
equal
toa
mini
question
classifier
,
headword
walrus
,
question
,
female
walrus
,
similarity
measure
,
animal
,
description
word
,
category
ent
,
animal
,
animal
,
feature
set
,
question
,
subsequence
,
agiven
question
,
unigram
,
wordsfeature
,
bigram
form
,
trigram
feature
,
thereason
,
word
sense3available
,
cogcomp
data
qa
qc
definition
,
html932disambiguation
,
question
,
long
didrip
winkle
sleep
,
bywh
word
,
head
word
feature
,
either
num
,
period
,
word
feature
ofsleep
help
,
period
classification
,
question
,
useful
forquestion
classification
,
instance
,
questionwho
,
ellington
,
mixed
shape
,
capital
letter
,
entityrecognizer
,
word
shape
feature
,
upper
,
mixed
,
accuracyof
,
classifier
,
first
experiment
,
theindividual
contribution
,
different
feature
type
toquestion
classification
accuracy
,
thesvm
,
training
data
,
following
feature
set
,
wh
word
head
word
,
wordshape
,
wh
word
,
head
word
,
depends
,
head
word
,
thesecond
experiment
,
feature
set
,
parameter
,
bothsvm
,
classifier
,
default
value
,
comparison
,
result
,
thequestion
classification
performance
,
proportion
,
classified
question
,
test
question
,
question
classification
accuracyof
svm
,
individual
feature
set
,
coarse
,
fine
class
,
featuresets
,
wh
word
head
word
,
question
classification
,
first
word
net
semantic
feature
augment
,
inclusion
,
direct
hypernym
,
thefine
class
,
question
classification
accuracy
,
svm
andme
,
individual
feature
set
,
class
overuiuc
dataset6
class
,
classsvm
svm
mewh
word
head
word
,
word
depth
,
hypernym
depth
,
indirect
hypernymunigram
,
phenomenon
,
krishnan
,
wordnet
,
benefit
,
fine
class
classification
,
semantic
feature
,
entities
,
wordnet
,
class
specific
related
word
,
distributional
similarity
,
category
,
theirsystem
,
withthe
help
,
semantic
feature
,
wordnet
didn
,
contribute
,
result
,
wordnet
,
reason
,
thattheir
system
,
hypernym
,
wordin
,
question
,
headword
,
augmentation
,
information
,
notice
,
inclusion
,
various
depth
,
hypernym
result
,
different
accuracy
,
under50
class
,
wordnet
semantic
feature
,
indirect
use
,
hypernym
,
asgood
,
first
approach
,
theaccuracy
gain
,
fine
class
forsvm
,
reason
,
description
word
,
threewords
,
question
category
,
indirect
use
,
hypernym
,
wordnet
similarity
package
,
efficient
asdirect
use
,
hypernym
,
surface
word
feature
,
unigram
feature
perform
,
forsvm
,
svm
under6
class
,
word
shape933feature
,
small
gain
,
question
classification
,
enough
information
,
question
classification
,
toboost
,
good
classifier
,
secondexperiment
,
individual
feature
contribution
,
wethen
,
wh
word
,
head
word
,
direct
hypernym
,
headword
,
word
shape
,
table2
,
question
classification
accuracy
,
fine
class
,
maindifficulty
,
question
classification
,
whattype
question
,
baseline
,
wh
head
word
andhead
word
result
,
fine
class
classification
,
for6
coarse
class
,
incremental
use
,
hypernymfeature
,
slightgain
,
slight
loss
,
coarseclasses
,
unigram
feature
,
gain
,
in50
class
,
word
shape
,
accuracy
increase
,
svm
andme
,
for50
class
,
coarse
class
,
bestaccuracy
,
result
feature
space
,
binary
feature
,
question
,
to30
active
feature
,
feature
sizeof
,
bigram
,
overall
accuracy
,
fine
class
,
theuse
,
trigram
lead
svm
,
inclusion
,
bigram
,
trigram
feature
,
information
,
head
word
,
useful
informationwhich
,
bigram
,
trigram
,
wh
word
,
headword
feature
,
unigram
feature
,
outperforms
bigram
,
trigram
,
isle
sparse
,
addition
,
indirectuse
,
hypernym
,
direct
use
,
hypernym
,
overall
accuracy
,
individual
,
contribution
,
understanding
,
error
distribution
,
respect
,
question
category
,
precision
,
recall
,
result
,
category
,
andenty
,
product
,
others
,
individual
,
precision
,
recall
,
manner
,
reason
,
country
,
mountain
,
distance
,
period
,
classificationaccuracy
,
cdataset
,
snow
accuracy
,
therelated
word
dictionary
,
withthe
,
related
word
dictionary
,
related
word
dictionary
,
otheralgorithms
,
result
,
classifier
,
question
classification
accuracy
,
incremental
feature
set
,
classes6
coarse
classestype
quest
wh
headword
headword
hypernym
unigram
word
,
svm
svm
mewhat
,
fine
classestype
quest
wh
headword
headword
hypernym
unigram
word
,
svm
svm
mewhat
,
question
,
instance
,
question
,
proper
name
,
female
walrus
,
animal
,
itstill
,
error
rate
,
fineclasses
,
reason
,
inherently
ambiguity
,
question
,
forinstance
,
question
,
mad
cow
disease
,
type
des
,
inconsistent
labeling
,
training
dataand
test
data
,
instance
,
population
,
type
num
,
otherwhile
,
population
,
type
num
,
county
,
chicago
,
typeloc
,
county
,
type
loc
,
parser
canproduce
incorrect
parse
tree
,
inwrong
head
word
extraction
,
instance
,
headword
,
speed
hummingbirds
fly
,
hummingbird
,
correct
,
correct
num
,
onclusionin
contrast
,
approach
whichmakes
use
,
feature
space
,
effective
feature
,
head
word
feature
,
classification
accuracy
,
classli
,
linear
svm
,
tree
svm
,
krishnan
,
entropy
model
,
semantic
feature
,
suchhead
word
,
wordnet
,
addition
,
sword
sense
disambiguation
algorithm
,
hypernym
feature
,
optimizedthrough
cross
validation
,
introduceuseful
information
,
augment
,
wh
word
,
unigramfeature
,
word
shape
feature
,
linear
svm
,
usingme
,
fine
class
,
cknowledgmentsthis
research
,
telecomgrant
ct1080028046
,
bis
c
program
,
ucb
erkeley
,
natural
languageprocessing
,
computational
linguistics
,
support
vector
machine
,
software
availableat
http
,
cjlin
libsvm
,
johnson
,
coarse
to
fine
n
b
est
parsing
,
maxent
discriminative
reranking
,
annual
meeting
,
association
,
computational
linguistics
,
head
driven
statistical
model
fornatural
language
parsing
,
phd
thesis
,
ofp
ennsylvania
,
fellbaum
,
electronic
lexical
database
,
mit
press
,
question
classification
,
support
vector
machine
,
error
correcting
code
,
association
,
computational
linguistics
,
human
language
technology
,
semantics
based
answer
pinpointing
,
conference
,
relational
learning
,
linear
threshold
elements
,
conference
,
artificial
intelligence
,
manning
,
unlexical
ized
parsing
,
association
,
computational
linguistics
,
chakrabarti
,
enhanced
answer
type
inference
,
question
usingsequential
model
,
conference
,
human
language
technology
,
empirical
method
,
naturallanguage
processing
,
automatic
sense
,
machine
readable
dictionary
,
design
,
communication
proceedingsof
,5
th
annual
international
conference
,
systemsdocumentation
,
question
classifier
,
international
conference
,
computationallinguistics
,
question
classifier
,
semantic
information
,
natural
languageengineering
,
optimization
,
maxentmodels
,
conditional
estimation
,
tutorial
,
hlt
naa
cl
,
inference
,
a
s
ystemfor
question
answering
,
semistructured
datathe
,
digital
libraries
,
grewal
,
probabilistic
question
,11
thinternational
conference
,
world
wide
,
nyberg
,
semantic
extension
,
theephyra
qa
system
,
intrinsicinformation
content
metric
,
semantic
similarity
inw
ordnet
,
proceeding
,
conference
ofa
rtificial
intelligence
,
nature
,
statistical
learningtheory
,
springer
verlag
new
york
,
ofthe
,
question
answering
track
,
question
,
support
vector
machine
,
acm
ir
conference
,
informaion
retrieval
,
conference
,
empirical
method
,
natural
language
processing
,
afn
lpin
vestigation
,
question
classifier
,
question
answeringzhiheng
huangeecs
departmentuniversity
,
californiaat
berkeleyca
,
thintintelligent
system
research
telecom
groupchief
technology
officemarcus
,
californiaat
berkeleyca
,
accurate
question
classifier
,
toa
question
,
basedquestion
classifier
,
use
ofhead
word
feature
,
wordnet
pernyms
,
question
classifier
,
standard
uiu
questiondataset
,
quantitativelythe
contribution
,
question
,
question
,
accurate
question
,
standard
question
,
features
,
question
,
system
performs
,
significant
attention
,
last
decade
,
prager
,
attempts
,
question
,
naturallanguage
,
answer
phrase
,
important
step
inquestion
,
question
,
answer
,
forexample
,
question
,
information
,
thesearch
space
,
correct
answer
string
,
addition
,
information
,
differentstrategies
,
candidate
answer
,
combination
,
question
,
entity
recognition
,
key
approachin
modern
question
,
voorheesand
dang
,
question
classification
,
simply
,
question
wh
words
,
satisfactory
result
,
difficulty
liesin
,
question
,
capital
,
yu
goslavia
,
location
,
ph
scale
,
withthe
previous
,
krishnan
,
moschitti
etal
,
statisticalquestion
classifier
,
head
word
feature
,
augmentsemantic
feature
,
head
word
,
word
net
,
addition
,
algorithm
,
depth
ofhypernym
feature
,
augment
,
standard
feature
,
unigrams
,
fine
class
,
uiu
dataset
,
addition
,
accurate
questionclassifier
,
contribution
,
thisquestion
classifier
,
question
,
rank
model
,
question
,
rankmodel
,
question
type
information
,
instance
,
question
,
type
ofsport
,
aresport
entity
,
good
use
,
accurate
question
type
information
,
fine
grained
ner
tool
,
ner
package
,
finkel
,
namedentities
,
coarse
,
entityrecognizer
,
important
role
,
determiningthe
performance
,
question
,
following
,
maximum
entropymodel
,
question
classification
,
question
,
ranking
,
question
classification
,
question
classification543accuracy
,
uiu
question
dataset
,
question
answer
feature
,
result
,
question
answer
dataset
,
berger
,
manning
,
exponential
learning
model
,
general
purpose
machine
,
techniquefor
classification
,
prediction
,
natural
language
processing
,
speech
tagging
,
entityrecognition
etc
,
maximum
entropy
model
,
many
heterogeneous
information
source
,
classification
,
constraint
,
givena
,
classlabels
,
datapoints
,
maximal
entropy
model
attempt
,
logexp
,
feature
indicator
function
,
question
classificationand
question
,
question
answercontext
,
function
,
instance
,
thepresence
,
absence
,
dictionary
entity
,
question
,
iarethe
parameter
,
importance
,
machinelearning
approach
,
snow
learningarchitecture
,
uiu
question
classification
dataset1
,
test
question
,
ll
question
,
dataset
,
coarse
,
category
,
shownin
table
,
coarse
class
,
fine
class
,
uiu
dataset
,
platform
,
thefollow
up
research
,
hacioglu
,
question
,
coarse
,
fine
question
,
manner
codeexp
plant
reason
countentity
product
,
dateanimal
religion
group
distancebody
,
individual
moneycolor
substance
title
ordercreative
symbol
desc
othercurrency
technique
loc
perioddis
,
term
city
vehicle
country
,
moschitti
,
approach
whichmakes
use
,
effective
feature
,
features
,
detailed
information
,
question
,
wh
word
feature
,
thequestion
wh
word
,
question
,
wh
word
,
question
,
thepopulation
,
head
word
head
word
,
singleword
,
object
,
questionseeks
,
head
word
,
whatis
,
thisis
,
previous
,
liand
,
krishnan
,
whichhas
,
contiguous
span
,
thenoisy
information
,
non
head
wordof
,
syntactic
parser
,
petrov
,
toextract
head
word
,
wordnet
hypernym
wordnet
hypernym
,
head
word
,
question
,
classic
lesk
algorithm
,
probable
sensefor
,
head
word
,
question
context
,
hypernym
,
hypernym
,
to544six
,
general
term
,
extracted
headword
,
instance
,
head
word
,
question
,
proper
name
,
femalewalrus
,
walrus
,
directhypernyms
,
mammal
,
animal
,
informative
feature
,
animal
,
unigram
word
bag
,
word
feature
,
suchfeatures
,
useful
question
context
information
,
word
shape
five
word
shape
feature
,
allupper
,
mixed
,
alldigits
,
training
question
,
the500
test
question
,
accuracy
of6
coarse
class
,
features
,
question
classification
performance
,
proportion
,
test
question
,
baseline
,
question
classification
accuracy
,
incremental
feature
set
,
class
overuiuc
split
,
head
word
,
hypernym
,
word
shape
,
coarse
,
fine
class
classification
,
incremental
use
,
head
word
,
informativeness
,
suchfeature
,
inclusion
,
hypernym
feature
,
resulting
,
slight
loss
,
coarse
class
,
furtheruse
,
unigram
feature
,
word
shape
,
accuracy
increase
,
best3we
,
cross
validation
experiment
,
training
data
,
various
depth
,
signifies
,
depth
constraint
,
individual
feature
contribution
,
detail
,
feature
ablation
ex
periment4
,
theexperiment
,
head
wordand
,
hypernym
feature
,
essential
rolein
,
accurate
question
classifier
,
question
classification
accuracy
,
classesover
uiu
split
,
classoverall
,
result
feature
space
,
binary
feature
,
question
,
active
feature
,
featuresize
,
featurespace
,
classificationaccuracy
,
question
classifier
,
last
,
addition
,
crossvalidation
experiment
,
uiu
training
corpus
,
result
is89
,
result
,
for6
class
,
moschitti
,
uestion
answer
featuresfor
,
question
,
binary
feature
,
entire
feature
,
snow
accuracy
,
related
worddictionary
,
relatedword
dictionary
,
word
dictionary
,
algorithm
,
result
,
uiu
split
,
uiu
test
data
,
question
classifier
,
dataset
,
classli
,
linear
svm
,
tree
svm
,
krishnan
,
maximum
entropy
model
,
ll
,
entity
presence
featurewe
use
,
finkel
,
ll
style
nes7as
possible
answer
string
,
candidate
,
question
,
thequestion
,
mis
entity
,
candidate
answer
,
ifthe
question
,
ll
per
,
org
entity
,
question
,
ll
loc
,
mis
entity
,
question
,
nocandidate
ll
ne
,
binaryfeature
ne
,
presence
,
absence
,
ll
entity
,
ne
misc
,
presence
,
mis
entities
,
ll
,
entity
,
the101
dictionary
file
,
ephyra
project
,
schlaefer
,
hese
dictionary
file
,
actornames
,
spacey
,
foreach
question
,
head
word
,
question
,
dictionaryfile
,
noun
phrase
,
candidate
sentenceis
,
presence
,
binary
dic
feature
,
forexample
,
question
,
nimitz
reach
,
military
rank
,
head
word
rank
,
nounphrases
,
inthe
military
rank
,
result
,
word
admiral
,
dic
feature
,
present
inthe
military
rank
,
implementation
tip
,
theproximity
match
,
dictionary
look
,
consider
,
question
,
jar
jarbinks
,
question
head
word
film
,
dictionary
namedfilm
,
noun
phrase
,
candidate
sentenceis
,
dictionary
entity
,
candidate
sentence
best
playsjar
jar
binks
,
two
legged
creaturein
,
war
,
episode
,
phantom
menace
,
warsepisode
,
phantom
menace
,
dictionary
,
notice
,
war
,
episode
,
phantommenace
,
dictionary
entitystar
war
episode
,
phantom
menace
,
identical
spelling
,
proximity
look
,
edit
distance
,
countand
num
,
date
etc
,
questionsseek
,
numerical
answer
,
amount
ofmoney
,
duration
,
period
,
regular
expression
pattern
,
suchentities
,
a
n
um
,
money
typedquestion
,
annual
revenue
,
money
regular
expressionpattern
,
string
,
currency
sign
,
candidateanswer
,
candidate
sentence
rohm
,
annual
sale
,
patterns
,
binary
feature
num
,
presence
,
possible
numerical
answer
,
question
dependent
,
question
,
born
,
pattern
jamesdean
,
number
number
,
suchquestion
,
binary
feature
spe
,
presence
,
ques
546tion
,
ourexperiments
,
following
,
born
feature
,
question
,
person
name
,
keyword
,
candidate
sentence
contains
person
name
,
pattern
,
number
number
,
born
feature
,
question
,
person
name
,
keyword
,
candidate
sentence
contains
person
name
,
a
n
um
,
date
entity
,
key
word
,
born
feature
,
question
,
whereis
,
person
name
,
key
word
,
candidate
sentence
containssuch
person
name
,
a
n
loc
entity
,
key
wordborn
,
die
feature
,
question
,
andfollows
,
person
name
,
key
worddie
,
candidate
sentence
contains
person
,
pattern
,
number
number
,
die
feature
,
question
,
andfollows
,
person
name
,
keyword
die
,
candidate
sentence
contains
personname
,
a
n
um
,
date
entity
,
key
word
,
question
,
many
andfollows
,
anumber
,
cooccurrent
feature
,
phrase
arguments
,
question
,
first
phrase
,
construction
,
access
,
extractednamed
entity
,
born
feature
,
pattern
,
information
,
a
n
um
,
date
entityand
,
born
feature
,
pattern
,
information
,
candidate
sentence
containsa
ner
loc
entity
,
pattern
,
born
feature
,
die
feature
havesimilar
structure
,
implementation
,
many
feature
,
passenger
,
question
howmany
passenger
,
amtrak
,
thecooccurrent
feature
,
anexample
,
cooccurrent
feature
,
thearguments
,
husband
,
andwife
,
question
,
husband
,
young
,
airline
pilot
,
argument
,
different
,
questionwhen
,
member
ofthe
world
bank
group
,
reason
,
thecooccurrence
,
verb
role
,
informationthan
word
,
dependencypath
,
question
word
,
common
word
,
question
,
thepath
,
candidate
answer
,
ll
neand
numerical
entity
,
common
word
foreach
pair
,
question
,
candidate
sentence
usingstanford
dependency
parser
,
manning
,
marneffe
,
forquestion
,
die
,
candidatesentence
,
actor
,
two
car
collision
,
cholame
,
pathes
,
advmod
,
and1955
,
prep
in
,
nsubjpass
,
question
andsentence
,
advmod
,
nsubjetc
,
grammatical
relation
,
paired
path
,
question
,
whichall
pair
,
grammatical
relation
,
training
,
dep
feature
,
true
validity
feature
,
path
betweenthe
question
,
possible
,
true
pair
,
candidate
noun
phrase
,
thesentence
path
,
true
answer
,
uestion
answer
experimentsrecall
,
question
answer
featuresdepend
,
question
classifier
,
instance
,
ne
feature
,
presence
,
absence
ofc
onll
style
,
entity
,
quality
,
question
classifier
,
thequestion
,
trainingand
c04
factoid
question
,
comparison
,
others
,
klakow
,
kenlitkowski8
,
training
,
test
datasets
,
apply
key
word
search
,
question
,
data
point
,
pair
ofquestion
,
sentencecan
answer
,
question
,
labelingis
,
gold
factoid
answerpattern
,
extra
step
,
training
set
,
test
data
,
high
quality
training
,
thecorrectness
,
training
data
point
,
removethe
false
positive
one
,
support
thequestion
,
answer
,
addition
,
training
data
,
false
data
point
,
question
,
question
,
nolimit
,
true
label
data
point
,
question
,
trainingdata
point
,
true
label
,
question
,
data
point
,
true
labels
,
training
data
,
maximumentropy
model
,
testdata
set
,
classification
task
,
question
classifier
,
ranking
process
requires
,
extra
step
,
data
point
,
question
,
probability
,
true
label
,
data
point
,
previous
,
top1
prediction
accuracy
,
predictionaccuracy
,
test
data
set
,
amongthe
,
question
,
correct
answer
,
upper
boundof
mrr
score
,
quality
,
question
classifiers
,
question
answering
,
question
classifier
,
trainthese
question
classifier
,
performanceare
,
bestquestion
classifier
,
question
classifier
,
wh
word
head
,
individual
contribution
,
various
feature
,
usingthree
question
classifier
,
baseline
result
,
result
,
dep
feature
,
baseline
,
keyword
search
,
maximum
entropymodel
,
question
,
dic
feature
,
dic
feature
,
question
classifier
,
better
question
classifier
,
considerable
gain
forne
,
contribution
,
question
answering
,
questionclassifier
qc3
,
top5
contribution
,
neand
ne
,
top1
score
result
,
unexpected
result
,
question
,
evenqc2
produce
correct
prediction
,
ne
feature
,
over
confidentscores
,
spe
anddep
,
question
classifier
,
individual
contribution
,
different
question
classifier
,
question
classifier
,
important
feature
,
spe
andreg
,
mrr
scoreover
,
others
,
significantgains
,
various
feature
andthe
result
,
questionclassifier
,
memodel
,
inclusion
,
spe
resultsin
significant
boost
,
question
classifier
qc3
,
result
,
top1
score
,
largeportion
,
num
type
question
,
test
dataset
,
thespe
feature
,
high
precision
,
answeringbirth
death
time
location
question
,
ne
4result
,
reasonable
gain
,
dep
feature
contributes
,
thatdep
,
ahigh
mrr
score
,
question
typeclassifier
,
essential
role
,
incremental
feature
set
,
question
classifier
,
question
answer
system
,
thefeatures
,
question
,
significantly
boost
,
overall
performance
,
question
classifier
qc3
,
top5
score
,
good
question
classifier
qc2
,
thegain
,
top5
score
,
onecan
imagine
,
fine
grained
ner
,
type
coarse
ner
,
thepotential
gain
,
reason
,
question
,
affectsthe
question
,
upstream
source
,
incorrect
classification
,
downstream
answer
search
process
,
forquestion
,
annual
revenue
,
question
classifier
,
money
andthus
,
candidate
answer
,
inferior
question
classifier
,
thereby
,
correct
answer
,
figure
,
individual
mrr
score
,
question
,
test
question
,
test
question
,
accurate
question
classifier
qc3
,
mrr
score
,
klakow
,
idm
rr
qc
,
individual
mrr
score
,
questionswhich
,
question
type
using
qc3
,
training
,
test
datasets
,
systems
,
mrr
scoreof
,
question
answer
,
accurate
question
classifier
,
various
system
performance
comparison
,
system
mrr
top1
top5tanev
,
klakow
,
conclusionin
,
question
classifier
,
efficient
feature
,
question
,
outperforms
previous
question
classifier
,
standard
uiu
question
dataset
,
quality
,
impact
,
question
answer
system
,
accurate
question
classifier
,
essential
rolein
question
,
accuratequestion
classifier
,
standard
question
answer
feature
,
question
,
system
performs
,
acknowledgmentswe
,
anonymous
reviewers
,
invaluable
comment
,
research
,
telecom
grantct1080028046
,
bis
c
program
,
uc
ley
,
maximum
entropy
approach
,
natural
languageprocessing
,
computational
linguistics
,
head
driven
statistical
model
fornatural
language
parsing
,
phd
thesis
,
ofp
ennsylvania
,
trec
questionanswering
,
manning
,
incorporating
nonlocal
information
,
informationextraction
system
,
ofa
cl
,
hacioglu
,
question
classification
,
support
vector
machine
,
error
correcting
code
,
question
classification
,
head
word
,
hypernym
,
inp
roc
,
emn
lp
,
unlexi
calized
parsing
,
pages423
,
chakrabarti
,
enhanced
answer
type
inference
,
question
usingsequential
model
,
automatic
sense
disambiguation
using
machine
readable
dictionary
,
pinecone
,
ice
cream
cone
,
acm
special
interest
group
,
design
,
communication
proceedings
,5
th
annual
international
conference
ons
,
documentation
,
question
classifiers
,
international
conference
,
computational
linguistics
,
question
classifier
,
semantic
information
,
natural
languageengineering
,
optimization
,
maxent
model
,
conditional
estimation
without
magic
,
tutorial
,
hlt
naa
cl
,
typed
dependency
par
,
structure
par
,
shallow
semantic
kernel
,
question
answer
classification
,
inp
roc
,
petrov
,
inference
,
parsing
,
hlt
naa
cl
,
prager
,
open
domain
question
answering
,
foundation
,
information
retrieval
,
nyberg
,
semantic
extension
,
theephyra
qa
system
,
klakow
,
correlationof
dependency
relation
path
,
extraction
,
magnini
,
linguistic
processing
,
mining
forquestion
answering
,
c
,
ofthe
,
question
,
strza
lkowski
,
albany
ilq
ua
int
rec
,
question
,
support
vector
machine
,
acm
irconference
,
information
retrieval
,
conference
,
empirical
method
,
natural
language
processing
,
afn
lpac
,
semantic
class
classifier
,
coreference
resolutionzhiheng
huang1
,
guangping
zeng1
,
weiqun
xu3
,
zhiheng
,
gpzeng
,
asli
eec
,
school
,
information
engineering
,
science
,
technology
beijing
,
china3thinkit
,
acoustic
,
academy
,
science
,
beijing
,
chinaxuweiqun
hccl
,
considerable
attemptsto
incorporate
semantic
knowledge
intocoreference
resolution
system
,
differentknowledge
source
,
wordnet
andwikipedia
,
newways
,
wordnet
feature
,
thisfeature
,
feature
asnamed
entity
feature
,
classifier
,
addition
,
sc
classification
error
,
relaxed
scagreement
feature
,
accurate
sc
classifier
,
relaxation
,
scagreement
feature
,
coreferenceevaluation
,
baseline
systemby
,
muc
score
andanaphor
accuracy
,
whichnoun
phrase
,
pronoun
,
proper
name
,
common
noun
,
entity
indocuments
,
much
,
coreference
resolutionis
,
decision
tree
classifier
,
mention
ascoreferent
,
aim
,
aspect
,
new
feature
,
former
cast
,
pair
wisemention
classification
,
various
form
,
graph
cut
,
nicolae
,
nicolae
,
integer
linear
programming
,
baldridge
,
partition
,
wellner
,
develop
,
new
linguistic
feature
,
problem
,
instance
,
wordnet
,
poesio
,
ponzetto
,
strube
,
semantic
neighbor
word
,
deeper
linguistic
knowledge
,
coreference
resolution
,
higherlevel
,
kehler
,
semantic
knowledge
,
coreference
resolution
system
,
usedto
filter
,
coreference
,
semanticallyincompatible
np
,
difficulty
,
semantic
class
feature
,
inthis
paper
,
wordnet
,
traditional
suchas
,
ponzetto
andstrube
,
semantic
class
features
,
new
,
wordnetand
,
effectiveness
,
semantic
class
,
noun
phrase
,
addition
,
classification
,
ofthe
sc
classifier
,
propose
,
relaxed
scagreement
feature
,
featuresand
standard
syntactic
feature
,
coreference
systems
,
coreference
resolution
system
,
increase
,
muc
score
,
anaphor
accuracy
,
baseline
,
workwordnet
,
fellbaum
,
important
knowledge
source
,
previous
coreference
resolution
,
harabagiu
,
wordnet
relations
,
synonym
,
patterns
,
wordnet
path
,
antecedentsand
anaphor
,
nature
,
rule
basedcoreference
system
,
contrast
,
machine
learning
,
weight
,
relation
,
vieira
,
poesio
,
markert
,
nissim
,
word
net
synonym
,
hyponym
etc
,
ifan
anaphor
,
previousnp
,
ponzetto
,
strube
,
word
net
semantic
similarity
,
relatedness
score
between
antecedent
,
candidate
anaphor
,
their1232work
,
various
relation
,
ponyms
,
meronym
,
useof
hypernym
,
particular
wordnet
relations
,
word
net
hypernym
,
sc
classification
,
sc
compatibility
,
semantic
class
classification
,
impact
,
coreference
resolution
,
proper
name
,
recognizer
,
many
coreference
resolution
system
simply
assign
,
common
noun
,
mostfrequent
,
wordnet
synset
,
markert
,
nissim
,
heuristics
,
good
performance
,
coreference
resolutionsystem
,
anaccurate
sc
classifier
,
heterogeneoussemantic
knowledge
source
,
wordnet
,
several
knowledge
source
,
wordnet
,
features
,
semantic
neighbor
feature
,
ponzetto
,
strube
,
thatthe
wordnet
feature
,
informative
thanthe
community
generated
wikipedia
feature
,
inthis
paper
,
investigation
,
various
usage
,
wordnet
,
sc
classificationtask
,
comparable
toours
,
similar
,
et
,
jointprobabilistic
model
,
coreference
,
md
task
,
mention
,
pronoun
,
person
,
theboundary
detection
,
nounphrases
,
noun
phrase
chunker
andne
recognizer
,
joint
probabilistic
model
models
,
coreference
,
whileour
focus
,
emantic
class
classificationin
,
corpus
,
extract
feature
,
result
,
coreferencecorpus
,
organization
,
location
,
facility
,
rest
noun
phrase
,
classifier
,
sc
information
,
thecoreference
resolution
stage
,
audience
,
person
,
security
industry
,
organization
,
thistask
,
classification
,
person
,
ne
recognizer
,
association
,
audience
,
person
,
semantic
language
source
,
wordnet
,
second
,
thesame
noun
phrase
,
different
context
,
authorities
,
person
,
organization
,
samenoun
phrase
,
sometimes
,
people
,
annotated
asperson
sc
,
testset
,
inconsistent
annotation
,
performanceof
,
sc
classifier
,
causeerrors
,
coreference
stage
,
strict
sc
agreement
featureto
address
,
instance
creationwe
use
phase
c
oreference
corpus
,
sc
classifier
,
noun
phrase
,
noun
phrase
chunker
,
training
instance
,
instanceis
,
semantic
feature
,
np
under
consideration
,
fiveace
sc
,
corpus
,
classification
,
associated
training
instance
,
sc
,
instance
,
corpus
,
training
set
,
test
setwhich
comprise
,
training
,
new
training
,
adevelopment
set
,
former
consists
,
original
training
instances
,
latter
consists
,
instances
,
test
set
,
dataset
,
sc
distributions
,
development
datasets
,
thesame
distribution
,
stratificationprocedure
,
proportion
,
training
,
development
datasets
,
feature
parameter
,
developmentset
,
report
performance
,
developmentset
,
test
set
,
distribution
,
corpus
,
per
org
gpe
fac
loc
oth
train
,
featuresand
,
classifier
,
training
stage
,
wepresent
,
binary
lexical
feature
set
,
bigram
,
n
gram
subsequence
,
nounphrase
,
word
feature
,
bigram
form
,
word
feature
,
andso
forth
,
word
unigram
andbigram
feature
,
last
word
,
last
word
,
firstword
,
last
word
store
,
fromthe
np
,
main
store
,
influence
,
first
word
,
forexample
,
head
word
,
head
word
,
last
word
,
head
word
,
style
rule
,
head
word
,
givennps
,
training
corpus
,
head
word
company
,
organization
,
sparseness
,
trainingdata
,
potential
importance
,
named
entity
,
person
,
location
,
organization
,
sc
classification
,
wordnet
,
large
lexicon
,
related
word
,
cognitive
synonym
,
synset
,
wordnet
,
useful
tool
,
word
semantics
analysis
,
natural
language
,
applications
,
wordnet
,
synset
,
hierarchies
,
hypernym
hyponym
relationship
,
hypernym
,
hyponym
,
wordnet
,
wn
cla
s
feature
,
foreach
keyword
,
right
column
of2it
,
nominal
noun
phrase
andnot
,
pronoun
,
proper
noun
phrase
,
head
noun
,
ponym
,
wordnet
,
becomes
,
thatthese
keywords
,
scsand
,
experimentation
withwordnet
,
sc
,
training
data
,
handcrafted
keywords
,
poor
coverage
,
generalcases
,
result
,
full
use
ofw
ordnet
semantic
knowledge
,
individual
feature
contribution
experimentin
,
keywords
,
wordnet
semantic
feature
,
social
groupfac
establishment
,
construction
,
building
,
facility
,
workplacegpe
country
,
province
,
government
,
administration
,
society
,
island
,
communityloc
dry
land
,
region
,
landmass
,
watergeographical
area
,
wordnet
,
semantic
feature
extraction
,
ponzettoand
strube
,
wordnet
similarity
measure
,
coreference
resolution
,
thedifference
,
coreference
resolution
stage
,
wordnet
similarity
,
antecedentand
anaphor
,
comparison
,
a
w
ordnet
similarity
,
wn
,
np
head
,
key
word
,
wordnetsimilarity
package
,
model
thelength
,
head
word
,
wordnet
network
,
semantic
similarity
,
similarity
,
company
andsocial
group
,
similarity
betweencompany
,
person
,
key
word
,
similarity
,
head
word
,
wn
cla
s
feature
,
thecoverage
problem
,
wn
feature
,
dependent
,
definition
,
inappropriate
forcoreference
resolution
task
,
use
ofw
ordnet
knowledge
,
hypernym
,
np
head
word
,
first
synset
,
similar
workto
,
whichtwo
,
common
synset
,
wordnet
,
allwords
,
hypernym
,
extractedas
feature
,
hypernymsfor
non
head
word
,
introducing
noisy
information
,
hypernym
feature
space
,
a
w
ordnet
hypernym
structure
,
word
company
,
firstsynset
,
institution
,
business
,
unique
id
,
description
word
,
t
third
synset
,
beingwith
someone
,
description
word
,
company
,
companionship
,
society
,
synset
,
itshypernym
synset
,
direct
hyper
nym
,
first
synset
,
synset
,
institution
,
augmentation
,
hypernym
,
nphead
word
,
useful
information
,
butcan
,
head
word
,
synsetof
head
word
,
anoptimal
use
,
wordnet
hypernym
,
questions
,
many
depth
,
generality
,
specificity
,
synset
,
representation
,
id
orsynset
word
,
hypernym
depth
,
question
,
guidelineto
,
optimal
use
,
wordnet
,
theoptimal
configuration
,
wn
hyp
feature
,
companydepth
,1
social
groupinstitution
,
establishmentorganization
,
organisation
,
friendship
,
friendlyrelationshipfellowship
,
society
,
company
,
companionship
,
companyrelationshipfigure
,
wordnet
hypernym
hierarchy
,
theword
company
,
synset
,
as08053576
,
regard
,
entry
word
company
,
berger
,
manning
,
exponential
learning
model
,
sc
classification
task
,
maximum
entropy
model
,
heterogeneous
information
source
,
classification
,
constrainton
,
training
set
,
class
label
,
setof
feature
,
data
point
,
maximumentropy
model
attempt
,
logexp
,
feature
indicator
function
,
parameter
,
memodels
,
sc
classification
,
mention
pairclassification
,
accuracyof
,
classifier
,
individual
contribution
,
different
feature
set
,
sc
classification
accuracy
,
a
m
model
,
instance
,
following
feature
,
variant
,
hw
wn
hyp
,
note
thathw
wn
cla
s
,
semantic
feature
,
semantic
feature
,
wordnet
similarity
measure
,
seco
etal
,
variant
,
hw
wn
hyp
,
thework
,
headword
,
semantic
feature
,
fact
thatwordnet
feature
,
head
wordsand
,
secondexperiment
,
feature
ablation
experiments
,
timefrom
,
entire
feature
set
,
accuracyloss
,
sc
classification
performance
,
proportion
,
correctly
classified
instance
,
test
instance
,
individual
feature
contribution
table
,
showsthe
sc
classification
accuracy
,
non
pronoun
np
,
development
,
test
datasets
,
individual
feature5the
optimal
,
hw
wn
hyp
configuration
,1235
sets
,
lexical
feature
,
usingindividual
feature
set
,
development
,
testace2
datasets
,
feature
type
dev
,
wn
cla
s
,
wn
,
synsetture
,
thedevelopment
dataset
,
bigram
feature
,
usuallyconsist
,
first
last
wordfeature
,
prefix
word
,
head
word
,
high
accuracy
,
head
word
feature
,
thesparsity
,
result
,
non
pronoun
nps
classification
,
np
sc
classification
,
bi
gram
,
ner
performs
,
entity
recognition
task
,
result
,
np
sc
classification
,
inability
,
pronoun
,
common
noun
,
government
,
removal
,
pronoun
significantlyboosts
,
semantic
feature
hw
wn
cla
s
,
headword
,
conforms
,
small
gain
,
using
wn
cla
s
feature
,
hw
wn
feature
,
hw
wn
cla
s
,
theaccuracy
reach
,
variant
,
optimal
depth
,
synset
,
np
headword
,
ratherthan
synset
word
,
hypernym
depth
,
various
depth
,
signifies
,
depthconstraint
,
optimal
depth
,
development
dataset
,
synset
word
,
id
withdepth
,
result
,
optimum
,
id
without
hypernym
depth
information
,
synsetsare
,
hypernym
extraction
,
previous
finding
,
coreference
resolution
system
,
first
wordnetsynset
performs
,
result
reachesthe
accuracy
,
semantic
feature
,
lexical
feature
,
development
dataset
,
gainin
,
test
dataset
,
from72
,
training
,
development
datasets
,
optimal
configuration
,
hw
wn
hyp
semantic
feature
,
lexical
feature
,
theoptimal
hw
wn
hyp
feature
,
combination
,
development
datasets
,
showsthe
sc
classification
accuracy
,
non
pronoun
np
,
training
development
,
hereafter
,
test
datasets
,
usingincremental
feature
set
,
training
,
test
,
wn
hyp
,
first
last
word
,
overfit
ting
problem
,
interesting
evaluation
,
remains
,
test
data
,
inclusion
,
feature
result
,
test
datasetthan
,
training
dataset
,
inclusion
,
the6in
fact
,
test
data
support
,
first
synset
,
allsynsets
result
,
accuracy
increase
,
test
dataset
,
synset
,
synset
id
encoding
,
result
,
increase
,
np
sc
classification
,
test
data
,
effectivenessof
,
wn
hyp
,
sparsityof
head
word
feature
,
bigram
andfirst
last
word
,
reasonable
accuracygain
,
final
inclusion
,
overall
performance
,
fornon
pronoun
np
,
test
data
,
result
,
sc
classification
accuracy
,
non
pronoun
np
,
large
difference
,
trainingaccuracies
,
classifier
,
training
dataset
,
scclassifier
,
bbn
entity
type
corpus
,
weischedel
,
brunstein
,
corpus
,
wordnet
,
multipleknowledge
source
,
reuters
corpus
,
dependency
,
thesaurus
,
sc
classifier
,
wordnet
hyper
nym
,
ne
feature
,
thesmall
accuracy
gain
,
considering
,
test
data
size
,
feature
ablation
experiment
,
feature
ablation
experiment
,
importance
,
individual
feature
,
entire
featureset
,
sc
classification
,
non
pronoun
np
,
training
,
test
datasets
,
training
,
testace2
datasets
,
re
7all
np
accuracy
,
thepronouns
,
training
,
test
data
,
corpus
,
pattern
,
sc
ofcommon
noun
,
ne
feature
result
,
accuracyloss
,
nounson
test
data
,
wn
hyp
,
hw
wn
hyp
asone
feature
,
removal
,
result
,
accuracyloss
,
test
data
,
first
last
word
,
head
word
,
reason
,
removalof
ne
result
,
much
significant
loss
,
due
tothe
fact
,
ne
feature
,
different
fromother
feature
,
strength
,
sc
forproper
name
,
target
,
common
noun
,
proposed
use
,
hw
wn
hyp
,
gainon
top
,
informative
lexical
feature
,
first
last
word
,
scclassifier
,
second
probable
label
isvery
,
actual
label
,
first
probable
one
,
classifierto
,
probable
label
,
classification
,
actual
label
isone
,
prediction
,
classificationaccuracy
,
noun
phrase
,
annotated
class
,
sometimes
,
forthe
,
people
,
personsc
,
instance
,
classifier
,
othersemantic
class
,
annotation
inconsistency
issue
,
sc
agreement
,
strict
match
,
coreference
resolution
feature
,
first
probablesc
,
second
probable
sc
,
coreference
resolutionwe
,
coreference
resolution
system
,
thissection
,
wordnet
hypernym
features
help
,
coreference
resolution
performance
,
coreference
corpus
,
raw
text
,
corpus
,
pipeline
,
nlp
components
,
text
chunking
,
statistic
ofcorpus
,
mention
extraction
,
table6
,
g
mention
,
extractedmentions
,
mentions
,
recall
,
gold
mention
,
training
,
test
data
,
statistic
,
corpus
,
mentions
,
text
mention
g
mention
gold
recall
,
coreference
system
,
maximum
entropy
model
,
np
arecoreferent
,
ponzetto
andstrube
,
instance
,
positive
instance
,
antecedent
,
negative
instance
,
npjpaired
,
instance
,
semantic
feature
,
follows
,
training
data
,
maximum
entropy
model
,
test
stage
,
corefer
ent
,
antecedent
,
suchnp
,
antecedent
,
natural
language
processing
,
information
extraction
,
factoevaluation
metric
,
open
question
whichevaluation
,
suitable
one
,
evaluation
,
extracted
mention
,
contrast
,
goldmentions
,
comparisonwith
previous
,
metric
,
commonly
used
muc
scorer
,
vilain
,
anaphoric
reference
,
ponzetto
,
anaphoric
reference
,
antecedent
,
thesame
coreference
chain
,
resulting
partition
,
baseline
feature
,
inthis
paper
,
detailed
informationand
implementation
,
versley
,
thealias
feature
,
thevalue
,
true
mean
,
antecedent
,
theanaphor
refer
,
entity
,
person
,
organization
,
location
,
feature
detection
,
yearvalues
,
person
,
last
word
,
noun
phrase
,
organization
name
,
alias
detection
checksfor
acronym
match
,
internationalbusiness
machine
,
lexical
,
spelling
,
removing
article
,
demonstrative
pronoun
,
grammatical
,
i
pronoun
,
true
ifn
piis
,
pronoun
,
ronoun
,
npjisreflexive
pronoun
,
pronoun
,
trueif
npjis
proper
noun
,
npjstarts
,
npjstarts
withthis
,
nominal
,
true
ifn
piand
npjagree
,
trueif
npiand
npjagree
,
npiand
npjare
apposition
,
many
sentences
npiand
npjare
,
following
semantic
class
,
female
,
person
,
organization
,
location
,
object
,
semantic
class
,
ordnet
synset
,
semantic
class
determination
module
,
semantic
class
,
thefirst
synset
,
head
noun
,
hyponym
,
semantic
class
,
semantic
class
,
unknown
class
,
theagreement
,
semantic
class
,
npiand
npjisunknown
,
assigned
class
,
assigned
class
,
false
otherwise
,
notice
,
wordnet
use
,
principle
,
sc
classification
whilethe
latter
,
coreference
resolution
,
different
semantic
class
categories
,
wordnet
agreement
,
instance
,
npiand
npj
,
sc
classifier
,1238
bels
,
sc
agreement
feature
,
npiand
npj
,
addition
,
sc
agreement
feature
,
thesc
classification
error
,
rel
ax1
,
thefirst
probable
,
second
probable
,
second
probableof
npi
,
thesecond
probable
label
,
purpose
inusing
sc
rel
ax1
,
sc
rel
ax2
,
strict
sc
agreement
feature
,
hopethat
partial
sc
match
,
coreference
resolution
,
muc
score
,
corpusand
,
partition
,
npaper
,
baseline
,
semantic
feature
,
baseline
,
sem
class
,
sc
rel
ax2
,
intothe
sc
strict
feature
set
,
sc
strict
,
muc
score
,
anaphor
accuracy
,
baseline
from57
,
new
use
,
wordnet
canobtain
significant
gain
,
muc
scoreand
anaphor
accuracy
,
precision
,
allnps
show
,
sc
strict
feature
,
semantic
incompatible
pairsof
antecedent
,
anaphor
,
accordance
withour
hypothesis
,
relaxation
,
strict
sc
agreement
,
performancefurther
,
muc
score
andanaphor
accuracy
,
sc
agreement
feature
result
,
maximal
accuracy
,
sc
rel
ax1
results
,
maximal
muc
score
gain
,
muc
score
,
muc
score
,
accuracyof
anaphor
,
thanthe
accuracy
,
extensive
experiment
,
different
machine
learning
algorithm
,
alternative
use
,
eitherconstraint
,
normal
feature
,
heterogeneousknowledge
source
,
employswordnet
,
ner
semantic
source
,
different
muc
,
accuracy
score
,
nontrivial
,
coreference
systems
,
discussion
,
whichevaluation
,
showing
,
sc
classifier
,
significant
boost
,
baseline
,
accuracy
metric
,
onclusionwe
,
traditional
use
,
word
net
,
coreference
resolution
,
wordnet
semantic
knowledge
,
proposed
new
,
wordnet
feature
,
thisfeature
,
namedentity
feature
,
classifier
,
addition
,
classification
error
,
sc
classifierand
,
sc
agreement
feature
,
withpart
,
classification
error
,
accurate
sc
classifier
,
relaxation
,
sc
agreement
feature
,
baseline
coreferenceresolution
system
,
muc
score
,
anaphor
accuracy
,
acknowledgmentswe
,
yannick
versley
,
support
,
bar
coreference
resolution
system
,
anonymous
reviewer
,
invaluablecomments
,
research
,
britishtelecom
ct1080028046
,
bis
c
pr
,
uc
,
maximum
entropy
approach
,
natural
languageprocessing
,
computational
linguistics
,
head
driven
statistical
model
fornatural
language
parsing
,
phd
thesis
,
ofp
ennsylvania
,
large
scaleexploration
,
effective
global
feature
,
jointentity
detection
,
baseline
,
sc
agreement
feature
,
strict
,
baldridge
,
joint
determinationof
anaphoricity
,
coreference
resolution
,
integer
programming
,
hlt
naa
cl
,
fellbaum
,
electronic
lexical
database
,
mit
press
,
manning
,
incorporating
nonlocal
information
,
informationextraction
system
,
ofa
cl
,
zitouni
,
complex
model
,
study
inmention
detection
,
bunescu
,
knowledge
mining
,
coreferenceresolution
,
naa
cl
,
utility
,
predicate
argument
frequenciesfor
pronoun
interpretation
,
automatic
retrieval
,
clusteringof
similar
word
,
collocation
statistic
,
information
extraction
,
mention
synchronous
coreferenceresolution
algorithm
,
tree
,
manning
,
optimization
,
maxent
model
,
conditional
estimation
without
magic
,
tutorial
,
hlt
naa
cl
,
markert
,
nissim
,
knowledge
source
,
nominal
anaphora
resolution
,
computational
linguistics
,
mcc
allum
,
wellner
,
conditional
models
,
identity
uncertainty
,
application
,
nouncoreference
,
semantic
class
induction
,
corefer
ence
resolution
,
semantics
,
coreferenceresolution
,
ijc
ai
,
nicolae
,
nicolae
,
coreference
resolution
,
ofthe
emn
lp
,
hitzeman
,
bridging
reference
,
inp
roc
,
strube
,
semantic
role
labeling
,
wordnet
,
wikipedia
forcoreference
resolution
,
intrinsicinformation
content
metric
,
semantic
similarityin
wordnet
,
conference
ofa
rtificial
intelligence
,
resolution
,
nounphrases
,
computation
linguistics
,
moschitti
,
modular
toolkit
,
system
demo
,
vieira
,
poesio
,
empirically
basedsystem
,
definite
description
,
computational
linguistics
,
model
theoretic
coreferencescoreing
scheme
,
weischedel
,
brunstein
,
entity
type
corpus
,
linguistic
dataconsortium
,
coreference
resolution
using
semantic
relatedness
information
,
patten
,47
th
annual
meeting
,
ijc
nlp
,
afn
lp
,
suntec
,
afn
lpa
graph
based
semi
supervised
learning
,
question
answeringasli
celikyilmazeecs
departmentuniversity
,
jacksonville
,
graph
based
semi
supervisedlearning
,
using
textual
entailment
analysis
,
natural
language
question
,
searchengine
,
high
level
attribute
,
entailment
problem
,
question
type
named
entitymatching
,
utilization
,
unlabeled
datapoints
,
answer
rankingtask
,
labeledand
,
match
scores
oftextual
entailment
feature
,
similarityweights
,
data
point
,
summarization
method
,
computation
,
largedatasets
,
new
representation
ofgraph
based
ssl
,
qa
datasets
,
handful
,
limited
amount
,
generalization
,
process
,
answers
,
question
,
collection
,
textfiles
,
intensive
research
,
evaluation
based
conference
,
research
,
factoid
question
,
answer
,
short
string
,
entity
,
typical
qa
system
,
pipeline
,
extraction
,
candidate
sentencesto
,
true
answer
,
performance
many
research
focuson
different
structure
,
question
processing
,
information
retrieval
,
information
extraction
,
sag
gion
,
gaizauskas
,
harabagiu
,
answer
extraction
,
qa
system
,
similar
pipeline
structure
,
new
temodule
,
information
extraction
phase
,
hypothesis
,
harabagui
,
usingte
,
answer
,
current
qa
system
,
answer
,
question
,
correctness
,
answer
,
derive
information
,
question
,
hypothesis
,
candidate
sentenceas
,
containment
oftrue
answer
,
inference
recognitionas
classification
problem
,
question
text
,
candidate
text
,
challenges
,
limited
amount
,
entailment
,
environment
,
withan
emphasis
,
graph
based
method
,
information
extractionfrom
data
,
question
classification
,
classification
,
liu
etal
,
relation
extraction
,
otterbacher
,
various
natural
language
,
part
of
speech
tagging
,
named
entity
recognition
,
suzuki
,
isozaki
,
word
sense
disam
719biguation
,
situation
,
module
,
paired
question
,
designing
,
classifier
,
feasible
graph
based
ssl
method
,
main
contribution
,
construction
,
a
t
module
,
matching
structure
,
question
,
identifyinggood
,
concerningdifferent
sentence
structure
,
linguistic
system
,
special
graph
,
te
score
,
novel
affinity
matrix
,
graph
summarization
methodto
enable
learning
,
sophisticated
learning
toolsin
,
result
ofexperiments
,
real
datasets
,
entailmentimplementation
,
different
te
model
,
qa
task
,
supervised
learning
method
,
harabagiu
,
wherein
system
,
search
engine
,
correctanswer
,
question
,
category
,
information
extraction
,
qa
system
,
ourte
model
,
analysis
,
wegive
brief
description
,
space
limitation
,
feature
extractionwe
,
following
preprocessing
modulesfor
feature
extraction
,
textual
entailment
analysis
,
taskof
,
question
amonga
,
type
ofa
question
,
thesearch
space
,
answer
,
question
,
entity
,
location
,
description
,
fine
category
,
manner
,
almost90
accuracy
,
instance
,
question
,
howmany
state
,
course
category
,
finer
category
,
jointlyas
num
,
qc
model
,
considering
different
feature
,
semantic
headword
feature
,
variation
,
rule
,
hypernym
extraction
,
lesk
word
disambiguation
,
regular
expression
,
wh
word
indicator
,
capital
,
named
entity
recognizer
,
formulatequestion
type
matching
feature
,
component
identifies
,
classifies
basic
entity
,
proper
name
,
person
,
organization
,
product
,
location
,
numerical
expression
,
various
measurement
suchas
weight
,
contact
,
address
,
thisis
,
fundamental
layer
,
informationextraction
,
qa
system
,
ner
moduleis
,
combination
,
lesk
word
disambiguation
,
wordnet
,
miller
,
lookup
,
many
user
defined
dictionary
lookup
,
renown
place
,
people
,
organization
name
,
duringthe
ner
extraction
,
phrase
analysis
,
phrase
utility
extraction
,
entity
,
coarse
,
fine
category
,
qc
module
,
pi
module
undertakes
basic
syntactic
analysis
,
shallow
parsing
,
un
embedded
linguistic
structure
,
basicprepositional
phrase
,
verb
group
,
particular
pi
module
,
differentsemantic
structure
,
dependency
parser
,
meaningful
word
,
pitched
cry
,
ach
phrase
,
witha
headword
,
modifier
,
question
,
affirmative
form
,
linguistic
information
,
statement
,
question
,
affirmativeform
,
wh
word
,
placeholder
,
question
word
,
qc
module
,
capital
,
affirmative
form
,
capital
off
ranceloc
,
country
,
answer
textof
loc
,
using
shallow
semantics
,
underlyingdependency
tree
,
linguistic
relationships
,
forinstance
,
human
,
lynch
human
,
component
ofquestions
,
questionanalysis
,
questions
,
object
,
alsoin
copula
,
theyare
,
affirmed
questions
,
copulasentences
,
instance
,
group
name
,
information
,
paired
sentence
analysiswe
,
te
feature
,
semantic
analysis
,
pairsand
,
qa
task
,
classification
problem
,
semantic
feature
,
one
,
question
type
candidate
sentence
ner
match
feature
,
finener
,
thecoarse
ner
,
ner
match
,
question
component
match
features
,
affirmed
question
,
candidatesentence
pair
,
semantic
components
,
modifier
,
semantic
component
,
question
,
matching
com
1one
option
,
non
copulaquestions
,
copula
question
,
ponent
,
forthe
,
following
component
match
feature
,
ead
match
,
subject
match
,
nixon
,
convertedquestions
,
subject
,
modifier
,
feature
foreach
,
mm
atch
feature
,
thefeature
value
,
exact
match
,
containment
,
synonym
match
,
s
match
feature
,
head
match
,
lexsem
,
lexico
syntactic
alignmentfeatures
,
consecutive
word
overlap
,
unigram
bigram
,
individual
pair
,
token
,
verb
count
,
matchingsynonyms
,
wordnet
,
common
sens
,
verb
match
statistic
,
wordnet
,
cause
andentailment
relation
,
result
,
entailmentinformation
,
semi
supervisedlearning
,
entailment
rankingwe
,
semi
supervised
entailment
rankscores
,
information
,
question
,
candidate
sentencepair
,
output
labels
,
labeled
part
,
presentation
,
binary
classification
,
topredict
label
,
property
,
afeature
vector
,
entailment
relation
information
,
allcomponents
,
hypothesis
,
affirmative
question
,
high
similarity
,
component
,
score
betweenthem
,
sentenceswith
similar
structure
,
high
entailment
score
,
similarity
,
edge
weight
,
total
entailment
score
,
largertheir
edge
weight
,
containing
,
question
,
pair
withnon
copula
type
affirmed
question
,
copula
,
different
set
,
eachtype
,
edge
weight
,
follows
,
xjq
dcpxi
,
xjq
dncpxi
,
diagonal
degree
matrix
,
graphg
,
general
graph
based
ssl
,
afunction
,
itsatisfies
,
condition
,
observedlabels
,
second
term
,
regularizer
,
globalconsistency
,
combinatorial
laplacian
,
secondterm
,
loss
function
,
graph
based
ssl
,
new
test
point
,
delalleau
,
induction
scheme
,
induction
,
avoidre
construction
,
new
test
point
,
raph
summarizationresearch
,
graph
based
ssl
algorithm
,
effectiveness
,
real
application
,
stilla
need
,
efficient
ssl
method
,
vast
amount
,
useful
information
,
delalleau
,
convergence
rate
,
propagation
algorithmsof
ssl
method
iso
,
eigenvectors
,
graph
laplacian
,
neighbor
,
theweight
matrix
,
data
point
,
weighted
edge
,
function
viagraph
,
question
,
reducethe
data
point
,
weight
matrix
,
summarization
,
representative
vertex
,
data
point
,
edge
weight
,
suffice
,
similar
data
point
,
region
,
hyperspace
,
likely
tohave
label
,
boundary
,
groupof
similar
data
point
,
respect
,
summary
information
,
newrepresentative
vertex
,
boundary
,
representative
vertex
,
summary
graph
,
dataset
,
data
point
,
sample
dataset
andq
,
sample
datasets
,
x
datasets
,
vapnik
,
append
observed
label
,
graph
summarization
,
actual
data
point
,
predicted
class
label
,
view
ofa
single
node
,
representative
vertex
,
weight
,
degree
d
matrices
,
diagonal
element
,
column
vector
,
high
degree
vertex
,
surroundedwith
large
number
,
close
neighbor
,
algorithm
,
initial
neighbor
node
,
inner
square
,
middle
black
node
,
high
degree
node
,
immediate
neighbor
,
dark
blue
colorednodes
,
algorithm
continues
,
secondary
neighbor
,
neighbor
,
theneighbors
,
opposite
labeled
node
,
instance
,
corresponding
node
,
neighbor
,
third
level
,
opposite
,
boundary
bsi
,
corresponding
nodeand
,
neighbor
,
bsi
xsi
,
maximum
number
,
nodesof
absi
,
whereybsi
,
boundary
bsi
,
edge
weight
,
eachnode
,
boundary
bsi
,
weighted
average
,
vertex
,
representativesummary
node
,
xsi
xsj
,
immediate
neighbors
,
opposite
class
label
,
similarlysome
,
onlyimmediate
neighbor
node
,
instance
,
boundary
,
secondary
neighbor
,
dashedouter
boundary
,
representative
data
point
,
indicators
,
class
label
,
others
,
factthat
,
denser
region
,
labeledpoints
,
information
,
local
density
constraint
,
new
vertex
,
local
density
constraint
,
total
number
,
neighboring
node
,
normalized
density
constraint
,
calculations
,
sample
summary
dataset
,
local
density
constraint
vector
,
local
density
,
crucial
,
inference
,
overall
dataset
,
algorithm
g
,
large
dataset1
,
subsets3
,
random
subset
,
repetitions5
,
end
for8
,
density
constrains
,
data
point
,
sampledataset
x
,
summary
representative
vertex
,
local
density
constraint
,
summarization
algorithm
,
eachrandom
,
asa
result
number
,
summary
datasets
,
data
point
,
representative
sample
,
original
dataset
,
data
point
,
random
sample
,
summarized
dataset
,
data
point
,
summarized
data
,
sample
,
original
size
,
representative
datapoint
,
summarized
dataset
,
local
density
constraint
,
different
sentence
structure
,
copula
,
representative
data
,
summary
dataset
,
final
summary
dataset
,
hybrid
graph
summarymodels
,
new
testing
datasetinstead
,
large
dataset
,
summary
dataset
,
predicted
label
,
local
density
constraint
,
class
label
,
ntenumber
,
graph
basedssl
method
,
summarized
dataset
,
labeled
datapoints
,
unlabeleddata
point
,
local
density
constraint
,
unlabeled
data
point
,
weuse
constant
,
local
density
constraintcolumn
vector
,
local
density
constraints
,
second
term
,
vector
,
labeledvector
,
local
densityconstraints
,
edge
weight
,
densethat
node
,
result
,
experiments
,
graph
representation
,
textual
entailment
information
,
performance
ofthe
qa
system
,
unlabeled
data
,
graph
summarization
,
qa
model
,
c
,
search
,
large
newswire
corpus
foreach
question
,
candidate
sentence
astrue
,
false
entailment
,
containment
,
true
answer
string
,
soundness
,
entailment
,
quality
training
,
wealso
,
qa
type
sentence
pair
fromrte02
,
question
form
,
additional
set
,
labeled
training
dataset
xl
,
graph
based
qa
system
,
question
,
c04
,
dataset
,
voorhees
,
test
questionsand
,
true
false
entailment
,
test
data
,
unlabeled
training
data
,
document
headlinesfrom
,
large
newswire
corpus
,
matching
headline
,
document
asin
,
harabagiu
,
different
approach
,
headline
,
engine
,
headline
,
the1st
,
assumption
,
probability
,
headline
,
probability
,
ofthese
headline
candidate
sentence
pair
,
asadditional
unlabeled
,
head
2http
,
lucene
,
apache
,
org
java
,
model
mrr
top1
top5baseline
,
lexsem
svm
,
qcomp
svm
,
different
feature
,
method
,
converted
question
,
question
type
feature
,
headline
,
candidatesentence
,
question
type
ner
match
feature
,
feature
extraction
step
,
unlabeled
training
,
datasets
,
rank
score
,
search
engine
,
baseline
,
present
theperformance
,
prediction
accuracy
,
commonly
used
performance
measure
,
qa
system
,
voorhees
,
manual
iterativeparameter
optimization
,
training
,
onprediction
accuracy
,
k
nearest
parameter
,
rbf
kernel
,
different
experiment
,
present
individual
result
,
graph
summarization
,
execute
ssl
,
large
unlabeled
datasets
,
assumptions
,
method
incomparison
,
ssl
method
,
individual
contribution
,
qa
system
,
ssl
method
,
summarization
,
labeled
training
,
datasets
,
forssl
,
training
,
testingas
unlabeled
dataset
,
transductive
,
entailment
score
,
result
,
lexsem
,
lexico
semantic
feature
,
subject
,
object
,
andthree
complement
,
comparison
,
baseline
,
significant
effect
,
accuracyof
,
qa
system
,
mrr
performance
,
lexsem
feature
,
minimal
semantic
property
,
mrr
performance
,
performance
ofgraph
summarization
,
separateexperiments
,
first
part
,
selected
subset
,
different
sample
size
,
random
selection
,
thelabeled
dataset
,
ssl
usingdifferent
size
,
reportsthe
mrr
performance
,
qa
system
,
testingdataset
,
graph
summary
ssl
,
gsum
ssl
,
method
,
second
part
,
graph
summarization
,
copula
,
non
copula
question
,
obtainedrepresentative
point
,
labeled
summarydataset
,
similarity
function
,
transduction
,
ybrid
gsum
ssl
,
svm
model
,
thesame
,
training
dataset
intotwo
,
copula
,
non
copula
question
,
svm
method
,
dataset
,
copula
sentence
datasetsare
,
datasetand
vice
versa
,
predicted
score
,
overall
performance
,
hybrid
svm
model
,
different
randomsamples
,
result
,
labeledpoints
,
data
increase
,
svm
performance
increase
,
graph
summary
ssl
isstill
,
separate
model
,
copula
andnon
copula
question
,
different
feature
,
theperformance
,
overall
model
,
increases
,
method
,
hybridgraph
summary
ssl
,
hybrid
gsum
ssl
,
whenthe
number
,
labeled
data
,
performance
improvement
,
hybrid
gsumssl
,
hybrid
,
different
size
,
mrr
top1
top525k
,
effect
,
unlabeled
dataon
mrr
,
hybrid
graph
summarization
ssl
,
labeled
data
,
hybrid
svm
model
,
performance
increase
,
state
of
the
artmrr
performance
,
c04
datasets
,
klakow
,
factthat
,
seperate
entailment
modelsfor
copula
,
non
copula
,
useful
information
,
betterrepresentation
,
specific
data
,
ssl
method
,
information
,
unlabeleddata
,
becomes
,
numberof
data
point
,
various
research
,
usage
oflarge
number
,
unlabeled
dataset
challenge
,
de
lalleau
,
hybrid
gsum
ssl
,
different
approach
,
large
datasetsinto
representative
data
point
,
original
spatial
information
,
data
point
,
local
density
constraint
,
ssl
summarization
schema
,
labeled
data
,
summary
dataset
,
additional
spatial
,
performanceof
,
graph
summary
model
,
unlabeled
data
sample
,
effect
,
theperformance
,
dataset
,
result
,
unlabeled
data
,
discussionsin
,
graph
based
ssl
algorithm
,
qa
taskby
,
unlabeled
entailment
relation
between
affirmed
question
,
candidate
sentencepairs
,
syntactic
feature
,
textual
entailment
analysis
,
tothe
baseline
,
new
graph
representation
,
textual
entailment
relation
,
different
question
structure
,
summarization
,
graph
based
ssl
,
qa
taskperformance
,
unlabeled
data
,
several
direction
,
ourwork
,
result
,
graph
summarizationon
,
large
unlabeled
data
,
thanbest
svm
result
,
affirmed
question
,
whereinheadlines
,
proper
sentence
form
,
adversely
effect
,
entity
match
,
entity
,
semantic
match
component
feature
extraction
,
usingreal
question
,
different
source
,
constructdifferent
test
datasets
,
distance
measure
,
entailment
between
,
compare
,
transductive
approach
,
lim
,
zhengyuniu
,
relation
extraction
,
label
propagation
,
semi
supervised
learning
,
proceedingsof
,
egidio
,
question
,
passage
selection
,
domain
question
answering
,
strzalkowski
,
andharabagiu
,
springer
,
delalleau
,
yoshua
bengio
,
ler
oux
,
efficient
nonparametric
function
induction
,
semi
supervised
learning
,
proceedingsof
ais
,
delalleau
,
yoshua
bengio
,
ler
oux
,
large
scale
algorithm
,
harabagiu
,
hickl
,
methodsfor
,
textual
entailment
,
open
domain
question
answering
,
pages905
,
zhiheng
huang
,
thint
,
zengchang
qin
,
question
classification
,
headword
andtheir
hypernym
,
proceeding
,
conferenceon
empirical
method
,
klein
,
manning
,
accurate
unlexicalized
parsing
,
proceeding
,
the41st
meeting
,
lesk
,
true
thing
,
calledthem
,
wrong
name
vocabulary
problem
,
retrieval
system
,
annual
conferenceof
,
waterloo
,
newoed
,
rong
liu
,
jianzhong
zhou
,
ming
liu
,
agraph
based
semi
supervised
learning
algorithm
forweb
classification
,
sixth
int
,
ntelligent
system
design
,
application
,
miller
,
wordnet
,
lexical
database
forenglish
,
communication
,
zheng
yu
niu
,
chew
lim
,
word
sense
disambiguation
,
labeledpropagation
,
semi
supervised
learning
,
inp
roceedings
,
jahna
otterbacher
,
gunes
erkan
,
radevdragomir
,
lexrank
,
passage
,
random
walk
,
question
based
prior
,
information
processing
,
management
,
prager
,
dragomir
radev
,
krzysztof
czuba
,
search
engine
ortwo
,
question
answering
,
text
retrieval
conference
,
saggion
,
gaizauskas
,
experiments
,
passage
selection
,
extraction
,
question
,
advance
,
naturallanguage
processing
,
springer
,
shen
,
klakow
,
correlation
,
dependency
relation
path
,
extraction
,
proceeding
,
vikas
sindhwani
,
wei
chu
,
sathiya
keerthi
,
semi
supervised
gaussian
process
classifier
,
proceeding
,
suzuki
,
hideki
isozaki
,
semi
supervisedsequential
labeling
,
segmentation
,
giga
word
scale
unlabeled
data
,
proceeding
,
theacl
,
nguyen
thanh
tri
,
akira
shi
mazu
,
semi
supervised
learning
forquestion
classification
,
vilademir
vapnik
,
nature
,
theory
,
springer
verlag
,
new
york
,
voorhees
,
overview
,
text
retrieval
conference
,
voorhees
,
overview
,
trec2004
question
,
dengyong
zhou
,
scho
,
random
walk
,
proceeding
,
schlkopf
,
berlin
,
ger
many
,
springer
,
dengyong
zhou
,
bousquet
,
ja
,
scho
,
global
consistency
,
advancesin
neural
information
processing
system
,
xiaojin
zhu
,
lafferty
,
zoubin
ghahramani
,
semi
supervised
learning
,
gaus
sian
field
,
gaussian
process
,
technical
report
cmu
cs
,
carnegie
mellon
,
pittsburgh
,50
th
annual
meeting
,
association
,
computational
linguistics
,
republic
,
association
,
long
,
crespo
,
anlei
dong
,
sathiya
keerthi
,
su
wuy
ahoo
labs701
first
avenue
,
sunnyvaleca
,
zhiheng
huang
,
jfcrespo
yahoo
,
sulin
yahoo
,
comabstractsequential
modeling
,
variety
,
important
application
includingnamed
entity
recognition
,
shallow
parsing
,
real
timelarge
scale
tagging
application
,
decoding
speed
,
bottleneck
,
existing
sequential
tagging
algorithm
,
paper
,
k
best
iterative
viterbi
algorithms
,
sequential
decoding
,
efficiency
,
algorithm
forfive
nlp
tagging
task
,
iterative
viterbi
decoding
,
several
time
,
magnitude
,
thanthe
state
of
the
algorithm
,
taskswith
,
large
number
,
applications
,
thousand
,
ra
biner
,
lafferty
,
perceptron
,
nlp
application
,
sequentialdecoding
,
tag
sequence
,
important
part
,
sequentialtagging
framework
,
viterbi
algorithm
,
viterbi
,
algorithm
isquite
efficient
,
label
size
,
problem
,
timecomplexity
,
input
token
size
,
label
size
,
viterbi
decoding
,
label
size
,
problem
modeledconsists
,
viterbi
algorithm
,
sequence
,
tolerableresponse
time
,
esposito
,
radi
cioni
,
a
c
arpediem
algorithmwhich
,
necessary
node
,
thebest
sequence
,
staggered
decoding
algorithm
,
datasets
,
large
numberof
label
,
aforementioned
literature
,
k
best
sequential
decoding
problem
,
practice
,
exampleto
pursue
,
high
recall
ratio
,
entity
recognition
system
,
k
best
sequence
,
true
entity
,
bestone
,
k
best
par
,
syntactic
parsing
context
,
paulsand
klein
,
context
,
knowledge
,
state
of
the
k
best
sequential
decodingalgorithm
,
iterative
process
,
algorithm
,
algorithm
,
several
time
,
tagging
task
,
contribution
,
search
framework
,
problem
,
classic
viterbi
decoding
,
iterative
decoding
algorithm
,
algorithm
,
decoding
algorithm
,
comparison
,
asignificant
gap
,
algorithm
,
k
best
iterativeviterbi
,
algorithm
,
latter
,
several
time
ororders
,
magnitude
,
crf
pp
,
sourceforge
,
lingpipe
,
lingpipe
,
package
,611
k
best
decoding
algorithm
,
algorithm
makesreal
time
large
scale
tagging
application
,
thousands
,
sequential
decoding
problem
,
context
,
perceptron
algorithm
,
lafferty
,
formally
,
aperceptron
model
isf
,
a
c
rf
model
isp
,
observation
sequence
,
label
sequence
,
sequence
position
,
sequence
size
,
feature
function
andk
,
feature
function
,
parameters
,
importance
,
feature
function
,
prediction
,
instance
specific
normalization
functionz
,
decoding
,
perceptron
,
theprobability
,
input
sequence
,
decodingfor
perceptron
,
discussion
,
featuresinto
,
unigram
label
feature
,
bi
gram
label
feature
,
unigram
feature
,
current
label
,
arbitrary
feature
pattern
,
input
sequence
,
bigram
feature
,
thecurrent
label
,
problem
asargmaxyt
,
understanding
,
position
,
sscore
,
sequential
decoding
problem
,
cast
asa
score
,
problem2
,
discussionhereafter
,
analysis
,
algorithm
,
bestand
k
best
sequential
decoding
,
thesealgorithms
,
algorithms
,
b
est
viterbithe
viterbi
algorithm
,
classic
dynamic
programming
,
algorithm
,
computational
complexity
,
inputsequence
size
,
label
size3
,
computes
,
startingposition
,
edge
score
,
nodesyt
,
node
score
,
initialization
,
recursion
definedabove
,
corresponding
sequence
,
therecursive
computation
,
forwardpass
,
computing
,
lattice
,
backward
pas
computes
,
initialization
,
maxscore
,
forward
,
backward
pas
,
sequence
,
summarizesthe
computational
complexity
,
viterbi
,
complexityof
tl2
,
decoding
algorithm
,
iterative
viterbi
,
algorithm
,
seesection
,
iterative
viterbikaji
,
efficientsequential
decoding
algorithm
,
name
iterative
viterbi
,
constraint
,
position
,
feature
size
term
,
simplicity
,
algorithm
,
reason
,
iterative
process
,
role
,
algorithm
,
iterative
process
,
k
best
sequential
decoding
,
main
idea
,
coarse
,
active
label
,
degeneratelabels
,
active
label
,
label
,
position
,
new
label
,
inactive
label
,
dotted
one
,
active
label
,
degenerate
one
,
degenerate
label
,
maxscore
,
inactive
label
,
degenerate
label
,
edge
score
,
adegenerate
label
,
active
label
,
maxedge
score
,
degenerate
label
,
themax
edge
score
,
definition
,
bestsequence
,
degenerate
lattice
,
upper
bound
,
sequence
,
theoriginal
lattice
,
sequence
,
degenerate
label
,
sequencefor
,
lattice
consisting
,
active
label
,
lattice
consisting
,
active
label
,
degenerate
one
,
position
,
onedegenerate
label
,
pseudo
code
,
algorithm
,
lgorithm
,
lattice
,
oneactive
label
,
degenerate
label
,
position
,
probability
,
training
data
,
viterbi
algorithm
,
lattice
,
sequence
,
sequence
,
active
label
,
algorithm
terminates
,
sequence
,
active
sequence
,
lattice
,
thelattice
,
sequence
score
,
beam
search
,
beam
size
,
forward
,
with4the
maximum
score
,
active
sequence
,
active
sequence
score
,
lattice
,
theformer
,
latter
,
expansion
,
lattice
ensures
,
lattice
,
active
labelsas
,
position
,
figure
,
thecolumn
wise
expansion
step
,
activelabels
,
column
,
sequence
,
degenerate
lattice
pass
,
thedegenerate
label
,
column
,
algorithm
iterative
viterbi
algorithm1
,
beam
search2
,
init
lattice3
,
consists
,
active
label
,
return
y11
,
end
if12
,
lattice
,
then13
,
expand
lattice16
,
end
foralgorithm
f
orward1
,
compute
,
prune
yi
,
current
lattice5
,
end
if6
,
end
for7
,
return
sequence
,
column
wise
lattice
expansion
,
bestsequence
,
initial
degenerate
lattice
,
degenerate
label
,
first
column
,
column
wise
expansion
,
sequence
,
notice
,
active
label
inthe
first
column
,
final
result
,
forward
pas
,
thenode
pruning
,
sequence
,
lattice
,
removal
,
anoptimal
sequence
,
nodeis
,
lattice
,
bound
lb
,
anactive
sequence
,
property
,
efficiency
,
iterativeviterbi
algorithm
,
backward
pas
,
alternative
call
,
forward
,
backwardpasses
,
algorithm
,
alternative
updating
lowering
,
node
forward
,
backward
score
,
node
pruning
,
forward
pas
,
algorithm
,
backward
pas
,
bound
lb
,
iterationof
,
main
loop
,
algorithm
,
forwardand
backwards
score
,
bound
lb
increase
,
morenodes
,
iterative
viterbi
algorithm
,
computationalcomplexity
,
kaji
etal
,
m
th
iteration
,
algorithm
,
iterative
viterbi
decoding
,
active
label
,
t4i
time
complexity
,
m
th
iteration
,
thebest
,
time
complexity
,
isthe
ceiling
function
,
real
number
,
integer
,
time
complexity
isorder
,
b
est
carpediemesposito
,
radicioni
,
anovel
best6
sequential
decoding
algorithm
,
car
pediem
,
necessarynodes
,
sequence
,
lattice
,
carpediem
,
complexity
,
andtl2
,
description
,
algorithm
,
spacelimitations
,
carpediem
,
baseline
,
ourexperiments
,
speed
comparison
,
k
best
sequence
,
best
label
,
k
best
sequence
,
suboptimal
label
,
thek
best
sequential
decoding
,
bestlabel
memorization
,
dynamic
programmingparadigm
,
k
best
label
,
k
best
sequence
,
k
best
viterbialgorithm
,
computational
complexity
ofk
tl2
,
k
best
label
,
lattice
,
k
best
viterbi
algorithm
,
forward
,
best
viterbi
decoding
,
k
best
solution
,
end
position
,
k
best
sequence
,
knowledge
,
efficient
k
best
sequence
algorithm
,
viterbi
algorithm
asshown
,
algorithm
,
algorithm
consists
,
oneforward
pas
,
backward
pas
,
forwardpass
computes
,
viterbi
forward
score
,
current
node
,
addition
,
node
store
,
backlinkwhich
,
predecessor
,
part
,
algorithm
,
backward
,
algorithm
,
agenda
,
sequence
,
operation
,
correspond
,
operation
,
sequence
,
position
,
sequence
,
representsthe
,
sequence
,
ofthe
sequence
,
sequence
,
however
,
i
th
best
,
sequencemay
,
sequence
,
priority
,
sequencewhich
,
algorithm
,
agenda
,
alternative
candidate
node
,
orsequences
,
agenda
,
double
nested
loop
,
optimal
node
,
sequence
,
agenda
allnodes
,
sequence
,
thejust
,
interpretation
,
popped
node
,
sequence
,
first
viterbiforward
pas
,
figure
,
lattice
,
position
,
sequence
,
new
node
,
agenda
,
tothe
double
,
algorithm
,
pushed
node
,
sequence
,
sequence
,
consists
,
viterb
sequence
,
start
to1
,
sequence
,
candidate
,
next
agenda
pop
operation
,
algorithm
,
optimal
node
,
k
best
sequence
,
optimal
node
,
algorithm
has614tbcdbcdbcdbcda
a
a
a31
20f
igure
,
alternative
node
push
,
optimal
node
,
computation
complexity
,
bestand
,
first
term
accounting
,
second
term
accounting
,
backward
process
,
bottleneck
,
viterbi
forward
pas
,
algorithm
3k
best
viterbi
algorithm1
,
backlink
,
then11
,
new
node
,
position
,
end
for16
,
backlink17
,
end
for18
,
return
,
sequence
,
algorithmsin
,
algorithm
,
large
number
,
iterative
decoding
algorithm
,
best
sequential
decoding
,
wethen
,
algorithm
,
k
best
decoding
algorithm
,
iterativeprocess
,
viterbi
algorithm
,
theiterative
viterbi
,
algorithm
,
norvig
,
asa
classic
search
algorithm
,
syntactic
parsing
,
manning
,
general
idea
,
toconsider
label
yt
,
thebest
sequence
,
current
nodeand
,
scorefrom
,
current
node
,
target
,
anagenda
,
nodesare
,
iscalled
monotone
,
isguaranteed
,
sequence
,
heuristic
,
node
yt
,
sumof
edge
score
,
position
,
node
score
,
position
,
heuristic
,
differentcoarse
level
,
viterbi
backward
passto
different
degenerate
lattice
,
viterbibackward
,
different
heuristic
,
differentdegenerate
lattice
,
different
iterations
,
algorithm
,
m
th
iteration
corresponds
,
lattice
,
accurate
heuristic
,
result
,
efficient
search
,
efficiency
,
theprice
,
morecomputation
time
,
viterbi
backward
pas
,
naive
heuristic
,
position
,
expansion
result
,
allnodes
,
next
position
,
agenda
,
search
issimilar
,
beam
search
,
beam
size
,
complexity
,
position
,
expansion
result
,
agenda
,
complexity
,
iterative
process
,
iterativeviterbi
decoding
,
algorithm
,
iterative
,
algorithm
,
simplicity
,
naive
heuristic
,
iterative
,
algorithm
,
initializethe
lattice
,
active
label
,
degeneratelabel
,
position
,
algorithm
,
degenerate
lattice
,
thebest
sequence
,
sequence
,
lattice
,
iterationuntil
,
active
sequence
,
similar
toiterative
viterbi
algorithm
,
complexity
,
extension
,
memorization
,
k
best
label
,
best
,
computational
complexity
,
algorithm
,
k
best
decodingbest
,
casebeam
tl
tl
ktl
ktl
viterbi
tl2
tl2
,
naive
heuristic
,
ordifferent
coarse
level
heuristic
,
first
node
,
agenda
,
trackthe
k
best
sequence
,
algorithm
hasthe
computational
complexity
,
k
best
iterative
viterbi
algorithm
,
algorithm
,
iterativeprocess
,
viterbi
algorithm
,
majordifference
,
best
iterative
viterbi
algorithm
,
algorithm
,
algorithm
,
thelatter
,
afterthe
,
sequence
,
sequencesare
,
algorithm
,
k
best
sequence
,
active
sequence
,
k
best
active
sequence
,
lattice
,
search
,
thenext
iteration
,
iterative
viterbi
algorithm
,
position
,
forwardor
backward
pass
,
tighter
,
bound
lb
,
bound
lb
,
k
th
bestscore
,
beam
search
,
beam
size
,
atline
,
beam
search
,
theoriginal
lattice
,
active
label
perposition
,
beam
search
time
,
total
decoding
time
,
update
lb
,
active
sequences
,
current
lb
,
k
th
active
sequencescore
,
lbwith
,
k
th
active
sequencescore
,
active
sequence
,
viterbi
algorithm
,
manner
,
update
,
viterbi
algorithm
,
k
th
active
sequence
,
turn
offer
,
tighter
,
expense
,
additional
time
,
backwarda
process
,
lb
updateson
line
,
fast
decoding
,
updating
,
viterbi
sequence
,
line19
,
different
,
integer
,
result
,
speed
boost
,
algorithm
4k
best
iterative
viterbi
algorithm1
,
lb
k
th
,
original
lattice
,
init
lattice3
,
consists
,
active
label
,
y
consists
,
active
sequence
,
return
ys13
,
end
if14
,
end
if15
,
lb
k
th
,
lattice
,
then16
,
lb
k
th
,
lb
k
th
,
end
if21
,
expand
lattice22
,
end
for5
experimentswe
compare
,
k
best
sequential
decoding
algorithm
,
datasets
inthis
,
ll2000
joint
pos
tagging
,
chunking
,
ll
,
joint
pos
tagging
,
entity
,
matsuzaki
,
search
,
dataset
,
training
,
testing
,
pos
task
,
pos
tag
,
chunk
tag
,
ll
,
similarly
,
pos
tag
,
chunk
tag
,
entity
tag
,
joint
tag
,
ll
,
tagjoining
,
different
tag
decoding
,
entity
,
effectiveapproaches
,
joint
tag
,
problem
,
thesearch
query
ner
dataset
,
in
house
annotateddataset
,
semantic
label
,
product
,
business
tag
,
search
query
,
training
,
test
set
size
,
average
token
length
,
test
dataset
andthe
label
size
,
datasets
,
su
pertag
datasets
,
query
datasets
assign
tag
,
phrase
,
standard
bio
encoding
,
ll
,
searchquery
datasets
,
training
,
test
datasets
size
,
average
tokenlength
,
test
set
,
label
size
,
datasets
,
test
token
length
label
sizepos
,45
c
onll2000
,
long
crf
training
time
,
stochastic
gradient
training
,
forthese
large
label
size
datasets
,
percep
tron
algorithm
,
training
,
iteration
,
timetakes
minute
,
datasets
,
thatthe
selection
,
algorithm
,
process
,
decoding
,
identical
forboth
crf
,
training
algorithm
,
common
feature
,
previous
study
,
periera
,
unigrams
,
itsneighboring
word
,
word
bigram
,
prefix
,
suffixes
,
current
word
,
capitalization
,
punctuation
,
tag
bigram
,
ll2000and
ll
,
datasets
,
supertag
dataset
,
andthe
unigrams
,
bigram
,
gold
pos
input
,
forsearch
query
dataset
,
token
accuracy
,
datasets
,
facilitate
comparison
,
previous
,
ll2000
,
ll
,
supertag
,
search
query
,
comparable
tothe
state
of
the
,
baseline
tocompare
,
last
dataset
,
publicly
available7
,
task
specific
feature
,
top
ofthe
standard
feature
,
feature
engineering
,
many
iteration
,
averageare
,
iterative
viterbi
,
iterative
viterbia
algorithm
,
iteration
size
,
position
,
ll
,
dataset
,
total
iterationnumber
,
whole
lattice
,
thandlog2
le
,
different
position
,
large
number
,
iterations
,
algorithm
,
especiallyiterative
viterbi
algorithm
,
algorithm
,
iteration
number
,
iterative
viterbi
,
iterative
viterbi
algorithm
,
second
,
decoding
algorithms
,
decoding
speed
,
different
decoding
algorithm
,
feature
extraction
,
beam
searchdecoding
,
baseline
,
thatbeam
decoding
,
algorithm
,
algorithmsproduce
,
beam
decoding
,
best
decoding
,
iterative
viterbi
,
choice
,
pos
task
,
gap
betweeniterative
viterbi
,
ll
,
dataset
,
second
,
latter
onlydecodes
,
second
,
iterativeprocess
,
speed
ofiterative
viterbi
,
viterbi
,
iterative
compared7the
,
dynamic
nature
,
test
query
token
,
training
set
,
viterbi
,
iterative
process
,
node
pruning
procedure
,
pruning
,
algorithm
,
ll
,
removal
,
pruning
,
best
iterative
viterbi
decoding
,
second
,
carpediem
,
carpediem
,
node
score
,
dominant
factorsto
,
sequence
,
assumption
,
animportant
role
,
best
decoding
,
k
best
viterbi
,
proper
heuristic
,
secondfor
ll
,
dataset
,
classic
viterbi
,
decoding
speed
,
second
,
ll
,
dataset
,
exception
,
supertag
dataset
,
second
whilethe
,
scalability
issue
,
viterbi
algorithm
,
datasets
,
thousand
label
,
iterativeviterbi
,
winner
,
factor
,
ll2000
,
ll
,
supertag
,
query
search
,
iterative
viterbia
,
beam
search
,
figure
,
k
best
,
algorithm
decoding
speed
,
respect
,
different
value
,
algorithm
,
thanthe
viterbi
,
algorithm
,
theiterative
viterbi
,
former
converges
,
latter
,
k
best
sequence
,
overthe
whole
lattice
,
iteration
,
iterativeviterbi
algorithm
,
k
best
sequences
,
degenerate
lattice
,
overhead
,
multiple
iteration
,
decodingspeed
,
viterbi
algorithm
,100020406080100120140160180200
ksentences
second
l
v
iterbia
,
algorithmsfor
various
,
ll
,
workthe
viterbi
algorithm
,
nlp
application
,
espositoand
radicioni
,
algorithm
,
necessary
node
,
lattice
,
thebest
sequence
,
staggered
decoding
,
iterative
baseddecoding
algorithm
,
exact
decoding
,
algorithm
,
beamsearch
,
tsuruoka
andtsujii
,
easiest
first
deterministic
decoding
,
siddiqi
,
parameter
tying
approach
,
fast
inference
,
similar
idea
,
optimality
,
can
not
,
approximate
algorithm
,
k
best
parsing
,
chiang
,
efficient
algorithm
,
k
best
viterbi
algorithm
,
inthis
paper
,
viterbi
forward
pas
,
search
,
algorithm
,
viterbipass
,
iterative
viterbi
,
viterbi
,
coarse
,
parsing
,
charniak
,
degenerate
label
,
coarse
level
,
however
,
difference
,
coarse
to
fine
parsingis
,
approximate
decoding
,
different
coarse
level
,
heuristic
usedin
decoding
,
framework
,
felzen
szwalb
,
iterative
process
,
burkett
,
bothexploit
,
iterative
algorithm
,
inthis
paper
,
addition
,
algorithm
,
k
best
sequential
decoding
,
k
best
iterative
algorithm
,
several
timesor
order
,
magnitude
,
state
of
the
k
best
decoding
algorithm
,
application
,
thousand
oflabels
,
acknowledgmentswe
,
yusuke
miyao
,
nobuhiro
kajifor
,
hps
g
tr
,
invaluable
comment
,
theanonymous
reviewer
,
second
,
best
decoding
algorithm
,
viterbi
,
second
,
best
decoding
algorithm
,92
v
iterbi
,
viterbi
,
optimal
graphsearch
,
iterated
graph
cut
,
proceeding
,
multilevel
coarse
to
finepcfg
parsing
,
proceeding
,
naa
cl
,
efficient
inference
,
large
conditionalrandom
field
,
proceeding
,
discriminative
training
method
,
markov
model
,
theory
,
proceeding
,
emn
lp
,
esposito
,
carpediem
,
viterbi
algorithm
,
application
tos
,
sequential
learning
,
journal
,
machinelearning
research
,
felzenszwalb
,
mca
llester
,
architecture
,
journal
,
artificial
intelligenceresearch
,
nilsson
,
or
mal
basis
,
heuristic
determination
,
system
scienceand
cybernetics
,
chiang
,
k
best
parsing
,
proceeding
,
international
workshop
,
efficient
inference
,
large
scale
natural
language
data
,
proceeding
,
acl
ijc
nlp
short
paper
,
kitsuregawa
,
decoding
,
sequence
labeling
,
proceeding
,
manning
,
parsing
,
parse
selection
,
proceeding
,
pereira
,
conditional
random
field
,
probabilistic
model
,
labeling
sequence
data
,
proceeding
,
tsujii
,
supertagging
,
cfg
filtering
,
proceeding
,
ijc
ai
,
proceedings
,
hidden
markov
modelsand
,
application
,
speech
recognition
,
proceedings
,
pattern
analysis
,
machine
intelligence
,
norvig
,
pereira
,
conditional
random
field
,
proceeding
,
hlt
naa
cl
,
fast
inference
,
large
state
space
hmm
,
proceeding
,
tsujii
,
bidirectional
inference
,
easiest
first
strategy
,
sequence
data
,
proceeding
,
convolutionalcodes
,
information
theory
