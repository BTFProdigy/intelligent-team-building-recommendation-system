e
dmonton
,
t
utorials
,
p
roceedings
,
hlt
-
naa
cl
exponential
prior
,
maximum
entropy
model
joshua
goodman
,
joshuago
microsoft
,
com
abstract
maximum
entropy
model
,
common
mod
,
technique
,
exponential
distribution
,
prior
lead
,
absolute
discounting
,
previous
technique
,
a
g
aussian
prior
,
error
rate
,
exponential
prior
,
simpler
learning
algorithm
,
behavior
,
furthermore
,
exponential
prior
,
success
,
technique
,
simple
variation
,
1
i
ntroduction
conditional
maximum
entropy
,
maxent
,
variety
,
lan
guage
modeling,rosenfeld,part-of-speech
tag
ging
,
prepositional
phrase
attachment
,
word
selection
,
machine
translation
,
berger
,
sentence
boundary
,
reynar
,
ratnaparkhi
,
logistic
regression
model
,
maximum
likeli
hood
exponential
model,log-linear
model
,
perceptrons
single
layer
neural
network
,
perceptrons
,
standard
sigmoid
function,log-loss
,
multi-layer
neural
network
,
log
loss
,
discussion
,
conditional
maxent
model
,
a
g
aussian
prior
,
parameter
,
exponential
distribution
,
several
advantage
,
exponential
prior
,
ac
tual
distribution
,
parameter
,
improved
accuracy
,
simpler
learning
algorithm
,
addition
,
exponential
prior
inspires
,
improved
version
,
discounting
,
perplexity
,
conditional
maxent
model
,
input
vector
,
output
,
called
indicator
function
,
feature
value
,
particular
property
,
number
,
feature
,
parameter
,
weight
,
instance
,
word
sense
disambiguation
,
context
,
occurrence
,
context
,
financial
sense
,
large
positive
number
,
maxent
model
,
several
valuable
property
,
della
pietra
,
good
overview
,
constraint
satisfaction
,
many
time
,
training
data
,
parameter
,
many
time
,
maxent
model
,
property
,
equality
,
constraint
,
next
important
property
,
likelihood
,
training
data
,
uniform
distribution,kullback-leibler
divergence
,
constraint
,
maximum
entropy
model
,
last
property
,
similarity
,
uniform
distribu
tion
,
regularization
,
mod
el
,
possible
feature,near-zero
probability
,
infi
nite
,
actual
zero
probability
,
symptom
,
num
ber
,
approach
,
problem
,
detail
,
section
,
relevant
approach
,
rosenfeld
,
a
g
aussian
prior
,
maxent
model
,
technique
,
technique
,
language
,
baseline
,
comparison
,
similar
considera
tions
,
technique
,
exponential
prior
,
al
place
a
g
,
variance
,
model
parameter
,
posterior
probability
,
maxent
model
,
parameter
,
argmax
,
data
instance
,
a
g
aussian
prior
,
arg
max
,
trained
model
,
con
straints
,
rosenfeld
,
constraint
,
observed
count
,
observed
count
minus
,
language
modeling
term
,
process
,
single
prior
,
problem
type
,
experimen
tal
result
section
,
particular
set
,
parameter
,
parameter
distribution
,
prior
distribution
,
alternate
form
,
parameter
seach
,
much
simpler
,
exponential
prior
,
distribution
,
exponential
prior
,
arg
max
,
maximization
,
gaussian
maximization
,
equa
tion
,
property
,
observed
count
,
reciprocal
,
standard
deviation
,
constraint
,
constraint
,
gaussian
prior
,
constraint
,
change
,
section
,
discounting
,
constant
,
common
technique
,
language
model
,
goodman
,
bayesian
justification
,
section
,
different
task
,
grammar
checking
,
collaborative
filtering
task
,
exponential
prior
yield
,
error
rate
,
gaussian
,
algorithm
,
discounting
,
section
,
learning
algorithm
,
expo
nential
prior
,
provable
convergence
property
,
simple
discounting
formula
,
simple
learning
algorithm
,
important
con
tribution
,
algorithm
,
a
g
aussian
prior
,
previous
related
work
,
laplacian
prior,two-sided
exponential
,
diffi
cult
time
,
algorithm
,
lapla
cian
,
continuous
first
derivative
,
exponential
prior
,
standard
gra
dient
descent
type
algorithm
,
poor
behav
ior
,
williams
,
full
ten
page
,
heuristic
approach
,
prob
lem
,
discussion
,
reader
,
algorithm
,
deter
mining
successive
search
direction
,
ferred
value
,
step
size
,
simple
variation
,
standard
algorithm
,
general
,
ratcliff
,
problem
,
gis
us
,
update
rule
,
illiams
,
algorithm
,
complex
case,multi-layer
network
,
layer
case
,
obvious
simplification
,
approach
,
layer
case
,
modified
algorithm
,
different
style
,
common
case
,
output
,
word
sense
disam
biguation
problem
,
financial
sense
,
ques
tions
,
indicator
,
indicator
function
,
indicator
,
result
,
exponential
,
exponential
prior
,
positive
val
ues
,
double
,
indicator
,
towards
,
weight
,
weight
,
answer
,
constraint
,
algorithm
,
derivation
,
algorithm
,
rosenfeld
,
expression
,
expression
,
objective
function
,
convex
function
,
global
maximum
value
,
objective
function
,
maximum
,
change
thing
,
maximum
,
discontinuity
,
objective
function
,
partial
deriva
tive
,
respect
,
implies
,
optimum
,
observed
count
,
absolute
discounting
equation
,
possible
optimal
point
,
optimum
,
notice
,
important
property
,
exponential
prior
,
similiar
property
,
laplace
prior
,
williams
,
tibshirani
,
pa
rameters
,
natu
ral
pruning
,
exponential
prior
,
gaussian
prior
,
pruning
,
additional
pruning
,
memory
sav
ings
,
feature
selection
technique
,
infrequent
parameter
,
update
equation
,
deriva
tion
,
minor
change
,
a
g
aussian
prior
,
update
equation
,
equation
,
corresponding
equation
,
a
g
aussian
prior
,
rosenfeld
,
equation
,
closed
form
solution
,
numerical
method
,
new
ton
,
method
,
exponential
prior
,
derive
variation
,
update
,
stance
,
appendix
,
update
,
iterative
scaling
,
della
pietra
,
exponential
prior
,
update
,
sequential
conditional
general
,
goodman
,
several
time
,
scg
update
,
binary
feature
,
exponential
prior
,
method
,
maxent
,
mal
comparison
,
gradient
method
,
pilot
study
,
version
,
cg
use
heuris
tic
,
step
size
,
lose
convergence
guarantee
,
conjugate
gradient
,
conjugate
gradient
li
brary
,
parameter
constraint
,
expo
nential
prior
,
discontinuous
derivative
,
a
l
apla
cian
,
standard
conjugate
gradient
technique
,
discounting
,
language
,
several
time,rosenfeld,update,good-turing
,
update
,
problem
,
proponent
,
constraint
,
simple
variation
,
update
,
exponential
prior
,
problem
,
observed
,
stitute
a
b
,
convex
objective
function
,
global
maximum
,
update
function
,
towards
,
maximum
,
variation
,
constraint
,
equation
,
ob
jective
function
,
furthermore
,
experimental
result
section
,
language
,
modified
update
function
,
traditional
update
,
motivated
approach
,
exponential
prior
,
simple
variation
,
performance
,
revious
work
,
fair
amount
,
previous
work
,
regu
larization
,
maxent
model
,
early
approach
,
feature
selection
,
della
pietra
,
count
cutoff
,
rosenfeld
,
feature
,
extra
probability
left-over
,
unobserved
event
,
maximum
entropy
fashion
,
problem
,
approach
,
useful
information
,
low
count
,
low
discrimination
feature
,
valuable
information
,
re
cent
approach,rosenfeld,technique,good-turing
discount
,
number
,
approach
,
khudanpur
,
newman
,
fuzzy
maxent
frame
work
,
della
pietra
,
della
pietra
,
rosenfeld
,
rosenfeld
,
com
plete
discussion
,
approach
,
rosenfeld
,
suggestion
,
lafferty
,
a
g
aussian
prior
,
maxent
mod
el
,
technique
,
technique
,
language
,
technique
,
tibshirani
,
laplacian
prior
,
lin
ear
model
,
linear
regression
,
ob
jective
function
,
absolute
value
,
parameter
,
least
absolute
shrinkage
,
feature
se
lection
,
exponential
prior
,
laplacians
,
technique
,
williams
,
a
l
aplacian
prior
,
neu
ral
network
,
single
layer
neural
network
,
cer
tain
loss
function
,
logistic
regres
,
maximum
entropy
model
,
williams
,
algorithm
,
general
case
,
figueiredo
,
unpublished
independent
work
,
laplacian
prior
,
lo
gistic
regression
,
complex
al
gorithm
,
supervised
learning
,
result
,
perkins
,
theiler
,
logisitic
regression
,
a
l
aplacian
prior
,
learning
algorithm
,
conjugate
gradient
descent
,
gra
dient
method
,
continuous
first
derivative
,
weight
,
main
contribution
,
laplacian
prior
,
logistic
regression
,
first
good
learning
algorithm
,
model
type
,
contribution
,
explicit
comparison
,
aussian
prior
,
improved
performance
,
real
data
,
fixed
point
,
dis
counting,iterative-scaling
style
,
parameter
,
4
k
neser-ney
smoothing
,
section
,
excellent
performance,kneser-ney
smoothing
,
performing
language
model
,
technique
,
justification
,
important
question
,
method
work,guidance,kneser-ney
,
problem
,
fractional
count
,
solution
,
goodman
,
extensive
comparison
,
different
smoothing
,
regularization
,
tech
niques
,
language
modeling
,
ver
sion,kneser-ney
smoothing
,
kneser
,
performing
technique
,
unfortu
,
partial
theoretical
justification,kneser-ney
smoothing
,
marginals
,
important
part,justification,kneser-ney
smoothing
discount
count
,
con
ventional
regularization
technique
,
dirichlet
prior,discounting,version,kneser-ney
smoothing
,
excellent
ap
proximation
,
maximum
entropy
model
,
ex
ponential
prior,kneser-ney
smoothing
,
absolute
discounting
,
distribution
,
observed
discount
,
difference
,
tween
backoff
kneser-ney
smoothing
,
maxent
mod
el
,
exponential
prior
,
backoff
version
,
marginals
,
approx
imation
,
backoff
kneser-ney
,
discounting
,
result
,
equivalent
,
neg
ative
value
,
interpolated
version,kneser-ney
smoothing
,
interpolated
version
,
kneser
ney
,
serf
marginals
,
sec
ondary
distribution
,
primary
distri
bution
,
several
effect
,
equivalent
,
5
e
xperimental
result
,
section
,
experimental
result
,
exponential
prior
,
gaussian
prior
,
different
data
set
,
inspire
improvement
,
experiment
,
language
model
ex
periments
,
single
variance
,
gaus
sian
,
exponential
prior
,
param
eter
,
variance
,
language
modeling
experiment
,
variance
,
unigram
,
bigram
,
trigram
model
,
exponential
prior
,
actual
examination
,
data
set,grammar-checking
data
,
ver
sion
,
small
amount
,
matter
,
large
amount
,
distribu
tion
,
parameter
value
,
exper
iment
,
a
g
aussian
prior
,
large
amount
,
parameter
,
training
instance
,
parameter
re
,
distribution
,
param
eters
,
distribution
,
machine
learning
community
,
examine
distribution
,
model
parameter
,
good
way
,
inspiration
,
parameter
,
enough
data
,
correct
form
,
exact
value,result,figure,histogram,distribution,upside-down
parabola
,
distribution
,
triangle
,
bottom
,
extent
,
exponential
prior
,
problem
,
argu
ment
,
accuracy
,
next
experiment
,
param
eters
,
exponential
prior
,
exponential
prior
,
application
,
-
2o
course
,
parameter
,
different
prior
,
little
data
,
technique
,
inspiration
,
evidence
,
parameter
,
mixture
,
gaussians
,
approximation
,
instance
,
true
bayesian
posterior
,
f
p
ar
,
uc
ke
t
v
alue
,
lambda
figure
,
histogram
,
value
ments
,
error
rate
,
small
data
set
,
sentence
,
training
data
,
different
confusable
word
,
training
sentence
,
ex
amples
,
confusable
word
pair
,
interest
,
actual
number
,
training
example,word-pair
,
different
prior
,
gaussian
,
exponential
prior
,
single
prior
,
ten
pair
,
setting
,
geometric
average
error
rate
,
exponential
prior
,
gaussian
,
cheating
,
different
word
pair
,
parameter
,
result
,
exponen
tial
,
gaussian
prior
,
test
set
word
,
experiment
,
consistent
difference
,
matter
,
small
amount
,
training
data,experiment,collaborative-filtering
style
task
,
television
show
recommendation
,
nielsen
data
,
dataset
,
definition
,
random
train
test
split
,
result
,
experiment
,
different
prior
,
section
,
training
data
,
feature
,
test
data
,
a
g
aussian
prior
,
cf
score
,
exponential
prior
,
large
improvement
,
experiment
,
language
modeling
,
mixed
success
,
train
ing
data
,
small
model
,
mat
ters
,
wsj
corpus
,
trigram
model,cluster-based
speedup
,
goodman
,
test
data
,
standard
language
,
perplexity
,
experiment
,
katz
smoothing,version,good-turing
,
perplexity,discounting,maxent,plexity,variation,good-turing
,
exponential
prior
,
exponential
prior
,
perplex
ity
,
a
g
aussian
prior,perplexity,kneser-ney
smoothing
,
plexity
,
exponential
prior
,
aussian
prior
,
terpolated
kneser-ney
smoothing
,
known
smoothing
technique
,
goodman
,
parameter
,
time
consuming
,
good
turing
,
meth
od
,
method
,
exponential
prior
,
perplexity
,
exponential
prior
,
entropy
measure
,
gaussian
prior
,
accuracy
,
exponential
prior
,
informa
tion
,
variation
,
informa
tion
,
entropy
,
gaussian
,
6
c
onclusion
,
exponential
prior
,
maxent
mod
el
,
simple
update
formula
,
plement
,
ob
servations
,
constraint
,
underlying
model
,
application
,
accuracy
,
improved
version,good-turing
,
perplexity
,
exponential
prior
explains
,
discount
,
alternative
,
dirichlet
pri
or,kneser-ney
smoothing
,
performing
,
technique
,
language
modeling
,
future
,
technique
,
distribution
,
model
pa
rameters
,
problem
,
laplacian
exponential
,
perfor
mance
,
problem
,
observation
,
acknowledgment
,
john
platt
,
lapla
cian
prior
,
chris
meek
,
helpful
discussion
,
jeff
bilmes
,
version
,
thanks
,
stan
chen
,
roni
rosenfeld
,
derivation
,
exponential
prior
,
derivation
,
gaussian
prior
,
reference
,
paucity
,
data
problem
,
berger
,
stephen
,
della
pietra
,
vincent
,
della
pietra
,
maximum
entropy
approach
,
natural
language
processing
,
computational
linguis
tic
,
stanley
,
joshua
goodman
,
empir
ical
study
,
technique
,
language
mod
,
computer
speech
,
language
,
october
,
stanley
chen
,
ronald
rosenfeld
,
survey
,
technique
,
ee
trans
,
speech
,
darroch
,
ratcliff
,
iterative
scaling,log-linear
model
,
annals
,
mathe
matical
statistic
,
stephen
della
pietra
,
vincent
della
pietra
,
sta
tistical
modeling
,
maximum
entropy
,
manuscript
,
stephen
della
pietra
,
vincent
della
pietra
,
john
laf
ferty
,
feature
,
random
field
,
ee
transaction
,
pattern
analysis
,
machine
intelli
gence
,
balaji
krishnapuram
,
lawrence
carin
,
alexander
,
hartemink
,
population
frequency
,
specie
,
estimation
,
population
parameter
,
biometrika
,
joshua
goodman
,
tropy
training
,
ica
ssp
,
joshua
goodman
,
sequential
,
iterative
scaling
,
christopher
meek
,
david
hecker
man
,
collaborative
filtering
system
,
posterior
,
weight
,
evidence
,
proceed
ings
,
method
,
maximum
entropy
e
timation
,
relaxed
constraint
,
john
hop
,
university
language
modeling
workshop
,
reinhard
kneser
,
hermann
ney,backing-off
,
m-gram
language
modeling
,
raymond
lau
,
adaptive
statistical
language
mod
,
master
,
extension
,
january
,
hermann
ney
,
ute
essen
,
reinhard
kneser
,
probabilistic
dependence
,
stochastic
language
modeling
,
computer
,
speech
,
language
,
james
theiler
,
feature
selection
,
august
,
adwait
ratnaparkhi
,
maximum
entropy
model
,
natural
language
ambiguity
resolution
,
thesis
,
university
,
ratnaparkhi
,
maximum
en
tropy
approach
,
sentence
boundary
,
ronald
rosenfeld
,
adaptive
statistical
language
modeling
,
a
m
aximum
entropy
approach
,
carnegie
mellon
university
,
robert
tibshirani
,
regression
shrinkage
,
se
lection
,
technical
report
,
williams
,
bayesian
regularization
,
a
l
aplace
,
neural
computation
,
a
d
erivation
,
update
equation
,
iteration
,
increase
,
objective
function
,
con
straint
,
equation
,
gaussian
prior
,
function
,
important
property
,
function
,
whenever
,
constraint
,
equa
tion
,
modify
equation
,
jensen
,
inequality
,
convex
function
,
probability
distribution
,
equation
,
partial
derivative
,
maximum
occurs
,
version
,
improved
iterative
scaling
,
exponential
prior
,
vari
ations
,
generalized
iterative
scaling
,
algorithm
,
equation
,
derivation
,
slack
parameter
,
slack
parameter,near-zero
variance
,
practice
,
equation
,
illegal
new
value
,
monotonicity
,
equation
,
respect
,
legal
value
,
joshuago
microsoft
,
com
abstract
,
speedup
,
conditional
maxi
mum
entropy
model
,
algorithm
,
simple
vari
ation
,
generalized
iterative
scaling
,
magnitude
,
number
,
constraint
,
way
speed
,
model
pa
rameters
,
algorithm
,
algorithm
,
memory
,
improvement
,
maximum
entropy
problem
,
ntroduction
conditional
maximum
entropy
model
,
variety
,
natural
language
task
,
language
modeling
,
rosenfeld
,
part
of-speech
tagging
,
prepositional
phrase
attachment
,
ratnaparkhi
,
word
selection
,
machine
translation
,
berger
,
ing
sentence
boundary
,
reynar
,
ratnaparkhi
,
maximum
entropy
,
maxent
,
typical
training
algorithm
,
maxent
,
ratcliff
,
computer
time
,
single
model
,
several
attempt
,
max
ent
training
,
della
pietra
,
khu
danpur
,
goodman
,
appli
cability
,
limited
number
,
application
,
darroch
,
ratcliff
,
describe
gis
,
joint
probabil
ities
,
mention
,
fast
variation
,
conditional
maxent
com
munity
,
fast
variation
,
conditional
probability
,
problem
,
traditional
speedup
technique
,
good
speedup
,
speedup
,
magnitude
,
typical
problem
,
disadvantage
,
many
possi
ble
output
value
,
memory
,
technique
,
speedup
technique
,
goodman
,
disadvantage
,
conditional
maxent
model
,
input
vector,output,so-called
indicator
function
,
feature
value
,
particular
property
,
weight
,
instance
,
word
sense
disambiguation
,
context
,
oc
currence
,
con
text
,
financial
sense
,
large
positive
number
,
maxent
model
,
several
valuable
proper
tie
,
constraint
satisfaction
,
many
time
,
training
data
,
parameter
,
many
time
,
model
pre
,
property
,
equality
,
constraint
,
addi
tional
property
,
equa
tion
,
maxent
model
,
probability
,
training
data
,
property
,
max
ent
model
,
uniform
distribution
,
satisfaction
,
maximum
entropy
model
,
simple
algorithm
,
iteration
,
di
rection
,
likelihood
,
p
roceedings
,40
th
annual
meeting
,
association
,
step
size
,
likelihood
,
training
data
increase
,
iteration
,
converges
,
global
optimum
,
guaran
tee
,
step
size
,
maximum
number
,
active
con
straints
,
maxent
model
,
ability
,
many
different
kind
,
information
,
weakness
,
gis
mean
,
maxent
model
,
variation
,
parameter
,
beginning
,
new
algorithm
converges
,
original
one
,
sequential
learning
,
improvement
,
subcomputa
tions
,
combination
,
improvement
,
magnitude
,
2
a
lgorithms
,
classic
gis
algorithm
,
gis
converges
,
simple
idea
,
observed
,
problem
,
interaction
,
update
,
iteration
,
similar
effect
,
factor
,
total
value
,
update
,
log
observed
,
update
,
joint
model
,
darroch
,
ratcliff
,
conditional
version
,
unpublished
,
rosenfeld
,
pseudocode
,
figure
,
number
,
instance
,
published
version
,
gis
algorithm
require
,
clusion
,
indicator
function
,
number
,
constraint
,
applies
,
practice
,
indicator
function
,
includ
,
slack
indicator
,
corresponding
,
training
instance
,
output
,
output
,
i
f
igure
,
iteration
,
generalized
iterative
scal
,
number
,
indicator
function
,
number
,
output
class
,
data
structure
listing
,
training
instance
,
variation
,
indicator
function
,
update
,
indicator
function
,
par
ticular
,
first
change
,
training
instance
,
indicator
function
,
notice
,
data
structure
,
training
data
,
sparse
matrix
,
indicator
function,non-zero
value
,
instance
,
sparse
matrix,instance,non-zero
value
,
indicator
,
matrix
,
next
change
,
inner
loop
,
feature
,
meaning
,
change
,
original
version
,
feature
,
feature
,
equation
,
improves
,
iteration
,
global
optimum
,
feature
,
output
,
instance
,
output
,
instance
,
figure
,
iteration
,
feature
,
many
max
ent
application
,
factor
,
last
change
,
speedup
,
instance
,
output
,
normalizing
factor
,
invariant
,
important
change
,
substantial
speedup
,
transformed
algorithm
,
figure
,
equation
,
single
global
optimum
,
convergence
proof
,
darroch
,
ratcliff
,
convergence
,
algorithm
,
joint
model
,
section
,
space
re
quirements
,
space
result
,
number
,
output
class
,
small
amount
,
section
,
technique
,
many
output
class
,
speedup
,
number
,
output
,
space
issue
,
training
data
,
sparse
matrix,non-zero
indicator
function
,
instance
,
output
,
matrix
,
relationship
,
scg
clearer
,
algorithm
,
figure
,
wasted
space
,
instance
,
simple
array
,
matrix
,
meaning
,
algorithm
,
time
analysis,space-wasting
technique
,
training
matrix
,
indicator
function
,
appear
,
training
data
,
training
data
,
albeit
,
differ
ent
form
,
matrix
,
outermost
index
,
indicator
function
,
observed
,
many
problem
,
problem
,
language
modeling
,
overall
space,optimization,section,algorithm,iteration,instance,output,non-zero
indicator
function
,
practice
,
notice
,
top
loop,non-zero
indicator
function
,
output
,
training
instance
,
training
matrix
,
bottom
loop
,
top
loop
,
training
data
,
iteration
,
iteration
,
practice
,
im
plementation
,
iteration
,
speedup
,
step
size
,
update
,
update
,
factor
,
faster
,
many
application
,
speedup
,
step
size
,
speedup
,
fact
observe
,
improvement
,
caching
,
caching
,
iteration
,
iteration
,
caching
,
key
component
,
caching
,
iteration
,
iteration
,
convergence
,
reduction
,
speedup
,
step
size
,
exact
speedup
,
step
size
,
many
factor
,
correlated
feature
,
problem
,
maxent
training
data
,
main
memory
,
reasonable
time
,
access
,
training
data
,
large
amount
,
reasonable
efficiency
,
random
access
,
main
mem
ory
,
analysis
,
train
ing
data
,
precomputed
sparse
matrix,non-zero
value
,
training
instance
,
output
,
application
,
lan
guage
modeling
,
thought
,
data
structure
,
rosenfeld
,
technique
,
maximum
entropy
,
maximum
entropy
model
,
natu
rally
,
constraint
,
practice
,
enough
constraint
,
training
data
,
rosenfeld
,
technique
,
a
g
au
,
parameter
,
constraint
,
test
data
,
probability
,
train
ing
data
,
different
objective
function
,
probability
,
training
data
,
prior
probability
,
probability
,
simple
normal
distribu
tion
,
standard
deviation
,
rosenfeld
,
modified
update
rule
,
update
,
observed
,
2
s
cgis
,
similar
way
,
update
rule
,
observed
,
3
p
revious
work
,
sequential
updating
,
joint
probability
,
original
paper
,
darroch
,
ratcliff
,
conditional
model
,
nlp
community
,
max
ent
model
,
conditional
model
,
knowledge
,
main
reason
,
speedup
,
conditional
model
,
joint
model
,
con
ditional
model
,
speedup
,
known
us
,
conditional
maxent
model
,
language
mod
,
rosenfeld
,
number
,
output
class
,
vocabulary
size
,
application
,
number
,
train
ing
instance
,
algorithm
,
examination
,
definition
,
related
work
,
speedup
,
several
previous
attempt
,
maxent
modeling
,
best
known
,
della
pietra
,
algorithm
,
function
,
equation
,
special
case
,
reduces
,
equation
,
training
instance
,
iis
update
,
application
,
difference
,
limited
experiment
,
lafferty
,
factor
,
speedup
,
equation
,
algorithm
,
newton
,
method
,
function
,
training
data
,
right
hand
side
,
equation
,
bucketing
,
first
option
,
complexity
,
algorithm
,
update
rule
,
observed
,
many
model
type
,
equation
,
reduces
,
normal
scg
,
joint
probability
,
jelinek
,
conditional
probabili
tie,binary-valued
feature
,
algorithm
,
jelinek
,
advantage
,
speedup
,
variation
,
feature
,
improvement
,
smoothing
technique
,
rosenfeld
,
conditional
maxent
model
,
language
mod
el
,
space
precludes
,
full
discussion
,
technique
,
unigram
caching
,
cluster
expan
sion
,
lafferty
,
khudanpur
,
word
clustering
,
goodman
,
word
clustering
,
factor
,
speedup
,
additional
advantage
,
scg
speedup
,
large
number
,
output
,
speedup
,
problem
,
many
output
,
notice
,
key
loop
,
output
,
certain
optimization
,
length
,
number
,
output
,
change
,
language
model
,
vocabulary
size
,
output
,
word
cluster
,
clus
ter
,
output
,
output
,
mod
el
,
example
,
practice
,
speedup
,
speedup
,
factor
,
model
form
,
original
model
,
perplexity
,
perplex
ity
,
single
model
,
disadvantage
,
word
clustering
technique
,
multiple
level
,
instance
,
superclusters
,
speech
,
cluster
,
similar
word
,
speech
,
level
model
,
technique
,
2
y
level
,
output
,
mean
ing
,
space
requirement
,
step
size,cluster-based
speedup
work
,
ner
loop
,
whchi
scg
share
,
technique
,
speedup
,
preliminary
language
,
experi
ments
,
analysis
,
recent
unpublished
work
,
experimental
setting
,
dense
feature
,
many
natural
language
task
,
result
,
particu
lar
,
version
,
conjugate
gradient
descent
,
problem
domain
resembles
minka
,
gradient
descent
,
technique
,
technique
,
realistic
task
,
collins
,
sequential
version
,
maxent
,
difference
,
algorithm
,
collins
,
feature
,
collins
,
feature
,
algorithm
,
storage
,
algorithm
,
fast
imple
mentation
,
storage
,
training
data
matrix
,
feature
,
transpose
,
training
data
matrix
,
update
,
xperimental
result
,
section
,
experimental
result
,
magni
tude
,
num
ber,non-zero
indicator
function
,
method
,
performance
,
performance
,
maxent
model
,
ob
jective
function
,
test
data
,
percent
correct
,
test
data
,
objective
function
,
smoothing
,
equation
,
probabil
ity
,
training
data
,
probability
,
interesting
measure
,
percent
correct
,
test
data
,
test
corpus
,
training
,
problem
,
feature
set
,
confusable
word
,
problem
,
data
size
,
different
pair
,
good
deal
,
different
behavior
,
standard
set,feature,window,part-of-speech
tag
,
window
,
tag
feature
,
window
,
alto
gether
,
feature
type
,
many
thousand
,
feature
,
exact
model
,
training
,
test
instance
,
performance
,
scg
versus
gis
,
different
ax
,
important
variable
,
number
,
feature
,
addition
,
feature
type
,
feature
set
,
feature
type
,
win
dow
,
feature
,
feature
type
,
window
,
window
,
unigram
,
window
,
training
data
size
,
configuration
,
feature
type
,
train
ing
,
column
,
different
confusable
word
,
column
,
elapsed
time
,
result
,
iteration
,
performance
level
,
iteration
,
average
,
column
,
objective
function
,
equation
,
column
,
test
entropy
,
column
,
test
error
rate
,
measurement
,
factor
,
average
,
experiment
,
objective
function
,
smoothing
,
training
entropy
,
increase
,
word
train
ing
,
feature
type
,
second
,
iteration
,
second
,
feature
type
,
sec
onds
,
second
,
many
experiment
,
datasets
,
feature
type
,
time
scale
,
data
size
,
objec
ent
cor
accept
,
affect
effect
,
peace
piece
,
weather
,
average
,
baseline
,
objec
ent
cor
accept
,
affect
effect
,
peace
piece
,
weather
,
average
,
baseline
,
criterion
,
test
entropy
,
percentage
correct
,
increase
,
smoothing
,
result
,
medium
feature
set
,
speedup
,
feature
set
,
feature
type
,
speedup
,
medium
sized
feature
,
feature
type
,
base
line
speedup
,
feature
,
notice
,
experiment
,
objective
function
,
test
data
entropy
,
test
data
correctness
,
objective
function
measure
,
data
entropy
,
test
data
entropy
,
ror
rate
,
chance
,
unexpected
result
,
possibility
,
test
data
,
ent
cor
accept
,
affect
effect
,
peace
piece
,
weather
,
small
feature
,
feature
type
,
ent
cor
accept
,
affect
effect
,
peace
piece
,
principal
principle
,
weather
,
medium
feature
,
feature
type
,
entropy
,
test
data
error
rate
,
alternative
explanation
,
dif
ferent
data
set
,
newsgroups
,
technique
,
ex
act
setting
,
effect
,
effect
,
xxx
case
,
effect
,
gis
beat
scg
,
explanation
,
early
stopping
,
explanation
,
experiment
,
baseline
experiment
,
training
data
size
,
individual
speedup
,
different
size
,
5
d
iscussion
,
many
reason
,
speedup
,
application
,
active
learning
,
parameter
optimization
,
feature
,
selection
,
many
round
,
maxent
,
fast
algo
rithms
,
winnow
,
ex
perience
,
problem
,
maxent
model
,
classifier
,
winnow
,
furthermore
,
many
fast
classification
algo
rithms
,
winnow
,
output
probabil
ities
,
precision
recall
curve,non-equal
tradeoff
,
false
positive
,
false
negative
,
output
,
classifier
,
many
application
,
maxent
,
huge
amount
,
lan
guage
modeling
,
maxent
model
,
experiment
,
instance
,
language
,
experiment
,
single
model
,
clearly
,
speedup
,
technique
,
significant
speedup
,
magnitude
,
training
data
matrix
,
extra
array
,
standard
gis
,
model
type
,
speedup
,
feature
type
,
many
interacting
feature
,
maxent
model
,
additional
resource
,
large
number
,
output
class
,
much
space
,
standard
gis
,
large
number
,
output
class
,
speedup
technique
,
goodman
,
additional
speedup
,
space
requirement
,
real
impediment
,
applicable
gain
,
acknowledgement
,
ciprian
chelba
,
stan
chen
,
chris
meek
,
anonymous
reviewer
,
useful
comment
,
reference
,
paucity
,
data
problem
,
berger
,
stephen
,
della
pietra
,
vin
cent
,
della
pietra
,
maximum
entropy
approach
,
natural
language
processing
,
compu
tational
linguistics
,
roukos
,
unpublished
,
transla
tion
model
,
learned
feature
,
approximation
,
ability
distribution
,
information
,
rosenfeld
,
gaussian
prior
,
maximum
entropy
model
,
tech
nical
report
cmu
cs-99-108
,
computer
science
department
,
carnegie
mellon
university
,
michael
collins
,
robert
,
schapire
,
yoram
singer
,
logistic
regression
,
adaboost
,
bregman
distance
,
machine
learning,darroch,ratcliff,log-linear
model
,
annals
,
mathematical
statistic
,
stephen
della
pietra
,
vincent
della
pietra
,
john
lafferty
,
feature
,
random
field
,
ee
transaction
,
pattern
analysis
,
machine
intelligence
,
joshua
goodman
,
fast
maximum
entropy
training
,
ica
ssp
,
frederick
jelinek
,
statistical
method
,
mcc
allum
,
con
ditional
random
field
,
probabilistic
model
,
sequence
data
,
john
lafferty,gibbs-markov
model
,
science
,
statistic
,
proceeding
,27
th
symposium
,
interface
,
thomas
minka
,
algorithm
,
maximum
likelihood
logistic
regression
,
medium
,
tpminka
paper
,
adwait
ratnaparkhi
,
maximum
entropy
model
,
natural
language
ambiguity
resolu
tion
,
thesis
,
university
,
ratnaparkhi
,
maximum
entropy
approach
,
sentence
bound
aries
,
ronald
rosenfeld
,
adaptive
statistical
lan
guage
modeling
,
a
m
aximum
entropy
approach
,
thesis
,
carnegie
mellon
university
,
khudanpur
,
method
,
maximum
entropy
language
model
,
ics
lp
,
volume
,
asymmetric
clustering
,
statistical
language
modeling
jianfeng
gao
m
icrosoft
research
,
asia
beijing
,
com
joshua
,
goodman
m
icrosoft
research
,
redmond
washington
,
com
guihong
cao1
d
epartment
,
computer
science
,
engineering
,
tianjin
university
,
china
h
ang
li
m
icrosoft
research
,
asia
beijing,n-gram
model
,
stochastic
model
,
next
word
,
previous
word
,
conditional
word
,
word
sequence,n-gram
model,variant,n-gram
model
,
similar
word
,
cluster
,
different
cluster
,
conditional
word
,
classical
cluster
model
,
cluster
,
formal
definition
,
detail
,
methodology
,
effectiveness
,
realistic
application
,
japanese
kana-kanji
conversion
,
e
xperimental
result
,
substantial
improvement
,
comparison
,
classical
cluster
model
,
word
n-gram
model
,
model
size
,
ur
analysis,high-performance
,
asymmetry,n-gram
model
,
many
application
,
speech
recognition
,
machine
translation
,
asian
language
text
input
jelinek
,
stochastic
model
,
next
word
,
previous
n-1
word
,
conditional
word,n-gram
model
,
variant
,
word
n-gram
model
,
similar
word
,
cluster
,
effective
way
,
data
sparseness
problem
,
memory
,
realistic
application
,
recent
research
yamamoto
,
different
cluster
,
conditional
word
,
classical
cluster
model
,
cluster
,
similar
model
,
previous
study
,
goodman
,
yamamoto
,
several
issue
,
effective
methodology
,
thorough
comparative
study
,
classical
cluster
model
,
word
model
,
realistic
application
,
analysis
,
reason
,
formal
definition
,
detail
,
methodology
,
asymmetric
clustering
algorithm
,
different
metric
,
conditional
word
,
method
,
model
parameter
optimization
,
optimal
cluster
number
,
different
cluster
,
real
application
,
japanese
kana-kanji
conversion
,
phonetic
kana
string
,
proper
japanese
orthography
,
performance
,
ur
result
,
substantial
improvement
,
comparison
,
classical
cluster
model
,
word
n-gram
model,high-performance
,
p
roceedings
,40
th
annual
meeting
,
association
,
structure
,
smoothing
,
asymmetry
,
section
,
introduces
,
research
topic
,
section
,
review
related
work
,
section
,
describes
,
detail
,
method
,
model
construction
,
section
,
japanese
kana-kanji
conversion
task
,
main
experiment
,
discussion
,
finding
,
conclusion
,
section
,
work
large
amount
,
previous
research
,
clustering
,
cluster
,
kneser
,
sagisaka
,
ueberla
,
bellegarda
,
small
difference
,
performance
,
different
technique
,
cluster
,
research
,
novel
technique
,
cluster
,
different
cluster
,
conditional
word
,
discussion
,
extension
,
several
study
,
first
similar
cluster
model
,
goodman
,
clustering
technique
,
goodman
,
detailed
description
,
asymmetric
clustering
algorithm
,
impact
,
asymmetric
clustering
,
performance
,
cluster
model
,
thorough
empirical
study
,
technique
,
asian
language
,
application
,
perplexity
result
,
real
application
,
simplified
bigram
ac
,
a
c
hinese
text
input
system
gao
,
technique
,
a
c
hinese
language
modeling
system
,
contribution
,
language
modeling
improvement
,
word
trigram
model
,
rosenfeld
,
practical
value
,
realistic
application
,
memory
constraint
,
tradeoff
,
lm
performance
,
perplexity
,
performance
,
different
model
,
pruning
,
similar
size
,
symmetric
cluster
model
,
model
t
,
next
word
wi
,
history
,
conditional
probability
,
trigram
approximation
,
wi
wi-2wi-1
,
next
word
,
preceding
word
,
different
cluster
,
different
position
,
predicted
word
,
cluster
,
cluster
,
conditional
cluster
,
cluster
,
probability
,
preceeding
word
,
product
,
probability
,
probability
,
predicted
cluster
pwi
,
conditional
cluster
,
probability
,
cluster
pwi
,
conditional
cluster
cwi-2
,
cwi-2cwi-1pwi
,
sub-models
,
word
sub-model
,
data
sparseness
problem
,
backoff
scheme
,
parameter
estimation,sub-model
,
backoff
scheme
,
probability
,
gram
estimate
,
basic
idea
,
different
cluster
,
conditional
word
,
cluster
,
conditional
word
,
symmetric
cluster
model
,
example
,
predicted
word
,
cluster
,
conditional
word
,
different
cluster
,
cluster
,
factor
,
metric
,
impact
,
factor
,
asymmetric
,
basic
criterion
,
statistical
clustering
,
resulting
probability
,
resulting
perplexity
,
training
data
,
technique
brown
,
attempt
,
average
mutual
information
,
adjacent
cluster
,
cluster
,
conditional
word
,
technique
symmetric
clustering
,
cluster
,
cluster
,
asymmetric
clustering
,
different
cluster
,
conditional
word
,
conditional
word
,
perplexity
,
training
data
,
bigram
,
wi
wi-1
,
total
number
,
cluster
conditional
cluster
,
predicted
word
,
perplexity
,
cluster
,
cluster
,
wp
wwp
,
wwp
wwpwwp
,
wp
wwp
,
wpwp
wp
,
clustering
,
selection
,
cluster
,
op
posite
,
conditional
clustering
,
lillian
lee
,
justification
,
predictive
cluster
,
clustering
tool
,
program
,
raw
count
,
technique
,
binary
branching
tree
,
hard
cluster
model
,
clustering
tree
,
single
leaf
,
predicted
word
,
conditional
word
,
approach
,
algorithm
,
iteration
,
cluster
,
cluster
,
splitting
,
maximal
entropy
decrease
,
iteration
,
cluster
,
convergence
,
entropy
decrease
,
algorithm
,
algorithm
,
parameter
optimization
asymmetric
,
result
,
certain
level
,
wide
variety
,
different
number
,
cluster
,
instance
,
actual
number
,
cluster
,
cluster
,
tree
cut
,
cluster
,
single
word
,
ac
model
,
realistic
application
,
memory
constraint
,
correct
balance
,
model
size
,
performance
,
stolcke
,
method
,
many
acm
,
different
model
size
,
experiment
,
technique
,
performance
,
perplexity
,
lm
technique
,
model
size,stolcke,pruning,entropy-based
cutoff
3
n
otice
,
experiment
,
basic
top-down
algorithm,cluster,experiment,quality,cluster,perplexity,cluster,method,n-grams
,
change
perplexity
,
threshold
,
threshold
,
word
sub-model
,
wi
cwi-2kcwi-1kpwil
,
different
parameter
,
number
,
cluster
,
pruning
threshold,brute-force
approach
,
large
number
,
parameter
,
large
number
,
combination
,
parameter
,
alternative
technique
,
simple
math
show
,
perplexity
,
wi
cwi-2kcwi-1kpwil
,
perplexity
,
perplexity
,
word
sub-model
,
wi
cwi-2kcwi-1kpwil
,
overall
model
,
large
number
,
pruning
threshold
tc
,
perplexity
,
large
number
,
separate
threshold
tw
,
compatible
pair
,
perplexity
,
overall
model
,
allows
,
search
,
large
search
space
,
4
e
xperimental
result
,
discussion
,
japanese
kana-kanji
conversion
task
japanese
kana-kanji
conversion
,
standard
method
,
japanese
text,syllabary-based
kana
string
,
appropriate
combination
,
ideographic
kanji
,
similar
problem
,
recognition
,
acoustic
ambiguity
,
performance
,
number
,
character
,
phonetic
string
,
number
,
character
,
correct
transcript
,
possible
word
string
,
typed
phonetic
symbol
string
,
word
string,5-10
error
,
con
version
,
real
data
,
wide
variety
,
domain
,
setting
,
experiment
,
japanese
newspaper
corpus
,
nikkei
newspaper
corpus
,
yomiuri
newspaper
corpus
,
text
corpus
,
lexicon
,
experiment
,
model
performance
,
perplexity
,
performance
,
pilot
experiment
,
subset
,
nikkei
newspaper
corpus
,
nikkei
corpus
,
language
model
training,held-out
data
,
data
set
,
japanese
kana-kanji
conversion
experiment
,
language
model
,
subset
,
nikkei
newspaper
corpus
,
parameter
optimization,subset,held-out
data
,
yomiuri
newspaper
corpus
,
subset
,
yomiuri
newspaper
corpus
,
experiment
,
word
cluster
,
bigram
count
,
training
corpus,out-of-vocabulary
word
,
perplexity
,
error
rate
computation
,
impact
,
asymmetric
clustering
,
section
,
metric
,
cluster
,
cluster
,
cluster
,
conditional
cluster
,
cluster
,
series
,
experiment
,
impact
,
different
type
,
cluster
,
variant
,
trigram
acm
,
predictive
cluster
model
,
wi
wi-2wi-1
,
conditional
cluster
model
,
wi
wi-2wi-1
,
conditional
word
,
ibm
model
,
special
case
,
cluster
,
conditional
word
,
cluster
trigram
model
,
perplexity
,
cer
result
,
japanese
kana
kanji
conversion
,
different
type
,
cluster
,
cluster
type
,
number
,
cluster
,
result
,
benefit
,
different
cluster
,
different
position
,
cluster
trigram
model
,
result
,
cluster
,
wi
wi-2wi-1
,
performance
,
cluster
wi
,
predictive
cluster
pwi
,
ibm
model
,
result
,
conditional
cluster
,
conditional
word
,
ibm
model
,
pwi
cwi-2cwi-1
,
perplexity
,
model
cer
,
--
p
erplexity
,
--
p
re
model
cer
,
--
p
erplexity
,
comparison
,
different
cluster
type,cluster-based
model
,
impact
,
parameter
optimization
,
section
,
pilot
experiment
,
optimal
parameter
set
,
section
,
ibm
model
,
superiority
,
acm
result
,
structure
,
section
,
performance
,
perplexity
,
total
number
,
parameter
,
parameter
,
bigram
,
trigram
,
parameter
,
normalization
parameter
,
parameter
,
unigram
,
conditional
cluster
model
,
parameter
,
figure
,
performance
,
number
,
cluster
,
word
trigram
model
,
conditional
cluster
model
,
conditional
cluster
model
,
discard
information
,
predictive
cluster
model,wi-2wi-1pwil
,
sample
setting
,
parameter
,
figure
,
simplicity
,
pruning
threshold
value,sub-models
,
predictive
cluster
model
,
perplexity
result
,
cluster
,
baseline
word
trigram
model
,
conditional
word
,
different
number
,
cluster
,
combined
cluster
model
,
different
value
,
threshold
,
model
parameter
,
pilot
experiment
result
,
predictive
cluster
model
,
combined
cluster
model
,
outer
envelope
,
model
type
,
perplexity
,
first
point
,
worse
point
,
result
,
figure
,
cluster
number
,
ibm
model
,
performance
,
ibm
model
,
experiment
,
cluster
model
,
result
,
predictive
cluster
model
,
good
performance
,
combined
one
,
combined
model
,
model
size
,
predictive
cluster
model
,
special
case
,
combined
model
,
conditional
position
,
experiment
,
combined
model
,
good
performance
,
large
number
,
cluster
,
conditional
word
,
interesting
analysis
,
sample
setting
,
parameter
,
combined
cluster
model
,
figure
,
parameter
setting
,
several
level
,
model
size
,
notice
,
model
size
,
predictive
cluster
model
,
stolcke
pruning
parameter
,
f
irst
,
notice
,
pruning
parameter
,
column
,
theory
,
relative
entropy,predicts,parameter,cwi-2kcwi-1kpwil
,
traditional
ibm
clustering,wi-20wi-10wil
,
size
pe
rp
lex
ity
,
cluster
,
cluster
,
cluster
word
trigram
f
igure
,
comparison
,
conditional
model
,
different
number
,
cluster
,
pe
rp
lex
ity
,
cluster
,
cluster
,
cluster
,
cluster
word
trigram
f
igure
,
comparison
,
predictive
model
,
different
number
,
cluster
,
pe
rp
lex
ity
acm
ibm
word
trigram
predictive
model
f
igure
,
comparison
,
word
trigram
model
type
,
cluster
,
conditional
word
,
ur
result
,
figure
,
performance
,
ibm
model
,
magnitude
,
addition
,
symmetric
cluster
model
,
traditional
ibm
model
,
assumption
,
result
,
unequal
setting
,
assumes
,
optimal
setting
,
exact
opposite
,
wi
cwi-2kcwi-1kpwil
,
wi
wi-2wi-1pwil
,
previous
word
,
independence
assumption
,
word
dependency
,
something
,
previous
word
,
predictor
,
previous
cluster
,
important
finding
,
setting
,
unpruned
model
,
normal
trigram
model
,
wi
cwi-2kcwi-1kpwil
,
unpruned
model
,
wi
wi-2wi-1
,
analysis
,
compression
,
structure
,
cluster
,
cluster
,
structure
,
regularity
,
performance
,
word
trigram
model
,
6
a
ll
,
6
a
ll
,
sample
parameter
setting
,
cer
result
,
present
cer
result
,
japanese
kana-kanji
conversion
system
,
method
,
practice
,
o
ne
,
common
method
,
backoff
n-gram
model,n-gram
probability
,
weight
,
tree
structure
,
hypothetical
root
node
,
unigram
node
,
first
level
,
unigram
node
,
turn
branch
,
bigram
node
,
second
level,storage,n-gram
probability
,
wi
wi-1
,
weight,wi-2wi-1
,
bigram
,
node
array
,
clarkson
,
rosenfeld
,
tree
structure
,
representation
issue
,
example
,
weight
,
bigram
,
node
array
,
separate
tree
,
probability
,
backoff
weight
,
result
,
tree
structure
,
practice
,
word
sub-model
,
wi
cwi-2kcwi-1kpwil
,
effect
,
storage
structure
,
addition
,
several
technique
,
model
parameter,n-gram
probability
,
backoff
weight
,
storage
space
,
example
,
point
value,n-gram
probability
,
backoff
weight
,
small
number
,
quantization
level,quantization,n-gram
probability
,
backoff
weight
list
,
separate
quantization
level
look-up
table,parameter,8-bit
quantization
,
performance
decline
,
experiment
,
ur
goal
,
tradeoff
,
performance
,
model
size
,
word
trigram
model
,
model
size,sub-models
,
specific
size
,
comparison
,
word
trigram
model
,
evaluation
,
significant
improvement
,
word
trigram
model
,
model
size
,
word
trigram
model
,
different
model
size
,
word
trigram
,
addition
,
structure
,
section
,
benefit
,
smoothing
,
probability
,
tuesday
,
cluster
wee
kday
,
probability
,
simple
math
show,decomposition,smoothing,consideration,probability,non-clustered
probability
,
instance
,
example
,
tuesday
,
example
,
phrase
,
wednesday
,
example
,
wee
kday
tuesday
,
order
model
,
wee
kday
,
estimate
,
experiment
,
several
test
set
,
different
backoff
rates4
,
backoff
rate
,
test
set
,
trigram
model
,
number
,
trigram
probability
,
backoff
bigram
probability
,
number
,
ce
result
,
word
trigram
model
,
figure
,
increase
,
backoff
rate
increase
,
word
trigram
model
,
difference
,
upward
trend
,
cer
difference
,
figure
,
result
,
smoothing
,
backoff
rate
increase
,
word
trigram
model
,
portion
,
benefit
,
smoothing
,
rate
er
ro
ate
word
trigram
model
acm
f
igure
,
backoff
rate
,
baseline
trigram
model
,
choice
,
tuesday
party
,
wee
kday
,
backoff
rate
er
ro
ate
di
ffe
re
nc
f
igure
,
onclusion
,
main
contribution
,
formal
definition
,
detail
,
methodology
,
asymmetric
clustering
,
optimal
cluster
number
,
positive
impact
,
performance
,
effectiveness
,
research
focus
,
technique
,
technique
,
algorithm
,
actual
representation
,
realistic
application
,
comparison
,
word
trigram
model
,
reason
,
superiority
,
instance
,
analysis
,
benefit
,
structure
,
hard
clustering
,
cluster
,
soft
clustering
,
cluster
,
pereira
,
utility
,
soft
clustering
,
method
,
single
cluster
,
information
,
interesting
question
,
technique
,
hard
clustering
,
soft
clustering
model
,
hard
clustering
model
,
multiple
cluster
,
training
instance
,
wi
wi-2wi-1
,
multiple
count,class-based
language
model
,
contextual
statistic
,
algorithm
,
latent
semantic
analysis,lass-based
n-gram
model
,
natural
language
,
computational
linguistics
,
cmu
cambridge
toolkit
,
rhodes
,
technique
,
language
model
,
application
,
asian
language
,
computational
linguistics
,
chinese
language
processing
,
unified
approach
,
statistical
language
,
asian
language
information
processing
,
progress
,
computer
speech
,
language
,
october
,
speech
recognition
,
reading
,
probability
,
language
model
component,acoustic,speech,technique,class-based
statistical
language
modeling
,
berlin
,
probabilistic
dependence
,
stochastic
language
,
speech
,
english
word
,
proceeding
,
annual
meeting
,
decade
,
statistical
language
modeling
,
ie
ee,mixed-order
markov
model
,
statistical
language
processing
,
backoff
language
model
,
transcription
,
understanding
workshop
,
algorithm
,
statistical
language
model
,
ee
transaction
,
speech
,
spoken
language
processing
using
multiple
word
cluster
,
annual
meeting
,
association
,
toulouse
,
connection
direction
,
proceeding
,
ie
ee
international
conference
,
acoustic
,
speech
,
signal
processing
,
phoenix
,
joshuago
microsoft
,
com
abstract
,
problem
,
stan
dard
technique
,
probabilistic
decision
list
,
simple
,
cremental
algorithm
,
prob
lem
,
variation
,
standard
sorting
algo
rithm
,
decision
list
,
similar
improvement
,
experimental
result
,
new
algorithm
,
error
rate
,
entropy
,
magnitude
,
standard
algorithm
,
1
i
ntroduction
decision
list
,
rivest
,
variety
,
natural
language
task
,
accent
restoration
,
yarowsky
,
word
sense
disam
biguation
,
yarowsky
,
past
tense
,
english
,
mooney
,
califf
,
sev
eral
problem
,
problem
,
standard
algorithm
,
probabilistic
deci
sion
list
,
incremental
algorithm
,
obvious
im
plementation
,
algorithm
,
new
algorithm
,
entropy
,
error
rate
,
formalism
,
word
sense
disambiguation
task
,
financial
sense
,
river
sense
,
decision
list
,
kearns
,
schapire
,
instance
,
probability
,
understanding
algorithm
,
cision
list
,
output
,
output
,
e
if
word
,
output
,
charles
,
occcurs
,
output
,
output
,
condition
,
matching
rule
,
algorithm
,
appropriate
probability
,
terminates
,
last
rule
,
trigger
,
probability
,
standard
algorithm
,
decision
list
,
yarowsky
,
entropy
,
decision
list
,
entropy
,
cision
,
expected
entropy
,
entropy
,
output
,
entropy
,
decision
list
,
many
rea
son
,
de
cision
list
,
data
analysis
tool
,
decision
list
,
factor
,
guideline
,
doctor
,
particular
drug
,
decision
list
,
easy
a
ssociation
,
computational
linguistics
,
philadelphia
,
p
roceedings
,
conference
,
empirical
method
,
natural
,
practice
,
standard
al
gorithm
,
decision
list
,
important
flaw
,
rule
order
,
important
way
,
al
gorithm
,
average
entropy
,
expected
entropy
,
location
,
simple
incremental
algorithm
,
basic
sorting
al
gorithm
,
algorithm
,
reverse
order
,
entropy
,
position
,
computation
,
algorithm
,
algorithm
,
section
,
traditional
algorithm
,
decision
list
,
detail
,
new
algorithm
,
new
algorithm
,
variation
,
detail
,
sim
plicity
,
algorithm
,
binary
output
case
,
algorithm
,
general
case
,
traditional
algorithm
decision
list
learner
,
test
data
,
test
data
,
se
ries
,
corresponding
result
,
instance
,
word
sense
disambiguation
task
,
correct
sense
,
probability
p
d
,
standard
way
,
entropy
,
test
data
,
entropy
,
many
justification
,
entropy
,
others
,
probability
distribution
,
possible
entropy
,
entropy
corresponds
,
probabil
ity
,
training
data
,
decision
list
,
possible
question
,
word
sense
disambiguation
,
question
,
complex
one
,
charles
,
training
data
,
system
,
number
,
train
ing
data
,
output
,
total
number
,
probability
,
answer
,
probability
,
training
data
,
training
data
,
instance
,
training
data
,
probability
,
estimate
,
latter
,
underestimate
,
estimate
,
good
man
,
interpolated
absolute
discounting
method
,
tradi
tional
algorithm
,
new
algorithm
,
smoothing
method
,
exact
smoothing
technique
,
relative
performance
,
algorithm
,
total
number
,
training
,
tal
number,probability,number,non-zero
y
,
question
,
par
ticular
,
occurences
,
question
,
training
sample
,
heldout
data
,
discount
,
probability
distribution
,
predicted
entropy
,
question
,
typical
training
algorithm
,
decision
list
,
training
data
,
predicted
entropy
,
question
,
question
,
entropy
,
output
,
decision
list
,
question
,
question
,
special
question
,
al
way
,
unigram
probability
,
question
,
entropy
,
new
algorithm
consider
,
weatherman
,
seattle
,
winter
,
following
,
seattle
weather
,
chance
,
lazy
weatherman
,
smart
weatherman
,
true
probability
,
chance
,
rain
tomorrow
,
wind
today
,
chance
,
rain
tomorrow
,
entropy
,
weatherman
,
lazy
weatherman
,
chance
,
rain
tomorrow
,
average
entropy
,
weatherman
,
chance
,
rain
tomorrow
,
entropy
,
smart
weatherman
,
chance
,
rain
tomorrow
,
entropy
,
smart
weatherman
,
entropy
,
lazy
weatherman
,
entropy
,
decision
list
equivalent
,
classic
learner
,
question
,
probability
,
proba
bility
,
entropy
,
output
,
output
,
output
,
course
,
third
rule
,
windy
day
,
probabiliy
,
weatherman
,
naive
algorithm
,
new
algorithm
,
baseline
,
question
,
question
,
entropy
,
entropy
,
prepend
,
entropyreduce
,
list
prepend
,
figure
,
new
algorithm
,
simple
version
,
unigram
probability
,
question
,
question
,
new
algorithm
,
notation
entropy
,
training
entropy
,
poten
tial
decision
list
,
entropy
,
prepend
,
training
entropy
,
question
,
output
,
parable
,
weatherman
,
new
learning
algorithm
,
baseline
,
probability
,
en
tropy
,
entropy
,
entropy
reduction
,
entropy
,
new
question
,
versus
,
reduction
,
question
,
output
,
output
,
output
,
course
,
third
rule
,
decision
list
,
entropy
sorter
,
smart
learner
,
dumb
rule
,
lazy
weatherman
,
current
sit
,
tree
bottom
,
tree
top-down
,
decision
tree
,
question
,
entropy
,
entropyreduce
,
list
prepend
,
instanceent
,
figure
,
new
algorithm
,
efficient
version
uation,problem,algorithm,figure,straight-forward
way
,
problem
,
inner
loop
,
entropy
,
prepend
,
naive
way
,
training
data
,
possible
decision
list
,
practice
,
actual
question
,
triple
,
sim
ple
question
,
instance
,
actual
question
,
total
number
,
question
,
possible
new
decision
list
,
question
,
entropyreduce
,
entropy
,
current
value
,
question
,
change
,
algorithm
,
figure
,
efficient
version
,
new
algorithm
,
question
,
entropy
,
list
question
,
entropy
,
question
,
question
,
reverse
order
entropyreduce
,
entropyreduce
,
compromise
,
delete
bad
question
note
,
efficient
version
,
algorithm
,
large
amount
,
question
,
training
instance,question,number,speed-space
tradeoff
,
instance
,
update
loop
,
possible
tradeoff
,
instance
,
question
,
con
junction
,
simple
question
,
instance
,
update
loop
,
simple
question
,
number
,
true
instance
,
instance
,
instance
,
original
algorithm
,
instance
,
lazy
weatherman
example
,
output
,
output
,
output
,
second
rule
,
decision
list
,
practice
,
ques
tion
,
output
,
main
reason
,
decision
list
,
understandability
,
small
size
,
optimization
,
full
im
plementation
,
new
algorithm
,
compromise
algorithm
,
figure
,
entropy
,
training
data
,
benefit
,
improvement
,
threshold
,
revious
work
,
modest
amount
,
previous
work
,
probabilistic
decision
list
,
fair
amount
,
related
field,transformation-based
learning,non-probabilistic
decision
list
,
similar
formalism,two-class
case,non-probabilistic
decision
list
,
output
,
tbl
output
rule,current-class
,
change
class
,
class
case,current-class
,
instance
,
difference
,
decision
list,two-class
tbl
,
last
one
,
decision
list
,
first
one,two-class
case,current-class
,
rule
order
,
equivalent
non-probabilistic
decision
list,vice-versa
,
notice
,
incre
mental
algorithm
,
algorithm
,
training
data
error
rate
,
prob
abilistic
decision
list
learner
,
training
data
entropy
,
equivalence
,
important
case,answer,question,instance,part-of-speech
tagging
,
answer
,
question
,
nearby
word
,
problem
,
equivalence
,
decision
list
,
reason
,
connec
tion
,
previous
work
,
new
algorithm
,
prob
abilistic
version
,
ramshaw
,
marcus
,
algorithm
,
gorithm
store
,
error
rate
improvement
,
question
,
algorithm
store
,
ramshaw
,
marcus
algorithm
,
dynamic
problem
,
space
efficient
,
compound
question
,
section
,
static
probabilistic
version
,
efficient
tbl
,
florian
,
second
reason
,
connection
,
cision
list
,
natural
way
,
florian
et
al
,
prob
abilistic
version
,
technique
,
conversion
,
deci
sion
tree,technique,advantage,multi-class
case
,
decision
tree
,
current
state
,
decision
list
learner
,
advantage
,
extra
dependency
introduces
data
sparse
ness
,
empirical
question
,
depen
dencies
,
current
state
,
probabilistic
decision
list
,
competitive
way
,
ad
vantage,list-structure
,
simplicity
,
possible
disadvantage
,
dependency
,
current
state
,
yarowsky
,
improvement
,
standard
algorithm
,
op
tional
,
algorithm
,
technique
,
probability
,
global
probability
distribution
,
question
,
local
probability
,
question
,
probability
,
pruning
technique
,
question
,
accuracy
,
section
,
technique
,
varia
tions
,
percentage
,
ques
tions
,
accuracy
,
yarowsky
,
structure
,
decision
list
,
advantage
,
decision
tree
,
decision
list
,
combination
,
hybrid
decision
list
,
improved
smoothing
,
performer
,
system
,
senseval
evaluation
,
technique
,
technique
,
result
,
decision
,
local
probability
,
reason
,
local
probability
,
final
list
,
yarowsky
,
technique
,
local
probability
,
question
,
algorithm
,
natural
prob
abilistic
version,non-probabilistic
decision
list
learner
,
difficulty
,
approach
,
probabilistic
framework
,
entropy
reduction
,
natural
solution
,
4
e
xperimental
result
,
discussion
,
section
,
experimental
result
,
new
algorithm
,
standard
algorithm
,
accu
racy
,
linear
classifier
,
decision
list
algorithm
,
problem
,
probabilistic
decision
list
algorithm
,
text
context
,
determine
,
choice
,
accent
restoration
,
yarowsky
,
word
sense
disambiguation
,
yarowsky
,
problem
,
framework
,
similar
feature
type
,
problem
,
grammar
checking
,
result
,
related
problem
,
training
,
problem
,
feature
set
,
hese
problem
,
confusable
word
,
typical
machine
learning
problem
,
data
size
,
different
pair
,
good
deal
,
different
behavior
,
standard
set,feature,window,part-of-speech
tag
,
window
,
tag
feature
,
window
,
feature
type
,
feature
,
training
data
,
comparison
,
different
algo
rithms
,
variation
,
stan
dard
probabilistic
decision
list
learner
,
standard
,
decision
list
learner
,
algorithm
,
figure
,
negative
infinity
,
entropy
,
unigram
distribution
,
entropy
,
training
data
,
learner
,
threshold
,
entropy
,
unigram
distribution
,
entropy
,
training
data
,
algorithm
,
threshold
,
attempt
,
incremental
algorithm
,
threshold
,
entropy
,
addition
,
various
decision
list
algorithm
,
several
algorithm
,
probabilistic
decision
list
,
probabilis
tic
analog
,
research
,
cision
list
,
several
success
,
simple
linear
model
,
perceptron
model
,
max
imum
entropy
,
maxent
,
rosenfeld
,
perceptron
algorithm
,
varia
tion
,
margin
requirement
,
iteration
,
change
,
j
f
igure
,
perceptron
algorithm
,
margin
,
incremental
,
transformation
,
perceptron
,
figure
,
geometric
mean
,
error
rate
,
training
size
,
herbrich
,
simple
algorithm
,
vec
tor
,
answer
,
question
,
weight
vector
,
output
,
margin
,
question
,
sepa
rate
threshold
variable
,
algorithm
,
standard
perceptron
algorithm,inclusion,non-zero
margin
,
vergence
guarantee
convergence
,
separable
data
,
solution
,
linear
support
vector
machine
,
krauth
,
mezard
,
extreme
simplicity
,
algorithm
,
algorithm
,
several
oth
er
,
perceptron
,
margin
,
favorite
algorithm
,
probability
,
model
size
,
algorithm
,
parame
ters
,
additional
con
fusable
word
pair
,
parameter
tuning
,
chose
parameter
value
,
entropy
,
error
rate
,
data
size
,
additional
word
pair
,
discount
value
,
threshold
,
incremental
learner
,
perceptron
algorithm
,
min
imum
number
,
traditional
value
,
incremental
,
transformation
,
maxent
,
perceptron
,
figure
,
geometric
mean
,
model
size
,
training
size
,
10m
50m
sorted
,
figure
,
arithmetic
mean
,
entropy
,
maxent
model
,
smooth
ing
,
variance
,
learning
algorithm
,
training
size
,
figure
,
error
rate
,
algo
rithm
,
different
training
size
,
test
set
,
geomet
ric
mean
,
error
rate
,
ten
word
pair
,
geometric
mean
,
error
rate
,
result
,
figure
,
geometric
mean
,
model
size
,
number
,
maxent
,
perceptron
mod
el
,
total
number
,
feature
,
feature
,
sorted
,
maxent
,
perceptron
model
,
model
size
,
factor
,
perfor
mance
,
additional
factor
,
accuracy
,
en
tropy
,
algorithm
,
entropy
,
arithmetic
mean
,
notice
,
traditional
probabilistic
decision
list
,
algorithm
,
equivalent
,
error
rate
,
entropy
,
entropy
,
accuracy
,
incremental
,
error
rate
,
entropy
,
many
rule
,
probabilistic
decision
list
learner
,
algorithm
,
probability
,
error
rate
,
low
est
entropy
,
accuracy
,
linear
model
,
maxent
,
perceptron
algorithm
,
margin
work
,
ex
pense
,
new
algorithm
,
small
size
,
probability
,
algorithm
,
decision
tree
,
yarowsky
,
florian
et
al
,
improve
ments
,
simple
decision
list
structure
,
additional
split
,
florian
et
al
,
notice
,
chief
advantage
,
decision
list
,
linear
model
,
compact
size
,
un
derstandability
,
technique
,
aspect
,
additional
split
,
sophisticated
smoothing
technique
,
yarowsky
,
incremental
algo
rithm
,
probabilistic
decision
list
,
duce
model
,
entropy
,
standard
,
learning
algorithm
,
new
algorithm
,
increased
time
,
complexity
,
variation
,
sorted
algorithm
,
thresholding
,
technique
,
section
,
list
size
,
substantial
improvement
,
al
gorithm
,
ad
vantage
,
compactness
,
understandability
,
probabilistic
decision
list
,
reference
,
paucity
,
data
problem
,
large
corpus
,
natural
language
disambiguation,transformation-based
error-driven
learn
ing
,
natural
language
processing
,
case
study,part-of-speech
tagging
,
stanley
,
joshua
goodman
,
empir
ical
study
,
technique
,
language
mod
,
computer
speech
,
language
,
rosenfeld
,
gaussian
prior
,
maximum
entropy
model
,
technical
re
port
cmu
cs-99-108
,
computer
science
department
,
henderson
,
confidence
,
old
friend
,
probabilistic
classifi
cation
,
transformation
rule
list
,
schapire
,
efficient
distribution
free
learning
,
probabilistic
concept
,
computer
,
system
science
,
mezard
,
optimal
stability
,
neural
network
,
journal
,
physic
,
mooney
,
califf
,
induction
,
first
order
decision
list
,
result
,
past
tense
,
english
verb
,
international
workshop
,
induc
tive
logic
programming,florian,transformation-based
learning
,
fast
lane,na-acl
,
marcus
,
statis
tical
derivation
,
transformational
rule
sequence,part-of-speech
tagging
,
proceeding
,
balanc
ing
act
workshop
,
combining
symbolic
,
statis
tical
approach
,
decision
list
,
machine
learn
,
dan
roth
,
natural
language
ambiguity
,
unified
approach
,
decision
list
,
ferred
rule
,
second
singapore
international
conference
,
intelligent
system
,
david
yarowsky
,
decision
list
,
lexical
ambi
guity
resolution
,
application
,
restoration
,
david
yarowsky
,
hierarchical
decision
list
,
word
sense
disambiguation
,
computer
,
hu
manities
,
hugo
zaragoza
,
ralf
herbrich
,
perceptron
,
reuters
,
workshop
,
machine
learning
