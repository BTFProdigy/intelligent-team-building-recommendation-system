multiple document summarization tsutomu  hir ao,jun  suz uki ,hideki  iso zaki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan hirao,isozaki,method,automatic sentence alignment,extract,ab stracts,automatic summarization research,method,similarity,extended string subsequence kernel,siders sequential pattern,proce dures,many-to-one correspondence,sentence,experiment,different similarity measure,alignment accuracy,performance,ntroduction many researcher,automatic summariza tion,system,abstract,document,extract,abstract,various method,sentence compaction,sentence combination,technique,large-scale corpus,original sentence,summary sentence,corpus,training,sentence extraction system,corpus,figure,example,summary sentence,original sentence,multiple document summa rization data,okumura,many-to-many correspondence,instance,correspondence,sentence,accurate alignment,alignment,syntactic information,semantic infor mation,method,joint conference,empirical method,natural language processing,computational natural language learning,prague,association,computational linguistics online large-margin training,statistical machine translation taro watanabe jun suzuki hajime tsukada hideki isozaki ntt communication science laboratory 2-4 hikaridai,seika-cho,soraku-gun,japan taro,tsukada,isozaki cslab,art performance,statistical machine translation,large number,feature,online large-margin training algorithm,mil lion,parameter,small development,sentence,experiment,arabic-to english translation,sparse binary feature,conventional  smt system,small number,feature,ntroduction,recent advance,statistical machine transla tion,small number,real-valued feature,phrase-based translation,chiang,syntax-based translation,galley,large number,feature,million,tillmann,bangalore,sparse binary feature,statistical machine translation,large training corpus,framework,prob lem,translation,sequential labeling problem,part-of-speech tagging,shallow parsing,large number,feature,signifi cant improvement,conventional small feature,bangalore,lexical choice model,large number,feature,labeling bias problem,lafferty,feature,online discriminative algorithm,decoding,online training ap proach,list one-by-one,1-best output,averaged perceptron al gorithm,1-best translation,watanabe,binary feature,small development,variant,voted perceptron,k-best translation,improvement,baseline translation system,good translation,method,large num ber,parameter,million,online training algorithm,small development,sentence,im proved performance,method,sentence,weight,iteration,model parameter,memorization variant,local updating strategy,parameter,good translation,k-best list,iteration,objective function,watanabe,sentence  ble,document wise loss,parameter,margin,dependency,joint-labeling chunking task,shimizu,arabic to-english translation task,signif icant improvement,conventional minimum er ror training,small number,feature,sec tion,framework,chine translation,baseline  smt system,hierarchical phrase-based translation,efficient left-to-right generation,watanabe,chiang,binary sparse feature,numeric feature,baseline system,section,introduces,online large-margin training algorithm,key component,experiment,section,discussion,section,2 s tatistical machine translation,log-linear approach,foreign language sentence,language,large-dimension feature vector,weight vector,contribution,feature,feature,real value,n-gram language model,fluency,lexicon model,phrase-wise correspondence,hierarchical phrase-based  smt chiang,hierarchical phrase,translation approach,non-terminals,phrase,translation,phrase,non-terminals,quasi-syntactic structure,reordering,phrase,conventional phrase-based ap proach,non-terminal em,phrase,bilingual corpus,syntactic struc ture,hierarchical phrase-based modeling,left-to-right target generation method,watanabe,method,translation,grammar,target side,phrase prefixed form,target normalized form,second,translation,left-to-right manner,phrase-based approach,earley-style top-down parsing,source side,target normalized form,n-gram language model,search,target normalized form,chiang,production rule,rank-2,rule con tains,non-terminals,target normal ized form,watanabe,constraint,target side,aligned right-hand side,reibach normal form,structure,source side string,arbitrary terminal,non-terminals,target side,string,termi nals,phrase,string,non-terminals,one-to-one mapping,tween non-terminals,phrase,prefix,strength,phrase base framework,contiguous english side,discontiguous foreign language side,phrase-bounded local word reordering,target normalized framework,phrase,restricted man ner,left-to-right target generation decoding,source side,projected target side,earley-style top-down parsing approach,watanabe,zoll mann,venugopal,basic idea,projected target side,left-to-right manner,search,push-down automaton,uncovered source,word position,rest-cost esti mation,bottom-up way,decoder,likely translation,target,decoding procedure,rule form,target side,inte gration,n-gram language model,prefixed phrase,hierarchical phrase-based translation system,standard numeric value feature,n-gram language model,fluency,target side,direction,relative count,weighted model,lexical transla tion model,low probability,lexical translation model,bender,acktrack-based penalty,dis tortion penalty,phrase-based modeling,watanabe,sparse feature,addition,baseline feature,large number,binary feature,mt system,binary feature,nglish word,violate,arabic word,feature,efficiency,word alignment structure,hierarchical phrase transla tion pair,cal phrase,word alignment,multiple word alignment,example,sparse feature,phrase translation,source,target side,word alignment,grammar size,word correspon dence,hierarchical phrase,figure,example,sparse feature,phrase,word alignment,phrase,tract word pair feature,bigram,word pair,contextual dependency,target side order ing,instance,figure,bigram word pair,target side,implies,correspond,source side,reordering,hierarchical phrase,dependent word pair,boundary,feature,figure,feature,translation,spurious word,target side,insertion fea tures,word alignment,target,inserted word,source sentence,non-aligned word,source sentence,figure,simplicity,example,phrase translation pair,feature,hierarchical phrase,example hierarchical feature,deletion fea,non-aligned source word,target sentence,complex decoding,hypothesis,feature,fluency,n-gram lan guage model,instance,bi gram feature,figure,addition,phrase,feature,feature,hierarchical struc ture,figure,example,hierarchical phrase,source side,dependency,source word,parent phrase,source word,child phrase,figure,hierarchical feature,source word,target side,fea ture size,normalization,generalization capability,normalized token,surface form,4-letter prefix,suffix,instance,word algorithm  1 o nline training algorithm training data,m-best oracle,oraclem,violate,prefix,suffix,sequence,ex ample,possible combination,example,word pair feature,vi olate,4-letter prefix token type,4 o nline large-margin training algorithm,generic online training algo rithm,algorithm,online training algorithm,tillmann,date oracle translation,good trans lations,decoder,papineni,k-best list,cur rent weight vector wi,training instance,training instance,reference translation,source sentence,k-best list,m-best oracle translation ot,oraclem,eration,decoder generate translation,reference transla tions,beam search pruning,assign score,reference translation,possible oracle translation,objective function,tillmann,problem,oracle translation,advance,strategy,parameter,oracle translation,translation,past iteration,k-best list ct,respect,oracle translation ot,iteration,algorithm,averaged weight vector,online training algorithm,selection,scheme,margin,relaxed algorithm,margin,online version,large-margin training algorithm,structured clas sification,taskar,dependency,joint-labeling chunking,shimizu,basic idea,update,weight vector,margin,incorrect classification,weight vector update procedure,algorithm,argmin wi,subject,non negative slack,influence,objective function,implies,update,loss function,instance difference,difference,reference translation,update,margin,correct,correct translation,incorrect translation,distance,correct,incor rect translation, mcd onald et al,k-best translation,margin,number,constraint,translation task,multiple translation,margin,m-oracle translation,amount,large-margin con straints,online training,active feature,offline training,possible feature,advance,lagrange dual form,subject,weight vector update,equation, a q p-solver,co ordinate ascent algorithm,amount,update,single oracle,1-best translation, a q p-solver,following perceptron-like update,shimizu,update amount,margin,correct,incor rect translation,closeness,transla tions,feature vector,averaged perceptron algorithm,tillmann,different update style,convex loss function,experimental result,normalized token,surface form,feature,prefix suffix,word class,token type,learning rate,convergence,approximated  ble uwe, ble score,papineni,loss function,n-gram precision,hypothesized translation,reference translation,sentence,sin gle sentence,weight vector,document-wise  ble,tillmann,problem,sentence-wise  ble,sentence-wise scoring,document-wise score,n-gram precision statistic,brevity penalty statistic,sentence,approximated  ble score,sentence set,difference,particular sentence,watanabe,translation,oracle translation ot1,hypothesized translation,training instance,approximated  ble mea,document-wise loss,cor rect translation,incorrect translation,normalization,sentence-wise score,document-wise score,5 e xperiments,online large-margin training pro cedure,arabic-to-english translation task,training data,ara bic english news un bilingual corpus,data amount,sentence,arabic part,bilingual data,arabic script,punctuation mark,development,mt2003 arabic english  nis evaluation test,consisting,sentence,news domain,reference translation,performance,news domain mt2004 mt2005 test set consisting,sentence,hierarchical phrase translation pair,standard way,chiang,bilingual data,word alignment,di rections,second,word alignment,grow-diag-final heuristic,phrase translation pair,hierarchical phrase,last step,hierarchical phrase,target normalized form con straint,5-gram language model,english side,bilingual data,english gigaword,normalized token type,sec tion,setting,structural feature,section,normalized token,surface form,online large-margin train,algorithm,iteration,experimental result,structural feature,feature,target bigram,insertion,experimental result,m-oracle translation,feature,10-best list,decoding,1000-best list,oracle translation,training,opteron,translation quality,case-sensitive  nis,doddington,papineni,number,active feature,non zero value,weight,addition,prefix suffix,number,active feature,development data,worse result,open test,word class3,surface form,overfitting problem,digit se quence normalization,similar generaliza tion capability,moderate increase,active feature size,token type,test set,experiment indi cates,token normalization,small data,normalized token type,structural feature,bigram feature,fluency,target side,source target correspondence,gov speech test,english,arabic,clusion,target bigram,development data,problem,insertion feature,ac count,agreement,source side,word pair feature,hierarchi cal feature,dependency structure,source side,online training algo rithm,sparse feature,baseline system,baseline hierarchical phrase-based system,sparse feature,result,m-oracle,structural feature,token type,sentence-wise  ble,objective function con,10-best list,oracle 1-best configuration,significant im provements,baseline system,k-best list,devel opment,degraded translation quality,m-oracle size,1-best list,reduced active feature size,translation,translation,10-oracles,feature,significant improvement,two-fold cross validation experiment,k-best list size,sentence-wise  ble,objective,improvement,test set,experiment,test set,domain mismatch,fold cross validation,test set,effect,optimization, mer baseline system,open test,line large-margin training,constraint,approximated  ble loss function,baseline sys tem,open test,development data,setting, mer approach,mixed domain,different epoch,6 d iscussion,translation model,mil lion,feature,poor overfitting,feature,word-based feature,structure,hierarchical phrase,benefit,flexibility,many constraint,m-oracle con straints,experiment,experiment,hierarchical phrase-based trans lation,online training algorithm,translation system,phrase-based trans lations,syntax-based translation,online discriminative training,tillmann,approach,training,large corpus,sparse feature,phrase translation pair,of-word pair,phrase,tillmann,split data,document,sentence,k-best list generation,step-by-step one-best merging method,decoding,training step,weight vector update scheme,convex loss function,method,k-best list,fast decoding method,watanabe,iteration,benefit,expen sive cost,k-best list,million,feature,averaged percep tron algorithm,training instance,perceptron update,weight vector,incorrect translation,oracle translation,k-best list,translation,past iter ations,experiment,small development,sparse feature,reranking,k-best translation,watanabe,variant,voted percep tron,significant improvement,improvement,performance,baseline system,good translation,sparse feature,dp-based search,design,sparse feature,word alignment structure,phrase,lation pair,reorder,phrase-based translation,monotone,trained model,single feature function,approach differs,feature,7 c onclusion,large number,binary feature,statistical machine translation,small development,optimiza tion,online version,large-margin training algorithm,mil lion,sparse feature,small development set,algorithm,million,feature,significant im provements,conventional method,small number,feature,result,many alternative feature,small data set,ap proach,data set,performance,future work,feature,acknowledgement,reviewer,colleague,useful comment,discussion,reference srinivas bangalore,patrick haffner,stephan kan thak,sequence classification,machine trans lation,pittsburgh,oliver bender,richard zen,evgeny matusov,mann ney,alignment template, iws lt,david chiang,hierarchical phrase-based model,statistical machine translation,ann arbor,michigan,koby crammer,ofer dekel,joseph keshet,shai shalev shwartz,yoram singer,online passive aggressive algorithm,journal,machine learning research,george doddington,automatic evaluation,ma chine translation quality,n-gram co-occurrence statistic,human lan guage technology,michel galley,jonathan graehl,kevin knight,daniel marcu,steve  den eefe,wei wang,ignacio thayer,scalable inference,training,context-rich syntactic translation model,sydney,au tralia,philipp koehn,franz josef och,daniel marcu,statistical phrase-based translation, naa cl,edmonton,canada,john lafferty,andrew  mcc allum,fernando pereira,conditional random field,probabilistic model,sequence data,international conf,machine learning,morgan kaufmann,percy liang,dan klein,ben taskar,end-to-end discriminative approach,machine translation,sydney,australia,ryan  mcd onald,koby crammer,fernando pereira,large-margin training,dependency parser,ann ar bor,michigan,franz josef och,hermann ney,system atic comparison,various statistical alignment mod el,computational linguistics,franz josef och,hermann ney,align ment template approach,statistical machine transla tion,computational linguistics,franz josef och,minimum error rate training,statistical machine translation,sapporo,kishore papineni,salim roukos,todd ward,wei jing zhu,method,automatic eval uation,machine translation,philadelphia,pennsylvania,brian roark,murat saraclar,michael collins,mark johnson,discriminative language model,conditional random field,percep tron algorithm,barcelona,nobuyuki shimizu,andrew haas,sequence,main conference poster session,sydney,australia,ben taskar,dan klein,mike collins,daphne koller,christopher manning,max-margin parsing, emn lp,barcelona,christoph tillmann,tong zhang,discrimi native global training algorithm,sydney,australia,taro watanabe,jun suzuki,hajime tsukada,hideki isozaki, iws lt, iws lt,taro watanabe,hajime tsukada,hideki isozaki,left-to-right target generation,hierarchi cal phrase-based translation,sydney,australia,dekai wu,hongsing wong,machine transla tion,stochastic grammatical channel, col ing ,montreal,quebec,canada,richard zen,hermann ney,discriminative reordering model,statistical machine translation,new york city,andreas zollmann,ashish venugopal,syntax,machine translation,new york city,proceeding,joint conference,empirical method,natural language processing,computational natural language learning,prague,association,computational linguistics semi-supervised structured output learning, a h ybrid generative,discriminative approach jun suzuki,akinori fujino,2-4 hikaridai,seika-cho,soraku-gun,isozaki cslab,framework,semi-supervised structured output,sequence labeling,hybrid generative,discrim inative approach,objective function,hybrid model,writ ten,log-linear form,predic tor,generative model,incor porate,unlabeled data,generative manner,crease,discriminant function,output,parameter estima tion,experiment,data show,hybrid model,state of-the-art performance, sol method,1 i ntroduction structured output,method,interdependent output space,important methodology,part-of-speech tagging,syntactic chunking,sequence labeling task,nature,sequence labeling task,semi-supervised approach,number,feature,parameter,example,parameter space,thousand,labeled ex amples,many attempt,semi-supervised  sol method, mcc allum,brefeld,scheffer,generative approach,corporate unlabeled data,probabilistic model,al gorithms,dempster,example,baum-welch algorithm,well-known algorithm,se quence learning,generally,sequence,chunking,performance,discriminative approach,supervised learning setting,contrast,generative approach,discriminative approach,training data,discriminative training criterion,ex ample,effect,unlabeled data,objective function,unlabeled data,traditional,conditional probability model,several attempt,unlabeled data,discriminative approach,approach,pairwise similarity,data point,class label,unlabeled data,brefeld,scheffer,approach,joint inference,whole data set,prediction,regard,large data set,standard sequence,discrim inative approach,semi-supervised  sol,incorporation,entropy regularizer,bengio,approach,parameter,likelihood,negative conditional entropy,unlabeled data,structured predictor,unlabeled data,entropy criterion,parameter estimation,contrast,previous study,semi-supervised  sol framework,hybrid generative,discriminative approach,hybrid approach,setting,semi-supervised approach,classifier,generative mod el,unlabeled data,framework,output domain,sequence labeling task,objective function,incor poration,discriminative model,pre dictors,original framework,combination,gen erative classifier,result,hybrid model,state-of-the-art perfor mance,supervised  sol method,large amount,experiment, con ll, con ll-2000 chunking data,addition,ss-crf-mer,hybrid model,several good characteristic,low calculation cost,robust optimization,sensitiveness,hyper-parameters,detail,section,sequence labeling task,syntactic chunking, sol problem,input sequence,particular output se quence,special fixed label,beginning,sequence,regard,sequence,method,constitute flex,powerful model,structured predictor,undirected graphical model,lafferty,parameter vector,feature vector,corresponding position,product,potential function,clique,malization factor,output value,partition function,parameter estimation,maximum  a p,parameter estimation,criterion,following objective function,abbreviation,prior probability distribution,-bf g,nocedal,gradient,partition func tion,linear-chain  crf,dynamic programming algo rithm,nature,forward-backward al gorithm,efficient calculation,lafferty,prediction,probable output,argmaxy,viterbi algorithm,ybrid generative,discriminative approach,semi-supervised  sol,section,formulation,hybrid approach,parameter estima tion method,sequence predictor,unlabeled data,i-units,discrimina tive model,j-units,generative model,structured predictor,discriminative combination,sev eral joint probability density,posterior probability,hybrid model,log-values,feature,log-linear model,discriminative combination weight,represent model parameter,individual model,unlabeled data,third line,second line,hy brid model,discrimina tive model,generative model,hereafter,hybrid model,discriminative model,generative model,sequence modeling,first order hmm,transition probability,sym bol emission probability,s-th position,corresponding input sequence,formalization,log linear combination,hybrid model,sim ilar, lop -crf,combination,discriminative model,objective func tion, lop -crf,framework,extension, lop -crf,unlabeled data,discriminative combination,parameter,objective function,parameter,prior probability distribution,global maximum,arbitrary fixed value,domain,con cave function,gradient-based optimization algorithm, l-b fgs ,nocedal,incorporating unlabeled data,unlabeled data,discriminative training,correct,unlabeled data,generative approach,unlabeled data,incomplete data,variable,mix ture model,well-known way,corporation,log likelihood,respect,marginal distribution,generative model,unlabeled data,mixture model,text classification performance,discriminant function,gener ative classifier,generative model,logarithm,discriminant function,missing variable,unlabeled data,unlabeled data,hybrid model,discriminant function,hybrid model,mixture model,following objective function,model parameter,generative model,prior probability distribution,discriminant function,output,hybrid model,numerator,third line,denominator,determination,local max imum,initialized value,iterative computation,em algo rithm,dempster,estimate,current step,jensen,inequality, a q function,update,intuitive effect,unlabeled data,respect,distribution,formation,constraint,generative model structure,parameter estimation procedure,definition,estima tions,param eters,structured predictor,algorithm,model parameter,hybrid model,param eters,generative model,solu tion,parameter estimation,search,objective function,algorithm,model parameter,figure,estimation,train ing data,labeled training data,estimation,several possible way,over-fit,labeled training data dl,distinct set,experiment,labeled training data dl,denominator,normalization factor,potential function,s-th position,sequence,i-th  crf,probability,s-th position,j-th  hmm,ap pendix,derivation,fig ure,derivative,respect,parame ters,generative model,following derivative,respect,log pdi,log  v d,second term,erative procedure,optimization,figure,beginning,proce dure,forward,backward state cost,position,output,product,total value,transition cost,corresponding input sequence,third term,expectation,potential func tions,forward backward algorithm,log  v d,log  v d,partition function,hybrid model,calculation,derivative,respect,forward-backward algorithm,stan dard  crf,derivative,respect,parameter,generative model,second term,expec tation,transition probability,symbol emis sion probability,forward-backward algorithm,manner,difference,fig ure,forward-backward algorithm,standard  hmm,q-function,standard  hmm,difference,method,marginal probability,standard  hmm,forward-backward algorithm,efficient calculation,param eter estimation process,hybrid model,combination,discriminative model,forward-backward algo rithm,sample,optimization pro cedures,required number,execution,forward-backward al gorithm,parameter estimation,number,hybrid model,addition,training,parameter value,single parameter vector,viterbi algorithm,unseen sample,standard  crf,additional cost,4 e xperiments,sequence,syntactic chunking,chunking,shared task, con ll-2000,tjong kim sang,buch holz, con ll-2003,tjong kim sang,meulder,baseline method,training procedure,pereira,p-crf ,hybrid model,formalism,hybrid model,extension, lop -crf,sec tion,gaussian prior,second term,hyper-parameter,gaussian prior,contrast, lop -crf, hys ol,dirichlet prior,second term,lwords,wtypes,wtypes,wtypes,wtypes,pref1s,lwords,wtypes,lwords,lwords,wtypes,wtypes,lwords,wtypes,lwords,lwords,lowercase,1-4 character prefix,word suf1-4,1-4 character suffix,word table,feature, ner experiment,hyper-parameters,dirichlet,named entity recognition experiment,english  ner data consists,sen tences,training,development,test data,entity tag,unlabeled data consists,sentence,data set, con ll-2003,feature set,feature type,word type,word prefix,suffix,example,word type,punctuation,baseline feature,sutton,regular expression,several previous study,addi tional information,external resource,gazetteer,feature, lop -crf, hys ol,base dis criminative model,different feature set,design,fea ture set,suggestion,performance,several feature division,comparison method,feature,experiment ture type,section,hys ol,parameter,discriminative mod el,labeled training data,feature,generative model,feature,figure,feature type,non-overlapping fea ture,feature,symbol,syntactic chunking experiment,chunking data,corpus,section,training data,sentence,section,test data,sentence,different chunk-tags,region,target chunk, lop -crf, hys ol,base discriminative model,different feature set,feature,chunking experiment,feature set,exten sion,additional feature type, hys ol,unlabeled data, ner experiment,division,labeled training data,feature set,generative model,man ner, ner experiment,section,labeled training data,feature type,generative model,method,method,5 r esults,discussion,performance,evaluation measure,sentence accuracy,method,experiment,sequence loss,result,chunking experiment,column,performance,sentence accu racy,hyper parameter,dirichlet prior,certain value,develop ment set1,second row,performance,base discriminative model, hys ol,feature,training data,third row, hys ol,performance,generative model, lop -crf,perfor mance,third  hys ol, lop -crf,discrimina tive model, hys ol, hys ol,ment set,preliminary examination,labeled training data,development data,hyper-parameter value,change,performance,con vergence condition value,figure, hys ol,performance,supervised set, lop -crf,regard,chunking experiment,impact,incorporating unlabeled data,contribution,hybrid model,com parison,performance,third row, hys ol,point f-score,point sentence accuracy gain, ner exper iments,point f-score,tence accuracy gain,chunking experiment,key idea,unlabeled data,approach,improvement,state-of-the-art performance,discriminative model,supervised setting,unlabeled data,similar effect,soft-clustering,information,correct output,sec ond,combination,generative model,flexibility,feature design,unlabeled data,example,ar bitrary overlapping feature,discriminative model,unlabeled data,assign,feature type,generative model,experiment,impact,iterative parameter estimation figure,change,performance,convergence condition value, hys ol dur,parameter estimation iteration,chunking experiment,figure, hys ol,conver,gence condition,small number,iteration,experiment,change,per formance,iteration,optimization procedure,optimization,local maximum,convergence condi tion,model parame ters,sufficient fixed number,itera tions,parameter,maximum objective value,comparison,ss-crf-mer, hys ol,method,perfor mance,ss-crf-mer,chunking experiment,ss-crf-mer,algorithm,forward-backward algorithm,performance,hybrid approach,several good characteris tic,ss-crf-mer,order algorithm,backward algorithm,parameter estimation,unlabeled data,time complexity,unlabeled data,output label size,unlabeled sample length,hybrid approach,unlabeled data, hys ol,standard forward-backward algorithm,time complexity,question,ss-crf-mer,practical time,large amount,unlabeled data,experi ments,unlabeled data,future,million,billion,unlabeled data,improvement,sensitive hyper-parameter,objec tive function,influence,contrast,objective function,hyper-parameter,prior distribution,standard  map estimation,experimental result,additional resource,large gazetteer,large gazetteer,elaborated feature,gazetters,previous top system,experiment,additional resource,unlabeled data,reuters,full parser output,matsumoto,previous top system,indicate,respect,hyper-parameter,good performance,prior distribution,comparison,previous top system,respect,performance,performance,figure,unlabeled data,additional information,method,result,performance,source,unlabeled data,cer tain reason,unlabeled data, ner ex periments,detail, tre corpus,chunking experiment,training,test data,chunking,supplied unla, con ll-2003,chunking,advantage,approach,hybrid model,gener ative model,design,effective auxil iary problem,target problem,additional information, 27m word,gazetters,add dev, hys ol performance,score optimization technique,additional resource, hys ol performance,f-score optimization technique,experiment,unlabeled data,construct,auxiliary problem,structure,auxiliary problem,performance,main problem,method,exam ple,feature,method,base discriminative model,hybrid model,method,possibility,performance,simple combination,top system,aso semi boost performance,exter nal hand-crafted resource,large gazetteer,result, hys ol,gazetteer, con ll-2003,feature, hys ol,applying,optimization technique,addition,f-score opti mization technique,sequence,suzuki,hys ol performance,base discriminative model,discriminative combination,hybrid model,optimization procedure,f-score gain,f-score optimization technique,f-score optimization technique,f-score,performance,additional resource,feature engineering, hys ol,addi tional resource,performance gain,third row,performance, 10m word,unlabeled data, 27m word,article,reuters corpus,fifth row,performance,supplied gazetters, con ll-2003 data,feature,development data,training data, hys ol,com parable performance,chunking,iments, ner experiment,fair comparison,additional resource,gazetters,training,onclusion,future work,framework,semi-supervised  sol,hybrid generative,discriminative ap proach,experimental result,incorpo rating unlabeled data,generative manner,state-of-the-art performance,supervised  sol method,hybrid approach,discrimina tive model,appropriate model,feature design,unlabeled data,performance,experiment,appendix let  v d,following rear rangement,consistency, pos tag, pos tag,supplied data set,new  10m word,unlabeled data, a p o tagger, wsj corpus,reference,belkin,max imum margin semi-supervised learning,variable,igh-performance semi-supervised learning method,text chunking,scheffer,semi-supervised learning,structured output variable,entity recognition, a m aximum entropy approach, con ll-2003,maximum likelihood,incomplete data,em algorithm,journal,royal statistical soci ety,series,entity recognition,classifier combi nation, con ll-2003,ybrid gen erative discriminative approach,semi-supervised classifier design, aaa i-05,bengio,semi-supervised learning,entropy minimization,schuur man,semi-supervised conditional random field,improved sequence segmentation,la beling,matsumoto,sup port vector machine, naa cl,pereira,condi tional random field,probabilistic model,seg menting,sequence data,icm l-2001, mcc allum,semi-supervised se quence,syntactic topic model, aaa i-2005,nocedal,limited memory bfg s me thod,large scale optimization,programming,mitchell,classification,document, mcc allum,classification,hybrid generative discriminative model,pereira,condi tional random field,osborne,logarith mic opinion pool,conditional random field, mcc allum,weight undertraining,structured discriminative learning, htl -naa cl,isozoki,conditional random field,multivariate evalua tion measure,tjong kim sang,buchholz,introduc tion, con ll-2000 shared task,chunking, con ll-2000,tjong kim sang,de meulder,intro duction, con ll-2003 shared task,language independent named entity recognition,con ll-2003,johnson,text chunking, a g eneralization,winnow,ma chine learning research,lafferty,learning,gaussian field,har monic function, icm l-2003,proceeding,conference,empirical method,natural language processing,singapore,6-7 august, afn lp,empirical study,semi-supervised structured conditional model,dependency parsing jun suzuki,619-0237,japan jun cslab,carreras csail,mcollins csail,abstract,empirical study,high-performance dependency parser,semi-supervised learning ap proach,extension,conditional model,dependency,problem,framework,suzuki,isozaki,extension,parsing,first ex tension,ss-scms,semi-supervised approach,exten sion,approach,second order parsing model,carreras,stage semi-supervised learning approach,effectiveness,method,dependency,experiment,test collection,penn treebank,en glish,prague dependency tree bank,result,test data,datasets,parent-prediction accuracy,en glish,1 i ntroduction recent work,depen dency,many language,algorithm,buchholz,unlabeled data,addition,example,po tential,improved performance,method,dependency parsing,large amount,unlabeled data,semi-supervised ap proaches,previous work,method,dependency parsing,cludes,eisner,semi-supervised approach,clus ter feature,unlabeled data,state-of-the-art result,depen dency,test collection,english,simple approach,significant perfor mance improvement,state of-the-art,dependency parser,pereira,alternative method,semi-supervised learning,dependency parsing,approach,framework pro,suzuki,isozaki,dependency parsing,framework,structured condi tional model,se ries,generative model,parameter,unlabeled data,basic method,ap proach,addition describes,extension,first extension,method,cluster-based semi-supervised method,second extension,approach,parsing model,carreras,two-stage semi-supervised learning approach,experiment,dependency parsing,english,penn treebank data,basic ss-scm,dependency,combina tion,ss-scm,approach,unlabeled data,method,stage semi-supervised learning approach,corporates,second-order parsing model,ad dition,ss-scm,english,pendency,large amount,unlabeled data,emi-supervised structured conditional model,dependency parsing suzuki,semi-supervised learning method,conditional random field,method,dependency,problem,extended method,remainder,section,approach,basic model,input sentence,labeled depen dency structure,sentence,labeled dependency structure,pendencies,head-word,dependency,modifier word,dependency,sentence,access,labeled training example,addition,unlabeled example,conditional log-linear model,dependency parsing,conditional random field,lafferty,distribu tion,dependency structure,sentence,partition function,pa rameter vector,feature vector represent,context,definition,feature,unlabeled data,scalar parameter,func tions,generative model,unlabeled data,parameter,relative strength,function,convenience,vector,parameter,generative model,full model,conditional distribution,parameter value,three-step parameter estima tion method,function,generative model,uniform distribution,parameter value,new function,unlabeled data,distribution,labeled example,end result,combine,gen erative model,unlabeled data,generative model,generative model,unlabeled data,direct use,feature-vector definition,dependency parser,first step,fea tures,separate feature vector,result,concatenation,feature vector,experiment,dependency parsing,tioned,separate feature vec tor,different feature type,example,feature vector,feature,word bigram,indicator function,word bigram,generative model,parameter,multinomial distribution,con straints,naive bayes,distribution,fea ture vector,next section,parameter,unlabeled data,parameter,function,log probability,generative model,definition,factor,experiment,number,feature,ap pear,unlabeled data,frequent fea tures,contribution,modification,parameter,generative model,method,parameter,generative model,initial parameter,distribution,dependency structure,generative model,example,likelihood function,function re,function,em algorithm,hidden label,dependency structure,conditional dis tribution,estimate,function,vector,variant,inside-outside algorithm,eisner,dependency-parsing data structure,paskin,projective dependency structure,matrix-tree theorem,non projective dependency structure,estimate,slight modification,follow ing estimate,pa rameter,ap estimate, ad irichlet,parameter,complete parameter-estimation method,section,full parameter estima tion method,algorithm,labeled example,example,feature-vector defini tion,partition,feature vec tor,generative mod el,output,algorithm,parameter vector,generative model,parameter,probabilistic dependency,algorithm proceeds,estimation,initial value,generative model,regularized log-likelihood function,labeled example,generative model,conventional regularized log-likelihood function, crf model,parameter,regular ization,initial pa rameters,parameter,conventional method,parameter,log-likelihood function, lbf g,nocedal,gradient,log-likelihood function,inside-outside algorithm ap,dependency parse structure,matrix-tree theorem,non-projective structure,estimation,generative mod el,count vector,distribution,generative model parameter,definition,estimate,generative model,new value,generative model,re-estimation,final step,analogous way,log-likelihood,labeled example,generative mod el,final output,algorithm,parameter,method,multiple time,suzuki,isozaki,experiment,xtensions,cluster-based feature koo,semi-supervised approach,cluster-based feature,competitive result,dependency,benchmark,method,two-stage approach,hierarchical word cluster,unlabeled data,al clustering algorithm,new feature,represent ing word,bit-strings,various length,cluster,different level,feature,conven tional feature,part-of-speech tag,new feature,conventional discriminative,approach,averaged perceptron algorithm,important point,approach,unlabeled data,construction,new feature,affect,algo rithms,cluster,feature,ss-scm approach,cluster,feature-vector representation,ap proach,pereira,car reras,second-order parsing model,information,grandparent,relationship,dependen cies,significant improvement,accu racy,first-order parsing model,principle,ss-scm approach,parsing model,practice,bottle neck,method,estimation,generative model,unlabeled data,calculation,marginals,unlabeled data,second-order parsing model,costly inference method,cal culation,marginals,large quantity,unlabeled data,two-stage,ap proach,ss-scm approach,second-order,carreras,first stage,first-order,generative model,unlabeled data,second stage,generative model,feature,second-order parsing model,approach,first-order parsing model,section,supervised learning,second-order parsing model,mc donald,real-values feature,two-stage ap proach,two-stage ss-scm,experiment,pereira,version,1-best  mir,difference,third line,sentence,data set,sentence,development,detail,training,development,test data,data set,unlabeled data,experiment parameter-estimation method,second-order parsing model,optimization,re-estimating,correct output,th sample,function,tunable scaling factor,represent,feature vector,second-order parsing part,4 e xperiments,experiment,ef fectiveness,ss-scm approach,depen dency,experiment,first order parsing model,extension,cluster-based feature,second-order parsing model,previous section,data set,experiment,english,czech data,wall street journal section,mar cu et,source,english,tate comparison,previous work,training,development,test set corpus article name,token bll ip,tipster,american reu,reuters,english,gigaword,detail,unlabeled data set,english dependency parsing,sentence,length,computational reason,pereira,english dependency,data set,stan dard set,head-selection rule,yamada,mat sumoto,phrase structure syn tax,treebank,tree repre sentations,section,training,section,velopment,section,czech data set,train ing development test partition,english,brown laboratory,linguistic information pro,sentence,raw text section,sentence,data set,unlabeled data,disjoint,training,development,test set,datasets,experiment,addition,experiment,amount,unlabeled data,english dependency,detail,unlabeled data set,experiment,sentence,computational rea son,total size,unlabeled data,approxi 2w,sentence,unlabeled data,sentence,bll ip corpus,super-set,labeled training data,feature,input sentence,baseline feature,simi lar,feature,pos bigram,contextual feature,pendencies,distance feature, mxp ost ,section,czech  pos tag,feature-based tagger,method,collins,assigned rich pos tag, pos tag,second set,experiment,feature,semi-supervised approach,cluster-based feature,data set,hierar chical clustering,approach,feature set,s -sc approach,unlabeled data,cluster,generative model,ss-scm model,generative model,section,generative mod el,ss-scm approach,partition,original feature vector,feature vector,similar approach,suzuki,isozaki,different feature vector,different feature type,feature template,systematic way,feature design,approach,experimental setting,result,experiment,parent-prediction accuracy,unla 3t raining,development,tains  pos tag,feature-based tagger,dependency,parent prediction,punctuation token,english,punctuation,setting,evaluation setting,previous work,method,carreras,second-order parsing model,method,projective dependency structure, pdt training data,non-projective model,ap plication,matrix-tree theorem,first-order czech model,projective parser,section,1st-order parsing model,tunable parameter,regularization con stant,dirichlet prior,generative model,fixed value,preliminary experi ments,performance,development data,supervised  scm,develop ment data,two-stage ss-scm,second-order parsing model,tunable parameter,performance,development data,addition,result,full training set,experiment,labeled training set,random sampling,predefined subset,document id,labeled training data,5 r esults,discussion table,result,ss-scm method un der various configuration,second order parsing model,clus ter feature,amount,remainder,section,result,effect,quantity,labeled data,result,semi-supervised approach,intuitive meaning,pseudo,feature,new parameter value,random selection random selection,sentence token,feature type baseline cl baseline cl baseline cl baseline cl baseline cl,ss-scm,sentence token,feature type baseline cl baseline cl baseline cl baseline cl baseline cl,ss-scm,dependency,result,ss-scm method,different amount,labeled training data,baseline,second-order ap proaches,second-order approach,refers,cluster-based feature,refers,cluster-based feature,performance,various size,baseline method,experiment,strong baseline,method,data size,tendency,impact,ss-scm,cluster feature one important observation,result,ss-scms,performance,baseline method,cluster-based feature,generative model,ss-scm approach,cluster-based fea tures,impact,two-stage approach table,effectiveness,stage approach,section,ss-scm method,second-order parser,ss-scm method,feature,generative model,separate learning algorithm,algorithm,feature,cl baseline,data size,og-scale,ic io  a,impact,unlabeled data size,s -sc,development data,english dependency,impact,amount,unlabeled data figure,dependency,accuracy,english,function,amount,section,unlabeled data,section,english dependency,performance,unlabeled data,cluster-based feature,addi tion,performance,method,feature type baseline cl ss-scm,parent-prediction accuracy,develop ment data,english dependency,perfor mance,unlabeled data,modest dif ference,performance,unlabeled data,computational efficiency,main computational challenge,ap proach,estimation,generative mod el,unlabeled data,amount,unlabeled data,implementation, bll ip corpus,baseline feature,expected count,parameter,generative model,computation,generative model,corpus,method,separate process,generative model,amount,computation,standard dependency-parsing ap proaches,result,test data finally,final result,test data,result,setting,development data perfor mance,english dependency par,result,unlabeled data,im provements,test data,development data,statistical significance,difference,parent-prediction error-rates,sentence level,paired wilcoxon,rank test,comparison,token  uld feature,baseline cl ss-scm,feature,baseline cl ss-scm,parent-prediction accuracy,test data,setting,development data performance,condition,pereira, 43m  uld ss-scm,pereira, 39m  uld, 39m  uld ss-scm, 39m  uld table,comparison,previous top sys tems,unlabeled data,previous method table,performance,number,state-of-the-art approach,english,czech data set,language,ap proach,reported figure,datasets,result,relative error reduc tions, mcd onald,pereira,second-order,dependency parser,result,second order semi-supervised dependency parser,similarity,two-stage semi-supervised learning approach,semi-supervised learning method intro,blitzer,exten sion,method,method,two-stage approach,generative model,auxiliary problem,unlabeled data,trained model,feature,method,direct use,feature-vector definition,representation,unlabeled data,7 c onclusion,extension,semi-supervised learning approach,suzuki,isozaki,dependency,problem,addition,extension,cluster-based feature,second-order parsing model,experiment,approach,significant im provements,state-of-the-art method,pendency,performance,amount,unlabeled data,ap proach,guages,ss-scm approach,di rect use,feature-vector representa tion,discriminative model,design,new feature,main choice,approach,partitioning,component,experience,reference,kubota ando,ramework,predictive structure,multiple task,unlabeled data,journal,trainable grammar,speech recognition,speech communication paper,meeting,acoustical society,amer ica,pereira,adaptation,structural correspondence learn, emn lp-2006,della pietra,class-based n-gram model,natural language,computational lin guistics,ll-x shared task,multilingual dependency parsing, con ll-x,experiment, a h igher-order projective dependency parser, emn lp con ll,tillmann,tatistical parser,new probabilistic model,dependency parsing,exploration,col ing-,jan haji,corpus,prague dependency treebank,valency,meaning,jarmila panevov,prague karolinum,collins,prediction model,matrix tree theorem, emn lp-conll,collins,simple semi-supervised dependency parsing,pereira,condi tional random field,probabilistic model,seg menting,sequence data,icm l-2001,nocedal,limited memory  bfg s me thod,large scale optimiza tion,programming, a l arge annotated corpus,en glish,penn treebank,computational linguis tic,pereira,online learning,approximate dependency parsing algorithm,com plexity,non-projective data-driven dependency parsing,crammer,pereira,line large-margin training,dependency parser,non-projective dependency parsing,tree algorithm, con ll,shared task,dependency parsing,emn lp-conll,paskin,cubic-time parsing,algorithm,grammatical bigram,technical report,university,california,berkeley,ratnaparkhi,aximum entropy model,part-of-speech tagging, emn lp,eisner,feature-rich dependency parser,entropic pri or, emn lp-conll,probabilis tic model,nonprojective dependency tree, emn lp-conll,isozaki,semi-supervised sequential labeling,segmentation using giga word scale unlabeled data,convex training,dependency par,matsumoto,statistical de pendency analysis,support vector machine,multi-label text categorization,model combination,f1-score maximization akinori fujino,hideki isozaki,jun suzuki ntt communication science laboratory ntt corporation,hikaridai,seika-cho,soraku-gun,isozaki,jun cslab,fundamental task,natural language processing,multi-label categorization problem,text document,category,good statistical classifier,generalization ability,multi-label categorization,classifier,sign method,model combination,score maximization,formu lation,multiple model,binary classification,category,training dataset,experi mental result,method,datasets,many combination,cat egory label,1 i ntroduction text categorization,fundamental task,aspect,natural language processing,informa tion retrieval,information extraction,text min ing,text document,category,real task,web page,ternational patent categorization,text categorization,category label,data sample,classifier,generaliza tion ability,multi-label categorization task,important issue,machine learning,conventional machine,ap proach,multi-label categorization,bi nary classification,approach,independence,category,design,binary classifier,category,category label,sample,statistical classifier,naive bayes,binary classi fiers,joachim,text categorization,performance,binary classifier,joachim,jansche,method,binary classifier,score performance,minimum error rate,maximum likelihood,train ing conventional classifier,large imbalance,nega tive sample,multi-label categorization,micro-averaged,uate classification performance,multi-label classification perfor mance,binary classifier,classification framework,combination,many previous work,wolpert,larkey,witten,ghahramani,fumera,classi,system,combination research field,linear combination,multiple classifier,classifica tion performance,individual classifier,classifier design method,combination,multiple binary classifier,multi-label classification performance,framework,multiple binary classifier,category,bi nary classifier,weight,microor macro-averaged,multi-label classifier,timate combination weight,score maximization training algorithm,jansche,real text datasets,design method,conventional bi nary classification approach,multi-label catego rization,method,binary classification,proach,kazawa,method,data sample,combination,assigned category la bel,method,conventional binary classification ap proaches,method,direct mapping method,score maximization training,score maximization training method,linear model,logistic function,jansche,binary classification setting,classi fiers,data sample,feature vector,class label,nth feature vector,discriminative function,binary classifier,linear model,model parameter vector,inner product,binary classifier,predicted class label assignment,binary classifier,class posterior probability distribution,determines,model parameter vector,likelihood,prior proba bility density,classifier design approach,training method,contrast,training method,jansche,discriminative function,training dataset,training method,approximate form,logistic function,precision,recall,number,data sample,predicted class label assign ments,correspond,number,data sample,number,data sample,dataset,jansche,discriminative,logistic function,approximate distribution,dataset,estimate,gradient method,classi fier design approach,training method,method,framework,multi-label classifier,combination,multiple mod el,formulation,multiple model,weight,training dataset,section,formulation,model combination,training method,combination weight,combination,multiple model,multi-label categorization multi-label categorization,multiple category label,pre-defined cat egory label,data sample,multi-label classifier,feature vector,kth category label,formulation,multiple mod el,binary classification,category,number,discriminative function,jth model,kth category,model param eter vector,model parameter,model parameter vector,algorithm,dis criminative function,multi-label classifier,multiple model,weight parameter vector,combination weight,jth model,bias factor,threshold,category label assignment,micro-averaged,multi-label categorization perfor mance,approximate form,method,dataset,training data sample,bias estimation,n-fold cross-validation,training data sample,wolpert,model parameter,training data subset,objective function,prior probability density, a g aussian prior,rosenfeld,hyperparameters,gaussian prior,estimate,initial value,quasi-newton method,formula tion,model combination,micro-averaged,method,multi-label categorization problem,classifier,average labeling,average labeling performance,classifier,data sample,kazawa,dataset,approxi mate form,fl-scores,present similar objective function,next section,per formance,classifier,combination method,fl-scores,4 e xperiments,method,test collection,reuters-21578,reuters,datasets,reuters,english document datasets,bench mark test,multi-label classifier,reuters,contains news article,reuters newswire,topic cate gories,article,training,test sample,subset consisting,training,test sample,topic cate gories,vocabulary word,ei ther,stoplist,article,vocabulary word,dataset, wip dataset,patent document,taxonomy,tax onomy,hierarchical layer,section,subclass,patent document,ated classifier,assigned category label,patent document,dataset,patent docu ments,training,test sample,vocabulary word,reuters,vocabulary word,dataset, jpa dataset,iwayama,con sists,japanese patent document,japanese patent office,document,taxonomy consisting,f-terms,top-label category,patent document,reuters  wip o jpat nav,statistical information,datasets,maximum num ber,assigned category label,data sample,number,category label,data sample,number,category label combination,dataset,patent document,classifier,assigned category label,f-terms,patent document,patent doc uments,training sample,patent document,test sam ples,japanese noun,adjec tives,patent document,morpholog ical analyzer,vocab ulary word,patent document,vocabulary word,dataset,statistical information,category label assignment,data sample,datasets,average number,assigned cat egory label,data sample,reuters,number,category label combina tions,reuters,statistical information result,complex multi-label dataset,reuters,experimental setting,text categorization task,word frequency vector,document,feature vector,classifier,independent word,representation,representation,l1 norm,word-frequency vector,effect,vector size,computation,method,sourceforge,multi-label text classifier,binary classifica tion model,method,section, a g aussian prior,prior proba bility density,parameter vector,linear kernel function,parameter,hyperparameter,method,microand,average label,test sample,classifier,perfor mance,classifier,binary classification approach,binary classification,binary classification ap proaches,initial pa rameter vector,parameter estimate,binary classifier design approach, svm perf,linear kernel function,parameter,parameter,hyperparameter,performance,method,kazawa,feature vector,label assignment vector,formance,binary classification approach,hyperparameter,good performance,test sample,computational cost,training,hyperparameters,fold cross-validation,training sample,result,discussion,classification performance,datasets,method,section,evaluation score,micro-averaged,svmlight,joachim,svmlight,joachim,org svm perf,microand,average labeling,con ventional method,macro-averaged,test sam ples,precision,category,data sample,posi tive sample,fm score,datasets,training method,fm score,jansche,joachim,fl-scores,fm score,fl-scores,model combination method,fm score,fl-scores,binary classifier,category,mc-fl classifier,weight,fl-scores,experimental result,training method,combination weight,multi-label classifier,regard,reuters,fl score,category label combination,reuters,result,data sample,category label assignment,feature vector,category label assignment vector,training dataset,contrast,model combination method,binary classifier,category,overfitting problem,model combination method,complex datasets,many category label combination,5 c onclusion,multi-label classifier design method,model combination,main idea,method,multiple model,weight,evalua tion score,microand,average labeling,real text datasets,method,perfor mance,conventional binary classification ap proaches,multi-label categorization,method,datasets,many combination,category label,future work,multi-label classifier,sample,data sample,category label assignment,mass function,knowledge,data engineering,stanley,ronald rosenfeld,aussian prior,maximum entropy model,technical report,benzineb,karetka,categorization,giorgio fumera,fabio roli,exper imental analysis,linear combiners,pattern analysis,ma chine intelligence,zoubin ghahramani,hyun-chul kim,combination,technical report,gatsby computa tional neuroscience unit,university college london,makoto iwayama,atsushi fujii,noriko kando,overview,classification subtask, ntc ir-6 patent re trieval task,proceeding, ntc ir workshop meeting,evaluation,martin jansche,maximum,f-measure train ing,logistic regression model,proceeding,human language technology conference,conference,empirical method,joachim,categorization,support vector machine,many relevant feature,proceeding,10th european conference,joachim,support vector method,multi variate performance measure,proceeding,international conference,hideto kazawa,tomonori izumitani,hirotoshi taira,eisaku maeda,maximal margin,multi topic text categorization,advance,larkey,bruce croft,classi fiers,text categorization,proceeding, acm international conference,research,development,kai ming ting,witten,stacked generalization,journal,artificial intelligence research,wolpert,generalization,newral net work,xin liu,re-examination,text categorization method,proceeding, acm international conference,research,development,hierarchical directed acyclic graph kernel,method,structured natural language data jun suzuki,tsutomu hirao,yutaka sasaki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan jun,sasaki,hierarchical di,kernel,structured natural language data,hda g ke rnel,several lev el,relation,weighed sum,number,common attribute sequence, hda g,method,question classifica tion,sentence alignment task,performance,similarity mea sure,kernel function,result,experiment,hda g ke rnel,kernel function,baseline method,1 i ntroduction,structured corpus,annotated text,many researcher,machine,technique,accuracy,basic  nlp tool, pos tagger,chunkers,entity,dependency analyzer,practical applica tions,motivation,information,performance, nlp application,con trast,feature vector,bag of-words,salton,method,nu merical feature vector,feature,natural language data,orig inal natural language data,researcher,symbolic data,numeric data,process,feature extraction,nature,differs, nlp task,neat formulation,feature vector,grammatical structure,kernel method,vapnik,cristianini,shawe-taylor,suitable,convolution kernel,haussler,kernel,discrete struc tures,string,remarkable property,kernel method ology,original representation,object,algorithm manipulate,object,kernel function,ner product,object,feature vector,efficient calculation,inner product,kernel method,machine learning method,kernel function,similarity function,certain property,cristianini,shawe taylor,measure,important factor,application area,machine trans lation,text categorization,information retrieval,question answering,kernel,sev eral,structure,similarity,regard,structure,practical cost, hda g ke rnel,similarity measure, nlp task,following section, hda g ke rnel,algorithm,result, hda g ke rnel,question classification,sentence alignment,2 c onvolution kernel convolution kernel,concept,kernel,discrete structure,framework,kernel function,input object,convolution,sub-kernels,kernel,decomposition,object,positive integer,separable metric space,special case,countable set,feature selection,natural language processing task jun suzuki,hideki isozaki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan jun,isozaki,sequence,tree ker nels,concept,ac curacy,experiment,over-fitting problem,ker nels, nlp task,convolution kernel,new approach,statistical feature selec tion,method,original kernel calculation process,sub-structure mining algorithm,experiment,real  nlp task,prob lem,conventional method,performance,method,ntroduction,past year,many machine,method,state-of-the-art performance,kernel method,support vector machine,cortes,vapnik,text categorization,joachim,matsumoto,collins,feature,kernel methodology,high accuracy,kernel function,natural language data,sequence,discrete structure,parsed tree,relational graph,discrete ker nels,sequence kernel,tree kernel,collins,graph kernel,suzuki,excellent result,discrete kernel,convolution kernel,haussler,con cept,kernel,discrete structure,convolution kernel,structural feature,feature vector,input object,accuracy,concept,experiment,critical issue,convolution kernel, nlp task,collins,cancedda,suzuki,over-fitting problem,sub structure,kernel calculation,result,machine,approach,large sub-structures,feature,main reason,convolution kernel,structural feature,small struc tures,advantage,convolution kernel,convolution kernel,new method,statis tical feature selection,method,feature,signif icant,kernel calculation,large significant sub structure,method,original kernel cal culation process,sub-structure mining al gorithms,next section,brief overview,convolution kernel,section,discus,convolution kernel,main topic,conventional method,section,new approach,statistical feature selec tion,convolution kernel,example consisting,sequence kernel,section,application,method,convolution kernel,section,performance,conven tional method,method,real  nlp task,question classification,sentence modality identification,experimental result,section,advantage,method,2 c onvolution kernel convolution kernel,con cept,kernel,discrete structure,se quences,framework,kernel function,input object,con volution,sub-kernels,kernel,decomposition,object,discrete object,conceptually,convolution kernel,sub structure,inner product,feature mapping,discrete object,feature space,sequence kernel,input object,sequence,sub-sequence,tree kernel,collins,kernel,quadratic time,input object,kernel value,discussion,kernel,convolution kernel,briefly,section,many kind,sequence kernel,variety,different task,framework,word sequence kernel,cancedda,gapped word sequence,kernel value,finite symbol,symbol,sequence,symbol,number,symbol,sub-structure,sequence,sequence,tj represent,jth symbol,abac abc abact,0f igure,example,sequence kernel output sequence,length,sequence,existence,position,length,example,notation,sequence kernel,decay factor,gap present,common sub-sequence,simple example,output,kernel,number,dimension,feature space,efficient recursive calculation,discussion,sequence kernel,notation,sequence kernel,function,common sub-sequences,function,indicator function,sub-sequences,tween si,equation,3 p roblem,applying convolution kernel,section,convolution kernel,original definition,convolu tion kernel,sub-structures,kernel,number,sub structure,input object size,result,kernel value,situation,machine learning process,memory-based learning,result,pre cise,low recall,conventional method,approach,kernel value,feature,sub-structure size,sequence kernel,feature elimination method,sub-sequence,kernel calcula tion deal,sub-sequences,tree kernel,ollins,method,feature,sub-trees depth,method,surface,good result,main reason,convolution kernel,structural feature,sub structure,full benefit,convolution kernel,result,sub-structures,sub-structures,sig nificant feature,regard,target prob lem,sub-structures,contingency table,notation,row u o uc  y o uc,conventional method deal,possibility,performance furthermore,significant sub-structure effi, nlp task,feature selection method,approach,statistical feature selection,contrast,conventional method,sub-structure size,understanding,classifica tion problem,approach,statisti cal deviation,sub-structures,training sample,appearance,positive sample,negative sample,significant sub-structures,kernel value,approach,feature,kernel,dp algorithm,kernel,statistical feature selection method,significant feature,method,statistical feature selection,kernel calculation,statistical metric,chi-squared value,many kind,statistical metric,chi-squared value,correlation coefficient,chi-squared feature selection,effec tive method,text classification,information,statistical feature selection criterion,contingency,resent,positive class,abac abc abact,0f igure,example,statistical feature selection,negative class,number,po itive sample,number,negative sample,number,ap pear,number,number,sample,positive class,sub-sequence,number,sample,total number,sample,number,positive sample,function,normalized deviation,vation,expectation,feature selection criterion,basic idea,feature selection,threshold,signif icant,feature,kernel value,sequence kernel,difference,condition,significant sub-sequence,con dition,simple example,kernel value,calculation method,possible,naive exhaustive method,approach,sub-structure mining al gorithm,sequential pattern mining technique,prefix pan,statistical metric prun,method,apriori  smp,morishita,technique,sig nificant sub-sequences,depth-first search,prun ing,concept,significant feature,concatenation,sequence,specific sequence,sequence,suffix,upper bound,inequation,sub-sequences uv,feature,sub sequence uv,feature,prefixspan algorithm,sig nificant sub-sequences,depth-first search, a t rie  structure,sig nificant sequence,internal result,prefixspan,concatenation,sequence,symbol,condi tions,sub-sequence uw,significant feature,sub-sequence uw,arbitrary sub sequence,suffix,search space,significant feature,significant feature,upper bound,search,figure,simple example,prefixspan,1-1 1-1-1 class,se ar ch  df igure,efficient search,significant sub-sequences,prefixspan algorithm,depth-first search, a t rie ,tation,significant sequence,number, tri structure,figure,sig nificant sub-sequences,symbol, tri structure,prefixspan,method,kernel calculation,embedding feature selection,kernel calculation,section,statistical fea ture selection,kernel calculation,method,following equation,function,sum value,significant common sub sequence,sub-sequences,condi tion,detail,function,common sub-sequences,equation,sequence kernel,special symbol,empty sequence,function,matching value,condition,detail,equation,significant sub-sequences,sub-sequences,feature,satisfy condition,sub-sequences,sub sequence,empty set,equation,operation,implementation,original sequence kernel,feature selection condition,significant feature,pre fixspan algorithm, tri representation,significant feature,recursive calculation,parallel,result,statistical feature selection,oroginal sequence kernel calculation,dynamic programming technique,property,method,several important advan tages,conventional method,feature selection criterion,statistical measure,significant fea tures,method,original kernel calculation process,calculation procedure,conventional method,difference,original sequence kernel,method,sub-structure mining algorithm,kernel calculation,kernel calculation,uni fies,method,train ing time,feature selection,sub-sequences,rie  data structure,fast calculation technique,matsumoto,method,classification,classification part,feature,sub sequence,learning part,sub-sequences,classification,method applied,convolution kernel,insufficient space,subject,detail,relation,convolution kernel,proposal,ker nels,collins,postorder traversal,sequential pattern mining technique,significant sub-trees,original sub-tree form,encoding representation,parameter value,kernel,support vector machine parameter value soft margin,decay factor,threshold,result,tree kernel,sta tistical feature selection,original tree kernel calculation,sequential pattern min ing technique,proposal,tured graph kernel,suzuki,simple extension,hierarchical structure,6 e xperiments,performance,method,actual  nlp task,sec tion,kernel,joachim,baseline method,kernel-based classifier,training,classifi cation,parameter value,comparison,threshold,method,significance,distribu tion,degree,freedom,significant test,question classification question classification,categorization,question,question type,performance,english,suzuki,japanese question classification,experimental setting, tim e top ,coarse,one-vs-rest classifier,multi-class classification method,figure,example,question classifi cation data,question type,object,information,example,english question classification data,result,japanese question classification,f-measure, tim e top ,k1 fss k2 sk bow,sentence modality identification,example,sentence modality identification tech niques,automatic text analysis system,modality,sentence,opinion,description,mainichi news arti cles,modality tag,opinion,cision,description,sen tence,data size,sentence,sentence,opinion,decision,result,5-fold cross validation,esults,discussion table,result,en glish question classification,result,sentence modality identifica tion,threshold,sub-sequence size,possible sub sequence,structural feature,structural feature,performance, nlp task,detail,content,result,maximum performance,per formance,exceeds,sub-structures degrade classification performance,result,tendency,previous study,section,precision,recall,classifier,high precision,low recall,evidence,over-fitting,experiment,precision,recall,performance,con ventional method,experiment con,important fact,case maximum performance,sub-sequences,large structure,course,feature space,feature space,perfor mance,significant feature,performance,classification prob,substructure,optimum performance,difference,performance,method,regard sub structure size,sub-structure size,approach,large sub-structures,conventional approach,sub-sequences,8 c onclusion,statistical feature selection method,convolution kernel,approach,significant feature,statistical significance test,method,kernel calcula tion process,convolution kernel,sub structure mining algorithm,result,english question classification,accuracy,result,sentence modality identification,f-measure, -e xperiments,method,conventional method,result,complex feature,method,over-fitting problem,benefit,concept,performance,reference,word-sequence kernel,convolution ker nels,natural language,neural,formation,cortes,support vector network,machine learning,convolution kernel,dis crete structure,technical report  ucs -crl,categorization,sup port vector machine,many rel evant feature,european conference,matsumoto,japanese depen dency analysis using cascaded chunking,conference,matsumoto,fast method,kernel-based text analysis,annual meeting,association,question clas sifiers,international con ference,watkins,text classification using string kernel,journal,machine learn,research,lattice,statistical metric pruning, acm  sig act-sigmod -sig art sy mp,prefixspan,mining sequential pattern,prefix-projected pattern growth,international conference,high-performing feature selection,text classification, acm  cik m in ternational con ference,information,knowledge manage ment,hierarchical directed acyclic graph ker nel,method,natural language data,annual meeting,associ ation,kernel,structured natural language data,17th annual conference,neural infor mation,annual meeting,ann arbor,association,computational linguistics boosting-based parse,2-4 hikaridai,seika-cho,soraku,japan taku,isozaki cslab,new application,boost ing,parse reranking,several parser,tree kernel,all-subtrees repre sentation,compara ble accuracy,small set,subtrees,boosting algo rithm,all-subtrees representa tion,relevant feature,experiment,parse rerank,method,performance,kernel method,efficiency,ntroduction recent work,statistical natural language par,discriminative tech niques,novel discriminative approach,discriminative machine learning algorithm,n-best output,conditional parser,discrimina tive reranking method,vari ous kind,feature,correct parse tree,candidate,feature design flexibility,appropriate feature,good discriminative ability,parse reranking,early study,feature set,task-dependent feature template,collins,collins,hese ad-hoc solu tions,reasonable level,google japan inc,taku google,com formance,task dependent,careful design,optimal fea ture,kernel method,ele gant solution,problem,infinite number,feature,generalization,known kernel,tree kernel,collins,feature vec tor,subtrees,kernel method,useful feature,subtrees,main question ad,accuracy,non-redundant set,subtrees,new application,parse reranking,tree kernel,all-subtrees representation,algorithm us,feature space,l1-norm regularization,relevant feature,small feature,practice,parsing,variant,branch-and-bound technique,efficient feature selection,iteration,2 g eneral setting,general setting,parse reranking,input output pair,put sentence,correct parse,function,date parse tree,particular sentence,feature function,parse tree,rd space,parameter vector,output parse,input sentence,argmaxy,question,regard,formula tion,parameter,well-known solution,problem,next subsection,parameter estimation,parameter,argminw,loss function,following form,arbitrary loss function,variety,parameter estimation method,loss function,follow ing,loss function,logloss,hingeloss,boostloss,parse rerank,standard maximum,lihood optimization,tropy model,hingeloss,maximum margin strategy,vapnik,boosting algorithm,collins,collins,real setting,condition,parse tree,correct parse tree,feature function,good ability,correct parse yi,candidate,early study,feature function,feature template,collins,collins,heuristic selection,useful feature,overall accuracy,special family,loss func tions,problem,dual form,inner product,instance,property,kernel trick,explicit feature function,example,tree kernel,collins,convolution kernel,instance,all-subtrees space,fea ture space,inner product,feature space,dynamic programming,tree kernel,fea ture,all-subtrees repre sentation,efficiency,ankboost,subtree feature,simple question,kernel-based parse,subtrees,final parameter,suppose,large tree,single node,equivalent discrimi native ability,subtrees,motivation,observation,small set,sub tree,final parameter,relevant feature,practice,crease,parsing,new boosting,algorithm,all-subtrees representation,architecture,method,connec tion,algorithm,sparse feature,f igure,subtree relation tion,preliminary let,definition,notation,definition  1 l,ordered tree,sib ling,first child,second child,third child,definition  2 s ubtree let,ordered tree,subtree,one-to-one function,condition,parent-daughter relation,relation,number,example,feature space,subtrees,parse tree,labeled ordered tree,output,part of-speech tagging,shallow parsing,dependency analysis,feature,consists,subtrees,existence,indicator function,feature space,tree kernel,tree kernel,cardinality,parameter estimation method,vari ant,rankboost algorithm,rankboost,collins,collins,proceeds,iteration,boostloss,iteration,single feature,hypothesis,weight,suppose,current parameter,new parameter,single feature,weight,increment,update,new loss,rankboost algorithm,differential,optimal solution,freund,collins,function,probabilistic history-based parser,output,parse tree,log probability,experiment,optimal setting,simplicity,experiment,log probability,reranking,log probability,tree un der,base parser,parameter,binary feature,problem,approximation technique,freund,sparse feature representation recent study,schapire,vapnik,similar strategy,optimal parameter,margin,negative ex amples,critical difference,definition,margin,vector,iterative feature selec tion,l1-norm,regularization,contrast,l2 norm,regularized algorithm,relationship,regularization,machine,l1-norm,problem,feature,l2-norm,feature,advantage,l1-norm regularizer,solution,feature,weight,irrelevant feature,regard classification,l1-norm regularization,set ting,feature,subtrees,redundant feature,4 e fficient computation,boosting iteration,number,subtrees,problem,np-hard,real appli cation,problem,max imum number,subtrees,problem,variant,branch-and-bound algorithm,matsumoto,efficient enumeration,tree abe,efficient method,rightmost-extension,subtrees,algorithm,single node,new node,inef ficient,arbitrary position,enumeration,algorithm,rightmost extension,avoids dupli,enumeration,position,tachment,definition,rightmost extension,restriction,detail,definition,ightmost extension,ordered tree,rightmost extension,condition,single node,unique path,rightmost leaf,rightmost path,rightmost sibling,figure,example tree,convenience,figure,original number,depth-first enumeration,rightmost-path,position,rightmost,sim ply,single node,rightmost path,rightmost path,label set,rightmostpath rightmost extension,cba figure,extension tal,original tree,rightmost-extension process re,search space,rightmost extension,canonical search space,subtrees,upper bound,subspace,canonical search space,obser vation,convenient way,upper bound,search space,rightmost extension,upper bound,traverse,subtree lattice,recursive process,rightmost extension,suboptimal gain,super-tree,search space,subtree,contrast,super-tree,ad-hoc technique,real application,following practical method,training cost,nation,size threshold,use subtrees,con straint,extension,frequency-based cut-off,feature selection,use subtrees,different sentence,similar branch-and-bound technique,cut-off,fre quency,frequency,super-trees,10-iterations,boosting,pseudo iteration,optimal feature,feature,previous iteration,ob servation,feature,number,iteration increase,pseudo iteration,branch-and-bound algorithm,new feature,5 e xperiments,wall street journal text,experiment,data set,collins,penn treebank,training data,section,test data,training data con,sentence,average,distinct par,train ing sentence,sentence,rankboost algorithm,remain,sentence,development data,model2,collins,training,test data,lexical information,parse tree,standard  cfg tree,lexicalized-cfg tree,non-terminal node,extra lexical node,head word,constituent,figure,example,lexicalized-cfg tree,experiment,leftmost constituent size parameter,frequency parameter,data set,ex periments,unrestricted parameter,list result,test data,model2,collins,several previous study,precision,sentence,method,absolute improvement,av erage precision,recall,sentence,relative reduc tion,recall,precision,datasets,tences,similar performance,collins,kernel method,collins,all-subtrees representation,recall,precision,result,all-subtrees repre sentation,different parameter estimation method,recall,pre cision,sentence,shallow parsing,data set, con ll,tjong kim sang,buchholz,penn treebank,training data,section,test data,baseline model,shallow parser,pereira,remarkable result,number,combination,forward mod el,sentence,lp cbs  0 c b  2 c b co99,boosting,sentence,lp cbs  0 c b  2 c b co99,boosting,result,section, wsj treebank lr lp,average number,cross bracket,sentence, 0 c b,per centage,sentence,bracket,collins,collins,collins,search,search algorithm yield optimal n-best result, crf score,sentence,distinct par,log probability, crf shallow parser,reranking,collins,training set,portion, crf shallow parser,output,base parser,base phrase,right-branching tree,adjacent base phrase,parent-child re lationship,figure,example,shallow parsing task,virtual node,right boundary,local transi tions,size parameter,frequency parameter,list result,test data,baseline crf parser,several previous study,measure,baseline  crf,parser, svm parser,matsumoto,f-measure,winnow,additional linguis tic feature,accuracy,simi lar,additional feature,result,chunk type,representation,shallow parsing,right-branching tree,virtual node,baseline,8 s vms-voting,matsumoto,rw linguistic feature,boosting,result,harmonic mean,precision,recall,6 d iscussion,efficiency,number,feature, wsj parsing,shallow parsing,subtrees,feature candi date,selects,relevant subset,feature,subtrees,tree kernel,number,ac tive feature,million,accuracy,sparse feature space,tree ker nel,result,first intuition,subtrees,parameter,sparse feature representation,practice,feature,show example,active feature,weight,shallow parsing task,long depen dencies,subordinate,markov-based shal low parser,cap ture,long dependency,model automat,useful subtrees,improve ment,subordinate phrase,precision recall,con jp,result,large positive weight,negative weight,improvement,subordinate phrase,rel ative error reduction,subordinate phrase,f-measure,rerank, wsj parsing,shallow parsing,real application,relationship,previous work tree kernel,all-subtrees representation,problem,calculation,inner-products,implicit calculation yield,practical computa tion,training,number,kernel evaluation,real application,tree kernel need,decay factor,contribution,sub tree,optimal decay factor,accuracy,selection,method,all-subtrees rep resentation,exact computa tion,np-complete,several ap proximations,efficient parsing,critical difference,sparse solution,redun dant subtrees,subtrees, a l inux pc,pentium,active tree,effect,active tree,example,active feature,subtrees,s-expression,shallow parsing task,special phrase,select relevant subtrees,result, wsj parsing,technique,regularization framework,re dundant subtrees,method,million,subtrees,real problem,7 c onclusions,new application,parse reranking,subtrees,distinct feature,set-up greatly,feature space,l1-norm regularization,relevant feature,accuracy,kernel method,small num ber,feature,subtrees,eferences kenji abe,shinji kawasoe,tatsuya asai,hiroki arimura,setsuo arikawa,substructure discovery,semi-structured data,grammar,experience,rens bod,minimal set,fragment,maximal parse accuracy,eugene charniak,maximum-entropy-inspired parser, naa cl,michael collins,nigel duffy,algo rithms,kernel,discrete struc tures,voted perceptron,michael collins,head-driven statistical model,natural language parsing,thesis,university,pennsylvania,michael collins,discriminative reranking,natural language parsing,michael collins,algorithm,named-entity extraction,boosting,voted perceptron,yoav freund,robert,schapire,yoram singer,efficient boosting algorithm,preference,journal,machine learning research,taku kudo,yuji matsumoto,support vector machine, naa cl,taku kudo,yuji matsumoto,algo rithm,classification,semi-structured text,emn lp,simon perkins,kevin lacker,james thiler,graft ing,incremental feature selection,gradient descent,function space,journal,gunnar,robust boosting,convex optimiza tion,thesis,department,computer science,uni versity,potsdam,robert,schapire,yoav freund,peter bartlett,wee sun lee,margin,new explanation,effectiveness,method,fei sha,fernando pereira,conditional random field, hlt -naa cl,tjong kim sang,sabine buchholz,introduc tion, con ll-2000 shared task,chunking, con ll-2000,vladimir,vapnik,statistical learning theory,wiley interscience,guizhen yang,complexity,maximal fre quent itemsets,maximal frequent pattern,frequent tree,forest,tong zhang,fred damerau,david johnson,generalization,winnow,journal,machine learning research,proceeding,international conference,computational linguistics,annual meeting,sydney,association,computational linguistics training conditional random field,multivariate evaluation measure jun suzuki,erik  mcd ermott,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan jun,isozaki cslab,framework,multivariate evaluation mea sures,non-linear measure,f-score,framework,error minimization,proach,simple solution,evaluation mea sure,sequential segmentation task,text chunking,entity recognition,loss function,evaluation measure,segmentation,ex periments,method,standard  crf training,formalism,lafferty,conditional model,output,discriminative model,markov random field,ob servations,generative model,f model,output,distribution,flexible feature,function,multiple observation,great ben efit,several application,shallow par,pereira,information,introduction,intensive re search,effec tiveness,first approach, crf pa rameters,criterion,conditional probability,ml criterion,training data,large number,correlated feature,maximum,criterion,parameter,natural choice,pereira,bayes approach,prior distribution,parameter,large margin criterion,model parameter,taskar,tsochantaridis,excellent re sults,various task,real world task,task-specific evaluation mea sures,non-linear measure,criterion,op timization,linear combination,av erage accuracy,error rate,task-specific evaluation measure,example,text chunking,entity recognition,segmentation,inconsistency,objective function dur,training,task evaluation measure,suboptimal result,inconsistency,multivariate optimization method,joachim,f-score optimization method,logis tic regression,jansche,spirit,generalization framework, crf training,error rate,evaluation mea sure,framework,evaluation measure,interest,loss function,loss function,training objective function,framework,ap proach,error rate minimization,speech,pattern recognition com munity,framework,katagiri,framework, mce criterion training,theoretical background,method,ap proach,following,new framework,ex ample,multivariate evaluation mea sures,segmen tation f-score loss function,training criterion given,con ditional probability,product,po tential function,clique,interdependency,non-negative real value,tential function,normalization factor,output value,definition,pereira,log-linear combination,weighted fea tures,individual potential function,feature vector,clique,global feature vector,probable output,arg maxy,decision,following dis criminant function,maximum,conditional probability,parameter,basic  crf training criterion,correct output,conditional log-likelihood,log-loss function,follow ing loss function,ml criterion training,over-fitting,maximum  ap,criterion,parameter,criterion,following loss function, map criterion training,several possible choice,siders, a g aussian prior,essential difference,sim ply,prior term,objective function,refers, map criterion training,parameter,gradient,parameter,gradient,gradient term,detail,actual optimization procedure,linear chain  crf,typical  crf ap plication,pereira,3 m ce criterion training,frame work,family,ap proaches,design,criterion,empirical loss,smooth ap proximation,classification error, mce loss,misclassifica tion measure,discriminant func tions,smoothing parame ters, mce loss function,binary classification error,impor tant property,framework,principle,optimal bayes error,incorrect modeling assumption, mce framework,evaluation measure,classification error,linear combination,error rate,variety,evaluation measure,ap proach,article,framework, mce crite rion training,error rate optimiza tion,example,different multivariate evaluation measure, mce criterion training,brief overview,output,bayes decision rule,probable output,maximum,general discriminant function,discriminant function,possi ble output,correct output,mi classification,minimization,error rate,minimization,training data,step function,oth erwise,discrimi nant function,correct output,maximum incorrect output,appropriate function,op timization,discontinuous function,parameter,ne choice,contin uous misclassification measure,soft-max,positive constant,approach,converges,misclassifi cation measure,non-linear measure,example,appendix,concern,step function,minimization,smoothing function,typical choice,sigmoid function,good approximation,0-1 loss,hyper-parameter,choice,logistic function,upper bound,0-1 loss,logistic loss,conventional  crf loss function,convexity,sigmoid function,function,hyper-parameters,training,regularization term,over-fitting,objective func tion, mce criterion,regularization term,objective function, mce criterion,error rate,fmc el,actual use,formalization,discriminant function, mce criterion,selection,re striction,choice,logistic function,gradient,loss function,derivative,derivative,parameter,loss function,appropriate choice,detail,appendix,special case,framework,method,generalized framework, crf training,optimization procedure,linear chain  crf,ob jective function,gradient,variant,viterbi,pereira,parameter optimization process,gradient descent,quasi-newton method, l-b fgs ,nocedal,correct,maximum incor rect output,maximum output,viterbi al gorithm,maximum incorrect output,algorithm,maximum output,viterbi algorithm,objective func tion,problem,optimization,le roux,first order,optimization method,second-order method,quick prop,fahlman,s-based method,good experimental optimization re sults,4 m ultivariate evaluation measure,error rate,framework,mce criterion training,embedding,linear combination,error rate,evaluation measure,non-linear measure,several non-linear objective function,f-score,text classification, ble u-score,evaluation mea sures,statistical machine translation,reference,framework, mce criterion training,hereafter,sequence,linear chain  crf,linear chain  crf,text chunking,shared task,conference,natural language learn,typical crf application,extrac tion,pre-defined segment,tar get segment,show typ ical example,sequential labeling problem,scheme,ramshaw,marcus,scheme,scheme,question,beginning,target segment,therefore,segment,sequence,output,segmentation f-score loss,standard evaluation measure,segmentation f-score,buchholz,current account deficit,nation official    e keus smith head,y1 y2 y3 y4 y5 y6 y7 y8  y9d,example,text chunking,false negative count,respec,individual evaluation unit,individual output yi,output sequence,segment,segment-wise loss,contrast,standard  crf loss,sequential loss,kakade,point-wise decision,point-wise discriminant function,length,tain yi,th position,output,point-wise dis criminant function,kakade,marginals,output sequence correspond,j-th segment,sj repre,sequence,example,output,position,segment-wise discriminant function,output,piece-wise dis criminant function,property,segment,correct segment,test data,consistency,test ing,training,length,seg ments,segment-wise misclassifica tion measure,sim ply,discriminant function,entire sequence,segment wise,segment sequence,correct output,possible segment,evaluation function,return,segment sj,target seg ment,otherwise, ner data,target segment,segment,sigmoid loss,second summation,summation,correct segment,contrast,second summation,possible segment,account,correct segment,efficient way,possible segment,context,semi-markov  crf,sarawagi,simple alter native method,segment,maximum incorrect output,segment,practice,calculation cost,method,experiment,next section,segmentation,objective function,seg mentation, mce criterion,derivative,wherezn andzd,numerator,nominator,optimization process,segmentation f-score objective function,cal culate,forward,backward viterbi algorithm,variant,forward backward algorithm,pereira,numerical optimization method,optimization,5 e xperiments,chunking,shared task, con ll,buchholz,de meulder,corpus,section,training data,sentence,section,test data,sentence,different chunk-tags,outside,target chunk,segment,english  ner data,reuters corpus21,sentence,training,development,test data,entity tag,comparison method,parameter,training procedure,pereira, l-b fgs  optimization,gov data reuters reuters,quickprop optimization2,l2 norm regularization,interval,development data3,tuning,function hyper-parameters,experiment,performance,evaluation measure,performance,average sentence ac curacy,sequential accuracy,feature,regard,basic feature,chunking,matsumoto,feature,result, con ll-2000,basic feature,bigram combination,feature,part-of-speech tag,window size,contrast,original feature,feature,addition,character-level regular expression,others,prefix,suffix,let ters,basic feature,bigram combination,window size,feature,ex ternal information,dictionary,many previous study,ex periments,result,discussion,experiment,impact,inconsistency,objective function,evaluation measure,result,chunking,column,perfor mance,segmentation f-score,convergence,online gpd optimization,first ten iteration,common development,system,sentence,training data,development,training data,performance,text chunking,entity recognition data,sentence accuracy,result,addition,error rate version,sigmoid function,quickprop,con ditions,result,chunking,significant difference, mcn emar test,correctness,individual output,sentence,training,test data,correspond,chunking data,imbalanced data set,accuracy-based evaluation,rea son, ner result,chunking result,difference,objective function,correspond,result,effectiveness,ob jective function,evalua tion measure,target task,result,error rate,opti mal,segmentation f-score eval uation measure,inconsistency,task evaluation measure,objec tive function,training,overall performance,initial parameter,convex,parameter,objective function,initial parameter,optimization,performance,initial parameter,result,quickprop, l-b fgs ,local optimum,previous experiment,parameter,ex periment,parameter,initial value,evaluation,ap pear,feature,result,chunking,parameter initialization,initialization, map parameter value,improves performance,work various loss function,kakade,design,loss function,consideration,general framework,loss function,non-linear loss function,pre vious work,matsumoto,f-score,vot ing,several model,support vec tor machine,experimental setting,feature,map parameter initialization,f-score,result,manual parameter,direct compari son,previous work,experimental setting,different feature,performance,mc callum,map training,feature selection,chitecture,similar result, map re sults,7 c onclusions,framework,optimization criterion,multivariate evaluation measure,general framework, mce criterion,approximate segmen,f-score objective function,experimental result,inconsistency,task evaluation measure,ob jective function,overall performance,target task,change,feature,appendix misclassification,misclassification measure,soft-max,katagiri,comparison,loss function,reference,hofmann,loss function,optimization method,discrimi native learning,label sequence, emn lp,fahlman,empirical study,learning speech,backpropagation network,technical re port  cmu cs-88-162,axi mal figure-of-merit approach,text categorization,nilsson,raphael,ormal basis,heuristic determination,system science,maximum, f-m easure training,logistic regression model,upport vector method,multivari ate performance measure, icm l-2005,katagiri,discriminative learning,signal processing,roweis,alterna tive objective function,markovian field,icm l-2002,new dis criminative training algorithm,generalized descent method, iee e wo rkshop,neural network,signal processing,matsumoto,support vector machine, naa cl-2001,pereira,conditional random field,probabilistic model,segmenting,labeling sequence data, icm l-2001,nocedal,limited memory bfg s me thod,large-scale optimization,mathematic programming,early result,named entity recognition,conditional random field fea ture induction,web-enhanced lexicon,con ll-2003,minimum error rate training,statistical machine translation,szummer,bayesian con ditional random field,ramshaw,transformation-based learning, mcd ermott,optimization method,discriminative training,tjong kim sang,buchholz,introduction, con ll-2000 shared task,chunking,tjong kim sang,de meulder,introduction, con ll-2003 shared task,language-independent named entity recognition, con ll-2003,semi-markov condi tional random field,information extraction,pereira,con ditional random field,koller,max-margin markov network,hofmann,margin method,structured,proceeding,columbus,association,computational linguistics semi-supervised sequential labeling,segmentation,giga-word scale unlabeled data jun suzuki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan jun,isozaki cslab,evidence,unlabeled data,semi-supervised learn ing,performance,part-of-speech tagging,syntactic chunking,entity recognition,powerful semi-supervised discriminative model appropriate,large scale,experiment,test collection, con ll,task data,nlp task,1g-words,unlabeled data,amount,unlabeled data,performance improvement,addition,result,re sults,test collection,1 i ntroduction today,large amount,applica tions,performance,development,effective framework,unlabeled data,tractive,machine learning, nlp com munities,supervised learning,real world application,impor tant  nlp task,tag ging,syntactic chunking,application,sequential labeling,segmentation problem,large amount,labeled training data,situation,learning,competitive result,result,state-of-the art,learning,substantial performance improvement com,state-of-the-art,re sults,syntactic chunking, con ll,task data,tjong kim sang,buchholz, con ll,task data,tjong kim sang,meulder,question,behavior,unlabeled data,question,large amount,unlabeled data,presence, ssl method,regard,unlabeled data,scalable model,used test collec tions,mar cu et, pos tagging, con ll,task data,syntactic chunking,task data,1g-words,unlabeled data,performance improvement,respect,data size,addition,per formance improvement,unseen data,viewpoint,unlabeled data coverage,result,current system,contribution,simple,power ful task-independent model,semi-supervised se quential labeling,segmentation,current result,collection,unlabeled data,improvement,2 c onditional model,natural semi,extension,conventional supervised conditional random field,approach,suzuki,conventional,output,possible input,clique,undirected graphical model,interdependency,output,corresponding clique,conditional probability,product,addition,feature vector,pa rameter vector,length, ac rf,par tition function,po tential function,non-negative real value func tion,feature,clique,feature vector,corresponding clique, crf suppose,probability mod el,j-th joint pm,model parameter,graph structure,clique,corporate generative model,bayesian net work,joint pm,difference,generative model,graphical model,conditional pm,difference,vi olations,approach,concatenation,feature vector,log likelihood,j-joint pm,new potential function,corresponding clique,potential func tion,conditional model,conditional model,conditional model, map estimation,prior probability distribution,equation,equation,log-likelihood,log pj,fea ture function, jes s-cm,joint pm,global con vergence condition,result,global maximum,gradient,supervised  crf,pereira,forward-backward algorithm,sequence model,gradient-based optimization algorithm,manner,supervised  crf parameter estimation,unlabeled data,standard discriminative learning method,correct,unlabeled data,generative ap proach,well-known way,incorpora tion,data du xm,mation,setting,marginal distri bution,joint pm,variable,parameter estimation approach,non-generative approach,suzuki,discriminant function,parameter, jes s-cm, mdf estimation,following objective func tion,prior probability distribution,normalization factor,determination,discriminant func tion, jes s-cm,equation,local maximum,iter ative computation,em algorithm,scalability,efficient training algorithm parameter estimation algorithm,objective function,figure,algorithm, jes s-cm,situation,unlabeled data,calculation cost,unlabeled data,overall parameter estimation procedure input,unlabeled data du xm,uniform distribution,estimate,procedure,output, a j ess-cm,parameter estimation algorithm, jes s-cm,large scale, mdf estimation,figure,addition,cal culation cost,parameter,number,suzuki,result, jes s-cm param eters,iteration, mml estimation,single hmm,em algorithm,time opti mizations, map estimation,conventional supervised  crf,addition,parameter estimation algorithm,parallel computation,comparison,hybrid model ssl,hybrid generative discriminative ap proach,suzuki,log-linear model,several discriminative model,generative model,hybrid model,labeled training data,labeled training data,unlabeled data,solution,amount,labeled training data,distinct set,feature,sev eral set,dis criminative model,addi tional process,entire parameter estimation procedure,single pas, jes s-cm,simpler version,hybrid model,model structure, con ll,suzuki,configuration,performance im provement,full bene fit,labeled training data,parameter,conditional model,hybrid model,labeled training data, jes s-cm,eral advantageous characteristic,hybrid model,3 e xperiments,experiment, pos tagging,syntac tic chunking, ner performance,1g-words,unlabeled data,data set,performance,previ ous study,test collec tions, pos tagging experiment,wall street journal, ptb  iii,marcus,data split,syntactic chunking,periments,training,devel opment,test data,shared task, con ll,tjong kim sang,buchholz,tjong kim sang,meul der,training,development,test data,experiment,english gigaword corpus, tip -1t, ner experiment,described,pereira, iob tag,previous position label, ptb  iii,development, ptb  iii, con ll,task data, con ll,task data,b-tagging 2nd-order encoding,data set,time period,development,detail,training,development,test data,experiment data abbr,time period,word tipster,reuters,corpus,gigaword,experiment,corpus,wall street jour nal article,english gigaword corpus,article,news source,unlabeled data,total size,data reach 1g-words,design, jes s-cm,graph structure,linear chain crf, jes s-cm,regard,design,fea ture function,feature tem plate,experiment,focused token position,bi-gram,feature,po sitions,feature template,word type,feature,capitalization,existence,punctua tion,sutton,regular expression,template,syntactic chunking,template,template,part-of-speech lwd,lowercase,word type,suffix,word table,feature template,experiment,roceedings,47th annual meeting, ijc nlp , afn lp,suntec,singapore,2-7 august, afn lp a s yntax-free approach,japanese sentence compression tsutomu  hir ao,jun  suz uki ,2-4 hikaridai,seika-cho,soraku-gun,japan hirao,isozaki cslab,syntactic parser,sentence,mean ing,reference compres sion,syntactic structure,original sentence,demand sentence compression,parsing stage,alternative,syntactic par,novel term,technique,positional infor mation,original sentence,novel language model,statistic,original sentence,general corpus,experiment,human subjective evaluation,au tomatic evaluation,method,method,state-of-the art conventional technique,method,syntactic parser,method,1 i ntroduction,sentence,original meaning,subject-predicate rela tionship,original sentence,compression,accordance,conventional sentence compression method,syntactic parser,english sentence,full parser,parse tree,knight,turner,charniak,dependency tree,full parse tree,takeuchi,matsumoto,nomoto,parsing approach,compressed output,compression,moderate compression rate,approach,nomoto,clarke,lapata,sentence,se quence,structural information,dependency tree,sequence,feature,method,arbitrary word,original sentence,boundary deter,tree structure,syntactic information,dependency tree,syn tactic structure,sentence,example,many case,sen tence,intermediate node,syn tactic tree,source sentence,compression,depen dent,syntax,reference compression,on-demand sentence compression,parsing stage,syntax-free sequence,sentence compression method,subject-predicate relationship,sentence,retain fluency,syntactic parser,novel fea tures,intra-sentence positional term,positional informa tion,summarization-oriented fluency statistic,original sentence,general lan guage model,weight parameter,feature,katagiri,framework,experiment,human subjective,automatic evaluation,method,sentence chunk7,chunk6 part,suitei shi ta,nitsuite fukutake ga,kouhyou,te nai center,chunk  1c hunk  2 c hunk,suitei shi ta,nitsuite fukutake ga center,center,compression compound,if igure,example,dependency relation,original sentence,compressed variant,conventional sequence-oriented meth od,syntactic parser,2 a nalysis,reference compression syntactic information,im proved compression performance,syntactic structure,sentence,figure,exam ple,english translation,source sentence,ential treatment,regard,assessed score,question,series,center examination,compression,fukutake,preferential score,question,series,figure,syntactic chunk,bunsetsu,solid arrow,pendency relation,words2,dependency relation,com pression,compound noun,component,different portion,original sentence,regard,syntactic constraint,compressed sen tence,content,functional word,content word,dependency relation,bun setsu,word dependency,matsumoto,compression,tree trimming,investigation,corpus,japanese sentence,experimental evaluation,segment,original tree structure,compress sentence,intermediate node,dependency tree,compression,adequacy,flu ency,sentence compression,syntax,reference compression,sentence compression method,intermediate node,syntactic tree ag,tree-scoped boundary,addition,sentence compression method,syntactic parser,problem,sentence output,state-of-the-art japanese dependency parser contain,matsumoto,sentence,source,training data,parser,performance,overall performance,sentence compression,summariza tion system,megabyte,document,parser,demand summarization system,syntax free sequence-oriented sentence compression method,alternative,syntactic parsing,novel feature,intra-sentence positional term,syntax-free sentence com pressor,sentence compression, ac ombinatorial optimization problem suppose,compression system,sen tence x1,j-th word,input sentence,system,th word,output sentence,word yi,original sentence,example,source sentence,compressed variant,significance score,sentence,method,first term,impor tance,output sentence,linguistic likelihood,adjacent word,output sentence,feature,significance score,linguistic likelihood,global term,scheme,significance score,text corpus,contrast,term weighting,positional significance score,sentence,following hypothesis,significance,position,sentence,main subject,sentence,beginning,main verb phrase,phrase,sentence,knowledge,function,term weight,relative position,original sentence,length,number,character,source sentence,accumulated run,character,standard deviation,normal distribution,mixture param eter,part-of-speech tag,framework,language model many study,sentence compression,n-gram language model,linguistic likelihood,compressed sentence,huge volume,text data,long sentence,n-gram distribution,short sen tences,long sentence,n-gram probability,dis agrees,intuition,sentence com pression,huge corpus,compressed sentence,headline,sentence,newspaper article,corpus size,novel linguistic likelihood,statistic,original sentence,patched language model,word bigram probability,first line,equation,observation,sentence align ment task,bigram,compressed sen tence,original sentence, pos bigram,un grammatical sentence,linguistic likelihood,adja cent word,parameter optimization,sentence compression,class problem,original sentence class label,compressed output,interdependence,framework,katagiri,goodness,sequence,t-th original sentence,training data,reference compression,compressed sentence output,system, mce framework,misclas sification measure,difference,reference sentence,non-reference output,parameter,gradient,function,following sigmoid func tion,measure,constant parameter,parameter,iterative form,example,parameter optimization procedure,ald et,lafferty,reason,4 e xperimental evaluation,evaluation measure,lead sentence,lead sentence,first sentence,article exclud,headline,length,number,mainichi newspaper,different ideal compression,reference compres sion,sentence,compression rate,average length,input sentence,reference compression, mce learning,reference compression, ble score,reference compression,correct data,training,ref erence compression,reference compression,automatic evaluation,hu man subjective evaluation,automatic evalua tion,papineni,whole data set,train ing,remainder,evaluation,test data,test block,compressed sentence,human subject,sentence,fluency,importance,source sentence,compressed sentence,comparison,sentence compression method,effectiveness,feature,method,state-of-the-art japanese sentence compressor,sequence-oriented approach,feature,exper iment,result,automatic evaluation label  ble upr,method,dependency parser,example, idf term,word bigram,part-of-speech bigram,dependency probability,word dependency probability,relative-cabocha,mat sumoto,n-gram lan guage model,year set,mainichi news paper article,parameter,framework,esults,discussion,automatic evaluation table,evaluation result,compression rate,dependency probability,method, ble score,performance,result,hypothesis,significance score,position,sentence,sentence compression,figure,example,gaussian mixture,example,gaussian mixture,predicted parameter,parameter,figure,positional weight,many case,subject,beginning,japanese sentence,predicate,bigram language model,performance sig,result,n-gram lan guage model,sentence compres sion,n-gram probability,corpus,long sentence,bigram,compressed sentence,source sentence,dependency probability,ex ample, ble score,difference,significant difference,wilcoxon,rank test,ble score,introduction,dependency probability,probability,feature,word pair,bigram,source sentence,dependency probability,informa tion,feature vector,number,feature,result,human subjective evaluation label fluency importance,result,human subjective evaluation,human subjective evaluation,method,human compression,perfor mance,sentence,test corpus,compressed variant,fluency,importance,result,mean score,judgement,standard deviation,result,human compression,fluency,impor tance,human compression,compression method,result,sentence,compression rate,automatic method,method,fluency,importance,performer,method sig,metric,method outper,difference,significant test,significant dif ferences,method,explicit syntactic information,fluency,importance,effectiveness,new feature,comparison,method,method,decoding time,test sentence,coretm  2 e xtreme qx9650,result,second,sentence sec,second,method,latter,depen dency parser,speed advantage,on-demand sentence compression,work conventional sentence compression method,approach,sentence,meaning,stance,english sentence compression,full parse tree,generative model,knight,turner,charniak,tive model,knight,japanese sentence,full parse tree,sentence compression method trim dependency tree,discrim inative model,takeuchi,matsumoto,nomoto,simple lin ear,feature,approach guarantee,sentence,source,section,ap proach,japanese sentence com pression,many case,human-produced compression,alternative,tree trimming approach,sequence-oriented approach,nomoto,clarke,lapata,random field,approach,clarke,lin ear model,simple combined feature,sentence,word sequence,structural information,full parse tree,dependency tree,sequence,feature,advantage,method,approach,arbitrary word,original sen tence,boundary,tree structure,approach,japanese compression,tree trimming,syntactic information,pendency tree,syntactic parser,7 c onclusions,syntax free sequence-oriented japanese sentence compression method,novel feature,method, a p o tagger,supe rior,method,syntactic parser,experiment, a j apanese news corpus re,effectiveness,new feature,method,explicit syntactic information,statistical significance,state of-the-art japanese sentence compression method,sequence-oriented approach,contribution,japanese sentence,syntactic structure,intermediate node,dependency tree,drop word,bunsetsu,alternative,syntactic parser,novel feature,intra-sentence positional term,effectiveness,human evaluation,method,method,dependency parser,reference,clarke,lapata,sentence compression,comparison,domain,train ing requirement,evaluation measure, col ing ,new approach,decomposition,human-written summary sentence, sig ir,katagiri,discriminative learning,signal processing,summarization,yond sentence extraction,artificial intelligence,matsumoto,algo rithm,classification,semi-structured text, emn lp,matsumoto,japanese de pendency,using relative preference,de pendency,pereira,condi tional random field,probabilistic model,seg menting,sequence data,crammer,pereira,line large margrin training,dependency parser,discriminative sentence com pression,soft syntactic evidence,discriminative sentence compres sion,conditional random field,information processing,management,generic sentence trimmer,evaluation,japanese sentence compression method,phrase significance,inter-phrase dependency,method,automatic evaluation,machine translation,40th annual meeting,association,matsumoto,acquisition,sentence reduction rule,quality,text summary, nlp r,charniak,sentence compression,tsujii,cfg parse tree,sentence compression,machine,approach, col ing ,question classification, hda g ke rnel jun suzuki,hirotoshi taira,yutaka sasaki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan jun,sasaki,machine,question classification method,kernel function,hierarchical di,kernel, hda g ke rnel,natural language data,sev eral level,relation,kernel func tion,practical cost,structure,method,ques tion classification experiment,japanese question,question type,result,method,performance,question classification,conventional method,bag-of-words,combination,extraction,correct answer,free-form factual question,large collection,question answering track, tre c-8,definition, tre c qa track,passage retrieval method, tre c-8, odq task,question,string,large set,news wire, odq task,exact answer,question,instance,system,question,queen victoria,system,following compo nents,question analysis,question,question type,keywords,text retrieval,top paragraph,docu ments,result,question anal ysis component,answer candidate extraction,didates,question,docu ments,text retrieval component,result,question type,answer selection,question,answer candidate,answer,extraction component,important process,target,intention,question,sought-after answer,process,question type,question,question classification,question type,result,question classification,cor rect answer,possible answer,didates,noun phrase,entity,question clas sification,benefit,powerful restric tion,practical number,answer candidate,answer se lection process,machine,approach,classification,harabagiu,hermjakob,ker nel,suzuki,structured natural language data,struc tures,feature,structure,explicit representa tion,numerical feature vector,framework,question classification,suzuki,information,se mantical information,question,question classification performance,information,simple key term,section,question classifica tion problem,section,method,question classification,sec tion,experiment,result,2 q uestion classification question classification,question,question type,general concept,qa system,result,question classification,downstream process,selection,correct answer,large number,answer candidate,source document,result,question classification,la bel,question type,number,answer candidate,noun phrase,source docu ments,correct answer,question,candidate,result,question classification,efficient method,correct answer,question classification,important process, aq system,performance,question classi fication,total performance,qa system,question type,question numerous question taxonomy,standard exists, tre c qa track,system,question taxonomy,original question type,large hierarchical question taxonomy,ques tion type,hierarchical question taxonomy,taxonomy,question type,viewpoint,target intention,question,hierarchical struc tures,question taxonomy,different researcher,pur pose,question classification,large number,answer candidate,intention,question type,question taxonomy,hierarchical structure,downstream,question type,target intention,hierarchical structure,property question classification,cate gorization,major task,quire classification,text catego rization,document,newspaper article,article,question classification,short question sentence,target answer,intention,question,question classification,complicated feature,text categorization,question classification,information,simple key term,bag-of-words,high performance,text classification,previous work,suzuki,sequential pattern,different level,attribute,semantical information,performance,question classification,ex periments,previous work,semantical feature,ques tions,potential,performance,question classification,high performance question classification,semantical feature,question,learning,classification task,machine,ap proach,question classification,machine,approach,several advantage,man ual method,construction,manual classifier,question,tedious task,analy si,large number,question,question,question type,lexical item,explicit represen tation,mapping,machine learning,feature,classifier,new taxonomy,short time,machine,algorithm,joachim,haruno,state-of-the-art performance,text categorization,ques tion classification,similar process,catego rization,dag ke rnel,design,kernel function,hot topic,research field,machine learning,specific kernel,perfor mance,specific task,specific kernel,new feature space,conventional method, hda g ke rnel,new kernel function,structured natural lan guage data,discussion,pre vious section,information,semantical information,high performance question classification, hda g ke rnel,performance,question classifica tion, hda g ke rnel,various linguis tic structure,relation,feature,convert ing structure,numerical feature vector,feature space figure,example,structure,question, hda kernel,figure, hda kernel,several level,relation,node represent several level,re lations,suppose,graph inside,non-terminal node,attribute,part-of-speech tag,semantic information,wordnet,fellbaum,class name,entity,attribute,attribute,attribute sequence,sequence,tribute,sub-paths,hda g,attribute sequence,element,feature vector,framework, hda g ke rnel,node skip,ex traction,attribute sequence,conference,empirical method,natural language processing,seattle,washington,18-21 october,association,computational linguistics shift-reduce word reordering,hajime tsukada,jun suzuki,seika-cho,soraku-gun,619-0237 japan,hayashi,novel word,shift-reduce parser,inversion transduction grammar,rich syntax parsing feature,word re ordering,linear time,phrase-based machine trans,japanese-to-english patent task,experimental result,method,significant improvement, ble score, ble score,baseline  pbm system,tree-based mt,graehl,knight,chiang,galley,system,great success,many problem,distinct language pair,long-distant word reordering,word reordering,promis,translation pro ce,preordering,collins,uszkoreit,probabilistic model,reorder,decision,syntactic parse tree,parser-based word reorder,shift-reduce parser,version transduction grammar,knowledge,first study,shift-reduce parser,word reordering,parser-based reordering approach,rich syntax parsing feature,decision,propoesd method,description,mt system,non-local feature,gram word,reordered string,non-local feature,complexity,shift-reduce parser,optimal solution,experiment,method,j-to-e patent task,training data,little noise,method,used j-to-e setup,language-dependent scheme,method, a j to-e postordering method,key algorithm,training data,2 p ostordering,translation step,input sentence,source-ordered transla tions,translation,target language,postordering,second step,second step,training data,postordering parser,language-dependent rule,head finalization,syntactic head,lexicalized parse tree,english sentence,telescope,telescope,telescope,telescope,article,wawatashi figure,example,head-finzalizaton process,english-japanese sentence pair,left-hand side tree,original english tree,right-hand side tree,head-final english tree,corresponding syntactic constituent,result,terminal symbol,english tree, a j apanese-like order,example,head-finalization,right hand side,english tree,left-hand side,parent node,swapped edge,sym bol,example,nonterminal symbol pp,noun phrase,telescope,word alignment,isozaki,article,japanese,article,inserted japanese particle,english sentence,nonterminals,phrase,deleted article,article,phrase,original english sentence, hfe tree,symbol,annotated article,japanese parti cles,parser, hfe tree,main difference,berkeley parser,petrov,shift-reduce parsing model,non-local task specific feature,gram word,reordered string,efficiency,method,knight,chander,reordering,insert article,english translation,insert,action,parser,article generation problem,gram language model,compli cates,approach,parser,advantage,shift-reduce parser,additional operation,monolingual  itg tree,uszkoreit,nonterminals,terminal,production rule,nontermi nals,article,start symbol,terminal production rule,binary production rule,nonterminals,right-hand side,second rule generates,phrase,reverse order,experi ments,unary production rule,hift-reduce parsing given,shift-reduce parser,partial derivation,buffer,input word,action,parse tree,following,parser,configuration,step size,leftmost span index,next input word,buffer,predictor states1,stack element,following com ponents,partial derivation tree,wright,part-of-speech tag,subtree,head index,article,leftmost,phrase,proposed system,action,reduce-sr-x,shift-x action,next input word,part-of-speech tag,deduction step,insert-x action,article,wright,wright,side condition,parser,article,phrase,article,input string,reordered string,reduce-mr-x action,deduction rule,notion,predictor state,detail,feature,right subnodes,head index,buffer element,part-of-speech tag,wright0,wright1,action,wright0,nonterminal,head word wh0,right nonterminal,action,straight order,leftmost word,phrase,word wleft1,rightmost word,phrase,word wright0,difference,reduce-sr-x action,new stack element,reduce-sr-x action generates,wright1,action,reverse order,leftmost word,rightmost word,linear model,averaged perceptron,collins,feature,experiment,feature,feature,train dev test9 test10,data statistic,4 e xperiments,experiment, ntc ir-9,patent data, a j apanese-english language pair,mecab2,japanese morphological analysis,tsujii,english training data,converted parse tree, hfe tree,head-finalization scheme,grammar rule, hfe tree, hfe tree,shift-reduce parser,lexical ized reordering, sri lm,stolcke,japanese sentence, hfe sentence,english sentence,shift-reduce parser,1-best  hfe sentence,strategy,linear inteporation,mt cost,parser cost,lm cost,english sentence,n-best  hfe sentence,main result,main result,method,conventional pbm system,method,system,system3,feature,feature,accuracy,analysis,gram precision,system,improvement,1-gram pre,google,mt toolkits,experiment,test9 test10 ble u ribes bleu  rib e hfe,effect,article generation,note evaluation score,translation,system,article,system, hfe data,article,mt system,shift-reduce parser,system inserted article,translation,article generation method,gram precision moses,precision,system,test9 data,cisions,main factor,ter performance,system, pbm system,1-gram presicions,effectiveness,joint reordering,approach,result,article,great effect,mt evaluation,system,result,poste diting,arti cles,joint approach,system,5 c onclusion,shift-reduce word,j-to-e postordering,experi mental result,method,performance, a p bmt  system,future work,method,use fulness,various language datasets,general method,word align ments,swap information,galley,test9 test10 ble u ribes  time,system comparison,sentence,denotes,experiment,eferences david chiang,hierarchical phrase-based model,statistical machine translation,proceeding,annual meeting,association,computa tional linguistics,michael collins,brian roark,incremental parsing,perceptron algorithm,proceeding,annual meeting,association,com putational linguistics,michael collins,philipp koehn,ivona kuc,statistical machine translation,proceeding,annual meet,association,computational linguistics,john  den ero,jakob uszkoreit,sen tence structure,parallel corpus,proceeding,conference,empirical method,natural language processing,michel galley,jonathan graehl,kevin knight,daniel marcu,steve  den eefe,wei wang,ignacio thayer,scalable inference,training,context-rich syntactic translation model,proceed ings,international conference,compu tational linguistics,annual meeting,association,computational linguistics,isao goto,masao utiyama,eiichiro sumita,post-ordering,japanese-english statisti cal machine translation,proceeding,50th annual meeting,association,computational linguistics,jonathan graehl,kevin knight,tree transducer,liang huang,kenji sagae,dynamic program ming,linear-time incremental parsing,proceed ings,annual meeting,association,computational linguistics,hideki isozaki,jun suzuki,hajime tsukada,masaaki nagata,sho hoshino,yusuke miyao,asian language,ishwar chander,postediting,document,proceeding,national conference,artificial intelligence,philipp koehn,hieu hoang,alexandra birch,chris callison-burch,marcello federico,nicola bertoldi,brooke cowan,wade shen,christine moran,richard zen,open source toolkit,sta tistical machine translation,proceeding,45th annual meeting,interactive poster,demonstration session,ichi tsujii,feature,probabilistic hpsg parsing,computational linguistics,slav petrov,dan klein,inference,unlexicalized parsing,human language tech nologies,conference,north american chapter,association,computational linguis tic,andreas stolcke,jing zheng,wen wang,victor abrash,sixteen,update,outlook,proceeding, iee e au tomatic speech recognition,understanding workshop,katsuhito sudoh,hajime tsukada,masaaki nagata,post-ordering,statistical machine translation,stochastic inversion transduction grammar,bilingual parsing,parallel corpus,computational linguistics,proceeding,conference,empirical method,october,association,computational linguistics dependency-based discourse parser,single-document summarization yasuhisa yoshida,jun suzuki,tsutomu hirao,seika-cho,soraku-gun,619-0237 japan yoshida,suzuki,tsutomu,current state-of-the-art single document summarization method gen,problem,optimal,sub tree,document,gold  dep dt,gold rhetorical structure theory-based discourse tree,large difference,rou ge score,system,gold dep dt,system, a d ep-dt, rou ge score,novel discourse parser,evaluation result,parser,state-of-the-art  rst dt parser,equivalent  rou ge score,document,coherent summary,several discourse,summarization method,kikuchi, rou ge score,summarization benchmark data, rst discourse treebank,carlson,method,utilizes discourse tree,discourse,summarization approach,high-quality summary,possible weakness,discourse-based sum marization technique,accuracy,discourse parser,example,discourse-based summa rization method utilize discourse tree,thompson,discourse information,current state-of-the-art  rst parser,hernault,off-the-shelf discourse parser,empirical evidence,summary,discourse tree,gold dis course tree,background,appropriate discourse parser,discourse-based summarization,first focus,discourse-based single document summarization method,method,single doc ument summarization problem,method,heuristic rule, a d ep-dt parser, dep dts,state-of-the-art discourse-based summarization method,summary,parser, rou ge,gold  dep dts,parsing accuracy, rou ge score,ingle-document summarization,knapsack problem hirao,single-document summarization, a t kp, dep -dt,summary,rooted subtree,document,elabora,elabora,elabora,elabora,elabora,elabora,example, rst dt, rst dt,non-terminal node, dep dt,incorrect  rst dt,nucleus-satellite relationship,correct  dep dt, rst dt,i-th  edu,maxi mum number,par ent  edu,parent,parent,dimensional binary vector,denotes,summary,ilp problem,parent,term frequency,doc ument, dep -dt,ree knapsack problem,dependency-based discourse parser, dep dt,method, dep dts,transformation,detail,example,document,terminal node correspond,elementary discourse unit,non-terminal node,contiguous  edu,nu cleus,satellite,writer,purpose,satellite,nucleus,discourse relation,nucleus,satellite,nucleus,a d ep-dt,method,ac curacy, rst parser,example,fig ure, rst dt,satellite,nu cleus,incorrect  dep dt,figure,transformation algorithm,nucleus-satellite relationship,dependency relationship,figure,correct  dep dt,figure,example,parser,root edu,summary ex, dep dt,low  rou ge score,result,new dis course parser, dep dts,similar transformation algorithm,transformation algorithm,discourse,dependency,parser document summary discourse,dependency,parser tree,knapsack,problem parser,training,e1 e3 e8 e10 e4 e5 e7 e9 e6 elabora3on elabora3on elabora3on elabora3on an3thesis example evidence concession background,e1 e3 e8 e10 e4 e5 e7 e9 e6 elabora3on elabora3on elabora3on elabora3on an3thesis example evidence concession background,method,parser training phase,parser, dep dts,summarization phase,document,parser training phase,parser, rst dts,summarization phase,document,overview, dep dt parser,parser training phase, rst dts, dep dts,parser, dep dts,summarization phase,method,raw document, a d ep-dt,description,discourse dependency parser,parser,feature,fea tures,hernault,beginning, pos tag,beginning,distance,num ber,feature,belong,sentence,dom inance,feature,syntactic label,lexical head,attachment node,dominance relationship,strong compositionality feature,rhetorical structure feature,subtree structure,advance,feature,parser,margin,crammer,denote,score function,weight vector,loss function,number,incorrect parent  edu,following optimization problem,weight vector,t-th iteration,redesign,loss function,tree knapsack problem, a t kp,parent-child relationship,parent child relationship,root  edu,dep dt,root  edu,intuition, a d ep-dt parser,infor mation,following loss function,indicator function,parent,section,port result, rst dt corpus,carlson,experimental evaluation,corpus con sists,wall street journal article, rst annotation,document,human-made reference summary,document,test document,summarization evaluation,document,training data,parser,test document,summarization evaluation,parser,summarization evaluation,system, a d ep-dt, a d ep-dt,figure,overview,system,p-dis-dep-loss, a d ep-dt automat,discourse dependency,figure,overview,system,loss function, a r st-dt, hil da,state of-the-art  rst dt parser,hernault,overview,system, tkp -hil da,method,method,simple knapsack model,maximum coverage model, lea method, tkp -hil da,baseline,evaluation condition,number,summary,number,rou ge-1  rou ge-2 tkp -gol,tkp -dis -dep,tkp -dis -dep -los,tkp -hil da,human-annotated reference summary,aver age length,reference summary,source document,used evaluation con dition,single-document summarization, rst dt corpus,recall, rou ge-1,evaluation measure,show  rou ge score, rst dt corpus, tkp -dis -dep, tkp -di s-dep-loss, tkp -hil da, rou ge, tkp -go ld,wilcoxon,rank test, rou ge,null hypothesis,difference, tkp -hil da, tkp -di s-dep ,test document,difference,pro posed system,p-dis-dep , tkp -dis -de p-loss,-hil da,overlap,summary,system,summary, tkp -gol,overlap,recall,precision,sys tem, tkp -gol,first line,result,p-dis-dep , tkp -dis -dep -los, tkp -hil da,regard,aver age f-values,result, tkp -dis -de, tkp -dis -dep -los, tkp -gol, tkp -hil da,result,evidence, tkp -dis -dep,tkp -dis -dep -los, tkp -hil da, rou ge score,parser, dep -dt,gold  dep dt,salient  edu,document,summary,second line,method,tkp -dis -dep  tkp -dis -dep -los s tkp-hildaav,f-value,average dependency accuracy,rank test,null hypothesis,p-gold,elcotel inc,fiscal second-quarter earnings,result,elcotel,telecommunication company,net income,year-earlier second quarter,result,elcotel,elcotel,automatic call processor,automatic call processor,system,telephone,elcotel inc,fiscal second-quarter earnings,result,elcotel,telecommunication company,net income,year-earlier second quarter,george pierce,chairman,chief executive officer,interview,business,next year,elcotel,elcotel,fiscal second-quarter earnings,result,several new product,performance,second half,george pierce,chairman,chief executive officer,interview,elcotel,minimum,net earnings,machine,elcotel,automatic call processor,automatic call processor,system,telephone,figure,summary,sentence,bold-face,root  edu, dep dt, dep dt,high accuracy,coherency,summary,average depen dency accuracy,parent,sum mary,parent,parent  edu,correct parent-child relationship,das equal,summary,rooted subtree,gold  dep,third line,result,result,sum mary, tkp -dis -dep, tkp -dis -de p-loss tend,upper level depen dency relationship,summary,system, edu correspond ing, dep dt,system,boldface,root  edu,gold  dep dt,summary, tkp -dis -dep, tkp -di s-dep-loss,summary gener, tkp -hil da,5 c onclusion,novel dependency,discourse parser,single-document sum marization,parser,dep dt,evaluation result,parser,state-of-the-art rst dt parser,equivalent rou ge score,daniel marcu,mary ellen okurowski,rst discourse treebank,ldc2002t07,koby crammer,ofer dekel,joseph keshet,shai shalev-shwartz,yoram singer,passive-aggressive algorithm,journal,ma chine learning research,daniel marcu,noisy channel model,document compression,pro ceedings,40th annual meeting,associa tion,helmut prendinger,mitsuru ishizuka,discourse parser,support vector machine classification,dialogue,dis course,yasuhisa yoshida,masaaki nishino,norihito yasuda,masaaki nagata,single-document summarization,tree knapsack problem,proceeding,conference,emn lp,yuta kikuchi,tsutomu hirao,hiroya takamura,okumura,masaaki nagata,single document summarization,nested tree struc ture,proceeding,annual meeting,association,computational linguistics,short paper,baltimore,maryland,association,computational lin guistics,ziqiang cao,text-level discourse dependency,proceeding,annual meeting,sociation,computational linguistics,volume,long paper,baltimore,maryland,association,computational linguistics,william,sandra,thompson,rhetorical structure theory,text organization,daniel marcu,summarization,rhetorical parsing tuning,workshop,ryan  mcd onald,koby crammer,fernando pereira,large-margin training,pendency parser,proceeding,nual meeting,association,computational lin guistics,association,computational linguistics,ryan  mcd onald,fernando pereira,kiril ribarov,jan hajic,non-projective dependency par,tree algorithm,proceed ings,human language technology conference,conference,empirical method,natural language processing,vancouver,british columbia,canada,october,association,computational linguistics,radu soricut,daniel marcu,sentence level discourse,lexical infor mation,proceeding,human lan guage technology conference,north ameri,chapter,association,computational linguistics,frank wilcoxon,individual comparison,method,december,proceeding,annual meeting,association,computational linguistics,shortpapers,portland,oregon,association,computational linguistics learning,feature representation,data set,learning jun suzuki,hideki isozaki,2-4 hikaridai,seika-cho,soraku-gun,619-0237 japan suzuki,isozaki,hideki,novel approach,unsupervised data,addi tion,unsupervised data,ate informative,condensed feature represen tations,original feature,supervised  nlp system,main con tribution,method,fer dense,low-dimensional feature space, nlp task,state-of the-art performance,high-performance semi-supervised learning technique,method,result,current state-of-the-art system,feature,feature, con ll-2003  ner data,feature,depen dency, ptb -iii,1 i ntroduction,last decade,learning,standard way,system,simple,powerful approach,perfor mance,large amount,unsupervised data,approach,word representation,unsupervised data,additional feature,learning,substantial perfor mance gain,state-of-the-art,system,typical  nlp task,tity recognition,turian et,dependency parsing,approach, icw approach, icw approach,en hancement,simplicity,generality,simple,general framework,proach,state-of-the-art super, nlp system,difference,icw approach,method,original feature,supervised learning, cwr feature,new feature,method,icw approach,addition,contrast,method,supervised learning,contribution,method,architecture,number, 10m feature,feature,construct ing,condensed feature representation,unique property,previous semi-supervised learn,method, icw approach,noteworthy feature,method,sparse,high-dimensional feature space,many supervised  nlp system,main cause,data sparse ness problem,supervised leaning algorithm,high performance,condensed feature,2 c ondensed feature representation let,condensed feature,feature,supervised learning,original feature,number,feature  inf,condensed feature hm,potency,positive constant, 4s ection,feature potency quantization feature potency section,original feature,section,feature potency estimation feature,effect,-c s ection,feature potency discountingfeature potency,feature seth,condensed feature,feature,original feature,potency,condensed feature,outline,method,condensed feature,figure,notation,feature,feature,assumption,condensed fea tures,original feature,original feature,condensed feature,condensed feature,original fea tures,formally,possible input,output,target task,output,possible output,n-th feature function,original fea tures,them-th feature function,condensed fea tures, cof er,method,original feature,condensed feature,purpose,feature po tency,unsupervised data set,fig ure,brief sketch,process,condensed feature,section,self-taught-style feature potency estimation,learning,original feature,following equation,output,model parameter,widely-used model,many  nlp system,explanation,function,oth erwise,average,figure,figure,unsupervised data set,positive correlation,output,sum mation,feature value,estimation,output,unsupervised data,negative correlation,feature potency,intuitive explanation,distribution,negative correlation,output,potent feature,distribution,correlation,uninformative feature,perspective,measure,feature potency,essence,feature,certain model,context,suzuki,isozaki, mcc allum,simpler framework,imple menting,fundamental idea,semi-supervised learning, nlp task,simple framework,flexibility,extendability,applicability,framework,feature merging,elimination architecture,effective con,feature set,learning,feature potency,low potency value,feature potency,figure,difference,log-domain,non-negative constant,introduction,l1 regularization technique,algorithm,singer,feature potency quantization,positive user-specified constant,integer,calculation,fea ture,discrete,integer,respect,integer,condensed feature construction suppose,different quantized feature po tency value,quantized feature potency value,section,m-th con,summation,original feature,feature fusion process,feature,feature potency,weight,potency,regard,number,condensed feature,number,condensed feature,upper bound,number,condensed feature,number,original feature,unnecessary original feature,condensed feature,feature fn,section,feature,effect,output decision,potency,section,many original feature,fea ture potency,new feature,reason behind,infor mative,supervised learning,impor tant,performance gain,method,purpose,feature set,result,condensed fea ture,method,calculation cost,linear discriminant func tion,supervised learning,preliminary step,test phase,method,linear model,number,feature,method,application,structured prediction task,method,pre diction problem,calculation cost,structured prediction problem,output structure,lo cal sub-structures,calculation cost,sparsity,factorization,re stricting feature,formation,decomposed local sub-structure,lo cal sub-structure,output,output,local sub structure,n-th feature,local sub,typi cal firstor second-order factorization model,restriction,algorithm,typical case,many  nlp task,entity recognition,dependency parsing,sub stitutions,efficient algorithm,dynamic programming,method,feature potency,mation,prediction problem,low cost,efficient feature potency computation,feature potency estimation,section,implementation,mapreduce framework,ghemawat,parallel com,framework,summation,data-wise cal culation,map phase,feature,feature potency estimation,map-reduce process,4 e xperiments,experiment,different  nlp task,dependency parsing,cilitate comparison,performance,previ ous method,experimental setting,suzuki,isozaki,dependency par,suzuki,supervised datasets,tjong kim sang, dem eulder,task data,corpus,marcus,dependency parsing,unsupervised data,instruc tions,suzuki,comparative method,effectiveness, cof er,brown algorithm, icw approach,state-of-the-art re sults,dependency, ptb -iii, con ll,task data,compar, cof er,effective ness,feature,learning,term active feature,feature,model param eter,supervised learning,non-active feature,trained model,learning,perfor mance,number,active feature,supervised learning,number,active feature, cof er,number,feature,fair comparison,archi tecture,original feature,supervised learning,fair comparison,l1-regularized,algo rithms,non-zero parameter, owl qn,andrew,online struc,output,version, fob o,singer,tsuruoka,l1 regularization,ostl1fobos,dependency par,addition,lafferty,-bf g,nocedal,online,output,version,passive-aggressive algorithm,dependency,baseline performance regardless,ac tive feature number,setting, cof er,baseline,mod el, cof er, l1c rficwr, l2c rf  icw,active feature,ostpa  cof er,ostl1fobosicwr,ostl1fobossup,ostpa sup,ostl1fobos,active feature,log-scale unlabe,tachme nt sco re,f-score,figure,performance,active feature,trained model,development set,addition,result, cof er,mod el,straightforward approach,several different type,fea tures,part-of-speech tag,word sur face form,combination,suppose,different feature type,feature template,isozaki,lin andwu,experiment,merging,feature,condensed feature construction process,fea tures,feature type,feature set,number,feature typesk,dependency parsing experiment,feature par tition,context,semi-supervised learning,suzuki,isozaki,result,discussion figure,performance,develop ment,respect,number,active fea tures,trained model,learning algorithm,dependency,relation,number,active feature,result, cof er,effective feature set,certain  nlp task,noteworthy result,figure,performance,recent top-line system,dependency parsing,result,top-line semi -ne system dev, l1c rf,l1c rf,suzuki,isozaki,ratinov,turian et,ostl1fobos,ostl1fobos,collins,suzuki,comparison,previous top-line system,test data,active feature,trained model,system,active feature,addition,combination,result,point gain,point gain,dependency parsing,5 c onclusion,condensed feature representation,simple,general framework,performance,supervised  nlp system,method,condensed fea ture set,discrete feature potency,unsupervised data,cof er,feature potency estimation,informative dense,low-dimensional feature space,supervised learning,sparse,high-dimensional feature space,many  nlp task, nlp system,performance,condensed feature,reference rie kubota ando,tong zhang,igh performance semi-supervised learning method,text chunking,proceeding,annual meet ing,association,computational linguistics,galen andrew,jianfeng gao,scalable training,l1-regularized log-linear model,zoubin ghahramani,editor,proceeding,annual international conference,machine learn,omnipress,wenliang chen,ichi kazama,kiyotaka uchimoto,kentaro torisawa,dependency,subtrees,auto-parsed data,pro ceedings,conference,empirical meth od,natural language processing,koby crammer,ofer dekel,joseph keshet,shai shalev shwartz,yoram singer,online passive aggressive algorithm,journal,machine learning research,jeffrey dean,sanjay ghemawat,mapreduce,simplified data processing,large cluster,gregory druck,andrew  mcc allum,high performance semi-supervised learning,generative model,pro ceedings,international conference,john duchi,yoram singer,batch learning using forward backward splitting,journal,machine learning research,terry koo,michael collins,efficient third order dependency parser,proceeding,annual meeting,association,computational linguistics,xavier carreras,michael collins,simple semi-supervised dependency parsing,pro ceedings,john lafferty,andrew  mcc allum,fernando pereira,conditional random field,probabilistic mod el,sequence data,proceeding,international conference,dekang lin,phrase cluster,discriminative learning,proceeding,joint conference,47th annual meeting,international joint conference,natural language processing, afn lp,jorge nocedal,limited memory  bfg s me thod,large scale optimization,programming,mitchell,marcus,beatrice santorini,mary ann marcinkiewicz, a l arge annotated corpus,english,penn treebank,computa tional linguistics,andre martin,noah smith,eric xing,pedro aguiar,mario figueiredo,parser,depen dency,approximate variational inference,proceeding,conference,empirical method,natural language processing,dan roth,design challenge,misconception,named entity recognition,proceeding,thirteenth conference,jun suzuki,hideki isozaki,semi-supervised sequential labeling,segmentation using giga word scale unlabeled data,proceeding,jun suzuki,hideki isozaki,xavier carreras,andmichael collins,empirical study,structured conditional model,depen dency parsing,proceeding,conference,empirical method,natural language process ing,erik tjong kim sang,fien de meulder,intro duction, con ll-2003 shared task,language independent named entity recognition,proceed ings, con ll-2003,yoshimasa tsuruoka,ichi tsujii,sophia ana niadou,stochastic gradient descent training,l1-regularized log-linear model,cumula tive penalty,proceeding,joint conference,47th annual meeting,international joint conference,natural language processing, afn lp,joseph turian,lev-arie ratinov,yoshua bengio,word representation, a s imple,general method,semi-supervised learning,proceed ings,annual meeting,association,computational linguistics,proceeding,annual meeting,association,computational linguistics,bulgaria,august,association,computational linguistics,feature, a d iscrete constraint jun suzuki,seika-cho,soraku-gun,619-0237 japan suzuki,framework,feature,complexity mod el,main idea,method,discrete constraint,dual decom position technique,experiment,well-studied  nlp task,dependency par,demonstrate,method,state-of-the-art performance,degree,freedom,trained model,significant benefit enables,compact model representation,actual use,1 i ntroduction,supervised model learning,following form,corresponding input,output,dimensional vector representation,optimization variable,feature weight,loss function,regularization term,nowadays,supervised learning method,optimization problem,feature weight,entity recognition,dependency parsing,semantic role labeling,last decade,l1-regularization tech nique,l1-norm,many nlp task,tsuruoka,reason,l1-regularizers encour age feature weight,po sible,model learning,resul tant model,sparse solution,many zero-weights,feature,weight,trained model1,l1-regularizers,ability,compact model,strong concern,feature selection,compact model,clear advantage,practice,instance,memory,memory occupa tion,decoding,cache mem ory,background,framework,model complexity,sim ply,l1-regularizers,concept,au tomatic feature grouping,tibshirani,bondell,framework,feature group,discrete constraint,model learning,2 f eature,concept,l1-regularized sparse modeling,automatic feature,example,tibshirani,pur suit, osc ar,bon dell,automatic feature grouping,accurate model,degree,freedom,equiva lent,optimization variable,simple example,unique value,several merit,degree,paper refers,completion,trained model,freedom,example,previous study,chance,training data,important property,many  nlp task,high-dimensional feature space,over-fitting problem,stability,non-zero fea tures,standard l1 regularizer,existence,cor related feature,rnsten,hastie,duce model complexity,feature,feature weight value,trained model,single fea ture cluster,feature,section,proposal,feature,solution,integration, a d iscrete constraint let,finite set,discrete value,set integer,detailed discussion,experiment section,objective,feature grouping,model learning,cartesian power,difference,additional dis crete constraint,con straint,variable,feature weight,trained model,n-th factor,result,feature weight,trained model,model learning,basic idea,feature,concern, a n p-hard combi natorial optimization problem,time complex ity,direct optimization,feasible algorithm,dual decomposition formulation hereafter,convex,proper tie,method,selection,spe cific definition,section,typical case,experiment section,dual decomposition technique,everett,difference,optimization,objec tive,decomposition,standard loss minimization problem,additional discrete constraint regu larizer,dual decomposition technique,optimization,direction method,mercier,efficient optimiza tion framework,problem,dual decom position form,dual variable,troduces,augmented lagrangian term,strict convexity,robustness3,optimization problem,series,iterative optimiza tion problem,derivation,general case,entire model,framework,method,remarkable point,adm work,optimization variable set,variable,iteration,convergence,optimiza tion problem,l2-regularizer,direction,regularization,origin,standard l2-regularizer,learning,l2 regularizer,optimization,optimization problem,regularizers,regularizer,tandard dual decomposition,training data,parameter,condition,figure,entire,framework,method,combinatorial optimization problem,optimization,procedure,exact solution,remarkable point,costly combinatorial optimization problem,feature-wise calculation,total time complexity  iso,similar technique,costly combinatorial problem,optimization, osc ar-regularizers,proximal gradient method,teboulle,detailed derivation,space reason,key property,convex,symmetric function,respect,optimal solution,discrete constraint,optimal solution,optimal solution,constraint,optimization,mixed l2,l1-norms,teboulle,valid point,binary search,time complexity,procedure,valid point,l2-distance,valid point,vertex,thotopes,hyperrectangles,parameter space,feature weight,valid point,dual variable,learning rate,iteration,dual residual,small primal,online learning,online learning algorithm, adm framework,quire exact minimization,one-pass update,adm iteration,total calculation cost,method,crease,original online,algo rithm,calculation cost,4 e xperiments,experiment,well-studied nlp task,dependency,basic setting,setting,previous study,tjong kim sang,de meulder,marcus,dency tree,decoding model,viterbi algorithm,lafferty,second order,carreras, dep ar,feature,feature template,pre vious study,cluster fea tures,method,additional feature,method,system,evaluation measure,purpose,ex periments,effectiveness,method,performance,complexity,trained model,evaluation measure,task performance,objective,method,experiment,de -pa,comparison,previous study,model complexity,number,non-zero active feature,degree,number,feature,corresponding feature weight,trained model,number,unique non-zero feature weight,baseline method,main baseline,l1 regularized sparse modeling,online leaning,owl qn,andrew, ner experiment,method, dep ar, l-b fgs ,nocedal,passive-aggressive algorithm,de -pa,l2-regularizer,re sults,l1-regularizer,fair comparison,proce dure,simple quantization method,trained model,l1-regularized model learning,result,experiment,l1-regularized  fob o,singer,variant,tsuruoka,variant,configuration,method base,algorithm,setting,method,experiment,l1-regularized,algorithm,purpose,experiment,effectiveness,standard l1-regularized learning algo rithms,possible setting,baseline l1-regularized learning algorithm,-ad mm,baselinel2-regularized learning algorithm,l2-regularizer,difference,objective func tion,l1 regularizer,dcwl1-admm discard regularizer,configuration,objective,formulation,algorithm,result,dc-admm,space reason,result,dcwl1-admm,dc-admm,definition,fi nite,performance,considerable point,method,several setting,example,tem plate,large feature,represent non-negative real-value con stants,positive integer,finite set,non-negative integer,example,intuition,template,distribu tion,feature weight,trained model,power law,large feature set,exponential function,dc-admml1crf,ete sen tence ccurac,degree,dc-admml1rad,plete entenc e a ccu racy,degree,performance,degree,freedom,trained model,development data,upper bound,trained model,upper bound,negative side,ex periments,tunable parameter,experiment,result,discussion fig,task performance,develop ment data,model complexity,degree,freedom,trained model,dc -ad mm andl1-regularized method,standard l1-regularized method,regularization constant,final result,experiment,test data,tunable param eters,performance,development data,figure,re markable point,dc-admm,task performance,degree,freedom,performance drop-off,upper bound,feature group,dc -ad mm performance,reason,low degree,freedom prevent over-fitting,training data,surpris,good result,qt approach offer,guarantee,optimal solution,opti mal solution,discrete constraint,trained model,comparison result,method,feature weight,indexed structure,fea ture string,corresponding feature weight,mer part,successful reduction,lat ter part,feature string structure,detail,main topic,feature string,feature weight,entire trained model, dep ar case,onclusion,framework,feature grouping,incorporation,simple discrete con straint,optimization,infeasible combi natorial optimization part,entire learning algorithm, adm technique,experiment,dc-admm,model complexity,degree,freedom,trained model,performance,cleverer approach,grouping,performance,dc-admm,upper bound,actual use,reference galen andrew,jianfeng gao,scal able training,l1-regularized log-linear model,zoubin ghahramani,editor,proceeding,annual international conference,omnipress,amir beck,marc teboulle,ast iter ative shrinkage-thresholding algorithm,imaging sci,howard,bondell,simulta neous regression shrinkage,variable selection,clustering,predictor, osc ar,biometrics,stephen boyd,neal parikh,eric chu,borja peleato,jonathan eckstein,opti mization,statistical learning,alternat,direction method,multiplier,foundation,machine learning,carreras,experiment, a h igher order projective dependency parser,proceed ings, con ll shared task session, emn lp con ll,koby crammer,ofer dekel,joseph keshet,shai shalev-shwartz,yoram singer,line passive-aggressive algorithm,journal,ma chine learning research,john duchi,yoram singer,batch learning using forward backward splitting,journal,machine learning research,kevin duh,jun suzuki,masaaki nagata,learning-to-rank,streaming data,alternating direction method,multiplier,big learning workshop,hugh everett,lagrange multiplier method,problem,optimum alloca tion,resource,operation research,daniel gabay,bertrand mercier,ual algorithm,solution,nonlinear variational problem,finite element approximation,com puters,mathematics,galen andrew,mark johnson,kristina toutanova,comparative study,parameter estimation method,statistical natural language processing,proceeding,nual meeting,association,computational linguistics,prague,czech repub lic,association,computational linguis tic,simulta neous gene clustering,subset selection,sample classification via  mdl,bioinformatics,terry koo,xavier carreras,michael collins,simple semi-supervised dependency par,proceeding,john lafferty,andrew  mcc allum,fernando pereira,conditional random field,prob abilistic model,segmenting,labeling se quence data,proceeding,international conference,jorge nocedal,limited memory  bfg s me thod,large scale optimiza tion,programming,mitchell,marcus,beatrice santorini,mary ann marcinkiewicz, a l arge annotated corpus,english,penn treebank,computa tional linguistics,ryan  mcd onald,koby crammer,fernando pereira,large-margin training,dependency parser,proceeding,nual meeting,association,computational lin guistics,hsin-cheng huang,pursuit, a r egularization solution sur face,journal,american statistical associa tion,robert tibshirani,michael saunders,saharon ro set,ji zhu,keith knight,sparsity,smoothness,fused lasso,journal,royal statistical society series,erik tjong kim sang,fien de meulder,introduction, con ll-2003 shared task,language-independent named entity recognition,proceeding, con ll-2003,yoshimasa tsuruoka,ichi tsujii,sophia ana niadou,stochastic gradient descent training,l1-regularized log-linear model,cumu lative penalty,proceeding,joint confer ence,47th annual meeting,international joint conference,natural lan guage processing, afn lp,lin xiao,dual averaging method,regular,stochastic learning,online optimization,journal,machine learning research,leon wenliang zhong,efficient sparse modeling,automatic feature grouping,hui zou,trevor hastie,regularization,variable selection,elastic net,journal,royal statistical society,series