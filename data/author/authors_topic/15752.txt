proceeding,joint conference,empirical method,natural language processing,computational natural language learning,jeju island,association,computational linguistics translation model,cross-lingual language model adaptation,word model,bo xu interactive digital medium technology research center institute,automation,chinese academy,science,zhongguancun east road,haidian district,beijing,medium,cn abstract,cross-lingual data selec tion model,word model,source sentence,translation task,probability,sentence,target lm training corpus,traditional ap proaches,first pas translation hypothesis,cross-lingual data selection mod el,problem,noisy proliferation,furthermore,phrase tm,cross-lingual data selection model,traditional approach,bag-of word model,contextual information,selection,phrase,ex periments,large-scale data,demonstrate,approach,state-of-the-art approach,lm perplexity, smt performance,critical role,universal truth,lm performance,training data,training data,reason,output,large training data,domain,discus sion,mismatch problem,translation task,reason,researcher,sim ilar training data,large training corpus,past year,al2004,masskey,axelrod,ac curate lexical probability,translation task,axelrod,previous data selection approach,lm adaptation,first pas transla tion hypothesis,al2004,masskey,sentence,translation,scheme,quality,translation hypothesis,initial translation hy,sentence,great deal,development,recent year,trans lation hypothesis,many noisy data,noisy translation hypothesis,data selection pro ce,al2006,noisy data,training data,noisy prolifera tion,performance,traditional approach,lm adap tation,bag-of-words model,state-of-the-art performance,tf-idf,al2004,hildebrand,al2005,foster,centroid simi larity,masskey,perform,word level,scheme,account,contextual information,se lection,single word,isolation,quality,sentence,data selection,source transla tion task,contextual infor mation,lm adaptation,cross-lingual data selection model,lm adapta tion,word model,data selection,cross-lingual model,source sentence,translation task,probability,translation,sentence,target lm train,corpus,translation task,mono lingual adaptation,problem,noisy proliferation,knowledge,empirical study,phrase,cross-lingual data selection,lm adap tation,transform probabili ty,multi-term phrase,source sentence giv,phrase,target sentence,corpus,bag-of-words model,word-based tm,single word,isolation,phrase level,contextual information,selection,phrase,precise data se lection,phrase,linear ranking model framework,performance,linear discriminant function,collins,al2005,pattern classification,different model,feature,experiment,general tm,textrank algorithm,mihalcea,unimportant word,non-topical word,common word,corpus pre processing,construct tm,important word,average number,cross lingual data selection model,ef ficiency,textrank,contex information,term weight,lee et al2008,phrase tm,cross lingual data selection model,advantage,contextual information,performance,remainder,fol low,section,introduces,related work,lm adaptation,section,framework,cross-lingual data selection,lm adaptation,section,cross lingual data selection model,word model,section,present large-scale experiment,analysis,conclu sion,future work,section,work tf-idf,cosine similarity,lm adaptation,hildebrand,al2005,foster,auxiliary data,centroid sim ilarity score,centroid,in-domain data,main idea,method,sen tences,first pas translation,in-domain corpus,large lm training corpus,bias lm, smt system,translation quality,al2007,bilingual -ls model,lm adaptation, lsa marginal,target generic lm,marginal adaptation,kullback leibler divergence,al2011a,probability,lm train,corpus,cross-lingual ap proaches,modify lm,ifferent,data selection method,lm adap tation,comparable experiment,first time,cross-lingual data selection,lm adaptation,smoothing measure,sparse vector representation,similarity computation,performance,measure,tf-idf,experiment,snover,al2008,word tm,system,subset,tar get document,source document,data sparseness,document state,document level,large quantity,irrele vant text,adapted lm,ananthakrishnan,al2011b,word tm,cross-lingual data selection model,sentence level,smooth ing mechanism,background word frequency model,performance,axelrod,al2011,bilingual cross-entropy difference,lect data,parallel corpus,domain adaptation,contextual information,monolingual cross-entropy differ ence,advantage,bilingual data selection,performance,parallel in-domain cor pu,application,3 c ross-lingual data selection,language model adaptation,lm adaptation,unsupervised similar train ing data selection,cross-lingual data selection model,source sentence,translation task,target lm training sen tences,similar sentence,selected sentence,corresponding bias lm,high probability,phrase,desired output translation,snover,al2008,linear interpola tion,performance,individual model,interpolation factor,powell search algorithm,cross-validation,cross-lingual data selection model,word model,quality,perfor mance,source sentence,translation task,sentence,general target lm training corpus,lingual data selection model,uniform,sen tences,sentence,sentence,word-based translation model,snover,interpolation,constant1,word-based tm,al1993,parallel corpus,un-smoothed background,sentence model,translation task,number,time occurs,number,time occurs,translation task,current target sentence,al2001,candidate sentence,data sparseness,sentence state,data selection experiment,berger,al1999,following smoothing mechanism,un-smoothed background model,lm training corpus,c refers,interpolation weight,un-smoothed model,phrase-based translation model,phrase-based tm,superior performance com,phrase-based tm,rather,single word,isolation,phrase model transfer,sequence,sequence,incorporat,contextual information,web search,al2010,question re trieval,al2011,following generative process,sentence,non empty word sequence,new non-empty word sequence,phrase,permutat ed,sentence,phrase,consecutive sequence,generative process,segmentation,bi-phrases,permutation  ofk element,final ranking step,probability distribution,rewrite pair,transfer,uniform probability,segmentation,phrase-based selection probability,maximum approximation,generative model,ranking score function,sen tence,becauseq,different length,length,corpus,many word,key difference,pre vious work,berger,lafferty,al2011,sentence-query selection,distillation,sentence,selec tion,natural language,attention,key sen tence word,distillation,unaligned word,key sentence word,key sentence word,word alignment,hidden,word alignment,term position,word position aj,position,key sentence word,attention,consistency,word alignment,final permutation,factor,transfer,fol low,phrase translation probability,parallel corpus,relative frequency,lexical weight ing,format,phrase translation proba bility,lexical weight probability,approach,mono tone,quantity aj,maximal probability,likely sequence,phrase,first word,probability,following re cursion,initialization,induction,candidate sentence,candi date sentence,linear discriminant function,collins,al2005,pattern classifi cation,linear rank,model framework,cross-lingual data selec tion model,different model,incorporat ed,feature,linear ranking model,feature,feature,arbitrary function,corresponding parameter,feature,parameter,search algorithm,cross validation,used feature,linear ranking model,equation,phrase translation probability,equation,inverted phrase translation proba bility,equation,lexical weight probability,equation,inverted lexical weight probability,number,unaligned term,total number,word-based tm,efficiency,cross-lingual data,lection process,translation task,lm training corpus,parallel corpus,key word,importan word,construct tm,key word,important word,key difference,nate unimportant word,retrieval,al2008,average number,total word number inq,cross-lingual sentence selection mod el,efficiency,cross-lingual data selection,variant,textrank algorithm,mihalcea,ranking model,key word extraction,state-of-the-art accuracy,unimportant word,corpus,low significance,corpus,traditional approach,tf-idf,tex -tr ank,context information,term weight,lee et al2008,performance, clp tm,experiment,lee et al2008,algorithm proceeds,document,vertex,fixed-sized window,number,co-occurrence,weight,vertex,pagerank,algorithm,graph itera,convergence,textrank score rkwi,word wi,document,kth iteration,factor,edge weight,experiment,propor tion,total word,document,important word,5 e xperiments,utility,lm adap tation approach,refer ence translation,perplexity,adapted lm,formance,adapted lm,experiment,chinese-to-english translation task,dialogue domain,nis t-06,bilingual training corpus, bte c3,corpus,chi nese english word,lm training corpus,english side, cwm t20085,english word,test set,iws lt-07 test set,sentence,development set, iws lt-05 test set,bilingual training corpus,sentence pair,lm training corpus,english side,parallel data,english gigaword corpus7,sentence,test set, nis t mt evaluation test set,sentence,velopment set, nis t mt evaluation test set,sentence,al2008,china workshop,figure,english reference translation,perplexity,adapted lm,training data,different approach,development set,perplexity analysis,development,subset,5-fold cross-validation experi ments,parameter,parameter,subset,re maining subset,experiment,generic 4-gram lm,en tire lm training corpus,baseline,top-n sentence,velopment,bias 4-gram lm,n-gram cutoff,sentence,interpolate,generic 4-gram lm,adapted lm, sri lm toolkit,stolcke,lm performance,perplexi,performance,perplexity,adapted lm accord,reference translation,figure,perplexity,adapted lm,foundation,solution,tf -id,state-of-the-art performance,lm adaptation,hildebrand,foster,cross-lingual similarity,ananthakr,al2011a,optimization,al2008,un-smooth ver task method perplexity reduction iws lt-07 baseline,english reference translation,perplexi ty,adapted lm,different approach,test set,sentence, iws lt-07,top  16k sentence, nis t-06, clw tm, clp tm,maximum phrase length,target sentence,result,figure,indicate,english ref erence translation,perplexity,adapted lm,increase,top-n sentence,consistent ly,certain size,approach,proper size,similar sentence,transla tion task,adapted lm perform,many noisy data,sentence,performance,similar observation,1 b aseline,2 t f-idf ,4 c l,6 c lwtm, clw tm,comparison, smt performance,different approach,lm adaptation,test set,axelrod,approach,ap proaches,perplexity result,figure,sentence, iws lt-07,top  16k sentence, nis t-06,test set,show english reference translation,perplex ity,adapted lm,test set,approach e,reduction,perplexity com,approach,result,adapted lm,predictor,corresponding translation task,adapted lm,next translation experiment,detailed perfor mance,training data,lm adaptation,translation experiment,translation experiment,chiang,system,utility,lm adaptation, smt performance,ble score,papineni,generic lm,perplexi ty analysis experiment,minimum error rate training,feature weight,maximum  ble score,development,serval group,different start weight,main translation result,task translation hypothesis  ble uiwslt-,impact,noisy data,translation hy potheses,performance,lm adaptation,test set,improvement,sig nificant,confidence interval,respect,baseline,comparison result,clear trend,traditional approach,first pas translation hypothesis,detailed impact,noisy data,translation,data selection,next section,section,word-based tm,accurate cross-lingual data selection model,single cross-lingual pro jection,ananthakrishnan,snover,mechanism,sentence state,clw tm,performance,performs,state-of-the-art approach,bag-of-words model,word-based tm,impact,noisy data,translation,experiment result,sec ond pas translation hypothesis,tf-idf,first pas trans lation hypothesis,translation,noisy data,new translation hypothe s,second pas,similar sentence,lm adaptation,tf-idf,impact,noisy data,translation,performance,adapt ed lm,observed improvement,initial translation,noisy data,task phrase length  ble,impact,phrase length, clp tm,performance,lm adaptation,maximum phrase length,sec ond iteration translation,cross-lingual data selection,lm adaptation,problem,noisy pro liferation,impact,phrase length,result,phrase,visible improvement,maximum length,proper tie,phrase,feature,performance,phrase length,single word-based tm,suspec,feature,linear ranking model,method,improvement,phrase,feature,incorporat ed,future work,impact,eliminating unimportant word table,result,textrank al gorithm,performance,lm adap tation,unimportant word,average number,average number,total word number,cross-lingual data selection model,average number,unimportant word, nis t-06,cross-lingual data selection process, clw tm,performance,initial state,initial state,cause textrank algorithm,context infor task method average ble unu mber  clw tm clp tm,nis t-06 initial,impact,unimportant word,textrank algorithm,performance,lm adaptation,mation,term weight, clp tm,advantage,contextual information,6 c onclusions,future work,novel tm,cross lingual data selection model,lm adaptation,word model,lm training corpus,translation task,general tm,textrank algorithm,unimportant word,cor pu preprocessing,construct tm,importan word,traditional approach e,first pas translation hypothe s,cross-lingual data selection,prob lem,noisy proliferation,furthermore,phrase,cross-lingual data selection,effec tive,traditional approach,bag of-words model,contextual information,election,phrase,large-scale exper iments,lm perplexity, smt performance,result,approach,disadvan tages,state-of-the art method,lm adaptation,research,future,approach,large-scale corpus,infrastructure system,approach,domain,speech transla tion system,second,significant improvement,lm adaptation,cross-lingual data selection,oth er,cross-lingual data selection,lm adaptation,latent semantic model,acknowledgment,program,helpful discussion,suggestion,anonymous reviewer,insightful comment,reference sankaranarayanan ananthakrishnan,rohit prasad,prem natarajan,on-line language model bias,statistical machine translation,proceeding,sankaranarayanan ananthakrishnan,stavros tsakalidis,rohit prasad,prem natarajan,line language model,multi-pass automat ic speech recognition,proceeding, int er -sp eech,amittai axelrod,jianfeng gao,adaptation,pseudo in-domain data selec tion,proceeding, emn lp,adam berger,john lafferty,information re trieval,statistical translation,proceeding,si -gi,thorsten brant,jeffrey dean,large language model,machine translation,proceeding, emn lp,vincent,della pietra,stephen,della pietra,robert,mercer,mathematic,statistical machine translation,parameter,computational linguistics,david chiang,hierarchical phrase-based model,statistical machine translation,proceeding,david chiang,hierarchical phrase-based transla tion,computational linguistics,michael collins,discriminative training meth od,hidden markov model,theory,experi ments,perceptron algorithm,proceeding, emn lp,richard,pattern classification,matthias eck,stephan vogel,alex waibel,model adaptation,statistical machine translation,information retrieval,proceed ings,327-330,george foster,roland kuhn,mixture-model adaptation,proceeding,jianfeng gao,jian-yun nie,linear discriminative model,informa tion retrieval,proceeding, sig ir,jianfeng gao,jian-yun nie,clickthrough-based translation model,web search,word model,proceeding,almut silja hildebrand,matthias eck,stephan vogel,alex waibel,adaptation,transla tion model,statistical machine translation,formation retrieval,proceeding,woosung kim,model adaptation,automatic speech recognition,statistical machine translation,thesis,john hopkins univer sity,philipp koehn,franz josef och,daniel marcu,statistical phrase-based translation,proceed ings, naa cl,jung-tae lee,sang-bum kim,young-in song,hae chang rim,lexical gap,question,compact translation model,proceeding,emn lp,sameer masskey,abhinav sethy,auxiliary data,language model adaptation,ma chine translation,speech,proceeding, ica -sp,rada mihalcea,paul tarau,textrank,proceeding, emn lp,robert,william lewis,intelligent selection,language model training data,proceed ings,franz josef och,statistical mahcine transla tion,single word model,template,franz josef och,minimum error rate training,statistical machine translation,proceeding,franz josef och,hermann ney,alignmen,template approach,statistical machine translation,computational linguistics,kishore papineni,salim roukos,todd ward,method,automatic eval uation,machine translation,proceeding,william,teukolsky,william,vetter ling,flannery,numerical recipe,cambridge university press,matthew snover,bonnie dorr,richard marcu,language,translation model adaptation,comparable corpus,proceeding, emn lp,andreas stolcke,toolkit,proceeding, ics lp,yik-cheung tam,tanja schultz,unsuper,language model adaptation,latent seman tic marginals,proceeding, ics lp,yik-cheung tam,ian lane,tanja schultz,bilingual-lsa,lm adaptation,spoken lan guage translation,proceeding,yik-cheung tam,ian lane,tanja schultz,bilingual-lsa,adaptation,statistical machine translation,machine translation,bin wei,christopher pal,cross lingual adap tation,experiment,sentiment classification,proceeding,chanh nguyen,probabilistic model,formation retrieval,proceeding, sig ir,jiwoon jeon,bruce croft,retrieval model,question,answer archive,proceeding, sig ir,bing zhao,matthias eck,stephan vogel,model adaptation,statistical machine translation,structured query model,proceed ings,guangyou zhou,li cai,jun zhao,kang liu,phrase-based translation model,question retrieval,community question answer archive,proceed ings,proceeding,annual meeting,association,computational linguistics,baltimore,maryland,association,computational linguistics learning new semi-supervised deep auto-encoder feature,institute,automation,chinese academy,science,beijing,cn abstract,new fea tures,intuition,linguistic knowl edge,domain,effective feature,paradigm,phrase-based translation model,parameter,put original phrase feature,teacher,new semi-supervised  dae feature, dbn feature,high dimensional feature representation,natural horizontal compo sition,large hidden lay er,chinese english task,semi-supervised  dae feature,significant im provements,unsupervised  dbn feature,base line feature,1 i ntroduction recently,many new feature,significant performance,translation quality,syntactic feature,sparse feature,feature,feature,linguistic phenomenon,bilingual language pair,new feature,tuition,linguistic knowledge,domain,first time,maskey,possibility,new feature,unsuper,fashion,hierarchical phrase-based trans lation model,original phrase fea tures,phrase table,input feature,contrastive divergence,hinton,new unsupervised dbn feature,forward computation,new feature,extra feature,phrase table,translation decoder,approach,major shortcoming,input original feature, dbn feature learning,phrase feature,phrase pair,bidirectional phrase translation probabil ity,bidirectional lexical weighting,bottleneck,ef fective feature representation,second,unsupervised layer-wise pre-training,stacked set,training objective,performance relies,empirical parameter,approach,improvement,shortcoming,po sibility,new feature,multi layer,neural network,name deep learning,first shortcoming,effective phrase feature,input feature,new  dnn feature learn ing,feature,sig nificant improvement,phrase fre quency,phrase length,hopkins,generative probability,foster,improvement,new phrase feature,experiment,second shortcoming,successful use,handwrit ten,recognition,hinton,salakhutdinov,hinton,information retrieval,salakhutdinov,hinton,mirowski,speech spectrogram,new feature,semi-supervised  dae,phrase-based translation model,input data,teacher,fine-tuning process, dae ad,problem,back-propagation,abstract feature,hinton,salakhutdinov, dae feature,unsupervised pre-trained  dbn,parameter,input original phrase fea,teacher,semi-supervised back propagation,unsupervised dbn feature,semi-supervised  dae feature,high dimensional feature representation,natural horizontal composition,large hidden layer representation,improvement,single  dae,experiment,non-parametric feature expansion,invari ance,specific embodiment,original feature,feasible feature gen eration approach,deep model,feature learning,shal low model, hcd ae,new non-parametric feature,sim ilar evolution,speech recognition,hinton,non-linear combination,input original feature,strong capture high order correlation,activity,original feature,deep learn,paradigm,original feature,large-scale experiment, iws lt, nis t ch inese-english translation task,result,solution,shortcoming,semi-supervised dae feature, dbn feature,baseline feature,introduced input phrase,performance, dae feature learning,remainder,fol low,section,briefly,recent re,application, smt task,section,present,introduced input fea tures, dnn feature learning,section,semi-supervised  dae fea tures,section,describes,large-scale experimental result,conclusion,section,interest, smt task,translation quality,n-gram translation model,transla tion probability,continu ous representation,translation unit,standard discrete representation,kalchbrenner,blunsom,recurrent contin uous translation model,continuous sentence-level translation mod el,joint lan guage,translation model,recur rent neural network,target word,unbounded history,source,target word,log-linear model,novel additive neural network,translation model,shortcom ings,log-linear model,linearity,deep interpretation,tation,feature,classifier,recursive auto encoders,vector space representa tions,variable-sized phrase,semantic information,max-margin,socher, hpb translation,force decoding, a r nn,word topology model, hpb translation,topological structure,source side,meaningful order,new feature,input data,suitable feature,representation,superiority,possibility,3 i nput feature, dnn feature,phrase-based translation model,supe rior performance,current smt system,implementation,translation model,original phrase feature,input fea tures, dae feature,baseline phrase,maskey,phrase fea tures,phrase pair,phrase table,first type,input fea tures,bidirectional lexical weight ing,phrase pair similarity zhao,term weight,vector space,addi tional evidence,phrase pair translation quality,phrase pair similarity,weight,content,non-content word,phrase translation pair,bidirectional phrase pair simi larity,cosine distance,bidirec tional word translation probability,term weight,source,target word,transformed weight,source target word,target source dimen sion,phrase length,average phrase length,second type,input feature,bidirectional phrase genus tive probability,input feature,domain adaptation,foster,background lm,bidirectional,source target side,forward,backward phrase generative probability,bidirectional forward,4-gram lm,third type,input feature,phrase frequency,bidirectional phrase frequency,input feature,total num ber,phrase,source target side,bilingual corpus,denominator,total number,phrase,length,forth type,input feature,preceding,context,current word,backward lm,sentence,training data,original order,reverse order,corpus,translation model,experiment,detail,section,important role,trans lation process,hopkins,maximum phrase length,last type,input feature,first type,phrase feature,maskey,effec tive phrase feature,input original phrase feature,fea tures,experiment, dae network,first layer,visible node,visible node corresponds,original feature,phrase pair,4 s emi-supervised deep auto-encoder feature learning,translation rule,phrase-based transla tion model,number,feature,log-linear model,semi-supervised  dae feature,section, dae network,various network structure,new feature,learning  a d eep belief net,maskey,deep generative model,feature,multiple layer,latent variable,first layer represent,visible feature vector,stacked set,hinton, a r bm,full connectivity,connection,connection,visible layer bias,contrastive divergence,hinton,carreira-perpinan,hinton,hidden layer,factorial conditional distribution,visible layer, ht wt,logistic sigmoid,element-wise conditional distribution,pre-training consists,unsupervised dbn,conditional distribution,generative model, vt wh,first  rbm,acti vation probability,hidden unit,second  rbm,th rbm,output,deep architecture, a d bn,th layer,hidden unit,single forward pas,hinton,figure,greedy,layer-by-layer pre-training,peated several time,feature,tures strong high-order correlation,activity,feature,real-valued input feature,gaussian visible unit,variance,dimension,first  rbm, vt wh,appropriate identity matrix,figure,unsupervised pre-training,back-propagation,error derivative,pre-training,entire phrase pair,feature,phrase table,small mini-batches,weight,mini batch,entire phrase pair,weight,learning rate,momentum,weight decay,weight,weight matrix,small random value,zero-mean normal distribution,vari ance,pre-training,phrase pair,phrase table, dbn feature,maskey,original phrase featuresx,computation,auto-encoder,semi-supervised  dae,unroll,layer  dbn,weight ma trice,2n-1 layer network,matrix,upper layer,matrix,reverse order,hinton,salakhutdinov,salakhutdinov,hinton,figure,layer-wise learning,pre-training stage,good region,parameter space,parameter,region,output,propagation,reconstruction error,output,fine-tuning,method,conjugate gradient,mini-batches,line search,mini-batch,adequate number,fraction phrase table,test performance,validation phrase table,fine-tuning,en tire phrase table,various value,noise variance,threshold,learn ing rate,momentum,weight-decay parame ters,pre-training,batch size,fine-tuning,result,ro bust,variation,parameter,precise weight,pre-training,good region,parameter space,fine-tuning,feature representa tion,central layer, dae work,salakhutdinov,hinton,fine-tuning,phrase pair,phrase table, dae feature,original phrase feature,encoder,computation,learned feature,dae feature,log-linear model,impact,non-linear learning mechanism,maskey,feature,av erage,dimensional respective feature,feature,phrase pair,phrase table,extra feature,horizontal composition,ab stract feature representation,learned feature,dimensionality,dimensionality,input feature,successful use,handwritten digit,nition,hinton,salakhutdinov,hinton,information retrieval,salakhutdinov,hinton,mirowski,figure,horizontal composition,high-dimensional feature,speech spectrogram,phrase feature,feature,bottleneck,large hidden layer,representation,information,performance,high-dimensional  dae feature,single  dae,high-dimensional feature representa tion,performance,natural horizontal composition,large hidden layer representation,figure,single  dae,architecture,hidden layer,hidden feature representation,subsequent lay er,overall architecture,dimensional  dae feature,extra feature,phrase table,difference,dimensional hidden representation,many dif ferent mechanism,algorithm,ini tializations,training sample,distortion measure,difference,different initial izations,different fraction,phrase table,detail,used network structure,example,architecture,lay er,network depth,16-dimensional input feature,hidden unit,2-dimensional output feature,new  dae feature,fine-tuning,network structure,correspondingly,input feature,result, dae feature,chinese-english translation task,bilingual corpus,chinese english part,cor pu,sentence pair,chi nese english word,lm corpus,en glish side,sentence,ur development set, iws lt,sentence,test set, iws lt,bilingual corpus,sen tence pair,lm corpus,english side,paral lel data,sentence,ment set,mt evaluation set,sentence,test set,mt eval uation,sentence,framework,phrase-based ma chine system,4-gram lm, sri lm toolkit,modified kneser-ney,china workshop,translation result,new  dnn feature,feature,maskey, hcd ae feature,extra feature,phrase table,represent,input feature,x-dimensional feature,feature,improvement,test set,bootstrap resampling,baseline,n x  xf,e x  xf,discounting,opti mization,hopkins,feature weight,translation quality,case-insensitive  ibm  ble u-4,baseline translation model,default parameter setting,contrast experiment, hcd ae fea tures,extra feature,phrase table,detail,used network structure,experiment,result table,main translation result, hcd ae,net work depth,input feature,8-dimensional feature,result,clear trend,new  dnn feature,extra feature,translation accuracy,increase,baseline,unsupervised  dbn fea tures,semi-supervised  dae feature,translation decoder,variance distribution,dimen sional  dbn, dae feature, dae feature,variance distribution,feature,lt  nis,variance distribution,dimensional, dbn feature, dae feature,figure,result,feature,different network structure,development set,feature,effectiveness,put feature,input feature,dimensional feature, dae feature,discriminative power,variance distribution,single  dae,high dimensional feature learning,performance, dae feature learning,limited input feature,input feature, dae feature learn ing,detailed effectiveness,input feature, dae feature learning,result,feature, dae feature learn,original feature,translation performance,increase,baseline, hcd ae feature,non-linear combination,original feature,strong capture high-order correlation,activity,original feature,original feature,analysis figure,result,single dae, hcd ae,feature,development set,figure, dae feature, dbn feature,various network structure,input feature,performance, dae feature learning,performance, dbn feature,figure,little change,per formance,single  dae,different dimensional  dae feature,4-dimensional feature,single  dae,high dimensional representation,peak point,condition,figure,network depth,dae feature learning,net work depth,network depth,fine-tuning, dae network,fine-tuning,poor local minimum,decreased performance,6 c onclusions,new feature,intuition,linguistic knowledge,new feature,phrase-based translation model,unsupervised pre-trained  dbn,parameter,input original phrase fea,teacher,semi-supervised back propagation,semi-supervised  dae feature, dbn feature,maskey,performance,simple,effective feature,input feature,feature learning,high dimensional feature representation,natural horizontal composition,large hidden layer,chinese-english translation task,result,solution,shortcoming, dae feature,sig nificant improvement, dbn fea tures,baseline feature,baseline phrase,input original phrase feature,performance, dae feature, dbn feature,result, hcd ae,feature,original feature,significant improve ments,baseline feature,original feature, hcd ae,feature,non-linear combi nation,original feature,strong cap ture high-order correlation,activity,original feature,deep learning paradigm,original feature,acknowledgment,program,xingyuan peng,lichun fan,hongyan li,helpful discussion,anonymous reviewer,insightful com ments,reference michael auli,michel galley,chris quirk,geoffrey zweig,joint language,translation model,recurrent neural network,proceeding, emn lp,pierre baldi,autoencoders,learn ing,workshop,learning,miguel,carreira-perpinan,geoffrey,hinton,contrastive divergence learning,pro ceedings,statistic,george dahl,alex acero,context-dependent pre-trained deep neural network,speech,language processing,li deng,mike seltzer,abdel rahman mohamed,geoffrey,hinton,binary coding,speech spectrogram,deep auto-encoder,proceeding, int erspeech,george foster,cyril goutte,roland kuhn,discriminative instance,domain adap tation,statistical machine translation,proceed ings, emn lp,geoffrey,hinton,product,ex perts,contrastive divergence,neural computation,geoffrey,hinton,li deng,abdel-rahman mohamed,navdeep jaitly,andrew senior,vincent vanhoucke,patrick nguyen,tara sainath,brian kingsbury,deep neural network,acoustic modeling,hinton,alex krizhevsky,auto-encoders,pro ceedings,geoffrey,hinton,ruslan,salakhutdinov,dimensionality,neural network,science,geoffrey,hinton,simon osindero,yee-whye teh,fast learning algorithm,deep belief net,neural computation,mark hopkins,jonathan may,ranking,proceeding, emn lp,nal kalchbrenner,phil blunsom,recur rent continuous translation model,proceeding, emn lp,philipp koehn,statistical significance test,achine translation evaluation,proceeding,philipp koehn,statistical machine translation,cambridge university press,philipp koehn,hieu hoang,alexandra birch,chris callison-burch,marcello federico,nicola bertoldi,brooke cowan,wade shen,christine moran,richard zen,chris dyer,ondrej bojar,alexandra constantin,evan herbst,open source toolkit,statistical machine translation,proceeding,demonstration session,philipp koehn,daniel marcu,statistical phrase-based translation,proceeding, naa cl,ois yvon,continuous space translation model,neural network,proceeding, naa cl,maosong sun,recursive autoencoders,translation,proceed ings, emn lp,lemao liu,taro watanabe,eiichiro sumita,tiejun zhao,additive neural network,statistical machine translation,proceeding,791-801,xiaoyin fu,recursive neural network,word topology model,hierarchical phrase-based speech transla tion,proceeding,philip resnik,soft syntactic constraint,hierarchical phrase-based translation,proceeding,sameer maskey,bowen zhou,unsuper,deep belief feature,speech translation,proceeding, int erspeech,piotr mirowski,marcaurelio ranzato,yann le cun,dynamic auto-encoders,semantic indexing,proceeding, nip s-2010 workshop,deep learning,patrick nguyen,milind mahajan,non-parametric feature,statis tical machine translation,proceeding,hermann ney,sta tistical alignment model,proceeding,hermann ney,discriminative training,maximum entropy model,statistical machine translation,proceeding,hermann ney,alignment template approach,statistical machine translation,computational linguistics,david rumelhart,geoffrey,hinton,ronale williams,internal representation,back-propagation error,parallel distributed processing,foundation,t pr es,ruslan,salakhutdinov,geoffrey,hinton,semantic hashing,international journal,approximate reasoning,richard socher,andrew,christopher,manning,natural scene,natural language,recursive neural network,proceeding,deyi xiong,min zhang,language model,statistical machine translation,backward n-grams,formation trigger,proceeding,bing zhao,stephan vogel,alex waibel,phrase pair,term weighting,sta tistical machine translation,proceeding,emn lp