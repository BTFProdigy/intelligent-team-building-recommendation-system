proceeding,international conference,computational linguistics,annual meeting,sydney,association,computational linguistics improved discriminative bilingual word alignment robert,moore wen-tau yih andreas bode microsoft research redmond,scottyhi,abode microsoft,com abstract,many year,statistical machine trans,generative model,bilingual word alignment,several independent effort,discriminative model,standard genus tive approach,substantial improvement,word-alignment accuracy,improved training method,selection,ter feature,alignment error rate,canadian hansard bilingual data,1 i ntroduction,statistical ma chine translation,word alignment,combination,generative prob abalistic model,dependent effort,fraser,taskar,ittycheriah,roukos,trained model,alignment accu racy,standard model,usual unla,bilingual training corpus,human-annotated word alignment,small subset,training data,various training procedure,wide variety,feature,fac torization,generative model,information,arbitrary feature,discriminative model,advantage,building,framework,training,test data,careful choice,feature,modest improvement,training procedure,error rate,word alignment,canadian hansard data,verall approach,previous work,weighted linear combination,fea ture value,sentence,word alignment,possible alignment,overall score,sentence pair,alignment,feature,weight,large number,bilin gual sentence pair,small number,word alignment,training procedure,hand alignment,different subset,overall training corpus,feature,surface statistic,training data,hypothesized alignment,entire training cor pu,feature,parallel sentence,statistic,alignment,sec tion,section,many feature,combina tions,feature,final selection,training,alignment search,method,previous beam search procedure,detail,minor modification,possiblity,alignment,account,beam search,beam size,possible alignment,structural tie breaker,alignment,one-best,alignment,change,alignment result,principal training method,adaptation,averaged perceptron learning,collins,cur rent,method,observation,perceptron training,learner,large-margin training technique,tsochantaridis,training procedure,section,3 s tage  1 m odel,previous stage,fea tures,informative feature,bilingual word-association score,word pair,log likelihood ratio,feature,degree,non monotonicity,alignment,alignment,source sentence token,instance,target sentence token,to-right order,feature,number,backwards jump,target sentence token,magnitude,trade-off,many-to-one alignment,feature,number,alignment link,fifth feature,number,sentence pair,addition,feature,hard constraint,constraint,alignment pattern,many-to-many link pat tern,single word,second constraint,possible link,degree,associ ation,sentence pair,association,sentence pair,new stage,feature,constraint,align ment pattern,constraint,association,addition,new stage,following feature,association score rank,association,respect,sentence pair,number,association type,word-type,association score,sentence pair,contraint,strength,association,requirement,corresponding association,hard constraint,feature,association rank,feature,associa tion rank,respect,second feature,minimum,association rank,respect,alignment,previous hard constraint,second feature,jump distance difference feature,origi nal model,feature,nonmonotonicity,likelihood,various forward jump distance,alignment,nonmonotonicity,correspond,large backwards jump,something,word alignment,free translation,alignment,different forward jump,ward jump,alignment,feature,difference,distance,source word,distance,target word,likelihood,large forward jump,source,align ment,language,distinction,parameter, ibm model,feature,fea ture sum,number,last word,feature,intuition,function word,anything,exact match feature,feature,number,identical word,proper name,language,advantage,high association score,lexical feature taskar,gain con siderable benefit,feature,particular high frequency word,feature,frequent non-punctuation word,language,feature,feature,bilingual word pair,co occurrence,labeled training data,addi tion,feature,number,unlinked occurrence,occurrence,labeled training data,new stage,many lexical feature,training data,first op,weight,feature,weight,lexical feature,weight,optimium val ues,lexical feature,4 s tage  2 m odel,original stage,log likelihood-based word association statistic,logarithm,conditional prob ability,cluster,aligned sentence,sentence pair,probability,max imum likelihood estimate,small fixed amount,condi tional link probability,cluster,number,discount,number,training,important difference,model con siders,word-to-word link,multiple link,alignment,one-to-many link,conditional probability,one-to-many cluster,cluster,disjoint,original stage,addtional fea tures,original stage,feature,number,non-one-to-one link cluster,new stage,model differs,number,original version,conditional probability,cluster,condi tional odds,cluster,estimated con ditional link odds,cluster,add-one,smooth ing,discount,additional feature,new stage,unaligned word feature,following feature,nonmonotonicity feature,previous nonmonontonicity fea ture,magnitude,backwards jump,backwards jump,target sentence order relative,source,sentence order,backwards jump,source sentence order relative,target sentence order,feature,number,backwards jump,multi-link feature,feature,num ber,link cluster,cluster,link score,one-to-one cluster,parameterized jump distance fea ture,advantage,alignment,feature,jump dis tances,alignment link,difference,source,target distance,source,target distance,alignment,full training data,possible target distance,possi ble source distance,corresponding target distance,feature,consist,scaled log odds,con secutive link,hypothesized alignment,source sentence,target sentence order,feature,source,target jump distance,cluster,many-to-one cluster,feature value,good result,training,good result,training,log odds estimate,absolute value,estimate,5 p erceptron training,feature weight,modification,averaged perceptron learning,collins,initial set,feature weight value,algorithm,effective training,training,feature weight,scale factor,original feature value,training data multiple time,comparing,sentence pair,alignment ahyp ac,current model,sentence pair,weight,feature,multiple,difference,feature,alignment,feature,reference alignment,updated feature weight,next sentence pair,learning rate,averaged percep tron,feature weight,final model,average,weight value,final sen tence pair,final iteration,difference,approach,collins,feature weight,evaluation pas af ter,feature weight,average value,learning pas,set  aer,procedure,local minimum,training,weight,informative feature,word-association score,conditional link probability,feature weight,weight,informative fea ture,weight,vary allows many equivalent set,weight,constant scale factor,weight,spurious apparent degree,freedom,learning rate,single learning rate,sequence,transition,feature weight,optimum value,previous learning rate,current work,number,mod ifications,procedure,feature weight,averaged value,begining,learning pas,convergence,local  aer minimum,multiple learning rate,learning rate,initial learning rate,maximum ab solute value,word pair cluster,word association,probability,odds feature,number,labeled training sentence pair,feature value,simple count,minimal difference,feature value,training ex ample,count feature,weighted value,informative feature,single pas,learning search terminates,learning rate,learning rate,fac tor,iterate,local minimum,learning rate,learning rate,entire pas,data produce fea ture weight,beginning,training,final modification,real ization,result,perceptron training,training data,training, aer increase,unfortunate ordering,therefore,aer increase,additional time,initial weight,different random,derings,learning rate,entire training process multiple time,feature weight,final averaging,graepel,6 s vm training,extensive experiment,perceptron train ing,re sults,sophisticated training method,perceptron training,good result,problem,poor result,taskar,word-alignment problem,method,put space,tsochantaridis,standard  svm learning,method,hyperplane,training,margin,large number,possible output label,possible alignment,sentence,optimal hyperplane,desired error rate,plane algorithm,iteration,al gorithm,incorrect prediction,current model,constraint,weight vector,main advantage,algorithm,special restriction,put structure,sev eral feature,method,decomposable feature,method,variety,loss function,simple 0-1 loss,alignment,sentence pair,anything, svm method,number,free param eters,different way,perceptron training,five-fold cross validation,method,training data,disjoint subset,parameter value,averaged  aer,test subset,training set,final model,entire training set,7 e valuation,training,test data,previous work,subset,canadian hansard bilingual corpus,bilingual word alignment workshop, hlt -naa cl,mihalcea,pedersen,subset com,english-french sentence pair,word-aligned sentence pair,labeled training data,sen tences,test data,automatic sentence alignment,training data,ul rich germann,hand alignment,franz och,mann ney,baseline,re sults,result,ibm model,alignment recall precision  aer prev,giza union,giza intersection,baseline result,package,default configuration file,prev  llr,version,condi tional link probability,alignment,prev  llr,heuristic alignment model,result,ibm model,direction,french-to english,intersection,combina tion,alignment,result,new stage,first line,section,non-lexical feature,lexical feature,second line,result,feature,next line,lexical feature,last line,original stage,improved perceptron training method,error rate,previous stage,phase training,lexical feature,reduction,purpose,two-phase training,training data,training set  aer,result,total reduction,error rate,new non lexical feature,lexical feature,two-phase training,improve ments,perceptron training,present result,perceptron training,new stage,first line,section,log odds,second line,chris quirk,giza alignment,alignment recall precision  aer two-phase train,one-phase train,lex feat,prev  llr,new train,stage  1 m odel result,alignment recall precision  aer log,stage  2 m odel result,result,similiar model,log probability,log odds,link model,jump model,result,log-odds-based model,dif ference,sig nificance,test sen tence pair,2-tailed paired test,result,third line,show result,probability,new stage,model alignment,recent modification,training,result,preceding model,fourth line,alignment,perceptron training method,result,new training method,difference,result, svm training,perceptron training,free parameter,entire training set,5-fold cross val idation,training set,cross-validation method,test-set  aer,result,result,perceptron training,difference,8 c omparisons,experiment,word alignment,canadian hansard bilingual data,direct comparison,difference,alignment recall precision  aer min,result,total training data,training data,test data,result,combination, ibm mod el, hmm model,bilingual dictionary,refined alignment combination,much data,cherry,method,mihalcea,peder sen,previous lowest,error rate,method, ibm mod el,cherry,method,explicit estimate,probability,co-occurence,parser,english side,corpus,many-to-one link,taskar,discrimina tive model,prediction,inter section, ibm model,alignment,feature,result,information, ibm model,experiment,section,aware development,research,taskar,previous approach,many-to-one align ments,first-order interaction,align ments,lacoste-julien,information,complex  ibm model,result,cludes,feature,intersected  ibm model,prediction,feature,val ues,alignment probability, hmm alignment model,di rections,align ment probability, ibm model,prediction,probabili tie,bidirectional  hmm model,additional fea ture value,feature,information,dif ference,result,lacoste julien,english french sentence pair,word align ment workshop,sentence pair,feature value,probability,log odds, hmm probability,high probabili tie,low probabil ities,training,small non-zero probability,experiment,unlabled training data,sentence pair,little dif ference,test-set  aer,perceptron,model feature, hmm log odds, svm train,5-fold cross validation,substan tial reduction,test-set  aer,pre cision,ficult,result,ibm model,intersection feature,result,training,test-set  aer,model feature weight, svm training, hmm log odds feature, hmm log odds feature weight,ibm model,intersection feature weight,ceptron training,test-set  aer,precision,recall,9 c onclusions,canadian hansard data,test-set  aer,aligner,expensive  ibm model,test-set  aer,combination, hmm log odds,intersection fea tures,general conclusion,result,discrim -3a,writing,difference,result,lacoste-julien,inative word alignment model,model struc ture,feature,dis criminative training method,secondary impor tance,small improvement,training method,differ ences,feature,reference necip fazil ayan,bonnie,christof monz,neuralign,combining word alignment using neural network,proceeding,human language technol ogy conference,conference,empirical method,natural language processing,vancouver,british columbia,stephen,della pietra,vincent,della pietra,robert,mercer,mathematics,statistical machine translation,parameter estimation,computational linguis tic,colin cherry,dekang lin,roba bility model,word alignment,proceeding,annual meeting,sapporo,michael collins,discriminative training method,hidden markov model,theory,experiment,perceptron algorithm,proceeding,conference,empiri cal method,natural language processing,philadelphia,pennsylvania,alexander fraser,daniel marcu,articipation,romanian-english align ment task,proceeding, acl work shop,building,using parallel text,ann arbor,michigan,ralf herbrich,thore graepel,scale bayes point machine advance,abraham ittycheriah,salim roukos,aximum entropy word aligner,arabic english machine translation,proceeding,human language technology conference,conference,empirical method,nat ural language processing,vancou ver,british columbia,simon lacoste-julien,ben taskar,dan klein,michael jordan,word alignment,quadratic assignment,proceeding,human language technology conference,north american chapter,association,computational linguistics,new york city,percy liang,ben taskar,dan klein,alignment,agreement,proceeding,human language technology conference,north american chapter,association,computational linguistics,new york city,yang liu,qun liu,shouxun lin,log linear model,word alignment,proceed ings,annual meeting,ann arbor,michigan,rada mihalcea,ted pedersen,eval uation exercise,word alignment,pro ceedings, hlt -naa cl,workshop,building,using parallel text,data driven machine translation,beyond,ed monton,alberta,robert,iscriminative frame work,bilingual word alignment,pro ceedings,human language technology conference,conference,empirical meth od,natural language processing,british columbia,franz joseph och,hermann ney,ystematic comparison,various statistical alignment model,computational linguistics,ben taskar,simon lacoste-julien,dan klein,iscriminative matching approach,word alignment,proceeding,human language technology conference,conference,empirical method,natural language processing,vancouver,british columbia,ioannis tsochantaridis,thomas hofmann,thorsten joachim,yasemin altun,margin method,structured,interdependent output variable,journal