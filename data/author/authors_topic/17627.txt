proceeding,conference,empirical method,natural language processing,seattle,washington,18-21 october,association,computational linguistics feature noising,log-linear structured prediction sida,christopher,manning department,statistic stanford university,stanford,mengqiu,pliang,edu swager stanford,abstract nlp model,sparse feature,regularization,versus underfitting,re popularized form,regularization,erate fake training data,real data,noising,explicit regularizer,second-order formula,fake data,method,prediction,multinomial lo gistic regression,linear-chain  crf,key challenge,dy namic program,gradient,regularizer,regularizer,transduc tive extension,classification,method,abso lute performance gain,standard l2 regularization,1 i ntroduction nlp model,million,attested feature,result,versus,good weight regulariza tion,key issue,formance,l1 regularization,simple type,regularization,feature,uniform way,account,property,actual model,alternative approach,regularization,fake training data,random noise,input feature,original training data,author,paper ing feature,unseen synonym,effectiveness,technique,machine learning,abu mostafa,burges,simard,van der maaten,many cor,dataset,feature,tractable deterministic objective,feature reduces,special form,reg ularization,matsuoka,bishop,example,bishop,feature,additive gaussian noise,l2 regularization,low noise limit,new objective function,artificial noise,manning,maaten,central contribution,training,noised feature,context,log-linear struc,prediction,dropout noise,hinton,artifi cial feature noise,random subset,feature,training example,dropout,variant,l2 regularization,various task,hinton,manning,spirit,deliberate removal,feature,removal,preset way,sutton,approach,second-order approx imation,others,bishop,dropout noise,adap tive regularization,method,prediction,log-linear model,second derivative,multiclass classification,tropy model,multinomial logis tic regression,sequence model,linear chain  crf,noising scheme,advantage,clique structure,noising regularizer,pair wise marginals,simple forward-backward-type dynamic program,gradient,implementation,scalability,semi-supervised learning,approximation,regularizer,general approach,clique structure,addition,linear chain,clique marginals,feature noising,structured prediction,semi-supervised set ting,regularizer,feature noising,log-linear model,unlabeled data,semi-supervised approach,input feature,unlabeled data, mcc allum,semi-supervised dropout training,logistic regression,similar intuition,technique,entropy regu larization,grandvalet,bengio,ductive  svm,joachim,confident prediction,unlabeled data,dropout,advantage,label probability,unlabeled data,l2 regularizer,heavy-handed modeling,entropy regularization,expecta tion regularization, mcc allum,experimental result,simulated feature,absolute boost yt,figure,illustration,dropout feature,linear-chain  crf,transition feature,node feature,green square,node fea tures,orange square,edge fea tures,conceptually,training ex,feature,parameter update,equivalent objective,performance,l2 regularization,text classification, ner sequence,2 f eature,log-linear model consider,standard,prediction problem,sentence,feature vector,weight vector,vector,output,exp sy,log-partition function,parameter esti mation,key idea,feature noising,feature vector,average log-likelihood,feature,motivation,predictor,ro bust,randomly perturbed version,feature,noised feature,scheme,feature noising,regularization,feature,feature,dif ference,log-normalization constant,jensen,inequality,regularizer,general non convex,regularizer,possible noised feature vector,exponential effort,number,feature,flat classification,bishop,second-order taylor expansion,covariance,noised feature,quadratic approximation,noised score vector,unnoised score vector,approximation,expression,source,tractability,exponential number,noised score vector,com ponents,component,model prob ability,variance,un certainty,uncertainty,feature,product,variance term,reg ularization,stable score,presence,feature noise,multiclass classification,regularizer,structured prediction,classification,moment,separate weight vector,output,component,following,scheme,example,possible noise scheme,original feature,distribution,regularization,variance term var,second derivative,log-partition function,variance,contribution,regularizer,el ementwise product,vector,vector,independent component,probability,multiplica tive gaussian,dropout scheme,equiva lent,key observation,noising regularizer,example,output,unlabeled data,example,unlabeled example,regularizer,linear combination,regular izer,datasets,tradeoff,dunlabeled,linear-chain  crf,regularizer,log-linear model,current form,multiclass classification,decomposable structure,new noising scheme,possible output,key idea,local feature vec tor,se quence,linear chain  crf,feature vector,local feature vec tor,consecutive tag,position,score sy,collection,local score,posi tion,scheme,corresponding collection,noised score,log-partition function,local score,first derivative yield,edge marginals,diagonal element,marginal variance,follow ing regularizer,model uncertainty,edge marginals,uncertainty,regularizer,confident prediction,stable score,feature noise,partial derivative,feature noising,derivative,difference,log-partition function,log-partition function,property,first deriva tive,quadratic function,product rule, a d ynamic program,conditional expectation naive computation,full forward-backward pas,position,time algo rithm,section,running time,intricate dynamic program,markov property,partial sum,local feature vector,position,feature,quantity,expected feature vector,prefix,suffix,tag sequence,forward,backward message,standard  crf inference,exception,vector,scalar,message,standard way,forward recurrence,similar recurrence,backward,resulting dynamic program,storage,number,sequence length,number,active feature,dependence,normal crf training,additional dependence,number,active feature,training,ast gradient computation,section,efficiency,gradient calculation,long-range interaction,feature sparsity,exploiting feature sparsity,co-occurrence,forward-backward pas,training ex,conditional ex pectations,feature,example,dynamic program,section,active feature,total com plexity,number,active fea tures,sen tence length,practice,number,example,average number,active feature,compu tational cost,sequence tagging, nlp task,majority,feature,ac tive feature,lo cation,sequence,particular token trigger,rare feature,indicator,position,conditional ex pectations,model expectation,result,feature,feature,preprocessing step,identical expectation,feature,experiment cut,accuracy,exact trick,general  crf gradient computation,similar speedup,short-range interaction,method,gradient,dynamic program,section,manageable amount,technique,quantity,intuition,window size,approximation result,expression,local feature vec tor,last expression,second expectation,edge marginals,accuracy,approximation,long range dependency,addi tional effort,experiment,difference,real derivative,computational saving,bounded-window method,dataset  d k ntrain ntest con ll,san cl,description,non-zero feature,ntrain,number,training example,number,test example,5 e xperiments,experimental result,several doc ument classification task,standard split,ran dom,test set,train set,equal size,development set,regulariza tion parameter, san cl test set,answer,newsgroups,review,velopment,regularizer,simple case,classification,performance,regularizer,supervised setting,transductive learning,transductive learning setting,learner,train time,method,section,classification data,datasets,cn home dengcai data textdata, san cl dataset,additional genre,weblogs,access,development set,dataset  k n,l2 drop test con ll,r21578,classification performance,transduc tive learning result,standard datasets,regularization,quadratic ap proximation,test set,feature noising,transductive setting,test data,regularizer,unlabeled data,test data,similar goal,semi-supervised setting,semi-supervised idea,datasets,dataset,training set,test set,unlabeled dataset,result,semi-supervised accuracy,transductive accuracy,labeled data,semi-supervised classifier,transductive one,second-order approximation,result,ap proximate,second-order taylor expansion,validity,approximation,gaussian method,manning,two-class classification task,20-newsgroups alt,atheism v soc,religion,christian classification task,result,figure,exam -4t,nl result,result,transductive one,reason,original  con ll test set,different distribution,training set,semi-supervised experi ment,train set,distribu tion,semi-supervised task,original one,dataset,con ll,r21578,result,standard datasets,full dataset,training,l2 gaussian dropout l2 quadratic dropout figure,effect,testset perfor mance,accuracy,logis tic regression,function,l2 regular izer,gaussian dropout,manning,default,regularizer,additional l2,choice,feature,training set,test set,broad range,dropout,l2 regularization performs,l2 regularization,gaussian dropout,quadratic approxima tion,quadratic approximation,multiclass case,prediction,gaus sian dropout,ap proximation,reasonable trade-off,computational efficiency,prediction accuracy, crf experiment,quadratic dropout regularizer,linear-chain  crf,sequence, con ll,tjong kim sang,de meulder, san cl,petrov,standard  con ll-2003 english,task benchmark dataset,tjong kim sang,de meul der,collection,document,reuters newswire article,tity type,person,location,organization,miscellaneous, bio tag, crf model,compre hensive set,feature,finkel,state-of-the-art result,total number,feature,con ll-2003 training dataset,important feature,word shape,letter n-grams,prediction,word shape,conjunction,conjunction,word shape,previous word,current word,name title,result,absolute gain,dev set,result,precision,recall,improvement,signifi cant,paired boot strap,method,iteration,tibshirani, san cl,petrov, crf framework,much simpler set,feature,word bigram, con ll summary,result,reg ularization,l2 drop,review dev,answer dev,official evaluation set,consistent improvement,quadratic dropout regularizer,l2-regularized  crf baseline,difference, san cl,performance difference,test set,review,newsgroups,sig nificant,situation,feature,l2 regularization,improve ment,regularization overall matter,6 c onclusion,new regularizer,log-linear model,multiclass logistic regres sion,conditional random field,regularizer,second-order approximation,fea ture,scheme,attempt,mod el,method,key challenge,feature correlation,structured prediction,several way,addition,regularizer,setting,method,different datasets,con sistent gain,standard l2 regularization,precision recall,regularization precision recall,l2 reg ularization precision recall,dropout regularization tag precision recall,regularization precision recall,l2 reg ularization precision recall,dropout regularization, con ll  ner result,precision,recall,development set,bottom,test set performance,non-convex reg ularizer online,promising future di rections,author,anonymous re viewer,comment,support,program,opinion,finding,conclusion,mendations,material,author, dar pa,ej eaves  sgf fellowship,reference yaser,abu-mostafa,neural network,journal,bishop,equiva lent,tikhonov regularization,robert bryll,ricardo gutierrez-osuna,francis quek,bagging,accuracy,classifier ensemble,random feature sub set,pattern recognition,burges,bernhard scho,accuracy,support vector ma chine,advance,neural information processing system,brad efron,robert tibshirani,introduction,bootstrap,new york,jenny rose finkel,trond grenager,christopher manning,non-local informa tion,information extraction system,gibbs sam,proceeding,annual meeting,association,computational linguistics,yves grandvalet,yoshua bengio,entropy regularization,semi-supervised learning,united kingdom,springer,geoffrey,hinton,nitish srivastava,alex krizhevsky,ilya sutskever,ruslan,salakhutdinov,neural network,co-adaptation,feature detector,iv preprint arx iv,feng jiao,shaojun wang,chi-hoon lee,russell greiner,dale schuurmans,conditional random field,improved se quence segmentation,labeling,proceeding,annual meeting,association,joachim,transductive inference,text classification,support vector machine,proceeding,international conference,ma chine learning,wei li,andrew  mcc allum,semi-supervised sequence,syntactic topic model,proceeding,20th national conference,arti ficial intelligence volume,gideon,andrew  mcc allum,sim ple,robust,scalable semi-supervised learning,ex pectation regularization,proceeding,inter national conference,machine learning,kiyotoshi matsuoka,injection,back-propagation learning,system,cy bernetics,slav petrov,ryan  mcd onald,overview,first workshop,syntactic analysis,salah rifai,yann dauphin,pascal vincent,yoshua ben gio,xavier muller,manifold tangent classifier,advance,neural information processing system,salah rifai,xavier glorot,yoshua bengio,pascal vincent,regularized objective,iv preprint arx iv,patrice,simard,le cun,denker,bernard victorri,transformation invariance,pattern recognition,tangent distance,propagation,international journal,imaging system,tech nology,andrew smith,trevor cohn,mile osborne,logarithmic opinion pool,conditional random field,proceeding,annual meeting,association,computational linguistics,computational linguistics,charles sutton,michael sindelar,andrew  mcc al lum,feature bagging,weight un dertraining,structured discriminative learning,cen ter,ma sachusetts,tjong kim sang,fien de meulder,introduction,conll-2003 shared task,language,entity recognition,proceeding,seventh conference,natural language learn, hlt -naa cl,volume,laurens van der maaten,minmin chen,stephen tyree,kilian,weinberger,corrupted feature,proceeding,international conference,machine learning,stefan wager,sida wang,percy liang,dropout training,adaptive regularization,iv preprint,li wan,matthew zeiler,sixin zhang,yann  lec un,rob fergus,regularization,neural network,dropconnect,proceeding,interna tional conference,machine learning,sida wang,christopher,manning,training,proceeding,international conference,machine learning,proceeding,50th annual meeting,association,computational linguistics,republic,association,computational linguistics baseline,bigram,simple,good sentiment,topic classification sida wang,christopher,manning department,stanford,abstract variant,baseline method,text classification,performance varies,model variant,feature,task dataset,inclusion,word bigram,consistent gain,sentiment analysis task,short snippet sentiment task,document,oppo site result,novel  svm variant,nb log-count ratio,feature value,datasets,observation,simple nb, svm variant,result,sent ment analysis datasets,new state-of-the-art performance level,baseline,method,text categorization,sentiment analy si research,performance varies,variant,feature,datasets,researcher,sufficient attention,tion issue,variant,state-of-the-art method,many datasets,method,variant,circumstance,important distinction,sentiment classification,topical text classifica tion,usefulness,bigram feature,feature sentiment classification,usefulness,mixed bag,topical text classifica tion task,short snip pet sentiment task,review,literature,feature model,strong performer,snippet sent ment classification task,nb model,structure-sensitive model,recent work,discriminative classifier,simple model variant,nb log-count ratio,feature value,robust performer,result,result,standard  mnb,datasets,result,main model variant,linear clas sifiers,prediction,test case,equivalent probabilistic formulation,feature count vector,stanford,feature,number,oc currences,feature vj,count vector,number,negative training case,metsis,l2-regularized l2-loss  svm,l1-loss  svm,li -bl inear  library,nb feature,elemen twise product,long document,interpolation, svm performs,docu ments,result,mean magnitude,interpolation parameter,interpolation,regularization,trust nb,3 d atasets,result,following datasets,statistic,short movie,sentence,review,statistic,number,number,cross validation split,difference,customer review dataset,nakagawa, mpq dataset,subjectivity dataset,subjective re view,objective plot summary,full-length movie re view dataset,full length review, bbc rypt,classify pair,newsgroups,20-newsgroups dataset,header,ver sion5,window,graphic,4 e xperiments,result,provided tokenizations,anything,bigram,amaas data sentiment,people,use stopwords,lexicon,resource,result,nbs vm,comparison,result,10-fold cross-validation,test split,dataset,cv column,specifies,standard split,approximate upper-bounds,difference re,column,snippet,moilanen,pulman,statistical method,datasets,hundred,example,handle snippet datasets,rule-based sys tem,example,inhumane monster6,cancer,overall positive sentiment,nega tive word,previous work,snippet,pre-defined polarity reversing rule,moilanen,pulman,com plex model,parse tree,nakagawa,socher,rule-based method,baseline,nakagawa,state-of-the-art method,lexicon,reversal rule,unsupervised pretraining,result, svm uni result, bof nod ic,nakagawa, a s vm,second order polynomial kernel,additional feature,exception,linear  svm,weak baseline, nbs vm,sentiment snippet task,result,hypothe -6a positive example,discrimi native,document,method,mnb bi,svm bi,result,snippet datasets,recursive autoen coder,socher,wikipedia,collobert,weston,sentiment lexicon,hard-coded reversal rule,polarity,phrase,odd number,reversal phrase,ancestor,method,rule-based system,snippet datasets,snippet,document,jordan,training case,short document,contrast,result,training case,snippet,full-length review,review,ex cellent performance,snippet datasets,many poor assumption,rennie,full-length sentiment analy si task,result,exceeds,ap proaches previous state-of-the art method,result rt-2k  imd b su,mnb bi,svm bi,bow svm ,valence shifter,taxonomy,result,long review,dataset subj,comparison,result,row 7-11,linear  svm,word feature,binary,malization,delta idf,full model,additional unlabeled data,word  svm,inkpen,word representation restricted boltzmann machine,additional data,sentiment anal ysis result,benefit,bigram depends,task word bigram feature,text classification task,usual term,overall limited utility,topical text classifica tion task,certain topic keywords,bigram,performance,result,presum,sentiment classification,trigram,method athr xgraph  bbc rypt mnb,mnb bi,svm bi,activesvm,disclda,20-newsgroup subtasks,disclda,lacoste-julien,ac tivesvm,schohn,bigram, nbs vm,robust performer nbs vm,snippet,doc uments,sentiment,subjectivity clas sification,lished result,strong baseline,sophisti,method,feature,disadvantage, nbs vm,performance,document,snippet, nbs vm,extreme value,snippet task, nbs vm,raw count feature,difference,snippet,logistic regression,similar result,result,reference,collobert,weston,unified architecture,natural language processing,deep neural network,multitask learning,proceeding,george,hugo larochelle,restricted boltzmann machine,word observation, arx iv,kai-wei chang,cho-jui hsieh,xiang-rui wang,chih-jen lin,large linear classification,journal,minqing hu,bing liu,mining,summariz,customer review,alistair kennedy,diana inkpen,sentiment classification,movie review,contextual va lence shifter,computational intelligence,fei sha,michael,jordan,disclda,discriminative learning,dimen sionality reduction,classification,proceeding,andrew,raymond,dan huang,andrew,christopher potts,word vector,sentiment analysis,pro ceedings,justin martineau,tim finin,delta tfidf,improved feature space,sentiment analysis,pro ceedings, icw sm,andrew  mcc allum,kamal nigam,compar ison,event model,naive bayes text classification, aaa i-98 workshop,vangelis metsis,ion androutsopoulos,georgios paliouras,naive bayes,proceeding,karo moilanen,stephen pulman,sentiment composition,proceeding, ran lp,september,tetsuji nakagawa,kentaro inui,sadao kurohashi,dependency tree-based sentiment classification,hidden variable,proceeding,andrew  y n,michael  i j ordan,comparison,lo gistic regression,naive bayes,proceeding,volume,bo pang,lillian lee,sentimental education,sentiment analysis,subjectivity summarization,minimum cut,proceeding,bo pang,lillian lee,exploiting class relationship,sentiment categorization,respect,rating scale,proceeding,rennie,lawrence shih,jaime teevan,karger,poor assump tions,naive bayes text classifier,proceeding,greg schohn,david cohn,ac tive learning,support vector machine,pro ceedings,richard socher,jeffrey pennington,christopher,manning,semi-supervised recursive autoencoders,sentiment distribution,proceeding,emn lp,casey whitelaw,navendu garg,shlomo argamon,appraisal taxonomy,sentiment anal ysis,proceeding, cik m-05,janyce wiebe,theresa wilson,claire cardie,expression,opinion,emotion,language,language resource,evaluation,proceeding,annual meeting,association,computational linguistics,bulgaria,august,association,computational linguistics fast,adaptive online training,feature-rich translation model spence green,sida wang,daniel cer,christopher,computer science department,stanford university spenceg,danielcer,stanford,edu abstract,scalable online method,statistical machine trans lation model,large feature set,feature,recent discrimi native algorithm,sparse feature,translation quality gain,large system,method,stochastic gradient descent,adaptive learning rate,million,feature,thousand,sentence,large-scale experiment,chinese-english show,method,significant trans,quality gain,sparse fea tures,analysis,technique,overfitting,domain mismatch,ap ply,recent discriminative method,machine translation,1 i ntroduction sparse,feature,gram context,many  nlp system,parser,tagger,adaptation,discriminative learning method,feature,system,tech niques,dense feature,active research area,past half-decade,research success,feature-rich model,annual mt evaluation,exam ple,submission,participant,feature,hasler,method,implementation complexity,practical difficulty,configur,specific translation task,gimpel,simianer,inter alia,new method,feature rich mt system,algorithm,million,feature,lo gistic objective identical,hopkins,stochastic gradient descent,objective,learning rate,adagrad,mixture,sparse feature,mt model,feature selection,efficient l1 regularization,singer,algorithm,batch alternative,good weight,sparse feature,algorithm,benefit,natural source,bitext,bitext present,prob lem,single reference,quality,multiple reference,mt competition,second,large bi text,many text genre,haddow,virtue,classical dense mt mod el,high dimensional model,significant domain adaptation problem,standard test set,analysis separate,quantifies,large-scale translation quality,iments,baseline,implementation,k-best  mir,cherry,foster,online,chiang,feature-rich model,first experiment,standard tuning,test set, nis t op enmt competition,second ex periment,test set,large bitexts,new method yield significant improvement,experiment,phrasal,toolkit,2 a daptive online algorithm machine translation,unusual machine,setting,multiple correct translation,decoding,large feature,large data set,batch method,online method,practice,better solution,bottou,bousquet,inter alia,fundamental online method,weight,tth example,gradient,respect,param eters,mt system,feature,language model,sparse feature,translation rule,applies,coordinate,gradient,undesirable property,good sparse feature,sparse feature,dense feature,adagrad adagrad,method,adaptive learn,good theoretical guaran tee,theoretical improvement,sparse fea tures,adagrad,following update,diagonal approximation,adagrad,single dimension,scalar,theupdate rule,loss function,section,prior online algorithm,mt adagrad,previous online,method,adaption,crammer,following update,first term,conservativity,weight,sin gle example,minimum,relationship,derivative,result, aro model,current weight,covari ance,kl-divergence term,sensitive conservativity,third term,result,conser vative,direction,kl-divergence,gaussians,closed form,third term,confidence term,adaptivity,notion,variance,direction,direction,example,indicator feature,confidence term,weight,rarer feature,variance,vice-versa,generalized linear model,differentiate,precision,updated feature,high precision,large variance,feature, aro update wt,adaptive update,adagrad,square root,step size,specification,conser vativity direction,update,adaptivity direction,adagrad,decaying stepsize,adaptive stepsize,weight update,feature,sparsity,adaptivity,conservativity,withmixed dense sparse feature,specific normalization,adagrad,closed-form,hinge loss,ada grad,gradi ent,loss function,algorithm  1 a daptive online,repeat,random order,decode n-best list ni,convergence linearized  aro,ada grad empirically2,theoretical guar antees,adagrad,adaptivity,conservativity,experiment,adaptivity,3 a daptive online mt algorithm,full algorithm,adagrad,crucial piece,loss function,regularization technique,parallelization strategy,section,pairwise logistic loss function algorithm,gradient com putation,pairwise ranking,herbrich,inter alia,hopkins,pairwise,proach result,convex loss function,online learning,derivation,ranking,ranking,gold sentence-level metric,single source sentence,derivation,n-best list,model score,derivation,pairwise agreement,experiment,pairwise agreement,derivation pair,difference vector,1-class separa tion problem,derivation pair,algorithm,hopkins,familiar logistic loss,hinge loss,logistic loss result,1-class  svm problem,class separation problem,binary classification problem,positive data,negative data,logistic re gression solver,algorithm,mini batch,single example,tuning,mini-batch,example,updating,regularization algorithm,compute,adaptive learn,weight,regulariza tion,section,adagrad,rate computation,weight,forward-backward split,singer,framework,operation,two-step fob o update,unregularized gradient descent step,regularization term,gradient step,efficient l1 regulariza tion,good feature,many irrelevant feature,feature selection,example,sim ple indicator feature,re-ordering class,fea ture,parallelization method,algorithm,require,repeat,random order,thread,convergence search,feature,previous work,heuristic filtering,chiang,inter alia,contrast,closed form solution,wt sign,function,weight,threshold,dimension,takingelement-wise product,active feature,current example,fob o framework,million,weight,practical benefit, fob o,update,active dimension,approximation,parallelization algorithm,standard online learning,decoding,algo rithm,stale gradient,method,langford,fixed threadpool,worker computes gradient,parallel,master thread,central weight vector,crucially,weight update,synchronization,worker,consequence,update,al gorithm,respect,convergence result,stale updating,l1 regularization,neverthe,gimpel,framework,non-convex objective,good empirical result,stochastic method,cal appeal,tuning run,online method,tuning,weight vector, a m ert- style batch method,exploration,search space,learner,ro bust,local optimum,bottou,bousquet,adaptive algorithm identifies,mixture,sparse feature,large data structure,phrase table exist,mem ory,remote query,4 e xperiments,chinese-english mt system,phrasal,phrase,system,alignment template,experiment,derive,several  ldc source,bitext ac,overlap,test set,alignment,berkeley aligner,standard setting,grow-diag heuristic,language, sri lm,stolcke,5-gram lm,modified kneser-ney smoothing,monolin gual english data,respective target bitexts,feature,baseline,feature,moses baseline feature,re-ordering model,galley,man ning,indicator,unique rule,sparse,feature set,discrimina -3w,english,package,stan ford parser,manning,penn treebank standard,marcus,arabic,stanford arabic segmenter, den ero,accord,penn arabic treebank standard,maamouri,stanford chinese segmenter,penn chinese treebank standard,monolingual corpus,experiment,monolingual english data,xinhua section,indicator,phrase table,indica tor,phrase-internal alignment,source word,discriminative re,indicator,lexicalized,tone swap discontinuous class,sim pler moses,non-monotone class,tuning algorithm,primary baseline,dense feature,phrasal implemen tation,line search algorithm,uniform initialization,high dimensional baseline,dif ferent algorithm,batch  pro,default setting,phrasal,k-best batch,foster,imple mentation,online version,small-scale experiment,batch variant,cherry,foster,result,implementation,standard setting,moses5,discriminative phrase table implementation,hasler,implementation,phrasal,phrasal,phrase table,lm format,decoder,multi-stack beam search,method,uniform initialization,thread,mini-batch size,development set,language,gradient,system setting,experiment,distortion limit,maximum phrase length,n-best size,january,model feature algorithm tuning set mt02 mt03 mt04 mt09 dense, mer t mt,paper mt06,paper mt06, pro mt06,paper mt06, mer t mt,paper mt05,chiang, mir a mt,chiang, aro w mt,sentence,experiment,tuning,test set,reference,sentence,concatenated mt05,sentence,statistical significance relative,baseline,significance,permutation test,riezler,maxwell,similar-sized bitext,much monolingual data,algorithm tuning set mt02 mt03 mt04 dense, mer t mt,paper mt06,paper mt06, pro mt06,paper mt06, mer t mt,paper mt05,sentence,sentence,asymmetry,derivation pair,ex ample, nis t op,experiment,first experiment,algorithm,tuning,standard test set,reference,feature,algo rithm,standard-sized tuning,tuning set,ar-en result,algorithm,low dimensional,setting,pt feature,additional feature,algorithm im,additional feature,underperformance,kb-mira,difference,phrasal,pt feature set,nevertheless,kb-mira,unnecessarily large model,full feature,pt al lo,pt feature,algorithm,model feature algorithm tuning set,mt04 dense, mer t mt,paper mt05,bitext,experiment,statistical significance,dense baseline,comparison, nis genre,algorithm tuning set,mt04 dense, mer t mt,paper mt05,bitext,experiment,algorithm,desirable property,feature,show zh-en result,somewhat,algorithm,dense setting,discrimina tive phrase table,algorithm improves,kb -mi ra,batch  pro,evaluation set,feature,tuning,pt al lo fea ture,l2 regularization,adagrad,consequence,stochastic learn ing,algorithm decodes,example,new weight vector,search space,tuning,bitext tuning experiment table,example,translation quality,nevertheless,tuning set,bitext,simianer,significant translation quality gain,bitext,bitexts,test set,bitexts,large-scale system,mismatch matter,dense feature,haddow,bitext,sentence,test set,mt04 mt568,bitext5k,bitext15k,number,various zh-en dataset pair,test set,dense model,feature-rich model,bi text tuning,ar-en result,translation quality gain,bitext5k-test relative,multiple reference,bitext5k model,nis evaluation set,mt04 result,show similar trend,nalysis,tun ing,test set,phrase table feature,non-zero weight,datasetda,several zh-en test set,column db list,experiment,bitext5k,bitext5k,fea ture overlap,phrase table feature,correct phrase,l1 regularization,feature selection,number,feature,weight,feature,bitext15k,coverage,generalization,domain adaptation analysis,domain adaptation issue,non-zero weight,ar-en model,bi text5k,statisti cal idiosyncrasy,american,british spelling,program programme,diagonal,cause mt05,amer ican agency,bitext,many source,agence france presse,course,discrepancy,feature-rich model,feature-rich model,example,model learns rule,program,program,contrast,bitext5k model,programme,programme,elaborate rule,programme expense,space flight programme,similar trend,particular genre problem,language-specific pre-processing,sys tem,data-driven manner,re-ordering analysis,re-ordering difference,arabic matrix clause,subject,translat ing,english,re-ordering difference,feature-rich model,mt09 segment,programme,program,pt rule,pt rule,program,comparison,token count,programme,program,bot tom,rule count,spelling correspond,	x dhkr,output,mer t de nse model,method,full feature,source segment,translation pair,dif ferent word order,matrix clause,feature rich model,method,lebanese prime minister,fuad siniora,lebanese prime minister fuad,lebanese prime minister fouad siniora,newspaper,television,newspaper,television,television,newspaper,feature-rich model,re-orders,insert,coordinated subject,embedded subject,runtime comparison table,method,implemen tations,runtime increase, l-b fgs,slow procedure,feature,convergence,approximate runtime,minute,zh-en experiment,dedicated system,number,thread,kb-mira,java implementation,phrasal,phrasal,moses  pro implementation,l2 regularization,weight,update,multiple pass,n-best list,moses implementation,weight updating,method,inner product,adaptive learning rate vector,gra dient,large feature set,lazy regularization,inner product,hundred-dimensional vector,method,n-best list,practice,algorithm,hasler,sparse,dense feature,discriminative phrase table,baseline,result,initialization, mer t-tuned weight,weight,dis criminative phrase table,contrast,algorithm,good high dimensional mod el,uniform starting point,chiang,previous work,online  mir,chiang,watanabe,improvement,novel hope fear search,conservativity gain,adaptivity,ticated parallelization,contrast,adagrad,conservativity,adaptivity,simianer,pairwise perceptron objective,algorithm,iterative parameter,stale gradient method,section,weight update,empirical comparison,strategy,interesting fu ture contribution,watanabe,pairwise sample,softmax,hinge loss,result,latter,parallelization strategy,line search,many discriminative technique,gimpel,hinge loss,cherry,foster,haddow,ittycheriah,ittycheriah,roukos,perceptron,till mann,hese work,different experimental setup,knowl edge,cherry,foster,work compare,high dimensional baseline,comparison,method,7 c onclusion,outlook,new online method,feature-rich translation model,method,million,feature,converges,efficient l1 regularization,feature selection,feature scaling,heuristic filter,prior work,vanilla  sgd,method easy,basic discriminative fea tures,fresh approach,sophisticated mt feature engineering,acknowledgment,john  den ero,helpful com ments,first author, an ational science foundation graduate research fellowship,support,program,opin ion,finding,conclusion,recommendation,material,author, dar pa,u government,reference,online,method,discriminative training,phrase,statistical machine translation,bottou,bousquet,tradeoff,large scale learning,optimization,machine learning,regu larization,search,minimum error rate training,phrasal,statistical machine translation toolkit,new model feature,chinese word segmentation,machine translation performance,foster,strategy,statistical machine translation,resnik,line large-margin training,structural translation feature,new feature,statistical machine translation,discrimina tive training,statistical translation model,singer,dredze,adap tive regularization,weight vector,singer,efficient online,batch learning,singer,adaptive sub gradient method,online learning,effective hierarchical phrase,ramp loss minimization,machine translation,asynchronous online learning,natural language processing,discriminative feature-rich mod,syntax-based machine translation,thesis,language technology institute, den ero,class-based agree ment model,inflected trans lations,effect,out-of-domain data, smt system,sampler,phrase-basedmachine translation, ued system, iws lt,evaluation,sparse,feature,topic adaptation,maximum, ble training,phrase,lexicon translation model,obermayer,support vector,ordinal regression,ranking,roukos,direct translation model,accurate,parsing,open source toolkit,statistical machine translation,zinkevich,slow learner,online em,unsuper,taskar,end-to-end discriminative approach,machine translation,alignment,agreement,method,automatic evaluation metric,machine translation,kulick,enhanc,arabic treebank,collaborative effort,new annotation guideline,marcus,marcinkiewicz,santorini,large annotated corpus,en glish,penn treebank,computational linguis tic,training strategy,structured perceptron,feature selection,rotational invariance,discriminative training,maximum entropy model,statistical machine translation,alignment template approach,statistical machine translation,compu tational linguistics,minimum error rate training,statis tical machine translation,method,automatic evaluation,ma chine translation,pitfall,automatic evaluation,significance testing, acl workshop,intrinsic,extrinsic evalua tion measure,machine translation,joint feature selection,stochastic learning,large scale discriminative training,a s tolcke,extensible language,toolkit,discriminative global training algorithm,isozaki,large-margin training,statistical ma chine translation,online rank,machine translation, hlt -naa cl,associa tion,ittycheriah,discriminative feature-tied mixture,statistical machine translation,palmer,penn chinese treebank,phrase structure annotation,large corpus,natural language engineering,proceeding,eighth workshop,statistical machine translation,bulgaria,august,association,computational linguistics feature-rich phrase-based translation,translation task spence green,daniel cer,kevin reschke,bauer sida wang,christopher,computer science department,stanford university center,east asian study,stanford university,department,linguistics,stanford university spenceg,kreschke,robvoigt,horatio,stanford,edu abstract,stanford university  nlp group submission,workshop,statistical machine translation shared task,effectiveness,new adaptive,algorithm,large feature,english-german,feature-rich mod el,dense baseline,compare,method,1 i ntroduction green,describe,online,adaptive tuning algorithm,feature-rich translation mod el,considerable translation quality improvement,hopkins,language,research setting,purpose,submission,workshop,shared task,algo rithm,method,evaluation,english german,system,fea tures,sentence,system,new feature set,practical extension,original algorithm,2 t ranslation model,system,phrasal,phrase-based system,alignment template,many mt system,phrasal model,predictive translation distribution,target sequence,source se quence,vector,feature map,malizing constant,many year,dimension,feature,past ten,feature,submission,real-world translation quality,high-dimensional feature map,sociated weight vector,online,adaptive tuning algorithm followinghopkins andmay,tun ing,pairwise ranking,single source sentence,associated reference,derivation,n-best list,linear model scorem,derivation,gold metric,pairwise agreement,derivation pair,difference vector,1-class separa tion problem,derivation pair,algorithm,hopkins,source sentence ft,familiar logistic loss,hopkins,batch algorithm,n-best list,lattice decoding,use adagrad,variant,stochastic gradient,learning rate,informally,adagrad,weight,geometry,data ob,iteration,particu lar dimension,scalar,ada-grad update rule,practice,diagonal approximation,vanilla  sgd,mt system,feature map,many irrelevant feature,l1 norm,weight vec tor,effective regularizer,efficient way,l1 regularization,forward-backward split,singer,following two-step update,unregularized gradient descent step,regularization term,gradient step,l1 regularization,closed-form solution,wt sign,function,weight,threshold,algorithm,exception,algorithm,weight update,green et al,parallelization technique,extension,sentence-level metric,bigram precision,multiple reference,single-reference setting,unigram precision,reference length,extension,ble improvement,test time,language,subsequent experiment,data set,standard  ble,multiple reference,custom regularization parameter green,large feature-rich model,certain fea tures,others,custom regularization strength,feature,solution,problem,technique,overfitting problem,section,convergence criterion standard  mer imple mentation,previous n-best list,updated weight vector,approximation,infeasi ble,algo rithms,n-best list,1-best hypothesis,segment,corpus-level  ble,hypothesis set,stored hypothesis,next epoch,approximation,convergence criterion,conventional method,algorithm,iteration,held-out accuracy,3 f eature,baseline,feature,baseline feature,hierarchical lexicalized re-ordering model,galley,manning,bitext count,translation rule,indicator,unique rule,final dense feature set,language,en-fr system,second language model,en-de system,future cost component,linear distortion model,future cost estimate,distortion limit,decrease,translation quality,hy pothesis extension,prior work,sparsemt feature,feature extractor,feature,regularizer,informative feature,several,feature extractor,source-side part,sequence,dependency par,annotation,stanford corenlp pipeline,discriminative phrase table,dicator feature,derivation,feature weight,adjustment,dense phrase table feature,discriminative alignment,lexicalized indi cator feature,phrase-internal alignment,derivation,many-to-many alignment,clique,aligned token,lexical sort,feature string,lexicalized indi cator feature,derivation,following orientation,green et al,non-monotone class swap,simpler non-monotone class,feature weight,adjustment,generative lexicalized re-ordering model,source content-word deletion count-based feature,source content word,target,content word,adjective,deleted source word,frequent target word,target bitext,deleted word,feature,particular source  pos,aggregate feature,speech,separate feature,head content word,target word,inverse document frequency numeric fea tures,source,target word frequen cies,inverse document fre quency,training bitext,translation rule,target side,source side,rule bilingual monolingual sentence token token en-fr,gross corpus statistic,data selection,pre-processing,en-fr monolingual count,source token,target,numeric feature,source,target,target feature,target feature,feature,asymmetric rule,rare word,frequent word,vice versa,lexicalized dis criminative re-ordering model,re-ordering feature,source part,speech,derivation,source  pos sequence, pos sequence,next rule,indicator feature,conjunction,previous pos sequence,indicator feature,feature,4 d ata preparation table,pre-processed corpus,system,data selection,parallel en de data,constrained condition,french monolingual data,5m-sentence bitext,approx, 40m available en-fr parallel sentence,sentence,corpus,tuning,test set,feature decay algorithm,sentence,target corpus,principled method,phrase table size,relevant training example,english,source,penn treebank standard,marcus,stanford corenlp,french data,package,stanford french parser,scheme,french treebank,abeill,pervasive compounding,english tokenizer,compound,lattice-based model,segmentation marker,alignment,bitexts,berkeley aligner,alignment,language,unfiltered 5-gram language model,heafield,heafield,memory effi ciency,binary format,german lm,monolingual data,target side,en-de bitext,analogous model,addition,separate french lm,gigaword data,french verb,number,person,subject,adjective,past participle,number,gender,phrasal align ment,side language,yield cor rect agreement inflection,inflection,number,english verb,person,absence,person pronoun,en glish,encode gender,adjective inflection,language modeling,mt system,different weight,primary lm,problem,automatic inflection correction post-processing step,dependency par,system, bon sai ,candito,a f rench-specific extension,berkeley parser,petrov,dependency,adjective,past participle,subject,machine-readable french lexicon,gender,number,correct inflection,sentence development set,correction scheme,correction,perfect accuracy, ble score,reference translation,different adjective inflection,german de-compounding split german compound,translation,process,adjacent token,compound,german compounding rule,dic tionary lookup procedure,backoffs,dic tionary,pre-processing,final translation,compound sequence,segmentation marker,dictionary,dictionary entry,com pound,suffix,compound,first word,first word,suffix,character,example,compound,phrasal,lm-based recaser,target side,bitext,language,newstest2012,velopment data,german recaser,ac curate,french recaser,accurate,5 t ranslation quality experiment,system development,newstest2008,sentence,iteration feature,newstest2012 newstest2013,en-fr  ble u-4 uncased result,tuning set,newstest2013,cased score, wmt organizer,iteration,tune newstest2012,en-de  ble u-4 uncased result,feature-rich model,baseline,en-de system parameter,200-best list,maximum phrase length,distortion limit,future cost estimation,en-fr system parameter,200-best list,maximum phrase length,distortion limit,online tuning algorithm,default,mini-batch size,regularization strength,dis criminative re-ordering model,dense feature,result table,en-de result,feature-rich,full complement,sparse fea tures,improvement,baseline,result,result,significant translation quality improvement,dense baseline,chinese english,multiple target refer ences, wmt data set,difference,example,translation rule,reference,sentence-level score,rule indicator feature,evidence,fig ure,devel opment,language pair,dense model converges,iteration,feature-rich model,separate experiment,held-out set,generalization,iteration,6 c onclusion,feature-rich mt system,sparse feature,measur able improvement,baseline dense feature,result,number,ref erences, nis tuning,test set,reference, wmt data set,sparse feature,setting,individual feature,fluence,large component,gradient,phenomenon,custom reg ularization strength,robust sentence level,improvement,model size,generalization problem,nev ertheless,feature-rich model,state-of-the-art,acknowledgment,program,opinion,finding,conclusion,recommendation,material,author, dar pa,u government,kinyon,treebank,french,instance selection,machine translation,feature decay algorithm,semi-supervised word cluster, 10e poch ble new test2,model densefeature, 10e poch ble new test2,model densefeature,en-de tuning figure,learning curve,newstest2008,phrasal,statistical machine translation toolkit,new model feature,singer,efficient online,batch learning,singer,adaptive sub gradient method,online learning,decoder,alignment,framework,context-free trans lation model,maximum entropy model,segmentation lattice,effective hierarchical phrase,distortion cost,statistical ma chine translation,marneffe,multiword expression,computational linguistics,adaptive online training,feature-rich trans lation model,scalable modified kneser-ney language model estimation,faster,lan guage model query,ranking,open source toolkit,statistical machine translation,alignment,agreement,method,automatic evaluation metric,machine translation,kambhatla,marcinkiewicz,santorini,large annotated corpus,en glish,penn treebank,computational linguis tic,sentence-level  ble,yield short translation,feature selection,rotational invariance,alignment template approach,statistical machine translation,compu tational linguistics,minimum error rate training,statis tical machine translation,interpretable tree annotation,syntactic lexicon,french