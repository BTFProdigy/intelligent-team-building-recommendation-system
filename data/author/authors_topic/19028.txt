proceeding,annual meeting,association,computational linguistics,baltimore,maryland,association,computational linguistics low-rank tensor,dependency structure tao lei,yu xin,yuan zhang,regina barzilay,tommi jaakkola computer science,artificial intelligence laboratory massachusetts institute,technology taolei,yuanzh,regina,tommi csail,abstract accurate scoring,syntactic structure,head-modifier arc,dependency,high dimensional feature representation,small subset,feature,feature,clear linguistic meaning,embeddings,information,feature,tensor,high-dimensional fea ture vector,low dimensional repre sentations,pa rameters,low-rank tensor,low dimensional representation,syntactic role,mod ularity,tensor,easy training,online algorithm,parser, mst parser,different language, uas result,language,1 i ntroduction,expressive representation,input sen tences,accurate parsing,syntac tic relation,broad range,surface indicator,feature,re search,direct connec tion,feature,predicted syntac tic relation,head-modifier,relation,dependency parsing,first order parser,result,high-dimensional vector representation,discrete fea tures,cross product,auxiliary information,word 1o ur code,github,continuous vector representation,dimen sionality,rich feature vector,difficulty,associated parameter,limited training data,predominant way,high dimen sionality,feature,meaningful set,feature template,different type,feature,mc donald,collins,reason,feature,clear linguistic interpretation,distribu tional feature,continuous vector embeddings,second,small subset,tem plate,feature,rel evant linguistic information,feature,instance,morphological proper tie,part-of-speech tag,turn relate,positional feature,feature,per formance loss,small subset,feature,rich feature,over-fitting problem,leverage high dimensional feature vector,low dimensional representation,high-dimensional feature vector,multi-way cross-products,feature vec tor,syntactic rela tions,parameter,tensor,multi-way array,low rank,performance,tensor,low-rank form,direct control,effective dimensionality,parameter,role-dependent low-dimensional representation,modifier,accuracy,standard online algorithm,low-rank tensor component,overall approach,computational advantage,ur low dimensional embeddings,syntactic context,low dimensional syntactic abstrac tion, pos tag,small number,dimension,wide array,feature,parser,word vec tor,feature,low-rank factorization model,context,firstand third-order depen dency,language,dependency data, con ll, con ll,result,martin,parser,low-rank parser achieves average performance,language,turbo parser,low-rank model,ab sence,part-of-speech tag,instance,english dataset,low-rank model,achieves,first-order parsing,baseline,condition,core  pos tag,word vec tor representation,contrast,baseline,work selecting feature,dependency,great deal,research,engineering,lazaridou,marton,marton,state-of-the-art parser,feature,ald et,collins, mcd onald,petrov,automatic feature selec tion method,popularity,martin,ballesteros,nilsson,nugues,ballesteros,stan dard machine learning practice,algorithm,subset,feature,performance,development set,feature selection method,promis ing,scenario,optimal feature set,small subset,original set,candidate feature,technique,contrast,relevant information,related feature,dependency,recent work,mapping word,vector space,collobert,weston,dhillon,mikolov,vector representation,co-occurrence,sentence,syntactic role,co-occurring word,word-level representation,herent sparsity problem,full lexi calization,word-level vector space embeddings,impact,performance,computational perspective,non sparse vector,feature,combination,num ber,active feature,syntactic struc tures,word vector,unigram feature,combination,parser,improvement,overall parsing perfor mance,application,word vector,compositional vector grammar,socher,method,word combination,vector,word-level vector representation,con trast,vector,manner,frame work,embeddings,word vector,feature,parsing performance,dimensionality reduction many machine,problem,matrix problem,matrix,co-varying parameter,problem,example,multi-task learning,collaborative filtering,parameter,others,parameter,low dimensional subspace,parameter,parameter matrix,low-rank assumption,low-rank constraint,generalization,srebro,srebro,evgeniou,pontil,strict low-rank assumption,recent approach,problem,parameter matrix,low rank,sparse matrix,matrix,small number,parameter,low-dimensional subspace,chandrasekaran,composition,parameter matrix,tensor,tensor,multi-way generalization,ma trice,analogous notion,tensor,spec tral estimation,kakade,includ ing, nlp problem,local optimum,maximum likelihood estimation,contrast,feature,multi-way tensor,operate,explicit low-rank representation,associ,parameter tensor,explicit representa tion,inherent complexity problem,tensor rank,hillar,sparse,turbo parser feature,low-rank tensor,roblem formulation,first-order,pendency,tensor estimation problem,notation,formal description,dependency,3-dimensional tensor,element,tensor,shorthand,integer,ele ments,matrix,vector,inner product,tensor,matrix,tensor,matrix,ele ments,column vector,squared norm,tensor matrix,kronecker product,vector,rank-1 tensor,vector,column,row vector,orientation,example,rank-1 matrix uv,column vector,row vector,tensor,kruskal form,th row,model parameter,dependency parsing let,sentence,possible dependency tree,head-modifier dependency arc,col lection,index word,word cor,dependence,whenever,context,complicated way,predicted parse,key problem,rich feature,head-modifier arc,sentence,chosen arc,depen dence,feature representation,2n ote,high-order parsing,local score,syntactic structure,martin,complete list,structure,unigram feature,form-p form-n lemma lemma-p lemma-n po pos-p pos-n morph bias bigram feature,pos-n po,lemma morph,lemma trigram feature,word feature template,morph stand,fine  pos tag,word form,word lemma,morphology feature, con ll,current word,bias term,suffix,current word re,example, pos tag,current word,number,pa rameters,possible feature,arc feature,rank-1 tensor,kronecker product,simpler feature vector,vector,modifier,vector,dimensional,ear lier,example,indicator,binned arc length,feature,cross-product,component feature vector,full fea ture representation,rank-1 ten sor,element,rank-1 tensor,feature combination,fea ture crossing,rank-1 tensor,substantial feature expansion,current version,binned arc length,possible feature,example, pos tag,modifier,boolean flag,occurence,in-between punctutations,conjunction,tensor representation,adjustable parametersa,ten sor,typical dimension,compo nent feature vector,parameter,full english training set, con ll-2008,tensor,mst feature vector,feature,feature explosion,parameter,low rank,low-rank dependency scoring,rank-r tensor,pa rameter matrix,result,arc score,tensor,dimensional vector,sparse vector,parameter,function,dependency parsing,context dependent embeddings,specif,sentence,dimensional vector representation,head word,analogous representation,vector,supplemental arc-dependent informa tion,embedding,syntactic role,dependency,several aspect,low-rank tensor scoring,example,rate additional useful feature,feature vec tor,low-rank assump tion,uncontrolled feature expansion,amount,information,component fea ture vector,statistical estimation problem,di mensions,low-rank constraint,unseen arc,distance,sentence,available train ing data,contrast,low-rank con straint,arc score,model aim,strength,traditional feature, mst turbo parser,new low-rank tensor feature,wide range,information,auxiliary feature,feature explosion,full accessibility,fea tures,model parameter,hyper-parameters,training,consists,sentence,corresponding gold,target,parameter,performance,maximum soft-margin framework,learning problem,parameter,number,mismatched arc,non-negative slack variable,constraint,gold tree,alternative,margin,distance,respect,explicit representation,low-rank tensor,parameter,example,linear function,re sult,objective,re spect,standard tool,learning,online,passive-aggressive learning algorithm,crammer,setting,parameter set,alternating manner,method,online learning,online,parameter,sentence,passive-aggressive algorithm,ex ample,alternating manner,closed-form update,pa rameters,objective function,respect,similar form,original passive-aggressive algorithm,training sentence,update involves,constraint,parameter increment,optimization problem attempt,parameter change,single instance,problem,hadamard,element wise,product,magnitude,change,parameterc,varyingc,appropriate step size,online,update,effect, mst component,score relative,low-rank tensor score,arc score,low rank tensor,therefore du,initialization,online algorithm relies,update,context,random initialization,parameter,dimension,nature,alternating update,reasonable determinis,initialization method,low-rank parameter,parameter,majority,feature, mst component,element,create,tensor representation,corresponding parameter value,low-rank version,ini tialization,tensor,dimension,instance,rank-1 tensor,top-r  svd,unfolded matrix,right singular vector iq,matrix,right singular vector,matrix,implementation,low-rank parameter,tensor,parameter,passive-aggressive algorithm,increment,overall regularization,large parameter val ues,over-fitting,effect,parameter averaging,turbo parser,final parameter,collins,simplicity,algorithm,5 e xperimental setup datasets,dependency model,language,english dataset,con ll,datasets,con ll,buchholz,surdeanu,dependency tree,morphological information,standard practice,information,feature,method,turbo parser,non-projective dependency par,parser,first-order parsing model,section,third-order model,third order parser,high-order feature,turbo parser,component,decoding algorithm,third-order parsing,turbo parser,recent pub,result,martin,mst parser,re cent version,addition,additional baseline,first order,nt-3rd,third order,tensor component,feature,feature template,head modifier vector,complete set,feature template,similar set,feature template,order parsing,auxiliary word vector representation,available word vector,sourceforge,net project,first-order,high-order nt-1st  mst turbo ours-3rd nt-3rd  mst,turbo-3rd best published arabic,bulgarian,chinese,danish,english,german,portuguese,slovene,spanish,swedish,turkish,average,first-order parsing,high-order parsing,result, con ll-2006 datasets,english dataset, con ll-2008,experiment,parameter,tensor,experiment,nt-1st,nt-3rd,last column show result,accurate parser,petrov, mcd onald,raw data,word vector,collection,dimensionality,representa tion varies,language,english,dimen sional word vector,dimensional word vector,word vector,feature value,feature vector,sentence,word vector,vector,right word,model parameter,low-rank form,low-rank projection,feature tensor,parameter,sentence,num ber,feature vector,contrast,cross-product,auxiliary word vector value,lexical item,context,crossed val ues,normal model,number,feature,thousand,significant impediment,efficiency,evaluation,standard practice,full model,baseline,github,com wolet sprml13-word-embeddings,evaluation measure,punc tuation,experiment,hyper parameter,first-order model,third-order model,6 r esults overall performance table,per formance,baseline,con ll datasets,variant,tensor component,improvement,low-rank model,lan guages,result,first order parser,language,nt-3rd,full model,low-rank,abso lute improvement,first-order parsing,improvement,third-order parsing,language,next focus,first-order model,impact,tensor component,hyper-parameter,tensor score,tradi tional  mst turbo score component,figure,average  uas, con ll test datasets,training epoch,im provement,low-rank tensor,various choice,hyper parame,figure,average  uas, con ll testsets,different epoch,full model,nt-1st,variation,tensor component,different choice,hyper-parameter,word vector,result,unsupervised word vec tor,tensor,information,consistent improvement,language,tensor component,learning,tensor,function,convex,respect,tensor,component achieves,generalization,test data,training epoch,ability,feature,unsupervised word vector,previous section,dif ferent coordinate,word vector,information,parsing performance,language,instance,absolute improvement,swedish,syntactic abstraction,compressed representation,fea ture vector,per formance,part-of-speech tag,rationale,feature,representa tions,similar role,column show,re sults, pos tag,last column,performance,parser,core  pos tag,low-rank model outperforms,large margin,adding word vector,ther improves performance,performance,traditional parser drop,example,perfor mance gap,experiment,low-rank parser,absence,performance,original parser,english,example,derived projection,analyze low-dimensional projection,syntactic abstraction,purpose,ten sor component,accu rate tensor,english dataset,low dimensional embeddings,r-dimension vector,vector,vector,cosine similarity,show example,neighbor,queried word,neighbor,hibit similar syntactic behavior,example,preposition,impact,syntactic context,derived projection,bottom part,neighbor,syntactic role,example,crease,context phrase,crease,different phrase,time table,impact,low-rank tensor parameter,run ning time,algorithm,comparison,nt-1st time,typical language,arabic dataset,av erage sentence length,chinese dataset,actively earnings,predicts,outright revenue,member,increase,gain advance,payment halt,member exchequer,subsidiary,attack,distributes monopoly,stayed pill,sang sophistication,removed venture,eased factor,neighbor,queried word,upper part,learned embeddings group word,similar syn tactic behavior,bottom part,table demonstrate,projection,syntactic context,arabic,comparison,typical datasets,second column,number,data set,third column,average sentence length,first-order mod el,single process,sentence length, con ll,result,rank-50 tensor, mst parameter,running time,factor,7 c onclusions,scoring,syntactic structure,head-modifier arc,dependency,high-dimensional feature rep resentations,low-rank factoriza tion method,high dimensional feature vector,low dimensional representa tions,method,parameter,low-rank tensor,low dimensional repre sentations,syntactic role,modularity,tensor,easy train,online algorithm,approach,first-order,third-order dependency,parser,mst parser,language,future work,tensor com ponent,higher-order structure,second-order structure,grandparent-head-modifier,dimensionality,tensor,tensor,five-way array,online update algorithm,di mension,alternating fashion,author,support,program,w911nf-10-1-0533, dar pa bol program,research,col laboration, lya project,volkan cirik,unsupervised word vector data,thanks,amir globerson,andreea gane,member, mit  nlp group, acl reviewer,suggestion,com ments,opinion,finding,conclusion,recommendation,author,funding organization,reference miguel ballesteros,joakim nivre,mal -to ptimizer,optimization tool,maltparser,association,computer linguistics,miguel ballesteros,effective morpholog ical feature selection,maltoptimizer,spm rl,proceeding,fourth workshop,statistical parsing,morphologically-rich language,association,computational linguistics,sabine buchholz,erwin marsi,ll-x shared task,multilingual dependency,proceeding,tenth conference,computa tional natural language learning,computational linguistics,venkat chandrasekaran,sujay sanghavi,pablo  a p ar rilo,alan  s w illsky,rank-sparsity,coherence,optimization,volkan cirik,ai-ku system, spm rl,feature,dependency parsing,proceed ings,fourth workshop,statistical parsing,morphologically-rich language,association,computational linguistics,shay  b c ohen,karl stratos,michael collins,dean  pf oster,lyle ungar,spectral learning,latent-variable  pcf g,proceeding,50th annual meeting,association,computa tional linguistics,papers-volume,associ ation,computational linguistics,michael collins,discriminative training meth od,hidden markov model,theory,exper iments,perceptron algorithm,proceeding,conference,empirical method,sociation,weston,unified architec ture,natural language processing,deep neural network,multitask learning,international conference,koby crammer,ofer dekel,joseph keshet,shai shalev-shwartz,yoram singer,passive-aggressive algorithm,journal,ma chine learning research,tim van,thierry poibeau,anna korho nen,tensor-based factorization model,semantic compositionality, hlt -naa cl,sociation,computational linguistics,paramveer,dhillon,dean foster,lyle ungar,multiview learning,word embeddings,advance,neural information process,system,vgeniou,massimiliano pontil,multi task feature,advance,neural infor mation,system,proceeding,conference, mit press,amir globerson,gal chechik,fernando pereira,naftali tishby,euclidean embedding,co occurrence data,journal,machine learning re search,christopher hillar,lek-heng lim,tensor problem,np-hard,iv preprint arx iv,daniel hsu,sham  m k akade,mix tures,spherical gaussians,moment method,spectral decomposition,proceeding,conference,innovation,terry koo,michael collins,efficient third order dependency parser,proceeding,annual meeting,association,com putational linguistics,terry koo,alexander  m r ush,michael collins,tommi jaakkola,david sontag,dual decomposition,non-projective head automaton,proceeding,conference,empirical method,natural language process,association,computational linguistics,angeliki lazaridou,eva maria vecchi,marco baroni,fish transporter,miracle home,compositional distributional semantics,proceeding,conference,empirical method,natural lan guage processing,association,computational linguistics,daniel  d l ee, h s ebastian seung,object,non-negative matrix factor ization,nature,yariv maron,michael lamar,elie bienenstock,application,part of-speech induction,advance,neural infor mation,system,noah  a s mith,eric  p x ing,mq aguiar,ario figueiredo,parser,dependency parsing,approximate variational inference,proceeding,conference,empirical method,natural lan guage processing,association,computational linguistics,dual decomposition,component,proceeding,conference,empirical method,computational linguistics,noah  a s mith,pedro mq aguiar,ario figueiredo,spar sity,structured prediction,proceeding,conference,empirical method,natural lan guage processing,association,computational linguistics,miguel  b a lmeida,noah  as mith,fast third-order non-projective turbo parser,proceeding,annual meeting,association,compu tational linguistics,association,computational linguistics,yuval marton,nizar habash,owen rambow,arabic dependency,inflectional morphological feature,proceeding, naa cl  hlt,first work shop,statistical parsing,computa tional linguistics,yuval marton,nizar habash,owen rambow,arabic dependency,functional morphological feature,proceeding,annual meeting,association,computational linguistics,human language technology,association,computa tional linguistics,ryan  mcd onald,koby crammer,fernando pereira,large-margin training,pendency parser,proceeding,nual meeting,association,ryan  mcd onald,fernando pereira,kiril ribarov,jan haji,non-projective dependency par,tree algorithm,proceeding,conference,human language technology,empirical method,natural language pro,association,computational linguistics,ryan  mcd onald,kevin lerman,fernando pereira,multilingual dependency analysis,two-stage discriminative parser,proceeding,tenth conference,computational natural language learning,association,computational linguistics,tomas mikolov,kai chen,greg corrado,jeffrey dean,efficient estimation,word represen tations,vector space,peter nilsson,pierre nugues,automatic discovery,feature set,dependency parsing,proceeding,international conference,computational linguistics,organizing committee,joakim nivre,johan hall,eryiit,svetoslav marinov,pseudo projective dependency,support vector machine,proceeding,tenth conference,computational natural language learning,sociation,computational linguistics,joakim nivre,johan hall,jens nilsson,sandra,svetoslav marinov,erwin marsi,maltparser,language-independent system,data-driven depen dency,natural language engineering,alexander rush,slav petrov,efficient multi-pass dependency,conference,north american chapter,association,computational linguistics,alexander  m r ush,slav petrov,efficient multi-pass dependency,proceeding,conference,north american chapter,association,computa tional linguistics,human language technology,association,computational linguistics,richard socher,john bauer,christopher,manning,andrew,compo sitional vector grammar,proceeding,annual meeting,association,compu tational linguistics,nathan srebro,tommi jaakkola,low-rank approximation,nathan srebro,jason rennie,tommi  s j aakkola,maximum-margin matrix factorization,advance,neural information,system,mihai surdeanu,richard johansson,adam meyers,arquez,joakim nivre,con ll-2008,joint parsing,semantic dependency,proceeding,twelfth conference,computational natu ral language learning,computational linguistics,min tao,xiaoming yuan,low rank,sparse component,matrix,noisy observation,journal,optimization,joseph turian,lev ratinov,yoshua bengio,word representation,simple,general method,semi-supervised learning,proceeding,annual meeting,association,com putational linguistics,andrew  e w aters,aswin  c s ankaranarayanan,richard baraniuk,sparcs,low rank,sparse matrix,compressive mea surements,advance,neural information pro,system,hao zhang,ryan  mcd onald,higher-order dependency,cube prun,proceeding,joint conference,empirical method,natural language process ing,computational natural language learn,compu tational linguistics,hao zhang,ryan  mcd onald,higher-order dependency,cube prun,proceeding,joint conference,empirical method,natural language process ing,computational natural language learning,association,computational linguistics,hao zhang,liang huang kai zhao,ryan  mcd,online,inexact hypergraph search,proceeding, emn lp,yuan zhang,tao lei,regina barzilay,tommi jaakkola,amir globerson,cellence,simple inference,refined scoring,dependency tree,proceeding,nual meeting,association,computational linguistics,association,computational linguis tic,tianyi zhou,dacheng tao,low-rank sparse matrix decomposition,noisy case,proceeding,international conference