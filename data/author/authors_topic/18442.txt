proceeding,conference,empirical method,natural language processing,seattle,washington,18-21 october,association,computational linguistics,efficient language model using double-array structure makoto yasuhara toru tanaka,jun-ya norimatsu mikio yamamoto department,computer science university,tsukuba,corpus size,con sume considerable resource,efficient method,ngram model,double array structure,method,backwards suffix tree,double-array structure,ef ficiency,optimization method,efficiency,data representation,double-array structure,probability,unused space,double-array structure,model size,word id,language model,method,large language model,division method,method,method,viewpoint,model size,query speed,optimization method,1 i ntroduction ngram language model,jelinek,probabilistic model,sentence,natural language processing,wide use,internet,dramatic increase,available corpus,significant improvement,model quality,performance,statistical machine translation sys tems,increas,training corpus,language model,corpus,resource,recent year,many method,efficiency,language model,problem,kenneth heafield,required memory size,double-array structure,ouble-array structure,text processing,compact representation,fredkin,fast tran sitions,trie node,ability,manipulate try,performance,language model,query speed,model size,memory,common representation,data structure,language mod el,double-array structure,language model,compactness,double-array structure,language model,probability,backoff weight,optimization method,embedding,method,model size,query speed,embedding,ef ficient method,ngram probability,backoff weight,whereby,vacant space,double-array language model structure,language model information,probability,backoff weight,ordering,method,word id,word id,model size,optimization method,experiment,language model,corpus, ntc ir patent retrieval task,atsushi fujii,atsushi fujii,atsushi fujii,makoto iwayama, arp file format,experiment,query speed,model size,result,optimization meth od,state-of the-art method,backwards suffix tree try,fredkin,tree structure,ngram language model,memory requirement,common prefix,query speed,number,input word,query speed,ngram model increase,backwards suffix tree,stolcke,germann,efficient representation,language mod el,ngrams,reverse order,history word,figure,example,backwards suf fix tree representation,example,word list,rectangular ta bles,target word,ngrams,tree denote history word,target word,history word,reverse order,trace history word,reverse,target word,example,trigram,history,reverse order,backwards,unknown ngrams,backwards,tree figure,example,backwards suffix tree,branch type,backwards suffix tree,history word,target word,history word,cir cles,target word,word list,representation,backoff cal culation,example,manner,tri gram,behavior,stolcke,language model toolkit,utilizes backwards,data structure, sri lm,64-bit pointer link,mem ory,access speed,ngram probability,efficient language model,recent year,several method,language model,memory,talbot,osborne,effi cient method,bloom filter,method,bloom filter,count information,prior work,bloom filter,certain data,count informa tion,language model,probability,probability,talbot,method,perfect hash function,filter,method,perfect hash func tions,ngrams,encode value,probability,ngrams,training corpus,large array,guthrie,hepple,language model,sheflm,minimal perfect hash function,belazzougui,ngrams,vacant space,sim ple dense coding,fredriksson,nikitin,high compression ratio,ngrams,training corpus,method store probability,ngrams,advantage,compression,floating-point number,generally,compression,floating-point number,probability values1,method,lossy language model,model size,expense,model qual ity,method,model perfor mance,berke leylm,implicit encoding struc ture,ngrams,sorted ngrams,trie structure,efficient method,variable length coding,block compression,small model size,query speed,addition,heafield,efficient language model toolkit,machine translation system,large language model,different main structure type,probing,trie structure,probing structure,language model struc ture,double-array structure,section,double-array structure,compact representation,several technique,performance,double-array structure,per spective,query speed,experience,weight,probability,floating-point number,knowl edge,method,double-array structure,language model,double-array structure,trie representation,parallel array,compact way,structure,several efficient language model representation technique,section,addi tion,construction algorithm,double-arrays,section,section,naive implementation, wor did ,function,word id,number,argument word,th slot,nth row, nex array,node number,node number,memory,used memory,result,mem ory space,double-array structure,problem,advantage,sparseness,nex array,two-dimensional array  nex,one-dimensional array  bas, nex array,result,memory,se rialization,naive implementation, a c heck array,transition,information,par ticular slot, a c heck array,transition error,child node,chosen node,definition,node link,node n,next node nnext,figure,corresponding double-array struc ture,node n,node n,array structure,trie tran sition,node n,ac cording,following step,step  1 c,destination,step  2 c,transition,transition,node n,figure,example,transition,parent node n,existence,ngram history,transition,leaf node,fragment,total ngram history,endmarker symbol,ngram history,end marker symbol,last node,endmarker symbol,symbol,double-array structure, che ck array,byte array variable,structure,greedy insertion,trie element,double-array, bas value,store node,high filling rate,node transition,addition,comparison,transition,double-array structure,greedy construction greedy algorithm,construct,static double-array structures2,construction step,root node,double-array structure, bas value,double-array structure,practice, bas value,position, bas value,figure,example,construc tion,example,node position, bas value,original source,tech nique,method,double-array implementation,con struction step,isfies,condition,efficient construction algorithm,construction time,double-array structure,challenge,effi cient method,nakamura,mochizuki,naive method,double array structure,naive method,long time,method,empty doubly-linked list,algorithm,efficient con struction method,figure,example,empty doubly-linked list, bas value,thecheck array,next empty slot,example, bas value,first child node,position,position,nakamura,mochizuki,computational cost,node insertion,naive method,original naive method,node insertion,number,unique word type,number,algorithm,empty double-linked list,number,unused slot,section,efficient method,several week,large language model structure,method,method,figure,empty doubly-linked list,unused  che ck slot,next unused slot, bas slot,previous unused slot, che ck array,doubly-linked list,number,inef fective trial,method,section,application,double-array structure,backwards,basic structure,double-array structure,simple structure,performance,section,backwards,double array structure,branch,target word,history node,double array structure,branch type,target word,history word,branch discrimination,prior work,endmarker symbol,language model,backwards suffix tree,ngrams,endmarker symbol,end-of-history word,target word,ngrams,chil dren,endmarker,endmarker symbol,target word,ordinary node,figure,example,construction,tinguish target word,history word,back ward suffix tree,exam ple,query trigram,figure,trigram,figure,example,backwards suffix tree,endmarker symbol,branch type,ordinary trie,double-array structure,advantage,tree structure,original backwards suffix tree,procedure,prob ability,backoff weight, ava lue array,probability,backoff weight,ngrams,figure,simple  dal structure,backwards,tree store,weight,probability,target word,simple  dal,respective position,correspond,embedding embedding,method,model size,simple  dal structure,many va cant space, che ck array,vacant space,backoff weight,probability,figure,show vacant space,simple  dal structure, bas array slot,target word node,target word,leaf position,backwards,child node,example,figure,probability value,method,model size,probability, bas array,result, val ue array figure,simple  dal data structure, che ck array,double-array structure,probability,backoff weight, a v alue array,figure,unused slot,simple  dal structure,information,probability,backoff weight, che ck array slot,endmarker sym bols,endmarker symbol transition,target word,endmarker symbol tran sitions, che ck array slot,endmarker symbol,backoff weight,false positive,backoff weight,po sitions,backoff weight, val ue array,negative number,unknown ngram encounter,endmarker symbol node, che ck array, val ue array,memory requirement,figure,example,embeddingmethod,figure,implementation,method,vacant space, val ue array,probability,backoff weight,backoff weight,negative sign,false positive,backoff weight,val ue array, val ue array,ordering,method,double array structure,query speed,word id,un igram probability,preprocessing stage,reason,method,interpretation,double array construction,figure,figure,previous section,insertion problem,problem, bas value,parent node, bas value,problem,shift length,insertion array,insertion array,flag bit,po sitions,word id,child node,used array,flag bit,original slot,double-array structure,situ ation,shift length,problem, bas value,used array,double-array structure,insertion,figure,intuitive example,efficiency,method,word id,unigram proba bility,insertion array,figure,interpretation,double-array construction,insertion problem,double-array structure,finding problem,shift length,insertion array,double array structure,used array,beginning,insertion array form cluster,sertion,unordered insertion array,shift length,insertion,double-array structure result,performance,method,experiment,ngram model,large training corpus,specification,training data,publication,unexamined japanese patent application, ntc ir,patent retrieval task,atsushi fujii,atsushi fujii,atsushi fujii,makoto iwayama,period,example,word id,effi ciency,word id,insertion array,advance,length,ordered array,unordered one,result,double-array structure,unordered array,corpus,model specification,model corpus unique ngram size type type,mwords,5 g word, mt est,paragraph,background,example,method, ntc ir  7 p atent translation task,large training data set,mwords, 5 g word,mwords,access speed,ngram probability,server,experiment,view point,model size,program,result,second run,final performance,figure,comparison,non-tuned double-array structure,comparison,non-tuned double-array structure,simple,performance, dal m,language model,experiment,method,double array structure,query speed,mwords model,comparison, a d alm , 5 g word model,result,figure,model size,query speed,latter,balanced method,divided double-array structure building,double-array structure,long time,construction,double-array structure, 5 g word model,original double-array structure,section,efficient algo rithm,insertion,total number,inser tions,number,unused slot,ith insertion,proportionality con stant,building time,build time,original trie,several part,building part,origi nal trie,double-array structure,parallel,query result,divided try,ngram statistic,original trie,method,randomized lan guage model,talbot,difference,meth od,double-array structure,comparison,mwords model,optimization method,previous section,figure,model size,optimization,query speed,number,several double-array structure,undivided structure,result,increase,figure,trade-off relation,tween model size,query speed, 5 g word model,exper iments,environment, 5 g word,comparison,original double-array structure,number,double-array structure,double-array structure,efficient algorithm,section,original structure,efficient building algorithm,research,comparison,method,mwords, 5 g word mod el,meth od,kenneth heafield,stolcke,experiment,method,original trie,double-array structure,result,figure,result,mwords model,result, 5 g word model,experimental result,method,model size,difference, 5 g word version,comparison,mwords model,hash-based language model,ad vantage,higher-order ngrams,large language model,trie-based language model,higher-order ngrams,practical situation,comparison,method,lan guage model system,experiment,application,statistical machine translation,language model sys tems,probability,many unnatu ral ngrams,query speed,unnat ural ngrams,many backoffs,trie-based lm,queried ngram history,hash-based lm,truncated ngram history,6 c onclusion,method,language model,double-array structure,method  dal,method,embedding,ordering,embedding,method,empty space,ngram prob ability,backoff weight,ordering,method,word id,method,model size,query speed,optimization method,performance,division method,model structure,several part,construction,double-array structure,procedure result,slight increase,model size,double-array structure,compactness,original structure,building double array structure,bottleneck,model structure,high performance,future work,algorithm,double-array structure,state-of-the-art language model implementation method,ex periments,method,higher-order ngrams,acknowledgment,anonymous reviewer,many valu able comment, jsp skakenhi gr ant number,efficient digital search algorithm,software engineering,atsushi fujii,makoto iwayama,noriko kando,overview,patent retrieval task,ntc ir-4,atsushi fujii,makoto iwayama,noriko kando,overview,patent retrieval task, ntc ir,atsushi fujii,makoto iwayama,noriko kando,overview,patent retrieval task,ntc ir-6 workshop,djamal belazzougui,fabiano,botelho,martin di etzfelbinger,displace,compress,timothy,cleary,witten,compression,thorsten brant,jeffrey dean,large language model,machine translation,proceeding,joint conference, emn lp-conll,jelinek,self-organized language modeling,speech recognition,edward fredkin,trie memory,communication,kimmo fredriksson,fedor nikitin,simple compression code supporting random access,fast string matching,proceeding,ternational conference,experimental algorithm,springer-verlag,atsushi fujii,masao utiyama,mikio yamamoto,takehito utsuro,overview,patent trans lation task, ntc ir-7 workshop,ulrich germann,eric joanis,samuel larkin,packed try,fit large model,memory,load fast,proceed ings,workshop, set qa-nlp,david guthrie,mark hepple,memory,space efficient language model,constant time retrieval,proceeding,conference, emn lp,kenneth heafield,faster,smaller language model query,proceeding,sixth workshop,makoto iwayama,atsushi fujii,noriko kando,aki hiko takano,overview,patent retrieval task, ntc ir-3,yasumasa nakamura,hisatoshi mochizuki,fast computation,updating method, a d ictio nary,compression digital search tree,trans action,information processing society,adam paul,dan klein,faster,smaller n-g ram language model,proceeding,annual meeting, acl -hlt,extensible language mod,toolkit,seventh international conference,spoken language processing,david talbot,thorsten brant,language model,perfect hash function,pro ceedings,david talbot,mile osborne,bloom filter language model,tera-scale lm,proceeding,joint confer ence, emn lp-conll