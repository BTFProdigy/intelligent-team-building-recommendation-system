proceeding, naa cl  hlt,workshop,active learning,natural language processing,los angeles,california,association,computational linguistics domain adaptation meet active learning piyush rai,avishek saha,suresh venkatasubramanian school,computing,university,piyush,avishek,edu abstract,active learning,target,domain,infor mation,source,domain,algorithm,source domain data,possible initializer hypothesis,active learning,target domain,label complexity,variant,algorithm,domain divergence information,informative point,target domain,reduc tions,label complexity,experimental re sults,variety,datasets,effi cacy,method,1 i ntroduction acquiring,supervised learning model,many prob lem domain,active learning,informative example,several case,label complexity,number,queried label,learning,attempt,similar problem,target domain,domain adaptation,problem,source domain,supervised domain adaptation setting,large amount,source domain,large amount,unlabeled data,target domain,small bud get,target domain,information,usual domain adaptation sense,information,source domain,query label,target domain,possible classifier,target domain,classifier,inter-domain,formation,fixed budget,active learning setting,settle,several way,clas sifier,first approach,classifier,initializer,online,tive learning,target domain,variant,first approach,domain-separator hypothesis,target,source domain,basic setup,source,domain-adapted source,initializer,active learn ing,target domain,fixed budget,framework,possible classi -1f,instance,supervised classi fier,source data,unsupervised domain adaptation technique,blitzer,sugiyama,source domain,unlabeled data,source,target domain,lt act ivelearning ora cle,figure,block diagram,basic approach,stage-1,sugiyama,source,query ing label,target domain example,information,classifier,phase-1,2 o nline active learning,active learning phase,algorithm,cesa-bianchi,section,approach,completeness,algorithm,algorithm,weight vector  w0t,proceeds,example xi,proba bility bb ri,confidence,margin,current weight vector,pa rameter,large value,implies,large num ber,conservative sampling,small value,small number,example,algorithm,current weight vector,total number,algorithm,supervised domain adaptation setting,small budget,tar get domain,active learning,target domain,information,source domain,online active learning ap algorithm  1 c bgzin,round initialization,label yi,proach,cesa-bianchi,section,algorithm,possible classifier,target la,figure,initializer hypothesis,target domain,hy pothesis,online fashion,cesa-bianchi,amount, w0t v0,algorithm,algorithm,mistake bound,label complexity, cbg algorithm,presentation,version,intuitive argument,label com plexity,hypothesis v0,related source domain,sequence,hypothesis,produce,confidence margin,hypothe si,proba bility,number,queried label,nothing,4 u sing domain separator hypothesis,relatedness,source,target domain,algo rithm,section,source,target domain,target domain example,source domain example,il lustration,typical distribution sepa rator hypothesis,blitzer,source,target example,source,target domain,separator hypothesis,example,domain,domain,overlap,target domain example,acquiring la bel,example,account exam ples,target example,second algorithm,algorithm,algo rithm,distribution sepa rator hypothesis,source,target unlabeled example,preprocessing step,target example,algorithm,ds-aoda,domain-separator,target example,approach,number,queried label,section,illustrative diagram,distribution separator hypothesis wds,source data,tar get data,actual target hypothesis 5 e xperiments,section,empirical perfor mance,algorithm, aa lgorithm  2 d s-aoda input,distribution separator hypoth esis,number,round initialization,source side,label yi,number,baseline,meth od,brief description,state-of-the-art supervised domain adap tation method,la bel,method,active fash ion,description,active learning,target,randomly ini,initialized base hypothesis,respec,classifier,source,initializer,target data,initializer,experiment,instance,ap proach,sugiyama,domain adaptation step,unsupervised domain adaptation technique,approach,classification accuracy,budget,labeled target example,number,fixed pool,target example,accuracy,sda  uda,target data,fed a fr,zia l ze ro,cesa-bianchi et al,active learning,fixed label budget yes sia l so urce hypothesis,active learning yes aod a uda ,source hypothesis,active learning yes table,description,method,vanilla perceptron,base classifier,algorithm,experiment,data order permutation,datasets,empirical result,sent ment classification,blitzer,user review,product type,apparel,electron ic,kitchen,ama zon,data dimensionality,sentiment classification task,dataset,binary classification,review,sentiment dataset,several domain pair,a-distance,domain separation,ben-david,present,domain pair,experiment,corresponding domain divergence,a-distance,ben-david,compute thea-distance,finite sample,source,target domain,surrogate,true a-distance,proxy a-distance,manner,ben-david,linear classifier,source domain,target domain,unlabeled example,average per-instance hinge-loss,classifier,esti mate,proxy a-distance,separable distribution,distribution,general rule,high score,domain,proxya-distances,domain,first experiment,first ap proach,unadapted source,baseline,domain pair,sentiment,large distance,small distance,target budget,result,result,datasets,approach,baseline approach,state-of-the-art,domain adap tation algorithm,approach,latter case,impor tant,sensible initialization,label complexity result next,various algorithm,number,accuracy,complete pool,unlabeled example,target domain,approach,label complexity,mettarget budget hod,standard deviation table,classification accuracy,fixed target budget,mettarget budget hod,standard deviation table,classification accuracy,fixed target budget,tive learning,baseline,classification accuracy,non-zero hypothesis,similar number,algo rithms,classification accuracy,sensible initializing hypothesis,ds-aoda result finally,distribution separator hy pothesis,section,experimental result,main pair,approach,number,first approach  aod,information,domain separation,perceptible loss,classification accuracy,simi lar improvement,label complexity,distribution separator hypothesis,source,label acc,label ria,standard deviation table,accuracy,label complexity,full target training data,unlabeled pool,ac cu ra cy,aod ads,la bel uer,domain aod ads -aod afi gure,test accuracy,label complexity,result,unadapted source,classifer,ve instance,first stage,weight,instance weighting,accu rate,importance weight estimation,reason,unsupervised domain adaptation technique,work active learning,domain adaptation setting,little attention,interesting set ting,active learning,word sense disambiguation,domain adaptation,active learn,setting,pool-based whereas,stream ing,online,setting,second algo rithm,domain separator hypothesis,target example,source,combination,transfer,active learning,approach,re quirement,initial pool,labeled target domain data,in-domain classifier,in-domain classifier,transfer learning,setting,7 d iscussion,several interesting variant,ap proach,instance,hybrid oracle,source classifier v0,oracle,con fident,prediction,rel ative confidence,actual classifier,confidence measure,active learning,distri bution separator hypothesis,approach,sec tion,significant reduc tions,label-complexity,tuitive argument,amount,label-complexity,representation,domain adaptation,structural correspondence learning,emn lp,domain adaptation,bol lywood,boom-boxes,blender,domain adapta tion,sentiment classification,cesa-bianchi,claudio,zaniboni,worst-case analysis,selective sampling,active learning,word sense disambiguation,ladner,richard,generalization,active learning,machine learn ing,daniel,domain adaptation,statistical classifier,journal,artificial intelli gence research,easy domain adaptation,finkel,jenny rose,manning,christopher,hier archical bayesian domain adaptation, naa cl,com puter science technical report,university,wisconsin-madison,xiaoxiao,jiangtao,transfer domain knowledge,model selection,application,covariate shift adaptation,proceeding,workshop,domain adaptation,uppsala,sweden,association,computing university,abhishek kumar school,computing university,avishek saha school,computing university,edu abstract,extension,notion,augmented space,harness unlabeled data,target,transfer,infor mation,source,approach,adaptation,pre-processing step,supervised learner,experimental result,sequential labeling task,efficacy,method,1 i ntroduction,domain adaptation approach,sequential la,approach,source domain feature space,feature,preprocessing step,classifier,target,hence applies,source,target,domain adaptation,source,target,approach, eas yadapt,superior perfor mance,ap proaches,source,unlabeled data,target,unsupervised domain adaptation,prior work,supervised domain adaptation,learning,yadapt,algorithm,multi task,parameter,evgeniou,pontil,multi-task regularization,task parameter,mean parameter,deviation,base classifier,algorithm,standard svm dual optimization,framework,evgeniou,pontil,dredze,multi domain setting,prior work,semi-supervised approach,adaptation,lit erature,extraction,specific feature,available dataset,arnold,co hen,blitzer,domain adaptation,co-adaptation,combination,co-training,domain adaptation,approach,adaptation,semi-supervised em algorithm,domain adap tation,semi-supervised approach,label propagation method,domain adaptation,semi-supervised extension,domain adaptation,present extensive empirical result,meth od,use specific,datasets,particular base classifier,method,conjunction,base classifier,prime limitation,inca pability,unlabeled data,sim plicity,generality,semi-supervised setting,co-regularization,semi-supervised extension,approach,result,single pair,source,target domain,source setting,source,single target domain,co-regularizer,source-target pair,space constraint,detail,full version,instance space,label space,source,example,target,example,target,respect,target domain,lin ear,tech niques,non-linear hypothesis,empirical error,hypothesis,source,target labeling function,corresponding,shorthand,section,brief overview,eas yadapt,augmented space,single pair,source,augmented space,augmented feature map,source,target domain,vector,zero vector,dimension,first dimensional segment corresponds,commonality,source,target,second d-dimensional segment corresponds,source domain,last segment corresponds,target domain,source,target domain feature,feature map,augmented fea ture space,un derlying,classifier,ap pealing property, eas yadapt,augmented space,standard,learning approach,linear classifier,perceptrons,linear hypothesis,augmented space,work considers,technique,non-linear hypothesis,dimension,source,target-specific component,prediction,target data,incoming target feature,fea ture,good intuitive insight,simple algorithm work,practice,state-of-the-art algorithm,hypothesis,source domain,target domain,commonality,domain,source,target domain specific information,technique,multi-domain scenario,version,algorithm,previous section,eas yadapt algorithm,conjunction,drawback, eas yadapt,unlabeled target data,large quantity,practical problem,section,semi-supervised extension,algorithm,desirable classifier-agnostic prop erty,motivation,multi-view approach,semi-supervised learn,algorithm,sindhwani,different hypothesis,different view,unlabeled data,hypothesis,unlabeled sample,domain adaptation,source,target data,different distribution,source,tar get domain,similar form,regularization,unlabeled data,similar co-regularizer,approach,unlabeled data,improved empir ical result,domain adaptation task,technique applies,particular base clas,unlabeled data,semi-supervised extension,yadapt,source,target hypothesis,unlabeled data,algorithm,yadapt,linear hypothesis,hypothesis,com mon,source,sub-hypotheses,original space,section,source hypothesis w,target hypothesis wt,unlabeled data,predic tions,following condition,unlabeled data,every unlabeled sample,feature space,re sults,application,feature map,source,sample,target,unlabeled sample,figure,predicted value,unlabeled sample,warrant exposition,implementation specific,0l sl lt lt ut,lt ut figure,diagrammatic representation,feature augmentation,present,ea approach,implementation,section,implementation specific detail,detail,algorithm,classifier,augmented space initialize,training,training,output,dual form, svm optimiza tion function,ner product,feature,sample,la bel,many copy,binary classification,ev ery,unlabeled sample,learner attempt,prediction,unlabeled sample,figure,hinge loss,ef fective loss,unlabeled sample,4 e xperiments,section,formance,unlabeled data,experimental setup,sequence,following datasets,pubmed-pos,blitzer,dataset,domain, wsj portion,penn treebank,source domain,pubmed,target domain,loss function,unlabeled sample,unlabeled pubmed abstract,classifier,labeled  wsj,pubmed data,treebank-chunk data consists,following domain,standard  wsj domain, con ll, ati switchboard domain,brown corpus,brown corpus,subdomains,treebank chunk,shallow parsing task,penn treebank,treebank brown,treebank-chunk task,treebank-brown,brown corpus,single domain,present,summary,datasets,datasets,feature,lexical information,capitalization,prefix,suffix,membership,gazetteer,averaged perceptron classifier,megam framework,implemen tation,training sample size varies,amount,target data,total amount,source,target data,task dom tr de te ft pubmed,pos tgt,summary,datasets,column,domain,training,development,test data set,number,unique fea tures,training data,result,empirical performance,baseline,classifier,source la,classi fier,number,target,sample,number,source,sample,classifier,small amount,target,sam ples,one-tenth,amount,source la,sample,labeled sample,sou rceonly ,augmented feature space,input training,augmented feature space,input training,equal amount,unlabeled target data,approach,entire amount,available tar,test data,figure,learning curve,x-axis repre,number,sample,predictor,number,training sample,particular approach,corresponding number,labeled source,target sample,summa tion,source,target sample,number,sample,tgtonly-full tgtonly ea ea,number,sample,figure,test accuracy,amount,unlabeled target data,addition,equal amount,source target,number,improvement,amount,unlabeled target data,different value,different curve,error rate,figure,normal ea,unlabeled case,increase,number,sample,gap increase,unlabeled case result,labeled case,similar trend,data set,figure,ea performs,semi-supervised extension,unlabeled data,performance,improved accuracy,sequential labeling task,datasets,domain adaptation problem,semi-supervised domain adapta tion problem,uture work,feature,source,target space,augmented feature space,fea tures,source,target,term algorithm,feature sharing algo rithms,feature,algorithm,domain adaptation,preprocessing step,state-of-the-art technique,domain adaptation,simplicity,empirical success,algorithm,prior work,intuition,formal theoretical analysis,domain adaptation,prior work,maurer,multi-task reg ularization approach,evgeniou,pontil,cumu lative loss,multi-task,multi-domain,setting,adaptation,target domain,superior perfor mance,gener alization guarantee,interesting line,fu ture work,approach,feature,approach,co regularization,context,multiview learning,theoretical analysis,rosenberg,bartlett,sindhwani,rosenberg,technique,interest,approach,empirical performance,theoretical framework,reference andrew arnold,william,intra document structural frequency feature,domain adaptation,napa valley,california,john blitzer,ryan mcdonald,fernando pereira,adaptation,structural correspon dence learning,sydney,australia,wenyuan dai,gui-rong xue,qiang yang,naive bayes classifier,text classification,lm-bfgs optimization,logistic regression,august,frustratingly easy domain adap tation,prague,czech republic,mark dredze,alex kulesza,koby crammer,multi-domain learning,confidence,parameter combination,machine learn ing,tat-seng chua,adaptation,multiple source,auxiliary classifier,montreal,quebec,theodoros evgeniou,massimiliano pontil,multitask,andreas maurer,rademacher complexity,linear transformation class,pittsburgh,rosenberg,rademacher complexity,co-regularized kernel class,puerto rico,vikas sindhwani,rosenberg,multi-view learning,manifold co-regularization,helsinki,finland,vikas sindhwani,partha niyogi,mikhail belkin,co-regularization approach,multiple view, icm lwo rkshop,multiple view,germany,gokhan tur,co-adaptation,adaptive co-training,semi-supervised learning,taipei,taiwan,dikan xing,wenyuan dai,gui-rong xue,refinement,transfer learning,warsaw,poland