improved  crf,chinese language processing system, sig hanba keoff,xinhao wang,dianhai yu speech,hearing research center state key laboratory,machine perception,peking university,wangxh,zhangyaoz,abstract,system,system,sys tem,system,fourth international chinese language processing bakeoff,primary mod el, ner track,gram language model,system,ac count,level language information,performance,system,transformation,technique,post-processing,1 i ntroduction,open track,bakeoff,open  ner track,system,top level,several track,natu ral language processing track,excellent performance,test corpus,bakeoff,gen erative model,primary advantage,independence assump tions,multiple inter,feature,observation element,rela tion,basic unit,sequence,chinese character,level information,relationship,n-gram language model,system,word level language information,several pilot-experimental result,tagging error,pattern,error pattern,similar error, tbl post processor,system,addition,extra train ing data,people daily corpus,shiwen yu,transition rule,corpus,open track,remainder,fol low,scheme,system,section,section,evaluation result,system,conclu sion,section,2 w ord segmentation,w system,n-gram language model,post processing strategy,conditional random field conditional random field,statistical se quence labeling model,great success,natural language processing,chunking,fei sha,word segmentation,hai zhao,sixth  sig han wo rkshop,constraint,indepen dence assumption,natural language task,f model,observation,label sequence,observation,normalization term,fea ture function,clique,graphic,chinese character sequence,sentence, a c hinese character,label tag,position,hai zhao,character position,multi character word,single-character word tag,unigram feature template,current character,nth character,current character,basic bigram fea ture template,dependency,previous tag,multi-model integration,multi-model information,log-linear model,posterior probability,word sequence,char acter sequence,decision rule,parameter,standard approach,mini mum error rate training,machine transla tion, crf approach,special case,framework,following feature function,approach,logarithm,feature function,whole la bel sequence,character sequence,posterior prob ability,sub-sequence,n-gram language model,word information,log-linear model,feature function,dynamic programming search algorithm,efficient decoding,system,word lattice,word sequence,word lattice,decision rule,candidate word,word lattice,problem, oov word,unigram, oov word,minimal value,unigram score,language model,length, oov word,punishment factor,long  oov word,xin hao wang,post-processing strategy,division,combination rule,system,bakeoff,xinhao wang,post processing,system,sixth  sig han wo rkshop,chinese language processing,data transition,w open track,unique difference,closed track,additional training data,model refinement,simplified chinese track,additional training data,people daily cor pu,auto-extracted transition rule,process,heuristic strategy,con tains,raw people daily,system,closed track,cor pu,compare,result,people daily corpus,conflict pair,example,left phrase,people daily corpus segmentation guideline,right one,phrase,first set con,right phrase,second set,left phrase,second set,people daily cor pu,sentence,phrase,left side,first set,right one,transition rule,entity recognition,entity recognition track,character sequence,problem, ner sys tem,log-linear model,multi-model informa tion,error pattern,bl strategy,post-processing module,model description, ner track,log-linear model,logarithm,feature function,class-based n-gram lan guage model,label sequence,n-best tagging re sults,whole label sequence,character sequence,log-linear model,n-best tag,result, crf score,class-based n-gram language model score,chinese character,ten class,beginning,character,en tity,non-entity character,entity,person name,location name,organization name,basic feature,basic unigram feature,bigram transi tion,previous tag,class-based n-gram language model,class-based n-gram language model,character,single class,entity,single class,character sequence,label sequence,class sequence,sentence,class sequence,class-based gram language model,class se quence,analysis,experiment,tagging error,pattern,system,pattern,similar error,sixth  sig han wo rkshop,class sequence example transformation-based learning,symbolic ma chine,method,eric brill,main idea,transformation rule,tagging error,initial process,main procedure, tbl framework,initial state assignment,system,al lowable template,position window,name entity information,3-word window,combination consid,difference,result,system,similar error,part-of speech sequence,word sequence,system, ctb corpus,cor pora,limitation,resource,per formance, pos tag,previous word,feature,dynamic program ming strategy,closed track,feature,basic feature,combined feature,next word,current word,basic feature,anal ysis, oov word,last character,current word,length,cur rent word,effective feature,oov  pos,furthermore,long distance con straint word,current word,yan zhao,open track,nese parser,current word,feature,5 e xperiments,result,open ner track,n-gram language model,system,implementation, crf package1,taku kudo,maximum entropy toolkit2, sri lm toolkit,andreas stolcke,andreas stolcke,chinese word segmentation,closed track,bigram language model,training data,corpus,corresponding parameter,minimum error rate training approache,development data,development data,bakeoff,ten-fold cross val idation approach,pa rameter training,parameter,mean value,estimation,parameter,result,w system,closed track,word segmentation performance,different approach,closed track,open track,enough time,parameter estimation,new data,system,parameter,closed track,unique difference,chasen,org taku software,homepage,uk s0450736 maxent toolkit,sixth  sig han wo rkshop,chinese language processing track,extra training data,corpus,performance,chinese track,additional data,people daily corpus,transition strategy,tra ditional chinese track,additional data,training,early bake,system,ctb open track,training,early bakeoff,addi tional data,translated peo ple daily corpus,additional data,result,open w system,cit yu,word segmentation performance,different approach,result,system performance,parameter,useful parameter,closed track,bad role,open track,ad ditional training data,named entity recognition,closed  ner track,class-based tri gram language model,corpus,approach em,w track,parameter, ner system, tbl rule,five-fold cross val idation approach,post-processing procedure,report,result,closed ner system,experiment,method, tbl method,concurrent er rors,method,output result,output tag,cit yu,entity recognition f-value,different approach,closed track tbl,output probability,certain threshold, tbl result,training set,training data,development data,addition,corpus,chi nese treebank,training data, ctb open track,system,berke ley parser,slav petrov,long distance constraint word,per formance,method,corpus,performance,total-accuracy,different approach 6 c onclusion,system,bakeoff,ner system,log-linear model,tegrate  crf,language model,system,system integration approach, pos system,validity,addition,heuris tic strategy,additional train ing data,open w track,several post-processing strategy,system,sixth  sig han wo rkshop,chinese language processing reference jin kiat low,hwee tou ng,wenyuan guo,aximum entropy approach,chinese word seg mentation,proceeding,fourth  sig han wo rk shop,chinese language processing,jeju island,huihsin tseng,pichuan chang,galen andrew,daniel jurafsky,christopher manning,onditional random field word segmenter,sighan bakeoff,proceeding,fourth  sig han wo rkshop,chinese language processing,jeju island,hai zhao,chang-ning huang,improved chinese word segmentation system,conditional random field,proceeding,fifth sig han wo rkshop,chinese language processing,sydney,australia,wallach,conditional random field,introduction, cis tr ms-cis-04-21,huiming duan,specification,large-scale modern chinese corpus,proceeding, icm lp,urumqi,fei sha,fernando pereira,conditional random field,proceeding,ed monton,canada,franz josef och,hermann ney,discrimi native training,maximum entropy model,sta tistical machine translation,proceeding,40th annual meeting,association,franz josef och,minimum error rate train ing,statistical machine translation,proceeding,annual meeting,association,sapporo,xinhao wang,xiaojun lin,chinese word segmentation,maximum entropy,n-gram language model,fifth  sig han wo rkshop,chinese language pro,sydney,australia,eric brill,transformation-based error-driven learning,natural language processing,case study,part-of-speech tagging,computational lingusitics,xiaolong wang,bingquan liu,yi guan,fusion,trigger-pair feature,pos tagging,maximum entropy model,journal,computer research,development,andreas stolcke,proceeding,international conference,spoken language processing,denver,colorado,slav petrov,leon barrett,romain thibaux,dan klein,accurate,compact,inter pretable tree annotation,proceeding,ternational conference,computational linguistics,annual meeting,sydney,australia,sixth  sig han wo rkshop,chinese language processing proceeding,fifth  sig han wo rkshop,chinese language processing,sydney,association,computational linguistics chinese word segmentation,maximum entropy,n-gram language model wang xinhao,lin xiaojun,yu dianhai,tian hao,wu xihong national laboratory,machine perception,school,electronics engineering,computer science,peking university,wangxh,tianhao,abstract,chinese word seg mentation system,speech,hearing research group,na tional laboratory,university,third international chi nese word segmentation bakeoff,chinese character-based maximum entropy model,word segmentation task,classi fication task,system,linguistics information,n-gram language model,several post processing strate gy,open track,cor pora,system,evaluation,good performance,closed track,system,1 i ntroduction chinese word segmentation,core tech niques,chinese language processing,research interest,recent year,sev eral promising method,previous researcher,successful way,hwee tou ng,jin kiat low,chinese word segmentation task,classification problem,character,beginning,middle,multi character word,single-character word,emphasis,chinese character,debases,consideration,relationship,context word,several strategy,context word,relationship,linguistics information,system,n-gram language model,relationship,context word,desirable choice,system,scoring,analysis,preliminary experiment,combination ambiguity,division,combination strategy,sys tem,numeral word,number conjunction strategy,addition,long organization name problem,corpus,post processing strategy,organization name,remainder,fol low,section,system,detail,section,experiment,result,last section,conclusion,2 s ystem description,n-gram language model,several post processing strategy,system,detailed description,component,subsection,maximum entropy model,system,previous work,jin kiat low,hwee tou ng,word segmentation,4-classes learning process,beginning,middle,single-character word,following feature,jin kiat low,character,right position,open track,feature,external dictionary,current charac ter,punctuation,length,character,context,external dictionary,boundary tag,character,feature, a m model,output,character,regard,char acters,semiangle matrix,element wji,ma trix,ith charac ter,jth character,character,consequence,optimal segmentation re sults,overall score,dynamic programming algorithm,corresponding matrix,example,language model n-gram language model,method,natural language processing,context relation,system,bi gram model,path score,detail,bi gram,weight,word bound aries,approach,path score,following formula,jth character,last word,parameter,test set,international chi nese word segmentation bakeoff,vocabulary,bigram,unigram,unigram, oov word,minimal unigram value,length,word acting,punishment factor,long  oov word,post processing strategy,analysis,preliminary experiment,n-gram language model,several post processing strategy,final system,combination strategy,combination ambiguity issue,division,combination strategy,bigram,bigram,unigram,august,revolution,segmented word,training,bigram,august revolution,ap peares,character string,uni gram,training,bigram,subwords,example,economic system reform,instance,corresponding unigram,training,bigram,subwords,matrix,economic system,reform,exists,consequence,segment,numeral word,several word,instance,problem,numeral word processing strategy,strategy,arabic numeral,train ing,high frequency charac ters,num bers,training,numeral word issue,fol low,sentence,con joint word,numeral word,last char acter,former word,organization name,several word, msr corpus,system,corresponding strategy,problem,organiza tion name,training set,charac ters,prefix,frequency,child node,predefined threshold,frequency,current node,string,current node,prefix,child node,frequency,threshold,corresponding subtree,suffix,difference,character,lexical tree,successive word,2-5 word,following con ditions,number,full stop,suffix,appears,frequency,substring, oov word,multiple word,satisfy,condition,successive word,string,prefix,3 e xperiments,result,open track,corpus,corpus,corpus,system,system ii,system,maximum entropy toolkit,system,system,re gard,n-gram language model,post processing strategy,closed track,corpus,result,derived system,system  r p  f r oov rivia,effect,memodel,n-gram language model,post processing strategy,closed track,corpus,system ia,system ib,bigram lan guage model,system ic,division,combination strategy,numeral word,homepage,uk s0450736 maxent toolkit,processing strategy,system id,ganization name processing strategy,open track,external dictio nary,feature,external dictionary,source,chinese concept dictionary,stitute,computational linguistics,noun cyclopedia,word segmentation dictionary,institute,technology,chinese academy,sci ences,dictionary,insti tute,acoustic,dictionary,insti tute,computational linguistics,dictionary,big dictionary,dictionary,core dic tionary,following dictionary,word set,intersection,big dictionary,training data,training data,external dictionary,training data,effect,n-gram language model,strategy,open track,system io,basic feature,external dictionary,feature,derived system,system  r p  f r oov rivio,effect,memodel,n-gram language model,post processing strategy,open track,system ii,division,combination strategy,numeral word processing strategy,open track,cor pora  cki, cit yu,training set,test set,chinese word segmentation backoff,training,corpus  upu,cit yu,external dictionary,open track, msr a co rpus,official result,system ii, cit yu,corpus  r p  f r oov rivupuc,official result,system, upu cckip, cit yu, upu corpus,interesting observation,performance,open track,closed track,investigation,analy si lead,possible explanation,seg mentation standard,dictionary,external dictionary,differ ent, upu corpus,4 c onclusion,detailed description,several chi nese word segmentation system,n-gram language model,post processing strategy,closed track,integration,bi gram language model,recall ratio,performance,system,addition,strategy,combination ambiguity,numeral word,long organization name issue,evaluation result,valid ity,effectivity,approach,reference jin kiat low,hwee tou ng,wenyuan guo,maximum entropy approach,chinese word segmentation,preceedings,fourth sig han wo rkshop,chinese language process ing,hwee tou ng,jin kiat low,chinese part-of speech tagging,all-at-once,preceedings,conference,empirical method,zhang huaping,liu qun,chinese word rough segmentation, n-s hortest path method,journal,chinese informa tion processing,proceeding,conference,empirical method,october,association,computational linguistics improve statistical machine translation,context-sensitive bilingual semantic embedding model haiyang wu 1d axiang dong 1w ei,iaoguang hu 1d ianhai yu 1h ua wu 1h aifeng wang 1t,shangdi,street,beijing,china 2h,institute,technology,harbin,china wuhaiyang,dongdaxiang,huxiaoguang,yudianhai,wu hua,cn abstract,bilingual embedding,feature,spite bilingual embedding,success,contextual information,criti cal importance,translation quality,previous work,contextual information,memory-efficient model,bilingual embedding,source phrase,context,phrase,account,bilingual translation score,bilin gual embedding model,feature, smt system,experimental result,method,significant improvement,large-scale chinese-english translation task,1 i ntroduction,sys tem,translation,phrase,ambiguous meaning,example,phrase,jieguo,result,context,reason,problem,length,phrase pair,limitation,model size,training data,reason, smt system,contextual information,source sentence,phrase sense disam biguation,language model,target corpus,problem,context-sensitive bilingual semantic embedding,methodology,supervised model,phrase-pairs,source phrase,aligned target phrase,positive label,phrase,phrase table,negative label,different,previ ous work,bilingual embedding learning,framework,supervised model,contextual informa tion,source sentence,feature,phrase pair,weak label,bilingual seman tic embeddings,learning task,similarity,phrase pair,feature,learned model,phrase-based translation system,experimental result,system,baseline system, nis t08 chinese-english translation task,method,web dataset, ble im provement,baseline,work using vector,word meaning,essence,vector space model,capture word,syn tactic information,semantic similarity,distance,vector, vsm represent,vector,ture homonymy,polysemy,global document context,multiple word prototype,distinguishes,global context,joint training objective,research focus,representation,sin gle language,progress,representation,bilin gual word,bilingual word representation,peirsman,sumita,algorithm,boyd-graber,resnik,bilingual em bedding utilizes word alignment,monolin gual embeddings result,continuous vector,source language,target language,phrase,translation probability,vector distance,bilingual vector space,non-parallel data,seed lexicon,word sense disam biguation problem,carpuat,bilingual semantic embeddings,source content,target phrase,phrase, smt system,translation quality,ontext-sensitive bilingual semantic embedding model,memory-efficient model,contextual information,source phrase,aligned phrase,target cor pu,low dimension,assumption,high frequent word,multiple word,top frequent word,source corpus,selected word,focused phrase,bilingual embedding model,dis criminative contextual information,phrase,effective context sensi tive bilingual embedding,context fea tures,focused phrase,phrase,target translation,possible candidate,classification problem,target phrase,target phrase,high dimensional space,traditional linear classification model,problem,problem,ranking problem,large number,objective,scalable optimizer stochastic gradient descent,bilingual word embedding,linear embedding model,learning,cosine similarity,representation,score function,score function,target phrase,candidate phrase,score function,contextual feature vector,source sen tence,representation,target phrase,low rank ma trix,bag-of-words representation,memory efficient,dimensionality,practical setting,mean dimensionality,random variable,traditional linear model,max-entropy model,memory space,memory space,large scale vocabu lary setting,focused phrase,target phrase pair,nearby window,focused phrase,target word,phrase pair,source sentence,embedding,focused phrase,target phrase,context sensitive feature context,focused phrase,nearby window,experiment,window size,focused phrase,con text,feature,focused phrase,context,feature extraction,label generation process,chinese-to-english example,figure,window size,example,position feature,part-of-speech tagging feature,focused phrase,context,word fruit figure,feature extraction,label generation,aligned phrase,focused phrase,positive label,phrase result,phrase,phrase table result,feature window,beginning,sentence,problem,beginning,sentence,sentence,parameter learning,model parameter,ranking scheme,candidate,phrase table result,focused phrase,par ticular,focus phrase,phrase,positive label whereas phrase,candidate,phrase table,negative label,max-margin loss,max-margin hinge loss,implementation,margin,training,objective,stochastic gradient descent algorithm,training example,parameter,following form,instance,gradient,parameter,model param eters,syntactic informa tion,word vector,source,target corpus,word2vec,mikolov,pre-trained word vector,initial parameter,learned scoring function,feature,detail,bilingual semantic embedding,phrase-based  smt architecture,state-of-the-art phrase,translation model,decoding,context information,source phrase,phrase,source sen tence,following task,decoder,focused phrase,context,source sentence,xtract feature,focused phrase,context,et translation candidate,phrase pair,focused phrase,focused phrase,candidate phrase,target,word align ment,phrase,common target word,alignment,focused phrase,matching score,source content,target word,bilingual se,focus phrase,penalty value,target,translation candidate list,phrase, smt input sentence,bilingual seman tic score,additional feature,log-linear translation model,combination,typical context-independent  smt bilexicon probability,experiment,house phrase-based system,log-linear framework,system,phrase,lation model,n-gram language model,lexi calized reordering model,word penalty model,phrase penalty model,evaluation,papineni,data set,approach, ldc corpus,subset, nis top,method openmt08 webdata ble u bleu,result,lowercase  ble,location feature,part-of-speech feature,baseline,sentence pair,simple heuristic rule,sentence,messy code,mono lingual corpus,xinhua portion,english gigaword,monolingual corpus,ter sentence,messy code,mono lingual corpus,approach,realistic scenario,web data,sentence pair,bilingual website,com parable webpage,monolingual corpus,large website,50m sentence pair, 10b word monolingual corpus,result,analysis,word alignment,grow-diag-final heuristic,recall,language model,5-gram modified kneser-ney language model,minimum error rate training,openmt08 task,webdata task, nis t06,tuning set, nis t08,testing set,baseline system,standard phrase-based  smt system,language model,target side,bilingual corpus,result,chinese-english translation task,word position feature,part of-speech tagging feature,learning,translation score,bilingual phrase pair,bilingual embedding,contextual feature,source,different source sentence 4 n,neighbor,business,cial environment,titions,chinese dis,ex traordinary athletic abil ities,ability,ability,natu ral environment,costa rica,outcome,result,eastern district council,pro posal,result,result,focused phrase,bilingual semantic embedding word,part-of-speech tagging fea tures,target phrase,6 c onlusion,statistical machine translation,contextual infor mation,bilingual word sense disambiguation,bilingual semantic model,phrase-based  smt sys tem,experimental result,method,significant improvement,base line,large scale chinese-english translation task,industrial usage,training,large scale data set,large number,prediction time,regard,future,feature,contextual information,target sentence,acknowledgment,anonymous reviewer,valuable comment,niu gang,wu xianchao,discussion,reference jordan boyd-graber,philip resnik,holis tic sentiment analysis,language,multilin,latent dirichlet allocation,pro ceedings,conference,empirical meth od,natural language processing,october,association,compu tational linguistics,marine carpuat,sta tistical machine translation,word sense disam biguation,proceeding,joint con ference,empirical method,natural language processing,prague,czech republic,association,computa tional linguistics,jianfeng gao,li deng,continuous phrase rep resentations,translation modeling,eric huang,richard socher,christopher manning,word represen tations,global context,multiple word proto type,proceeding,50th annual meeting,association,computational linguistics,long paper,jeju island,association,computational linguis tic,ois yvon,continuous space translation model,neural network,proceeding,con ference,north american chapter,sociation,computational linguistics,human language technology,canada,association,computational lin guistics,tomas mikolov,ilya sutskever,kai chen,gregory,corrado,jeffrey dean,rep resentations,phrase,compo sitionality,franz josef och,hermann ney,sys tematic comparison,various statistical alignment model,computational linguistics,computational linguistics,franz josef och,minimum error rate train ing,statistical machine translation,proceed ings,annual meeting,association,computational linguistics,sap poro,association,computational linguistics,peirsman,sebastian pad,cross lingual induction,selectional preference,bilingual vector space,human language tech nologies,annual conference,north american chapter,association,compu tational linguistics,los ange le,california,association,computational linguistics,eiichiro sumita,lexical transfer,vector space model,proceeding,annual meeting,association,computational lin guistics,association,computational linguistics,august,yik-cheung tam,ian lane,tanja schultz,bilingual-lsa,lm adaptation,spoken lan guage translation,proceeding,nual meeting,association,computational linguistics,prague,czech repub lic,association,computational linguis tic,ivan vuli,marie-francine moens,cross lingual semantic similarity,similarity,semantic word response,proceeding,conference,north american chap ter,association,computational linguistics,human language technology,georgia,association,computational linguistics,bing zhao,bilingual topic admixture model,word alignment,pro ceedings,main confer ence poster session,sydney,au tralia,association,computational linguis tic,richard socher,daniel cer,christo,manning,bilingual word,phrase-based machine translation,pro ceedings,conference,empirical meth od,natural language processing,seattle,washington,october,associa tion,computational linguistics,proceeding, naa cl-hlt,atlanta,georgia,association,computational linguistics compound embedding feature,semi-supervised learning    m,tiejun zhao1,daxiang dong2,hao tian2,dianhai yu2 harbin institute,technology,harbin,china yumo,tjzhao mtlab,cn dongdaxiang,tianhao,com    a bstract,data sparsity problem,discriminative method,representation,lexical item,unlabeled data,feature,word representation,neural language model,word embeddings,direct us-age,disadvantage,large amount,computation,inadequacy,word ambiguity,rare-words,problem,linear non-separability,problem,compound feature,continuous word embeddings,experiment,com-pound,perfor-mances,several  nlp task,potential,embeddings,1 i ntroduction,method,great success,practice,method,problem,data sparsity, nlp task,situation,learning,easy-to-obtain unlabeled data,semi-supervised framework,learn word representa-tions,feature vector,lexical item,su-pervised system,method,large-scale unlabeled data,performance,system,variety,ck-str,development,hinton,researcher,word representation,word embed-dings,word embeddings,dense low dimensional real-valued vector,latent feature,seman-tic property,word embeddings,first layer,deep learning system,collobert,weston,socher,system,state-of-the-art model,hand-crafted feature,feature,state-of-the-art model,significant improvement,turian et al,direct usage,continuous embed-dings,effective method,state-of-the-art supervised mod-els,disadvantage,brown cluster fea-tures,turian et,embeddings,rare word,updated time,random initial value,turian et,main reason,feature,brown cluster feature,unique representation,different sens,ambiguous word,word embeddings,linear model,section,target label,combination,different dimension,word embeddings,discriminative information,differ-ent interval,dimension,treating embeddings,linear model,em-beddings,dense vector,large amount,computation,embeddings,disadvantage,high-dimensional cluster feature,sample,different class,linear model,feature,cluster,compound feature,compound feature,conjunctive feature,neighboring word, nlp model,performance,compound feature,embeddings,predict label,rare-words,ambiguous word,compound feature,embeddings,nearby word,property,compound feature,dense embeddings,sparse,cluster,   e xperiments,chunking,embeddings,compound fea-tures,performance,analysis,rea-sons,improvement,embedding-clusters,compound feature,feature,rare-words,word ambiguity,linear model,addition,con-sidered better,al2010,experi-ment result,comparison,compound,clustering,embeddings,deep learning,future,method,large space,performance growth,application,section,introduces,compound,feature,experimental result,section,analysis,advantage,com-pound feature,section,conclusion, 2 c lustering,learning word embeddings word,bengio,probability,context information,sentence,current word,n-1 word,n-gram model,em-beddings,embeddings,forward,network,current word,training data,embeddings,smoothing representation,clustering,embeddings,compound feature,embeddings,discrete cluster,embed-dings,k-means,algo-rithm,single sample,cluster,embeddings,similarity,cluster,eu-clidean distance,different number,k contain information,different granularity,result,different k,feature,embeddings,compound feature,cluster,powerful com-pound feature,compound feature,conjunction,basic feature,context,compound feature,brown cluster,im-provements,parsing,compound embed-ding feature,rare-words,ambiguous word,example,alt-hough embedding,rare-word,embeddings,context word,combination,embeddings,con-ducted analysis,theory,section4,compound,state-of-the-art human-craft feature,supervised model,example,resulted fea-ture template,feature,last row,example,compound feature,cluster,af-ter current word,compound feature extraction,compound feature,brown cluster,example,brown cluster,cluster,cccpwyy,chunking feature,cluster feature,brown cluster,cluster,sym-bol iy,iiiii www pre,cccpwyy,hyphen,first letter, 3 e xperiments,experimental setting,compound,brown cluster feature,comparison,feature template,cluster,em-beddings,turian et,problem,word embeddings,brown cluster,cluster,embeddings,turian et, rcv corpus, con ll2000 data, wsj corpus,word representation,similar domain,result,embeddings,brown cluster,unlabeled  wsj data,comparison,method,language,experiment,chinese  ner,large amount,training data exists,accuracy,people,penn  ctb,sentenc-es,others,sentence,chinese word representa-tions,chinese wikipedia,feature,chinese  ner,english,or-thography,pre suffix,feature,little pre-processing work,train-ing,word representation, wsj data,da-tasets,capital word,training,chinese wikipedia,article,fre-quencies,dataset,embeddings,dimension,7-gram model,brown cluster,method,schwenk,training,clustering,embeddings,combination,development,next section,sofia-ml toolkit,sculley,embeddings,experiment  crf model,comparison,direct usage,embeddings,turian et, crf suite,okazaki,feature,continuous value,performance,chunking result,result,turian et,word representation,compound,performance,feature,unla-beled  wsj data yield improvement,word representation,similar domain,supervised task,experiment,number,cluster,per-formances,development,figure,cluster feature,compound,feature,compound feature,result,direct usage,embeddings,performance,result,others,combination,result,development,  f igure,relation,number,cluster,performance,development,performance,baseline,turian et,first-order  crf,context information,ne tag,conclusion,embedding,brown embedding,english  ner,test data performance,chinese  ner,similar result,eng-lish  ner,method,oth-er language,embedding,brown embedding,chinese  ner,test data result,evidence,clus-tering embeddings,information,compound feature,perfor-mances,compound feature,performance,brown cluster,embeddings,combi-nation,embedding-clusters,brown-clusters,performance,different type,context information,feature,time cost,feature,example,sentence,english  ner,embeddings,compound,unning time,different feature,compound,feature,direct usage,embeddings,english  ner,section,anal-yses,reason,improvement,rare-words,ambiguous word,compound feature,abil-ities,rare word,number,different frequency,unlabeled data,word frequency,result,turian et al,compound,feature,direct usage,embed-dings,brown cluster,rare word,figure,compound feature,few-er error,baseline method,embeddings,frequency,compound,ambiguous word,figure,number,number,different  pos tag,penn treebank,am-biguous word,unambiguous one,figure,ambiguous word,linear separability,embeddings,reason,good performance,com-pound feature,linear model,embeddings,feature,experiment, con ll2003,clas-sification task,word be-longs,linear  svm,classifier,embeddings,feature,linear  svm,linear model,similar accuracy,training,de-velopment set,observation,linear model,non-nes well,embeddings,cluster,embeddings,fea-tures,accuracy,linear model,non-zero feature,sample,per-formances,information,clustering,accuracy,linear model,non-linear one,cluster,cluster feature,linear model,compound,linear model,non-linear one,extra context information,cluster,cluster,cluster,cluster,compound,non-linear model,development,feature, 5 c onclusion,perspective,clus-tering embeddings,com-pound feature,clustering,disadvantage,direct usage,continuous embeddings,experiment,compound,original word representation feature,embeddings,brown cluster,performance,analysis,com-pound feature,rare-words,ambiguous word,linear model,word embeddings,limitation,struc-tural information,language,future,research,task-specific representation,sub-structures,phrase,sub-trees,word embeddings,document representation,jie zhou,rui zhang,many discus-sions,anonymous reviewer,valuable suggestion,national natural science foundation,key project,national high technology research,development pro-gram,journal,natural language processing,deep neural network,multitask learning,proceeding,25th international conference,machine learning,non-local information,information extraction system,gibbs sampling,proceeding,annual meeting,association,computational linguistics,association,computational linguistics,sparsity,supervised sequence labeling,proceeding,joint conference,47th annual meeting,international joint conference,natural language processing, afn lp,volume  1-v olume,association,computational linguistics,association,computational linguistics,association,computational linguistics,language model,advance,neural information,system,dependency,proceeding, con ll shared task session, emn lp-conll,fast implementation,conditional random field,chokkan,software crfsuite,eb-scale k-means,proceeding,international conference,world wide web,unfolding recursive auto-encoders,paraphrase detection,advance,neural information processing system,emi-supervised recursive auto-encoders,sentiment distribution,proceeding,conference,empirical method,natural language processing,association,direct transfer,linguistic structure,proceeding,north american chapter,association,computational linguistics,human language technology,simple,general method,semi-supervised learning,annual meeting-association,computational linguistics,urbana,alternative text representation,tf-idf,bag-of-words,proceeding, acm conf,information,chinese treebank,phrase structure annotation,large corpus,natural language engineering,proceeding,annual meeting,association,computational linguistics,bulgaria,august,association,computational linguistics cross-lingual projection,language,different family mo yu1 tiejun zhao1 yalong bai1 hao tian2 dianhai yu2,computer science,technology,harbin institute,technology,harbin,china yumo,tjzhao,china tianhao,com abstract cross-lingual projection method,resource-rich language,prove performance, nlp task,resources-scarce language,method,difficulty,syntactic difference,language,language varies,projection,lan guages pair,projec tion method,word alignment,target-language word rep resentations,feature,novel noise,method,word representation,experiment,method,projection,english,chinese,ntroduction, nlp study,limited language,large set,example,resource-rich lan guages,lan guages, nlp task,resource-scarce language,cross-lingual projection method,resource,resource-rich language,source lan guage,resource-scarce lan guage,several type,projection method,intuitive,effective method,common feature space,language,language,language,di rect projection,popular re,main limitation,method,target language,source language,performance,phrase,tween source,target language,common type,projection method,resource-rich language sentence,resource-scarce one,parallel corpus,word alignment information,yarowsky,petrov,projection,word align ments,projection method,method,syntactic difference,language,target side,topology,target language,accurate projec tion method,strong generality,various pair,language,language,different family,projection method,word alignment,language difference,method,disadvantage,target side,parallel corpus,domain,performance,method,accuracy,word alignment,language,accurate,accurate projection method,strong generality,various pair,language,method,projec tion method,word alignment,advantage,syntactic difference,solution,difficulty,method,brown cluster,target language,projection model,brown clustering,word representation,similar function,cluster,large-scale unlabeled data,target language,parallel corpus,minor language,brown cluster,cross-lingual projec tions,great improvement,projection,euro pean language,direct projection method,language,different family,section,projection,method,method,brown cluster,assump tion,similar representation,brown cluster,similar label,research,word repre sentations,technique,projec tion method,different language pair,language,experiment,projection,english,chinese,ef fectiveness,method,section,cross-lingual projection method,evalu ations,section,section,remark,cross-lingual projection method,section,cross lingual projection method,word align ments,word represen tations,brown cluster,projec tion method,section,noise re,method,projection,word alignment,cross-lingual projec tion,word alignment,projection method,tween language pair,large difference,fig ure,procedure,cross-lingual projec tion method,projection,en glish,chinese,example,english,resource-rich language,chinese,tar get language,sentence,source side,parallel corpus,accurate model,gan luo,source language,rich resource,accurate  ner model,word alignment,parallel corpus,bridge,unlabeled word,target language,rongji,la bel,source language sentence,target sentence,example,projection,la bel,chinese sentence,bracket,source sentence,projection procedure,labeled dataset,target language,source sentence,projected dataset,large size,labeled dataset,target language,supervised way,sentence,target language,target language,projection approach,language difference,direct projection method,word representation feature,cross-lingual projection one disadvantage,method,cov erage,c1 transition,c1 table,cluster id,cover age,parallel corpus,example,figure,chinese politician,person name,recent politi cians,parallel corpus,coverage,word representation,feature,similar word representation,similar context,person name,word representation,large-scale unlabeled sentence,target language,parallel corpus,information,similar representation,parallel data,brown cluster,word rep resentations,target language,assigns word,hierarchical cluster,distribution,example,feature template,feature,cluster id,context word,con junction,cluster,feature,crf model,traditional word feature,brown cluster,cluster,binary string,prefix,cluster id,feature,cluster,small number,language,morphological change,pre suffix,orthography feature,cluster feature,language,noise removing,word representation space,disadvantage,projection method,accuracy,non-literate translation,word align ment error,data contain many noise,example,figure,entity,english,alignment error,accurate model,direct way,majority,parallel corpus,method,low frequency,appear ances,alignment er rors,situation,figure,example,occurrence,correct label,difficulty,brown cluster,observation,cluster,example,figure,luo gan,cluster,chinese politician,similar context,large portion,cluster,person name,cluster,combination,cluster,removing,context information,data instance,example,instance,bigram,cluster,target word,previous word,instance,cluster bigram,whole noise,method,following,target word wi,label yi,projection,prob ability,cluster ci,bigram,indicator function,practice,probability,method,dataset,3 e xperimental result,english,resource-rich language,resource-scarce lan guages,language,experiment,projection,pos tagging,resource-scarce language,training data, ner experiment,people,test data,sentence,evaluation,projection, pos tagging,test set,english,chinese,different annotation standard,language,universal pos tag set,petrov,petrov,source,tar get language,universal tag,fine-grained type,brown cluster,chinese wikipedia,article,cluster,algorithm,chinese word segmenta tion,english brown cluster,word cluster,tokenized en glish wikipedia,parallel corpus,sentence,word alignment,similar amount,parallel sentence,english,minor language,conclusion,problem,projection,real application,performance, ner projection table,performance, ner projec tion,direct projection method,cluster,method,european language pair,result,projection,word alignment,source,tar get language,different family,cluster,chinese wikipedia,section,great improvement,average f1 score,entity type,word representation,system,rec avg f1 direct projection,cluster,cluster,performance, ner projection,named entity,test set,per formances,category,entity,word repre sentation feature,improvement,person name,rea son,improvement,chinese,son name,single word,method,good word representa tions,entity,enti tie,person name,brown cluster,recall,source side,target side,word alignment,feature space,source language,target language,projected clus ters,feature,projection,word align ments,method,resource,experiment,cluster,english wikipedia,chinese word,perfor mance,result,direct pro jection method,resource,method,diverse language pair,effect,method,huge improvement,result,clus ter feature,experiment,effect,factor,result,method,cluster,improve ments,5-2 point,method,bigram feature,improvement,great improvement,person name,great proportion,vocabulary,person name,cluster,common noun,method,cluster,name entity,cluster bigram,context information,discrimination,mixed cluster,system  per  loc  org  avg,cluster,bigram,performance,method, pos projection,section,method,projection,english,chinese,method, nlp task,single word,target word,different pos tag,source side, pos tag,1-to-1 alignment,feature template,experiment, pos tagger,result,great difference,english,chinese,projection,word alignment,direct projection,word cluster feature,error re duction,projection,method, ner projection,improvement,cluster feature,baseline system,possible reason,meth od,single word, pos tagging,method accuracy direct projection,ckstro,projection,cluster,performance,projection,onclusion,perspective,brown cluster,target language,cross-lingual projection,method,experiment,technique,perfor mances,projection method,language,projection method,word alignment,syntactic dif ferences,topological difference,lan guages,importance reason,limitation,performance,cross-lingual projec tion,future,repre sentations,sub-structures,syntactic difference,complex task,projec tion,dependency,future improvement,direct projection,joint feature representation,method,multiple language,acknowledgment,anonymous review er,valuable comment,helpful sug gestions,national natural science foundation,key project,national high technol,research,development program,class-based n-gram mod el,natural language,computational linguistics,petrov,part-of speech,bilingual graph-based projec tions,proceeding,annual meeting,association,computational linguistics,hu man language technology,semi-supervised learning framework,cross-lingual projection,web intelligence,parser,syntactic projection,parallel text,natural language engineering,dependency,projection,word-pair classification,pro ceedings,annual meeting,associa tion,semi-supervised learning,natural language,thesis,massachusetts institute,petrov,multi source transfer,delexicalized dependency parser,proceeding,conference,empirical method,natural language processing,association,statis tical translation model, mcd onald,universal part-of-speech tagset,iv preprint arx iv,ckstro,uszkoreit,cross-lingual word cluster,direct transfer,lin guistic structure,ckstro, r m cdonald, j n ivre,language adaptation,discriminative transfer parser,proceeding,manning,conditional random field word segmenter,sighan bakeoff,pro ceedings,fourth  sig han wo rkshop,chi nese language processing,volume,jeju island,f x ia, w l ewis,multilingual struc tural projection,interlinear text,conference,palmer,penn chinese treebank,phrase structure annotation,large corpus,natural language engineering,wicentowski,multilingual text analysis tool,robust projection,corpus,proceeding,first international conference,human lan guage technology research,association,computational linguistics