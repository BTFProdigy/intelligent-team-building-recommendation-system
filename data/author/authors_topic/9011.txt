proceeding,human language technology conference,conference,empirical method,vancouver,october,association,computational linguistics minimum sample risk method,language modeling1   j ianfeng gao   m icrosoft research asia jfgao microsoft,fourth author,microsoft research asia,thanks,hisami suzuki,new discriminative training method,parameter,language model,text input,discriminative training method,loss function,objec tive,training error,heuristic training procedure,evaluation,japanese text input,large number,feature,train ing sample,regular trigram model,maxi mum likelihood estimation,discrimi native method,boosting,per ceptron algorithm,significant margin,1 i ntroduction language,wide range,application,speech recognition,asian language text input,jelinek,traditional approach,paramet ric model,method,data sparseness problem,approach,assumption,true distribution,parametric model,assumption,realistic application,alternative approach,framework,discriminative training,assumption,training,test data,distribution,distribution,traditional approach,likelihood,error rate,discriminative training method,error rate,training data,likelihood,solution,error rate,finite set,training sample,step function,model parameter,prob lem,previous research,development,loss function,exact error rate,method,method,property,convergence,generalization error,approximated loss function,original objective,new estimation pro cedure,discriminative training method,simple heuristic training algorithm,error rate,training sample,multidimensional function optimization algorithm,subset,feature,candidate feature,iteration,parameter,feature,feature selection,parameter optimization,criterion,training sample,evaluation,japanese text input, msr achieves,error rate reduc tion,newswire data set,discriminative method,boosting method,perceptron algorithm,theory,experiment,cross domain lm adaptation show,domain,different domain,traditional  lm adaptation method,achieves,result,boosting method,perceptron algorithm,paper study lm,text input,standard method,japanese text,input phonetic symbol,appropriate word string,task  ime,input method editor,commonly,number,character,phonetic string,number,character,correct transcript,current  ime system,conversion,real data,wide variety,a b ayes decision problem,netic string, ime system,likely word,candidate,candidate,speech recognition,acoustic ambiguity,phonetic string,account typing error,unique mapping,decision,direct evaluation test bed,speech recognition,advantage,large number,discriminative learning,optimal model parameter,training data,argu ments,assumption,underlying distribution,parameter,distribution,multi nomial distribution,language,example,trigram model,assumption,next word,preced ing word,many case,natural language,arbitrary distance,assumed model form,estimator,question, ime system,estimator,expected error rate,unseen test data,distribution,test data,error rate,training data,vapnik,simple heuristic training procedure,minimum sample risk,next section,inimum sample risk,problem definition,general framework,linear dis criminant model,following notation,collins,example input output pair,training sample,input phonetic string,reference,script,candidate word string,experiment,top word string,base line  ime system,word trigram model,feature,arbitrary function,real value,vector notation,generality,base feature,log probability,word trigram model,feature,experiment,parameter,model form,vector,dimension,feature function,decision rule,maxarg,ranking problem,ranking score,probability,number,conversion error,reference transcript wr,string edit distance function,error count,training sample sample risk,sample risk,pa rameters,basic  msr training algorithm,improvement,training algorithm, msr training algorithm,multidi mensional function optimization approach,feature vector,direction,feature,sample risk,direction,line search,second direction,minimum,whole set,direction,many time,objective function,simple method,assumption,implemen tation,line search,function,direction,second,number,candidate feature,feature,assumption,step function,regular gradient,procedure,grid search,problem,simple grid search,large grid,op timal solution whereas,fine-grained grid,slow algorithm,million,candidate feature,subsection,grid line search,implementation,grid search,modified version,fications,efficiency issue,large number,feature,sample,feature,simple grid search,interval,adjacent grid,feature,sequence,interval,different value,sample risk,training sample,discus sion,training sample,line search,current model parameter vector,selected feature,line search,optimal parameter,candidate word,fwfwwscore dd ddd dd,first term,right hand side,several candidate word string,relative rank,integer value,particular n-gram,candidate,candidate,candidate,active candidate,matter,candidate,active candidate,interval,particular active candidate,candidate,corresponding interval,result,training sample,sequence,interval,optimal value,se quence,midpoint,interval,sample risk  f igure,example,line search,process,whole training,sequence,interval,training sample,training set,global sequence,interval,corresponding sample risk,optimal value,minimal sample risk,global interval sequence,example,figure,line search,interval,interval,optimal value,sample risk,stable solution,robust solution,interval,interval,sequence,corresponding,sample risk,midpoint,interval,corresponding sample risk,interval,smoothed sample risk,interval,smoothing factor,experiment,figure,stable interval,addition,active candidate list,efficiency,line search,line search,small subset,training sample,distribution,feature,training sample,inverted index,feature,training sample,section,line search,large training,million,candidate feature,feature subset selection,section,method,million,feature,small subset,effective feature, msr learning,number,feature,reason,computational complexity,generalization property,linear model,large number,feature,large number,pa rameters,linear model,section,limited number,training sample,number,feature,simpler model,first step,feature selection algorithm,feature,effectiveness,feature,reduction,sample risk,sample risk,base feature,sample risk,parameter,line search,effectiveness,denominator,normalization term,selection procedure,candidate,top feature,feature vector,ad vantage,computational simplicity,feature,high correlation,instance,feature,rich discriminative information,little gain,feature vector,high cor relation,technique,correla tion information,feature selection criterion,et xmd,oolean value,sample risk reduction,d-th feature,m-th training sample,cross correlation coefficient,feature,theodoridis,koutroumbas,feature selection procedure,following step,selected feature,fj denotes,candidate feature,candidate,descending order,feature,second feature,cross correlation coefficient,feature f1,m-1 feature,second feature,weight,relative importance,held-out data,experi ments,selection,second feature,account,impact,sample risk,correlation,feature,feature,corre lation,select k-th feature,maxarg jic fej,next feature,account,average correlation,feature,optimal number,held-out data,line search,efficiency issue,feature selec tion method,esti mate,number,candidate feature,number,feature,resulting model,feature se lection method,candidate,efficiency,line search algorithm,estimate,medium-sized,new fea ture,feature,feature,number,estimate,experiment,computational cost,noticeable quality loss,algorithm,experiment,figure,feature se lection,optimization,feature,top feature,feature selection method,section,iteration,parameter,line search,  f igure, msr algorithm 4 e valuation,setting,newspaper corpus,training,test data,nikkei,yomiuri newspaper,corpus,lexicon,000-sentence subset,yomiuri newspaper corpus,number,iteration,feature,yomiuri newspaper corpus,nikkei newspaper corpus,training set,word lattice,baseline system,word trigram model,anther,000-sentence subset,nikkei newspaper corpus,subset,unseen phonetic symbol string,baseline system,efficiency,training,hypothesis,candidate conversion,discriminative training,oracle,hypothesis,minimum number,reference transcript,unigrams,bigram,training,feature,trigram feature,significant improvement,pilot study,total number,candidate feature,ur main experimental result,baseline result,word trigram model,notice,result,state-of-the-art performance,large amount,similarity,training,test data,result, msr algorithm,section, msr algorithm,state-of-the-art discriminative training method,boosting,implementation,improved algorithm,boosting loss function,collins,percep tron,implementation,averaged perceptron algorithm,collins,discriminative training method,notice,uni gram,bigram feature,baseline trigram model,improvement,high performance,perceptron,method,algorithm,subset,feature,min ute,perceptron algo rithm,boosting method,robustness issue,robustness,dis criminative training algorithm concern,ques tions,guarantee,algo rithm converges,training sample,convergence problem,second,training error reduction,algorithm,unseen test sample,generali zation problem,theoretical justification,empirical evi dence,robustness,robust linear model,training error,number,robustness, msr algorithm,feature selection method,different subset,feature,different setting,feature se lection method,section,different number,account,correlation,feature subset, msr algorithm, cer curve,training,test data set,figure, f igure,error curve, msr algorithm, f igure,test error curve,result reveal several fact,con vergence property,figure,error drop,iteration,feature,training data,test data,account,correlation,comparison, cer result,subset,feature,training error,figure,generalization property,test error,figure,domain adaptation result   t hough  msr achieves impressive performance, ce reduction,comparison method,section,experiment,newspaper text,training,testing,realistic scenario,application,section,result,additional experi ments,domain,so-called cross-domain lm adaptation paradigm,su zuki,detailed report,distinct source,nikkei newspaper corpus,section,background domain,word trigram model,adaptation domain,yomi uri,corpus,newspaper,source,encyclopedia,shincho,collection,domain,adaptation training data,000-sentence subset,held-out data,000-sentence subset,test data,corpus,training sample,adapta tion domain,hypothesis,candi date conversion,discriminative training,tation method,  b aseline,background word trigram model,section,posteriori,traditional lm adaptation method,parameter,background model,likelihood,terpolation,probability,background model,probability,adaptation data,history,trigram probability,interpolation,held-out data,  p erceptron,boosting,discriminative method,previous section,base feature,model yomiuri tuneup encarta shincho baseline,boosting,perceptron,result,adaptation test set,word trigram model,background data,n-gram feature,adap tation data,parameter,back ground model,adaptation data,adaptation domain,discriminative method,improvement margin,discriminative method, map correspond,similarity,background domain,adaptation,domain,background domain,yomiuri,dis criminative method,large margin,margin,domain,encarta,difference,adaptation method,likelihood,distribution,adaptation domain,background domain,difference,underlying distri butions,discriminative meth od,limitation,adaptation test set, ce result,discriminative method,improvement,method,domain,perceptron algorithm,yomiuri domain,result,work discriminative model,generative model,collins,collins,speech recogni tion,linear model,nature,probabilistic coun,terpart,logistic regression,rea son,jordan,parameter,discriminative model,conditional likelihood,training data,training error,objective function,system,spirit,method share,motivation,error function,finite data set,step function,previous research,error function,loss function,freund,alternative approach,simple heuristic training proce dure,training error,previous method,basic training algorithm,section,general framework,line search,extension,extension,large number,feature,training sample,previous algorithm,linear model,feature,feature selection method,particular implementation,feature selection method,koutroumbas,major difference,method,effectiveness,feature,expected training error reduction,previ ous method,metric,feature correlation,feature selection,finette,6 c onclusion,future work,successful discrimina tive training algorithm,experiment,conver sion performance, ime task, ml method,discrimi native method,boosting,perceptron method,theoretical underpinnings,interesting property,objective function,gradient,error rate,recall,large-scale training,mil lion,candidate feature,future work,wider variety, nl task,parsing,michael,discriminative training method,hidden markov model,theory,experiment,perceptron algorithm, emn lp,michael,discriminative reranking,natural language parsing,richard,breast tissue clas sification,diagnostic ultrasound,pattern rec ognition technique,method,schapire,singer,efficient boosting algorithm,jianfeng,hisami suzuki,yang wen,headword dependency,predictive clus,language modeling, emn lp,dis criminative model,information retrieval, sig ir,  j elinek,statistical method,speech recognition,cambridge,minimum classification error rate method,speech,audio processing,comparison,logistic regres sion,franz josef,minimum error rate training,statistical machine translation,vetterling,numerical recipe,scien tific computing,new york,cambridge univ,arul menezes,colin cherry,phrasal  smt,  r oark,murat saraclar,michael collins,large vocabulary  asr,perceptron algorithm, ica ssp ,hisami,jianfeng gao,comparative study,language model adaptation,new evaluation metric,sergios,konstantinos koutroumbas,pattern recognition,nature,new york,a s tudy,richer syntactic dependency,structured language modeling peng xu center,language,chelba microsoft,com frederick jelinek center,language,jelinek clsp,edu abstract,impact,syntac tic dependency,performance,dimension,im provement,baseline result,upenn treebank,corpus,analysis,formance show correlation,quality,parser,pre cision recall,language model,enriched  slm,baseline 3-gram model,isolation,second pas,n-best re-scoring,language model,ntroduction,structured language model,hidden parse tree,conditional word-level language model probability,chelba,jelinek,final best parse,reduction,relative,3-gram baseline,headword parametrization,word prediction,reduction,good guess,sen tence,left-to-right,entire sentence,regular statistical parser,technique,statistical parsing community,human annotator,performance,language model,syntactic structure,statistical parsing community,iou way,dependency structure,parametrization,probabilistic model,parse tree,charniak,upenn treebank, wsj corpus,chelba,simple way,probabilistic dependency, con struc- tor component, wer performance,simple modifica tion,training procedure,formance,simple way,syntactic dependency structure,chelba,result,parser,remarkable fact,first time,language model,elemen,   p roceedings,40th annual meeting,association,tary syntactic dependency,interpolation,3-gram model,n-best re,2 s lm review,extensive presentation,chelba,jelinek,probability,sentence,ev ery possible binary parse,terminal, pos tag,phrase headword,non terminal label,sentence,word-parse prefix word,sentence,connectionist model,frederick jelinek center,language,edu abstract,performance,compo nents,connectionist mod el,connectionist model,representation,history,context,back-off model,inherent capability,connection ist model,data sparseness problem,sub linear growth,model size,context length,connec tionist model,em procedure,procedure,experiment,connectionist model,back-off mod el, upe nn treebank corpus,baseline trigram lan guage model,em training procedure,connectionist model,hidden event, slm parser,1 i ntroduction,many system,natural speech,lan guage,automatic speech recognition,national science founda tion,statistical machine translation,language model,crucial component,large hypothesis space,state-of-the-art system,n-gram language mod el,effective,technique,lan guage model probability estimation,n-gram literature,goodman,various way,information,context span,normal n-gram language mod el,syntactical informa tion,word-based n-gram model,chelba,jelinek,charniak,uystel,language model,stochastic parsing technique,parse tree,input word sequence,condition,generation,lexical information,parse tree,language model,useful hierar chical characteristic,language,various task,improvement,syntactical dependency,severe data sparse ness problem,number,feature,recent promising work,distributional representation,neu ral network,language modeling,bengio,henderson,great ad vantage,approach,ability,data sparseness,model size,number,feature,method,regular n-gram model,perplexity,bengio,ability,method,context,exper iments,consistent improvement,context,component,length,em train ing procedure,component,connectionist model,em training,impact,neural network,component,approach,em training procedure,training,neural network model,probabilistic neural network model recently,new type,language model,multi-dimensional feature space,probability,sequence,neural network,neural network,feature vector,probability,next word,bengio,main idea,dimensionality,sequence,training data,generalization,unseen word sequence,probability,word sequence,unseen word sequence,similarity,multi-dimensional space,feature vector,input vocabulary,vocabulary,conditional probability,next word,function,input feature vector,neural network,probability,possible next word,output vocab,rela tionship,output vocabulary,feature vector,parameter,neural network,training,neural network,feature vec tor,output,conditional probability distribution,output vocabulary,feature vector,proba bility function,smooth function,feature value,small change,feature,small change,probability,architecture,neural network model,conditional probability function,output vocabulary random forest,language modeling peng xu,frederick jelinek center,language,speech processing,edu abstract,breiman,language modeling,problem,next word,new language mod,approach,randomly,automatic speech recog nition,rf approach,context,gram type language modeling,regu lar gram language model,language model,unseen data,complicated history,rf language model,regular gram language model,large vocabulary speech recognition system,1 i ntroduction,many system,natural speech,lan guage,automatic speech recognition,statistical machine translation,language model,crucial component,large hypothesis space,state-of the-art system,gram language model,effective,technique,language model probability estimation,gram literature,goodman,language model,potamianos,jelinek,similar history,probability estimation,result,potamianos,jelinek,fair comparison,decision tree language model,baseline gram model,dt language model,data sparseness problem,gram language model,neg ative result,data sparseness,dt construction al gorithms,tree split,potamianos,jelinek,technique,context,dt language model,significant improvement,gram model,neural network,language mod,approach,lan guage model,dimension ality,bengio,schwenk,gauvain,backoff smoothing,neu ral network model,baseline backoff mod el,neural network model,interpolation,gram model,gram model,low frequency word,improvement,gram model,performance,neural network model,approach,language modeling,language,recent success,classification,regression,breiman,collection,new dt language model,rf function,data sparseness problem,form well,unseen test data,col lective contribution,rf gen,unseen data,rf approach,gram language modeling,significant improvement,large vocabulary speech recognition system,section,language modeling,smoothing,section,language model,new dt,rf approach,language modeling,sec tion,performance,language model,discussion,analysis,future direction,section,2 b asic language,purpose,language model,probability,word string,string,chain rule,probability,roceedings,joint conference,empirical method,natural language processing,computational natural language learning,prague,association,computational linguistics large language model,machine translation thorsten brant ashok,popat peng xu franz,och jeffrey dean google,amphitheatre parkway mountain view,jeff google,abstract,benefit,large scale statistical language modeling,ma chine translation,distributed infrastruc ture,language model,n-grams,probabil ities,single-pass decoding,new smoothing method,stupid backoff,large data set,quality,kneser-ney smoothing,amount,data increase,1 i ntroduction,sentence,problem,machine translation,mathematics,problem,optimization,arg max em,feature function,weight,feature func tions,language model,n-gram language model,unlabeled monolingual text,general rule,data tends,language mod el,question,context include,language model,large amount,training data,translation performance improve,language model increase,return,performance,function,language model size,possible answer,first question,context,particular statis tical machine translation system,particu lar,distributed language model training,deployment infrastructure,efficient integration,hypothesis-search algorithm,follow-on re-scoring phase,two-pass,practice,single-pass decoding,source,potential information loss,2 n gram language model traditionally,statistical language model,probability,string,punctuation,string,fixed vocabulary,n-gram language model,probability,approximation, a m arkov assump tion,rele vant,next word,frequency,occurrence,target-language string,training data,probability estimate,n-grams,relative frequency,denominator,numerator,undefined probabil ity estimate,sparse data prob lem,reason,ml estimate,practice,goodman,discussion,n-gram model,principle,predictive accuracy,lan guage model,sparse data problem,present work,challenge,amount,training data sufficient,higher-order n-gram model,efficient use,decoder,distributed language model,distributed language model,two-pass approach,order n-gram,hypothesis-generation phase,k-best,hypothesis,large-scale distributed language model,resulting translation performance,hypothesis,first-stage system,amount,large-scale distributed language model,context,speech recognition,machine translation,underlying architecture,difference,tegrate,distributed language model,ma chine translation decoder,re port detail,integration,efficiency,approach,amount,experiment,approach,corpus,suffix array,sub-corpus,worker,raw count,work er,n-gram request,approach,probability,worker,n-gram,tech niques,worker,technique,context-dependent backoff,furthermore,suffix array,section,count cutoff,4 s tupid backoff state-of-the-art,us variation,con text-dependent backoff,following scheme,stored probabili tie,back-off weight,example,kneser-ney smoothing,kneser,backoff,linear interpola tion,jelinek,mercer,scheme,goodman,uniform distri bution,simpler scheme,stupid backoff,malized probability,main difference,discounting,relative frequency,probability,simple scheme,scheme,backoff factor,single value,training corpus,stupid backoff,environment,quality,kneser-ney,large amount,normalization,functioning,language model,present setting,solute feature-function value,5 d istributed training,mapreduce programming model,ghemawat,terabyte,terabyte,language model,programming model,user-specified map function,input key value pair,intermediate key value pair,function,intermediate value,multiple map task,different machine,dif ferent part,input data,multiple re duce task,fraction,intermediate data,intermediate key,reducer,additional detail,communication,machine,data struc tures,application example,reader,ghemawat,language model,main step,following section,vocabulary generation vocabulary generation,mapping,compression,original term,term fre quency,frequent term,small id,efficient variable-length encoding,good result,experiment,multiple value,n-gram order,result,pre-determined threshold,special id,unknown word,vocabulary generation map function,intermediate data,current section,sharding function determines,mapreduce framework,re duce function,simplified,function,following,key docid,document array word,okenize,string,int histo,histo emit,int shardforkey,int nshards,nshards,iterator value,reduce function,output key,inter mediate key,mapre duce,computation,map func tion,minor optimization,alternative,tokenized word,figure,example,input document,particular term,hash function,text color,exact par titioning,reducer,generation, n-g ram,process,n-gram generation,cabulary generation,main difference,maximum order,figure,vocabulary generation,simplified map function,follow ing,key docid,document array id,tokenize,maxorder-1 emit,id i-j,map function,section,aggregated count,function,vocabu lary generation,subsequent step,language model generation,relative frequency,step efficient,sharding function,numerator,denominator,hash function,first word,n-grams achieves,required gram,total count,first word,beginning,huge num ber,stopwords,punctuation mark,beginning-of-sentence marker,example,beginning-of-sentence marker,several time,average size,total runtime,process,int shardforkey,int nshards,prefix,nshards,unigram count,relative fre quencies,small amount,information,hundred,billion,n-grams,language model generation,language model generation step,output,n-gram generation step,information,late relative frequency,individ ual shard,sharding function,everything,stupid backoff,complex smoothing method,additional step,operation,full gram,language model generation step shard n-grams,unigrams,backoff operation,last word,method state-of-the-art technique,kneser-ney smoothing,katz backoff,expensive step,runtime,client,backoff factor,server,network traffic,method,history backoff factor,large fraction,stupid backoff,training,additional iteration,step  0 s tep  1 s tep,context counting unsmoothed probs,interpol,weight interpolated probability input,output,output,input value,output,output,intermediate key wii,intermediate value  fkn,output value  fkn,extra step,training,kneser-ney smoothing kneser-ney smoothing,lower-order gram,frequency,number,unique single word,original frequency,context count,n-gram counting step,quantity,n-gram counting, am apreduce,variant,kneser-ney smoothing,kneser-ney smoothing,goodman,discount constant,interpolation weight,probability,additional major mapreduces,describes,output key,output key,intermediate key,map function,mapreduce,emits n-gram history,intermediate key,func tion,history,unsmoothed probability,interpolation weight,mapreduce,interpolation,map function emits,intermediate key,ev ery,reducer function re ceives,sorted order,smoothed probability,n-gram order,simple book-keeping,katz backoff,similar additional step,kneser-ney smoothing,stupid backoff,amount,application,distributed language model,first pas,decoder,result,n-best list,lattice rescor ing,ortmanns,lan guage model,machine,decoder,decoder,distributed system,network latency,constant overhead,millisecond,board memory,new decoder archi tecture,decoder,number,request,server,network request,large number,n-grams,request,single n-grams,n-best search,machine translation,coder proceeds,search space,hypothesis,word position,source language,candidate exten sion,hypothesis,addi tional target-language word,variable-length source-language fragment,variable-length target-language frag ments,traditional setting,local language model,decoder,nec essary probability,figure,illustration,decoder graph,batch querying,language model,feature,decides,search graph,distributed language model,decoder,current hypothesis,n-grams,transmission,batch request,decoder,tentative hypothesis,assigns score,re-prunes,search graph,next round,exten sion,server,process,figure,trigram model,decoder policy,promising hypothesis,ac tive hypothesis,black disk,decoder,new node,target-language word,source-language word,hypothe s,language-model score,batch manner,decoder prune,black disk,new node,hypothesis,people,reason,onlooker,server,promising,process,pruning,word position,source sentence,average sentence length,test data,section,rounds3,sentence,average,num ber,sentence,decoder setting,beam size,re-ordering win dow,example,experiment section,sentence,average net work latency,millisecond,total latency,second,language model,average sentence,slight reduction,translation qual ity,average network latency,millisecond,number,sen tence,result,system,large distributed language model,second pas,n-best list,machine translation,describ,queued language model access,decoder,optical character recognition system,7 e xperiments,5-gram language model,amount,language mod el,training data size,result,language model,fraction,test data,language model, ble score,papineni,machine translation system,train ing size,test-set perplexity,kneser-ney smoothing,com parison,data set,language model,data set,trained language model,feature function,result,single model, gb lm,data size,x2 target,webnews web figure,number,amount,training data,target,english side,arabic-english parallel data,ldcnews,concatenation,several english news data set,webnews,several year,december,web page,english news article,general web data,arabic-english  nis t mt evaluation set,training data,sentence,newswire,newsgroup text,reference transla tions,test set,transla tion  ble score,english side,perplexity,n-gram coverage,language model,language model,total number,frequency cutoff,gov speech test,parallel resource,gigaword,tipster,acquaint, 1-f eb-2006,training data,target,web token,vocab size,machine,approximate training time,language model,frequency cutoff,minimum frequency,vocabulary,target,ldcnews,webnews data set,web data set,threshold,spe cial term  unk,unknown word,figure,number,lan guage model,logarithmic scale,right scale,approximate size,served language model,gigabyte,number,relative increase,language model size,number,gram grows,factor,amount,training data,data set,data size,straight line,log log space,linear least-squares regression,data set,web data set,relative increase,vocabulary cutoff,language model gen,contains,n-grams,show size,approximate training time,full target,webnews,web data set,process,standard current hardware,linux operating system,kneser-ney smoothing,stupid backoff,generation,kneser ney model,web data,experiment,runtime,machine,pe rp le xit yf ac tio,ra sl,data size,x2 target kn pp,kn pp target c5,c5 webnews c5,c5 figure,perplexity,fraction,perplexity, n-g ram coverage standard measure,language model quality,perplexity,inverse,average conditional prob ability,next word,perplexity,bet ter,figure,show perplexity,kneser-ney,data size,ldcnews,webnews data,perplexity,mix ture,test data,newswire,broadcast news,newsgroups,training data,news article,held-out set,newswire text re,perplexity,language mod el,full ldcnews model,per plexities, nis t mt,evaluation, nis t mt,perplexity,different language model,different vocabulary,fixed frequency cutoff,vocabulary,training data grows,perplexity,vocabulary, bl eu lm,data size,x2 target kn,kn webnews kn target sb,sb webnews sb,sb figure,amount,language model,stupid backoff,normalized probability,indication,potential quality,increased training,5-gram coverage,fraction,test data set,language model training data,coverage,language model,estimate,estimate,unseen event,fraction,5-grams,test data,language model,increase,coverage depends,training data set,constant growth,correlation r2,doubling,training data,number,growth oc cur,webnews data,growth,target data,machine translation result,state-of-the-art machine translation system,arabic,competitive  ble score,arabic english  nis subset,re-ordering window,gov speech test,mt06eval official result,result,number,experiment, nis evaluation system,mixture,7-gram model,optimized stupid backoff factor,fixed order,backoff fac tor,modification, ble score,amount,language model,system,result,figure,first part,target data,lan guage model,kneser-ney, ble score,mil lion token,data size,constant backoff parameter,average,doubling,training data,bootstrap resampling,noreen,second language model,ldc news data,first point,ldcnews,large improvement,last point,target,improvement,amount,new domain,improvement,doubling,ldcnews data,tween kneser-ney smoothing,stupid backoff,difference,significant difference,third language model,web news data,steady increase,kneser-ney,stupid backoff,result,stupid backoff,kneser-ney,difference,fourth language model,web data,stupid backoff,kneser ney model,data size,ex pensive,fourth model,system score,dif ferences,steady increase,kneser-ney model,goodman,kneser-ney smoothing,scheme,broad range,condition,experiment,advantage,language model size,advantage,data size,amount,benefit,training size,domain,data sets10,improvement,log scale,linear least-squares regres sion,correlation,method,sim ilar improvement,8 c onclusion distributed infrastructure,large-scale language model,chine translation,experimental result,effect,amount,5-gram language model size,magnitude,amount,training data,liter ature,three-to-four order,magnitude,structure,amount,training data,n-gram order,technique,judicious batching,score request,decoder,server client architecture,distributed computation,sophisticated method,language model increase,translation quality, ble score,language model size,finding,large language model,performance gain,direction,effect,example,data yield,second model,discussion,effect,space limitation,reference peter,stephen della pietra,vincent,della pietra,robert,mercer,mathematics,statistical machine translation,parameter,computational linguistics,stanley,joshua goodman,empiri cal study,technique,language model,technical report tr-10-98,harvard,jeffrey dean,sanjay ghemawat,mapreduce,simplified data processing,large cluster,sixth symposium,operating system design,ahmad emami,kishore papineni,jeffrey sorensen,large-scale distributed language modeling,proceeding, ica ssp-,joshua goodman,progress,language modeling,technical report  msr tr-2001-72,research,frederick jelinek,robert,mercer,estimation,markov source parameter,sparse data,pattern recognition,practice,north holland,slava katz,estimation,probability,language model component,acoustic,speech,signal processing,kneser,hermann ney,backing-off,m-gram language modeling,pro ceedings, iee e in ternational conference,acoustic,speech,signal processing,philipp koehn,statistical significance test,machine translation evaluation,proceeding,emn lp-04,barcelona,hermann ney,stefan ortmanns,dynamic programming search,noreen,computer-intensive method,hypothesis,franz josef och,hermann ney,align ment template approach,statistical machine transla tion,computational linguistics,kishore papineni,salim roukos,todd ward,wei jing zhu,method,automatic eval uation,machine translation,proceeding,almut silja hildebrand,stephan vogel,language,n-best list re-ranking,proceeding, emn lp-2006,sydney,australia,human language technology,annual conference,north american chapter,boulder,colorado,association,computational linguistics using  a d ependency parser,michael ringgaard,franz och google inc,amphitheatre parkway mountain view,ringgaard,och google,com abstract,novel precedence,approach,dependency parser,tistical machine translation system,approach,method,tic knowledge, smt system,complexity,order lan guages,significant improvement,ble score,english,approach,state-of-the-art phrase-based  smt system,1 i ntroduction,past ten year,statistical machine transla tion,many exciting development,phrase,system,machine transla tion field,translation,word sequence,single word,ap proach,robustness,local word reordering,existence,algorithm,phrase-based system,language,different word,long dis tance,key weak ness,method,recent year,problem,dif ferent aspect,first class,approach try,phrase,distance,distance,distortion model,simple way,phrase level,non-monotonicity,weight,number,source phrase,consecutive target phrase,phrase reordering,tillmann,al-onaizan,papineni,different weight,different phrase,hierarchical phrase,galley,manning,mine phrase boundary,efficient shift-reduce parsing,research,tive reordering model,maximum entropy classifier,improvement,distance,distortion model,word alignment step, smt system,word align ment error,distance,second class,approach put syntactic analysis,target language,modeling,decoding,direct model ing,target language constituent movement,ei ther constituency tree,yamada,knight,depen dency tree,signifi cant improvement,translation quality,translat ing language,chinese,arabic,english,simpler alternative,approach,chiang,promising result,chinese,english,similar,distance,reordering model,hierarchical approach,word alignment,mod el,machine translation,chart parsing,decoding complexity,re cent work,great improvement,decod ing efficiency,hierarchical ap proaches,chiang,phrase-based system,order language model,researcher,source language syntax,machine trans lation,syntactical analysis,source language,input sen tences, mcc ord,habash,mul tiple ordering,weighted option,elming,approach,input source sentence,syntactic analysis,reordering rule,reordering rule,deterministic,syntactic analysis,input sentence,good way,long distance reordering,complexity,pro ce,phrase-based system,preprocessing reordering,training data,ap proaches,distance,reordering,hi erarchical phrase reordering,phenomenon,preprocessing reordering,reordering approach,approach,analysis,key issue, svo language,language,exper imental result,approach, smt system,english, sov language,korean,japanese,approach,hierarchical phrase-based sys john,example alignment,english, ak orean sentence tems,future re search direction,ranslation, sov language,linguistics,basic word order,argument,possible permutation, sov lan guages,important area,representative, svo language,korean,representative, sov language,discussion,word order,figure,example sentence,english,corresponding translation,korean,alignment,sentence,phrase,phrase-based decoder,translation,following step,english,beginning,sentence,translates,translates,translates,phrase-based decoder,sentence,complex structure,number,decod ing,imagine,figure,dependency parse tree,example english sentence,sentence,english,second language,many country,decoder,translation,translation,phrase-based de coder,distance reordering,efficiency consideration,english,korean,proper word order,dependency parse tree,english sentence,reordering problem,simple example,fig ure,english sentence,subject,object,english, sov order,sentence,monotonic translation,dependency parser,en glish,reordering,3 p recedence, ad ependency parser figure,dependency tree,example sentence,previous section,subject noun,auxiliary verb,object noun,sentence,object noun,subtree,head verb,subject noun,simple rule,reality,many possible chil dren,relative ordering, sov language,ordering,pose precedence,depen dency parse tree,english,korean example,sov language,precedence reordering rule,mapping,depen dency parse tree node,dependency label,child node,weight,child node,multiple child,weight,weight,relative order,weight,num ber,order type  nor mal ,original order, rev erse,special label self,weight,precedence tuple,later discussion,suppose,example,figure, roo node,result,depen dency tree,root node, pos tag,left-hand-side,sen tence,precedence weight,precedence tuples,child node,dependency label,tuples,default weight,fault order type,child node,weight,weight,verb precedence rule verb movement,important movement,dependency parse tree,verb node,many child,example,passive auxiliary verb,main verb,move ment,example,figure,dependency parse tree,alignment, as entence,preposition modifier word order,ther category,phrasal verb particle,negation,english sentence,preposi tional phrase,prepositional phrase,direct object,korean counterpart,figure,phenomenon,adverbial clause modifier,whole adverbial clause,subject,main sentence,adverbial clause,ordering,verb reordering rule,clause,verb precedence rule,reordering phenomenon,rule set, pos tag,phrasal verb particle,auxiliary verb,passive auxiliary verb,auxpass,negation,verb group,sentence,adverbial clause modifier,beginning,sentence,noun subject,preposition modifier,anything,original order,verb group,direct object,adjective precedence rule similar,adjective,aux iliary verb,passive auxiliary verb,auxpass,auxpass,precedence rule,english,language order,bilingual speaker,text book exam ples,english,korean,dependency parse tree,english example,modifier,change,english,korean,verb rule,adjective precedence rule,second panel,auxiliary verb,passive auxiliary verb,negation,head adjective,modifier,adjective precedence rule,heuristic,exces sive movement,movement,punctuation,conjunction,sentence,reordering result,preposition precedence rule,korean,preposi tional phrase,prepositional phrase,happiness,relative clause mod ifier,head noun,preposition head node,object modifier,object,preposition,example,figure,reordering,preposi tion precedence rule,fourth panel,complex example,figure, roo node match,adjective rule,precedence weight, roo node,weight,sen tence becomes,future,whole adverbial phrase,beginning,sentence,child node,verb rule,weight,first group,ormal  order,second group,everse order,af ter reordering,sentence,future,exciting,verb rule,final re ordering,sentence,future,exciting,figure,sentence,monotonic alignment,reasonable ko rean translation,figure1,introduction,several study,source sentence re,syntactical analysis,statistical ma chine translation,precedence,ap proach,dependency parser,previous work,various way,several approach,syntactical analysis,multiple source sentence reordering option,word lattice,elming,weight,original set,approach,training,reorder unit,phrase,boundary,training,test data,system,matched condition,ei ther chinese,english,elming,long distance reordering,tween english, sov language,approach, mcc ord,habash,perform,syn tactic rule, mcc ord,habash,approach,difference,wide range, sov language,tracted precedence rule,language,next section,approach,strong baseline,advanced distance,distor tion model,precedence reordering rule,habash,verb rule,reordering,subject,object,preposi tion modifier,auxiliary verb,negation,head verb,alignment,habash,habash,writ ten rule,unseen child,order nat,default precedence weight,order type,specific condition,chil dren,backoff scheme,broad cov erage,fourth,dependency parse tree,constituency tree,syntactic word,der model,english,japanese machine transla tion,toutanova,global word order model,fea tures,word bigram,target sentence,displacement, pos tag,source,livingwhatfuture knowhas,csubj cop detmarkroot auxnsubj neg advcl nsubjdobj ccomp  pl iving,bg  vbz  dti njj vbpprp rb vb  nnw p vbz,label token pos figure,example,reordered english sentence,alignment,bottom,get side,log-linear model,feature,re-rank,baseline decoder,reordering problem,english,japanese transla tion,approach,5 e xperiments,experiment,state of-the-art phrase-based statistical machine transla tion system,system,english, 5 s ov,word alignment step,iteration, ibm model-1 training,iteration, hmm training,model-4,much value,system,pilot study,standard phrase extraction algorithm,al phrase,addition,regular distance distortion model,maximum entropy,lexicalized phrase,fea ture,source,target,feature,source word,current aligned word,current aligned word,next aligned word,target word,immediate history,feature,maxi mum entropy model,decoding process,entropy model,data word alignment, lbf g algorithm,system source target english,english,hindi  16m  17m english,urdu  17m  19m english,turkish  83m  76m table,training corpus statistic,system, 5 s ov language,feature,weight,implementation,lattice  mer,parallel training data,in-house col lection,parallel document,iou source,substantial portion,simple heuristic,po tential document pair,doc uments,training data,exact clean translation,actual statistic,training data,language, 5 s ov,target side,parallel data,monolingual text,gram language model, 10k english sentence,evaluation data,sentence, 5 s ov,sentence,reference,lation,subset,sentence,test contains,sentence,sentence,blindtest set,dev set, mer training,test set,trained weight,nondeterminism, mer training, ibm  ble,papineni,translation,level  ble,korean,reordering,reordering model,precedence rule,maximum entropy,baseline,system,distance distortion model,maximum entropy,lexicalized reorder,result,section,distance,lexicalized reordering model performs,system,approach,ply precedence reordering rule,preprocess ing,approach,reordering model,difference,precedence reordering,different phrase table,precedence rule,dependency parser,deterministic inductive dependency parser,scholz,efficiency,good ac curacy,implementation,deterministic dependency parser,maximum entropy model,classifier achieves,labeled attachment score,unlabeled attachment score,standard penn treebank evaluation,result,lan guages,precedence reordering rule, ble score,baseline system,statistical significance test,bootstrap method,signif icance level,baselie,statistical significance level,korean,prece dence reordering rule,absolute ble score improvement,turkish,english,ko rean sentence,korean,japanese,word order,korean,language system dev test,blindtest,en glish,ov language,various,op tions,maximum entropy,lexialized phrase,precedence rule,benefit,constraint,motivation,precedence re,english, sov lan guages,word order,therefore,decoding,translation,con trolled experiment,korean,example,clearly,precedence reordering rule,system,maximum,distance,figure, ble score,allowed reordering distance increase,order difference,english,korean,distance,ing time,decoding speed,approach,reordering,hierarchical model,hierarchical phrase-based approach,several system,chiang, 10m aximum allowed reordering distance,blindt,bl eu sc,lexreorderbaselineno lexreorder,parserreorderwith parserreorder figure,blindtest  ble u sc,different maximum allowed reordering distance,english,different,option,long distance re ordering, a p scfg model,english,language system,training data,described,previous section,hierarchical system,4-gram language model, 5 s ov, sam pack age,zollmann,venugopal,similar setting,source side,nonterminals,extract rule,initial phrase,application,grammar,chart item,source word,precedence,applies,hierarchical system,reordering rule,setting,regular hier archical system,result,hierarchical system,precedence re,normal phrase-based system, ble score,corresponding hierarchical system,confidence level,archical system,maximum entropy,lexicalized phrase,precedence re,hierarchical system,significant improvement,normal hierarchical system,simplicity,reordering rule,language system dev test,pr hier,pr hier,pr hier,urdu pr,pr hier,pr hier,blindtest,en glish,ov language,precedence rule,re ordering,hierarchi cal system,phenomenon,long distance reordering,hierar chical phrase-based system,possible reason,reordering rule,preprocessing,english sentence,training data, sov order,word alignment quality,hier archical phrase-based system,hence achievesbetter translation quality,hierarchical phrase-based system,algorithm,efficient dynamic pro gramming,phrase-based system,approach,appealing,realtime statistical machine translation system,6 c onclusion,novel precedence re,approach,dependency parser,approach,system,ov language,turkish,lan guages,significant improve ments,state-of-the-art phrase,baseline system,amount,training data,language varies,noisy data,approach,versatile,language,approach,hierarchical phrase-based baseline system,reorder,several  sov language,phenomenon,format,corpus,fu ture,direction,language,reordering,preliminary error analysis,sen tences,parser error,recent year,several study,word lattice,reordering,elming,improve ments,training,efficient procedure,em model,word alignment,word lattice,source side,parallel data,research,tree-to-string model,interesting direction,reference yaser al-onaizan,kishore papineni,distortion model,statistical machine translation,proceeding, acl pi-chuan chang,kristina toutanova,iscriminative syn tactic word order model,machine translation,proceeding, acl david chiang,ierarchical phrase-based model,statistical machine translation,proceeding, acl michael collins,philipp koehn,ivona kucerova,statistical machine translation,proceeding,acl jakob elming,syntactic reordering integrated,phrase,proceeding, col ingmi chel galley,christopher,ef fective hierarchical phrase reordering model,proceeding,emn lp michel galley,jonathan graehl,kevin knight,daniel marcu,steve den eefe,wei wang,ignacio thayer,scalable inference,training,context-rich syntactic translation model,pro ceedings, col ing-aclni zar habash,statistical machine translation,proceeding,mt summit liang huang,david chiang,rescoring,faster de,integrated language model,proceeding, acl philipp koehn,statistical significance test,machine trans lation evaluation,proceeding, emn lp philipp koehn,amittai axelrod,alexandra birch mayne,chris callison-burch,mile osborne,david talbot,edinborgh system description, iws lt speech translation evalu ation,international workshop,spoken language translation philipp koehn,daniel marcu,statistical phrase-based translation,proceeding,minghui li,yi guan,robabilistic approach,syntax-based reordering,statistical machine translation,proceeding, acl yang liu,qun liu,shouxun lin,tree-to-string alignment template,statistical machine translation,proceeding,col ing-aclwo lfgang macherey,ignacio thayer,jakob uszkoreit,lattice-based minimum error rate training,statistical machine translation,proceeding, emn lp robert malouf,comparison,algorithm,maximum en tropy parameter estimation,proceeding,sixth workshop,joakim nivre,mario scholz,deterministic dependency par,english text,proceeding, col ingfr,statistical machine translation,single word model,ger many franz,minimum error rate training,statistical ma chine translation,proceeding, acl franz,hermann ney,alignment template ap proach,statistical machine translation,computational linguis tic,roukos, a m ethod,automatic evaluation,machine translation,proceeding,acl chris quirk,arul menezes,colin cherry,dependency tree translation,informed phrasal  smt,proceeding,acl christoph tillmann,lock orientation model,statistical machine translation,proceeding, hlt -naa cl chao wang,michael collins,philipp koehn,chinese syntac tic reordering,statistical machine translation,proceeding,emn lp-conll dekai wu,stochastic inversion transduction grammar,bilingual parsing,parallel corpus,377-403 fei xia,michael  mcc ord, a s tatistical mt sys tem,automatically learned rewrite pattern,proceeding,col ingde yi xiong,qun liu,shouxun lin,maximum entropy,phrase reordering model,statistical machine translation,proceeding, col ing-aclke,yamada,kevin knight,yntax-based statistical translation model,proceeding, acl yuqi zhang,richard zen,hermann ney,improve chunk level,statistical machine translation,proceeding, iws lt richard zen,hermann ney,discriminative reordering model,statistical machine translation,proceeding,workshop,ashish venugopal,syntax augmented machine translation,chart parsing,proceeding, naa cl,workshop,statistical machine translation andreas zollmann,ashish venugopal,franz och,jay ponte,ystematic comparison,hierarchical,syntax-augmented statistical mt,proceeding, col ing,proceeding, naa cl,tutorial,boulder,colorado,association,computational linguistics distributed language model thorsten brant,   l anguage model,wide variety,natural language application,machine translation,speech recognition,correction,optical character recognition,ecent study,language model,better language model,author,constant machine translation improvement,doubling,training data size,large model,challenge,efficient method,distributed training,large language model,mapreduce,efficient way,distributed model,individual n-grams,communication,different machine,mapreduce model,intermediate data,communication overhead,good sharding function,   c hallenges,katz backoff,kneser-ney smoothing,distributed system,technique,distributed system,   s tupid backoff,linear interpolation,communication,aggregation,much impact,ntropy pruning,stupid backoff,effort,kneser-ney,distributed system,effect,extreme pruning,memory size,approximation,several quantizers,point value,andomized data structure    r,memory size,store model,n-gram,significant impact,quality,fast access,single n-gram,distributed setup,communication,machine,distributed language model,first-pass,decoder,n-gram request,arget audience target audience,researcher,large n-gram language model,   p resenters,saarland university,germany,part-of-speech tagging,parsing,statistical method,event detection,thorsten, a r esearch scientist,google,distributed language model,application,machine translation,research interest,information retrieval,entity detection,google,scientist,john hopkins university,research,statistical machine translation,google,statistical machine learning,information retrieval,speech recognition,4p roceedings,joint conference,empirical method,natural language processing,computational natural language learning,jeju island,association,computational linguistics a s ystematic comparison,phrase table,technique richard zen,daisy stanton,peng xu google inc,xp google,com abstract,large parallel corpus,phrase table component,machine translation system,vast computational resource,novel pruning criterion,phrase table pruning,sound theoretical foundation,systematic experiment,language pair,various data condition,principled approach,ad hoc,method,1 i ntroduction,last year,statistical machine translation,dominant approach,machine translation,mod eling,significant increase,availability,bilingual data,example,large data resource,google web,5-gram corpus,linguistic data consortium consist ing,5-gram count,web data,109-french-english bilingual corpus,workshop,hese enormous data set,translation model,process,statmt,org wmt11 translation-task,html modern computer,large model,long experiment cycle,hinders,situa tion,computational resource,instance,device,model size,utmost importance,resource-intensive component,sta tistical machine translation system,language model,phrase table,compact rep resentations,language model,attention,research community,instance,talbot,osborne,heafield,problem,statistical machine translation system,large phrase table,johnson,large por tions,phrase table,translation quality,systematic comparison,meth od,method,ad-hoc heuristic,theoretical foun dation,criterion,state-of-the art language model,criterion,en tropy measure,stolcke,derivation,desideratum,good phrase table,criterion,criterion,well-understood information-theoretic measure,translation model quality,pruning,phrase table,practical considera tion,phrase,phrase table,information,ood empirical behavior,large part,phrase table,significant loss,translation quality,technique,objective,criterion,objective,empirical evaluation,novel contribution,systematic description,phrase,method,sound phrase table,criterion,experimental comparison,method,several language pair,basic pruning method,probabil ity,count cutoff,technique,moses toolkit,pharaoh decoder,section,efficacy,systematic way,experimental result,johnson,large part,phrase ta,translation quality,pruning criterion relies,statisti cal significance test,significance-based pruning criterion,translation model quality,furthermore,compari son,method,systematic comparison,significance-based pruning,hi erarchical statistical machine translation,different approach,table pruning,usage statistic,sample data,method,approach,table pruning,trian gulation,additional bilingual corpus,source language,target lan guage,third bridge language,many situation,phrase extrac tion method,phrase table size,phrase extraction,approach,simple statistic,section,method,simple phrase table statistic,common class,method,solute phrase table pruning,relative phrase table,absolute,absolute,method,statistic,dependent,phrase,phrase table,method,section,translation,source phrase,application,method,observation count,method,probability,probability,rel ative frequency,potential problem,absolute,meth od,occurrence,source phrase,full set,target phrase,spe cific source phrase,method discard,phrase,target phrase,source phrase,pruning threshold,old pruning,histogram pruning,source phrase,method,target phrase,probability,count-based pruning,method,frequency,source phrase,account,empirical evaluation,translation quality,frequent source phrase,use ful,infrequent one,4 s ignificance,section,review significance prun,johnson,sig nificance pruning,source phrase,target phrase,co-occur,bilingual corpus,chance,simple statistic,bilin gual corpus,source phrase,target phrase,co-occurence count,source phrase,target phrase,number,sentence,bilingual cor pu,real problem,speculation,two-by-two contingency table,fisher,exact test,probability,contingency table,hyperge ometric distribution,p-value,probability,extreme,phrase pair,observed frequency,chance,pruning,detail,approach,johnson,fisher,exact test,context,word alignment,5 e ntropy-based pruning,section,novel entropy-based pruning criterion,motivational example,phrase table,subset,original phrase table,original translation model distribution,key difference,previous approach,redundant phrase,whereas,ous approach,low-quality,unreliable phrase,advan tage,method,redundancy,phrase,quality,example phrase,french-english  wmt phrase table,probability,french phrase le gouvernement franc,others,translation,french govern ment,government,france,translation,translation cost change,two-by-two contingency table,target phrase,gouvernement government,france,le gouvernement franc,french government,government,france,example phrase,translation,remain ing,phrase,phrase,gov ernment,france,cost dramat,shorter,probability,magnitude,original probability,phrase,french government,shorter phrase,ability,original probability,phrase,french gov ernment,translation cost,contrast,phrase,government,france,effect,large change,translation cost,pruning criterion,redundancy,phrase,quality,government,france,translation,french government,assumption,probability,shorter,entropy criterion,notion,re dundancy,original model,conditional relative entropy,thomas,pruned model,optimizing,subset,equivalent approximation,stol cke,language modeling,phrase pair,relative entropy,pruning threshold,phrase pair,contribution,relative entropy,threshold,pruned model,phrase-based system,different segmentation,source language sentence,phrase,segmen tation,phrase,system,translation,shorter phrase,decoder,phrase,translation,segmentation sk1,reordering,segmentation sk1,source,target phrase,permutation,alignment,sub-phrases,sub-phrase,normal phrase translation model,phrase-based decoder,maximum,criterion,search criterion,max sk1,pi 1k,segmentation probabil ity,pruning criterion,function,phrase table,special development,adaptation,segmentation,dynamic pro gramming,phrase-based model,target side,phrase,phrase,segmentation,phrase,value pc,many language pair,computation,experiment,pruning threshold,entire phrase table,entropy criterion,whole phrase table,approximation,short phrase,segmentation,phrase,phrase,pruned model score,effect,detailed experi mental analysis,future work,approximation,entropy,phrase length,one-word phrase,entropy criterion,two-word phrase,phrase,sub-phrases,proba bilities,unpruned one-word phrase,unpruned two-word phrase,three-word phrase,xperimental evaluation,section,data set,experiment,experiment,pub licly available  wmt,translation task,language pair,number,word language pair foreign english german english, mc zech english, ms panish english, mf rench english,training data statistic,number,separate system,direction,phrase,x-to-y,y-to-x,language,represent,nice range,corpus size,baseline system,experiment,following baseline system,phrase,statistical machine translation system,gram language model,target side,bilin gual corpus,second 4-gram language model,monolingual news data,lan guage model,kneser-ney smoothing,baseline system,common phrase translation model,lex ical model,phrase penalty,distortion penalty,lexicalized reordering model,word alignment,itera tions,erations, hmm alignment model,symmetric lexicon,weight,development,bleu criterion,macherey,full phrase table,feature weight,un necessary noise,length,baseline system,phrase table pruning,singleton,target language,source phrase,transla,phrase million,probthreshist figure,comparison,probability-based pruning method,tion quality significantly4,pruning experiment,result,section,experimental result,translation result,news commentary blind,translation quality,bleu score,papineni,function,phrase table size,number,phrase,corner,figure,comparison,method,figure,absolute,threshold,histogram pruning,sec tion,method,difference,abso lute,relative pruning method,relative method,bleu score drop,weighted model score,lexical weighting,different result,undesirable dependance,feature weight,phrase table pruning,number,source phrase,phrase table,result,result,language,result,absolute prun,method,representative,probability-based pruning,figure,transla tion quality,function,phrase table size,pruning threshold,different phrase table size,meth od,frequency,absolute probabil ity,significance test,x-axis,figure,logarith mic scale,difference,method,instance,quarter,number,phrase,countor significance-based pruning, a s panish-english bleu score,mil lion phrase,pruning meth od,small fraction,significance-based pruning perform,method,translation direction,language pair,figure,show compositionality statistic,pruned spanish-english phrase table,similar result,language pair,otal number,phrase,one-word phrase,segmentation,statistic,ach figure,composition,phrase ta ble,different phrase table,phrase ta ble size,phrase table,bleu score,ferent shade,grey correspond,different phrase length,instance,phrase table,count-based pruning,1-word phrase,phrase,2-word phrase,phrase,exception,probability-based prun ing,aggres,pruning,percentage,short phrase,entropy-based pruning re,long phrase,method,probability-based pruning,percentage,long phrase,phrase table,possible explanation,probability-based pruning,fre quency,source phrase,account,difference,poor performance,probability-based pruning,many phrase,statistic,computation,entropy criterion,language pair,phrase,singleton phrase,phrase,non-compositional phrase,phrase,non compositional,single source,target language word,number,non-trivial non-compositional phrase,total number,phrase,figure,effect,translation quality,function,phrase table size,translation quality,function,phrase table size,french-english,translation quality,function,phrase table size,english-czech,translation quality,function,phrase table size,german-english,english-german,per cen tag,f p ra,phrase million,per cen tag,f p ra,phrase million,count 6-word5-word 4-word3-word 2-word1-word figure,phrase length statistic,count-based pruning,per cen tag,f p ra,phrase million,fisher,per cen tag,f p ra,phrase million,entropy 6-word5-word 4-word3-word 2-word1-word figure,phrase length statistic,entropy-based pruning,non-compositional phrase,result,additional experi ments,language,translation direc tions,similar result,big difference,experiment,result,figure,figure,al ternative pruning method,much ad ditional saving,method,phrase table,criterion,bleu point,translation quality,marginal saving,significance-based pruning result,saving,albeit,high vari,corre sponds,ability,achieves,high saving,phrase ta ble,meth od,yield significant saving,significance-based pruning method,required phrase table size,last experiment,phrase-table pruning method,maximum phrase length,figure,comparison,method,length-based approach,length,6-word phrase,5-word phrase,single-word phrase,phrase length,number,source language word,significance-based pruning,length-based approach,similar result,language,phrase table, 1 b leu point,percentage,phrase,figure,translation quality,function,phrase table size,entropy,different constant,7 c onclusions phrase table pruning,ad-hoc way,heuristic,section,wrong technique,sig nificant drop,translation quality,phrase table size,novel entropy-based criterion,phrase ta ble pruning,sound theoretical foundation,systematic experimental comparison,method,new entropy criterion,experiment,language pair,medium,large data condition,conclusion,large part,phrase table,fre quency,source phrase,account,translation quality,function,phrase table size,significance-based pruning,saving,phrase table size,pruning method,previous work,novel entropy,bleu score,number,phrase,8 f uture work currently,ac count,segmentation,estimate,distortion cost,phrase model,language model,entropy,criterion,hierarchical machine translation system,chiang,reduc tions,phrase table size,en try,reference thorsten brant,jeffrey dean,large language mod el,machine translation,proceeding,joint conference,empirical method,nat ural language processing,prague,czech republic,association,computational linguistics,stephen,della pietra,vincent,della pietra,robert,mercer,mathemat ic,statistical machine translation,parameter,mation,computational linguistics,yu chen,andreas eisele,martin kay,statistical machine translation efficiency,triangulation,proceeding,ternational conference,language resource,marrakech,morocco,lrec-conf,org proceeding,martin kay,andreas eisele,multilingual data,faster,sta tistical translation,proceeding,human lan guage technology,annual conference,north american chapter,association,computational linguistics,boulder,colorado,association,computational lin guistics,david chiang,hierarchical phrase-based trans lation,computational linguistics,thomas,thomas,element,information theory,wiley-interscience,nan duan,ming zhou,phrase extraction, mbr phrase scoring,prun ing,proceeding,mt summit  xii,xiamen,september,matthias eck,stephan vogel,alex waibel,phrase pair relevance,machine transla tion,proceeding,copenhagen,denmark,september,matthias eck,stephan vogel,alex waibel,translation model,usage statistic,sta tistical machine translation,human language technology,conference,north american chapter,association,computa tional linguistics,companion volume,short paper,rochester,new york,association,computational linguistics,kenneth heafield,faster,language model query,proceeding,sixth workshop,statistical machine translation,edinburgh,scotland,association,computational linguistics,howard johnson,joel martin,george foster,roland kuhn,translation quality,proceeding,joint conference,empirical method,nat ural language processing,prague,czech republic,association,computational linguistics,philipp koehn,franz joseph och,daniel marcu,statistical phrase-based translation,human language technology conf,orth american chap ter,computational linguistics,edmon ton,canada,may june,philipp koehn,hieu hoang,alexandra birch,chris callison-burch,marcello federico,nicola bertoldi,brooke cowan,wade shen,christine moran,richard zen,chris dyer,ej bojar,alexandra constan tine,evan herbst,open source toolkit,statistical machine translation,nual meeting,computational linguis tic,prague,czech republic,philipp koehn,pharaoh,beam search decoder,phrase-based statistical machine translation mod el,machine translation,wolfgang macherey,franz och,ignacio thayer,jakob uszkoreit,lattice-based minimum error rate training,statistical machine translation,pro ceedings,conference,empirical meth od,natural language processing,october,association,computational linguistics,robert,log-likelihood-ratios,significance,rare event,proceeding,conference,empirical method,natural language processing,franz josef och,hermann ney,align ment template approach,statistical machine transla tion,computational linguistics,de cember,franz josef och,minimum error rate training,statistical machine translation,annual meet ing,sapporo,kishore papineni,salim roukos,todd ward,wei jing zhu,method,automatic evalua tion,machine translation,annual meeting,adam paul,dan klein,faster,n-gram language model,proceeding,annual meeting,association,computational linguistics,human language technology,portland,oregon,association,computational linguistics,german sanchis-trilles,daniel ortiz-martinez,gonzalez-rubio,jorge gonzalez,francisco casacuberta,bilingual segmentation,phrasetable pruning,statistical machine translation,proceeding,15th conference,european association,machine translation,leuven,belgium,andreas stolcke,entropy-based pruning,language model,news transcription,understanding workshop,david talbot,mile osborne,bloom filter language model,tera-scale lm,proceeding,joint conference,empirical method,natural language process ing,prague,czech re public,association,computational linguis tic,nadi tomeh,nicola cancedda,marc dymetman,complexity-based phrase-table filtering,sta tistical machine translation,proceeding,mt summit  xii,ottawa,ontario,canada,august,nadi tomeh,marco turchi,guillaume wisniewski,alexandre allauzen,ois yvon,phrase,phrase quality,single class classification,proceeding,inter national workshop,spoken language translation,san francisco,california,december,stephan vogel,hermann ney,christoph tillmann,statistical trans lation,copenhagen,denmark,august,mei yang,jing zheng,toward,hierarchical phrase-based  smt,pro ceedings, acl -ijc nlp ,conference short paper,suntec,singapore,august,association,computational linguistics,richard zen,hermann ney,discriminative reordering model,statistical machine translation,human language technology conf,chapter,workshop,sta tistical machine translation,richard zen,hermann ney,improvement,dynamic programming beam search,phrase-based statistical machine translation,proceeding,international workshop,spoken language transla tion,honolulu,hawaii,october,richard zen,franz josef och,hermann ney,phrase-based statistical machine translation,lakemeyer,editor,25th german conf,lecture note,aachen,germany,september,springer verlag,richard zen,evgeny matusov,hermann ney,word alignment,symmetric lexicon model,geneva,switzer land,august,proceeding,annual meeting,association,computational linguistics,portland,oregon,association,computational linguistics binarized forest,string translation hao zhang google research haozhang google,com licheng fang computer science department university,com xiaoyun wu google research xiaoyunwu google,com abstract tree-to-string translation,forest to-string translation approach,parser error,transla tion error,forest,alterna tive tree,source language parser,alternative approach,forest,sub-trees,binarization,binarization,non-consitituent phrase,sentence,desirable prop erty,nonterminal,grammar constant,decoding,purpose,search error,syn chronous binarization technique,forest-to string decoding,tech niques,fast shift-reduce parser,significant quality gain,ble point,phrase-based system,ble point,significant gain,english,german,french,spanish,czech track,1 i ntroduction,recent year,researcher,wide spectrum,approach,syntax,structure,machine translation model,uni fying framework,synchronous grammar,chiang,tree transducer,graehl,knight,monolingual parsing,source side,target side,inference,general category,framework,string-to-string,chiang,zollmann,venugopal,galley,tree-to-string,eisner,search,string-to-x model,possible source par,target side,tree-to-x model search,sub space,structure,source side,input tree,tree-to-x mod el,mod el,multi level tree fragment,source side,context,account,transla tion,poutsma,advan tage,efficiency,accuracy,forest-to string model,compact representation,many tree,tree-to-string model,tradition,forest,hyper-edge pruning,k-best search space,monolin gual parser,parameter,forest,forest,syntactic variant,structural variant,syntactic variant,parser,substring,noun phrase,verb phrase,certain case,structural variant,source span,translation,syn tactic variant,word sense disam biguation,spurious ambi guities,chiang,structural variant,family,bina rization algorithm,single constituent tree,packed forest,binary tree,combination,adjacent tree node,freedom,tree node binary combination,restrict,distance,common ancestor,tree node,result,distance,tree node,common grand-parent,contrast,conventional parser-produced-forest to-string model,parser,sub-structures,tree binarizer,arbitary pruning parameter,forest size,integer number,degree,tree structure violation,grammar constant, ghk rule,galley,multi-level tree fragment,synchronous grammar, ghk algorithm,chronous translation rule,non terminal,factor,source tree,first time,string-to-tree decoding,synchronous binariza tion,search error,translation quality,forest-to-string decoding,whole pipeline,parser,highest-scored tree,input sentence,second,parse tree,binarization algorithm,result ing,binary packed forest,forest-based variant, ghk algorithm,new forest,rule extrac tion,fourth,translation,applicable translation rule,synchronous binarization algorithm,binary translation forest,bottom-up,algorithm,intergrated lm intersection,cube pruning technique,chiang,section,overview,forest-to string model,section,flexible algorithm, ghk rule,principle,cube pruning,chiang,source tree binarization algorithm,binarized forest,section,synchronous rule factorization,forest-to-string decoder,experimental result,section,orest-to-string translation forest-to-string model,source string,tar get string,forest,synchronous derivation,target side yield,derivation,search problem,derivation,probability,deriva tions,parse tree,input sentence,log probability,derivation,lin ear combination,local feature,dy namic programming,optimal combination,galley,tree-to-string model,introduction,search space,first-best parse,good translation rule,first-best parse,good translation,alternative par,local feature,tree-to string rule,synchronous grammar rule,sequence,terminal,nontermi nals,source side,sequence,target terminal,nonterminals,map ping,nonterminals,terminal,figure,typical english-chinese tree-to-string rule,nonterminals,different number,terminal,vp vbd,x2 x1 figure,example tree-to-string rule,forest-to-string translation,first stage,rule extraction,word-aligned parallel text,source forest,second stage,rule enumeration,forest,input string,tree node,source side,tree fragment,tree fragment,cube-pruning style algorithm,rule extraction,training,rule enumeration,algorithm,first step,put forest,boolean,interest,tree fragment generation,admissible node,rule extraction,phrase pair,un derlying word alignment,decoding,complete ness,search,initial one-node tree fragment,admissible node,tree fragment generation process,second step,cube-pruning style bottom-up combination,pruned list,tree fragment,tree node,third step,and-match tree-to-string rule,tree fragment,admissible node, a c ube-pruning-inspired algorithm,tree fragment composition galley,minimal tree-to-string rule,galley,tree-to-string rule,impor tant,translation,anal ogy,word-based model,composed rule extraction,cube-pruning,chiang,process,k-best,lan guage model state,priority queue,k-best lm state,multiple list,pri ority queue,combina tions,top-most element,ranking function,breadth-first expansion heuristic,galley,fig ure,tree-to-string rule,height,tree fragment,number,bottom-level node,terminal,non-terminals,number,terminal,additive operator,min operator,operator,composition,operator,concrete example,figure,monotonicity property,operator,def inition,algorithm,like cube-pruning,approximate k-shortest-path algorithm,3 s ource tree binarization,motivation,tree binarization,rare structure,frequent one,generalization,example,penn treebank annotation,phrase level,translation rule,flat phrase,long sequence,vp-c vpb pp,vp-c vpb pp p n p-c,figure,tree-to-string rule composition,cube-pruning,composed rule,geometric measure,height,frontier,terminal,gluing rule,right part,cube view,combination space,top-left corner,neighbor,commonality,subsequence,example,binarization,head-out explore sharing,prefix,suffix,many binarization choice,algorithm,single bracketing structure,sequence,possible binarizations,a c yk algorithm,packed forest,bi nary tree,sibling sequence, cyk binarization,original tree structure,cross-bracket span,example,english,chinese,phrase,chinese,correct english parse tree,subject-verb boundary,result,tree-to-string translation,constituent phrase,good translation rule, cyk binarization,al gorithm,parameterization,basic  cyk binarization,binarization,parent node,distant ancestor, cyk algo rithm,ancestor,source tree,bina rization,common ancestor,ancestor chain,common ancestor,past gener ations,algorithm,new tree node,new tree node,node label,ancestor chain,algorithm,ancestor annotation,new tree node,semiring notation,good man,ancestor computation,ances tor chain,hyper-edge,ancestor chain,hyper-node,node label computation,concatenate,concatenate,concatenation,node la bel,length,tree-sequence,sequence,sub-trees,adjacent span,final label,new node,corresponds,tree sequence,min imum length,sequence,node span,ancestor chain,new node,common ancestor,minimum tree sequence,clarity,full  cyk,potential hyper-edges,length,source string,reality,scendants,ancestor,branching factor,number,descendant,generation,algorithm,input sentence,parameter,vbn pp,np-c vp vbd, p p 3 n p-cfi gure,alternative binary par,origi nal tree fragment,figure,binarization,binarization,chart representation,bottom,concatenation symbol,binarization,figure,example,alternative tree, cyk algorithm,example,standard  cyk binarization,new tree,algorithm,new tree,degree,freedom,ynchronous binarization,forest-to-string decoding,section,binarization,transla tion forest,translation hypergraphs,packed forest representation,synchronous derivation,tree-to-string rule,source forest,algorithm work,translation forest,source,binary source forest,binary translation,tree to-string rule,figure,source tree,np-c npb dt,x0 fuze,x0 fuze,np-c npb dt,x0 figure,synchronous binarization,tree-to-string rule,top rule,binary,source tree binarization,translation rule,variable,synchronous binariza tion,translation rule,vari ables,second rule,com mon pattern,many transla tion rule,derivation,search,factorized translation forest,challenge,synchronous binarization,forest-to-string system,large tree fragment,first step,solution,matching,original rule,synchronous binarization,matching rule,factor rule,derivation forest,offline binarization scheme,core algorithm,5 e xperiments,experiment,public data set,english,chinese,french,german,algorithm  1 t,-n b inarization algorithm,function,bottom-up topological order,forest output,ancestor,ancestor,label node,length,ancestor,node begin,create,new node,sequence,terminal,hyper-edge,translation,method,english-to-chinese translation,allowed training set,english,european language,training data set,callison burch,sen tences,parallel text,filtering limit,test data set,corpus statis tic,bilingual training data set,source word target word,english-czech  66m  57m,english-german  45m  43m,parallel text,word alignment step,iteration, ibm model-1,iteration,iteration, ibm model,addition,model-1,word align ments,heuris tic,standard phrase extraction heuristic,phrase pair,length limit,hierar chical phrase extraction algorithm,standard heuristic,chiang,phrase-length limit,maximum number,symbol,source side,target side,data set,tree-to-string rule extraction algorithm,section,tree node,default parser,experiment,dependency parser,scholz,attachment score,unlabeled attachment score,standard penn treebank test set,dependency par,part of-speech tag,head word,correspond,phrase structure,system,phrase-based sys tem,hierarchical phrase,system,chiang,forest-to string systemwith different binarization scheme,phrase-based decoder,jump width,hierarchical decoder,glue rule,forest-to-string sys tem,length-based reordering constraint,5-gram language model,kneser-ney,target lan guages,target side,par allel text,corpus,evaluation,gigaword corpus,chinese,news corpus,others,standard fea tures,phrase-based decoder, a m aximum entropy phrasal,hierarchi cal decoder,forest-to-string decoder,standard feature,feature weight tuning,minimum error rate training,n-best list,train ing,hypergraph-based  mer,translation result,papineni,translation result table,system,binarization scheme,phrase,system,hierarchical phrase-based sys tem,system,system,data set,english-chinese data set,improvement,phrase-based sys tem,hierarchi cal phrase-based system,european language,improvement,phrase-based baseline,hierar chical phrase-based system,improvement,difference,english-czech,confidence level,bootstrap method,strength,system,baseline system,re sults,data set, wmt work shop,forest-to-string system,lan guage,different binarization method,translation result,bf2s system,cyk binarization algorithm,bracket violation degree,section,dev test english-chinese pb,english-french wmt,translation result,binarized-forest-to-string system,phrase-based system,hierarchical phrase-based system,comparison,result,result,degree,forest,incremen,single tree,different tree binarization method,english-chinese task,optimal binarization parameter,language pair,non-standard data set,phrase,bracket,original tree,parser,reason,bracket,interesting phrase,translation unit,forest,parameter,resulting translation model,binarizer,parser,natural question,binarizer-generated forest,parser-generated forest,translation,question,ble rule,binarization,head-out,comparing different source tree binarization scheme,english-chinese translation,ble score,model size,normal phrase,leaf level,parser,packed forest,fast deterministic dependency parser,packed forest, a c rf constituent parser,finkel,state-of-the-art ac curacy,standard penn treebank test set,f-score, a c yk,programming inference,parser,posterior probability,parser,penn treebank training data,binarization,packed forest,pro duce,binarized forest,system,cyk-2 binarizer,forest, crf parser,threshold,parser,binary tree,cross-bracket cyk-2 binarization,forest,parser-generated forest,forest-to-string english-german translation,comparison,binarization,parser forest,english-german translation,result,cyk-2 forest performs,hyper-edges,negative log posterior probability,difference,pruning,forest,k-best algorithm,forest-pruning,full  cyk chart,result,aggressive pruning,parser forest,full exploration,parameter,parser-forest,constituent parser,efficiency bottleneck,advantage,binarizer,forest-to string scheme,parser,projective par,hand-tuning,parameter,synchronous binarization,section,effect,syn chronous binarization,forest-to-string translation,experiment,english-chinese data set,baseline system,k-way cube pruning,maximum number,nonterminals,right-hand side,synchronous translation rule,input grammar,system,synchronous binarization,section,grammar,input sentence,minimum branching factor,applies,way cube,binarization,cyk-2 cube,binarization,effect,synchronous binarization,forest-to-string system,english chinese task,synchronous binarization,search error,translation,setting,adjacent syntactic cate gories,various syntax-based model,zollmann,venugopal,hierarchial phrase,system,joint syntactic category,tree sequence-to-string translation rule,good solution,joint subtrees,con nection,tree structure,k-best forest,tree sequence,con straint,tree-sequence node,many bracket,target tree binarization,rule extraction,string-to-tree sys tem,binarization forest,cyk-1 forest,contrast,binarization scheme,tree-to-string rule,binarized forest,different method,translation rule binarization,argu ment,tree-to-string decoding target side binarization,synchronous binariza tion,discontinous source span,state space,forest-to-string senario,string-to tree decoding,state-sharing,experiment,synchronous binariza tion,forest-to-string case,7 c onclusion,new approach,tree-to-string translation,source tree binarization step,standard forest-to-string translation step,method,k-best parser,packed forest,state-of-the-art result,fast parser,simple tree binarizer,bracket,binarized node,search error,forest-to-string translation,syn chronous binarization technqiue,search,significant gain,addition,new cube-pruning-style algorithm,rule extraction,new algorithm,figure-of-merit,extraction,future,learning,trans lation rule,binarized forest,acknowledgment,member,mt team,google,ashish venugopal,franz och,dis cussions,daniel gildea,suggestion,reference chris callison-burch,philipp koehn,christof monz,kay peterson,mark przybocki,omar zaidan,finding,joint workshop,statisti cal machine translation,metric,machine trans lation,proceeding,joint fifth workshop,statistical machine translation,uppsala,sweden,association,computational linguistics,august,david chiang,hierarchical phrase-based model,statistical machine translation,proceeding,annual conference,association,david chiang,hierarchical phrase-based transla tion,computational linguistics,steve  den eefe,kevin knight,wei wang,daniel marcu,syntax-based mt learn,proceeding,joint conference,empirical method,natural lan guage processing,prague,czech republic,association,com putational linguistics,jason eisner,non-isomorphic tree map ping,machine translation,proceeding,meeting,association,computational linguistics,companion volume,sap poro,jenny rose finkel,alex kleeman,christopher,manning,conditional random field,proceeding,columbus,associa tion,computational linguistics,michel galley,mark hopkins,kevin knight,daniel marcu,translation rule,pro ceedings,meeting,north american chapter,association,computational linguis tic,michel galley,jonathan graehl,kevin knight,daniel marcu,steve  den eefe,wei wang,ignacio thayer,scalable inference,training,context-rich syntactic translation model,proceed ings,international conference,computational linguistics association,joshua goodman,computa tional linguistics,jonathan graehl,kevin knight,tree transducer,proceeding,meeting,north american chapter,association,liang huang,kevin knight,aravind joshi,statistical syntax-directed translation,extended domain,locality,proceeding,biennial conference,association,machine translation,liang huang,binarization,synchronous bina rization,target-side binarization,proceeding,syntax,struc ture,liang huang,reranking,non-local feature,proceeding,annual conference,association,compu tational linguistics,philipp koehn,franz josef och,daniel marcu,statistical phrase-based translation,proceed ings,meeting,north american chap ter,association,computational linguistics,alberta,philipp koehn,statistical significance test,machine translation evaluation,conference,empirical method,natural language process,barcelona,shankar kumar,wolfgang macherey,chris dyer,franz och,efficient minimum error rate train ing,minimum bayes-risk decoding,translation hypergraphs,lattice,proceeding,joint conference,47th annual meeting,international joint conference,natural lan guage processing, afn lp,sun tec,singapore,august,association,computational linguistics,dekang lin,path-based transfer model,machine translation,proceeding,ternational conference,geneva,switzerland,yang liu,qun liu,shouxun lin,tree-to string alignment template,statistical machine trans lation,proceeding,international conference,computational linguistics association,sydney,au tralia,yang liu,yun huang,qun liu,shouxun lin,forest-to-string statistical translation rule,pro ceedings,45th annual conference,associ ation,haitao mi,liang huang,forest-based transla tion rule extraction,proceeding,con ference,empirical method,natural language processing,honolulu,hawaii,octo ber,association,computational linguistics,qun liu,forest,translation,proceeding,nual conference,association,computational linguistics,joakim nivre,mario scholz,deterministic dependency parsing,english text,proceeding,coling,geneva,switzerland,hermann ney,align ment template approach,statistical machine transla tion,computational linguistics,franz josef och,minimum error rate training,statistical machine translation,proceeding,annual conference,association,salim roukos,todd ward,wei jing zhu,method,automatic eval uation,machine translation,proceeding,40th annual conference,association,data-oriented translation,proceeding,international conference,chris quirk,arul menezes,colin cherry,de pendency treelet translation,phrasal  smt,proceeding,annual con ference,association,ann arbor,michigan,libin shen,ralph weischedel,new string-to-dependency machine translation,target dependency language model,proceeding,annual conference,sociation,computational linguistics,human lan guage technology,wei wang,kevin knight,daniel marcu,syntax tree,syntax-based ma chine translation accuracy,proceeding,joint conference,empirical method,nat ural language processing,prague,czech republic,association,computational linguistics,richard zen,hermann ney,discriminative reordering model,statistical machine translation,proceeding,workshop,statistical ma chine translation,new york city,association,computational linguistics,hao zhang,liang huang,daniel gildea,kevin knight,synchronous binarization,machine translation,proceeding,meeting,north american chapter,association,min zhang,hongfei jiang,tree sequence alignment-based tree-to-tree translation model,proceeding,colum bus,association,computational lin guistics,hui zhang,min zhang,chew lim tan,forest-based tree sequence,translation model,proceeding,joint conference,47th annual meeting,international joint conference,natural language processing, afn lp,suntec,singapore,august,association,computa tional linguistics,andreas zollmann,ashish venugopal,syntax,machine translation,proceeding,workshop,statistical machine translation,new york city,sociation,computational linguistics